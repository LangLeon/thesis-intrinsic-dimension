model : table13slim
N : 5
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:58

Channel scaling factor: 2.25

The number of parameters is: 276579

The number of individual parameters is:

18
288
18
18
27
41796
27
27
54
125388
54
54
64
103680
64
64
4096
64
640
10
64
64

nonzero elements in E: 13828948
elements in E: 13828950
fraction nonzero: 0.9999998553758601
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.36; acc: 0.16
Batch: 20; loss: 2.28; acc: 0.09
Batch: 40; loss: 2.18; acc: 0.17
Batch: 60; loss: 2.23; acc: 0.23
Batch: 80; loss: 2.11; acc: 0.28
Batch: 100; loss: 2.16; acc: 0.23
Batch: 120; loss: 2.11; acc: 0.22
Batch: 140; loss: 2.17; acc: 0.23
Batch: 160; loss: 2.03; acc: 0.31
Batch: 180; loss: 2.27; acc: 0.19
Batch: 200; loss: 2.1; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.25
Batch: 240; loss: 2.02; acc: 0.36
Batch: 260; loss: 2.06; acc: 0.23
Batch: 280; loss: 2.04; acc: 0.27
Batch: 300; loss: 1.99; acc: 0.31
Batch: 320; loss: 1.92; acc: 0.42
Batch: 340; loss: 1.88; acc: 0.44
Batch: 360; loss: 1.96; acc: 0.33
Batch: 380; loss: 1.96; acc: 0.36
Batch: 400; loss: 1.96; acc: 0.36
Batch: 420; loss: 1.84; acc: 0.44
Batch: 440; loss: 1.95; acc: 0.3
Batch: 460; loss: 2.01; acc: 0.34
Batch: 480; loss: 1.96; acc: 0.44
Batch: 500; loss: 1.95; acc: 0.33
Batch: 520; loss: 2.1; acc: 0.25
Batch: 540; loss: 1.99; acc: 0.36
Batch: 560; loss: 1.97; acc: 0.38
Batch: 580; loss: 1.94; acc: 0.36
Batch: 600; loss: 1.92; acc: 0.36
Batch: 620; loss: 1.95; acc: 0.36
Batch: 640; loss: 1.95; acc: 0.38
Batch: 660; loss: 1.96; acc: 0.31
Batch: 680; loss: 1.93; acc: 0.42
Batch: 700; loss: 1.88; acc: 0.52
Batch: 720; loss: 1.91; acc: 0.44
Batch: 740; loss: 1.87; acc: 0.47
Batch: 760; loss: 1.96; acc: 0.39
Batch: 780; loss: 1.96; acc: 0.38
Train Epoch over. train_loss: 2.02; train_accuracy: 0.34 

2.311593016202096e-05
4.9499467422720045e-06
Batch: 0; loss: 1.93; acc: 0.36
Batch: 20; loss: 1.87; acc: 0.38
Batch: 40; loss: 1.76; acc: 0.52
Batch: 60; loss: 1.89; acc: 0.42
Batch: 80; loss: 1.93; acc: 0.34
Batch: 100; loss: 1.93; acc: 0.31
Batch: 120; loss: 1.87; acc: 0.47
Batch: 140; loss: 1.78; acc: 0.41
Val Epoch over. val_loss: 1.894093121692633; val_accuracy: 0.41968550955414013 

The current subspace-distance is: 4.9499467422720045e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.91; acc: 0.44
Batch: 20; loss: 1.98; acc: 0.3
Batch: 40; loss: 1.91; acc: 0.48
Batch: 60; loss: 1.95; acc: 0.36
Batch: 80; loss: 1.84; acc: 0.48
Batch: 100; loss: 1.91; acc: 0.45
Batch: 120; loss: 1.88; acc: 0.41
Batch: 140; loss: 1.88; acc: 0.42
Batch: 160; loss: 1.88; acc: 0.41
Batch: 180; loss: 1.95; acc: 0.38
Batch: 200; loss: 1.94; acc: 0.38
Batch: 220; loss: 1.92; acc: 0.42
Batch: 240; loss: 1.86; acc: 0.39
Batch: 260; loss: 1.85; acc: 0.45
Batch: 280; loss: 1.95; acc: 0.42
Batch: 300; loss: 1.93; acc: 0.3
Batch: 320; loss: 1.9; acc: 0.42
Batch: 340; loss: 1.84; acc: 0.42
Batch: 360; loss: 1.84; acc: 0.56
Batch: 380; loss: 1.91; acc: 0.41
Batch: 400; loss: 1.93; acc: 0.36
Batch: 420; loss: 1.86; acc: 0.47
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.88; acc: 0.45
Batch: 480; loss: 1.82; acc: 0.53
Batch: 500; loss: 1.85; acc: 0.44
Batch: 520; loss: 1.87; acc: 0.44
Batch: 540; loss: 1.9; acc: 0.45
Batch: 560; loss: 1.78; acc: 0.58
Batch: 580; loss: 1.83; acc: 0.5
Batch: 600; loss: 1.87; acc: 0.41
Batch: 620; loss: 1.85; acc: 0.42
Batch: 640; loss: 1.9; acc: 0.47
Batch: 660; loss: 1.9; acc: 0.41
Batch: 680; loss: 1.82; acc: 0.47
Batch: 700; loss: 1.86; acc: 0.44
Batch: 720; loss: 1.83; acc: 0.41
Batch: 740; loss: 1.83; acc: 0.42
Batch: 760; loss: 1.86; acc: 0.48
Batch: 780; loss: 1.84; acc: 0.47
Train Epoch over. train_loss: 1.89; train_accuracy: 0.43 

2.4778109946055338e-05
4.990864454157418e-06
Batch: 0; loss: 1.9; acc: 0.33
Batch: 20; loss: 1.79; acc: 0.5
Batch: 40; loss: 1.67; acc: 0.58
Batch: 60; loss: 1.8; acc: 0.5
Batch: 80; loss: 1.8; acc: 0.53
Batch: 100; loss: 1.88; acc: 0.47
Batch: 120; loss: 1.89; acc: 0.44
Batch: 140; loss: 1.6; acc: 0.66
Val Epoch over. val_loss: 1.8342265224760506; val_accuracy: 0.48168789808917195 

The current subspace-distance is: 4.990864454157418e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.85; acc: 0.5
Batch: 20; loss: 1.79; acc: 0.5
Batch: 40; loss: 1.79; acc: 0.47
Batch: 60; loss: 1.81; acc: 0.5
Batch: 80; loss: 1.94; acc: 0.36
Batch: 100; loss: 1.82; acc: 0.42
Batch: 120; loss: 1.82; acc: 0.5
Batch: 140; loss: 1.83; acc: 0.45
Batch: 160; loss: 1.84; acc: 0.47
Batch: 180; loss: 1.94; acc: 0.42
Batch: 200; loss: 1.92; acc: 0.42
Batch: 220; loss: 1.81; acc: 0.47
Batch: 240; loss: 1.88; acc: 0.36
Batch: 260; loss: 1.95; acc: 0.41
Batch: 280; loss: 1.86; acc: 0.47
Batch: 300; loss: 1.88; acc: 0.44
Batch: 320; loss: 1.78; acc: 0.52
Batch: 340; loss: 1.75; acc: 0.55
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.82; acc: 0.56
Batch: 400; loss: 1.76; acc: 0.58
Batch: 420; loss: 1.85; acc: 0.41
Batch: 440; loss: 1.79; acc: 0.53
Batch: 460; loss: 1.91; acc: 0.41
Batch: 480; loss: 1.81; acc: 0.48
Batch: 500; loss: 1.88; acc: 0.48
Batch: 520; loss: 1.78; acc: 0.42
Batch: 540; loss: 1.77; acc: 0.55
Batch: 560; loss: 1.9; acc: 0.42
Batch: 580; loss: 1.83; acc: 0.5
Batch: 600; loss: 1.85; acc: 0.53
Batch: 620; loss: 1.86; acc: 0.41
Batch: 640; loss: 1.73; acc: 0.55
Batch: 660; loss: 1.81; acc: 0.52
Batch: 680; loss: 1.72; acc: 0.53
Batch: 700; loss: 1.81; acc: 0.5
Batch: 720; loss: 1.79; acc: 0.52
Batch: 740; loss: 1.75; acc: 0.62
Batch: 760; loss: 1.81; acc: 0.53
Batch: 780; loss: 1.85; acc: 0.45
Train Epoch over. train_loss: 1.84; train_accuracy: 0.47 

2.7088564820587635e-05
7.146569259930402e-06
Batch: 0; loss: 1.85; acc: 0.52
Batch: 20; loss: 1.78; acc: 0.5
Batch: 40; loss: 1.59; acc: 0.67
Batch: 60; loss: 1.76; acc: 0.58
Batch: 80; loss: 1.74; acc: 0.52
Batch: 100; loss: 1.8; acc: 0.53
Batch: 120; loss: 1.86; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.72
Val Epoch over. val_loss: 1.790950397017655; val_accuracy: 0.5125398089171974 

The current subspace-distance is: 7.146569259930402e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.58
Batch: 20; loss: 1.77; acc: 0.47
Batch: 40; loss: 1.88; acc: 0.41
Batch: 60; loss: 1.77; acc: 0.45
Batch: 80; loss: 1.79; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.47
Batch: 120; loss: 1.78; acc: 0.47
Batch: 140; loss: 1.9; acc: 0.47
Batch: 160; loss: 1.77; acc: 0.56
Batch: 180; loss: 1.75; acc: 0.44
Batch: 200; loss: 1.84; acc: 0.48
Batch: 220; loss: 1.75; acc: 0.5
Batch: 240; loss: 1.91; acc: 0.39
Batch: 260; loss: 1.75; acc: 0.53
Batch: 280; loss: 1.84; acc: 0.44
Batch: 300; loss: 1.76; acc: 0.55
Batch: 320; loss: 1.85; acc: 0.45
Batch: 340; loss: 1.77; acc: 0.55
Batch: 360; loss: 1.77; acc: 0.56
Batch: 380; loss: 1.8; acc: 0.48
Batch: 400; loss: 1.92; acc: 0.36
Batch: 420; loss: 1.83; acc: 0.47
Batch: 440; loss: 1.81; acc: 0.5
Batch: 460; loss: 1.76; acc: 0.47
Batch: 480; loss: 1.84; acc: 0.48
Batch: 500; loss: 1.83; acc: 0.45
Batch: 520; loss: 1.71; acc: 0.58
Batch: 540; loss: 1.83; acc: 0.48
Batch: 560; loss: 1.78; acc: 0.52
Batch: 580; loss: 1.82; acc: 0.52
Batch: 600; loss: 1.83; acc: 0.41
Batch: 620; loss: 1.77; acc: 0.45
Batch: 640; loss: 1.81; acc: 0.52
Batch: 660; loss: 1.84; acc: 0.44
Batch: 680; loss: 1.81; acc: 0.52
Batch: 700; loss: 1.81; acc: 0.34
Batch: 720; loss: 1.82; acc: 0.41
Batch: 740; loss: 1.79; acc: 0.52
Batch: 760; loss: 1.74; acc: 0.64
Batch: 780; loss: 1.74; acc: 0.58
Train Epoch over. train_loss: 1.81; train_accuracy: 0.49 

2.8026506697642617e-05
7.508557246183045e-06
Batch: 0; loss: 1.8; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.45
Batch: 40; loss: 1.53; acc: 0.69
Batch: 60; loss: 1.73; acc: 0.59
Batch: 80; loss: 1.69; acc: 0.55
Batch: 100; loss: 1.77; acc: 0.5
Batch: 120; loss: 1.84; acc: 0.44
Batch: 140; loss: 1.44; acc: 0.77
Val Epoch over. val_loss: 1.7545554212703827; val_accuracy: 0.5213972929936306 

The current subspace-distance is: 7.508557246183045e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.45
Batch: 20; loss: 1.76; acc: 0.55
Batch: 40; loss: 1.87; acc: 0.44
Batch: 60; loss: 1.77; acc: 0.55
Batch: 80; loss: 1.74; acc: 0.61
Batch: 100; loss: 1.72; acc: 0.53
Batch: 120; loss: 1.78; acc: 0.5
Batch: 140; loss: 1.78; acc: 0.53
Batch: 160; loss: 1.68; acc: 0.59
Batch: 180; loss: 1.72; acc: 0.55
Batch: 200; loss: 1.77; acc: 0.47
Batch: 220; loss: 1.8; acc: 0.48
Batch: 240; loss: 1.65; acc: 0.55
Batch: 260; loss: 1.86; acc: 0.39
Batch: 280; loss: 1.79; acc: 0.55
Batch: 300; loss: 1.9; acc: 0.38
Batch: 320; loss: 1.9; acc: 0.34
Batch: 340; loss: 1.76; acc: 0.52
Batch: 360; loss: 1.81; acc: 0.5
Batch: 380; loss: 1.78; acc: 0.52
Batch: 400; loss: 1.78; acc: 0.55
Batch: 420; loss: 1.78; acc: 0.5
Batch: 440; loss: 1.81; acc: 0.48
Batch: 460; loss: 1.83; acc: 0.42
Batch: 480; loss: 1.79; acc: 0.44
Batch: 500; loss: 1.78; acc: 0.53
Batch: 520; loss: 1.72; acc: 0.58
Batch: 540; loss: 1.82; acc: 0.45
Batch: 560; loss: 1.71; acc: 0.61
Batch: 580; loss: 1.79; acc: 0.48
Batch: 600; loss: 1.82; acc: 0.44
Batch: 620; loss: 1.78; acc: 0.47
Batch: 640; loss: 1.81; acc: 0.5
Batch: 660; loss: 1.93; acc: 0.41
Batch: 680; loss: 1.7; acc: 0.58
Batch: 700; loss: 1.85; acc: 0.39
Batch: 720; loss: 1.84; acc: 0.45
Batch: 740; loss: 1.81; acc: 0.45
Batch: 760; loss: 1.81; acc: 0.48
Batch: 780; loss: 1.87; acc: 0.47
Train Epoch over. train_loss: 1.78; train_accuracy: 0.5 

2.9551101761171594e-05
7.933153028716333e-06
Batch: 0; loss: 1.79; acc: 0.5
Batch: 20; loss: 1.76; acc: 0.47
Batch: 40; loss: 1.52; acc: 0.67
Batch: 60; loss: 1.72; acc: 0.58
Batch: 80; loss: 1.69; acc: 0.55
Batch: 100; loss: 1.75; acc: 0.53
Batch: 120; loss: 1.83; acc: 0.41
Batch: 140; loss: 1.45; acc: 0.72
Val Epoch over. val_loss: 1.7400261702810882; val_accuracy: 0.5259753184713376 

The current subspace-distance is: 7.933153028716333e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.48
Batch: 20; loss: 1.76; acc: 0.53
Batch: 40; loss: 1.81; acc: 0.5
Batch: 60; loss: 1.75; acc: 0.48
Batch: 80; loss: 1.69; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.5
Batch: 120; loss: 1.81; acc: 0.45
Batch: 140; loss: 1.76; acc: 0.52
Batch: 160; loss: 1.86; acc: 0.44
Batch: 180; loss: 1.78; acc: 0.5
Batch: 200; loss: 1.8; acc: 0.55
Batch: 220; loss: 1.69; acc: 0.58
Batch: 240; loss: 1.93; acc: 0.44
Batch: 260; loss: 1.71; acc: 0.59
Batch: 280; loss: 1.82; acc: 0.39
Batch: 300; loss: 1.73; acc: 0.56
Batch: 320; loss: 1.81; acc: 0.52
Batch: 340; loss: 1.79; acc: 0.47
Batch: 360; loss: 1.81; acc: 0.48
Batch: 380; loss: 1.79; acc: 0.44
Batch: 400; loss: 1.74; acc: 0.5
Batch: 420; loss: 1.72; acc: 0.58
Batch: 440; loss: 1.74; acc: 0.48
Batch: 460; loss: 1.73; acc: 0.56
Batch: 480; loss: 1.77; acc: 0.47
Batch: 500; loss: 1.7; acc: 0.56
Batch: 520; loss: 1.73; acc: 0.53
Batch: 540; loss: 1.72; acc: 0.55
Batch: 560; loss: 1.71; acc: 0.52
Batch: 580; loss: 1.83; acc: 0.56
Batch: 600; loss: 1.76; acc: 0.53
Batch: 620; loss: 1.72; acc: 0.52
Batch: 640; loss: 1.84; acc: 0.47
Batch: 660; loss: 1.79; acc: 0.41
Batch: 680; loss: 1.81; acc: 0.42
Batch: 700; loss: 1.77; acc: 0.53
Batch: 720; loss: 1.75; acc: 0.5
Batch: 740; loss: 1.73; acc: 0.52
Batch: 760; loss: 1.69; acc: 0.53
Batch: 780; loss: 1.72; acc: 0.55
Train Epoch over. train_loss: 1.76; train_accuracy: 0.51 

3.112779086222872e-05
8.967245776148047e-06
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.5; acc: 0.67
Batch: 60; loss: 1.7; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.55
Batch: 100; loss: 1.76; acc: 0.48
Batch: 120; loss: 1.82; acc: 0.45
Batch: 140; loss: 1.47; acc: 0.64
Val Epoch over. val_loss: 1.7207448133237802; val_accuracy: 0.5321457006369427 

The current subspace-distance is: 8.967245776148047e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.59
Batch: 20; loss: 1.83; acc: 0.45
Batch: 40; loss: 1.89; acc: 0.41
Batch: 60; loss: 1.65; acc: 0.64
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.74; acc: 0.55
Batch: 120; loss: 1.78; acc: 0.45
Batch: 140; loss: 1.77; acc: 0.52
Batch: 160; loss: 1.6; acc: 0.66
Batch: 180; loss: 1.75; acc: 0.55
Batch: 200; loss: 1.76; acc: 0.47
Batch: 220; loss: 1.65; acc: 0.56
Batch: 240; loss: 1.76; acc: 0.52
Batch: 260; loss: 1.74; acc: 0.47
Batch: 280; loss: 1.81; acc: 0.45
Batch: 300; loss: 1.7; acc: 0.61
Batch: 320; loss: 1.69; acc: 0.52
Batch: 340; loss: 1.81; acc: 0.47
Batch: 360; loss: 1.84; acc: 0.44
Batch: 380; loss: 1.66; acc: 0.61
Batch: 400; loss: 1.81; acc: 0.42
Batch: 420; loss: 1.73; acc: 0.56
Batch: 440; loss: 1.83; acc: 0.45
Batch: 460; loss: 1.76; acc: 0.5
Batch: 480; loss: 1.83; acc: 0.45
Batch: 500; loss: 1.75; acc: 0.48
Batch: 520; loss: 1.76; acc: 0.42
Batch: 540; loss: 1.75; acc: 0.48
Batch: 560; loss: 1.7; acc: 0.5
Batch: 580; loss: 1.75; acc: 0.44
Batch: 600; loss: 1.84; acc: 0.41
Batch: 620; loss: 1.8; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.48
Batch: 660; loss: 1.81; acc: 0.48
Batch: 680; loss: 1.72; acc: 0.55
Batch: 700; loss: 1.77; acc: 0.41
Batch: 720; loss: 1.73; acc: 0.52
Batch: 740; loss: 1.64; acc: 0.56
Batch: 760; loss: 1.68; acc: 0.55
Batch: 780; loss: 1.71; acc: 0.58
Train Epoch over. train_loss: 1.75; train_accuracy: 0.51 

3.22667219734285e-05
1.0355871381761972e-05
Batch: 0; loss: 1.79; acc: 0.44
Batch: 20; loss: 1.74; acc: 0.5
Batch: 40; loss: 1.5; acc: 0.64
Batch: 60; loss: 1.67; acc: 0.58
Batch: 80; loss: 1.63; acc: 0.59
Batch: 100; loss: 1.76; acc: 0.52
Batch: 120; loss: 1.81; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.64
Val Epoch over. val_loss: 1.7014399880816222; val_accuracy: 0.5416998407643312 

The current subspace-distance is: 1.0355871381761972e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.62
Batch: 20; loss: 1.67; acc: 0.56
Batch: 40; loss: 1.78; acc: 0.56
Batch: 60; loss: 1.87; acc: 0.45
Batch: 80; loss: 1.79; acc: 0.56
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.8; acc: 0.47
Batch: 140; loss: 1.81; acc: 0.47
Batch: 160; loss: 1.76; acc: 0.45
Batch: 180; loss: 1.73; acc: 0.56
Batch: 200; loss: 1.8; acc: 0.52
Batch: 220; loss: 1.64; acc: 0.66
Batch: 240; loss: 1.65; acc: 0.59
Batch: 260; loss: 1.86; acc: 0.45
Batch: 280; loss: 1.63; acc: 0.56
Batch: 300; loss: 1.82; acc: 0.47
Batch: 320; loss: 1.64; acc: 0.55
Batch: 340; loss: 1.8; acc: 0.45
Batch: 360; loss: 1.8; acc: 0.48
Batch: 380; loss: 1.76; acc: 0.53
Batch: 400; loss: 1.65; acc: 0.58
Batch: 420; loss: 1.69; acc: 0.56
Batch: 440; loss: 1.74; acc: 0.55
Batch: 460; loss: 1.7; acc: 0.52
Batch: 480; loss: 1.6; acc: 0.64
Batch: 500; loss: 1.63; acc: 0.58
Batch: 520; loss: 1.69; acc: 0.52
Batch: 540; loss: 1.65; acc: 0.52
Batch: 560; loss: 1.8; acc: 0.44
Batch: 580; loss: 1.71; acc: 0.55
Batch: 600; loss: 1.76; acc: 0.55
Batch: 620; loss: 1.63; acc: 0.64
Batch: 640; loss: 1.63; acc: 0.61
Batch: 660; loss: 1.71; acc: 0.52
Batch: 680; loss: 1.72; acc: 0.58
Batch: 700; loss: 1.6; acc: 0.61
Batch: 720; loss: 1.62; acc: 0.62
Batch: 740; loss: 1.75; acc: 0.53
Batch: 760; loss: 1.64; acc: 0.59
Batch: 780; loss: 1.76; acc: 0.52
Train Epoch over. train_loss: 1.73; train_accuracy: 0.52 

3.341713818372227e-05
8.672690455568954e-06
Batch: 0; loss: 1.8; acc: 0.44
Batch: 20; loss: 1.72; acc: 0.52
Batch: 40; loss: 1.51; acc: 0.67
Batch: 60; loss: 1.66; acc: 0.56
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.75; acc: 0.5
Batch: 120; loss: 1.79; acc: 0.53
Batch: 140; loss: 1.45; acc: 0.66
Val Epoch over. val_loss: 1.6772464141724215; val_accuracy: 0.550656847133758 

The current subspace-distance is: 8.672690455568954e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.59
Batch: 20; loss: 1.75; acc: 0.53
Batch: 40; loss: 1.63; acc: 0.56
Batch: 60; loss: 1.77; acc: 0.38
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.69; acc: 0.53
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.66; acc: 0.55
Batch: 160; loss: 1.64; acc: 0.55
Batch: 180; loss: 1.64; acc: 0.62
Batch: 200; loss: 1.61; acc: 0.61
Batch: 220; loss: 1.57; acc: 0.55
Batch: 240; loss: 1.93; acc: 0.38
Batch: 260; loss: 1.74; acc: 0.47
Batch: 280; loss: 1.75; acc: 0.48
Batch: 300; loss: 1.71; acc: 0.58
Batch: 320; loss: 1.8; acc: 0.48
Batch: 340; loss: 1.63; acc: 0.62
Batch: 360; loss: 1.69; acc: 0.5
Batch: 380; loss: 1.67; acc: 0.55
Batch: 400; loss: 1.71; acc: 0.56
Batch: 420; loss: 1.63; acc: 0.59
Batch: 440; loss: 1.73; acc: 0.5
Batch: 460; loss: 1.76; acc: 0.47
Batch: 480; loss: 1.76; acc: 0.48
Batch: 500; loss: 1.71; acc: 0.47
Batch: 520; loss: 1.7; acc: 0.5
Batch: 540; loss: 1.69; acc: 0.45
Batch: 560; loss: 1.71; acc: 0.56
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.52; acc: 0.66
Batch: 620; loss: 1.65; acc: 0.53
Batch: 640; loss: 1.57; acc: 0.58
Batch: 660; loss: 1.77; acc: 0.38
Batch: 680; loss: 1.66; acc: 0.53
Batch: 700; loss: 1.74; acc: 0.52
Batch: 720; loss: 1.64; acc: 0.52
Batch: 740; loss: 1.63; acc: 0.53
Batch: 760; loss: 1.73; acc: 0.45
Batch: 780; loss: 1.73; acc: 0.47
Train Epoch over. train_loss: 1.69; train_accuracy: 0.53 

3.5697688872460276e-05
1.0741994628915563e-05
Batch: 0; loss: 1.77; acc: 0.39
Batch: 20; loss: 1.65; acc: 0.52
Batch: 40; loss: 1.46; acc: 0.72
Batch: 60; loss: 1.63; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.7; acc: 0.47
Batch: 120; loss: 1.75; acc: 0.56
Batch: 140; loss: 1.38; acc: 0.77
Val Epoch over. val_loss: 1.6245476911022405; val_accuracy: 0.56359474522293 

The current subspace-distance is: 1.0741994628915563e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.45
Batch: 40; loss: 1.74; acc: 0.47
Batch: 60; loss: 1.78; acc: 0.47
Batch: 80; loss: 1.62; acc: 0.62
Batch: 100; loss: 1.75; acc: 0.52
Batch: 120; loss: 1.64; acc: 0.58
Batch: 140; loss: 1.67; acc: 0.59
Batch: 160; loss: 1.55; acc: 0.58
Batch: 180; loss: 1.66; acc: 0.5
Batch: 200; loss: 1.73; acc: 0.5
Batch: 220; loss: 1.71; acc: 0.52
Batch: 240; loss: 1.6; acc: 0.56
Batch: 260; loss: 1.64; acc: 0.52
Batch: 280; loss: 1.64; acc: 0.59
Batch: 300; loss: 1.58; acc: 0.64
Batch: 320; loss: 1.72; acc: 0.5
Batch: 340; loss: 1.6; acc: 0.56
Batch: 360; loss: 1.59; acc: 0.56
Batch: 380; loss: 1.48; acc: 0.67
Batch: 400; loss: 1.6; acc: 0.58
Batch: 420; loss: 1.55; acc: 0.61
Batch: 440; loss: 1.6; acc: 0.48
Batch: 460; loss: 1.68; acc: 0.47
Batch: 480; loss: 1.6; acc: 0.55
Batch: 500; loss: 1.58; acc: 0.52
Batch: 520; loss: 1.59; acc: 0.56
Batch: 540; loss: 1.66; acc: 0.53
Batch: 560; loss: 1.58; acc: 0.56
Batch: 580; loss: 1.72; acc: 0.47
Batch: 600; loss: 1.7; acc: 0.48
Batch: 620; loss: 1.74; acc: 0.47
Batch: 640; loss: 1.65; acc: 0.55
Batch: 660; loss: 1.63; acc: 0.55
Batch: 680; loss: 1.68; acc: 0.47
Batch: 700; loss: 1.54; acc: 0.58
Batch: 720; loss: 1.58; acc: 0.56
Batch: 740; loss: 1.73; acc: 0.5
Batch: 760; loss: 1.56; acc: 0.62
Batch: 780; loss: 1.51; acc: 0.52
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.878045754390769e-05
1.4696621292387135e-05
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.61; acc: 0.56
Batch: 40; loss: 1.39; acc: 0.69
Batch: 60; loss: 1.61; acc: 0.59
Batch: 80; loss: 1.45; acc: 0.62
Batch: 100; loss: 1.65; acc: 0.53
Batch: 120; loss: 1.68; acc: 0.55
Batch: 140; loss: 1.34; acc: 0.77
Val Epoch over. val_loss: 1.5778180110226772; val_accuracy: 0.5658837579617835 

The current subspace-distance is: 1.4696621292387135e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.5
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.49; acc: 0.66
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.6; acc: 0.58
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.66; acc: 0.53
Batch: 140; loss: 1.63; acc: 0.56
Batch: 160; loss: 1.58; acc: 0.52
Batch: 180; loss: 1.79; acc: 0.44
Batch: 200; loss: 1.58; acc: 0.55
Batch: 220; loss: 1.64; acc: 0.5
Batch: 240; loss: 1.5; acc: 0.59
Batch: 260; loss: 1.55; acc: 0.56
Batch: 280; loss: 1.5; acc: 0.59
Batch: 300; loss: 1.54; acc: 0.61
Batch: 320; loss: 1.51; acc: 0.64
Batch: 340; loss: 1.51; acc: 0.61
Batch: 360; loss: 1.7; acc: 0.52
Batch: 380; loss: 1.81; acc: 0.45
Batch: 400; loss: 1.53; acc: 0.62
Batch: 420; loss: 1.71; acc: 0.42
Batch: 440; loss: 1.77; acc: 0.48
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.69; acc: 0.47
Batch: 500; loss: 1.61; acc: 0.56
Batch: 520; loss: 1.63; acc: 0.56
Batch: 540; loss: 1.53; acc: 0.55
Batch: 560; loss: 1.58; acc: 0.58
Batch: 580; loss: 1.66; acc: 0.55
Batch: 600; loss: 1.68; acc: 0.48
Batch: 620; loss: 1.62; acc: 0.56
Batch: 640; loss: 1.55; acc: 0.56
Batch: 660; loss: 1.57; acc: 0.56
Batch: 680; loss: 1.71; acc: 0.48
Batch: 700; loss: 1.66; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.5
Batch: 740; loss: 1.63; acc: 0.59
Batch: 760; loss: 1.58; acc: 0.56
Batch: 780; loss: 1.65; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.54 

4.1051935113500804e-05
1.6033447536756285e-05
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.56
Batch: 40; loss: 1.38; acc: 0.66
Batch: 60; loss: 1.62; acc: 0.58
Batch: 80; loss: 1.45; acc: 0.58
Batch: 100; loss: 1.66; acc: 0.53
Batch: 120; loss: 1.68; acc: 0.53
Batch: 140; loss: 1.36; acc: 0.78
Val Epoch over. val_loss: 1.577599492801982; val_accuracy: 0.5637937898089171 

The current subspace-distance is: 1.6033447536756285e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.56
Batch: 20; loss: 1.74; acc: 0.48
Batch: 40; loss: 1.59; acc: 0.55
Batch: 60; loss: 1.62; acc: 0.56
Batch: 80; loss: 1.71; acc: 0.5
Batch: 100; loss: 1.72; acc: 0.45
Batch: 120; loss: 1.72; acc: 0.52
Batch: 140; loss: 1.62; acc: 0.52
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.75; acc: 0.53
Batch: 200; loss: 1.47; acc: 0.64
Batch: 220; loss: 1.66; acc: 0.47
Batch: 240; loss: 1.66; acc: 0.48
Batch: 260; loss: 1.6; acc: 0.62
Batch: 280; loss: 1.76; acc: 0.39
Batch: 300; loss: 1.65; acc: 0.55
Batch: 320; loss: 1.58; acc: 0.59
Batch: 340; loss: 1.6; acc: 0.59
Batch: 360; loss: 1.51; acc: 0.53
Batch: 380; loss: 1.64; acc: 0.53
Batch: 400; loss: 1.59; acc: 0.5
Batch: 420; loss: 1.67; acc: 0.5
Batch: 440; loss: 1.62; acc: 0.58
Batch: 460; loss: 1.46; acc: 0.62
Batch: 480; loss: 1.5; acc: 0.66
Batch: 500; loss: 1.59; acc: 0.53
Batch: 520; loss: 1.59; acc: 0.55
Batch: 540; loss: 1.54; acc: 0.58
Batch: 560; loss: 1.53; acc: 0.61
Batch: 580; loss: 1.59; acc: 0.58
Batch: 600; loss: 1.64; acc: 0.53
Batch: 620; loss: 1.74; acc: 0.44
Batch: 640; loss: 1.56; acc: 0.56
Batch: 660; loss: 1.58; acc: 0.59
Batch: 680; loss: 1.58; acc: 0.62
Batch: 700; loss: 1.52; acc: 0.58
Batch: 720; loss: 1.63; acc: 0.5
Batch: 740; loss: 1.52; acc: 0.56
Batch: 760; loss: 1.63; acc: 0.58
Batch: 780; loss: 1.7; acc: 0.52
Train Epoch over. train_loss: 1.6; train_accuracy: 0.54 

4.0949424146674573e-05
1.445350244466681e-05
Batch: 0; loss: 1.72; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.55
Batch: 40; loss: 1.37; acc: 0.67
Batch: 60; loss: 1.61; acc: 0.61
Batch: 80; loss: 1.44; acc: 0.62
Batch: 100; loss: 1.65; acc: 0.56
Batch: 120; loss: 1.67; acc: 0.55
Batch: 140; loss: 1.36; acc: 0.77
Val Epoch over. val_loss: 1.565672769667996; val_accuracy: 0.5729498407643312 

The current subspace-distance is: 1.445350244466681e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.64
Batch: 20; loss: 1.66; acc: 0.53
Batch: 40; loss: 1.63; acc: 0.53
Batch: 60; loss: 1.5; acc: 0.56
Batch: 80; loss: 1.44; acc: 0.67
Batch: 100; loss: 1.54; acc: 0.59
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.57; acc: 0.52
Batch: 160; loss: 1.56; acc: 0.53
Batch: 180; loss: 1.56; acc: 0.58
Batch: 200; loss: 1.54; acc: 0.62
Batch: 220; loss: 1.69; acc: 0.48
Batch: 240; loss: 1.7; acc: 0.5
Batch: 260; loss: 1.55; acc: 0.62
Batch: 280; loss: 1.65; acc: 0.52
Batch: 300; loss: 1.59; acc: 0.53
Batch: 320; loss: 1.52; acc: 0.61
Batch: 340; loss: 1.73; acc: 0.47
Batch: 360; loss: 1.63; acc: 0.53
Batch: 380; loss: 1.59; acc: 0.55
Batch: 400; loss: 1.57; acc: 0.61
Batch: 420; loss: 1.59; acc: 0.5
Batch: 440; loss: 1.66; acc: 0.48
Batch: 460; loss: 1.51; acc: 0.58
Batch: 480; loss: 1.62; acc: 0.53
Batch: 500; loss: 1.51; acc: 0.56
Batch: 520; loss: 1.76; acc: 0.47
Batch: 540; loss: 1.66; acc: 0.52
Batch: 560; loss: 1.6; acc: 0.53
Batch: 580; loss: 1.64; acc: 0.5
Batch: 600; loss: 1.64; acc: 0.48
Batch: 620; loss: 1.61; acc: 0.56
Batch: 640; loss: 1.46; acc: 0.64
Batch: 660; loss: 1.64; acc: 0.58
Batch: 680; loss: 1.7; acc: 0.44
Batch: 700; loss: 1.56; acc: 0.55
Batch: 720; loss: 1.58; acc: 0.52
Batch: 740; loss: 1.55; acc: 0.5
Batch: 760; loss: 1.66; acc: 0.56
Batch: 780; loss: 1.56; acc: 0.55
Train Epoch over. train_loss: 1.6; train_accuracy: 0.55 

4.1533156036166474e-05
1.663905095483642e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.62; acc: 0.55
Batch: 40; loss: 1.32; acc: 0.73
Batch: 60; loss: 1.59; acc: 0.62
Batch: 80; loss: 1.42; acc: 0.66
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.64; acc: 0.55
Batch: 140; loss: 1.35; acc: 0.75
Val Epoch over. val_loss: 1.546613054670346; val_accuracy: 0.5726512738853503 

The current subspace-distance is: 1.663905095483642e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.61; acc: 0.52
Batch: 20; loss: 1.65; acc: 0.52
Batch: 40; loss: 1.57; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.64; acc: 0.55
Batch: 100; loss: 1.69; acc: 0.42
Batch: 120; loss: 1.46; acc: 0.66
Batch: 140; loss: 1.65; acc: 0.55
Batch: 160; loss: 1.61; acc: 0.56
Batch: 180; loss: 1.55; acc: 0.56
Batch: 200; loss: 1.59; acc: 0.58
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.57; acc: 0.58
Batch: 260; loss: 1.61; acc: 0.53
Batch: 280; loss: 1.52; acc: 0.59
Batch: 300; loss: 1.62; acc: 0.55
Batch: 320; loss: 1.49; acc: 0.69
Batch: 340; loss: 1.49; acc: 0.66
Batch: 360; loss: 1.71; acc: 0.47
Batch: 380; loss: 1.58; acc: 0.53
Batch: 400; loss: 1.64; acc: 0.59
Batch: 420; loss: 1.56; acc: 0.56
Batch: 440; loss: 1.53; acc: 0.55
Batch: 460; loss: 1.6; acc: 0.55
Batch: 480; loss: 1.5; acc: 0.67
Batch: 500; loss: 1.53; acc: 0.59
Batch: 520; loss: 1.73; acc: 0.48
Batch: 540; loss: 1.63; acc: 0.53
Batch: 560; loss: 1.62; acc: 0.53
Batch: 580; loss: 1.63; acc: 0.47
Batch: 600; loss: 1.53; acc: 0.61
Batch: 620; loss: 1.65; acc: 0.52
Batch: 640; loss: 1.6; acc: 0.5
Batch: 660; loss: 1.65; acc: 0.53
Batch: 680; loss: 1.54; acc: 0.64
Batch: 700; loss: 1.54; acc: 0.58
Batch: 720; loss: 1.78; acc: 0.36
Batch: 740; loss: 1.51; acc: 0.62
Batch: 760; loss: 1.48; acc: 0.66
Batch: 780; loss: 1.46; acc: 0.61
Train Epoch over. train_loss: 1.59; train_accuracy: 0.55 

4.2710838897619396e-05
1.425870959792519e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.56
Batch: 40; loss: 1.33; acc: 0.75
Batch: 60; loss: 1.6; acc: 0.56
Batch: 80; loss: 1.43; acc: 0.62
Batch: 100; loss: 1.65; acc: 0.56
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.36; acc: 0.78
Val Epoch over. val_loss: 1.5478928856029632; val_accuracy: 0.5730493630573248 

The current subspace-distance is: 1.425870959792519e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.56; acc: 0.55
Batch: 40; loss: 1.67; acc: 0.53
Batch: 60; loss: 1.46; acc: 0.67
Batch: 80; loss: 1.71; acc: 0.47
Batch: 100; loss: 1.64; acc: 0.5
Batch: 120; loss: 1.52; acc: 0.59
Batch: 140; loss: 1.55; acc: 0.66
Batch: 160; loss: 1.57; acc: 0.56
Batch: 180; loss: 1.65; acc: 0.52
Batch: 200; loss: 1.63; acc: 0.56
Batch: 220; loss: 1.59; acc: 0.53
Batch: 240; loss: 1.66; acc: 0.5
Batch: 260; loss: 1.54; acc: 0.53
Batch: 280; loss: 1.74; acc: 0.44
Batch: 300; loss: 1.67; acc: 0.48
Batch: 320; loss: 1.68; acc: 0.52
Batch: 340; loss: 1.49; acc: 0.56
Batch: 360; loss: 1.63; acc: 0.52
Batch: 380; loss: 1.56; acc: 0.59
Batch: 400; loss: 1.66; acc: 0.53
Batch: 420; loss: 1.54; acc: 0.58
Batch: 440; loss: 1.68; acc: 0.45
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.54; acc: 0.55
Batch: 500; loss: 1.58; acc: 0.59
Batch: 520; loss: 1.56; acc: 0.56
Batch: 540; loss: 1.59; acc: 0.53
Batch: 560; loss: 1.65; acc: 0.53
Batch: 580; loss: 1.63; acc: 0.56
Batch: 600; loss: 1.53; acc: 0.58
Batch: 620; loss: 1.62; acc: 0.53
Batch: 640; loss: 1.56; acc: 0.59
Batch: 660; loss: 1.6; acc: 0.53
Batch: 680; loss: 1.51; acc: 0.56
Batch: 700; loss: 1.63; acc: 0.45
Batch: 720; loss: 1.45; acc: 0.59
Batch: 740; loss: 1.5; acc: 0.62
Batch: 760; loss: 1.51; acc: 0.53
Batch: 780; loss: 1.52; acc: 0.66
Train Epoch over. train_loss: 1.58; train_accuracy: 0.55 

4.376284050522372e-05
1.83209813258145e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.53
Batch: 40; loss: 1.33; acc: 0.73
Batch: 60; loss: 1.61; acc: 0.53
Batch: 80; loss: 1.42; acc: 0.67
Batch: 100; loss: 1.64; acc: 0.56
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.38; acc: 0.73
Val Epoch over. val_loss: 1.5514337887429888; val_accuracy: 0.5673765923566879 

The current subspace-distance is: 1.83209813258145e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.5
Batch: 20; loss: 1.58; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.61
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.65; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.52
Batch: 120; loss: 1.53; acc: 0.62
Batch: 140; loss: 1.52; acc: 0.61
Batch: 160; loss: 1.5; acc: 0.56
Batch: 180; loss: 1.64; acc: 0.58
Batch: 200; loss: 1.63; acc: 0.53
Batch: 220; loss: 1.69; acc: 0.42
Batch: 240; loss: 1.55; acc: 0.55
Batch: 260; loss: 1.53; acc: 0.5
Batch: 280; loss: 1.68; acc: 0.53
Batch: 300; loss: 1.63; acc: 0.39
Batch: 320; loss: 1.59; acc: 0.53
Batch: 340; loss: 1.6; acc: 0.61
Batch: 360; loss: 1.59; acc: 0.52
Batch: 380; loss: 1.51; acc: 0.55
Batch: 400; loss: 1.58; acc: 0.62
Batch: 420; loss: 1.51; acc: 0.61
Batch: 440; loss: 1.52; acc: 0.58
Batch: 460; loss: 1.64; acc: 0.45
Batch: 480; loss: 1.69; acc: 0.53
Batch: 500; loss: 1.6; acc: 0.58
Batch: 520; loss: 1.53; acc: 0.56
Batch: 540; loss: 1.48; acc: 0.64
Batch: 560; loss: 1.51; acc: 0.53
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.52; acc: 0.56
Batch: 620; loss: 1.63; acc: 0.5
Batch: 640; loss: 1.58; acc: 0.56
Batch: 660; loss: 1.58; acc: 0.61
Batch: 680; loss: 1.69; acc: 0.5
Batch: 700; loss: 1.43; acc: 0.69
Batch: 720; loss: 1.62; acc: 0.55
Batch: 740; loss: 1.5; acc: 0.58
Batch: 760; loss: 1.5; acc: 0.61
Batch: 780; loss: 1.58; acc: 0.5
Train Epoch over. train_loss: 1.58; train_accuracy: 0.55 

4.426231680554338e-05
1.643835275899619e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.3; acc: 0.75
Batch: 60; loss: 1.59; acc: 0.58
Batch: 80; loss: 1.41; acc: 0.69
Batch: 100; loss: 1.62; acc: 0.58
Batch: 120; loss: 1.62; acc: 0.56
Batch: 140; loss: 1.38; acc: 0.7
Val Epoch over. val_loss: 1.5327783602817802; val_accuracy: 0.5731488853503185 

The current subspace-distance is: 1.643835275899619e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.55
Batch: 20; loss: 1.48; acc: 0.64
Batch: 40; loss: 1.61; acc: 0.5
Batch: 60; loss: 1.59; acc: 0.58
Batch: 80; loss: 1.62; acc: 0.48
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.53; acc: 0.64
Batch: 140; loss: 1.55; acc: 0.48
Batch: 160; loss: 1.69; acc: 0.53
Batch: 180; loss: 1.6; acc: 0.5
Batch: 200; loss: 1.54; acc: 0.61
Batch: 220; loss: 1.45; acc: 0.58
Batch: 240; loss: 1.6; acc: 0.53
Batch: 260; loss: 1.64; acc: 0.47
Batch: 280; loss: 1.53; acc: 0.66
Batch: 300; loss: 1.59; acc: 0.48
Batch: 320; loss: 1.57; acc: 0.58
Batch: 340; loss: 1.64; acc: 0.53
Batch: 360; loss: 1.6; acc: 0.48
Batch: 380; loss: 1.58; acc: 0.53
Batch: 400; loss: 1.56; acc: 0.58
Batch: 420; loss: 1.47; acc: 0.58
Batch: 440; loss: 1.78; acc: 0.47
Batch: 460; loss: 1.59; acc: 0.58
Batch: 480; loss: 1.54; acc: 0.55
Batch: 500; loss: 1.61; acc: 0.55
Batch: 520; loss: 1.58; acc: 0.47
Batch: 540; loss: 1.58; acc: 0.58
Batch: 560; loss: 1.67; acc: 0.45
Batch: 580; loss: 1.57; acc: 0.56
Batch: 600; loss: 1.56; acc: 0.61
Batch: 620; loss: 1.63; acc: 0.52
Batch: 640; loss: 1.76; acc: 0.38
Batch: 660; loss: 1.45; acc: 0.62
Batch: 680; loss: 1.61; acc: 0.53
Batch: 700; loss: 1.55; acc: 0.59
Batch: 720; loss: 1.62; acc: 0.58
Batch: 740; loss: 1.5; acc: 0.56
Batch: 760; loss: 1.51; acc: 0.58
Batch: 780; loss: 1.4; acc: 0.69
Train Epoch over. train_loss: 1.57; train_accuracy: 0.55 

4.406795051181689e-05
1.4446010027313605e-05
Batch: 0; loss: 1.67; acc: 0.55
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.27; acc: 0.75
Batch: 60; loss: 1.58; acc: 0.58
Batch: 80; loss: 1.4; acc: 0.66
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.72
Val Epoch over. val_loss: 1.5106480622747143; val_accuracy: 0.582703025477707 

The current subspace-distance is: 1.4446010027313605e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.61; acc: 0.48
Batch: 60; loss: 1.38; acc: 0.66
Batch: 80; loss: 1.74; acc: 0.52
Batch: 100; loss: 1.53; acc: 0.58
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.45; acc: 0.64
Batch: 160; loss: 1.71; acc: 0.41
Batch: 180; loss: 1.52; acc: 0.64
Batch: 200; loss: 1.49; acc: 0.64
Batch: 220; loss: 1.62; acc: 0.61
Batch: 240; loss: 1.52; acc: 0.55
Batch: 260; loss: 1.5; acc: 0.58
Batch: 280; loss: 1.5; acc: 0.55
Batch: 300; loss: 1.52; acc: 0.56
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.49; acc: 0.67
Batch: 360; loss: 1.5; acc: 0.59
Batch: 380; loss: 1.55; acc: 0.61
Batch: 400; loss: 1.61; acc: 0.52
Batch: 420; loss: 1.39; acc: 0.61
Batch: 440; loss: 1.43; acc: 0.59
Batch: 460; loss: 1.53; acc: 0.5
Batch: 480; loss: 1.57; acc: 0.55
Batch: 500; loss: 1.56; acc: 0.59
Batch: 520; loss: 1.56; acc: 0.55
Batch: 540; loss: 1.52; acc: 0.56
Batch: 560; loss: 1.55; acc: 0.58
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.46; acc: 0.58
Batch: 620; loss: 1.49; acc: 0.62
Batch: 640; loss: 1.53; acc: 0.58
Batch: 660; loss: 1.64; acc: 0.55
Batch: 680; loss: 1.64; acc: 0.48
Batch: 700; loss: 1.51; acc: 0.58
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.54; acc: 0.58
Batch: 760; loss: 1.59; acc: 0.62
Batch: 780; loss: 1.51; acc: 0.58
Train Epoch over. train_loss: 1.56; train_accuracy: 0.56 

4.446344610187225e-05
1.57357153511839e-05
Batch: 0; loss: 1.67; acc: 0.58
Batch: 20; loss: 1.62; acc: 0.47
Batch: 40; loss: 1.28; acc: 0.77
Batch: 60; loss: 1.57; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.66
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.6; acc: 0.59
Batch: 140; loss: 1.38; acc: 0.7
Val Epoch over. val_loss: 1.5104302904408449; val_accuracy: 0.5797173566878981 

The current subspace-distance is: 1.57357153511839e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.5
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.62; acc: 0.48
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.58; acc: 0.53
Batch: 120; loss: 1.49; acc: 0.61
Batch: 140; loss: 1.57; acc: 0.5
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.42; acc: 0.56
Batch: 200; loss: 1.43; acc: 0.69
Batch: 220; loss: 1.54; acc: 0.58
Batch: 240; loss: 1.61; acc: 0.5
Batch: 260; loss: 1.48; acc: 0.58
Batch: 280; loss: 1.53; acc: 0.61
Batch: 300; loss: 1.53; acc: 0.53
Batch: 320; loss: 1.67; acc: 0.5
Batch: 340; loss: 1.61; acc: 0.58
Batch: 360; loss: 1.55; acc: 0.55
Batch: 380; loss: 1.57; acc: 0.5
Batch: 400; loss: 1.62; acc: 0.53
Batch: 420; loss: 1.46; acc: 0.64
Batch: 440; loss: 1.47; acc: 0.64
Batch: 460; loss: 1.55; acc: 0.44
Batch: 480; loss: 1.51; acc: 0.58
Batch: 500; loss: 1.47; acc: 0.55
Batch: 520; loss: 1.56; acc: 0.56
Batch: 540; loss: 1.53; acc: 0.53
Batch: 560; loss: 1.47; acc: 0.62
Batch: 580; loss: 1.49; acc: 0.67
Batch: 600; loss: 1.54; acc: 0.55
Batch: 620; loss: 1.55; acc: 0.58
Batch: 640; loss: 1.65; acc: 0.44
Batch: 660; loss: 1.68; acc: 0.48
Batch: 680; loss: 1.41; acc: 0.69
Batch: 700; loss: 1.51; acc: 0.55
Batch: 720; loss: 1.58; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.52
Batch: 760; loss: 1.64; acc: 0.53
Batch: 780; loss: 1.72; acc: 0.53
Train Epoch over. train_loss: 1.55; train_accuracy: 0.56 

4.610150426742621e-05
1.7598064005142078e-05
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.62; acc: 0.45
Batch: 40; loss: 1.27; acc: 0.78
Batch: 60; loss: 1.56; acc: 0.58
Batch: 80; loss: 1.4; acc: 0.67
Batch: 100; loss: 1.59; acc: 0.59
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.38; acc: 0.69
Val Epoch over. val_loss: 1.5040283522028832; val_accuracy: 0.5814092356687898 

The current subspace-distance is: 1.7598064005142078e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.53
Batch: 20; loss: 1.39; acc: 0.66
Batch: 40; loss: 1.56; acc: 0.5
Batch: 60; loss: 1.67; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.56
Batch: 100; loss: 1.44; acc: 0.62
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.62; acc: 0.5
Batch: 160; loss: 1.54; acc: 0.59
Batch: 180; loss: 1.45; acc: 0.64
Batch: 200; loss: 1.68; acc: 0.38
Batch: 220; loss: 1.48; acc: 0.62
Batch: 240; loss: 1.66; acc: 0.5
Batch: 260; loss: 1.7; acc: 0.42
Batch: 280; loss: 1.6; acc: 0.5
Batch: 300; loss: 1.54; acc: 0.48
Batch: 320; loss: 1.52; acc: 0.59
Batch: 340; loss: 1.63; acc: 0.53
Batch: 360; loss: 1.37; acc: 0.66
Batch: 380; loss: 1.52; acc: 0.56
Batch: 400; loss: 1.52; acc: 0.53
Batch: 420; loss: 1.53; acc: 0.55
Batch: 440; loss: 1.59; acc: 0.59
Batch: 460; loss: 1.55; acc: 0.53
Batch: 480; loss: 1.34; acc: 0.75
Batch: 500; loss: 1.66; acc: 0.53
Batch: 520; loss: 1.55; acc: 0.5
Batch: 540; loss: 1.5; acc: 0.55
Batch: 560; loss: 1.55; acc: 0.55
Batch: 580; loss: 1.44; acc: 0.64
Batch: 600; loss: 1.56; acc: 0.58
Batch: 620; loss: 1.53; acc: 0.55
Batch: 640; loss: 1.53; acc: 0.5
Batch: 660; loss: 1.55; acc: 0.5
Batch: 680; loss: 1.51; acc: 0.61
Batch: 700; loss: 1.71; acc: 0.42
Batch: 720; loss: 1.57; acc: 0.62
Batch: 740; loss: 1.69; acc: 0.48
Batch: 760; loss: 1.61; acc: 0.48
Batch: 780; loss: 1.54; acc: 0.64
Train Epoch over. train_loss: 1.54; train_accuracy: 0.56 

4.646123124985024e-05
1.785411950550042e-05
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.26; acc: 0.78
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.38; acc: 0.69
Batch: 100; loss: 1.57; acc: 0.61
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.4882481265219913; val_accuracy: 0.5893710191082803 

The current subspace-distance is: 1.785411950550042e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.37; acc: 0.69
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.68; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.48
Batch: 100; loss: 1.54; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.61
Batch: 140; loss: 1.41; acc: 0.62
Batch: 160; loss: 1.63; acc: 0.5
Batch: 180; loss: 1.53; acc: 0.61
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.58; acc: 0.52
Batch: 260; loss: 1.69; acc: 0.44
Batch: 280; loss: 1.7; acc: 0.42
Batch: 300; loss: 1.53; acc: 0.48
Batch: 320; loss: 1.59; acc: 0.58
Batch: 340; loss: 1.45; acc: 0.61
Batch: 360; loss: 1.53; acc: 0.5
Batch: 380; loss: 1.55; acc: 0.55
Batch: 400; loss: 1.6; acc: 0.56
Batch: 420; loss: 1.51; acc: 0.56
Batch: 440; loss: 1.46; acc: 0.61
Batch: 460; loss: 1.53; acc: 0.59
Batch: 480; loss: 1.56; acc: 0.53
Batch: 500; loss: 1.49; acc: 0.61
Batch: 520; loss: 1.58; acc: 0.53
Batch: 540; loss: 1.58; acc: 0.53
Batch: 560; loss: 1.55; acc: 0.55
Batch: 580; loss: 1.49; acc: 0.59
Batch: 600; loss: 1.48; acc: 0.58
Batch: 620; loss: 1.63; acc: 0.52
Batch: 640; loss: 1.51; acc: 0.58
Batch: 660; loss: 1.62; acc: 0.56
Batch: 680; loss: 1.49; acc: 0.56
Batch: 700; loss: 1.56; acc: 0.58
Batch: 720; loss: 1.65; acc: 0.52
Batch: 740; loss: 1.47; acc: 0.62
Batch: 760; loss: 1.45; acc: 0.61
Batch: 780; loss: 1.59; acc: 0.5
Train Epoch over. train_loss: 1.53; train_accuracy: 0.56 

4.833801358472556e-05
2.082523496937938e-05
Batch: 0; loss: 1.63; acc: 0.53
Batch: 20; loss: 1.61; acc: 0.48
Batch: 40; loss: 1.25; acc: 0.77
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.36; acc: 0.69
Batch: 100; loss: 1.58; acc: 0.59
Batch: 120; loss: 1.56; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.66
Val Epoch over. val_loss: 1.4800321759691664; val_accuracy: 0.5854896496815286 

The current subspace-distance is: 2.082523496937938e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.52
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.59; acc: 0.47
Batch: 60; loss: 1.52; acc: 0.53
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.51; acc: 0.52
Batch: 140; loss: 1.57; acc: 0.52
Batch: 160; loss: 1.64; acc: 0.42
Batch: 180; loss: 1.49; acc: 0.59
Batch: 200; loss: 1.58; acc: 0.48
Batch: 220; loss: 1.45; acc: 0.64
Batch: 240; loss: 1.69; acc: 0.48
Batch: 260; loss: 1.59; acc: 0.56
Batch: 280; loss: 1.56; acc: 0.53
Batch: 300; loss: 1.5; acc: 0.58
Batch: 320; loss: 1.43; acc: 0.67
Batch: 340; loss: 1.4; acc: 0.66
Batch: 360; loss: 1.57; acc: 0.59
Batch: 380; loss: 1.65; acc: 0.47
Batch: 400; loss: 1.54; acc: 0.56
Batch: 420; loss: 1.41; acc: 0.69
Batch: 440; loss: 1.47; acc: 0.59
Batch: 460; loss: 1.65; acc: 0.47
Batch: 480; loss: 1.56; acc: 0.55
Batch: 500; loss: 1.5; acc: 0.61
Batch: 520; loss: 1.72; acc: 0.47
Batch: 540; loss: 1.51; acc: 0.53
Batch: 560; loss: 1.45; acc: 0.66
Batch: 580; loss: 1.41; acc: 0.66
Batch: 600; loss: 1.52; acc: 0.5
Batch: 620; loss: 1.5; acc: 0.62
Batch: 640; loss: 1.46; acc: 0.66
Batch: 660; loss: 1.41; acc: 0.61
Batch: 680; loss: 1.52; acc: 0.55
Batch: 700; loss: 1.5; acc: 0.62
Batch: 720; loss: 1.58; acc: 0.58
Batch: 740; loss: 1.47; acc: 0.62
Batch: 760; loss: 1.42; acc: 0.72
Batch: 780; loss: 1.54; acc: 0.58
Train Epoch over. train_loss: 1.53; train_accuracy: 0.56 

4.6480356104439124e-05
1.5793120837770402e-05
Batch: 0; loss: 1.65; acc: 0.52
Batch: 20; loss: 1.63; acc: 0.45
Batch: 40; loss: 1.28; acc: 0.75
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.38; acc: 0.64
Val Epoch over. val_loss: 1.4960830530543237; val_accuracy: 0.5806130573248408 

The current subspace-distance is: 1.5793120837770402e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.56
Batch: 20; loss: 1.43; acc: 0.59
Batch: 40; loss: 1.59; acc: 0.53
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.63; acc: 0.5
Batch: 100; loss: 1.63; acc: 0.5
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.56
Batch: 160; loss: 1.54; acc: 0.56
Batch: 180; loss: 1.43; acc: 0.67
Batch: 200; loss: 1.39; acc: 0.7
Batch: 220; loss: 1.5; acc: 0.61
Batch: 240; loss: 1.42; acc: 0.55
Batch: 260; loss: 1.47; acc: 0.55
Batch: 280; loss: 1.53; acc: 0.59
Batch: 300; loss: 1.58; acc: 0.59
Batch: 320; loss: 1.44; acc: 0.62
Batch: 340; loss: 1.54; acc: 0.47
Batch: 360; loss: 1.41; acc: 0.64
Batch: 380; loss: 1.51; acc: 0.53
Batch: 400; loss: 1.43; acc: 0.64
Batch: 420; loss: 1.36; acc: 0.56
Batch: 440; loss: 1.64; acc: 0.45
Batch: 460; loss: 1.48; acc: 0.66
Batch: 480; loss: 1.4; acc: 0.69
Batch: 500; loss: 1.68; acc: 0.47
Batch: 520; loss: 1.51; acc: 0.58
Batch: 540; loss: 1.55; acc: 0.55
Batch: 560; loss: 1.52; acc: 0.55
Batch: 580; loss: 1.37; acc: 0.62
Batch: 600; loss: 1.59; acc: 0.55
Batch: 620; loss: 1.49; acc: 0.59
Batch: 640; loss: 1.47; acc: 0.66
Batch: 660; loss: 1.51; acc: 0.58
Batch: 680; loss: 1.42; acc: 0.62
Batch: 700; loss: 1.5; acc: 0.55
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.47; acc: 0.56
Batch: 760; loss: 1.58; acc: 0.61
Batch: 780; loss: 1.68; acc: 0.56
Train Epoch over. train_loss: 1.52; train_accuracy: 0.56 

4.636734593077563e-05
1.4034912055649329e-05
Batch: 0; loss: 1.64; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.25; acc: 0.78
Batch: 60; loss: 1.53; acc: 0.55
Batch: 80; loss: 1.36; acc: 0.67
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.56; acc: 0.58
Batch: 140; loss: 1.36; acc: 0.62
Val Epoch over. val_loss: 1.4764315413821274; val_accuracy: 0.5886743630573248 

The current subspace-distance is: 1.4034912055649329e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.55
Batch: 20; loss: 1.56; acc: 0.56
Batch: 40; loss: 1.57; acc: 0.48
Batch: 60; loss: 1.44; acc: 0.66
Batch: 80; loss: 1.43; acc: 0.64
Batch: 100; loss: 1.46; acc: 0.62
Batch: 120; loss: 1.41; acc: 0.62
Batch: 140; loss: 1.62; acc: 0.56
Batch: 160; loss: 1.57; acc: 0.5
Batch: 180; loss: 1.67; acc: 0.45
Batch: 200; loss: 1.55; acc: 0.55
Batch: 220; loss: 1.58; acc: 0.5
Batch: 240; loss: 1.48; acc: 0.52
Batch: 260; loss: 1.6; acc: 0.56
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.43; acc: 0.61
Batch: 320; loss: 1.4; acc: 0.67
Batch: 340; loss: 1.51; acc: 0.52
Batch: 360; loss: 1.58; acc: 0.52
Batch: 380; loss: 1.48; acc: 0.59
Batch: 400; loss: 1.48; acc: 0.53
Batch: 420; loss: 1.47; acc: 0.56
Batch: 440; loss: 1.53; acc: 0.53
Batch: 460; loss: 1.53; acc: 0.52
Batch: 480; loss: 1.64; acc: 0.48
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.55; acc: 0.52
Batch: 540; loss: 1.64; acc: 0.52
Batch: 560; loss: 1.46; acc: 0.64
Batch: 580; loss: 1.61; acc: 0.45
Batch: 600; loss: 1.55; acc: 0.56
Batch: 620; loss: 1.5; acc: 0.64
Batch: 640; loss: 1.5; acc: 0.61
Batch: 660; loss: 1.37; acc: 0.67
Batch: 680; loss: 1.65; acc: 0.42
Batch: 700; loss: 1.63; acc: 0.45
Batch: 720; loss: 1.58; acc: 0.48
Batch: 740; loss: 1.55; acc: 0.56
Batch: 760; loss: 1.5; acc: 0.59
Batch: 780; loss: 1.42; acc: 0.61
Train Epoch over. train_loss: 1.52; train_accuracy: 0.57 

4.63164942630101e-05
1.5778021406731568e-05
Batch: 0; loss: 1.64; acc: 0.56
Batch: 20; loss: 1.62; acc: 0.5
Batch: 40; loss: 1.27; acc: 0.75
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.37; acc: 0.67
Batch: 100; loss: 1.59; acc: 0.59
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.67
Val Epoch over. val_loss: 1.4822778322134809; val_accuracy: 0.5940485668789809 

The current subspace-distance is: 1.5778021406731568e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.56
Batch: 20; loss: 1.46; acc: 0.64
Batch: 40; loss: 1.57; acc: 0.55
Batch: 60; loss: 1.44; acc: 0.62
Batch: 80; loss: 1.43; acc: 0.66
Batch: 100; loss: 1.55; acc: 0.53
Batch: 120; loss: 1.48; acc: 0.53
Batch: 140; loss: 1.65; acc: 0.45
Batch: 160; loss: 1.63; acc: 0.5
Batch: 180; loss: 1.55; acc: 0.5
Batch: 200; loss: 1.45; acc: 0.62
Batch: 220; loss: 1.52; acc: 0.62
Batch: 240; loss: 1.53; acc: 0.53
Batch: 260; loss: 1.45; acc: 0.61
Batch: 280; loss: 1.57; acc: 0.5
Batch: 300; loss: 1.47; acc: 0.58
Batch: 320; loss: 1.62; acc: 0.48
Batch: 340; loss: 1.69; acc: 0.45
Batch: 360; loss: 1.61; acc: 0.56
Batch: 380; loss: 1.45; acc: 0.61
Batch: 400; loss: 1.54; acc: 0.56
Batch: 420; loss: 1.61; acc: 0.53
Batch: 440; loss: 1.53; acc: 0.64
Batch: 460; loss: 1.47; acc: 0.64
Batch: 480; loss: 1.55; acc: 0.56
Batch: 500; loss: 1.57; acc: 0.53
Batch: 520; loss: 1.64; acc: 0.55
Batch: 540; loss: 1.5; acc: 0.64
Batch: 560; loss: 1.47; acc: 0.59
Batch: 580; loss: 1.66; acc: 0.5
Batch: 600; loss: 1.65; acc: 0.45
Batch: 620; loss: 1.53; acc: 0.52
Batch: 640; loss: 1.73; acc: 0.44
Batch: 660; loss: 1.62; acc: 0.5
Batch: 680; loss: 1.56; acc: 0.53
Batch: 700; loss: 1.4; acc: 0.67
Batch: 720; loss: 1.51; acc: 0.62
Batch: 740; loss: 1.48; acc: 0.61
Batch: 760; loss: 1.47; acc: 0.61
Batch: 780; loss: 1.59; acc: 0.44
Train Epoch over. train_loss: 1.52; train_accuracy: 0.57 

4.6918994485167786e-05
1.603444616193883e-05
Batch: 0; loss: 1.63; acc: 0.58
Batch: 20; loss: 1.61; acc: 0.45
Batch: 40; loss: 1.25; acc: 0.77
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.67
Batch: 100; loss: 1.58; acc: 0.58
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.66
Val Epoch over. val_loss: 1.4681086509850374; val_accuracy: 0.5973328025477707 

The current subspace-distance is: 1.603444616193883e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.45
Batch: 40; loss: 1.42; acc: 0.59
Batch: 60; loss: 1.54; acc: 0.61
Batch: 80; loss: 1.68; acc: 0.48
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.41; acc: 0.73
Batch: 140; loss: 1.42; acc: 0.56
Batch: 160; loss: 1.46; acc: 0.56
Batch: 180; loss: 1.53; acc: 0.58
Batch: 200; loss: 1.47; acc: 0.56
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.53; acc: 0.58
Batch: 260; loss: 1.56; acc: 0.59
Batch: 280; loss: 1.48; acc: 0.55
Batch: 300; loss: 1.51; acc: 0.48
Batch: 320; loss: 1.45; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.58
Batch: 360; loss: 1.69; acc: 0.5
Batch: 380; loss: 1.43; acc: 0.62
Batch: 400; loss: 1.48; acc: 0.66
Batch: 420; loss: 1.58; acc: 0.48
Batch: 440; loss: 1.51; acc: 0.62
Batch: 460; loss: 1.5; acc: 0.58
Batch: 480; loss: 1.46; acc: 0.58
Batch: 500; loss: 1.4; acc: 0.56
Batch: 520; loss: 1.42; acc: 0.66
Batch: 540; loss: 1.49; acc: 0.52
Batch: 560; loss: 1.5; acc: 0.52
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.49; acc: 0.52
Batch: 620; loss: 1.51; acc: 0.53
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.46; acc: 0.61
Batch: 680; loss: 1.52; acc: 0.61
Batch: 700; loss: 1.48; acc: 0.64
Batch: 720; loss: 1.52; acc: 0.61
Batch: 740; loss: 1.51; acc: 0.5
Batch: 760; loss: 1.47; acc: 0.62
Batch: 780; loss: 1.43; acc: 0.62
Train Epoch over. train_loss: 1.51; train_accuracy: 0.57 

4.7691188228782266e-05
1.7056549040717073e-05
Batch: 0; loss: 1.62; acc: 0.56
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.24; acc: 0.75
Batch: 60; loss: 1.52; acc: 0.53
Batch: 80; loss: 1.35; acc: 0.66
Batch: 100; loss: 1.56; acc: 0.62
Batch: 120; loss: 1.54; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.62
Val Epoch over. val_loss: 1.4601823478747324; val_accuracy: 0.5936504777070064 

The current subspace-distance is: 1.7056549040717073e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.52
Batch: 20; loss: 1.5; acc: 0.59
Batch: 40; loss: 1.46; acc: 0.58
Batch: 60; loss: 1.52; acc: 0.55
Batch: 80; loss: 1.51; acc: 0.58
Batch: 100; loss: 1.43; acc: 0.58
Batch: 120; loss: 1.57; acc: 0.5
Batch: 140; loss: 1.33; acc: 0.66
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.63; acc: 0.48
Batch: 200; loss: 1.62; acc: 0.56
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.58; acc: 0.53
Batch: 260; loss: 1.55; acc: 0.52
Batch: 280; loss: 1.52; acc: 0.53
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.5; acc: 0.55
Batch: 340; loss: 1.45; acc: 0.62
Batch: 360; loss: 1.59; acc: 0.53
Batch: 380; loss: 1.51; acc: 0.55
Batch: 400; loss: 1.59; acc: 0.59
Batch: 420; loss: 1.47; acc: 0.61
Batch: 440; loss: 1.43; acc: 0.59
Batch: 460; loss: 1.59; acc: 0.56
Batch: 480; loss: 1.69; acc: 0.53
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.59; acc: 0.48
Batch: 540; loss: 1.51; acc: 0.59
Batch: 560; loss: 1.63; acc: 0.5
Batch: 580; loss: 1.51; acc: 0.61
Batch: 600; loss: 1.45; acc: 0.61
Batch: 620; loss: 1.39; acc: 0.66
Batch: 640; loss: 1.46; acc: 0.56
Batch: 660; loss: 1.63; acc: 0.48
Batch: 680; loss: 1.47; acc: 0.58
Batch: 700; loss: 1.56; acc: 0.55
Batch: 720; loss: 1.55; acc: 0.53
Batch: 740; loss: 1.47; acc: 0.59
Batch: 760; loss: 1.48; acc: 0.56
Batch: 780; loss: 1.63; acc: 0.5
Train Epoch over. train_loss: 1.51; train_accuracy: 0.57 

4.804917625733651e-05
1.7440370356780477e-05
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.25; acc: 0.72
Batch: 60; loss: 1.53; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.67
Batch: 100; loss: 1.56; acc: 0.61
Batch: 120; loss: 1.55; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.66
Val Epoch over. val_loss: 1.4679958304022527; val_accuracy: 0.5928542993630573 

The current subspace-distance is: 1.7440370356780477e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.41; acc: 0.66
Batch: 20; loss: 1.42; acc: 0.69
Batch: 40; loss: 1.49; acc: 0.56
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.62; acc: 0.45
Batch: 100; loss: 1.38; acc: 0.59
Batch: 120; loss: 1.35; acc: 0.73
Batch: 140; loss: 1.44; acc: 0.62
Batch: 160; loss: 1.61; acc: 0.55
Batch: 180; loss: 1.48; acc: 0.61
Batch: 200; loss: 1.32; acc: 0.67
Batch: 220; loss: 1.48; acc: 0.58
Batch: 240; loss: 1.67; acc: 0.42
Batch: 260; loss: 1.64; acc: 0.47
Batch: 280; loss: 1.35; acc: 0.67
Batch: 300; loss: 1.52; acc: 0.53
Batch: 320; loss: 1.75; acc: 0.45
Batch: 340; loss: 1.51; acc: 0.59
Batch: 360; loss: 1.43; acc: 0.67
Batch: 380; loss: 1.46; acc: 0.61
Batch: 400; loss: 1.42; acc: 0.64
Batch: 420; loss: 1.53; acc: 0.58
Batch: 440; loss: 1.51; acc: 0.48
Batch: 460; loss: 1.69; acc: 0.45
Batch: 480; loss: 1.33; acc: 0.72
Batch: 500; loss: 1.45; acc: 0.66
Batch: 520; loss: 1.56; acc: 0.45
Batch: 540; loss: 1.59; acc: 0.58
Batch: 560; loss: 1.49; acc: 0.52
Batch: 580; loss: 1.43; acc: 0.58
Batch: 600; loss: 1.62; acc: 0.48
Batch: 620; loss: 1.36; acc: 0.69
Batch: 640; loss: 1.44; acc: 0.64
Batch: 660; loss: 1.64; acc: 0.47
Batch: 680; loss: 1.56; acc: 0.58
Batch: 700; loss: 1.47; acc: 0.58
Batch: 720; loss: 1.59; acc: 0.53
Batch: 740; loss: 1.51; acc: 0.56
Batch: 760; loss: 1.56; acc: 0.53
Batch: 780; loss: 1.63; acc: 0.45
Train Epoch over. train_loss: 1.51; train_accuracy: 0.57 

4.830766920349561e-05
1.8700529835768975e-05
Batch: 0; loss: 1.62; acc: 0.58
Batch: 20; loss: 1.6; acc: 0.48
Batch: 40; loss: 1.25; acc: 0.72
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.67
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.56; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.64
Val Epoch over. val_loss: 1.467153343425435; val_accuracy: 0.5969347133757962 

The current subspace-distance is: 1.8700529835768975e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.53
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.57; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.62
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.63; acc: 0.53
Batch: 140; loss: 1.49; acc: 0.58
Batch: 160; loss: 1.61; acc: 0.48
Batch: 180; loss: 1.45; acc: 0.62
Batch: 200; loss: 1.74; acc: 0.38
Batch: 220; loss: 1.58; acc: 0.56
Batch: 240; loss: 1.45; acc: 0.55
Batch: 260; loss: 1.53; acc: 0.61
Batch: 280; loss: 1.46; acc: 0.55
Batch: 300; loss: 1.42; acc: 0.66
Batch: 320; loss: 1.5; acc: 0.61
Batch: 340; loss: 1.44; acc: 0.61
Batch: 360; loss: 1.54; acc: 0.56
Batch: 380; loss: 1.45; acc: 0.56
Batch: 400; loss: 1.35; acc: 0.67
Batch: 420; loss: 1.38; acc: 0.66
Batch: 440; loss: 1.42; acc: 0.64
Batch: 460; loss: 1.62; acc: 0.47
Batch: 480; loss: 1.4; acc: 0.61
Batch: 500; loss: 1.36; acc: 0.67
Batch: 520; loss: 1.47; acc: 0.58
Batch: 540; loss: 1.49; acc: 0.59
Batch: 560; loss: 1.51; acc: 0.52
Batch: 580; loss: 1.39; acc: 0.66
Batch: 600; loss: 1.46; acc: 0.53
Batch: 620; loss: 1.52; acc: 0.62
Batch: 640; loss: 1.54; acc: 0.53
Batch: 660; loss: 1.46; acc: 0.58
Batch: 680; loss: 1.43; acc: 0.64
Batch: 700; loss: 1.53; acc: 0.56
Batch: 720; loss: 1.27; acc: 0.72
Batch: 740; loss: 1.57; acc: 0.59
Batch: 760; loss: 1.48; acc: 0.58
Batch: 780; loss: 1.63; acc: 0.52
Train Epoch over. train_loss: 1.5; train_accuracy: 0.57 

4.7261855797842145e-05
1.3596715689345729e-05
Batch: 0; loss: 1.63; acc: 0.56
Batch: 20; loss: 1.6; acc: 0.47
Batch: 40; loss: 1.26; acc: 0.7
Batch: 60; loss: 1.53; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.64
Batch: 100; loss: 1.57; acc: 0.64
Batch: 120; loss: 1.56; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.64
Val Epoch over. val_loss: 1.4697563169868129; val_accuracy: 0.59375 

The current subspace-distance is: 1.3596715689345729e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.36; acc: 0.66
Batch: 20; loss: 1.47; acc: 0.56
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.53
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.34; acc: 0.75
Batch: 120; loss: 1.46; acc: 0.58
Batch: 140; loss: 1.57; acc: 0.5
Batch: 160; loss: 1.53; acc: 0.59
Batch: 180; loss: 1.59; acc: 0.53
Batch: 200; loss: 1.63; acc: 0.42
Batch: 220; loss: 1.52; acc: 0.53
Batch: 240; loss: 1.57; acc: 0.48
Batch: 260; loss: 1.71; acc: 0.44
Batch: 280; loss: 1.38; acc: 0.66
Batch: 300; loss: 1.56; acc: 0.52
Batch: 320; loss: 1.63; acc: 0.55
Batch: 340; loss: 1.52; acc: 0.53
Batch: 360; loss: 1.46; acc: 0.58
Batch: 380; loss: 1.57; acc: 0.53
Batch: 400; loss: 1.4; acc: 0.67
Batch: 420; loss: 1.39; acc: 0.64
Batch: 440; loss: 1.45; acc: 0.56
Batch: 460; loss: 1.56; acc: 0.52
Batch: 480; loss: 1.52; acc: 0.59
Batch: 500; loss: 1.51; acc: 0.56
Batch: 520; loss: 1.42; acc: 0.64
Batch: 540; loss: 1.33; acc: 0.69
Batch: 560; loss: 1.52; acc: 0.52
Batch: 580; loss: 1.38; acc: 0.64
Batch: 600; loss: 1.43; acc: 0.64
Batch: 620; loss: 1.55; acc: 0.58
Batch: 640; loss: 1.44; acc: 0.58
Batch: 660; loss: 1.44; acc: 0.59
Batch: 680; loss: 1.6; acc: 0.52
Batch: 700; loss: 1.53; acc: 0.58
Batch: 720; loss: 1.43; acc: 0.59
Batch: 740; loss: 1.43; acc: 0.59
Batch: 760; loss: 1.39; acc: 0.62
Batch: 780; loss: 1.52; acc: 0.53
Train Epoch over. train_loss: 1.5; train_accuracy: 0.57 

4.985385749023408e-05
1.882416290754918e-05
Batch: 0; loss: 1.62; acc: 0.56
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.25; acc: 0.72
Batch: 60; loss: 1.52; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.64
Batch: 100; loss: 1.56; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.58
Batch: 140; loss: 1.36; acc: 0.62
Val Epoch over. val_loss: 1.4612909866746064; val_accuracy: 0.5902667197452229 

The current subspace-distance is: 1.882416290754918e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_5_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.25

The number of parameters is: 276579

The number of individual parameters is:

18
288
18
18
27
41796
27
27
54
125388
54
54
64
103680
64
64
4096
64
640
10
64
64

nonzero elements in E: 27657897
elements in E: 27657900
fraction nonzero: 0.9999998915318951
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.08
Batch: 20; loss: 2.35; acc: 0.06
Batch: 40; loss: 2.23; acc: 0.17
Batch: 60; loss: 2.11; acc: 0.27
Batch: 80; loss: 2.11; acc: 0.23
Batch: 100; loss: 2.07; acc: 0.33
Batch: 120; loss: 2.0; acc: 0.31
Batch: 140; loss: 2.09; acc: 0.27
Batch: 160; loss: 2.02; acc: 0.39
Batch: 180; loss: 1.85; acc: 0.45
Batch: 200; loss: 1.96; acc: 0.47
Batch: 220; loss: 1.94; acc: 0.33
Batch: 240; loss: 1.81; acc: 0.58
Batch: 260; loss: 1.7; acc: 0.62
Batch: 280; loss: 1.78; acc: 0.52
Batch: 300; loss: 1.79; acc: 0.56
Batch: 320; loss: 1.78; acc: 0.44
Batch: 340; loss: 1.82; acc: 0.5
Batch: 360; loss: 1.81; acc: 0.45
Batch: 380; loss: 1.82; acc: 0.45
Batch: 400; loss: 1.74; acc: 0.53
Batch: 420; loss: 1.69; acc: 0.53
Batch: 440; loss: 1.74; acc: 0.47
Batch: 460; loss: 1.72; acc: 0.56
Batch: 480; loss: 1.69; acc: 0.47
Batch: 500; loss: 1.68; acc: 0.56
Batch: 520; loss: 1.73; acc: 0.53
Batch: 540; loss: 1.71; acc: 0.47
Batch: 560; loss: 1.62; acc: 0.59
Batch: 580; loss: 1.73; acc: 0.47
Batch: 600; loss: 1.58; acc: 0.67
Batch: 620; loss: 1.66; acc: 0.53
Batch: 640; loss: 1.57; acc: 0.59
Batch: 660; loss: 1.6; acc: 0.61
Batch: 680; loss: 1.56; acc: 0.7
Batch: 700; loss: 1.69; acc: 0.56
Batch: 720; loss: 1.62; acc: 0.53
Batch: 740; loss: 1.54; acc: 0.64
Batch: 760; loss: 1.53; acc: 0.62
Batch: 780; loss: 1.65; acc: 0.52
Train Epoch over. train_loss: 1.8; train_accuracy: 0.48 

5.673653140547685e-05
5.193272954784334e-05
Batch: 0; loss: 1.59; acc: 0.58
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.41; acc: 0.69
Batch: 60; loss: 1.5; acc: 0.67
Batch: 80; loss: 1.52; acc: 0.64
Batch: 100; loss: 1.62; acc: 0.55
Batch: 120; loss: 1.65; acc: 0.55
Batch: 140; loss: 1.41; acc: 0.75
Val Epoch over. val_loss: 1.5587673354300724; val_accuracy: 0.6091759554140127 

The current subspace-distance is: 5.193272954784334e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.48
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.65; acc: 0.48
Batch: 60; loss: 1.5; acc: 0.69
Batch: 80; loss: 1.49; acc: 0.69
Batch: 100; loss: 1.58; acc: 0.53
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.61; acc: 0.52
Batch: 160; loss: 1.59; acc: 0.61
Batch: 180; loss: 1.61; acc: 0.55
Batch: 200; loss: 1.66; acc: 0.52
Batch: 220; loss: 1.6; acc: 0.55
Batch: 240; loss: 1.55; acc: 0.61
Batch: 260; loss: 1.55; acc: 0.59
Batch: 280; loss: 1.65; acc: 0.48
Batch: 300; loss: 1.56; acc: 0.59
Batch: 320; loss: 1.63; acc: 0.56
Batch: 340; loss: 1.48; acc: 0.62
Batch: 360; loss: 1.57; acc: 0.62
Batch: 380; loss: 1.59; acc: 0.56
Batch: 400; loss: 1.56; acc: 0.62
Batch: 420; loss: 1.52; acc: 0.64
Batch: 440; loss: 1.68; acc: 0.47
Batch: 460; loss: 1.54; acc: 0.61
Batch: 480; loss: 1.54; acc: 0.61
Batch: 500; loss: 1.57; acc: 0.59
Batch: 520; loss: 1.56; acc: 0.61
Batch: 540; loss: 1.44; acc: 0.66
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.44; acc: 0.59
Batch: 600; loss: 1.47; acc: 0.66
Batch: 620; loss: 1.59; acc: 0.52
Batch: 640; loss: 1.49; acc: 0.67
Batch: 660; loss: 1.75; acc: 0.44
Batch: 680; loss: 1.5; acc: 0.55
Batch: 700; loss: 1.49; acc: 0.53
Batch: 720; loss: 1.59; acc: 0.62
Batch: 740; loss: 1.52; acc: 0.64
Batch: 760; loss: 1.39; acc: 0.69
Batch: 780; loss: 1.54; acc: 0.59
Train Epoch over. train_loss: 1.55; train_accuracy: 0.59 

7.234549411805347e-05
6.71270172460936e-05
Batch: 0; loss: 1.54; acc: 0.58
Batch: 20; loss: 1.55; acc: 0.53
Batch: 40; loss: 1.25; acc: 0.78
Batch: 60; loss: 1.46; acc: 0.64
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.51; acc: 0.66
Batch: 120; loss: 1.55; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.8
Val Epoch over. val_loss: 1.47109439570433; val_accuracy: 0.6286823248407644 

The current subspace-distance is: 6.71270172460936e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.45
Batch: 20; loss: 1.6; acc: 0.58
Batch: 40; loss: 1.51; acc: 0.53
Batch: 60; loss: 1.44; acc: 0.64
Batch: 80; loss: 1.47; acc: 0.66
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.42; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.64
Batch: 160; loss: 1.36; acc: 0.77
Batch: 180; loss: 1.4; acc: 0.69
Batch: 200; loss: 1.46; acc: 0.64
Batch: 220; loss: 1.37; acc: 0.69
Batch: 240; loss: 1.52; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.61
Batch: 280; loss: 1.42; acc: 0.64
Batch: 300; loss: 1.38; acc: 0.66
Batch: 320; loss: 1.43; acc: 0.69
Batch: 340; loss: 1.55; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.59
Batch: 380; loss: 1.54; acc: 0.53
Batch: 400; loss: 1.48; acc: 0.53
Batch: 420; loss: 1.52; acc: 0.5
Batch: 440; loss: 1.45; acc: 0.62
Batch: 460; loss: 1.5; acc: 0.64
Batch: 480; loss: 1.37; acc: 0.64
Batch: 500; loss: 1.51; acc: 0.56
Batch: 520; loss: 1.7; acc: 0.5
Batch: 540; loss: 1.24; acc: 0.72
Batch: 560; loss: 1.59; acc: 0.55
Batch: 580; loss: 1.34; acc: 0.73
Batch: 600; loss: 1.41; acc: 0.62
Batch: 620; loss: 1.56; acc: 0.52
Batch: 640; loss: 1.45; acc: 0.61
Batch: 660; loss: 1.49; acc: 0.64
Batch: 680; loss: 1.57; acc: 0.55
Batch: 700; loss: 1.48; acc: 0.64
Batch: 720; loss: 1.48; acc: 0.61
Batch: 740; loss: 1.44; acc: 0.69
Batch: 760; loss: 1.36; acc: 0.64
Batch: 780; loss: 1.41; acc: 0.59
Train Epoch over. train_loss: 1.48; train_accuracy: 0.61 

8.57196791912429e-05
8.158932178048417e-05
Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.51; acc: 0.52
Batch: 40; loss: 1.14; acc: 0.81
Batch: 60; loss: 1.42; acc: 0.62
Batch: 80; loss: 1.34; acc: 0.67
Batch: 100; loss: 1.44; acc: 0.64
Batch: 120; loss: 1.49; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.83
Val Epoch over. val_loss: 1.4017187523993717; val_accuracy: 0.6587380573248408 

The current subspace-distance is: 8.158932178048417e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.45; acc: 0.58
Batch: 40; loss: 1.43; acc: 0.7
Batch: 60; loss: 1.51; acc: 0.62
Batch: 80; loss: 1.34; acc: 0.7
Batch: 100; loss: 1.48; acc: 0.62
Batch: 120; loss: 1.61; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.66
Batch: 160; loss: 1.37; acc: 0.67
Batch: 180; loss: 1.44; acc: 0.66
Batch: 200; loss: 1.42; acc: 0.61
Batch: 220; loss: 1.43; acc: 0.69
Batch: 240; loss: 1.44; acc: 0.61
Batch: 260; loss: 1.36; acc: 0.69
Batch: 280; loss: 1.5; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.59
Batch: 320; loss: 1.45; acc: 0.66
Batch: 340; loss: 1.43; acc: 0.52
Batch: 360; loss: 1.43; acc: 0.69
Batch: 380; loss: 1.46; acc: 0.61
Batch: 400; loss: 1.31; acc: 0.7
Batch: 420; loss: 1.51; acc: 0.61
Batch: 440; loss: 1.52; acc: 0.58
Batch: 460; loss: 1.37; acc: 0.66
Batch: 480; loss: 1.41; acc: 0.64
Batch: 500; loss: 1.52; acc: 0.53
Batch: 520; loss: 1.45; acc: 0.56
Batch: 540; loss: 1.42; acc: 0.56
Batch: 560; loss: 1.42; acc: 0.62
Batch: 580; loss: 1.44; acc: 0.64
Batch: 600; loss: 1.39; acc: 0.61
Batch: 620; loss: 1.5; acc: 0.59
Batch: 640; loss: 1.46; acc: 0.62
Batch: 660; loss: 1.31; acc: 0.69
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.26; acc: 0.72
Batch: 720; loss: 1.31; acc: 0.67
Batch: 740; loss: 1.42; acc: 0.66
Batch: 760; loss: 1.37; acc: 0.61
Batch: 780; loss: 1.37; acc: 0.67
Train Epoch over. train_loss: 1.41; train_accuracy: 0.64 

0.00010105282126460224
9.629945998312905e-05
Batch: 0; loss: 1.41; acc: 0.59
Batch: 20; loss: 1.44; acc: 0.59
Batch: 40; loss: 1.09; acc: 0.81
Batch: 60; loss: 1.32; acc: 0.7
Batch: 80; loss: 1.25; acc: 0.73
Batch: 100; loss: 1.36; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.64
Batch: 140; loss: 1.18; acc: 0.75
Val Epoch over. val_loss: 1.3287033609523895; val_accuracy: 0.6831210191082803 

The current subspace-distance is: 9.629945998312905e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.7
Batch: 20; loss: 1.33; acc: 0.7
Batch: 40; loss: 1.37; acc: 0.64
Batch: 60; loss: 1.34; acc: 0.7
Batch: 80; loss: 1.36; acc: 0.64
Batch: 100; loss: 1.39; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.62
Batch: 140; loss: 1.24; acc: 0.75
Batch: 160; loss: 1.39; acc: 0.7
Batch: 180; loss: 1.49; acc: 0.62
Batch: 200; loss: 1.25; acc: 0.69
Batch: 220; loss: 1.4; acc: 0.64
Batch: 240; loss: 1.21; acc: 0.75
Batch: 260; loss: 1.35; acc: 0.72
Batch: 280; loss: 1.22; acc: 0.78
Batch: 300; loss: 1.33; acc: 0.69
Batch: 320; loss: 1.23; acc: 0.73
Batch: 340; loss: 1.28; acc: 0.72
Batch: 360; loss: 1.3; acc: 0.64
Batch: 380; loss: 1.4; acc: 0.59
Batch: 400; loss: 1.45; acc: 0.56
Batch: 420; loss: 1.43; acc: 0.59
Batch: 440; loss: 1.29; acc: 0.67
Batch: 460; loss: 1.43; acc: 0.64
Batch: 480; loss: 1.33; acc: 0.64
Batch: 500; loss: 1.38; acc: 0.64
Batch: 520; loss: 1.23; acc: 0.7
Batch: 540; loss: 1.3; acc: 0.66
Batch: 560; loss: 1.11; acc: 0.73
Batch: 580; loss: 1.35; acc: 0.67
Batch: 600; loss: 1.27; acc: 0.69
Batch: 620; loss: 1.24; acc: 0.7
Batch: 640; loss: 1.36; acc: 0.64
Batch: 660; loss: 1.3; acc: 0.66
Batch: 680; loss: 1.28; acc: 0.62
Batch: 700; loss: 1.3; acc: 0.69
Batch: 720; loss: 1.3; acc: 0.66
Batch: 740; loss: 1.32; acc: 0.7
Batch: 760; loss: 1.37; acc: 0.67
Batch: 780; loss: 1.3; acc: 0.66
Train Epoch over. train_loss: 1.33; train_accuracy: 0.66 

0.00011519902909640223
0.00011036717478418723
Batch: 0; loss: 1.32; acc: 0.67
Batch: 20; loss: 1.34; acc: 0.69
Batch: 40; loss: 1.02; acc: 0.81
Batch: 60; loss: 1.24; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.3; acc: 0.66
Batch: 120; loss: 1.32; acc: 0.64
Batch: 140; loss: 1.11; acc: 0.78
Val Epoch over. val_loss: 1.2608638432375185; val_accuracy: 0.6888933121019108 

The current subspace-distance is: 0.00011036717478418723 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.67
Batch: 20; loss: 1.23; acc: 0.67
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.33; acc: 0.62
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.19; acc: 0.77
Batch: 120; loss: 1.27; acc: 0.69
Batch: 140; loss: 1.32; acc: 0.59
Batch: 160; loss: 1.25; acc: 0.64
Batch: 180; loss: 1.25; acc: 0.75
Batch: 200; loss: 1.22; acc: 0.67
Batch: 220; loss: 1.25; acc: 0.72
Batch: 240; loss: 1.29; acc: 0.64
Batch: 260; loss: 1.37; acc: 0.55
Batch: 280; loss: 1.45; acc: 0.61
Batch: 300; loss: 1.41; acc: 0.56
Batch: 320; loss: 1.27; acc: 0.66
Batch: 340; loss: 1.33; acc: 0.62
Batch: 360; loss: 1.38; acc: 0.59
Batch: 380; loss: 1.38; acc: 0.5
Batch: 400; loss: 1.22; acc: 0.7
Batch: 420; loss: 1.23; acc: 0.69
Batch: 440; loss: 1.25; acc: 0.7
Batch: 460; loss: 1.3; acc: 0.67
Batch: 480; loss: 1.36; acc: 0.61
Batch: 500; loss: 1.22; acc: 0.7
Batch: 520; loss: 1.34; acc: 0.62
Batch: 540; loss: 1.38; acc: 0.66
Batch: 560; loss: 1.29; acc: 0.58
Batch: 580; loss: 1.25; acc: 0.66
Batch: 600; loss: 1.31; acc: 0.61
Batch: 620; loss: 1.3; acc: 0.64
Batch: 640; loss: 1.17; acc: 0.69
Batch: 660; loss: 1.25; acc: 0.64
Batch: 680; loss: 1.4; acc: 0.56
Batch: 700; loss: 1.17; acc: 0.67
Batch: 720; loss: 1.14; acc: 0.66
Batch: 740; loss: 1.22; acc: 0.72
Batch: 760; loss: 1.22; acc: 0.69
Batch: 780; loss: 1.25; acc: 0.64
Train Epoch over. train_loss: 1.27; train_accuracy: 0.67 

0.00012769608292728662
0.00012345827417448163
Batch: 0; loss: 1.21; acc: 0.67
Batch: 20; loss: 1.24; acc: 0.72
Batch: 40; loss: 0.96; acc: 0.8
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 1.08; acc: 0.78
Batch: 100; loss: 1.24; acc: 0.67
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 1.09; acc: 0.75
Val Epoch over. val_loss: 1.1929112327326634; val_accuracy: 0.6977507961783439 

The current subspace-distance is: 0.00012345827417448163 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.67
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 1.28; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.39; acc: 0.59
Batch: 100; loss: 1.18; acc: 0.72
Batch: 120; loss: 1.27; acc: 0.73
Batch: 140; loss: 1.33; acc: 0.61
Batch: 160; loss: 1.23; acc: 0.64
Batch: 180; loss: 1.31; acc: 0.56
Batch: 200; loss: 1.22; acc: 0.7
Batch: 220; loss: 1.26; acc: 0.69
Batch: 240; loss: 1.31; acc: 0.56
Batch: 260; loss: 1.29; acc: 0.67
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.27; acc: 0.66
Batch: 320; loss: 1.1; acc: 0.77
Batch: 340; loss: 1.28; acc: 0.62
Batch: 360; loss: 1.15; acc: 0.67
Batch: 380; loss: 1.22; acc: 0.69
Batch: 400; loss: 1.32; acc: 0.61
Batch: 420; loss: 1.33; acc: 0.59
Batch: 440; loss: 1.32; acc: 0.59
Batch: 460; loss: 1.23; acc: 0.7
Batch: 480; loss: 1.08; acc: 0.8
Batch: 500; loss: 1.18; acc: 0.67
Batch: 520; loss: 1.18; acc: 0.66
Batch: 540; loss: 1.07; acc: 0.81
Batch: 560; loss: 1.07; acc: 0.8
Batch: 580; loss: 1.32; acc: 0.55
Batch: 600; loss: 1.02; acc: 0.7
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.23; acc: 0.59
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.11; acc: 0.7
Batch: 700; loss: 1.17; acc: 0.7
Batch: 720; loss: 1.41; acc: 0.56
Batch: 740; loss: 1.2; acc: 0.62
Batch: 760; loss: 1.07; acc: 0.75
Batch: 780; loss: 1.15; acc: 0.73
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.00014254479901865125
0.00013666943414136767
Batch: 0; loss: 1.15; acc: 0.69
Batch: 20; loss: 1.21; acc: 0.69
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.02; acc: 0.81
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.22; acc: 0.67
Batch: 140; loss: 1.06; acc: 0.72
Val Epoch over. val_loss: 1.1545288038861221; val_accuracy: 0.7015326433121019 

The current subspace-distance is: 0.00013666943414136767 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.22; acc: 0.69
Batch: 40; loss: 1.29; acc: 0.61
Batch: 60; loss: 1.26; acc: 0.62
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.07; acc: 0.77
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 1.18; acc: 0.69
Batch: 160; loss: 1.3; acc: 0.61
Batch: 180; loss: 1.27; acc: 0.59
Batch: 200; loss: 1.31; acc: 0.56
Batch: 220; loss: 1.25; acc: 0.61
Batch: 240; loss: 1.08; acc: 0.77
Batch: 260; loss: 1.12; acc: 0.72
Batch: 280; loss: 1.18; acc: 0.77
Batch: 300; loss: 1.21; acc: 0.75
Batch: 320; loss: 1.1; acc: 0.7
Batch: 340; loss: 1.14; acc: 0.66
Batch: 360; loss: 1.34; acc: 0.64
Batch: 380; loss: 1.08; acc: 0.75
Batch: 400; loss: 1.19; acc: 0.69
Batch: 420; loss: 1.22; acc: 0.67
Batch: 440; loss: 1.17; acc: 0.67
Batch: 460; loss: 1.06; acc: 0.78
Batch: 480; loss: 1.15; acc: 0.67
Batch: 500; loss: 1.06; acc: 0.73
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 1.16; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.66
Batch: 580; loss: 1.22; acc: 0.69
Batch: 600; loss: 1.17; acc: 0.62
Batch: 620; loss: 1.15; acc: 0.69
Batch: 640; loss: 1.16; acc: 0.66
Batch: 660; loss: 1.16; acc: 0.72
Batch: 680; loss: 1.24; acc: 0.64
Batch: 700; loss: 1.23; acc: 0.64
Batch: 720; loss: 1.1; acc: 0.75
Batch: 740; loss: 1.31; acc: 0.66
Batch: 760; loss: 1.06; acc: 0.73
Batch: 780; loss: 1.17; acc: 0.75
Train Epoch over. train_loss: 1.18; train_accuracy: 0.68 

0.0001511766604380682
0.00014618717250414193
Batch: 0; loss: 1.11; acc: 0.69
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.84
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.69
Batch: 140; loss: 1.04; acc: 0.7
Val Epoch over. val_loss: 1.1081449947539408; val_accuracy: 0.7150676751592356 

The current subspace-distance is: 0.00014618717250414193 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.64
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 1.12; acc: 0.72
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.13; acc: 0.78
Batch: 140; loss: 1.19; acc: 0.62
Batch: 160; loss: 1.07; acc: 0.73
Batch: 180; loss: 1.15; acc: 0.72
Batch: 200; loss: 1.3; acc: 0.59
Batch: 220; loss: 1.17; acc: 0.7
Batch: 240; loss: 1.13; acc: 0.72
Batch: 260; loss: 1.18; acc: 0.72
Batch: 280; loss: 1.2; acc: 0.62
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.16; acc: 0.69
Batch: 340; loss: 1.1; acc: 0.67
Batch: 360; loss: 1.19; acc: 0.67
Batch: 380; loss: 1.23; acc: 0.67
Batch: 400; loss: 1.12; acc: 0.7
Batch: 420; loss: 1.06; acc: 0.75
Batch: 440; loss: 1.05; acc: 0.78
Batch: 460; loss: 1.19; acc: 0.62
Batch: 480; loss: 1.16; acc: 0.67
Batch: 500; loss: 1.25; acc: 0.62
Batch: 520; loss: 1.11; acc: 0.69
Batch: 540; loss: 1.23; acc: 0.66
Batch: 560; loss: 1.14; acc: 0.75
Batch: 580; loss: 1.06; acc: 0.67
Batch: 600; loss: 1.23; acc: 0.69
Batch: 620; loss: 1.24; acc: 0.64
Batch: 640; loss: 1.3; acc: 0.56
Batch: 660; loss: 1.18; acc: 0.67
Batch: 680; loss: 1.18; acc: 0.67
Batch: 700; loss: 1.17; acc: 0.72
Batch: 720; loss: 1.14; acc: 0.78
Batch: 740; loss: 1.13; acc: 0.72
Batch: 760; loss: 1.04; acc: 0.75
Batch: 780; loss: 1.13; acc: 0.72
Train Epoch over. train_loss: 1.15; train_accuracy: 0.69 

0.00016143571701832116
0.0001553901965962723
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 0.92; acc: 0.88
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 1.04; acc: 0.73
Val Epoch over. val_loss: 1.0753767471404592; val_accuracy: 0.7338773885350318 

The current subspace-distance is: 0.0001553901965962723 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.69
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 1.15; acc: 0.7
Batch: 60; loss: 1.18; acc: 0.73
Batch: 80; loss: 1.17; acc: 0.66
Batch: 100; loss: 1.19; acc: 0.62
Batch: 120; loss: 1.1; acc: 0.77
Batch: 140; loss: 1.16; acc: 0.66
Batch: 160; loss: 1.17; acc: 0.67
Batch: 180; loss: 1.18; acc: 0.67
Batch: 200; loss: 0.93; acc: 0.8
Batch: 220; loss: 1.07; acc: 0.72
Batch: 240; loss: 1.25; acc: 0.56
Batch: 260; loss: 1.05; acc: 0.78
Batch: 280; loss: 1.3; acc: 0.61
Batch: 300; loss: 1.12; acc: 0.72
Batch: 320; loss: 1.09; acc: 0.69
Batch: 340; loss: 1.09; acc: 0.73
Batch: 360; loss: 1.07; acc: 0.75
Batch: 380; loss: 1.13; acc: 0.73
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 1.05; acc: 0.78
Batch: 440; loss: 1.18; acc: 0.67
Batch: 460; loss: 1.2; acc: 0.66
Batch: 480; loss: 1.14; acc: 0.67
Batch: 500; loss: 1.07; acc: 0.73
Batch: 520; loss: 1.04; acc: 0.8
Batch: 540; loss: 0.95; acc: 0.84
Batch: 560; loss: 1.08; acc: 0.78
Batch: 580; loss: 0.98; acc: 0.75
Batch: 600; loss: 1.18; acc: 0.61
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 1.3; acc: 0.59
Batch: 660; loss: 1.35; acc: 0.61
Batch: 680; loss: 1.38; acc: 0.56
Batch: 700; loss: 1.13; acc: 0.7
Batch: 720; loss: 1.24; acc: 0.64
Batch: 740; loss: 1.04; acc: 0.73
Batch: 760; loss: 1.16; acc: 0.67
Batch: 780; loss: 1.07; acc: 0.72
Train Epoch over. train_loss: 1.13; train_accuracy: 0.7 

0.00016824548947624862
0.00016196559590753168
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.73
Batch: 80; loss: 0.9; acc: 0.88
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.72
Val Epoch over. val_loss: 1.0704999793866636; val_accuracy: 0.7327826433121019 

The current subspace-distance is: 0.00016196559590753168 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.99; acc: 0.75
Batch: 60; loss: 1.14; acc: 0.67
Batch: 80; loss: 1.01; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.7
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.1; acc: 0.77
Batch: 160; loss: 1.12; acc: 0.69
Batch: 180; loss: 1.05; acc: 0.7
Batch: 200; loss: 1.14; acc: 0.7
Batch: 220; loss: 1.23; acc: 0.62
Batch: 240; loss: 0.98; acc: 0.78
Batch: 260; loss: 1.04; acc: 0.78
Batch: 280; loss: 1.03; acc: 0.8
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 1.1; acc: 0.69
Batch: 340; loss: 1.04; acc: 0.75
Batch: 360; loss: 0.95; acc: 0.78
Batch: 380; loss: 1.04; acc: 0.77
Batch: 400; loss: 1.03; acc: 0.78
Batch: 420; loss: 1.14; acc: 0.72
Batch: 440; loss: 1.07; acc: 0.73
Batch: 460; loss: 0.9; acc: 0.81
Batch: 480; loss: 1.06; acc: 0.72
Batch: 500; loss: 1.11; acc: 0.72
Batch: 520; loss: 1.04; acc: 0.67
Batch: 540; loss: 1.29; acc: 0.59
Batch: 560; loss: 1.03; acc: 0.73
Batch: 580; loss: 0.93; acc: 0.78
Batch: 600; loss: 1.19; acc: 0.7
Batch: 620; loss: 1.09; acc: 0.78
Batch: 640; loss: 1.08; acc: 0.77
Batch: 660; loss: 1.06; acc: 0.73
Batch: 680; loss: 0.96; acc: 0.8
Batch: 700; loss: 0.95; acc: 0.8
Batch: 720; loss: 1.05; acc: 0.69
Batch: 740; loss: 1.16; acc: 0.7
Batch: 760; loss: 1.2; acc: 0.64
Batch: 780; loss: 1.21; acc: 0.61
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.0001717947016004473
0.00016452140698675066
Batch: 0; loss: 1.07; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 1.06; acc: 0.78
Batch: 80; loss: 0.88; acc: 0.88
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 1.03; acc: 0.73
Val Epoch over. val_loss: 1.0466910099527638; val_accuracy: 0.7477109872611465 

The current subspace-distance is: 0.00016452140698675066 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.08; acc: 0.78
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.21; acc: 0.64
Batch: 100; loss: 1.18; acc: 0.61
Batch: 120; loss: 1.13; acc: 0.78
Batch: 140; loss: 1.19; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.67
Batch: 180; loss: 1.06; acc: 0.67
Batch: 200; loss: 0.98; acc: 0.75
Batch: 220; loss: 1.19; acc: 0.64
Batch: 240; loss: 0.94; acc: 0.77
Batch: 260; loss: 1.15; acc: 0.75
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 0.97; acc: 0.83
Batch: 340; loss: 1.05; acc: 0.75
Batch: 360; loss: 1.0; acc: 0.72
Batch: 380; loss: 1.2; acc: 0.67
Batch: 400; loss: 1.12; acc: 0.69
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.16; acc: 0.66
Batch: 460; loss: 1.0; acc: 0.75
Batch: 480; loss: 1.1; acc: 0.73
Batch: 500; loss: 0.92; acc: 0.78
Batch: 520; loss: 1.15; acc: 0.67
Batch: 540; loss: 1.07; acc: 0.73
Batch: 560; loss: 1.15; acc: 0.73
Batch: 580; loss: 1.03; acc: 0.81
Batch: 600; loss: 1.33; acc: 0.58
Batch: 620; loss: 1.11; acc: 0.64
Batch: 640; loss: 1.07; acc: 0.64
Batch: 660; loss: 1.13; acc: 0.67
Batch: 680; loss: 1.14; acc: 0.72
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.14; acc: 0.69
Batch: 760; loss: 1.0; acc: 0.7
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00017499775276519358
0.00016870760009624064
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 0.88; acc: 0.86
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.11; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.66
Val Epoch over. val_loss: 1.062639380336567; val_accuracy: 0.7323845541401274 

The current subspace-distance is: 0.00016870760009624064 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.0; acc: 0.77
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 1.1; acc: 0.64
Batch: 60; loss: 1.26; acc: 0.61
Batch: 80; loss: 1.07; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.22; acc: 0.62
Batch: 160; loss: 1.02; acc: 0.73
Batch: 180; loss: 1.18; acc: 0.67
Batch: 200; loss: 1.14; acc: 0.67
Batch: 220; loss: 1.14; acc: 0.64
Batch: 240; loss: 1.29; acc: 0.59
Batch: 260; loss: 1.2; acc: 0.64
Batch: 280; loss: 0.99; acc: 0.73
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.1; acc: 0.75
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.09; acc: 0.66
Batch: 380; loss: 1.04; acc: 0.7
Batch: 400; loss: 1.11; acc: 0.7
Batch: 420; loss: 1.03; acc: 0.73
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 1.03; acc: 0.77
Batch: 480; loss: 1.07; acc: 0.72
Batch: 500; loss: 0.94; acc: 0.78
Batch: 520; loss: 1.2; acc: 0.62
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.19; acc: 0.75
Batch: 580; loss: 1.1; acc: 0.67
Batch: 600; loss: 1.08; acc: 0.7
Batch: 620; loss: 0.87; acc: 0.83
Batch: 640; loss: 1.04; acc: 0.7
Batch: 660; loss: 1.02; acc: 0.8
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 1.13; acc: 0.67
Batch: 720; loss: 1.14; acc: 0.77
Batch: 740; loss: 1.17; acc: 0.64
Batch: 760; loss: 1.01; acc: 0.7
Batch: 780; loss: 1.17; acc: 0.66
Train Epoch over. train_loss: 1.1; train_accuracy: 0.7 

0.0001760808954713866
0.0001718306593829766
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.15; acc: 0.64
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 0.88; acc: 0.86
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.67
Val Epoch over. val_loss: 1.0520670805007788; val_accuracy: 0.7383558917197452 

The current subspace-distance is: 0.0001718306593829766 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.0; acc: 0.81
Batch: 20; loss: 0.97; acc: 0.78
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.08; acc: 0.73
Batch: 80; loss: 1.27; acc: 0.66
Batch: 100; loss: 1.13; acc: 0.69
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 0.98; acc: 0.75
Batch: 160; loss: 1.24; acc: 0.62
Batch: 180; loss: 1.16; acc: 0.73
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 1.27; acc: 0.59
Batch: 240; loss: 1.23; acc: 0.61
Batch: 260; loss: 1.21; acc: 0.69
Batch: 280; loss: 1.08; acc: 0.7
Batch: 300; loss: 1.26; acc: 0.59
Batch: 320; loss: 1.15; acc: 0.64
Batch: 340; loss: 0.89; acc: 0.8
Batch: 360; loss: 0.92; acc: 0.81
Batch: 380; loss: 1.11; acc: 0.67
Batch: 400; loss: 1.12; acc: 0.78
Batch: 420; loss: 1.18; acc: 0.64
Batch: 440; loss: 0.9; acc: 0.78
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 1.06; acc: 0.72
Batch: 500; loss: 1.09; acc: 0.69
Batch: 520; loss: 1.15; acc: 0.64
Batch: 540; loss: 1.12; acc: 0.64
Batch: 560; loss: 1.15; acc: 0.64
Batch: 580; loss: 1.13; acc: 0.69
Batch: 600; loss: 1.18; acc: 0.62
Batch: 620; loss: 1.06; acc: 0.72
Batch: 640; loss: 1.02; acc: 0.77
Batch: 660; loss: 1.06; acc: 0.66
Batch: 680; loss: 1.31; acc: 0.59
Batch: 700; loss: 1.01; acc: 0.72
Batch: 720; loss: 1.16; acc: 0.62
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 0.9; acc: 0.8
Batch: 780; loss: 1.03; acc: 0.77
Train Epoch over. train_loss: 1.1; train_accuracy: 0.7 

0.00017789055709727108
0.0001725683396216482
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 1.06; acc: 0.73
Batch: 80; loss: 0.87; acc: 0.86
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.12; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.67
Val Epoch over. val_loss: 1.0548239851453503; val_accuracy: 0.7345740445859873 

The current subspace-distance is: 0.0001725683396216482 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.67
Batch: 20; loss: 1.15; acc: 0.66
Batch: 40; loss: 1.15; acc: 0.67
Batch: 60; loss: 1.17; acc: 0.66
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 1.0; acc: 0.73
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 1.06; acc: 0.77
Batch: 160; loss: 0.94; acc: 0.77
Batch: 180; loss: 0.97; acc: 0.8
Batch: 200; loss: 1.18; acc: 0.62
Batch: 220; loss: 1.12; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.69
Batch: 260; loss: 1.06; acc: 0.72
Batch: 280; loss: 0.99; acc: 0.75
Batch: 300; loss: 1.23; acc: 0.64
Batch: 320; loss: 1.12; acc: 0.72
Batch: 340; loss: 1.13; acc: 0.69
Batch: 360; loss: 1.37; acc: 0.56
Batch: 380; loss: 1.26; acc: 0.62
Batch: 400; loss: 1.03; acc: 0.72
Batch: 420; loss: 0.99; acc: 0.75
Batch: 440; loss: 0.99; acc: 0.78
Batch: 460; loss: 0.99; acc: 0.81
Batch: 480; loss: 1.12; acc: 0.75
Batch: 500; loss: 1.08; acc: 0.69
Batch: 520; loss: 1.04; acc: 0.72
Batch: 540; loss: 1.17; acc: 0.7
Batch: 560; loss: 1.17; acc: 0.66
Batch: 580; loss: 1.16; acc: 0.67
Batch: 600; loss: 1.13; acc: 0.69
Batch: 620; loss: 1.26; acc: 0.67
Batch: 640; loss: 1.13; acc: 0.67
Batch: 660; loss: 1.16; acc: 0.64
Batch: 680; loss: 1.08; acc: 0.75
Batch: 700; loss: 1.19; acc: 0.72
Batch: 720; loss: 0.96; acc: 0.83
Batch: 740; loss: 1.14; acc: 0.69
Batch: 760; loss: 0.93; acc: 0.83
Batch: 780; loss: 1.07; acc: 0.75
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.00018132885452359915
0.00017528397438582033
Batch: 0; loss: 1.06; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 1.04; acc: 0.77
Batch: 80; loss: 0.85; acc: 0.86
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.72
Batch: 140; loss: 1.01; acc: 0.72
Val Epoch over. val_loss: 1.0261370289097926; val_accuracy: 0.742734872611465 

The current subspace-distance is: 0.00017528397438582033 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.04; acc: 0.78
Batch: 20; loss: 1.09; acc: 0.72
Batch: 40; loss: 1.12; acc: 0.72
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 1.16; acc: 0.61
Batch: 100; loss: 1.02; acc: 0.7
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 1.05; acc: 0.75
Batch: 160; loss: 1.15; acc: 0.7
Batch: 180; loss: 1.28; acc: 0.64
Batch: 200; loss: 1.14; acc: 0.69
Batch: 220; loss: 1.01; acc: 0.75
Batch: 240; loss: 1.01; acc: 0.77
Batch: 260; loss: 1.05; acc: 0.75
Batch: 280; loss: 1.2; acc: 0.61
Batch: 300; loss: 0.96; acc: 0.75
Batch: 320; loss: 1.16; acc: 0.66
Batch: 340; loss: 1.17; acc: 0.69
Batch: 360; loss: 1.26; acc: 0.56
Batch: 380; loss: 1.04; acc: 0.72
Batch: 400; loss: 1.13; acc: 0.67
Batch: 420; loss: 1.07; acc: 0.7
Batch: 440; loss: 1.02; acc: 0.77
Batch: 460; loss: 1.13; acc: 0.69
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 1.2; acc: 0.59
Batch: 520; loss: 1.03; acc: 0.77
Batch: 540; loss: 1.28; acc: 0.59
Batch: 560; loss: 0.97; acc: 0.7
Batch: 580; loss: 1.09; acc: 0.72
Batch: 600; loss: 0.92; acc: 0.78
Batch: 620; loss: 1.03; acc: 0.73
Batch: 640; loss: 1.19; acc: 0.64
Batch: 660; loss: 1.2; acc: 0.62
Batch: 680; loss: 1.11; acc: 0.67
Batch: 700; loss: 0.92; acc: 0.77
Batch: 720; loss: 1.04; acc: 0.69
Batch: 740; loss: 1.23; acc: 0.61
Batch: 760; loss: 1.0; acc: 0.72
Batch: 780; loss: 1.27; acc: 0.64
Train Epoch over. train_loss: 1.09; train_accuracy: 0.7 

0.00018452150106895715
0.00018018623813986778
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.86
Batch: 100; loss: 1.07; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.72
Batch: 140; loss: 1.02; acc: 0.7
Val Epoch over. val_loss: 1.0242336513889823; val_accuracy: 0.7409434713375797 

The current subspace-distance is: 0.00018018623813986778 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.01; acc: 0.77
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 0.88; acc: 0.8
Batch: 80; loss: 1.23; acc: 0.59
Batch: 100; loss: 0.98; acc: 0.81
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.75
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 1.05; acc: 0.72
Batch: 200; loss: 1.05; acc: 0.73
Batch: 220; loss: 0.96; acc: 0.73
Batch: 240; loss: 1.0; acc: 0.73
Batch: 260; loss: 1.09; acc: 0.7
Batch: 280; loss: 1.08; acc: 0.72
Batch: 300; loss: 1.0; acc: 0.81
Batch: 320; loss: 1.04; acc: 0.77
Batch: 340; loss: 1.01; acc: 0.72
Batch: 360; loss: 1.18; acc: 0.66
Batch: 380; loss: 1.09; acc: 0.77
Batch: 400; loss: 1.05; acc: 0.73
Batch: 420; loss: 1.01; acc: 0.72
Batch: 440; loss: 1.18; acc: 0.64
Batch: 460; loss: 0.96; acc: 0.73
Batch: 480; loss: 1.14; acc: 0.62
Batch: 500; loss: 1.15; acc: 0.67
Batch: 520; loss: 1.06; acc: 0.7
Batch: 540; loss: 0.96; acc: 0.81
Batch: 560; loss: 0.94; acc: 0.78
Batch: 580; loss: 1.09; acc: 0.69
Batch: 600; loss: 1.08; acc: 0.69
Batch: 620; loss: 1.18; acc: 0.62
Batch: 640; loss: 1.12; acc: 0.7
Batch: 660; loss: 1.1; acc: 0.67
Batch: 680; loss: 1.24; acc: 0.59
Batch: 700; loss: 1.04; acc: 0.69
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 1.14; acc: 0.75
Batch: 760; loss: 0.93; acc: 0.78
Batch: 780; loss: 1.1; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.71 

0.00018681358778849244
0.000175628942088224
Batch: 0; loss: 1.05; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 0.84; acc: 0.86
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.72
Batch: 140; loss: 1.02; acc: 0.7
Val Epoch over. val_loss: 1.0222739949347868; val_accuracy: 0.7453224522292994 

The current subspace-distance is: 0.000175628942088224 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.07; acc: 0.78
Batch: 40; loss: 0.96; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 1.01; acc: 0.69
Batch: 120; loss: 0.97; acc: 0.89
Batch: 140; loss: 1.01; acc: 0.7
Batch: 160; loss: 1.22; acc: 0.64
Batch: 180; loss: 1.13; acc: 0.7
Batch: 200; loss: 1.02; acc: 0.73
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.01; acc: 0.72
Batch: 260; loss: 1.1; acc: 0.77
Batch: 280; loss: 0.94; acc: 0.8
Batch: 300; loss: 1.12; acc: 0.7
Batch: 320; loss: 1.04; acc: 0.7
Batch: 340; loss: 1.07; acc: 0.73
Batch: 360; loss: 1.0; acc: 0.73
Batch: 380; loss: 0.9; acc: 0.8
Batch: 400; loss: 0.98; acc: 0.81
Batch: 420; loss: 1.19; acc: 0.69
Batch: 440; loss: 0.93; acc: 0.77
Batch: 460; loss: 1.02; acc: 0.72
Batch: 480; loss: 1.21; acc: 0.67
Batch: 500; loss: 1.19; acc: 0.66
Batch: 520; loss: 1.17; acc: 0.69
Batch: 540; loss: 0.95; acc: 0.78
Batch: 560; loss: 0.8; acc: 0.89
Batch: 580; loss: 0.91; acc: 0.81
Batch: 600; loss: 1.01; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.72
Batch: 640; loss: 0.85; acc: 0.91
Batch: 660; loss: 1.14; acc: 0.67
Batch: 680; loss: 1.11; acc: 0.7
Batch: 700; loss: 0.99; acc: 0.75
Batch: 720; loss: 1.2; acc: 0.67
Batch: 740; loss: 1.09; acc: 0.75
Batch: 760; loss: 1.04; acc: 0.72
Batch: 780; loss: 1.08; acc: 0.64
Train Epoch over. train_loss: 1.08; train_accuracy: 0.71 

0.0001861406199168414
0.0001812180271372199
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.86
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 1.01; acc: 0.69
Val Epoch over. val_loss: 1.0200196098370158; val_accuracy: 0.7453224522292994 

The current subspace-distance is: 0.0001812180271372199 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 1.02; acc: 0.75
Batch: 40; loss: 0.91; acc: 0.77
Batch: 60; loss: 0.87; acc: 0.83
Batch: 80; loss: 1.2; acc: 0.62
Batch: 100; loss: 1.13; acc: 0.69
Batch: 120; loss: 1.16; acc: 0.7
Batch: 140; loss: 1.13; acc: 0.66
Batch: 160; loss: 1.2; acc: 0.66
Batch: 180; loss: 1.07; acc: 0.69
Batch: 200; loss: 0.92; acc: 0.81
Batch: 220; loss: 0.94; acc: 0.75
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.17; acc: 0.67
Batch: 280; loss: 1.05; acc: 0.73
Batch: 300; loss: 0.93; acc: 0.78
Batch: 320; loss: 0.98; acc: 0.75
Batch: 340; loss: 1.06; acc: 0.77
Batch: 360; loss: 1.07; acc: 0.72
Batch: 380; loss: 1.11; acc: 0.62
Batch: 400; loss: 1.06; acc: 0.7
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 1.05; acc: 0.75
Batch: 460; loss: 1.16; acc: 0.72
Batch: 480; loss: 1.13; acc: 0.62
Batch: 500; loss: 1.05; acc: 0.7
Batch: 520; loss: 1.05; acc: 0.7
Batch: 540; loss: 0.96; acc: 0.78
Batch: 560; loss: 1.01; acc: 0.66
Batch: 580; loss: 0.77; acc: 0.84
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 1.05; acc: 0.64
Batch: 640; loss: 1.2; acc: 0.59
Batch: 660; loss: 0.95; acc: 0.7
Batch: 680; loss: 1.07; acc: 0.69
Batch: 700; loss: 1.03; acc: 0.73
Batch: 720; loss: 0.92; acc: 0.77
Batch: 740; loss: 1.02; acc: 0.73
Batch: 760; loss: 0.98; acc: 0.78
Batch: 780; loss: 1.24; acc: 0.61
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.00018728959548752755
0.00018022091535385698
Batch: 0; loss: 1.06; acc: 0.73
Batch: 20; loss: 1.14; acc: 0.61
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 0.83; acc: 0.86
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 1.02; acc: 0.69
Val Epoch over. val_loss: 1.0199722571737448; val_accuracy: 0.7437300955414012 

The current subspace-distance is: 0.00018022091535385698 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.99; acc: 0.73
Batch: 20; loss: 0.94; acc: 0.81
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 1.08; acc: 0.69
Batch: 100; loss: 1.03; acc: 0.75
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 1.1; acc: 0.66
Batch: 160; loss: 1.06; acc: 0.69
Batch: 180; loss: 1.03; acc: 0.69
Batch: 200; loss: 1.31; acc: 0.66
Batch: 220; loss: 1.33; acc: 0.62
Batch: 240; loss: 1.08; acc: 0.67
Batch: 260; loss: 0.86; acc: 0.77
Batch: 280; loss: 1.03; acc: 0.67
Batch: 300; loss: 1.01; acc: 0.69
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.96; acc: 0.73
Batch: 360; loss: 1.0; acc: 0.77
Batch: 380; loss: 1.06; acc: 0.73
Batch: 400; loss: 1.1; acc: 0.67
Batch: 420; loss: 1.11; acc: 0.69
Batch: 440; loss: 1.09; acc: 0.77
Batch: 460; loss: 1.27; acc: 0.59
Batch: 480; loss: 1.05; acc: 0.72
Batch: 500; loss: 1.14; acc: 0.72
Batch: 520; loss: 1.1; acc: 0.7
Batch: 540; loss: 0.91; acc: 0.75
Batch: 560; loss: 1.18; acc: 0.59
Batch: 580; loss: 1.07; acc: 0.72
Batch: 600; loss: 1.12; acc: 0.66
Batch: 620; loss: 0.95; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.67
Batch: 660; loss: 1.25; acc: 0.66
Batch: 680; loss: 1.16; acc: 0.66
Batch: 700; loss: 1.22; acc: 0.67
Batch: 720; loss: 1.2; acc: 0.59
Batch: 740; loss: 1.04; acc: 0.7
Batch: 760; loss: 1.05; acc: 0.7
Batch: 780; loss: 1.13; acc: 0.72
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.0001933383900905028
0.0001853161957114935
Batch: 0; loss: 1.05; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.61
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 0.82; acc: 0.86
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.1; acc: 0.7
Batch: 140; loss: 1.0; acc: 0.69
Val Epoch over. val_loss: 1.0100327065795849; val_accuracy: 0.7414410828025477 

The current subspace-distance is: 0.0001853161957114935 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.64
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.77
Batch: 100; loss: 1.1; acc: 0.69
Batch: 120; loss: 0.94; acc: 0.78
Batch: 140; loss: 1.08; acc: 0.75
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 1.07; acc: 0.73
Batch: 200; loss: 1.06; acc: 0.67
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 1.08; acc: 0.7
Batch: 260; loss: 1.12; acc: 0.69
Batch: 280; loss: 1.14; acc: 0.69
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.12; acc: 0.67
Batch: 340; loss: 0.95; acc: 0.78
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 0.89; acc: 0.81
Batch: 400; loss: 1.04; acc: 0.78
Batch: 420; loss: 1.02; acc: 0.73
Batch: 440; loss: 1.18; acc: 0.59
Batch: 460; loss: 1.15; acc: 0.72
Batch: 480; loss: 0.97; acc: 0.75
Batch: 500; loss: 0.99; acc: 0.8
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.17; acc: 0.66
Batch: 560; loss: 1.05; acc: 0.73
Batch: 580; loss: 1.19; acc: 0.66
Batch: 600; loss: 1.24; acc: 0.62
Batch: 620; loss: 0.92; acc: 0.8
Batch: 640; loss: 1.08; acc: 0.73
Batch: 660; loss: 0.92; acc: 0.78
Batch: 680; loss: 1.34; acc: 0.55
Batch: 700; loss: 1.13; acc: 0.7
Batch: 720; loss: 1.17; acc: 0.66
Batch: 740; loss: 1.12; acc: 0.64
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.38; acc: 0.58
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.00019218219676986337
0.00018695362086873502
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.71; acc: 0.88
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 0.82; acc: 0.86
Batch: 100; loss: 1.05; acc: 0.78
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.69
Val Epoch over. val_loss: 1.0138616876997006; val_accuracy: 0.742734872611465 

The current subspace-distance is: 0.00018695362086873502 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 1.0; acc: 0.78
Batch: 40; loss: 1.04; acc: 0.73
Batch: 60; loss: 0.94; acc: 0.81
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 0.99; acc: 0.7
Batch: 160; loss: 1.0; acc: 0.7
Batch: 180; loss: 1.08; acc: 0.7
Batch: 200; loss: 1.01; acc: 0.73
Batch: 220; loss: 1.2; acc: 0.67
Batch: 240; loss: 1.05; acc: 0.7
Batch: 260; loss: 1.18; acc: 0.58
Batch: 280; loss: 1.11; acc: 0.7
Batch: 300; loss: 0.99; acc: 0.8
Batch: 320; loss: 0.98; acc: 0.78
Batch: 340; loss: 1.11; acc: 0.67
Batch: 360; loss: 1.09; acc: 0.7
Batch: 380; loss: 0.99; acc: 0.75
Batch: 400; loss: 1.05; acc: 0.64
Batch: 420; loss: 0.98; acc: 0.78
Batch: 440; loss: 1.05; acc: 0.67
Batch: 460; loss: 0.99; acc: 0.7
Batch: 480; loss: 1.08; acc: 0.77
Batch: 500; loss: 1.16; acc: 0.62
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.08; acc: 0.78
Batch: 560; loss: 1.09; acc: 0.73
Batch: 580; loss: 1.09; acc: 0.69
Batch: 600; loss: 1.02; acc: 0.67
Batch: 620; loss: 1.16; acc: 0.62
Batch: 640; loss: 1.12; acc: 0.61
Batch: 660; loss: 1.01; acc: 0.75
Batch: 680; loss: 1.06; acc: 0.67
Batch: 700; loss: 1.11; acc: 0.7
Batch: 720; loss: 1.08; acc: 0.7
Batch: 740; loss: 0.93; acc: 0.77
Batch: 760; loss: 1.26; acc: 0.62
Batch: 780; loss: 1.09; acc: 0.64
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.00019336772675160319
0.00018916558474302292
Batch: 0; loss: 1.03; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 0.81; acc: 0.86
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.69
Val Epoch over. val_loss: 0.9987337019792788; val_accuracy: 0.7464171974522293 

The current subspace-distance is: 0.00018916558474302292 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.75
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 1.2; acc: 0.64
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.16; acc: 0.66
Batch: 100; loss: 1.01; acc: 0.72
Batch: 120; loss: 1.04; acc: 0.7
Batch: 140; loss: 1.01; acc: 0.73
Batch: 160; loss: 1.0; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.75
Batch: 200; loss: 1.3; acc: 0.61
Batch: 220; loss: 1.12; acc: 0.69
Batch: 240; loss: 0.98; acc: 0.78
Batch: 260; loss: 1.1; acc: 0.75
Batch: 280; loss: 0.97; acc: 0.8
Batch: 300; loss: 1.09; acc: 0.64
Batch: 320; loss: 1.14; acc: 0.66
Batch: 340; loss: 1.05; acc: 0.67
Batch: 360; loss: 0.94; acc: 0.77
Batch: 380; loss: 1.33; acc: 0.62
Batch: 400; loss: 0.99; acc: 0.7
Batch: 420; loss: 1.1; acc: 0.7
Batch: 440; loss: 0.91; acc: 0.84
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 1.08; acc: 0.64
Batch: 500; loss: 1.08; acc: 0.67
Batch: 520; loss: 1.03; acc: 0.7
Batch: 540; loss: 1.06; acc: 0.72
Batch: 560; loss: 1.07; acc: 0.75
Batch: 580; loss: 1.16; acc: 0.62
Batch: 600; loss: 1.0; acc: 0.75
Batch: 620; loss: 1.12; acc: 0.66
Batch: 640; loss: 1.0; acc: 0.67
Batch: 660; loss: 1.08; acc: 0.72
Batch: 680; loss: 1.17; acc: 0.69
Batch: 700; loss: 1.09; acc: 0.75
Batch: 720; loss: 1.23; acc: 0.66
Batch: 740; loss: 1.12; acc: 0.66
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.06; acc: 0.73
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00019682438869494945
0.0001899680501082912
Batch: 0; loss: 1.05; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.71; acc: 0.91
Batch: 60; loss: 1.02; acc: 0.73
Batch: 80; loss: 0.81; acc: 0.86
Batch: 100; loss: 1.03; acc: 0.78
Batch: 120; loss: 1.12; acc: 0.7
Batch: 140; loss: 0.99; acc: 0.7
Val Epoch over. val_loss: 1.0058327374184968; val_accuracy: 0.7484076433121019 

The current subspace-distance is: 0.0001899680501082912 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.62
Batch: 20; loss: 1.2; acc: 0.62
Batch: 40; loss: 1.12; acc: 0.72
Batch: 60; loss: 1.09; acc: 0.64
Batch: 80; loss: 1.0; acc: 0.75
Batch: 100; loss: 1.13; acc: 0.69
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 1.15; acc: 0.67
Batch: 160; loss: 1.15; acc: 0.69
Batch: 180; loss: 1.15; acc: 0.62
Batch: 200; loss: 1.19; acc: 0.67
Batch: 220; loss: 1.16; acc: 0.59
Batch: 240; loss: 1.02; acc: 0.77
Batch: 260; loss: 0.98; acc: 0.77
Batch: 280; loss: 1.04; acc: 0.72
Batch: 300; loss: 1.03; acc: 0.7
Batch: 320; loss: 1.18; acc: 0.66
Batch: 340; loss: 1.06; acc: 0.7
Batch: 360; loss: 1.19; acc: 0.62
Batch: 380; loss: 1.15; acc: 0.7
Batch: 400; loss: 1.09; acc: 0.67
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.98; acc: 0.73
Batch: 480; loss: 1.26; acc: 0.61
Batch: 500; loss: 0.79; acc: 0.83
Batch: 520; loss: 1.04; acc: 0.72
Batch: 540; loss: 1.05; acc: 0.67
Batch: 560; loss: 1.08; acc: 0.73
Batch: 580; loss: 1.06; acc: 0.7
Batch: 600; loss: 0.95; acc: 0.77
Batch: 620; loss: 1.12; acc: 0.75
Batch: 640; loss: 0.91; acc: 0.81
Batch: 660; loss: 1.16; acc: 0.59
Batch: 680; loss: 1.04; acc: 0.7
Batch: 700; loss: 1.1; acc: 0.7
Batch: 720; loss: 1.08; acc: 0.67
Batch: 740; loss: 1.14; acc: 0.72
Batch: 760; loss: 0.96; acc: 0.72
Batch: 780; loss: 1.17; acc: 0.69
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00019532782607711852
0.00018743932014331222
Batch: 0; loss: 1.03; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.68; acc: 0.91
Batch: 60; loss: 1.0; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.86
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.67
Batch: 140; loss: 0.98; acc: 0.69
Val Epoch over. val_loss: 1.001235842704773; val_accuracy: 0.7424363057324841 

The current subspace-distance is: 0.00018743932014331222 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.22; acc: 0.61
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 0.99; acc: 0.73
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 1.1; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.77
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 1.16; acc: 0.69
Batch: 220; loss: 1.25; acc: 0.58
Batch: 240; loss: 1.05; acc: 0.73
Batch: 260; loss: 1.32; acc: 0.61
Batch: 280; loss: 1.1; acc: 0.67
Batch: 300; loss: 1.18; acc: 0.64
Batch: 320; loss: 1.05; acc: 0.72
Batch: 340; loss: 1.24; acc: 0.61
Batch: 360; loss: 0.89; acc: 0.8
Batch: 380; loss: 1.01; acc: 0.7
Batch: 400; loss: 1.06; acc: 0.69
Batch: 420; loss: 1.05; acc: 0.7
Batch: 440; loss: 1.11; acc: 0.72
Batch: 460; loss: 0.99; acc: 0.75
Batch: 480; loss: 1.02; acc: 0.73
Batch: 500; loss: 1.12; acc: 0.72
Batch: 520; loss: 1.19; acc: 0.59
Batch: 540; loss: 1.13; acc: 0.69
Batch: 560; loss: 1.03; acc: 0.75
Batch: 580; loss: 1.21; acc: 0.64
Batch: 600; loss: 0.97; acc: 0.78
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 0.92; acc: 0.81
Batch: 660; loss: 1.03; acc: 0.73
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 1.16; acc: 0.69
Batch: 720; loss: 1.03; acc: 0.69
Batch: 740; loss: 0.97; acc: 0.72
Batch: 760; loss: 0.99; acc: 0.67
Batch: 780; loss: 1.04; acc: 0.7
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00019712075300049037
0.00018847458704840392
Batch: 0; loss: 1.04; acc: 0.72
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 0.81; acc: 0.86
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.12; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.000466579084943; val_accuracy: 0.7446257961783439 

The current subspace-distance is: 0.00018847458704840392 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.64
Batch: 20; loss: 1.04; acc: 0.72
Batch: 40; loss: 0.94; acc: 0.73
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 1.09; acc: 0.78
Batch: 100; loss: 0.98; acc: 0.8
Batch: 120; loss: 1.11; acc: 0.62
Batch: 140; loss: 1.09; acc: 0.77
Batch: 160; loss: 1.1; acc: 0.7
Batch: 180; loss: 1.06; acc: 0.75
Batch: 200; loss: 0.96; acc: 0.72
Batch: 220; loss: 1.01; acc: 0.72
Batch: 240; loss: 0.98; acc: 0.75
Batch: 260; loss: 1.04; acc: 0.7
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 1.09; acc: 0.73
Batch: 320; loss: 1.04; acc: 0.67
Batch: 340; loss: 0.99; acc: 0.81
Batch: 360; loss: 1.21; acc: 0.58
Batch: 380; loss: 0.98; acc: 0.73
Batch: 400; loss: 1.04; acc: 0.77
Batch: 420; loss: 1.1; acc: 0.72
Batch: 440; loss: 1.04; acc: 0.77
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.25; acc: 0.62
Batch: 500; loss: 1.08; acc: 0.64
Batch: 520; loss: 1.1; acc: 0.62
Batch: 540; loss: 1.23; acc: 0.62
Batch: 560; loss: 1.04; acc: 0.73
Batch: 580; loss: 1.22; acc: 0.66
Batch: 600; loss: 1.04; acc: 0.69
Batch: 620; loss: 1.06; acc: 0.7
Batch: 640; loss: 0.92; acc: 0.73
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 0.98; acc: 0.7
Batch: 700; loss: 1.08; acc: 0.69
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.14; acc: 0.67
Batch: 760; loss: 1.01; acc: 0.72
Batch: 780; loss: 1.03; acc: 0.77
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00019667109881993383
0.00018921637092716992
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.69; acc: 0.91
Batch: 60; loss: 1.01; acc: 0.77
Batch: 80; loss: 0.8; acc: 0.86
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.7
Batch: 140; loss: 0.99; acc: 0.67
Val Epoch over. val_loss: 1.0025501759948245; val_accuracy: 0.7467157643312102 

The current subspace-distance is: 0.00018921637092716992 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 0.89; acc: 0.78
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 1.05; acc: 0.75
Batch: 160; loss: 0.92; acc: 0.77
Batch: 180; loss: 1.15; acc: 0.72
Batch: 200; loss: 0.92; acc: 0.83
Batch: 220; loss: 1.07; acc: 0.73
Batch: 240; loss: 1.14; acc: 0.67
Batch: 260; loss: 1.1; acc: 0.7
Batch: 280; loss: 1.1; acc: 0.64
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 0.95; acc: 0.75
Batch: 340; loss: 1.15; acc: 0.67
Batch: 360; loss: 1.13; acc: 0.75
Batch: 380; loss: 0.99; acc: 0.72
Batch: 400; loss: 1.12; acc: 0.62
Batch: 420; loss: 1.06; acc: 0.7
Batch: 440; loss: 0.94; acc: 0.75
Batch: 460; loss: 0.92; acc: 0.75
Batch: 480; loss: 1.01; acc: 0.72
Batch: 500; loss: 0.96; acc: 0.73
Batch: 520; loss: 1.13; acc: 0.59
Batch: 540; loss: 1.06; acc: 0.67
Batch: 560; loss: 1.14; acc: 0.75
Batch: 580; loss: 1.09; acc: 0.7
Batch: 600; loss: 1.09; acc: 0.7
Batch: 620; loss: 1.19; acc: 0.61
Batch: 640; loss: 1.13; acc: 0.62
Batch: 660; loss: 1.23; acc: 0.58
Batch: 680; loss: 1.06; acc: 0.75
Batch: 700; loss: 1.0; acc: 0.7
Batch: 720; loss: 1.05; acc: 0.75
Batch: 740; loss: 1.08; acc: 0.77
Batch: 760; loss: 1.02; acc: 0.66
Batch: 780; loss: 1.13; acc: 0.69
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00020005475380457938
0.0001894671586342156
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.64
Batch: 40; loss: 0.7; acc: 0.89
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 0.82; acc: 0.86
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.0049700968584436; val_accuracy: 0.7422372611464968 

The current subspace-distance is: 0.0001894671586342156 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.94; acc: 0.75
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.06; acc: 0.66
Batch: 140; loss: 0.94; acc: 0.73
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.55
Batch: 200; loss: 0.97; acc: 0.69
Batch: 220; loss: 0.97; acc: 0.7
Batch: 240; loss: 0.95; acc: 0.75
Batch: 260; loss: 0.96; acc: 0.75
Batch: 280; loss: 1.04; acc: 0.78
Batch: 300; loss: 1.01; acc: 0.67
Batch: 320; loss: 1.05; acc: 0.72
Batch: 340; loss: 1.09; acc: 0.66
Batch: 360; loss: 1.12; acc: 0.73
Batch: 380; loss: 1.0; acc: 0.75
Batch: 400; loss: 0.93; acc: 0.78
Batch: 420; loss: 1.05; acc: 0.72
Batch: 440; loss: 0.9; acc: 0.8
Batch: 460; loss: 1.04; acc: 0.73
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 1.16; acc: 0.67
Batch: 520; loss: 1.35; acc: 0.58
Batch: 540; loss: 0.98; acc: 0.75
Batch: 560; loss: 0.9; acc: 0.77
Batch: 580; loss: 1.09; acc: 0.72
Batch: 600; loss: 0.96; acc: 0.77
Batch: 620; loss: 1.06; acc: 0.69
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 0.95; acc: 0.72
Batch: 680; loss: 1.02; acc: 0.7
Batch: 700; loss: 1.11; acc: 0.7
Batch: 720; loss: 0.89; acc: 0.8
Batch: 740; loss: 1.19; acc: 0.56
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 1.13; acc: 0.59
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.0001951305166585371
0.00018992318655364215
Batch: 0; loss: 1.02; acc: 0.72
Batch: 20; loss: 1.11; acc: 0.61
Batch: 40; loss: 0.67; acc: 0.91
Batch: 60; loss: 1.0; acc: 0.77
Batch: 80; loss: 0.79; acc: 0.86
Batch: 100; loss: 1.02; acc: 0.8
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.96; acc: 0.7
Val Epoch over. val_loss: 0.9894966445151409; val_accuracy: 0.7492038216560509 

The current subspace-distance is: 0.00018992318655364215 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 1.19; acc: 0.62
Batch: 60; loss: 0.99; acc: 0.73
Batch: 80; loss: 1.02; acc: 0.72
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 1.14; acc: 0.73
Batch: 140; loss: 0.93; acc: 0.77
Batch: 160; loss: 1.0; acc: 0.77
Batch: 180; loss: 0.92; acc: 0.83
Batch: 200; loss: 0.97; acc: 0.7
Batch: 220; loss: 1.11; acc: 0.69
Batch: 240; loss: 1.23; acc: 0.64
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 0.99; acc: 0.7
Batch: 300; loss: 1.01; acc: 0.73
Batch: 320; loss: 1.23; acc: 0.56
Batch: 340; loss: 1.06; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.69
Batch: 380; loss: 1.02; acc: 0.77
Batch: 400; loss: 0.95; acc: 0.77
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 0.93; acc: 0.75
Batch: 460; loss: 0.96; acc: 0.75
Batch: 480; loss: 1.13; acc: 0.64
Batch: 500; loss: 0.98; acc: 0.75
Batch: 520; loss: 1.02; acc: 0.7
Batch: 540; loss: 1.06; acc: 0.72
Batch: 560; loss: 1.11; acc: 0.67
Batch: 580; loss: 1.19; acc: 0.67
Batch: 600; loss: 1.23; acc: 0.62
Batch: 620; loss: 1.09; acc: 0.73
Batch: 640; loss: 1.06; acc: 0.7
Batch: 660; loss: 1.14; acc: 0.62
Batch: 680; loss: 0.95; acc: 0.83
Batch: 700; loss: 1.1; acc: 0.72
Batch: 720; loss: 1.1; acc: 0.66
Batch: 740; loss: 0.97; acc: 0.77
Batch: 760; loss: 1.11; acc: 0.62
Batch: 780; loss: 1.11; acc: 0.7
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.0001987573632504791
0.00018944850307889283
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 1.0; acc: 0.75
Batch: 80; loss: 0.79; acc: 0.88
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.97; acc: 0.7
Val Epoch over. val_loss: 0.9981264604884348; val_accuracy: 0.7438296178343949 

The current subspace-distance is: 0.00018944850307889283 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 1.04; acc: 0.7
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 1.06; acc: 0.72
Batch: 160; loss: 1.11; acc: 0.66
Batch: 180; loss: 1.16; acc: 0.67
Batch: 200; loss: 1.03; acc: 0.75
Batch: 220; loss: 0.95; acc: 0.73
Batch: 240; loss: 1.02; acc: 0.75
Batch: 260; loss: 1.03; acc: 0.75
Batch: 280; loss: 1.27; acc: 0.59
Batch: 300; loss: 1.08; acc: 0.67
Batch: 320; loss: 0.89; acc: 0.77
Batch: 340; loss: 0.94; acc: 0.77
Batch: 360; loss: 1.12; acc: 0.72
Batch: 380; loss: 0.96; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.73
Batch: 420; loss: 0.91; acc: 0.81
Batch: 440; loss: 1.1; acc: 0.7
Batch: 460; loss: 1.02; acc: 0.69
Batch: 480; loss: 1.05; acc: 0.67
Batch: 500; loss: 1.33; acc: 0.64
Batch: 520; loss: 1.19; acc: 0.72
Batch: 540; loss: 1.02; acc: 0.73
Batch: 560; loss: 1.14; acc: 0.72
Batch: 580; loss: 1.08; acc: 0.72
Batch: 600; loss: 0.95; acc: 0.75
Batch: 620; loss: 1.26; acc: 0.61
Batch: 640; loss: 1.37; acc: 0.62
Batch: 660; loss: 1.13; acc: 0.67
Batch: 680; loss: 1.15; acc: 0.72
Batch: 700; loss: 1.04; acc: 0.73
Batch: 720; loss: 0.97; acc: 0.72
Batch: 740; loss: 1.08; acc: 0.69
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 1.19; acc: 0.64
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.00019862315093632787
0.00019190240709576756
Batch: 0; loss: 1.03; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 0.79; acc: 0.88
Batch: 100; loss: 1.04; acc: 0.78
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.66
Val Epoch over. val_loss: 1.0067396152551007; val_accuracy: 0.7380573248407644 

The current subspace-distance is: 0.00019190240709576756 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_5_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.25

The number of parameters is: 276579

The number of individual parameters is:

18
288
18
18
27
41796
27
27
54
125388
54
54
64
103680
64
64
4096
64
640
10
64
64

nonzero elements in E: 55315796
elements in E: 55315800
fraction nonzero: 0.99999992768793
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.11; acc: 0.28
Batch: 40; loss: 2.01; acc: 0.33
Batch: 60; loss: 1.97; acc: 0.31
Batch: 80; loss: 1.96; acc: 0.34
Batch: 100; loss: 1.82; acc: 0.45
Batch: 120; loss: 1.82; acc: 0.47
Batch: 140; loss: 1.77; acc: 0.56
Batch: 160; loss: 1.7; acc: 0.58
Batch: 180; loss: 1.74; acc: 0.53
Batch: 200; loss: 1.63; acc: 0.61
Batch: 220; loss: 1.68; acc: 0.55
Batch: 240; loss: 1.64; acc: 0.69
Batch: 260; loss: 1.67; acc: 0.58
Batch: 280; loss: 1.61; acc: 0.61
Batch: 300; loss: 1.58; acc: 0.66
Batch: 320; loss: 1.48; acc: 0.75
Batch: 340; loss: 1.4; acc: 0.77
Batch: 360; loss: 1.53; acc: 0.66
Batch: 380; loss: 1.49; acc: 0.67
Batch: 400; loss: 1.46; acc: 0.67
Batch: 420; loss: 1.38; acc: 0.73
Batch: 440; loss: 1.42; acc: 0.69
Batch: 460; loss: 1.55; acc: 0.66
Batch: 480; loss: 1.43; acc: 0.62
Batch: 500; loss: 1.3; acc: 0.77
Batch: 520; loss: 1.27; acc: 0.78
Batch: 540; loss: 1.28; acc: 0.8
Batch: 560; loss: 1.34; acc: 0.72
Batch: 580; loss: 1.3; acc: 0.75
Batch: 600; loss: 1.3; acc: 0.75
Batch: 620; loss: 1.33; acc: 0.69
Batch: 640; loss: 1.3; acc: 0.75
Batch: 660; loss: 1.34; acc: 0.72
Batch: 680; loss: 1.22; acc: 0.8
Batch: 700; loss: 1.15; acc: 0.77
Batch: 720; loss: 1.13; acc: 0.92
Batch: 740; loss: 1.21; acc: 0.77
Batch: 760; loss: 1.25; acc: 0.73
Batch: 780; loss: 1.28; acc: 0.72
Train Epoch over. train_loss: 1.53; train_accuracy: 0.62 

6.568308162968606e-05
6.129874964244664e-05
Batch: 0; loss: 1.2; acc: 0.8
Batch: 20; loss: 1.27; acc: 0.67
Batch: 40; loss: 0.98; acc: 0.88
Batch: 60; loss: 1.16; acc: 0.8
Batch: 80; loss: 1.09; acc: 0.81
Batch: 100; loss: 1.15; acc: 0.75
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 1.03; acc: 0.83
Val Epoch over. val_loss: 1.1688860749742787; val_accuracy: 0.7693073248407644 

The current subspace-distance is: 6.129874964244664e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.75
Batch: 20; loss: 1.16; acc: 0.83
Batch: 40; loss: 1.11; acc: 0.83
Batch: 60; loss: 1.17; acc: 0.83
Batch: 80; loss: 1.11; acc: 0.77
Batch: 100; loss: 1.23; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.78
Batch: 140; loss: 1.2; acc: 0.72
Batch: 160; loss: 1.11; acc: 0.86
Batch: 180; loss: 1.15; acc: 0.8
Batch: 200; loss: 1.23; acc: 0.72
Batch: 220; loss: 1.18; acc: 0.77
Batch: 240; loss: 1.23; acc: 0.73
Batch: 260; loss: 1.21; acc: 0.7
Batch: 280; loss: 1.0; acc: 0.84
Batch: 300; loss: 1.16; acc: 0.78
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.2; acc: 0.78
Batch: 360; loss: 1.05; acc: 0.83
Batch: 380; loss: 1.0; acc: 0.86
Batch: 400; loss: 1.04; acc: 0.8
Batch: 420; loss: 1.0; acc: 0.86
Batch: 440; loss: 1.07; acc: 0.78
Batch: 460; loss: 1.3; acc: 0.61
Batch: 480; loss: 1.07; acc: 0.81
Batch: 500; loss: 1.15; acc: 0.77
Batch: 520; loss: 1.14; acc: 0.72
Batch: 540; loss: 1.11; acc: 0.75
Batch: 560; loss: 1.18; acc: 0.73
Batch: 580; loss: 1.17; acc: 0.67
Batch: 600; loss: 1.17; acc: 0.75
Batch: 620; loss: 1.15; acc: 0.72
Batch: 640; loss: 1.12; acc: 0.81
Batch: 660; loss: 1.08; acc: 0.78
Batch: 680; loss: 1.02; acc: 0.86
Batch: 700; loss: 1.19; acc: 0.72
Batch: 720; loss: 1.09; acc: 0.81
Batch: 740; loss: 1.0; acc: 0.8
Batch: 760; loss: 1.1; acc: 0.78
Batch: 780; loss: 1.06; acc: 0.81
Train Epoch over. train_loss: 1.14; train_accuracy: 0.76 

8.679652819409966e-05
8.198559226002544e-05
Batch: 0; loss: 1.05; acc: 0.86
Batch: 20; loss: 1.12; acc: 0.77
Batch: 40; loss: 0.84; acc: 0.91
Batch: 60; loss: 0.99; acc: 0.84
Batch: 80; loss: 0.95; acc: 0.89
Batch: 100; loss: 0.99; acc: 0.86
Batch: 120; loss: 1.12; acc: 0.73
Batch: 140; loss: 0.87; acc: 0.86
Val Epoch over. val_loss: 1.0335868783057875; val_accuracy: 0.802547770700637 

The current subspace-distance is: 8.198559226002544e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.09; acc: 0.8
Batch: 40; loss: 0.94; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.81
Batch: 80; loss: 1.04; acc: 0.83
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.06; acc: 0.84
Batch: 140; loss: 1.1; acc: 0.78
Batch: 160; loss: 1.1; acc: 0.78
Batch: 180; loss: 0.93; acc: 0.86
Batch: 200; loss: 1.1; acc: 0.77
Batch: 220; loss: 1.09; acc: 0.81
Batch: 240; loss: 1.1; acc: 0.77
Batch: 260; loss: 1.05; acc: 0.78
Batch: 280; loss: 0.94; acc: 0.86
Batch: 300; loss: 0.84; acc: 0.88
Batch: 320; loss: 1.27; acc: 0.7
Batch: 340; loss: 1.05; acc: 0.8
Batch: 360; loss: 0.88; acc: 0.91
Batch: 380; loss: 1.01; acc: 0.8
Batch: 400; loss: 1.02; acc: 0.8
Batch: 420; loss: 1.1; acc: 0.77
Batch: 440; loss: 0.95; acc: 0.81
Batch: 460; loss: 1.05; acc: 0.8
Batch: 480; loss: 0.94; acc: 0.86
Batch: 500; loss: 0.96; acc: 0.81
Batch: 520; loss: 1.11; acc: 0.77
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 1.14; acc: 0.7
Batch: 580; loss: 0.99; acc: 0.83
Batch: 600; loss: 1.0; acc: 0.8
Batch: 620; loss: 0.88; acc: 0.84
Batch: 640; loss: 0.99; acc: 0.81
Batch: 660; loss: 1.09; acc: 0.83
Batch: 680; loss: 0.99; acc: 0.83
Batch: 700; loss: 0.86; acc: 0.84
Batch: 720; loss: 0.97; acc: 0.81
Batch: 740; loss: 1.09; acc: 0.7
Batch: 760; loss: 1.1; acc: 0.7
Batch: 780; loss: 0.96; acc: 0.75
Train Epoch over. train_loss: 1.03; train_accuracy: 0.79 

0.0001040794377331622
9.886221960186958e-05
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.91
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.87; acc: 0.89
Batch: 100; loss: 0.86; acc: 0.89
Batch: 120; loss: 1.02; acc: 0.77
Batch: 140; loss: 0.77; acc: 0.89
Val Epoch over. val_loss: 0.9417163000744619; val_accuracy: 0.8122014331210191 

The current subspace-distance is: 9.886221960186958e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 0.98; acc: 0.81
Batch: 40; loss: 0.99; acc: 0.81
Batch: 60; loss: 0.94; acc: 0.8
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 0.84; acc: 0.89
Batch: 120; loss: 0.89; acc: 0.84
Batch: 140; loss: 0.91; acc: 0.78
Batch: 160; loss: 1.0; acc: 0.75
Batch: 180; loss: 0.84; acc: 0.83
Batch: 200; loss: 1.09; acc: 0.73
Batch: 220; loss: 1.11; acc: 0.72
Batch: 240; loss: 1.03; acc: 0.77
Batch: 260; loss: 0.92; acc: 0.88
Batch: 280; loss: 0.99; acc: 0.77
Batch: 300; loss: 0.8; acc: 0.84
Batch: 320; loss: 0.97; acc: 0.78
Batch: 340; loss: 0.88; acc: 0.84
Batch: 360; loss: 0.92; acc: 0.81
Batch: 380; loss: 0.89; acc: 0.8
Batch: 400; loss: 0.9; acc: 0.84
Batch: 420; loss: 0.9; acc: 0.84
Batch: 440; loss: 0.8; acc: 0.89
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 0.94; acc: 0.86
Batch: 500; loss: 0.97; acc: 0.78
Batch: 520; loss: 0.99; acc: 0.8
Batch: 540; loss: 0.91; acc: 0.81
Batch: 560; loss: 1.0; acc: 0.75
Batch: 580; loss: 0.96; acc: 0.75
Batch: 600; loss: 0.92; acc: 0.8
Batch: 620; loss: 1.03; acc: 0.78
Batch: 640; loss: 1.12; acc: 0.73
Batch: 660; loss: 0.92; acc: 0.81
Batch: 680; loss: 0.93; acc: 0.81
Batch: 700; loss: 0.83; acc: 0.83
Batch: 720; loss: 1.0; acc: 0.73
Batch: 740; loss: 0.98; acc: 0.81
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 1.02; acc: 0.7
Train Epoch over. train_loss: 0.96; train_accuracy: 0.8 

0.00011949139297939837
0.0001134222766268067
Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 0.66; acc: 0.89
Batch: 60; loss: 0.82; acc: 0.89
Batch: 80; loss: 0.8; acc: 0.89
Batch: 100; loss: 0.79; acc: 0.89
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.89
Val Epoch over. val_loss: 0.8749779645045093; val_accuracy: 0.826234076433121 

The current subspace-distance is: 0.0001134222766268067 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.83
Batch: 20; loss: 0.9; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.81
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.81
Batch: 140; loss: 1.11; acc: 0.72
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.99; acc: 0.73
Batch: 200; loss: 0.87; acc: 0.78
Batch: 220; loss: 0.81; acc: 0.84
Batch: 240; loss: 0.88; acc: 0.81
Batch: 260; loss: 0.89; acc: 0.81
Batch: 280; loss: 0.93; acc: 0.81
Batch: 300; loss: 0.8; acc: 0.88
Batch: 320; loss: 0.91; acc: 0.83
Batch: 340; loss: 0.87; acc: 0.81
Batch: 360; loss: 0.85; acc: 0.8
Batch: 380; loss: 0.94; acc: 0.78
Batch: 400; loss: 0.94; acc: 0.81
Batch: 420; loss: 0.8; acc: 0.84
Batch: 440; loss: 1.0; acc: 0.78
Batch: 460; loss: 1.01; acc: 0.75
Batch: 480; loss: 0.99; acc: 0.75
Batch: 500; loss: 0.69; acc: 0.92
Batch: 520; loss: 1.02; acc: 0.73
Batch: 540; loss: 0.95; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.81
Batch: 580; loss: 0.86; acc: 0.84
Batch: 600; loss: 0.86; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.83
Batch: 640; loss: 0.88; acc: 0.81
Batch: 660; loss: 0.79; acc: 0.88
Batch: 680; loss: 0.97; acc: 0.75
Batch: 700; loss: 0.86; acc: 0.81
Batch: 720; loss: 0.84; acc: 0.86
Batch: 740; loss: 0.83; acc: 0.83
Batch: 760; loss: 0.86; acc: 0.86
Batch: 780; loss: 0.85; acc: 0.81
Train Epoch over. train_loss: 0.9; train_accuracy: 0.81 

0.00013203528942540288
0.00012622849317267537
Batch: 0; loss: 0.85; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.6; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.89
Batch: 80; loss: 0.73; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.91
Batch: 120; loss: 0.92; acc: 0.8
Batch: 140; loss: 0.67; acc: 0.89
Val Epoch over. val_loss: 0.8120858282040638; val_accuracy: 0.8348925159235668 

The current subspace-distance is: 0.00012622849317267537 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.77
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 0.94; acc: 0.77
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 0.98; acc: 0.77
Batch: 120; loss: 0.78; acc: 0.84
Batch: 140; loss: 0.91; acc: 0.81
Batch: 160; loss: 0.81; acc: 0.81
Batch: 180; loss: 0.84; acc: 0.86
Batch: 200; loss: 0.91; acc: 0.81
Batch: 220; loss: 0.81; acc: 0.83
Batch: 240; loss: 0.91; acc: 0.8
Batch: 260; loss: 0.99; acc: 0.67
Batch: 280; loss: 0.95; acc: 0.75
Batch: 300; loss: 0.92; acc: 0.78
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.71; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.91
Batch: 380; loss: 0.93; acc: 0.78
Batch: 400; loss: 0.88; acc: 0.8
Batch: 420; loss: 0.97; acc: 0.75
Batch: 440; loss: 0.86; acc: 0.78
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 0.89; acc: 0.8
Batch: 500; loss: 0.86; acc: 0.84
Batch: 520; loss: 0.97; acc: 0.77
Batch: 540; loss: 0.78; acc: 0.88
Batch: 560; loss: 0.79; acc: 0.86
Batch: 580; loss: 0.94; acc: 0.75
Batch: 600; loss: 0.81; acc: 0.83
Batch: 620; loss: 0.79; acc: 0.86
Batch: 640; loss: 0.75; acc: 0.86
Batch: 660; loss: 0.8; acc: 0.88
Batch: 680; loss: 0.82; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.89
Batch: 720; loss: 0.84; acc: 0.81
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.81; acc: 0.8
Batch: 780; loss: 0.85; acc: 0.81
Train Epoch over. train_loss: 0.85; train_accuracy: 0.82 

0.00014304871729109436
0.0001379773602820933
Batch: 0; loss: 0.79; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.7
Batch: 40; loss: 0.57; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.7; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.91
Val Epoch over. val_loss: 0.767089724161063; val_accuracy: 0.8434514331210191 

The current subspace-distance is: 0.0001379773602820933 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.84
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 0.91; acc: 0.84
Batch: 80; loss: 0.83; acc: 0.84
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.77; acc: 0.84
Batch: 160; loss: 0.81; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.86
Batch: 200; loss: 0.85; acc: 0.81
Batch: 220; loss: 0.75; acc: 0.86
Batch: 240; loss: 0.87; acc: 0.81
Batch: 260; loss: 0.86; acc: 0.81
Batch: 280; loss: 0.69; acc: 0.89
Batch: 300; loss: 0.99; acc: 0.75
Batch: 320; loss: 0.74; acc: 0.86
Batch: 340; loss: 0.78; acc: 0.88
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.87; acc: 0.78
Batch: 400; loss: 0.98; acc: 0.77
Batch: 420; loss: 0.84; acc: 0.8
Batch: 440; loss: 1.0; acc: 0.72
Batch: 460; loss: 0.88; acc: 0.77
Batch: 480; loss: 0.77; acc: 0.81
Batch: 500; loss: 0.76; acc: 0.86
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 0.79; acc: 0.84
Batch: 560; loss: 0.82; acc: 0.78
Batch: 580; loss: 0.78; acc: 0.86
Batch: 600; loss: 0.84; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.89
Batch: 640; loss: 0.81; acc: 0.89
Batch: 660; loss: 0.7; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.79; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.86
Batch: 740; loss: 0.76; acc: 0.88
Batch: 760; loss: 0.73; acc: 0.84
Batch: 780; loss: 0.88; acc: 0.81
Train Epoch over. train_loss: 0.81; train_accuracy: 0.82 

0.00015361601253971457
0.00014772760914638638
Batch: 0; loss: 0.76; acc: 0.88
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.55; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.91
Val Epoch over. val_loss: 0.7226093796787748; val_accuracy: 0.8532046178343949 

The current subspace-distance is: 0.00014772760914638638 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.66
Batch: 20; loss: 0.77; acc: 0.84
Batch: 40; loss: 0.66; acc: 0.91
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.78; acc: 0.83
Batch: 160; loss: 0.84; acc: 0.78
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 0.7; acc: 0.88
Batch: 220; loss: 0.9; acc: 0.8
Batch: 240; loss: 0.82; acc: 0.83
Batch: 260; loss: 0.8; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.91
Batch: 300; loss: 0.76; acc: 0.83
Batch: 320; loss: 0.93; acc: 0.67
Batch: 340; loss: 0.85; acc: 0.78
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.7; acc: 0.81
Batch: 400; loss: 0.76; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.89
Batch: 440; loss: 0.83; acc: 0.8
Batch: 460; loss: 0.8; acc: 0.78
Batch: 480; loss: 0.79; acc: 0.78
Batch: 500; loss: 0.94; acc: 0.77
Batch: 520; loss: 0.83; acc: 0.81
Batch: 540; loss: 0.85; acc: 0.8
Batch: 560; loss: 0.97; acc: 0.7
Batch: 580; loss: 0.93; acc: 0.73
Batch: 600; loss: 0.59; acc: 0.94
Batch: 620; loss: 0.66; acc: 0.92
Batch: 640; loss: 0.83; acc: 0.8
Batch: 660; loss: 0.76; acc: 0.89
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.72; acc: 0.84
Batch: 740; loss: 0.83; acc: 0.77
Batch: 760; loss: 0.79; acc: 0.73
Batch: 780; loss: 0.73; acc: 0.88
Train Epoch over. train_loss: 0.77; train_accuracy: 0.83 

0.00016516688629053533
0.00016030968981795013
Batch: 0; loss: 0.75; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.52; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6896424741502021; val_accuracy: 0.8586783439490446 

The current subspace-distance is: 0.00016030968981795013 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.95
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.82; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.75; acc: 0.84
Batch: 160; loss: 0.83; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.75; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.96; acc: 0.67
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.73; acc: 0.78
Batch: 300; loss: 0.72; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.74; acc: 0.88
Batch: 380; loss: 0.78; acc: 0.81
Batch: 400; loss: 0.93; acc: 0.8
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 0.79; acc: 0.78
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.64; acc: 0.92
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.81; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.72; acc: 0.88
Batch: 620; loss: 0.77; acc: 0.73
Batch: 640; loss: 0.65; acc: 0.86
Batch: 660; loss: 0.76; acc: 0.81
Batch: 680; loss: 0.7; acc: 0.84
Batch: 700; loss: 0.7; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.81
Batch: 740; loss: 0.88; acc: 0.78
Batch: 760; loss: 0.72; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.84 

0.00017454700719099492
0.00016815902199596167
Batch: 0; loss: 0.73; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.6662396200143608; val_accuracy: 0.8627587579617835 

The current subspace-distance is: 0.00016815902199596167 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.95
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.89
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.52; acc: 0.94
Batch: 200; loss: 0.64; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.79; acc: 0.83
Batch: 260; loss: 0.77; acc: 0.88
Batch: 280; loss: 0.67; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.67; acc: 0.91
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.63; acc: 0.92
Batch: 400; loss: 0.67; acc: 0.8
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.76; acc: 0.78
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.7; acc: 0.86
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.73; acc: 0.88
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.58; acc: 0.92
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.72; acc: 0.81
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.72; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.0001878717157524079
0.00018060878210235387
Batch: 0; loss: 0.72; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.75
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.94
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6474532874146844; val_accuracy: 0.8624601910828026 

The current subspace-distance is: 0.00018060878210235387 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.92; acc: 0.7
Batch: 160; loss: 0.75; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.65; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.91
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.66; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.86
Batch: 380; loss: 0.77; acc: 0.8
Batch: 400; loss: 0.55; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.97
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.94
Batch: 540; loss: 0.72; acc: 0.77
Batch: 560; loss: 0.7; acc: 0.88
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.89
Batch: 640; loss: 0.71; acc: 0.78
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.8; acc: 0.78
Batch: 700; loss: 0.72; acc: 0.81
Batch: 720; loss: 0.8; acc: 0.8
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.85 

0.00018815397925209254
0.00018214739975519478
Batch: 0; loss: 0.71; acc: 0.88
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.94
Val Epoch over. val_loss: 0.6273576288845888; val_accuracy: 0.8663415605095541 

The current subspace-distance is: 0.00018214739975519478 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.54; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.95
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.81; acc: 0.77
Batch: 180; loss: 0.65; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.92
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.91
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.91
Batch: 320; loss: 0.94; acc: 0.75
Batch: 340; loss: 0.56; acc: 0.94
Batch: 360; loss: 0.79; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.76; acc: 0.77
Batch: 420; loss: 0.89; acc: 0.77
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.63; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.73; acc: 0.75
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.8; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.71; acc: 0.88
Batch: 760; loss: 0.65; acc: 0.89
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.85 

0.00019282787980046123
0.00018386349256616086
Batch: 0; loss: 0.71; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.95
Val Epoch over. val_loss: 0.6279038221213469; val_accuracy: 0.8683320063694268 

The current subspace-distance is: 0.00018386349256616086 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.84
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.76; acc: 0.77
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.75; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.65; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.97
Batch: 380; loss: 0.56; acc: 0.92
Batch: 400; loss: 0.62; acc: 0.89
Batch: 420; loss: 0.78; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.92
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.65; acc: 0.89
Batch: 540; loss: 0.82; acc: 0.81
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.68; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.63; acc: 0.89
Batch: 640; loss: 0.57; acc: 0.94
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.69; acc: 0.84
Batch: 700; loss: 0.78; acc: 0.77
Batch: 720; loss: 0.56; acc: 0.92
Batch: 740; loss: 0.75; acc: 0.78
Batch: 760; loss: 0.64; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.85 

0.00019363003957550973
0.00018754518532659858
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.6267207418657412; val_accuracy: 0.8668391719745223 

The current subspace-distance is: 0.00018754518532659858 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.94
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.74; acc: 0.8
Batch: 180; loss: 0.64; acc: 0.89
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.63; acc: 0.91
Batch: 240; loss: 0.82; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.71; acc: 0.86
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.69; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.91
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.91
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.63; acc: 0.91
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.89
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.81; acc: 0.83
Train Epoch over. train_loss: 0.67; train_accuracy: 0.85 

0.0001952383026946336
0.00018859289411921054
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.43; acc: 0.95
Val Epoch over. val_loss: 0.6147782544421542; val_accuracy: 0.868531050955414 

The current subspace-distance is: 0.00018859289411921054 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.89
Batch: 40; loss: 0.83; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.79; acc: 0.78
Batch: 180; loss: 0.65; acc: 0.89
Batch: 200; loss: 0.58; acc: 0.91
Batch: 220; loss: 0.64; acc: 0.89
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.49; acc: 0.92
Batch: 380; loss: 0.73; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.88
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.88
Batch: 480; loss: 0.7; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.92
Batch: 520; loss: 0.72; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.74; acc: 0.8
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.72; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.66; train_accuracy: 0.85 

0.0002001025277422741
0.0001925747055793181
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.6080050512104277; val_accuracy: 0.8706210191082803 

The current subspace-distance is: 0.0001925747055793181 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.78; acc: 0.81
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.69; acc: 0.89
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.89
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.91
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.65; acc: 0.89
Batch: 400; loss: 0.65; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.83
Batch: 460; loss: 0.72; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.91
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.6; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.92
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.95
Batch: 700; loss: 0.67; acc: 0.88
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.66; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.65; train_accuracy: 0.85 

0.00020105554722249508
0.0001948743883986026
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.604631131431859; val_accuracy: 0.870421974522293 

The current subspace-distance is: 0.0001948743883986026 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.94
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.82; acc: 0.78
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.76; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.88
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.91
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.55; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.68; acc: 0.8
Batch: 620; loss: 0.77; acc: 0.83
Batch: 640; loss: 0.76; acc: 0.77
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.72; acc: 0.84
Batch: 720; loss: 0.59; acc: 0.91
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.95
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.65; train_accuracy: 0.85 

0.00020532711641862988
0.0001974709884962067
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.97
Val Epoch over. val_loss: 0.5997825532582155; val_accuracy: 0.8735071656050956 

The current subspace-distance is: 0.0001974709884962067 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.88
Batch: 280; loss: 0.66; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.77
Batch: 360; loss: 0.53; acc: 0.94
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.91
Batch: 480; loss: 0.86; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.94; acc: 0.75
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.89
Batch: 660; loss: 0.63; acc: 0.89
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.85 

0.00020349727128632367
0.0001976155472220853
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.5959317450690421; val_accuracy: 0.8706210191082803 

The current subspace-distance is: 0.0001976155472220853 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.52; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.89; acc: 0.77
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.74; acc: 0.8
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.65; acc: 0.88
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.85; acc: 0.77
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.66; acc: 0.88
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.51; acc: 0.94
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.0002085131563944742
0.00020084690186195076
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.4; acc: 0.98
Val Epoch over. val_loss: 0.5906280197535351; val_accuracy: 0.871218152866242 

The current subspace-distance is: 0.00020084690186195076 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.91
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.91
Batch: 260; loss: 0.73; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.94
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.84; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.73
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.91
Batch: 440; loss: 0.65; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.82; acc: 0.78
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.73; acc: 0.83
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.52; acc: 0.94
Batch: 660; loss: 0.74; acc: 0.77
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.74; acc: 0.83
Batch: 740; loss: 0.63; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.89
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00020962842972949147
0.00020303850760683417
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.97
Val Epoch over. val_loss: 0.5844747424125671; val_accuracy: 0.8737062101910829 

The current subspace-distance is: 0.00020303850760683417 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.8; acc: 0.81
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.92
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.72; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.84
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.61; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.8; acc: 0.77
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.77; acc: 0.78
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.95
Batch: 700; loss: 0.77; acc: 0.75
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.89
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.0002104330196743831
0.00020191626390442252
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.5945477039571021; val_accuracy: 0.8702229299363057 

The current subspace-distance is: 0.00020191626390442252 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.76; acc: 0.8
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.71; acc: 0.78
Batch: 300; loss: 0.57; acc: 0.92
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.75
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.89
Batch: 480; loss: 0.65; acc: 0.89
Batch: 500; loss: 0.69; acc: 0.77
Batch: 520; loss: 0.57; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.72; acc: 0.8
Batch: 580; loss: 0.6; acc: 0.89
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 0.68; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.95
Batch: 660; loss: 0.62; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.91
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.75; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.92
Batch: 760; loss: 0.6; acc: 0.92
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00021236880274955183
0.00020327266247477382
Batch: 0; loss: 0.68; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.95
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.97
Val Epoch over. val_loss: 0.5872973232132614; val_accuracy: 0.8715167197452229 

The current subspace-distance is: 0.00020327266247477382 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.97
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.63; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.74; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.92
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.91
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.71; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.95
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00021044352615717798
0.00020354693697299808
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.97
Val Epoch over. val_loss: 0.5857138016800971; val_accuracy: 0.8721138535031847 

The current subspace-distance is: 0.00020354693697299808 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.86; acc: 0.73
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.92
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.91
Batch: 420; loss: 0.65; acc: 0.89
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 0.72; acc: 0.75
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.81; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00021180458134040236
0.00020549300825223327
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.4; acc: 0.97
Val Epoch over. val_loss: 0.5833513664591844; val_accuracy: 0.8728105095541401 

The current subspace-distance is: 0.00020549300825223327 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.88; acc: 0.73
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.92
Batch: 200; loss: 0.74; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.84
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.73; acc: 0.78
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.74; acc: 0.77
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.79; acc: 0.8
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.91
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.6; acc: 0.8
Batch: 560; loss: 0.76; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.94
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.73; acc: 0.78
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.94
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.89
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00021379791724029928
0.0002071189519483596
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.97
Val Epoch over. val_loss: 0.5796600392289982; val_accuracy: 0.87390525477707 

The current subspace-distance is: 0.0002071189519483596 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.74; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.77
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.64; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.84
Batch: 400; loss: 0.77; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.95; acc: 0.73
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.7; acc: 0.8
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.88
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.63; train_accuracy: 0.86 

0.0002155249530915171
0.00020931371545884758
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.4; acc: 0.95
Val Epoch over. val_loss: 0.5798842443782053; val_accuracy: 0.8721138535031847 

The current subspace-distance is: 0.00020931371545884758 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.95
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.58; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.88
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.71; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.74; acc: 0.84
Batch: 500; loss: 0.82; acc: 0.8
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.71; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.89
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.78; acc: 0.77
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00021864166774321347
0.00021007031318731606
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.97
Val Epoch over. val_loss: 0.5854182518591546; val_accuracy: 0.8745023885350318 

The current subspace-distance is: 0.00021007031318731606 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.7; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.7; acc: 0.77
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.92
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.94
Batch: 780; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.63; train_accuracy: 0.86 

0.00021440374257508665
0.00020940903050359339
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.95
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.39; acc: 0.97
Val Epoch over. val_loss: 0.5768634942686481; val_accuracy: 0.8703224522292994 

The current subspace-distance is: 0.00020940903050359339 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.78; acc: 0.78
Batch: 180; loss: 0.52; acc: 0.92
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.89
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.71; acc: 0.8
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.59; acc: 0.84
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.71; acc: 0.84
Batch: 740; loss: 0.52; acc: 0.92
Batch: 760; loss: 0.74; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00021554277918767184
0.00020695471903309226
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.95
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.5802821239848046; val_accuracy: 0.8747014331210191 

The current subspace-distance is: 0.00020695471903309226 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.8
Batch: 220; loss: 0.72; acc: 0.77
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.63; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.95
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.64; acc: 0.88
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.54; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.85 

0.00021803770505357534
0.00020989609765820205
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.97
Val Epoch over. val_loss: 0.5652708217596553; val_accuracy: 0.8767914012738853 

The current subspace-distance is: 0.00020989609765820205 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_5_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.25

The number of parameters is: 276579

The number of individual parameters is:

18
288
18
18
27
41796
27
27
54
125388
54
54
64
103680
64
64
4096
64
640
10
64
64

nonzero elements in E: 82973692
elements in E: 82973700
fraction nonzero: 0.9999999035839067
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.06; acc: 0.27
Batch: 40; loss: 1.87; acc: 0.42
Batch: 60; loss: 1.67; acc: 0.66
Batch: 80; loss: 1.86; acc: 0.5
Batch: 100; loss: 1.62; acc: 0.66
Batch: 120; loss: 1.6; acc: 0.61
Batch: 140; loss: 1.58; acc: 0.69
Batch: 160; loss: 1.52; acc: 0.66
Batch: 180; loss: 1.44; acc: 0.75
Batch: 200; loss: 1.45; acc: 0.69
Batch: 220; loss: 1.47; acc: 0.73
Batch: 240; loss: 1.59; acc: 0.5
Batch: 260; loss: 1.29; acc: 0.78
Batch: 280; loss: 1.54; acc: 0.61
Batch: 300; loss: 1.32; acc: 0.75
Batch: 320; loss: 1.4; acc: 0.69
Batch: 340; loss: 1.29; acc: 0.83
Batch: 360; loss: 1.4; acc: 0.73
Batch: 380; loss: 1.36; acc: 0.72
Batch: 400; loss: 1.34; acc: 0.7
Batch: 420; loss: 1.23; acc: 0.83
Batch: 440; loss: 1.27; acc: 0.77
Batch: 460; loss: 1.25; acc: 0.81
Batch: 480; loss: 1.28; acc: 0.73
Batch: 500; loss: 1.28; acc: 0.8
Batch: 520; loss: 1.38; acc: 0.73
Batch: 540; loss: 1.14; acc: 0.81
Batch: 560; loss: 1.22; acc: 0.81
Batch: 580; loss: 1.19; acc: 0.81
Batch: 600; loss: 1.31; acc: 0.77
Batch: 620; loss: 1.24; acc: 0.78
Batch: 640; loss: 1.22; acc: 0.83
Batch: 660; loss: 1.19; acc: 0.83
Batch: 680; loss: 1.24; acc: 0.75
Batch: 700; loss: 1.25; acc: 0.73
Batch: 720; loss: 1.26; acc: 0.77
Batch: 740; loss: 1.27; acc: 0.77
Batch: 760; loss: 1.11; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.75
Train Epoch over. train_loss: 1.41; train_accuracy: 0.69 

6.833306542830542e-05
6.393143848981708e-05
Batch: 0; loss: 1.02; acc: 0.88
Batch: 20; loss: 1.16; acc: 0.83
Batch: 40; loss: 0.83; acc: 0.92
Batch: 60; loss: 1.13; acc: 0.77
Batch: 80; loss: 0.94; acc: 0.92
Batch: 100; loss: 1.18; acc: 0.84
Batch: 120; loss: 1.27; acc: 0.75
Batch: 140; loss: 0.98; acc: 0.91
Val Epoch over. val_loss: 1.0912194715184012; val_accuracy: 0.8294187898089171 

The current subspace-distance is: 6.393143848981708e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.86
Batch: 20; loss: 1.09; acc: 0.86
Batch: 40; loss: 1.13; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.78
Batch: 80; loss: 1.11; acc: 0.83
Batch: 100; loss: 1.07; acc: 0.88
Batch: 120; loss: 1.12; acc: 0.75
Batch: 140; loss: 1.03; acc: 0.8
Batch: 160; loss: 1.03; acc: 0.81
Batch: 180; loss: 1.18; acc: 0.78
Batch: 200; loss: 1.05; acc: 0.89
Batch: 220; loss: 1.02; acc: 0.83
Batch: 240; loss: 1.08; acc: 0.81
Batch: 260; loss: 1.12; acc: 0.75
Batch: 280; loss: 1.07; acc: 0.8
Batch: 300; loss: 1.04; acc: 0.78
Batch: 320; loss: 1.13; acc: 0.8
Batch: 340; loss: 0.96; acc: 0.84
Batch: 360; loss: 0.92; acc: 0.91
Batch: 380; loss: 1.12; acc: 0.78
Batch: 400; loss: 1.03; acc: 0.8
Batch: 420; loss: 1.12; acc: 0.8
Batch: 440; loss: 0.97; acc: 0.91
Batch: 460; loss: 1.15; acc: 0.73
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 0.93; acc: 0.86
Batch: 520; loss: 0.92; acc: 0.86
Batch: 540; loss: 0.98; acc: 0.83
Batch: 560; loss: 0.82; acc: 0.88
Batch: 580; loss: 1.03; acc: 0.81
Batch: 600; loss: 1.03; acc: 0.86
Batch: 620; loss: 0.96; acc: 0.83
Batch: 640; loss: 0.96; acc: 0.81
Batch: 660; loss: 0.94; acc: 0.86
Batch: 680; loss: 0.95; acc: 0.86
Batch: 700; loss: 0.93; acc: 0.81
Batch: 720; loss: 1.05; acc: 0.84
Batch: 740; loss: 0.85; acc: 0.88
Batch: 760; loss: 0.98; acc: 0.8
Batch: 780; loss: 1.07; acc: 0.73
Train Epoch over. train_loss: 1.05; train_accuracy: 0.81 

9.045223123393953e-05
8.63419845700264e-05
Batch: 0; loss: 0.86; acc: 0.86
Batch: 20; loss: 1.01; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.97
Batch: 60; loss: 0.88; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.95
Batch: 100; loss: 0.94; acc: 0.89
Batch: 120; loss: 1.1; acc: 0.73
Batch: 140; loss: 0.84; acc: 0.89
Val Epoch over. val_loss: 0.8948516636897045; val_accuracy: 0.8541998407643312 

The current subspace-distance is: 8.63419845700264e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.88
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 0.93; acc: 0.81
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.96; acc: 0.78
Batch: 120; loss: 1.06; acc: 0.75
Batch: 140; loss: 1.01; acc: 0.78
Batch: 160; loss: 0.79; acc: 0.89
Batch: 180; loss: 0.97; acc: 0.8
Batch: 200; loss: 0.93; acc: 0.83
Batch: 220; loss: 0.92; acc: 0.83
Batch: 240; loss: 0.87; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.89
Batch: 280; loss: 1.02; acc: 0.78
Batch: 300; loss: 0.94; acc: 0.77
Batch: 320; loss: 0.95; acc: 0.83
Batch: 340; loss: 0.79; acc: 0.91
Batch: 360; loss: 0.89; acc: 0.84
Batch: 380; loss: 0.96; acc: 0.78
Batch: 400; loss: 0.95; acc: 0.81
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 0.97; acc: 0.81
Batch: 460; loss: 0.88; acc: 0.83
Batch: 480; loss: 0.84; acc: 0.88
Batch: 500; loss: 0.83; acc: 0.86
Batch: 520; loss: 0.85; acc: 0.83
Batch: 540; loss: 0.74; acc: 0.92
Batch: 560; loss: 1.13; acc: 0.72
Batch: 580; loss: 0.8; acc: 0.88
Batch: 600; loss: 1.03; acc: 0.73
Batch: 620; loss: 0.86; acc: 0.83
Batch: 640; loss: 0.72; acc: 0.89
Batch: 660; loss: 0.76; acc: 0.89
Batch: 680; loss: 0.74; acc: 0.88
Batch: 700; loss: 0.86; acc: 0.8
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.75; acc: 0.89
Batch: 760; loss: 0.87; acc: 0.81
Batch: 780; loss: 0.67; acc: 0.92
Train Epoch over. train_loss: 0.88; train_accuracy: 0.84 

0.00011103181168437004
0.00010725358879426494
Batch: 0; loss: 0.76; acc: 0.91
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.97
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.75; acc: 0.91
Batch: 120; loss: 0.92; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.7507817230786488; val_accuracy: 0.8735071656050956 

The current subspace-distance is: 0.00010725358879426494 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.88
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.84
Batch: 80; loss: 0.9; acc: 0.81
Batch: 100; loss: 0.91; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.86
Batch: 140; loss: 0.87; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.94
Batch: 180; loss: 0.82; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.92
Batch: 220; loss: 0.78; acc: 0.88
Batch: 240; loss: 0.76; acc: 0.86
Batch: 260; loss: 0.89; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.89
Batch: 300; loss: 0.81; acc: 0.86
Batch: 320; loss: 0.82; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.88
Batch: 360; loss: 0.81; acc: 0.81
Batch: 380; loss: 0.75; acc: 0.83
Batch: 400; loss: 0.8; acc: 0.84
Batch: 420; loss: 0.69; acc: 0.89
Batch: 440; loss: 0.79; acc: 0.88
Batch: 460; loss: 0.78; acc: 0.91
Batch: 480; loss: 0.8; acc: 0.81
Batch: 500; loss: 0.74; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.88
Batch: 540; loss: 0.68; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.94
Batch: 580; loss: 0.83; acc: 0.8
Batch: 600; loss: 0.77; acc: 0.86
Batch: 620; loss: 0.7; acc: 0.88
Batch: 640; loss: 0.76; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.91
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.73; acc: 0.86
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.91
Train Epoch over. train_loss: 0.77; train_accuracy: 0.86 

0.0001278885902138427
0.00012198599870316684
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.94
Val Epoch over. val_loss: 0.6694088974955735; val_accuracy: 0.8792794585987261 

The current subspace-distance is: 0.00012198599870316684 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.86
Batch: 140; loss: 0.81; acc: 0.86
Batch: 160; loss: 0.83; acc: 0.75
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 0.8; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.95
Batch: 240; loss: 0.59; acc: 0.94
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.79; acc: 0.75
Batch: 300; loss: 0.61; acc: 0.94
Batch: 320; loss: 0.78; acc: 0.83
Batch: 340; loss: 0.68; acc: 0.91
Batch: 360; loss: 0.64; acc: 0.91
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.64; acc: 0.89
Batch: 420; loss: 0.65; acc: 0.91
Batch: 440; loss: 0.66; acc: 0.91
Batch: 460; loss: 0.7; acc: 0.94
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.88
Batch: 520; loss: 0.69; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.92
Batch: 560; loss: 0.81; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.96; acc: 0.75
Batch: 620; loss: 0.72; acc: 0.94
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.71; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.86
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.7; train_accuracy: 0.86 

0.00013969981228001416
0.00013664181460626423
Batch: 0; loss: 0.66; acc: 0.92
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.47; acc: 0.95
Val Epoch over. val_loss: 0.6222393581062365; val_accuracy: 0.8876393312101911 

The current subspace-distance is: 0.00013664181460626423 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.78; acc: 0.86
Batch: 40; loss: 0.66; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.95
Batch: 160; loss: 0.74; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.92
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.68; acc: 0.86
Batch: 240; loss: 0.77; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.61; acc: 0.89
Batch: 320; loss: 0.74; acc: 0.78
Batch: 340; loss: 0.66; acc: 0.89
Batch: 360; loss: 0.65; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.94
Batch: 400; loss: 0.66; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.92
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.81; acc: 0.81
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.8; acc: 0.81
Batch: 600; loss: 0.75; acc: 0.8
Batch: 620; loss: 0.75; acc: 0.88
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.88
Batch: 680; loss: 0.75; acc: 0.86
Batch: 700; loss: 0.71; acc: 0.89
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.89
Batch: 760; loss: 0.79; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.94
Train Epoch over. train_loss: 0.67; train_accuracy: 0.87 

0.00015183073992375284
0.00014692408149130642
Batch: 0; loss: 0.63; acc: 0.95
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.97
Val Epoch over. val_loss: 0.587315671762843; val_accuracy: 0.887937898089172 

The current subspace-distance is: 0.00014692408149130642 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.94
Batch: 80; loss: 0.64; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.76; acc: 0.81
Batch: 180; loss: 0.55; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.94
Batch: 380; loss: 0.65; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.92
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.91
Batch: 540; loss: 0.68; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.72; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.63; acc: 0.88
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.92
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.81
Batch: 740; loss: 0.55; acc: 0.92
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.87 

0.0001607804442755878
0.00015543712652288377
Batch: 0; loss: 0.64; acc: 0.94
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.570967593959942; val_accuracy: 0.8904259554140127 

The current subspace-distance is: 0.00015543712652288377 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.74; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.95
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.92
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.67; acc: 0.86
Batch: 400; loss: 0.7; acc: 0.78
Batch: 420; loss: 0.59; acc: 0.91
Batch: 440; loss: 0.64; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.58; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.67; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.92
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.87 

0.00016970912110991776
0.0001636132801650092
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.97
Val Epoch over. val_loss: 0.5442140966084352; val_accuracy: 0.8924164012738853 

The current subspace-distance is: 0.0001636132801650092 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.92
Batch: 140; loss: 0.64; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.89
Batch: 180; loss: 0.6; acc: 0.92
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.95
Batch: 240; loss: 0.8; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.73; acc: 0.83
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.89
Batch: 600; loss: 0.75; acc: 0.8
Batch: 620; loss: 0.47; acc: 0.97
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.73; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.95
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.59; train_accuracy: 0.88 

0.00017796672182157636
0.00017239981389138848
Batch: 0; loss: 0.59; acc: 0.97
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.5300574638661305; val_accuracy: 0.8936106687898089 

The current subspace-distance is: 0.00017239981389138848 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.94
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.91
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.71; acc: 0.78
Batch: 400; loss: 0.47; acc: 0.94
Batch: 420; loss: 0.62; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.92
Batch: 500; loss: 0.69; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.65; acc: 0.78
Batch: 560; loss: 0.64; acc: 0.88
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.58; acc: 0.91
Batch: 620; loss: 0.86; acc: 0.73
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.92
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

0.0001870752457762137
0.00018189326510764658
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.5017623999126398; val_accuracy: 0.8971934713375797 

The current subspace-distance is: 0.00018189326510764658 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.97
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.6; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.94
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.95
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.56; train_accuracy: 0.88 

0.00019000348402187228
0.0001816781295929104
Batch: 0; loss: 0.58; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.5095953616746671; val_accuracy: 0.897093949044586 

The current subspace-distance is: 0.0001816781295929104 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.94
Batch: 200; loss: 0.72; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.63; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.97
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 0.51; acc: 0.91
Batch: 480; loss: 0.75; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.0001910015707835555
0.00018464519234839827
Batch: 0; loss: 0.56; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.4920625132360276; val_accuracy: 0.8972929936305732 

The current subspace-distance is: 0.00018464519234839827 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.94
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.61; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.00019226284348405898
0.0001855870650615543
Batch: 0; loss: 0.56; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.4915039248906883; val_accuracy: 0.8979896496815286 

The current subspace-distance is: 0.0001855870650615543 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.95
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.58; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.94
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.97
Batch: 440; loss: 0.41; acc: 0.95
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.63; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.91
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.47; acc: 0.95
Batch: 720; loss: 0.67; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.91
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.00019405040075071156
0.00019003788474947214
Batch: 0; loss: 0.57; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.4874839685904752; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 0.00019003788474947214 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.95
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.0001985557028092444
0.00019267220341134816
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4799874903290135; val_accuracy: 0.9003781847133758 

The current subspace-distance is: 0.00019267220341134816 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.95
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

0.00019919620535802096
0.00019110804714728147
Batch: 0; loss: 0.55; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4758361206882319; val_accuracy: 0.9021695859872612 

The current subspace-distance is: 0.00019110804714728147 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.98
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.44; acc: 0.94
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.92
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.57; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.76; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.6; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00020070196478627622
0.00019473127031233162
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4741197407815107; val_accuracy: 0.899781050955414 

The current subspace-distance is: 0.00019473127031233162 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.98
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.94
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.94
Batch: 600; loss: 0.48; acc: 0.94
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

0.00020392949227243662
0.0001964214607141912
Batch: 0; loss: 0.53; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.46443022294029307; val_accuracy: 0.9031648089171974 

The current subspace-distance is: 0.0001964214607141912 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.98
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.94
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.94
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.95
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

0.00020571878121700138
0.00020085088908672333
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.45664595807813535; val_accuracy: 0.9039609872611465 

The current subspace-distance is: 0.00020085088908672333 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.6; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.44; acc: 0.95
Batch: 580; loss: 0.62; acc: 0.81
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.98
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00021086247579660267
0.00020224758191034198
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4485718091582037; val_accuracy: 0.9045581210191083 

The current subspace-distance is: 0.00020224758191034198 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00020999160187784582
0.00020409762510098517
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.454325761859584; val_accuracy: 0.9036624203821656 

The current subspace-distance is: 0.00020409762510098517 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.94
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.94
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.94
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.95
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00021064491011202335
0.00020249560475349426
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.45487840521107814; val_accuracy: 0.9046576433121019 

The current subspace-distance is: 0.00020249560475349426 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.98
Batch: 180; loss: 0.47; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.81
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00021431704226415604
0.00020617095287889242
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.45280831406830224; val_accuracy: 0.9041600318471338 

The current subspace-distance is: 0.00020617095287889242 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.94
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.6; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.95
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.95
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00021111010573804379
0.00020513212075456977
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4404064789888965; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 0.00020513212075456977 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.44; acc: 0.94
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.65; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.94
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00021077156998217106
0.00020446804410312325
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.45368947288033307; val_accuracy: 0.9044585987261147 

The current subspace-distance is: 0.00020446804410312325 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.78
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.97
Batch: 400; loss: 0.46; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.97
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.00020828329434152693
0.00020585153833962977
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4462681991659152; val_accuracy: 0.9040605095541401 

The current subspace-distance is: 0.00020585153833962977 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.92
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.97
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.95
Batch: 380; loss: 0.57; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.95
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.94
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.0002128814230673015
0.00020706227223854512
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.45260825981000424; val_accuracy: 0.9063495222929936 

The current subspace-distance is: 0.00020706227223854512 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.78
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.97
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.0002135003451257944
0.00020826853869948536
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.44463247525843846; val_accuracy: 0.9050557324840764 

The current subspace-distance is: 0.00020826853869948536 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.94
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.5; acc: 0.94
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.95
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.94
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.00021372460469137877
0.00020515256619546562
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4386067047810099; val_accuracy: 0.9060509554140127 

The current subspace-distance is: 0.00020515256619546562 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.69; acc: 0.78
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.64; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.94
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.95
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.0002172050008084625
0.00021148273663129658
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4395697465178314; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 0.00021148273663129658 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_5_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.25

The number of parameters is: 276579

The number of individual parameters is:

18
288
18
18
27
41796
27
27
54
125388
54
54
64
103680
64
64
4096
64
640
10
64
64

nonzero elements in E: 110631589
elements in E: 110631600
fraction nonzero: 0.9999999005709038
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.38; acc: 0.09
Batch: 20; loss: 2.0; acc: 0.36
Batch: 40; loss: 1.8; acc: 0.52
Batch: 60; loss: 1.71; acc: 0.56
Batch: 80; loss: 1.54; acc: 0.64
Batch: 100; loss: 1.54; acc: 0.64
Batch: 120; loss: 1.58; acc: 0.64
Batch: 140; loss: 1.46; acc: 0.73
Batch: 160; loss: 1.46; acc: 0.67
Batch: 180; loss: 1.41; acc: 0.64
Batch: 200; loss: 1.34; acc: 0.75
Batch: 220; loss: 1.35; acc: 0.73
Batch: 240; loss: 1.36; acc: 0.7
Batch: 260; loss: 1.32; acc: 0.75
Batch: 280; loss: 1.38; acc: 0.69
Batch: 300; loss: 1.24; acc: 0.72
Batch: 320; loss: 1.26; acc: 0.75
Batch: 340; loss: 1.24; acc: 0.7
Batch: 360; loss: 1.29; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.75
Batch: 400; loss: 1.13; acc: 0.81
Batch: 420; loss: 1.27; acc: 0.75
Batch: 440; loss: 1.19; acc: 0.77
Batch: 460; loss: 1.05; acc: 0.84
Batch: 480; loss: 1.17; acc: 0.72
Batch: 500; loss: 1.14; acc: 0.81
Batch: 520; loss: 1.25; acc: 0.69
Batch: 540; loss: 1.1; acc: 0.75
Batch: 560; loss: 1.04; acc: 0.8
Batch: 580; loss: 1.19; acc: 0.72
Batch: 600; loss: 1.03; acc: 0.84
Batch: 620; loss: 1.09; acc: 0.8
Batch: 640; loss: 1.16; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.84
Batch: 680; loss: 0.99; acc: 0.83
Batch: 700; loss: 0.97; acc: 0.84
Batch: 720; loss: 0.98; acc: 0.81
Batch: 740; loss: 1.01; acc: 0.8
Batch: 760; loss: 1.0; acc: 0.89
Batch: 780; loss: 1.03; acc: 0.8
Train Epoch over. train_loss: 1.28; train_accuracy: 0.72 

2.5554314561304636e-05
8.811941370368004e-06
Batch: 0; loss: 1.08; acc: 0.8
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 0.67; acc: 0.97
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 0.81; acc: 0.86
Batch: 100; loss: 0.94; acc: 0.86
Batch: 120; loss: 1.1; acc: 0.67
Batch: 140; loss: 0.85; acc: 0.84
Val Epoch over. val_loss: 0.9530231721082311; val_accuracy: 0.828125 

The current subspace-distance is: 8.811941370368004e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.84
Batch: 20; loss: 1.03; acc: 0.78
Batch: 40; loss: 0.91; acc: 0.89
Batch: 60; loss: 0.91; acc: 0.83
Batch: 80; loss: 1.11; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.69
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 1.03; acc: 0.75
Batch: 160; loss: 0.96; acc: 0.81
Batch: 180; loss: 0.95; acc: 0.83
Batch: 200; loss: 1.06; acc: 0.77
Batch: 220; loss: 0.94; acc: 0.81
Batch: 240; loss: 0.85; acc: 0.89
Batch: 260; loss: 0.97; acc: 0.81
Batch: 280; loss: 0.87; acc: 0.84
Batch: 300; loss: 1.11; acc: 0.72
Batch: 320; loss: 0.85; acc: 0.81
Batch: 340; loss: 0.77; acc: 0.88
Batch: 360; loss: 0.84; acc: 0.83
Batch: 380; loss: 0.8; acc: 0.88
Batch: 400; loss: 0.87; acc: 0.86
Batch: 420; loss: 0.87; acc: 0.81
Batch: 440; loss: 0.86; acc: 0.83
Batch: 460; loss: 0.92; acc: 0.8
Batch: 480; loss: 0.84; acc: 0.86
Batch: 500; loss: 0.84; acc: 0.84
Batch: 520; loss: 1.02; acc: 0.77
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.91; acc: 0.77
Batch: 580; loss: 0.97; acc: 0.7
Batch: 600; loss: 0.86; acc: 0.83
Batch: 620; loss: 0.89; acc: 0.81
Batch: 640; loss: 0.91; acc: 0.81
Batch: 660; loss: 0.9; acc: 0.8
Batch: 680; loss: 0.87; acc: 0.81
Batch: 700; loss: 0.91; acc: 0.8
Batch: 720; loss: 0.76; acc: 0.91
Batch: 740; loss: 0.83; acc: 0.8
Batch: 760; loss: 0.74; acc: 0.89
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 0.89; train_accuracy: 0.82 

3.1696046789875254e-05
1.2813578905479517e-05
Batch: 0; loss: 0.92; acc: 0.78
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.47; acc: 0.98
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.59; acc: 0.94
Batch: 100; loss: 0.75; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.67; acc: 0.89
Val Epoch over. val_loss: 0.7571391349385499; val_accuracy: 0.8576831210191083 

The current subspace-distance is: 1.2813578905479517e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.92
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.83; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.83
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.8
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.86; acc: 0.78
Batch: 220; loss: 0.93; acc: 0.72
Batch: 240; loss: 0.75; acc: 0.88
Batch: 260; loss: 0.74; acc: 0.88
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.74; acc: 0.84
Batch: 320; loss: 0.83; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.86
Batch: 360; loss: 0.82; acc: 0.78
Batch: 380; loss: 0.75; acc: 0.88
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 0.6; acc: 0.91
Batch: 440; loss: 0.67; acc: 0.91
Batch: 460; loss: 0.76; acc: 0.88
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.65; acc: 0.89
Batch: 540; loss: 0.79; acc: 0.83
Batch: 560; loss: 0.82; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.91
Batch: 600; loss: 0.75; acc: 0.89
Batch: 620; loss: 0.67; acc: 0.92
Batch: 640; loss: 0.73; acc: 0.86
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.71; acc: 0.88
Batch: 700; loss: 0.73; acc: 0.88
Batch: 720; loss: 0.78; acc: 0.84
Batch: 740; loss: 0.89; acc: 0.72
Batch: 760; loss: 0.97; acc: 0.73
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.85 

3.49275505868718e-05
1.4362657566380221e-05
Batch: 0; loss: 0.78; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.37; acc: 1.0
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.97
Batch: 100; loss: 0.65; acc: 0.91
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.56; acc: 0.94
Val Epoch over. val_loss: 0.6456110568562891; val_accuracy: 0.877687101910828 

The current subspace-distance is: 1.4362657566380221e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.92
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.58; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.89
Batch: 280; loss: 0.58; acc: 0.92
Batch: 300; loss: 0.6; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.69; acc: 0.89
Batch: 380; loss: 0.86; acc: 0.77
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.72; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.69; acc: 0.86
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.91
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.79; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.6; acc: 0.89
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.67; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.67; train_accuracy: 0.86 

3.957545413868502e-05
1.8088085198542103e-05
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.97
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.81; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.5886654394447424; val_accuracy: 0.8847531847133758 

The current subspace-distance is: 1.8088085198542103e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.66; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.69
Batch: 80; loss: 0.7; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.94
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.63; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.89
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.63; acc: 0.91
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.63; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.89
Batch: 400; loss: 0.74; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.91
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.83
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.95
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.92
Batch: 660; loss: 0.54; acc: 0.91
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.94
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.55; acc: 0.92
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.62; train_accuracy: 0.87 

4.2794061300810426e-05
1.926207733049523e-05
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.53; acc: 0.95
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.92
Val Epoch over. val_loss: 0.5307518587370587; val_accuracy: 0.8963972929936306 

The current subspace-distance is: 1.926207733049523e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.92
Batch: 260; loss: 0.74; acc: 0.78
Batch: 280; loss: 0.59; acc: 0.89
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

4.553550388664007e-05
2.133195084752515e-05
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.48761786131342505; val_accuracy: 0.9066480891719745 

The current subspace-distance is: 2.133195084752515e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.95
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.51; acc: 0.92
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.94
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

4.768485086970031e-05
2.16192001971649e-05
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.45429233864993807; val_accuracy: 0.908937101910828 

The current subspace-distance is: 2.16192001971649e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.95
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.37; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

4.9997026508208364e-05
2.2333319066092372e-05
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4302513641156968; val_accuracy: 0.9124203821656051 

The current subspace-distance is: 2.2333319066092372e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.94
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.92
Batch: 540; loss: 0.65; acc: 0.78
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.6; acc: 0.81
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

5.2462964958976954e-05
2.3335240257438272e-05
Batch: 0; loss: 0.46; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.40208991707130604; val_accuracy: 0.9167993630573248 

The current subspace-distance is: 2.3335240257438272e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.32; acc: 1.0
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.97
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.95
Batch: 560; loss: 0.53; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.5818531109252945e-05
2.6320265533286147e-05
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3829093011701183; val_accuracy: 0.9204816878980892 

The current subspace-distance is: 2.6320265533286147e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.45; acc: 0.97
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.595398397417739e-05
2.466684782120865e-05
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.3732822520338046; val_accuracy: 0.919984076433121 

The current subspace-distance is: 2.466684782120865e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.797340054414235e-05
2.6553356292424724e-05
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.36674626949866107; val_accuracy: 0.9208797770700637 

The current subspace-distance is: 2.6553356292424724e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.63; acc: 0.81
Batch: 700; loss: 0.27; acc: 0.98
Batch: 720; loss: 0.54; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.838568904437125e-05
2.6824078304343857e-05
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.36808923105145713; val_accuracy: 0.9206807324840764 

The current subspace-distance is: 2.6824078304343857e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.97
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.97
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.98
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.840003723278642e-05
2.7649841285892762e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.36049585585381577; val_accuracy: 0.9212778662420382 

The current subspace-distance is: 2.7649841285892762e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.81236308789812e-05
2.6593173970468342e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3584851637767379; val_accuracy: 0.9210788216560509 

The current subspace-distance is: 2.6593173970468342e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.97
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.970553320366889e-05
2.9130664188414812e-05
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3538502980569366; val_accuracy: 0.9217754777070064 

The current subspace-distance is: 2.9130664188414812e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.98
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.223456875886768e-05
3.2193453080253676e-05
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.35309125226774035; val_accuracy: 0.92078025477707 

The current subspace-distance is: 3.2193453080253676e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.59; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.98
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.98
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.95
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.981754293316044e-05
2.635448799992446e-05
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3505526086327377; val_accuracy: 0.9222730891719745 

The current subspace-distance is: 2.635448799992446e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.0860838857479393e-05
2.7963011234533042e-05
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.34533402731843815; val_accuracy: 0.92296974522293 

The current subspace-distance is: 2.7963011234533042e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.94
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.97
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.208764534676448e-05
2.8839347578468733e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.34313520675252196; val_accuracy: 0.9220740445859873 

The current subspace-distance is: 2.8839347578468733e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.36; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.233069871086627e-05
2.885310459532775e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.33959203531408005; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.885310459532775e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 1.0
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.25; acc: 1.0
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.98
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.108946399763227e-05
2.8680731702479534e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.33822310929465443; val_accuracy: 0.92296974522293 

The current subspace-distance is: 2.8680731702479534e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.97
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.8
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.193021545186639e-05
2.8944961741217412e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.33658585740122826; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 2.8944961741217412e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.187244434840977e-05
2.850418968591839e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.33587965397698105; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.850418968591839e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.98
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.224934622878209e-05
2.8639204174396582e-05
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.33671183646864195; val_accuracy: 0.9225716560509554 

The current subspace-distance is: 2.8639204174396582e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.97
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.289441807894036e-05
2.934824260591995e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.334083957182374; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 2.934824260591995e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.98
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.97
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.25586835667491e-05
2.995869544974994e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.33435338650159774; val_accuracy: 0.9231687898089171 

The current subspace-distance is: 2.995869544974994e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.98
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.97
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.98
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.196509639266878e-05
2.7186935767531395e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.33607394556710674; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.7186935767531395e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.98
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.284399569267407e-05
2.870376374630723e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.335951599345845; val_accuracy: 0.924562101910828 

The current subspace-distance is: 2.870376374630723e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.81
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.98
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.357823440339416e-05
2.951117858174257e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3347135126400905; val_accuracy: 0.9216759554140127 

The current subspace-distance is: 2.951117858174257e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_5_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.25

The number of parameters is: 276579

The number of individual parameters is:

18
288
18
18
27
41796
27
27
54
125388
54
54
64
103680
64
64
4096
64
640
10
64
64

nonzero elements in E: 138289488
elements in E: 138289500
fraction nonzero: 0.9999999132255161
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 1.94; acc: 0.38
Batch: 40; loss: 1.81; acc: 0.53
Batch: 60; loss: 1.75; acc: 0.45
Batch: 80; loss: 1.55; acc: 0.69
Batch: 100; loss: 1.52; acc: 0.67
Batch: 120; loss: 1.48; acc: 0.7
Batch: 140; loss: 1.36; acc: 0.73
Batch: 160; loss: 1.43; acc: 0.72
Batch: 180; loss: 1.27; acc: 0.78
Batch: 200; loss: 1.26; acc: 0.83
Batch: 220; loss: 1.33; acc: 0.72
Batch: 240; loss: 1.39; acc: 0.62
Batch: 260; loss: 1.23; acc: 0.78
Batch: 280; loss: 1.12; acc: 0.81
Batch: 300; loss: 1.14; acc: 0.81
Batch: 320; loss: 1.13; acc: 0.78
Batch: 340; loss: 1.03; acc: 0.89
Batch: 360; loss: 1.2; acc: 0.73
Batch: 380; loss: 1.09; acc: 0.86
Batch: 400; loss: 1.13; acc: 0.78
Batch: 420; loss: 1.15; acc: 0.73
Batch: 440; loss: 0.97; acc: 0.86
Batch: 460; loss: 1.04; acc: 0.81
Batch: 480; loss: 1.06; acc: 0.86
Batch: 500; loss: 1.02; acc: 0.84
Batch: 520; loss: 0.97; acc: 0.81
Batch: 540; loss: 1.01; acc: 0.83
Batch: 560; loss: 0.93; acc: 0.83
Batch: 580; loss: 1.0; acc: 0.81
Batch: 600; loss: 1.0; acc: 0.84
Batch: 620; loss: 1.02; acc: 0.86
Batch: 640; loss: 0.88; acc: 0.89
Batch: 660; loss: 0.86; acc: 0.91
Batch: 680; loss: 0.87; acc: 0.84
Batch: 700; loss: 0.9; acc: 0.88
Batch: 720; loss: 1.0; acc: 0.84
Batch: 740; loss: 0.86; acc: 0.88
Batch: 760; loss: 0.85; acc: 0.89
Batch: 780; loss: 0.86; acc: 0.89
Train Epoch over. train_loss: 1.18; train_accuracy: 0.77 

2.6523368433117867e-05
9.721089554659557e-06
Batch: 0; loss: 0.81; acc: 0.89
Batch: 20; loss: 1.0; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.92
Batch: 100; loss: 0.82; acc: 0.92
Batch: 120; loss: 0.95; acc: 0.83
Batch: 140; loss: 0.69; acc: 0.97
Val Epoch over. val_loss: 0.8246145707786463; val_accuracy: 0.8771894904458599 

The current subspace-distance is: 9.721089554659557e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.86
Batch: 40; loss: 0.83; acc: 0.88
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 0.91; acc: 0.8
Batch: 100; loss: 0.85; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.95
Batch: 160; loss: 0.78; acc: 0.94
Batch: 180; loss: 0.86; acc: 0.83
Batch: 200; loss: 0.82; acc: 0.88
Batch: 220; loss: 0.88; acc: 0.86
Batch: 240; loss: 0.89; acc: 0.83
Batch: 260; loss: 0.83; acc: 0.84
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 0.79; acc: 0.88
Batch: 320; loss: 0.89; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.92
Batch: 360; loss: 0.76; acc: 0.91
Batch: 380; loss: 0.91; acc: 0.83
Batch: 400; loss: 0.81; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.91
Batch: 460; loss: 0.79; acc: 0.88
Batch: 480; loss: 0.78; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.91
Batch: 520; loss: 0.63; acc: 0.94
Batch: 540; loss: 0.73; acc: 0.88
Batch: 560; loss: 0.74; acc: 0.89
Batch: 580; loss: 0.71; acc: 0.89
Batch: 600; loss: 0.66; acc: 0.88
Batch: 620; loss: 0.81; acc: 0.83
Batch: 640; loss: 0.76; acc: 0.89
Batch: 660; loss: 0.88; acc: 0.75
Batch: 680; loss: 0.72; acc: 0.91
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.72; acc: 0.91
Batch: 740; loss: 0.81; acc: 0.81
Batch: 760; loss: 0.75; acc: 0.88
Batch: 780; loss: 0.76; acc: 0.88
Train Epoch over. train_loss: 0.79; train_accuracy: 0.87 

3.22391715599224e-05
1.2846839126723353e-05
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.97
Val Epoch over. val_loss: 0.6335960053334571; val_accuracy: 0.895203025477707 

The current subspace-distance is: 1.2846839126723353e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.91
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.92
Batch: 100; loss: 0.75; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.74; acc: 0.91
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.75; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.91
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.91
Batch: 380; loss: 0.58; acc: 0.92
Batch: 400; loss: 0.5; acc: 0.94
Batch: 420; loss: 0.59; acc: 0.92
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.94
Batch: 480; loss: 0.64; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.97
Batch: 540; loss: 0.63; acc: 0.94
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.98
Batch: 700; loss: 0.61; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.92
Train Epoch over. train_loss: 0.63; train_accuracy: 0.89 

3.692942482302897e-05
1.560466262162663e-05
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.98
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.98
Val Epoch over. val_loss: 0.517247984743422; val_accuracy: 0.9133160828025477 

The current subspace-distance is: 1.560466262162663e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.95
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.94
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.94
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.89
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.95
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.94
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.9 

4.128675936954096e-05
1.935400541697163e-05
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.98
Val Epoch over. val_loss: 0.44020600085425526; val_accuracy: 0.9202826433121019 

The current subspace-distance is: 1.935400541697163e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.94
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.95
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.98
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.95
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.91 

4.466674727154896e-05
2.1199119146331213e-05
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.38957803957401566; val_accuracy: 0.9260549363057324 

The current subspace-distance is: 2.1199119146331213e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.97
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.97
Batch: 360; loss: 0.41; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.97
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.98
Batch: 520; loss: 0.52; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.92 

4.780571543960832e-05
2.341094659641385e-05
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.344869227451124; val_accuracy: 0.9323248407643312 

The current subspace-distance is: 2.341094659641385e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.97
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.98
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.92 

5.0233844376634806e-05
2.5409650334040634e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3265076628441264; val_accuracy: 0.934812898089172 

The current subspace-distance is: 2.5409650334040634e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.98
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.25; acc: 1.0
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.233979027252644e-05
2.4837630917318165e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.31394802166777813; val_accuracy: 0.933718152866242 

The current subspace-distance is: 2.4837630917318165e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.21; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.98
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.98
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.98
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.98
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.4225758503889665e-05
2.572964149294421e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.2926177099157291; val_accuracy: 0.9391918789808917 

The current subspace-distance is: 2.572964149294421e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.98
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.6655895605217665e-05
2.893877353926655e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.2799229769000582; val_accuracy: 0.9411823248407644 

The current subspace-distance is: 2.893877353926655e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.98
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.97
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.739711195928976e-05
2.734840199991595e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.27681387538553043; val_accuracy: 0.9428742038216561 

The current subspace-distance is: 2.734840199991595e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.98
Batch: 280; loss: 0.35; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.17; acc: 0.98
Batch: 460; loss: 0.3; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.98
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.7566714531276375e-05
2.7635678634396754e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2745464829977151; val_accuracy: 0.9419785031847133 

The current subspace-distance is: 2.7635678634396754e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.19; acc: 1.0
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.883198537048884e-05
2.7851299819303676e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2677507053609866; val_accuracy: 0.9416799363057324 

The current subspace-distance is: 2.7851299819303676e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.19; acc: 1.0
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.894167043152265e-05
2.7451129426481202e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.266089243304198; val_accuracy: 0.9449641719745223 

The current subspace-distance is: 2.7451129426481202e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.18; acc: 1.0
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.98
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.870017048437148e-05
2.8269954782444984e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.26779327503624994; val_accuracy: 0.9432722929936306 

The current subspace-distance is: 2.8269954782444984e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.98
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.98
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.914903886150569e-05
2.8950133128091693e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.2645517991511685; val_accuracy: 0.9457603503184714 

The current subspace-distance is: 2.8950133128091693e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.98
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.96258178120479e-05
2.8849035516032018e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2618480674020804; val_accuracy: 0.9437699044585988 

The current subspace-distance is: 2.8849035516032018e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.143152131699026e-05
3.0101515221758746e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2570878696289791; val_accuracy: 0.9455613057324841 

The current subspace-distance is: 3.0101515221758746e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.84
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.160927296150476e-05
3.051860039704479e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2529105042006559; val_accuracy: 0.9453622611464968 

The current subspace-distance is: 3.051860039704479e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.199213385116309e-05
3.0420893381233327e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2530843941079583; val_accuracy: 0.9465565286624203 

The current subspace-distance is: 3.0420893381233327e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.21; acc: 1.0
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.1604063375853e-05
2.976547875732649e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.25113266758668196; val_accuracy: 0.9453622611464968 

The current subspace-distance is: 2.976547875732649e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.98
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.18; acc: 1.0
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.98
Batch: 460; loss: 0.29; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.280618981691077e-05
3.1927360396366566e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.250220748839105; val_accuracy: 0.9437699044585988 

The current subspace-distance is: 3.1927360396366566e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.231528095668182e-05
3.1139796192292124e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.25072374797550734; val_accuracy: 0.9449641719745223 

The current subspace-distance is: 3.1139796192292124e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.98
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.246613338589668e-05
2.9763285056105815e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.25142337139814525; val_accuracy: 0.9450636942675159 

The current subspace-distance is: 2.9763285056105815e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.17; acc: 0.98
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.17; acc: 1.0
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.251802551560104e-05
3.057848152820952e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2500529651334331; val_accuracy: 0.9452627388535032 

The current subspace-distance is: 3.057848152820952e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.2; acc: 1.0
Batch: 360; loss: 0.2; acc: 0.98
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.213584856595844e-05
3.0106937629170716e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2500512294803455; val_accuracy: 0.9463574840764332 

The current subspace-distance is: 3.0106937629170716e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.97
Batch: 180; loss: 0.14; acc: 1.0
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.98
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.306322029558942e-05
3.180410203640349e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2475857974332609; val_accuracy: 0.9455613057324841 

The current subspace-distance is: 3.180410203640349e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.98
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.129522080300376e-05
2.9322707632672973e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.24728789790325864; val_accuracy: 0.943968949044586 

The current subspace-distance is: 2.9322707632672973e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.98
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.98
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.321246473817155e-05
3.150326665490866e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.24638528846631383; val_accuracy: 0.9470541401273885 

The current subspace-distance is: 3.150326665490866e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.98
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.16; acc: 1.0
Batch: 320; loss: 0.22; acc: 0.98
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.97
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.98
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.220876821316779e-05
2.882596709241625e-05
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2475291448319034; val_accuracy: 0.9454617834394905 

The current subspace-distance is: 2.882596709241625e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_5_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:58/N_5_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
