model : table13slim
N : 16
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 16:23:19

Channel scaling factor: 1

The number of parameters is: 272670

The number of individual parameters is:

8
144
8
8
12
29952
12
12
24
89856
24
24
64
147456
64
64
4096
64
640
10
64
64

nonzero elements in E: 13633498
elements in E: 13633500
fraction nonzero: 0.9999998533025268
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.2; acc: 0.19
Batch: 40; loss: 2.22; acc: 0.23
Batch: 60; loss: 2.12; acc: 0.25
Batch: 80; loss: 2.15; acc: 0.23
Batch: 100; loss: 2.14; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.34
Batch: 140; loss: 2.13; acc: 0.33
Batch: 160; loss: 2.05; acc: 0.33
Batch: 180; loss: 2.1; acc: 0.2
Batch: 200; loss: 2.04; acc: 0.33
Batch: 220; loss: 2.06; acc: 0.31
Batch: 240; loss: 1.97; acc: 0.36
Batch: 260; loss: 2.06; acc: 0.28
Batch: 280; loss: 1.96; acc: 0.47
Batch: 300; loss: 2.03; acc: 0.28
Batch: 320; loss: 2.03; acc: 0.33
Batch: 340; loss: 2.02; acc: 0.42
Batch: 360; loss: 1.96; acc: 0.38
Batch: 380; loss: 2.1; acc: 0.25
Batch: 400; loss: 1.97; acc: 0.34
Batch: 420; loss: 1.96; acc: 0.36
Batch: 440; loss: 2.01; acc: 0.34
Batch: 460; loss: 1.95; acc: 0.39
Batch: 480; loss: 1.96; acc: 0.44
Batch: 500; loss: 1.9; acc: 0.41
Batch: 520; loss: 1.94; acc: 0.42
Batch: 540; loss: 2.05; acc: 0.28
Batch: 560; loss: 1.94; acc: 0.41
Batch: 580; loss: 2.07; acc: 0.33
Batch: 600; loss: 1.92; acc: 0.41
Batch: 620; loss: 2.03; acc: 0.36
Batch: 640; loss: 2.0; acc: 0.31
Batch: 660; loss: 1.95; acc: 0.36
Batch: 680; loss: 1.96; acc: 0.42
Batch: 700; loss: 1.96; acc: 0.38
Batch: 720; loss: 1.91; acc: 0.41
Batch: 740; loss: 1.95; acc: 0.34
Batch: 760; loss: 1.95; acc: 0.38
Batch: 780; loss: 1.92; acc: 0.45
Train Epoch over. train_loss: 2.02; train_accuracy: 0.35 

2.326536014152225e-05
4.304129561205627e-06
Batch: 0; loss: 1.97; acc: 0.36
Batch: 20; loss: 1.96; acc: 0.36
Batch: 40; loss: 1.82; acc: 0.44
Batch: 60; loss: 1.83; acc: 0.5
Batch: 80; loss: 1.83; acc: 0.48
Batch: 100; loss: 1.97; acc: 0.38
Batch: 120; loss: 1.9; acc: 0.45
Batch: 140; loss: 1.82; acc: 0.47
Val Epoch over. val_loss: 1.9149017645295259; val_accuracy: 0.41550557324840764 

The current subspace-distance is: 4.304129561205627e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.92; acc: 0.42
Batch: 20; loss: 1.93; acc: 0.42
Batch: 40; loss: 1.88; acc: 0.45
Batch: 60; loss: 1.92; acc: 0.47
Batch: 80; loss: 1.91; acc: 0.38
Batch: 100; loss: 2.01; acc: 0.39
Batch: 120; loss: 1.97; acc: 0.34
Batch: 140; loss: 1.87; acc: 0.47
Batch: 160; loss: 1.92; acc: 0.36
Batch: 180; loss: 1.94; acc: 0.36
Batch: 200; loss: 1.93; acc: 0.34
Batch: 220; loss: 1.92; acc: 0.36
Batch: 240; loss: 1.92; acc: 0.39
Batch: 260; loss: 1.85; acc: 0.39
Batch: 280; loss: 1.86; acc: 0.44
Batch: 300; loss: 1.89; acc: 0.41
Batch: 320; loss: 1.94; acc: 0.38
Batch: 340; loss: 1.93; acc: 0.39
Batch: 360; loss: 1.92; acc: 0.41
Batch: 380; loss: 1.95; acc: 0.39
Batch: 400; loss: 1.88; acc: 0.48
Batch: 420; loss: 2.0; acc: 0.28
Batch: 440; loss: 1.88; acc: 0.41
Batch: 460; loss: 1.89; acc: 0.44
Batch: 480; loss: 1.87; acc: 0.39
Batch: 500; loss: 1.87; acc: 0.48
Batch: 520; loss: 1.99; acc: 0.38
Batch: 540; loss: 1.88; acc: 0.44
Batch: 560; loss: 1.93; acc: 0.44
Batch: 580; loss: 1.87; acc: 0.48
Batch: 600; loss: 1.86; acc: 0.45
Batch: 620; loss: 1.91; acc: 0.42
Batch: 640; loss: 1.89; acc: 0.38
Batch: 660; loss: 1.87; acc: 0.38
Batch: 680; loss: 1.84; acc: 0.44
Batch: 700; loss: 1.9; acc: 0.39
Batch: 720; loss: 1.94; acc: 0.36
Batch: 740; loss: 1.9; acc: 0.41
Batch: 760; loss: 1.86; acc: 0.45
Batch: 780; loss: 1.86; acc: 0.47
Train Epoch over. train_loss: 1.9; train_accuracy: 0.42 

2.5863919290713966e-05
6.074616976547986e-06
Batch: 0; loss: 1.95; acc: 0.39
Batch: 20; loss: 1.91; acc: 0.34
Batch: 40; loss: 1.71; acc: 0.64
Batch: 60; loss: 1.81; acc: 0.48
Batch: 80; loss: 1.73; acc: 0.5
Batch: 100; loss: 1.86; acc: 0.48
Batch: 120; loss: 1.84; acc: 0.59
Batch: 140; loss: 1.74; acc: 0.5
Val Epoch over. val_loss: 1.8475835254997204; val_accuracy: 0.46914808917197454 

The current subspace-distance is: 6.074616976547986e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.88; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.41
Batch: 40; loss: 1.89; acc: 0.41
Batch: 60; loss: 1.73; acc: 0.55
Batch: 80; loss: 1.91; acc: 0.45
Batch: 100; loss: 1.9; acc: 0.38
Batch: 120; loss: 1.8; acc: 0.48
Batch: 140; loss: 1.85; acc: 0.48
Batch: 160; loss: 1.86; acc: 0.42
Batch: 180; loss: 1.97; acc: 0.38
Batch: 200; loss: 1.81; acc: 0.44
Batch: 220; loss: 1.87; acc: 0.44
Batch: 240; loss: 1.88; acc: 0.48
Batch: 260; loss: 1.88; acc: 0.44
Batch: 280; loss: 1.94; acc: 0.42
Batch: 300; loss: 1.93; acc: 0.36
Batch: 320; loss: 1.97; acc: 0.38
Batch: 340; loss: 1.8; acc: 0.52
Batch: 360; loss: 1.91; acc: 0.39
Batch: 380; loss: 1.77; acc: 0.55
Batch: 400; loss: 1.74; acc: 0.55
Batch: 420; loss: 1.87; acc: 0.39
Batch: 440; loss: 1.73; acc: 0.48
Batch: 460; loss: 1.95; acc: 0.33
Batch: 480; loss: 1.86; acc: 0.47
Batch: 500; loss: 1.85; acc: 0.41
Batch: 520; loss: 1.79; acc: 0.52
Batch: 540; loss: 1.91; acc: 0.41
Batch: 560; loss: 1.72; acc: 0.55
Batch: 580; loss: 1.84; acc: 0.5
Batch: 600; loss: 1.78; acc: 0.48
Batch: 620; loss: 1.78; acc: 0.45
Batch: 640; loss: 1.77; acc: 0.5
Batch: 660; loss: 1.77; acc: 0.55
Batch: 680; loss: 1.73; acc: 0.52
Batch: 700; loss: 1.82; acc: 0.48
Batch: 720; loss: 1.83; acc: 0.41
Batch: 740; loss: 1.89; acc: 0.34
Batch: 760; loss: 1.73; acc: 0.53
Batch: 780; loss: 1.8; acc: 0.45
Train Epoch over. train_loss: 1.84; train_accuracy: 0.45 

2.635301643749699e-05
5.436459559859941e-06
Batch: 0; loss: 1.88; acc: 0.44
Batch: 20; loss: 1.79; acc: 0.41
Batch: 40; loss: 1.61; acc: 0.61
Batch: 60; loss: 1.79; acc: 0.45
Batch: 80; loss: 1.64; acc: 0.55
Batch: 100; loss: 1.72; acc: 0.56
Batch: 120; loss: 1.76; acc: 0.52
Batch: 140; loss: 1.69; acc: 0.53
Val Epoch over. val_loss: 1.7803678064589288; val_accuracy: 0.488953025477707 

The current subspace-distance is: 5.436459559859941e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.77; acc: 0.52
Batch: 20; loss: 1.79; acc: 0.55
Batch: 40; loss: 1.81; acc: 0.38
Batch: 60; loss: 1.8; acc: 0.45
Batch: 80; loss: 1.78; acc: 0.45
Batch: 100; loss: 1.74; acc: 0.55
Batch: 120; loss: 1.79; acc: 0.45
Batch: 140; loss: 1.83; acc: 0.45
Batch: 160; loss: 1.9; acc: 0.42
Batch: 180; loss: 1.68; acc: 0.62
Batch: 200; loss: 1.8; acc: 0.45
Batch: 220; loss: 1.79; acc: 0.41
Batch: 240; loss: 1.83; acc: 0.39
Batch: 260; loss: 1.73; acc: 0.52
Batch: 280; loss: 1.67; acc: 0.58
Batch: 300; loss: 1.68; acc: 0.58
Batch: 320; loss: 1.81; acc: 0.42
Batch: 340; loss: 1.65; acc: 0.58
Batch: 360; loss: 1.79; acc: 0.45
Batch: 380; loss: 1.7; acc: 0.59
Batch: 400; loss: 1.75; acc: 0.5
Batch: 420; loss: 1.76; acc: 0.48
Batch: 440; loss: 1.71; acc: 0.48
Batch: 460; loss: 1.81; acc: 0.47
Batch: 480; loss: 1.92; acc: 0.41
Batch: 500; loss: 1.79; acc: 0.45
Batch: 520; loss: 1.68; acc: 0.52
Batch: 540; loss: 1.79; acc: 0.47
Batch: 560; loss: 1.59; acc: 0.62
Batch: 580; loss: 1.79; acc: 0.45
Batch: 600; loss: 1.72; acc: 0.55
Batch: 620; loss: 1.92; acc: 0.36
Batch: 640; loss: 1.74; acc: 0.5
Batch: 660; loss: 1.7; acc: 0.45
Batch: 680; loss: 1.7; acc: 0.48
Batch: 700; loss: 1.86; acc: 0.41
Batch: 720; loss: 1.73; acc: 0.45
Batch: 740; loss: 1.86; acc: 0.36
Batch: 760; loss: 1.61; acc: 0.53
Batch: 780; loss: 1.91; acc: 0.42
Train Epoch over. train_loss: 1.77; train_accuracy: 0.47 

2.9210808861535043e-05
7.5083034971612506e-06
Batch: 0; loss: 1.81; acc: 0.48
Batch: 20; loss: 1.72; acc: 0.39
Batch: 40; loss: 1.53; acc: 0.56
Batch: 60; loss: 1.75; acc: 0.45
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.64; acc: 0.56
Batch: 140; loss: 1.66; acc: 0.47
Val Epoch over. val_loss: 1.7083848593341318; val_accuracy: 0.4994028662420382 

The current subspace-distance is: 7.5083034971612506e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.83; acc: 0.41
Batch: 20; loss: 1.75; acc: 0.47
Batch: 40; loss: 1.6; acc: 0.58
Batch: 60; loss: 1.78; acc: 0.5
Batch: 80; loss: 1.72; acc: 0.5
Batch: 100; loss: 1.86; acc: 0.42
Batch: 120; loss: 1.77; acc: 0.47
Batch: 140; loss: 1.74; acc: 0.5
Batch: 160; loss: 1.75; acc: 0.47
Batch: 180; loss: 1.85; acc: 0.44
Batch: 200; loss: 1.8; acc: 0.39
Batch: 220; loss: 1.76; acc: 0.5
Batch: 240; loss: 1.72; acc: 0.48
Batch: 260; loss: 1.67; acc: 0.42
Batch: 280; loss: 1.73; acc: 0.5
Batch: 300; loss: 1.68; acc: 0.59
Batch: 320; loss: 1.71; acc: 0.56
Batch: 340; loss: 1.69; acc: 0.39
Batch: 360; loss: 1.63; acc: 0.53
Batch: 380; loss: 1.66; acc: 0.48
Batch: 400; loss: 1.65; acc: 0.48
Batch: 420; loss: 1.75; acc: 0.5
Batch: 440; loss: 1.79; acc: 0.48
Batch: 460; loss: 1.73; acc: 0.42
Batch: 480; loss: 1.61; acc: 0.56
Batch: 500; loss: 1.68; acc: 0.42
Batch: 520; loss: 1.66; acc: 0.47
Batch: 540; loss: 1.7; acc: 0.53
Batch: 560; loss: 1.7; acc: 0.42
Batch: 580; loss: 1.75; acc: 0.44
Batch: 600; loss: 1.65; acc: 0.52
Batch: 620; loss: 1.73; acc: 0.5
Batch: 640; loss: 1.81; acc: 0.42
Batch: 660; loss: 1.78; acc: 0.45
Batch: 680; loss: 1.66; acc: 0.44
Batch: 700; loss: 1.71; acc: 0.5
Batch: 720; loss: 1.78; acc: 0.47
Batch: 740; loss: 1.7; acc: 0.44
Batch: 760; loss: 1.65; acc: 0.52
Batch: 780; loss: 1.63; acc: 0.55
Train Epoch over. train_loss: 1.71; train_accuracy: 0.49 

3.2547697628615424e-05
9.165987648884766e-06
Batch: 0; loss: 1.74; acc: 0.5
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.45; acc: 0.67
Batch: 60; loss: 1.7; acc: 0.45
Batch: 80; loss: 1.51; acc: 0.58
Batch: 100; loss: 1.55; acc: 0.62
Batch: 120; loss: 1.56; acc: 0.58
Batch: 140; loss: 1.66; acc: 0.5
Val Epoch over. val_loss: 1.6567667775852666; val_accuracy: 0.5086584394904459 

The current subspace-distance is: 9.165987648884766e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.55
Batch: 40; loss: 1.69; acc: 0.44
Batch: 60; loss: 1.87; acc: 0.39
Batch: 80; loss: 1.69; acc: 0.48
Batch: 100; loss: 1.64; acc: 0.53
Batch: 120; loss: 1.77; acc: 0.45
Batch: 140; loss: 1.65; acc: 0.56
Batch: 160; loss: 1.71; acc: 0.53
Batch: 180; loss: 1.73; acc: 0.48
Batch: 200; loss: 1.7; acc: 0.48
Batch: 220; loss: 1.75; acc: 0.44
Batch: 240; loss: 1.73; acc: 0.42
Batch: 260; loss: 1.67; acc: 0.47
Batch: 280; loss: 1.67; acc: 0.48
Batch: 300; loss: 1.75; acc: 0.42
Batch: 320; loss: 1.87; acc: 0.3
Batch: 340; loss: 1.61; acc: 0.61
Batch: 360; loss: 1.71; acc: 0.53
Batch: 380; loss: 1.86; acc: 0.36
Batch: 400; loss: 1.85; acc: 0.27
Batch: 420; loss: 1.77; acc: 0.44
Batch: 440; loss: 1.59; acc: 0.56
Batch: 460; loss: 1.78; acc: 0.45
Batch: 480; loss: 1.77; acc: 0.41
Batch: 500; loss: 1.58; acc: 0.56
Batch: 520; loss: 1.6; acc: 0.58
Batch: 540; loss: 1.62; acc: 0.52
Batch: 560; loss: 1.51; acc: 0.64
Batch: 580; loss: 1.76; acc: 0.42
Batch: 600; loss: 1.87; acc: 0.41
Batch: 620; loss: 1.76; acc: 0.44
Batch: 640; loss: 1.67; acc: 0.5
Batch: 660; loss: 1.65; acc: 0.55
Batch: 680; loss: 1.87; acc: 0.39
Batch: 700; loss: 1.75; acc: 0.5
Batch: 720; loss: 1.64; acc: 0.52
Batch: 740; loss: 1.89; acc: 0.38
Batch: 760; loss: 1.74; acc: 0.45
Batch: 780; loss: 1.53; acc: 0.58
Train Epoch over. train_loss: 1.69; train_accuracy: 0.49 

3.3675347367534414e-05
1.0327285053790547e-05
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.69; acc: 0.44
Batch: 40; loss: 1.45; acc: 0.62
Batch: 60; loss: 1.7; acc: 0.47
Batch: 80; loss: 1.49; acc: 0.64
Batch: 100; loss: 1.53; acc: 0.62
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.67; acc: 0.45
Val Epoch over. val_loss: 1.6432762616758894; val_accuracy: 0.517515923566879 

The current subspace-distance is: 1.0327285053790547e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.61
Batch: 20; loss: 1.89; acc: 0.41
Batch: 40; loss: 1.74; acc: 0.44
Batch: 60; loss: 1.52; acc: 0.55
Batch: 80; loss: 1.74; acc: 0.45
Batch: 100; loss: 1.7; acc: 0.44
Batch: 120; loss: 1.64; acc: 0.5
Batch: 140; loss: 1.55; acc: 0.61
Batch: 160; loss: 1.86; acc: 0.38
Batch: 180; loss: 1.65; acc: 0.47
Batch: 200; loss: 1.69; acc: 0.42
Batch: 220; loss: 1.62; acc: 0.53
Batch: 240; loss: 1.85; acc: 0.44
Batch: 260; loss: 1.6; acc: 0.55
Batch: 280; loss: 1.64; acc: 0.55
Batch: 300; loss: 1.79; acc: 0.44
Batch: 320; loss: 1.57; acc: 0.56
Batch: 340; loss: 1.73; acc: 0.47
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.72; acc: 0.45
Batch: 400; loss: 1.66; acc: 0.5
Batch: 420; loss: 1.9; acc: 0.47
Batch: 440; loss: 1.65; acc: 0.45
Batch: 460; loss: 1.71; acc: 0.44
Batch: 480; loss: 1.61; acc: 0.52
Batch: 500; loss: 1.66; acc: 0.5
Batch: 520; loss: 1.65; acc: 0.47
Batch: 540; loss: 1.41; acc: 0.61
Batch: 560; loss: 1.66; acc: 0.47
Batch: 580; loss: 1.68; acc: 0.39
Batch: 600; loss: 1.64; acc: 0.52
Batch: 620; loss: 1.61; acc: 0.53
Batch: 640; loss: 1.54; acc: 0.55
Batch: 660; loss: 1.69; acc: 0.47
Batch: 680; loss: 1.51; acc: 0.66
Batch: 700; loss: 1.6; acc: 0.5
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.64; acc: 0.48
Batch: 760; loss: 1.53; acc: 0.61
Batch: 780; loss: 1.77; acc: 0.45
Train Epoch over. train_loss: 1.67; train_accuracy: 0.49 

3.55673000740353e-05
9.728561053634621e-06
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.69; acc: 0.44
Batch: 40; loss: 1.42; acc: 0.62
Batch: 60; loss: 1.67; acc: 0.48
Batch: 80; loss: 1.47; acc: 0.64
Batch: 100; loss: 1.53; acc: 0.62
Batch: 120; loss: 1.53; acc: 0.55
Batch: 140; loss: 1.65; acc: 0.47
Val Epoch over. val_loss: 1.6249886348748663; val_accuracy: 0.5150278662420382 

The current subspace-distance is: 9.728561053634621e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.41
Batch: 20; loss: 1.76; acc: 0.39
Batch: 40; loss: 1.58; acc: 0.58
Batch: 60; loss: 1.75; acc: 0.47
Batch: 80; loss: 1.75; acc: 0.36
Batch: 100; loss: 1.72; acc: 0.5
Batch: 120; loss: 1.73; acc: 0.5
Batch: 140; loss: 1.58; acc: 0.55
Batch: 160; loss: 1.73; acc: 0.41
Batch: 180; loss: 1.56; acc: 0.59
Batch: 200; loss: 1.51; acc: 0.52
Batch: 220; loss: 1.64; acc: 0.48
Batch: 240; loss: 1.57; acc: 0.52
Batch: 260; loss: 1.58; acc: 0.56
Batch: 280; loss: 1.56; acc: 0.56
Batch: 300; loss: 1.67; acc: 0.53
Batch: 320; loss: 1.63; acc: 0.52
Batch: 340; loss: 1.73; acc: 0.45
Batch: 360; loss: 1.74; acc: 0.36
Batch: 380; loss: 1.61; acc: 0.48
Batch: 400; loss: 1.6; acc: 0.52
Batch: 420; loss: 1.68; acc: 0.47
Batch: 440; loss: 1.67; acc: 0.44
Batch: 460; loss: 1.58; acc: 0.58
Batch: 480; loss: 1.76; acc: 0.47
Batch: 500; loss: 1.59; acc: 0.61
Batch: 520; loss: 1.76; acc: 0.47
Batch: 540; loss: 1.52; acc: 0.56
Batch: 560; loss: 1.82; acc: 0.42
Batch: 580; loss: 1.7; acc: 0.39
Batch: 600; loss: 1.75; acc: 0.44
Batch: 620; loss: 1.48; acc: 0.64
Batch: 640; loss: 1.75; acc: 0.34
Batch: 660; loss: 1.68; acc: 0.42
Batch: 680; loss: 1.74; acc: 0.47
Batch: 700; loss: 1.61; acc: 0.52
Batch: 720; loss: 1.75; acc: 0.48
Batch: 740; loss: 1.53; acc: 0.53
Batch: 760; loss: 1.62; acc: 0.47
Batch: 780; loss: 1.7; acc: 0.48
Train Epoch over. train_loss: 1.66; train_accuracy: 0.49 

3.718845982803032e-05
1.1456021638878155e-05
Batch: 0; loss: 1.72; acc: 0.47
Batch: 20; loss: 1.67; acc: 0.42
Batch: 40; loss: 1.42; acc: 0.67
Batch: 60; loss: 1.67; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.59
Batch: 100; loss: 1.52; acc: 0.62
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.66; acc: 0.45
Val Epoch over. val_loss: 1.6275090442341604; val_accuracy: 0.505672770700637 

The current subspace-distance is: 1.1456021638878155e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.66; acc: 0.56
Batch: 80; loss: 1.76; acc: 0.5
Batch: 100; loss: 1.71; acc: 0.44
Batch: 120; loss: 1.61; acc: 0.5
Batch: 140; loss: 1.51; acc: 0.59
Batch: 160; loss: 1.51; acc: 0.56
Batch: 180; loss: 1.71; acc: 0.48
Batch: 200; loss: 1.72; acc: 0.41
Batch: 220; loss: 1.6; acc: 0.47
Batch: 240; loss: 1.84; acc: 0.33
Batch: 260; loss: 1.73; acc: 0.42
Batch: 280; loss: 1.71; acc: 0.5
Batch: 300; loss: 1.63; acc: 0.53
Batch: 320; loss: 1.69; acc: 0.5
Batch: 340; loss: 1.61; acc: 0.52
Batch: 360; loss: 1.67; acc: 0.47
Batch: 380; loss: 1.49; acc: 0.58
Batch: 400; loss: 1.46; acc: 0.62
Batch: 420; loss: 1.59; acc: 0.55
Batch: 440; loss: 1.76; acc: 0.42
Batch: 460; loss: 1.65; acc: 0.5
Batch: 480; loss: 1.65; acc: 0.56
Batch: 500; loss: 1.58; acc: 0.52
Batch: 520; loss: 1.67; acc: 0.45
Batch: 540; loss: 1.81; acc: 0.39
Batch: 560; loss: 1.74; acc: 0.41
Batch: 580; loss: 1.85; acc: 0.36
Batch: 600; loss: 1.76; acc: 0.47
Batch: 620; loss: 1.67; acc: 0.48
Batch: 640; loss: 1.77; acc: 0.39
Batch: 660; loss: 1.6; acc: 0.58
Batch: 680; loss: 1.52; acc: 0.61
Batch: 700; loss: 1.53; acc: 0.53
Batch: 720; loss: 1.52; acc: 0.56
Batch: 740; loss: 1.62; acc: 0.52
Batch: 760; loss: 1.56; acc: 0.55
Batch: 780; loss: 1.67; acc: 0.44
Train Epoch over. train_loss: 1.65; train_accuracy: 0.49 

3.6941499274689704e-05
1.058536145137623e-05
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.42
Batch: 40; loss: 1.42; acc: 0.62
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.61
Batch: 100; loss: 1.53; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.56
Batch: 140; loss: 1.65; acc: 0.44
Val Epoch over. val_loss: 1.6206022432655285; val_accuracy: 0.5016918789808917 

The current subspace-distance is: 1.058536145137623e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.79; acc: 0.38
Batch: 60; loss: 1.83; acc: 0.38
Batch: 80; loss: 1.67; acc: 0.47
Batch: 100; loss: 1.57; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.56
Batch: 140; loss: 1.74; acc: 0.45
Batch: 160; loss: 1.59; acc: 0.56
Batch: 180; loss: 1.66; acc: 0.42
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.54; acc: 0.55
Batch: 240; loss: 1.75; acc: 0.41
Batch: 260; loss: 1.71; acc: 0.38
Batch: 280; loss: 1.65; acc: 0.41
Batch: 300; loss: 1.55; acc: 0.58
Batch: 320; loss: 1.51; acc: 0.58
Batch: 340; loss: 1.57; acc: 0.5
Batch: 360; loss: 1.62; acc: 0.47
Batch: 380; loss: 1.7; acc: 0.45
Batch: 400; loss: 1.59; acc: 0.53
Batch: 420; loss: 1.66; acc: 0.5
Batch: 440; loss: 1.63; acc: 0.47
Batch: 460; loss: 1.68; acc: 0.52
Batch: 480; loss: 1.63; acc: 0.53
Batch: 500; loss: 1.73; acc: 0.42
Batch: 520; loss: 1.77; acc: 0.42
Batch: 540; loss: 1.78; acc: 0.47
Batch: 560; loss: 1.57; acc: 0.55
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.67; acc: 0.45
Batch: 620; loss: 1.55; acc: 0.59
Batch: 640; loss: 1.69; acc: 0.47
Batch: 660; loss: 1.69; acc: 0.44
Batch: 680; loss: 1.57; acc: 0.58
Batch: 700; loss: 1.56; acc: 0.55
Batch: 720; loss: 1.56; acc: 0.59
Batch: 740; loss: 1.61; acc: 0.5
Batch: 760; loss: 1.71; acc: 0.45
Batch: 780; loss: 1.67; acc: 0.41
Train Epoch over. train_loss: 1.65; train_accuracy: 0.49 

3.8461381336674094e-05
1.0961267435050104e-05
Batch: 0; loss: 1.72; acc: 0.47
Batch: 20; loss: 1.64; acc: 0.42
Batch: 40; loss: 1.4; acc: 0.56
Batch: 60; loss: 1.65; acc: 0.47
Batch: 80; loss: 1.49; acc: 0.55
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.55
Batch: 140; loss: 1.63; acc: 0.47
Val Epoch over. val_loss: 1.6071851177580039; val_accuracy: 0.5040804140127388 

The current subspace-distance is: 1.0961267435050104e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.51; acc: 0.59
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.62; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.5; acc: 0.67
Batch: 100; loss: 1.52; acc: 0.55
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.67; acc: 0.48
Batch: 160; loss: 1.6; acc: 0.53
Batch: 180; loss: 1.74; acc: 0.47
Batch: 200; loss: 1.58; acc: 0.53
Batch: 220; loss: 1.73; acc: 0.45
Batch: 240; loss: 1.65; acc: 0.45
Batch: 260; loss: 1.61; acc: 0.44
Batch: 280; loss: 1.75; acc: 0.39
Batch: 300; loss: 1.6; acc: 0.45
Batch: 320; loss: 1.7; acc: 0.45
Batch: 340; loss: 1.64; acc: 0.5
Batch: 360; loss: 1.61; acc: 0.56
Batch: 380; loss: 1.61; acc: 0.55
Batch: 400; loss: 1.57; acc: 0.5
Batch: 420; loss: 1.64; acc: 0.48
Batch: 440; loss: 1.74; acc: 0.44
Batch: 460; loss: 1.6; acc: 0.59
Batch: 480; loss: 1.55; acc: 0.58
Batch: 500; loss: 1.56; acc: 0.55
Batch: 520; loss: 1.64; acc: 0.56
Batch: 540; loss: 1.6; acc: 0.55
Batch: 560; loss: 1.55; acc: 0.48
Batch: 580; loss: 1.73; acc: 0.48
Batch: 600; loss: 1.48; acc: 0.59
Batch: 620; loss: 1.65; acc: 0.52
Batch: 640; loss: 1.8; acc: 0.47
Batch: 660; loss: 1.73; acc: 0.45
Batch: 680; loss: 1.83; acc: 0.39
Batch: 700; loss: 1.47; acc: 0.59
Batch: 720; loss: 1.63; acc: 0.53
Batch: 740; loss: 1.59; acc: 0.5
Batch: 760; loss: 1.72; acc: 0.48
Batch: 780; loss: 1.67; acc: 0.48
Train Epoch over. train_loss: 1.64; train_accuracy: 0.49 

3.92870933865197e-05
1.2230908396304585e-05
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.62; acc: 0.42
Batch: 40; loss: 1.39; acc: 0.58
Batch: 60; loss: 1.65; acc: 0.45
Batch: 80; loss: 1.49; acc: 0.55
Batch: 100; loss: 1.51; acc: 0.58
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.6; acc: 0.48
Val Epoch over. val_loss: 1.6005199069430114; val_accuracy: 0.5054737261146497 

The current subspace-distance is: 1.2230908396304585e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.5
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.54; acc: 0.52
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 1.68; acc: 0.5
Batch: 100; loss: 1.65; acc: 0.41
Batch: 120; loss: 1.58; acc: 0.5
Batch: 140; loss: 1.62; acc: 0.47
Batch: 160; loss: 1.61; acc: 0.53
Batch: 180; loss: 1.72; acc: 0.47
Batch: 200; loss: 1.62; acc: 0.42
Batch: 220; loss: 1.61; acc: 0.53
Batch: 240; loss: 1.58; acc: 0.56
Batch: 260; loss: 1.51; acc: 0.61
Batch: 280; loss: 1.55; acc: 0.62
Batch: 300; loss: 1.63; acc: 0.5
Batch: 320; loss: 1.6; acc: 0.56
Batch: 340; loss: 1.53; acc: 0.58
Batch: 360; loss: 1.5; acc: 0.61
Batch: 380; loss: 1.58; acc: 0.52
Batch: 400; loss: 1.66; acc: 0.48
Batch: 420; loss: 1.62; acc: 0.5
Batch: 440; loss: 1.69; acc: 0.5
Batch: 460; loss: 1.61; acc: 0.5
Batch: 480; loss: 1.67; acc: 0.5
Batch: 500; loss: 1.54; acc: 0.56
Batch: 520; loss: 1.58; acc: 0.52
Batch: 540; loss: 1.56; acc: 0.59
Batch: 560; loss: 1.94; acc: 0.36
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.73; acc: 0.48
Batch: 620; loss: 1.66; acc: 0.42
Batch: 640; loss: 1.57; acc: 0.44
Batch: 660; loss: 1.53; acc: 0.5
Batch: 680; loss: 1.6; acc: 0.52
Batch: 700; loss: 1.55; acc: 0.56
Batch: 720; loss: 1.59; acc: 0.55
Batch: 740; loss: 1.68; acc: 0.45
Batch: 760; loss: 1.56; acc: 0.59
Batch: 780; loss: 1.43; acc: 0.53
Train Epoch over. train_loss: 1.64; train_accuracy: 0.49 

4.014374644611962e-05
1.2832722859457135e-05
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.62; acc: 0.44
Batch: 40; loss: 1.4; acc: 0.56
Batch: 60; loss: 1.65; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.63; acc: 0.47
Val Epoch over. val_loss: 1.6047941636128031; val_accuracy: 0.5088574840764332 

The current subspace-distance is: 1.2832722859457135e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.46; acc: 0.62
Batch: 40; loss: 1.71; acc: 0.41
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.65; acc: 0.5
Batch: 100; loss: 1.65; acc: 0.48
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.63; acc: 0.5
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.57; acc: 0.55
Batch: 200; loss: 1.88; acc: 0.33
Batch: 220; loss: 1.77; acc: 0.38
Batch: 240; loss: 1.56; acc: 0.5
Batch: 260; loss: 1.62; acc: 0.52
Batch: 280; loss: 1.69; acc: 0.48
Batch: 300; loss: 1.46; acc: 0.59
Batch: 320; loss: 1.65; acc: 0.48
Batch: 340; loss: 1.7; acc: 0.45
Batch: 360; loss: 1.53; acc: 0.56
Batch: 380; loss: 1.76; acc: 0.38
Batch: 400; loss: 1.58; acc: 0.53
Batch: 420; loss: 1.71; acc: 0.45
Batch: 440; loss: 1.86; acc: 0.3
Batch: 460; loss: 1.65; acc: 0.48
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.64; acc: 0.52
Batch: 520; loss: 1.6; acc: 0.5
Batch: 540; loss: 1.72; acc: 0.44
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.69; acc: 0.48
Batch: 600; loss: 1.66; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.52
Batch: 640; loss: 1.63; acc: 0.59
Batch: 660; loss: 1.5; acc: 0.55
Batch: 680; loss: 1.56; acc: 0.53
Batch: 700; loss: 1.58; acc: 0.52
Batch: 720; loss: 1.66; acc: 0.5
Batch: 740; loss: 1.72; acc: 0.41
Batch: 760; loss: 1.7; acc: 0.44
Batch: 780; loss: 1.55; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.49 

4.036574682686478e-05
1.3341655176191125e-05
Batch: 0; loss: 1.72; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.42
Batch: 40; loss: 1.4; acc: 0.55
Batch: 60; loss: 1.64; acc: 0.5
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.61; acc: 0.47
Val Epoch over. val_loss: 1.5996250377339163; val_accuracy: 0.5058718152866242 

The current subspace-distance is: 1.3341655176191125e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.48
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.58; acc: 0.56
Batch: 60; loss: 1.63; acc: 0.52
Batch: 80; loss: 1.48; acc: 0.61
Batch: 100; loss: 1.71; acc: 0.45
Batch: 120; loss: 1.62; acc: 0.55
Batch: 140; loss: 1.58; acc: 0.55
Batch: 160; loss: 1.69; acc: 0.42
Batch: 180; loss: 1.61; acc: 0.45
Batch: 200; loss: 1.61; acc: 0.55
Batch: 220; loss: 1.59; acc: 0.48
Batch: 240; loss: 1.53; acc: 0.53
Batch: 260; loss: 1.7; acc: 0.41
Batch: 280; loss: 1.52; acc: 0.52
Batch: 300; loss: 1.59; acc: 0.55
Batch: 320; loss: 1.62; acc: 0.44
Batch: 340; loss: 1.54; acc: 0.55
Batch: 360; loss: 1.5; acc: 0.56
Batch: 380; loss: 1.49; acc: 0.58
Batch: 400; loss: 1.6; acc: 0.58
Batch: 420; loss: 1.64; acc: 0.44
Batch: 440; loss: 1.65; acc: 0.44
Batch: 460; loss: 1.7; acc: 0.47
Batch: 480; loss: 1.6; acc: 0.48
Batch: 500; loss: 1.69; acc: 0.47
Batch: 520; loss: 1.78; acc: 0.38
Batch: 540; loss: 1.5; acc: 0.58
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.53; acc: 0.55
Batch: 620; loss: 1.79; acc: 0.41
Batch: 640; loss: 1.7; acc: 0.42
Batch: 660; loss: 1.51; acc: 0.52
Batch: 680; loss: 1.65; acc: 0.52
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.61; acc: 0.55
Batch: 740; loss: 1.73; acc: 0.42
Batch: 760; loss: 1.7; acc: 0.38
Batch: 780; loss: 1.91; acc: 0.34
Train Epoch over. train_loss: 1.63; train_accuracy: 0.49 

4.128787622903474e-05
1.4269479834183585e-05
Batch: 0; loss: 1.72; acc: 0.44
Batch: 20; loss: 1.62; acc: 0.41
Batch: 40; loss: 1.4; acc: 0.56
Batch: 60; loss: 1.64; acc: 0.47
Batch: 80; loss: 1.49; acc: 0.53
Batch: 100; loss: 1.51; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.61; acc: 0.45
Val Epoch over. val_loss: 1.5991877537624093; val_accuracy: 0.5047770700636943 

The current subspace-distance is: 1.4269479834183585e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.44
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.57; acc: 0.5
Batch: 80; loss: 1.62; acc: 0.48
Batch: 100; loss: 1.59; acc: 0.53
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.57; acc: 0.52
Batch: 160; loss: 1.61; acc: 0.44
Batch: 180; loss: 1.65; acc: 0.5
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.77; acc: 0.41
Batch: 240; loss: 1.68; acc: 0.47
Batch: 260; loss: 1.78; acc: 0.44
Batch: 280; loss: 1.74; acc: 0.44
Batch: 300; loss: 1.68; acc: 0.42
Batch: 320; loss: 1.59; acc: 0.55
Batch: 340; loss: 1.57; acc: 0.5
Batch: 360; loss: 1.72; acc: 0.45
Batch: 380; loss: 1.58; acc: 0.52
Batch: 400; loss: 1.65; acc: 0.48
Batch: 420; loss: 1.6; acc: 0.48
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.47; acc: 0.5
Batch: 480; loss: 1.55; acc: 0.52
Batch: 500; loss: 1.54; acc: 0.53
Batch: 520; loss: 1.67; acc: 0.42
Batch: 540; loss: 1.55; acc: 0.59
Batch: 560; loss: 1.58; acc: 0.53
Batch: 580; loss: 1.66; acc: 0.45
Batch: 600; loss: 1.64; acc: 0.53
Batch: 620; loss: 1.67; acc: 0.52
Batch: 640; loss: 1.61; acc: 0.5
Batch: 660; loss: 1.6; acc: 0.58
Batch: 680; loss: 1.65; acc: 0.48
Batch: 700; loss: 1.55; acc: 0.48
Batch: 720; loss: 1.67; acc: 0.44
Batch: 740; loss: 1.53; acc: 0.59
Batch: 760; loss: 1.61; acc: 0.55
Batch: 780; loss: 1.65; acc: 0.48
Train Epoch over. train_loss: 1.63; train_accuracy: 0.49 

4.1361054172739387e-05
1.4451661627390422e-05
Batch: 0; loss: 1.72; acc: 0.42
Batch: 20; loss: 1.61; acc: 0.38
Batch: 40; loss: 1.39; acc: 0.55
Batch: 60; loss: 1.64; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.5; acc: 0.58
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 1.62; acc: 0.45
Val Epoch over. val_loss: 1.5977845685497212; val_accuracy: 0.49572054140127386 

The current subspace-distance is: 1.4451661627390422e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.77; acc: 0.41
Batch: 20; loss: 1.55; acc: 0.56
Batch: 40; loss: 1.56; acc: 0.56
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.56
Batch: 120; loss: 1.74; acc: 0.45
Batch: 140; loss: 1.68; acc: 0.44
Batch: 160; loss: 1.66; acc: 0.48
Batch: 180; loss: 1.59; acc: 0.53
Batch: 200; loss: 1.65; acc: 0.47
Batch: 220; loss: 1.63; acc: 0.45
Batch: 240; loss: 1.58; acc: 0.53
Batch: 260; loss: 1.64; acc: 0.47
Batch: 280; loss: 1.49; acc: 0.64
Batch: 300; loss: 1.7; acc: 0.48
Batch: 320; loss: 1.61; acc: 0.5
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.64; acc: 0.48
Batch: 380; loss: 1.74; acc: 0.39
Batch: 400; loss: 1.77; acc: 0.41
Batch: 420; loss: 1.57; acc: 0.58
Batch: 440; loss: 1.6; acc: 0.53
Batch: 460; loss: 1.68; acc: 0.47
Batch: 480; loss: 1.65; acc: 0.45
Batch: 500; loss: 1.68; acc: 0.48
Batch: 520; loss: 1.75; acc: 0.38
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.74; acc: 0.48
Batch: 580; loss: 1.55; acc: 0.58
Batch: 600; loss: 1.71; acc: 0.42
Batch: 620; loss: 1.59; acc: 0.56
Batch: 640; loss: 1.72; acc: 0.48
Batch: 660; loss: 1.68; acc: 0.47
Batch: 680; loss: 1.64; acc: 0.45
Batch: 700; loss: 1.43; acc: 0.58
Batch: 720; loss: 1.62; acc: 0.5
Batch: 740; loss: 1.53; acc: 0.56
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.64; acc: 0.47
Train Epoch over. train_loss: 1.63; train_accuracy: 0.49 

4.100041769561358e-05
1.2502203389885835e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.39
Batch: 40; loss: 1.37; acc: 0.53
Batch: 60; loss: 1.62; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.58
Batch: 100; loss: 1.5; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.6; acc: 0.45
Val Epoch over. val_loss: 1.5878140129101503; val_accuracy: 0.5093550955414012 

The current subspace-distance is: 1.2502203389885835e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.79; acc: 0.38
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.6; acc: 0.58
Batch: 60; loss: 1.53; acc: 0.56
Batch: 80; loss: 1.72; acc: 0.42
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.68; acc: 0.42
Batch: 140; loss: 1.56; acc: 0.52
Batch: 160; loss: 1.65; acc: 0.39
Batch: 180; loss: 1.76; acc: 0.44
Batch: 200; loss: 1.75; acc: 0.39
Batch: 220; loss: 1.51; acc: 0.52
Batch: 240; loss: 1.48; acc: 0.59
Batch: 260; loss: 1.53; acc: 0.61
Batch: 280; loss: 1.7; acc: 0.48
Batch: 300; loss: 1.72; acc: 0.44
Batch: 320; loss: 1.63; acc: 0.52
Batch: 340; loss: 1.53; acc: 0.55
Batch: 360; loss: 1.68; acc: 0.45
Batch: 380; loss: 1.58; acc: 0.5
Batch: 400; loss: 1.54; acc: 0.64
Batch: 420; loss: 1.48; acc: 0.59
Batch: 440; loss: 1.56; acc: 0.52
Batch: 460; loss: 1.72; acc: 0.41
Batch: 480; loss: 1.59; acc: 0.58
Batch: 500; loss: 1.59; acc: 0.55
Batch: 520; loss: 1.62; acc: 0.5
Batch: 540; loss: 1.81; acc: 0.41
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.63; acc: 0.48
Batch: 600; loss: 1.52; acc: 0.47
Batch: 620; loss: 1.6; acc: 0.58
Batch: 640; loss: 1.65; acc: 0.44
Batch: 660; loss: 1.58; acc: 0.55
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.54; acc: 0.45
Batch: 720; loss: 1.65; acc: 0.47
Batch: 740; loss: 1.65; acc: 0.44
Batch: 760; loss: 1.56; acc: 0.53
Batch: 780; loss: 1.44; acc: 0.55
Train Epoch over. train_loss: 1.63; train_accuracy: 0.49 

4.107346467208117e-05
1.2226472790644038e-05
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.6; acc: 0.44
Batch: 40; loss: 1.39; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.55
Batch: 100; loss: 1.5; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.58; acc: 0.45
Val Epoch over. val_loss: 1.5906397165006894; val_accuracy: 0.5096536624203821 

The current subspace-distance is: 1.2226472790644038e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.47; acc: 0.56
Batch: 40; loss: 1.59; acc: 0.48
Batch: 60; loss: 1.54; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.55
Batch: 100; loss: 1.63; acc: 0.44
Batch: 120; loss: 1.74; acc: 0.47
Batch: 140; loss: 1.48; acc: 0.55
Batch: 160; loss: 1.77; acc: 0.34
Batch: 180; loss: 1.51; acc: 0.61
Batch: 200; loss: 1.53; acc: 0.58
Batch: 220; loss: 1.44; acc: 0.56
Batch: 240; loss: 1.58; acc: 0.53
Batch: 260; loss: 1.5; acc: 0.62
Batch: 280; loss: 1.58; acc: 0.53
Batch: 300; loss: 1.56; acc: 0.52
Batch: 320; loss: 1.57; acc: 0.5
Batch: 340; loss: 1.55; acc: 0.45
Batch: 360; loss: 1.57; acc: 0.58
Batch: 380; loss: 1.67; acc: 0.45
Batch: 400; loss: 1.56; acc: 0.52
Batch: 420; loss: 1.63; acc: 0.45
Batch: 440; loss: 1.71; acc: 0.5
Batch: 460; loss: 1.7; acc: 0.45
Batch: 480; loss: 1.68; acc: 0.39
Batch: 500; loss: 1.52; acc: 0.55
Batch: 520; loss: 1.57; acc: 0.5
Batch: 540; loss: 1.66; acc: 0.45
Batch: 560; loss: 1.63; acc: 0.42
Batch: 580; loss: 1.61; acc: 0.48
Batch: 600; loss: 1.44; acc: 0.58
Batch: 620; loss: 1.63; acc: 0.5
Batch: 640; loss: 1.54; acc: 0.55
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.74; acc: 0.41
Batch: 700; loss: 1.67; acc: 0.42
Batch: 720; loss: 1.67; acc: 0.44
Batch: 740; loss: 1.69; acc: 0.41
Batch: 760; loss: 1.61; acc: 0.44
Batch: 780; loss: 1.77; acc: 0.41
Train Epoch over. train_loss: 1.62; train_accuracy: 0.49 

4.179316965746693e-05
1.4235591152100824e-05
Batch: 0; loss: 1.72; acc: 0.42
Batch: 20; loss: 1.59; acc: 0.41
Batch: 40; loss: 1.39; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.47
Batch: 80; loss: 1.5; acc: 0.55
Batch: 100; loss: 1.5; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.59; acc: 0.44
Val Epoch over. val_loss: 1.5927743683954714; val_accuracy: 0.4974124203821656 

The current subspace-distance is: 1.4235591152100824e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.54; acc: 0.52
Batch: 20; loss: 1.45; acc: 0.55
Batch: 40; loss: 1.54; acc: 0.5
Batch: 60; loss: 1.57; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.64; acc: 0.45
Batch: 120; loss: 1.63; acc: 0.45
Batch: 140; loss: 1.83; acc: 0.36
Batch: 160; loss: 1.54; acc: 0.58
Batch: 180; loss: 1.67; acc: 0.5
Batch: 200; loss: 1.82; acc: 0.38
Batch: 220; loss: 1.54; acc: 0.58
Batch: 240; loss: 1.55; acc: 0.5
Batch: 260; loss: 1.63; acc: 0.48
Batch: 280; loss: 1.44; acc: 0.61
Batch: 300; loss: 1.53; acc: 0.59
Batch: 320; loss: 1.89; acc: 0.31
Batch: 340; loss: 1.46; acc: 0.59
Batch: 360; loss: 1.36; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.39
Batch: 400; loss: 1.74; acc: 0.45
Batch: 420; loss: 1.7; acc: 0.48
Batch: 440; loss: 1.45; acc: 0.56
Batch: 460; loss: 1.59; acc: 0.52
Batch: 480; loss: 1.51; acc: 0.55
Batch: 500; loss: 1.75; acc: 0.47
Batch: 520; loss: 1.72; acc: 0.44
Batch: 540; loss: 1.64; acc: 0.41
Batch: 560; loss: 1.73; acc: 0.44
Batch: 580; loss: 1.65; acc: 0.42
Batch: 600; loss: 1.76; acc: 0.36
Batch: 620; loss: 1.66; acc: 0.52
Batch: 640; loss: 1.57; acc: 0.55
Batch: 660; loss: 1.7; acc: 0.47
Batch: 680; loss: 1.57; acc: 0.53
Batch: 700; loss: 1.4; acc: 0.69
Batch: 720; loss: 1.59; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.47
Batch: 760; loss: 1.82; acc: 0.41
Batch: 780; loss: 1.5; acc: 0.55
Train Epoch over. train_loss: 1.62; train_accuracy: 0.49 

4.2413914343342185e-05
1.3825444511894602e-05
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.58; acc: 0.41
Batch: 40; loss: 1.37; acc: 0.56
Batch: 60; loss: 1.61; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.55
Batch: 100; loss: 1.48; acc: 0.56
Batch: 120; loss: 1.54; acc: 0.56
Batch: 140; loss: 1.55; acc: 0.45
Val Epoch over. val_loss: 1.5711022045961611; val_accuracy: 0.5122412420382165 

The current subspace-distance is: 1.3825444511894602e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.55
Batch: 20; loss: 1.54; acc: 0.52
Batch: 40; loss: 1.7; acc: 0.42
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.67; acc: 0.45
Batch: 100; loss: 1.54; acc: 0.55
Batch: 120; loss: 1.62; acc: 0.47
Batch: 140; loss: 1.72; acc: 0.41
Batch: 160; loss: 1.59; acc: 0.5
Batch: 180; loss: 1.62; acc: 0.45
Batch: 200; loss: 1.53; acc: 0.55
Batch: 220; loss: 1.74; acc: 0.45
Batch: 240; loss: 1.52; acc: 0.58
Batch: 260; loss: 1.68; acc: 0.44
Batch: 280; loss: 1.54; acc: 0.58
Batch: 300; loss: 1.52; acc: 0.53
Batch: 320; loss: 1.58; acc: 0.53
Batch: 340; loss: 1.62; acc: 0.47
Batch: 360; loss: 1.63; acc: 0.52
Batch: 380; loss: 1.6; acc: 0.48
Batch: 400; loss: 1.69; acc: 0.5
Batch: 420; loss: 1.42; acc: 0.59
Batch: 440; loss: 1.59; acc: 0.47
Batch: 460; loss: 1.53; acc: 0.58
Batch: 480; loss: 1.55; acc: 0.5
Batch: 500; loss: 1.57; acc: 0.52
Batch: 520; loss: 1.48; acc: 0.61
Batch: 540; loss: 1.67; acc: 0.42
Batch: 560; loss: 1.67; acc: 0.48
Batch: 580; loss: 1.54; acc: 0.56
Batch: 600; loss: 1.72; acc: 0.41
Batch: 620; loss: 1.71; acc: 0.45
Batch: 640; loss: 1.65; acc: 0.5
Batch: 660; loss: 1.77; acc: 0.38
Batch: 680; loss: 1.71; acc: 0.47
Batch: 700; loss: 1.64; acc: 0.48
Batch: 720; loss: 1.59; acc: 0.47
Batch: 740; loss: 1.74; acc: 0.5
Batch: 760; loss: 1.64; acc: 0.52
Batch: 780; loss: 1.59; acc: 0.45
Train Epoch over. train_loss: 1.62; train_accuracy: 0.49 

4.264568633516319e-05
1.396624884364428e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.58; acc: 0.39
Batch: 40; loss: 1.37; acc: 0.56
Batch: 60; loss: 1.61; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.55
Batch: 100; loss: 1.48; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.55
Batch: 140; loss: 1.56; acc: 0.44
Val Epoch over. val_loss: 1.5778573068084232; val_accuracy: 0.5079617834394905 

The current subspace-distance is: 1.396624884364428e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.5; acc: 0.56
Batch: 40; loss: 1.71; acc: 0.38
Batch: 60; loss: 1.51; acc: 0.55
Batch: 80; loss: 1.48; acc: 0.53
Batch: 100; loss: 1.86; acc: 0.34
Batch: 120; loss: 1.57; acc: 0.55
Batch: 140; loss: 1.65; acc: 0.44
Batch: 160; loss: 1.65; acc: 0.45
Batch: 180; loss: 1.58; acc: 0.53
Batch: 200; loss: 1.57; acc: 0.53
Batch: 220; loss: 1.71; acc: 0.42
Batch: 240; loss: 1.66; acc: 0.41
Batch: 260; loss: 1.65; acc: 0.42
Batch: 280; loss: 1.76; acc: 0.47
Batch: 300; loss: 1.67; acc: 0.5
Batch: 320; loss: 1.62; acc: 0.5
Batch: 340; loss: 1.7; acc: 0.45
Batch: 360; loss: 1.79; acc: 0.41
Batch: 380; loss: 1.5; acc: 0.56
Batch: 400; loss: 1.64; acc: 0.39
Batch: 420; loss: 1.76; acc: 0.42
Batch: 440; loss: 1.7; acc: 0.41
Batch: 460; loss: 1.57; acc: 0.47
Batch: 480; loss: 1.51; acc: 0.56
Batch: 500; loss: 1.55; acc: 0.56
Batch: 520; loss: 1.75; acc: 0.45
Batch: 540; loss: 1.61; acc: 0.53
Batch: 560; loss: 1.67; acc: 0.5
Batch: 580; loss: 1.51; acc: 0.59
Batch: 600; loss: 1.56; acc: 0.52
Batch: 620; loss: 1.52; acc: 0.55
Batch: 640; loss: 1.74; acc: 0.44
Batch: 660; loss: 1.6; acc: 0.52
Batch: 680; loss: 1.44; acc: 0.61
Batch: 700; loss: 1.57; acc: 0.48
Batch: 720; loss: 1.9; acc: 0.39
Batch: 740; loss: 1.66; acc: 0.45
Batch: 760; loss: 1.43; acc: 0.62
Batch: 780; loss: 1.52; acc: 0.5
Train Epoch over. train_loss: 1.62; train_accuracy: 0.49 

4.2349704017397016e-05
1.4051843209017534e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.57; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.58
Batch: 60; loss: 1.6; acc: 0.47
Batch: 80; loss: 1.49; acc: 0.55
Batch: 100; loss: 1.49; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.56
Batch: 140; loss: 1.56; acc: 0.44
Val Epoch over. val_loss: 1.5761402837789742; val_accuracy: 0.5133359872611465 

The current subspace-distance is: 1.4051843209017534e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.46; acc: 0.58
Batch: 40; loss: 1.53; acc: 0.56
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.72; acc: 0.44
Batch: 100; loss: 1.51; acc: 0.45
Batch: 120; loss: 1.62; acc: 0.47
Batch: 140; loss: 1.54; acc: 0.53
Batch: 160; loss: 1.58; acc: 0.59
Batch: 180; loss: 1.7; acc: 0.45
Batch: 200; loss: 1.63; acc: 0.47
Batch: 220; loss: 1.74; acc: 0.48
Batch: 240; loss: 1.66; acc: 0.42
Batch: 260; loss: 1.45; acc: 0.61
Batch: 280; loss: 1.5; acc: 0.61
Batch: 300; loss: 1.53; acc: 0.56
Batch: 320; loss: 1.7; acc: 0.41
Batch: 340; loss: 1.64; acc: 0.45
Batch: 360; loss: 1.51; acc: 0.56
Batch: 380; loss: 1.6; acc: 0.5
Batch: 400; loss: 1.61; acc: 0.5
Batch: 420; loss: 1.66; acc: 0.47
Batch: 440; loss: 1.52; acc: 0.58
Batch: 460; loss: 1.68; acc: 0.5
Batch: 480; loss: 1.57; acc: 0.53
Batch: 500; loss: 1.69; acc: 0.39
Batch: 520; loss: 1.68; acc: 0.47
Batch: 540; loss: 1.52; acc: 0.56
Batch: 560; loss: 1.57; acc: 0.56
Batch: 580; loss: 1.66; acc: 0.45
Batch: 600; loss: 1.52; acc: 0.56
Batch: 620; loss: 1.65; acc: 0.53
Batch: 640; loss: 1.61; acc: 0.58
Batch: 660; loss: 1.7; acc: 0.41
Batch: 680; loss: 1.71; acc: 0.44
Batch: 700; loss: 1.58; acc: 0.48
Batch: 720; loss: 1.62; acc: 0.47
Batch: 740; loss: 1.77; acc: 0.36
Batch: 760; loss: 1.65; acc: 0.38
Batch: 780; loss: 1.48; acc: 0.53
Train Epoch over. train_loss: 1.62; train_accuracy: 0.49 

4.3687807192327455e-05
1.4526886843668763e-05
Batch: 0; loss: 1.7; acc: 0.48
Batch: 20; loss: 1.58; acc: 0.44
Batch: 40; loss: 1.37; acc: 0.58
Batch: 60; loss: 1.6; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.5; acc: 0.58
Batch: 120; loss: 1.56; acc: 0.53
Batch: 140; loss: 1.55; acc: 0.44
Val Epoch over. val_loss: 1.574991859448184; val_accuracy: 0.5148288216560509 

The current subspace-distance is: 1.4526886843668763e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 1.73; acc: 0.48
Batch: 40; loss: 1.4; acc: 0.66
Batch: 60; loss: 1.52; acc: 0.5
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.57; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.53; acc: 0.58
Batch: 160; loss: 1.41; acc: 0.64
Batch: 180; loss: 1.76; acc: 0.44
Batch: 200; loss: 1.66; acc: 0.52
Batch: 220; loss: 1.73; acc: 0.42
Batch: 240; loss: 1.71; acc: 0.53
Batch: 260; loss: 1.56; acc: 0.52
Batch: 280; loss: 1.72; acc: 0.48
Batch: 300; loss: 1.73; acc: 0.38
Batch: 320; loss: 1.45; acc: 0.56
Batch: 340; loss: 1.61; acc: 0.47
Batch: 360; loss: 1.52; acc: 0.42
Batch: 380; loss: 1.68; acc: 0.42
Batch: 400; loss: 1.68; acc: 0.44
Batch: 420; loss: 1.5; acc: 0.55
Batch: 440; loss: 1.54; acc: 0.53
Batch: 460; loss: 1.91; acc: 0.41
Batch: 480; loss: 1.73; acc: 0.36
Batch: 500; loss: 1.58; acc: 0.5
Batch: 520; loss: 1.6; acc: 0.48
Batch: 540; loss: 1.45; acc: 0.59
Batch: 560; loss: 1.7; acc: 0.47
Batch: 580; loss: 1.59; acc: 0.59
Batch: 600; loss: 1.65; acc: 0.41
Batch: 620; loss: 1.72; acc: 0.42
Batch: 640; loss: 1.58; acc: 0.52
Batch: 660; loss: 1.75; acc: 0.45
Batch: 680; loss: 1.66; acc: 0.41
Batch: 700; loss: 1.66; acc: 0.5
Batch: 720; loss: 1.66; acc: 0.48
Batch: 740; loss: 1.56; acc: 0.44
Batch: 760; loss: 1.53; acc: 0.61
Batch: 780; loss: 1.53; acc: 0.53
Train Epoch over. train_loss: 1.61; train_accuracy: 0.49 

4.2842933908104897e-05
1.340272410743637e-05
Batch: 0; loss: 1.7; acc: 0.5
Batch: 20; loss: 1.57; acc: 0.41
Batch: 40; loss: 1.37; acc: 0.53
Batch: 60; loss: 1.6; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.54; acc: 0.47
Val Epoch over. val_loss: 1.5687641953207125; val_accuracy: 0.5125398089171974 

The current subspace-distance is: 1.340272410743637e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.52
Batch: 20; loss: 1.56; acc: 0.47
Batch: 40; loss: 1.7; acc: 0.42
Batch: 60; loss: 1.51; acc: 0.5
Batch: 80; loss: 1.62; acc: 0.53
Batch: 100; loss: 1.61; acc: 0.53
Batch: 120; loss: 1.51; acc: 0.59
Batch: 140; loss: 1.6; acc: 0.53
Batch: 160; loss: 1.53; acc: 0.53
Batch: 180; loss: 1.65; acc: 0.45
Batch: 200; loss: 1.7; acc: 0.44
Batch: 220; loss: 1.45; acc: 0.59
Batch: 240; loss: 1.6; acc: 0.55
Batch: 260; loss: 1.55; acc: 0.58
Batch: 280; loss: 1.55; acc: 0.55
Batch: 300; loss: 1.68; acc: 0.44
Batch: 320; loss: 1.66; acc: 0.45
Batch: 340; loss: 1.68; acc: 0.45
Batch: 360; loss: 1.5; acc: 0.61
Batch: 380; loss: 1.65; acc: 0.45
Batch: 400; loss: 1.62; acc: 0.44
Batch: 420; loss: 1.56; acc: 0.56
Batch: 440; loss: 1.69; acc: 0.42
Batch: 460; loss: 1.61; acc: 0.53
Batch: 480; loss: 1.52; acc: 0.52
Batch: 500; loss: 1.65; acc: 0.5
Batch: 520; loss: 1.58; acc: 0.58
Batch: 540; loss: 1.77; acc: 0.45
Batch: 560; loss: 1.59; acc: 0.47
Batch: 580; loss: 1.56; acc: 0.5
Batch: 600; loss: 1.61; acc: 0.52
Batch: 620; loss: 1.57; acc: 0.52
Batch: 640; loss: 1.62; acc: 0.47
Batch: 660; loss: 1.57; acc: 0.52
Batch: 680; loss: 1.58; acc: 0.5
Batch: 700; loss: 1.74; acc: 0.47
Batch: 720; loss: 1.48; acc: 0.55
Batch: 740; loss: 1.59; acc: 0.47
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.7; acc: 0.44
Train Epoch over. train_loss: 1.62; train_accuracy: 0.49 

4.256554166204296e-05
1.2521651115093846e-05
Batch: 0; loss: 1.7; acc: 0.5
Batch: 20; loss: 1.57; acc: 0.41
Batch: 40; loss: 1.36; acc: 0.58
Batch: 60; loss: 1.59; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.56; acc: 0.55
Batch: 140; loss: 1.55; acc: 0.44
Val Epoch over. val_loss: 1.57024469573027; val_accuracy: 0.5113455414012739 

The current subspace-distance is: 1.2521651115093846e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.78; acc: 0.39
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.63; acc: 0.44
Batch: 80; loss: 1.58; acc: 0.47
Batch: 100; loss: 1.59; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.53
Batch: 140; loss: 1.65; acc: 0.48
Batch: 160; loss: 1.56; acc: 0.55
Batch: 180; loss: 1.68; acc: 0.44
Batch: 200; loss: 1.7; acc: 0.52
Batch: 220; loss: 1.53; acc: 0.55
Batch: 240; loss: 1.55; acc: 0.48
Batch: 260; loss: 1.74; acc: 0.48
Batch: 280; loss: 1.56; acc: 0.56
Batch: 300; loss: 1.45; acc: 0.61
Batch: 320; loss: 1.67; acc: 0.39
Batch: 340; loss: 1.62; acc: 0.44
Batch: 360; loss: 1.64; acc: 0.42
Batch: 380; loss: 1.6; acc: 0.55
Batch: 400; loss: 1.54; acc: 0.56
Batch: 420; loss: 1.55; acc: 0.5
Batch: 440; loss: 1.59; acc: 0.53
Batch: 460; loss: 1.63; acc: 0.42
Batch: 480; loss: 1.58; acc: 0.48
Batch: 500; loss: 1.52; acc: 0.52
Batch: 520; loss: 1.6; acc: 0.41
Batch: 540; loss: 1.55; acc: 0.47
Batch: 560; loss: 1.59; acc: 0.5
Batch: 580; loss: 1.54; acc: 0.61
Batch: 600; loss: 1.78; acc: 0.45
Batch: 620; loss: 1.62; acc: 0.41
Batch: 640; loss: 1.56; acc: 0.53
Batch: 660; loss: 1.57; acc: 0.53
Batch: 680; loss: 1.72; acc: 0.36
Batch: 700; loss: 1.46; acc: 0.58
Batch: 720; loss: 1.56; acc: 0.52
Batch: 740; loss: 1.47; acc: 0.48
Batch: 760; loss: 1.71; acc: 0.52
Batch: 780; loss: 1.73; acc: 0.45
Train Epoch over. train_loss: 1.62; train_accuracy: 0.49 

4.3837702833116055e-05
1.5676298062317073e-05
Batch: 0; loss: 1.69; acc: 0.48
Batch: 20; loss: 1.56; acc: 0.41
Batch: 40; loss: 1.36; acc: 0.58
Batch: 60; loss: 1.59; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.55
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.55; acc: 0.44
Val Epoch over. val_loss: 1.5701356404905866; val_accuracy: 0.5108479299363057 

The current subspace-distance is: 1.5676298062317073e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.79; acc: 0.38
Batch: 20; loss: 1.55; acc: 0.52
Batch: 40; loss: 1.57; acc: 0.53
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.58; acc: 0.44
Batch: 100; loss: 1.61; acc: 0.42
Batch: 120; loss: 1.68; acc: 0.41
Batch: 140; loss: 1.66; acc: 0.45
Batch: 160; loss: 1.72; acc: 0.34
Batch: 180; loss: 1.65; acc: 0.45
Batch: 200; loss: 1.44; acc: 0.62
Batch: 220; loss: 1.6; acc: 0.56
Batch: 240; loss: 1.65; acc: 0.38
Batch: 260; loss: 1.57; acc: 0.55
Batch: 280; loss: 1.53; acc: 0.61
Batch: 300; loss: 1.63; acc: 0.45
Batch: 320; loss: 1.46; acc: 0.55
Batch: 340; loss: 1.6; acc: 0.53
Batch: 360; loss: 1.63; acc: 0.53
Batch: 380; loss: 1.63; acc: 0.41
Batch: 400; loss: 1.56; acc: 0.55
Batch: 420; loss: 1.53; acc: 0.5
Batch: 440; loss: 1.42; acc: 0.59
Batch: 460; loss: 1.53; acc: 0.48
Batch: 480; loss: 1.68; acc: 0.44
Batch: 500; loss: 1.64; acc: 0.5
Batch: 520; loss: 1.66; acc: 0.47
Batch: 540; loss: 1.67; acc: 0.45
Batch: 560; loss: 1.48; acc: 0.58
Batch: 580; loss: 1.65; acc: 0.41
Batch: 600; loss: 1.61; acc: 0.45
Batch: 620; loss: 1.63; acc: 0.56
Batch: 640; loss: 1.46; acc: 0.61
Batch: 660; loss: 1.52; acc: 0.55
Batch: 680; loss: 1.71; acc: 0.45
Batch: 700; loss: 1.65; acc: 0.48
Batch: 720; loss: 1.43; acc: 0.55
Batch: 740; loss: 1.76; acc: 0.34
Batch: 760; loss: 1.59; acc: 0.47
Batch: 780; loss: 1.62; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.49 

4.324863039073534e-05
1.3027916793362238e-05
Batch: 0; loss: 1.72; acc: 0.44
Batch: 20; loss: 1.57; acc: 0.41
Batch: 40; loss: 1.37; acc: 0.56
Batch: 60; loss: 1.6; acc: 0.48
Batch: 80; loss: 1.5; acc: 0.55
Batch: 100; loss: 1.49; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.55; acc: 0.44
Val Epoch over. val_loss: 1.5781913593316534; val_accuracy: 0.509156050955414 

The current subspace-distance is: 1.3027916793362238e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.67; acc: 0.44
Batch: 20; loss: 1.57; acc: 0.52
Batch: 40; loss: 1.65; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.45
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.79; acc: 0.44
Batch: 120; loss: 1.65; acc: 0.44
Batch: 140; loss: 1.46; acc: 0.66
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.75; acc: 0.38
Batch: 200; loss: 1.53; acc: 0.5
Batch: 220; loss: 1.5; acc: 0.53
Batch: 240; loss: 1.72; acc: 0.45
Batch: 260; loss: 1.62; acc: 0.58
Batch: 280; loss: 1.72; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.6; acc: 0.45
Batch: 340; loss: 1.57; acc: 0.55
Batch: 360; loss: 1.61; acc: 0.47
Batch: 380; loss: 1.42; acc: 0.59
Batch: 400; loss: 1.57; acc: 0.5
Batch: 420; loss: 1.68; acc: 0.52
Batch: 440; loss: 1.58; acc: 0.48
Batch: 460; loss: 1.62; acc: 0.52
Batch: 480; loss: 1.57; acc: 0.52
Batch: 500; loss: 1.58; acc: 0.53
Batch: 520; loss: 1.44; acc: 0.55
Batch: 540; loss: 1.47; acc: 0.59
Batch: 560; loss: 1.57; acc: 0.59
Batch: 580; loss: 1.52; acc: 0.56
Batch: 600; loss: 1.56; acc: 0.52
Batch: 620; loss: 1.64; acc: 0.44
Batch: 640; loss: 1.61; acc: 0.48
Batch: 660; loss: 1.8; acc: 0.36
Batch: 680; loss: 1.77; acc: 0.34
Batch: 700; loss: 1.58; acc: 0.47
Batch: 720; loss: 1.64; acc: 0.52
Batch: 740; loss: 1.59; acc: 0.5
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.58; acc: 0.56
Train Epoch over. train_loss: 1.61; train_accuracy: 0.49 

4.324056862969883e-05
1.2779175449395552e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.56; acc: 0.41
Batch: 40; loss: 1.37; acc: 0.53
Batch: 60; loss: 1.6; acc: 0.5
Batch: 80; loss: 1.5; acc: 0.52
Batch: 100; loss: 1.48; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.55
Batch: 140; loss: 1.55; acc: 0.42
Val Epoch over. val_loss: 1.5755637937290654; val_accuracy: 0.5034832802547771 

The current subspace-distance is: 1.2779175449395552e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.71; acc: 0.39
Batch: 40; loss: 1.76; acc: 0.34
Batch: 60; loss: 1.64; acc: 0.55
Batch: 80; loss: 1.65; acc: 0.41
Batch: 100; loss: 1.58; acc: 0.48
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.52; acc: 0.53
Batch: 160; loss: 1.61; acc: 0.5
Batch: 180; loss: 1.68; acc: 0.5
Batch: 200; loss: 1.69; acc: 0.47
Batch: 220; loss: 1.74; acc: 0.44
Batch: 240; loss: 1.62; acc: 0.53
Batch: 260; loss: 1.64; acc: 0.45
Batch: 280; loss: 1.72; acc: 0.36
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.54; acc: 0.48
Batch: 340; loss: 1.64; acc: 0.42
Batch: 360; loss: 1.55; acc: 0.52
Batch: 380; loss: 1.52; acc: 0.59
Batch: 400; loss: 1.68; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.69; acc: 0.42
Batch: 480; loss: 1.61; acc: 0.52
Batch: 500; loss: 1.67; acc: 0.5
Batch: 520; loss: 1.67; acc: 0.42
Batch: 540; loss: 1.51; acc: 0.53
Batch: 560; loss: 1.75; acc: 0.38
Batch: 580; loss: 1.6; acc: 0.47
Batch: 600; loss: 1.66; acc: 0.48
Batch: 620; loss: 1.7; acc: 0.36
Batch: 640; loss: 1.63; acc: 0.45
Batch: 660; loss: 1.5; acc: 0.62
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.52; acc: 0.52
Batch: 720; loss: 1.76; acc: 0.38
Batch: 740; loss: 1.58; acc: 0.45
Batch: 760; loss: 1.66; acc: 0.48
Batch: 780; loss: 1.63; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.49 

4.418504977365956e-05
1.4819088391959667e-05
Batch: 0; loss: 1.7; acc: 0.52
Batch: 20; loss: 1.55; acc: 0.39
Batch: 40; loss: 1.35; acc: 0.58
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.48; acc: 0.59
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.54; acc: 0.42
Val Epoch over. val_loss: 1.5665183302703176; val_accuracy: 0.5154259554140127 

The current subspace-distance is: 1.4819088391959667e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.53
Batch: 40; loss: 1.56; acc: 0.59
Batch: 60; loss: 1.47; acc: 0.58
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.52
Batch: 120; loss: 1.7; acc: 0.39
Batch: 140; loss: 1.48; acc: 0.62
Batch: 160; loss: 1.6; acc: 0.48
Batch: 180; loss: 1.72; acc: 0.42
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.52; acc: 0.52
Batch: 240; loss: 1.64; acc: 0.44
Batch: 260; loss: 1.78; acc: 0.42
Batch: 280; loss: 1.78; acc: 0.34
Batch: 300; loss: 1.72; acc: 0.45
Batch: 320; loss: 1.49; acc: 0.56
Batch: 340; loss: 1.49; acc: 0.56
Batch: 360; loss: 1.56; acc: 0.52
Batch: 380; loss: 1.67; acc: 0.47
Batch: 400; loss: 1.55; acc: 0.61
Batch: 420; loss: 1.68; acc: 0.38
Batch: 440; loss: 1.65; acc: 0.45
Batch: 460; loss: 1.51; acc: 0.58
Batch: 480; loss: 1.61; acc: 0.52
Batch: 500; loss: 1.63; acc: 0.48
Batch: 520; loss: 1.47; acc: 0.55
Batch: 540; loss: 1.74; acc: 0.41
Batch: 560; loss: 1.6; acc: 0.47
Batch: 580; loss: 1.69; acc: 0.42
Batch: 600; loss: 1.64; acc: 0.5
Batch: 620; loss: 1.67; acc: 0.48
Batch: 640; loss: 1.69; acc: 0.39
Batch: 660; loss: 1.65; acc: 0.45
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.58; acc: 0.47
Batch: 720; loss: 1.71; acc: 0.42
Batch: 740; loss: 1.51; acc: 0.55
Batch: 760; loss: 1.53; acc: 0.53
Batch: 780; loss: 1.61; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.49 

4.3256219214526936e-05
1.3701111129194032e-05
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.55; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.58
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.5; acc: 0.52
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.52; acc: 0.44
Val Epoch over. val_loss: 1.5702442583764435; val_accuracy: 0.511843152866242 

The current subspace-distance is: 1.3701111129194032e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.61
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.45; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.47
Batch: 80; loss: 1.48; acc: 0.55
Batch: 100; loss: 1.8; acc: 0.41
Batch: 120; loss: 1.65; acc: 0.45
Batch: 140; loss: 1.72; acc: 0.45
Batch: 160; loss: 1.63; acc: 0.52
Batch: 180; loss: 1.74; acc: 0.41
Batch: 200; loss: 1.58; acc: 0.47
Batch: 220; loss: 1.52; acc: 0.59
Batch: 240; loss: 1.72; acc: 0.38
Batch: 260; loss: 1.69; acc: 0.44
Batch: 280; loss: 1.64; acc: 0.44
Batch: 300; loss: 1.58; acc: 0.48
Batch: 320; loss: 1.61; acc: 0.48
Batch: 340; loss: 1.64; acc: 0.48
Batch: 360; loss: 1.48; acc: 0.52
Batch: 380; loss: 1.63; acc: 0.53
Batch: 400; loss: 1.71; acc: 0.42
Batch: 420; loss: 1.76; acc: 0.36
Batch: 440; loss: 1.6; acc: 0.48
Batch: 460; loss: 1.44; acc: 0.55
Batch: 480; loss: 1.46; acc: 0.62
Batch: 500; loss: 1.5; acc: 0.52
Batch: 520; loss: 1.54; acc: 0.59
Batch: 540; loss: 1.65; acc: 0.47
Batch: 560; loss: 1.48; acc: 0.55
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.63; acc: 0.42
Batch: 620; loss: 1.51; acc: 0.53
Batch: 640; loss: 1.6; acc: 0.5
Batch: 660; loss: 1.69; acc: 0.42
Batch: 680; loss: 1.59; acc: 0.42
Batch: 700; loss: 1.7; acc: 0.45
Batch: 720; loss: 1.63; acc: 0.5
Batch: 740; loss: 1.61; acc: 0.56
Batch: 760; loss: 1.64; acc: 0.52
Batch: 780; loss: 1.62; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.49 

4.3418349378043786e-05
1.1691201507346705e-05
Batch: 0; loss: 1.71; acc: 0.44
Batch: 20; loss: 1.56; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.56
Batch: 60; loss: 1.59; acc: 0.48
Batch: 80; loss: 1.5; acc: 0.55
Batch: 100; loss: 1.48; acc: 0.58
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.55; acc: 0.44
Val Epoch over. val_loss: 1.5754396558567216; val_accuracy: 0.5047770700636943 

The current subspace-distance is: 1.1691201507346705e-05 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_16_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1

The number of parameters is: 272670

The number of individual parameters is:

8
144
8
8
12
29952
12
12
24
89856
24
24
64
147456
64
64
4096
64
640
10
64
64

nonzero elements in E: 27266997
elements in E: 27267000
fraction nonzero: 0.9999998899768952
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.15; acc: 0.25
Batch: 40; loss: 2.17; acc: 0.19
Batch: 60; loss: 2.01; acc: 0.34
Batch: 80; loss: 2.14; acc: 0.2
Batch: 100; loss: 2.0; acc: 0.36
Batch: 120; loss: 1.95; acc: 0.39
Batch: 140; loss: 1.89; acc: 0.42
Batch: 160; loss: 1.84; acc: 0.48
Batch: 180; loss: 1.86; acc: 0.48
Batch: 200; loss: 1.87; acc: 0.39
Batch: 220; loss: 1.81; acc: 0.5
Batch: 240; loss: 1.87; acc: 0.42
Batch: 260; loss: 1.83; acc: 0.5
Batch: 280; loss: 1.78; acc: 0.53
Batch: 300; loss: 1.86; acc: 0.45
Batch: 320; loss: 1.83; acc: 0.5
Batch: 340; loss: 1.85; acc: 0.45
Batch: 360; loss: 1.76; acc: 0.5
Batch: 380; loss: 1.77; acc: 0.53
Batch: 400; loss: 1.79; acc: 0.52
Batch: 420; loss: 1.9; acc: 0.44
Batch: 440; loss: 1.77; acc: 0.52
Batch: 460; loss: 1.62; acc: 0.62
Batch: 480; loss: 1.79; acc: 0.5
Batch: 500; loss: 1.78; acc: 0.47
Batch: 520; loss: 1.76; acc: 0.53
Batch: 540; loss: 1.71; acc: 0.53
Batch: 560; loss: 1.73; acc: 0.53
Batch: 580; loss: 1.76; acc: 0.55
Batch: 600; loss: 1.76; acc: 0.53
Batch: 620; loss: 1.65; acc: 0.62
Batch: 640; loss: 1.72; acc: 0.48
Batch: 660; loss: 1.73; acc: 0.52
Batch: 680; loss: 1.71; acc: 0.56
Batch: 700; loss: 1.66; acc: 0.53
Batch: 720; loss: 1.7; acc: 0.52
Batch: 740; loss: 1.75; acc: 0.55
Batch: 760; loss: 1.7; acc: 0.52
Batch: 780; loss: 1.67; acc: 0.58
Train Epoch over. train_loss: 1.84; train_accuracy: 0.47 

5.446063369163312e-05
4.8845948185771704e-05
Batch: 0; loss: 1.69; acc: 0.48
Batch: 20; loss: 1.7; acc: 0.59
Batch: 40; loss: 1.49; acc: 0.67
Batch: 60; loss: 1.6; acc: 0.61
Batch: 80; loss: 1.62; acc: 0.62
Batch: 100; loss: 1.58; acc: 0.73
Batch: 120; loss: 1.7; acc: 0.58
Batch: 140; loss: 1.56; acc: 0.64
Val Epoch over. val_loss: 1.663957114432268; val_accuracy: 0.5826035031847133 

The current subspace-distance is: 4.8845948185771704e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.83; acc: 0.42
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.62; acc: 0.66
Batch: 60; loss: 1.79; acc: 0.52
Batch: 80; loss: 1.73; acc: 0.55
Batch: 100; loss: 1.62; acc: 0.56
Batch: 120; loss: 1.71; acc: 0.52
Batch: 140; loss: 1.78; acc: 0.42
Batch: 160; loss: 1.75; acc: 0.53
Batch: 180; loss: 1.68; acc: 0.58
Batch: 200; loss: 1.69; acc: 0.55
Batch: 220; loss: 1.59; acc: 0.61
Batch: 240; loss: 1.66; acc: 0.45
Batch: 260; loss: 1.6; acc: 0.69
Batch: 280; loss: 1.74; acc: 0.44
Batch: 300; loss: 1.8; acc: 0.42
Batch: 320; loss: 1.65; acc: 0.53
Batch: 340; loss: 1.66; acc: 0.55
Batch: 360; loss: 1.49; acc: 0.73
Batch: 380; loss: 1.58; acc: 0.62
Batch: 400; loss: 1.78; acc: 0.52
Batch: 420; loss: 1.57; acc: 0.58
Batch: 440; loss: 1.59; acc: 0.58
Batch: 460; loss: 1.56; acc: 0.61
Batch: 480; loss: 1.63; acc: 0.61
Batch: 500; loss: 1.68; acc: 0.59
Batch: 520; loss: 1.6; acc: 0.55
Batch: 540; loss: 1.61; acc: 0.59
Batch: 560; loss: 1.6; acc: 0.62
Batch: 580; loss: 1.67; acc: 0.56
Batch: 600; loss: 1.62; acc: 0.61
Batch: 620; loss: 1.49; acc: 0.7
Batch: 640; loss: 1.66; acc: 0.5
Batch: 660; loss: 1.7; acc: 0.5
Batch: 680; loss: 1.61; acc: 0.56
Batch: 700; loss: 1.62; acc: 0.53
Batch: 720; loss: 1.67; acc: 0.53
Batch: 740; loss: 1.52; acc: 0.64
Batch: 760; loss: 1.58; acc: 0.56
Batch: 780; loss: 1.67; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.57 

7.238070975290611e-05
6.69499349896796e-05
Batch: 0; loss: 1.55; acc: 0.56
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.35; acc: 0.8
Batch: 60; loss: 1.47; acc: 0.67
Batch: 80; loss: 1.47; acc: 0.64
Batch: 100; loss: 1.49; acc: 0.66
Batch: 120; loss: 1.56; acc: 0.59
Batch: 140; loss: 1.47; acc: 0.73
Val Epoch over. val_loss: 1.5519696489261214; val_accuracy: 0.6191281847133758 

The current subspace-distance is: 6.69499349896796e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.5; acc: 0.69
Batch: 20; loss: 1.68; acc: 0.52
Batch: 40; loss: 1.66; acc: 0.5
Batch: 60; loss: 1.69; acc: 0.48
Batch: 80; loss: 1.62; acc: 0.59
Batch: 100; loss: 1.58; acc: 0.62
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.74; acc: 0.55
Batch: 160; loss: 1.56; acc: 0.59
Batch: 180; loss: 1.47; acc: 0.62
Batch: 200; loss: 1.62; acc: 0.55
Batch: 220; loss: 1.53; acc: 0.62
Batch: 240; loss: 1.59; acc: 0.55
Batch: 260; loss: 1.62; acc: 0.62
Batch: 280; loss: 1.57; acc: 0.61
Batch: 300; loss: 1.6; acc: 0.53
Batch: 320; loss: 1.63; acc: 0.56
Batch: 340; loss: 1.65; acc: 0.52
Batch: 360; loss: 1.59; acc: 0.64
Batch: 380; loss: 1.57; acc: 0.62
Batch: 400; loss: 1.6; acc: 0.55
Batch: 420; loss: 1.44; acc: 0.7
Batch: 440; loss: 1.59; acc: 0.61
Batch: 460; loss: 1.46; acc: 0.66
Batch: 480; loss: 1.73; acc: 0.48
Batch: 500; loss: 1.65; acc: 0.56
Batch: 520; loss: 1.6; acc: 0.56
Batch: 540; loss: 1.59; acc: 0.56
Batch: 560; loss: 1.57; acc: 0.61
Batch: 580; loss: 1.52; acc: 0.64
Batch: 600; loss: 1.5; acc: 0.67
Batch: 620; loss: 1.57; acc: 0.59
Batch: 640; loss: 1.51; acc: 0.66
Batch: 660; loss: 1.68; acc: 0.61
Batch: 680; loss: 1.48; acc: 0.64
Batch: 700; loss: 1.5; acc: 0.62
Batch: 720; loss: 1.45; acc: 0.66
Batch: 740; loss: 1.52; acc: 0.64
Batch: 760; loss: 1.4; acc: 0.64
Batch: 780; loss: 1.63; acc: 0.62
Train Epoch over. train_loss: 1.57; train_accuracy: 0.6 

8.498928946210071e-05
7.890519918873906e-05
Batch: 0; loss: 1.5; acc: 0.58
Batch: 20; loss: 1.61; acc: 0.45
Batch: 40; loss: 1.29; acc: 0.8
Batch: 60; loss: 1.41; acc: 0.72
Batch: 80; loss: 1.41; acc: 0.67
Batch: 100; loss: 1.44; acc: 0.64
Batch: 120; loss: 1.52; acc: 0.64
Batch: 140; loss: 1.37; acc: 0.73
Val Epoch over. val_loss: 1.4862864419912836; val_accuracy: 0.6426154458598726 

The current subspace-distance is: 7.890519918873906e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.59
Batch: 20; loss: 1.41; acc: 0.66
Batch: 40; loss: 1.59; acc: 0.55
Batch: 60; loss: 1.43; acc: 0.69
Batch: 80; loss: 1.65; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.62
Batch: 120; loss: 1.59; acc: 0.52
Batch: 140; loss: 1.47; acc: 0.66
Batch: 160; loss: 1.48; acc: 0.55
Batch: 180; loss: 1.56; acc: 0.53
Batch: 200; loss: 1.53; acc: 0.59
Batch: 220; loss: 1.62; acc: 0.53
Batch: 240; loss: 1.52; acc: 0.58
Batch: 260; loss: 1.44; acc: 0.73
Batch: 280; loss: 1.61; acc: 0.5
Batch: 300; loss: 1.47; acc: 0.69
Batch: 320; loss: 1.54; acc: 0.62
Batch: 340; loss: 1.36; acc: 0.72
Batch: 360; loss: 1.57; acc: 0.64
Batch: 380; loss: 1.49; acc: 0.66
Batch: 400; loss: 1.53; acc: 0.62
Batch: 420; loss: 1.34; acc: 0.69
Batch: 440; loss: 1.59; acc: 0.53
Batch: 460; loss: 1.48; acc: 0.64
Batch: 480; loss: 1.4; acc: 0.66
Batch: 500; loss: 1.53; acc: 0.67
Batch: 520; loss: 1.45; acc: 0.61
Batch: 540; loss: 1.55; acc: 0.59
Batch: 560; loss: 1.52; acc: 0.61
Batch: 580; loss: 1.43; acc: 0.75
Batch: 600; loss: 1.47; acc: 0.52
Batch: 620; loss: 1.54; acc: 0.55
Batch: 640; loss: 1.46; acc: 0.64
Batch: 660; loss: 1.52; acc: 0.59
Batch: 680; loss: 1.55; acc: 0.61
Batch: 700; loss: 1.49; acc: 0.58
Batch: 720; loss: 1.46; acc: 0.61
Batch: 740; loss: 1.48; acc: 0.64
Batch: 760; loss: 1.42; acc: 0.67
Batch: 780; loss: 1.55; acc: 0.61
Train Epoch over. train_loss: 1.5; train_accuracy: 0.62 

9.655329631641507e-05
9.092938853427768e-05
Batch: 0; loss: 1.46; acc: 0.5
Batch: 20; loss: 1.56; acc: 0.55
Batch: 40; loss: 1.21; acc: 0.83
Batch: 60; loss: 1.35; acc: 0.7
Batch: 80; loss: 1.35; acc: 0.64
Batch: 100; loss: 1.38; acc: 0.69
Batch: 120; loss: 1.49; acc: 0.64
Batch: 140; loss: 1.27; acc: 0.7
Val Epoch over. val_loss: 1.4208277395576427; val_accuracy: 0.6574442675159236 

The current subspace-distance is: 9.092938853427768e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.66
Batch: 20; loss: 1.58; acc: 0.47
Batch: 40; loss: 1.46; acc: 0.67
Batch: 60; loss: 1.38; acc: 0.7
Batch: 80; loss: 1.33; acc: 0.75
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.45; acc: 0.66
Batch: 140; loss: 1.44; acc: 0.62
Batch: 160; loss: 1.53; acc: 0.58
Batch: 180; loss: 1.48; acc: 0.56
Batch: 200; loss: 1.51; acc: 0.56
Batch: 220; loss: 1.4; acc: 0.67
Batch: 240; loss: 1.37; acc: 0.7
Batch: 260; loss: 1.42; acc: 0.66
Batch: 280; loss: 1.48; acc: 0.59
Batch: 300; loss: 1.52; acc: 0.56
Batch: 320; loss: 1.43; acc: 0.62
Batch: 340; loss: 1.54; acc: 0.53
Batch: 360; loss: 1.49; acc: 0.55
Batch: 380; loss: 1.37; acc: 0.67
Batch: 400; loss: 1.42; acc: 0.66
Batch: 420; loss: 1.48; acc: 0.59
Batch: 440; loss: 1.41; acc: 0.67
Batch: 460; loss: 1.38; acc: 0.7
Batch: 480; loss: 1.42; acc: 0.64
Batch: 500; loss: 1.53; acc: 0.55
Batch: 520; loss: 1.51; acc: 0.58
Batch: 540; loss: 1.3; acc: 0.73
Batch: 560; loss: 1.39; acc: 0.69
Batch: 580; loss: 1.31; acc: 0.69
Batch: 600; loss: 1.47; acc: 0.52
Batch: 620; loss: 1.41; acc: 0.67
Batch: 640; loss: 1.52; acc: 0.61
Batch: 660; loss: 1.49; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.66
Batch: 700; loss: 1.5; acc: 0.59
Batch: 720; loss: 1.34; acc: 0.67
Batch: 740; loss: 1.34; acc: 0.72
Batch: 760; loss: 1.42; acc: 0.62
Batch: 780; loss: 1.31; acc: 0.75
Train Epoch over. train_loss: 1.44; train_accuracy: 0.63 

0.00011255404388066381
0.00010642224515322596
Batch: 0; loss: 1.44; acc: 0.5
Batch: 20; loss: 1.5; acc: 0.56
Batch: 40; loss: 1.14; acc: 0.8
Batch: 60; loss: 1.3; acc: 0.7
Batch: 80; loss: 1.28; acc: 0.66
Batch: 100; loss: 1.33; acc: 0.66
Batch: 120; loss: 1.48; acc: 0.61
Batch: 140; loss: 1.17; acc: 0.7
Val Epoch over. val_loss: 1.3569857276928652; val_accuracy: 0.6651074840764332 

The current subspace-distance is: 0.00010642224515322596 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.67
Batch: 40; loss: 1.47; acc: 0.56
Batch: 60; loss: 1.38; acc: 0.58
Batch: 80; loss: 1.46; acc: 0.62
Batch: 100; loss: 1.32; acc: 0.67
Batch: 120; loss: 1.49; acc: 0.56
Batch: 140; loss: 1.42; acc: 0.58
Batch: 160; loss: 1.44; acc: 0.64
Batch: 180; loss: 1.36; acc: 0.66
Batch: 200; loss: 1.4; acc: 0.66
Batch: 220; loss: 1.48; acc: 0.55
Batch: 240; loss: 1.29; acc: 0.72
Batch: 260; loss: 1.28; acc: 0.64
Batch: 280; loss: 1.34; acc: 0.72
Batch: 300; loss: 1.4; acc: 0.66
Batch: 320; loss: 1.4; acc: 0.67
Batch: 340; loss: 1.41; acc: 0.59
Batch: 360; loss: 1.24; acc: 0.73
Batch: 380; loss: 1.41; acc: 0.62
Batch: 400; loss: 1.37; acc: 0.59
Batch: 420; loss: 1.4; acc: 0.66
Batch: 440; loss: 1.41; acc: 0.62
Batch: 460; loss: 1.38; acc: 0.62
Batch: 480; loss: 1.33; acc: 0.62
Batch: 500; loss: 1.29; acc: 0.72
Batch: 520; loss: 1.4; acc: 0.66
Batch: 540; loss: 1.38; acc: 0.7
Batch: 560; loss: 1.29; acc: 0.62
Batch: 580; loss: 1.26; acc: 0.67
Batch: 600; loss: 1.34; acc: 0.73
Batch: 620; loss: 1.22; acc: 0.7
Batch: 640; loss: 1.27; acc: 0.7
Batch: 660; loss: 1.38; acc: 0.62
Batch: 680; loss: 1.39; acc: 0.64
Batch: 700; loss: 1.45; acc: 0.55
Batch: 720; loss: 1.39; acc: 0.62
Batch: 740; loss: 1.33; acc: 0.69
Batch: 760; loss: 1.41; acc: 0.66
Batch: 780; loss: 1.32; acc: 0.69
Train Epoch over. train_loss: 1.38; train_accuracy: 0.65 

0.00012458280252758414
0.00012004145537503064
Batch: 0; loss: 1.42; acc: 0.61
Batch: 20; loss: 1.43; acc: 0.56
Batch: 40; loss: 1.07; acc: 0.8
Batch: 60; loss: 1.25; acc: 0.72
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.29; acc: 0.64
Batch: 120; loss: 1.45; acc: 0.61
Batch: 140; loss: 1.1; acc: 0.7
Val Epoch over. val_loss: 1.302110118091486; val_accuracy: 0.6785429936305732 

The current subspace-distance is: 0.00012004145537503064 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.7
Batch: 20; loss: 1.47; acc: 0.56
Batch: 40; loss: 1.3; acc: 0.73
Batch: 60; loss: 1.38; acc: 0.64
Batch: 80; loss: 1.41; acc: 0.56
Batch: 100; loss: 1.28; acc: 0.66
Batch: 120; loss: 1.28; acc: 0.7
Batch: 140; loss: 1.28; acc: 0.67
Batch: 160; loss: 1.31; acc: 0.69
Batch: 180; loss: 1.41; acc: 0.55
Batch: 200; loss: 1.32; acc: 0.64
Batch: 220; loss: 1.4; acc: 0.58
Batch: 240; loss: 1.48; acc: 0.56
Batch: 260; loss: 1.3; acc: 0.69
Batch: 280; loss: 1.35; acc: 0.69
Batch: 300; loss: 1.31; acc: 0.69
Batch: 320; loss: 1.22; acc: 0.78
Batch: 340; loss: 1.29; acc: 0.7
Batch: 360; loss: 1.37; acc: 0.61
Batch: 380; loss: 1.36; acc: 0.62
Batch: 400; loss: 1.21; acc: 0.7
Batch: 420; loss: 1.36; acc: 0.62
Batch: 440; loss: 1.45; acc: 0.59
Batch: 460; loss: 1.33; acc: 0.66
Batch: 480; loss: 1.24; acc: 0.69
Batch: 500; loss: 1.27; acc: 0.7
Batch: 520; loss: 1.18; acc: 0.75
Batch: 540; loss: 1.25; acc: 0.62
Batch: 560; loss: 1.27; acc: 0.73
Batch: 580; loss: 1.31; acc: 0.69
Batch: 600; loss: 1.27; acc: 0.69
Batch: 620; loss: 1.34; acc: 0.61
Batch: 640; loss: 1.38; acc: 0.69
Batch: 660; loss: 1.32; acc: 0.66
Batch: 680; loss: 1.36; acc: 0.66
Batch: 700; loss: 1.32; acc: 0.67
Batch: 720; loss: 1.26; acc: 0.72
Batch: 740; loss: 1.3; acc: 0.61
Batch: 760; loss: 1.36; acc: 0.61
Batch: 780; loss: 1.33; acc: 0.61
Train Epoch over. train_loss: 1.33; train_accuracy: 0.65 

0.0001348520308965817
0.00013014355499763042
Batch: 0; loss: 1.39; acc: 0.61
Batch: 20; loss: 1.39; acc: 0.52
Batch: 40; loss: 1.03; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.73
Batch: 100; loss: 1.27; acc: 0.64
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 1.07; acc: 0.72
Val Epoch over. val_loss: 1.270038485906686; val_accuracy: 0.6833200636942676 

The current subspace-distance is: 0.00013014355499763042 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.32; acc: 0.64
Batch: 40; loss: 1.42; acc: 0.52
Batch: 60; loss: 1.2; acc: 0.73
Batch: 80; loss: 1.28; acc: 0.7
Batch: 100; loss: 1.43; acc: 0.62
Batch: 120; loss: 1.27; acc: 0.61
Batch: 140; loss: 1.37; acc: 0.62
Batch: 160; loss: 1.46; acc: 0.59
Batch: 180; loss: 1.27; acc: 0.67
Batch: 200; loss: 1.28; acc: 0.58
Batch: 220; loss: 1.38; acc: 0.61
Batch: 240; loss: 1.27; acc: 0.59
Batch: 260; loss: 1.32; acc: 0.53
Batch: 280; loss: 1.35; acc: 0.64
Batch: 300; loss: 1.31; acc: 0.59
Batch: 320; loss: 1.25; acc: 0.64
Batch: 340; loss: 1.35; acc: 0.61
Batch: 360; loss: 1.17; acc: 0.75
Batch: 380; loss: 1.25; acc: 0.67
Batch: 400; loss: 1.32; acc: 0.61
Batch: 420; loss: 1.3; acc: 0.72
Batch: 440; loss: 1.35; acc: 0.66
Batch: 460; loss: 1.26; acc: 0.67
Batch: 480; loss: 1.39; acc: 0.59
Batch: 500; loss: 1.37; acc: 0.61
Batch: 520; loss: 1.31; acc: 0.66
Batch: 540; loss: 1.22; acc: 0.69
Batch: 560; loss: 1.27; acc: 0.69
Batch: 580; loss: 1.35; acc: 0.64
Batch: 600; loss: 1.28; acc: 0.67
Batch: 620; loss: 1.26; acc: 0.72
Batch: 640; loss: 1.25; acc: 0.73
Batch: 660; loss: 1.27; acc: 0.66
Batch: 680; loss: 1.31; acc: 0.59
Batch: 700; loss: 1.42; acc: 0.59
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.18; acc: 0.67
Batch: 760; loss: 1.45; acc: 0.61
Batch: 780; loss: 1.29; acc: 0.59
Train Epoch over. train_loss: 1.3; train_accuracy: 0.66 

0.00014603698218706995
0.000142062155646272
Batch: 0; loss: 1.35; acc: 0.61
Batch: 20; loss: 1.35; acc: 0.53
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 1.2; acc: 0.75
Batch: 80; loss: 1.1; acc: 0.77
Batch: 100; loss: 1.25; acc: 0.62
Batch: 120; loss: 1.39; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.69
Val Epoch over. val_loss: 1.231288831704741; val_accuracy: 0.691281847133758 

The current subspace-distance is: 0.000142062155646272 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.17; acc: 0.72
Batch: 20; loss: 1.26; acc: 0.73
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.14; acc: 0.75
Batch: 80; loss: 1.37; acc: 0.62
Batch: 100; loss: 1.29; acc: 0.69
Batch: 120; loss: 1.21; acc: 0.67
Batch: 140; loss: 1.17; acc: 0.73
Batch: 160; loss: 1.21; acc: 0.75
Batch: 180; loss: 1.17; acc: 0.77
Batch: 200; loss: 1.16; acc: 0.8
Batch: 220; loss: 1.23; acc: 0.62
Batch: 240; loss: 1.29; acc: 0.66
Batch: 260; loss: 1.37; acc: 0.58
Batch: 280; loss: 1.22; acc: 0.72
Batch: 300; loss: 1.13; acc: 0.8
Batch: 320; loss: 1.29; acc: 0.67
Batch: 340; loss: 1.26; acc: 0.66
Batch: 360; loss: 1.09; acc: 0.81
Batch: 380; loss: 1.33; acc: 0.67
Batch: 400; loss: 1.35; acc: 0.59
Batch: 420; loss: 1.25; acc: 0.61
Batch: 440; loss: 1.22; acc: 0.67
Batch: 460; loss: 1.26; acc: 0.7
Batch: 480; loss: 1.14; acc: 0.77
Batch: 500; loss: 1.26; acc: 0.69
Batch: 520; loss: 1.27; acc: 0.64
Batch: 540; loss: 1.29; acc: 0.64
Batch: 560; loss: 1.27; acc: 0.61
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.49; acc: 0.47
Batch: 620; loss: 1.19; acc: 0.72
Batch: 640; loss: 1.2; acc: 0.73
Batch: 660; loss: 1.26; acc: 0.72
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.35; acc: 0.67
Batch: 720; loss: 1.18; acc: 0.73
Batch: 740; loss: 1.32; acc: 0.62
Batch: 760; loss: 1.11; acc: 0.75
Batch: 780; loss: 1.29; acc: 0.55
Train Epoch over. train_loss: 1.27; train_accuracy: 0.66 

0.0001519699435448274
0.0001462516956962645
Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 1.34; acc: 0.53
Batch: 40; loss: 0.96; acc: 0.8
Batch: 60; loss: 1.18; acc: 0.75
Batch: 80; loss: 1.07; acc: 0.8
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.38; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.73
Val Epoch over. val_loss: 1.2088285904781075; val_accuracy: 0.693968949044586 

The current subspace-distance is: 0.0001462516956962645 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.17; acc: 0.78
Batch: 20; loss: 1.28; acc: 0.67
Batch: 40; loss: 1.22; acc: 0.62
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.29; acc: 0.61
Batch: 100; loss: 1.29; acc: 0.72
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.62
Batch: 160; loss: 1.22; acc: 0.69
Batch: 180; loss: 1.2; acc: 0.61
Batch: 200; loss: 1.39; acc: 0.53
Batch: 220; loss: 1.26; acc: 0.58
Batch: 240; loss: 1.18; acc: 0.7
Batch: 260; loss: 1.25; acc: 0.69
Batch: 280; loss: 1.49; acc: 0.53
Batch: 300; loss: 1.5; acc: 0.56
Batch: 320; loss: 1.18; acc: 0.72
Batch: 340; loss: 1.33; acc: 0.69
Batch: 360; loss: 1.23; acc: 0.66
Batch: 380; loss: 1.21; acc: 0.62
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.4; acc: 0.58
Batch: 440; loss: 1.28; acc: 0.69
Batch: 460; loss: 1.14; acc: 0.69
Batch: 480; loss: 1.28; acc: 0.58
Batch: 500; loss: 1.27; acc: 0.66
Batch: 520; loss: 1.12; acc: 0.77
Batch: 540; loss: 1.41; acc: 0.48
Batch: 560; loss: 1.22; acc: 0.75
Batch: 580; loss: 1.26; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.72
Batch: 620; loss: 1.18; acc: 0.64
Batch: 640; loss: 1.19; acc: 0.61
Batch: 660; loss: 1.24; acc: 0.61
Batch: 680; loss: 1.29; acc: 0.62
Batch: 700; loss: 1.25; acc: 0.69
Batch: 720; loss: 1.26; acc: 0.67
Batch: 740; loss: 1.39; acc: 0.53
Batch: 760; loss: 1.16; acc: 0.69
Batch: 780; loss: 1.32; acc: 0.61
Train Epoch over. train_loss: 1.25; train_accuracy: 0.66 

0.00015953968977555633
0.00015379501564893872
Batch: 0; loss: 1.28; acc: 0.61
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.81
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.78
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.62
Batch: 140; loss: 1.01; acc: 0.73
Val Epoch over. val_loss: 1.1856057419898405; val_accuracy: 0.6928742038216561 

The current subspace-distance is: 0.00015379501564893872 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.22; acc: 0.72
Batch: 40; loss: 1.32; acc: 0.59
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.37; acc: 0.56
Batch: 100; loss: 1.34; acc: 0.62
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 1.1; acc: 0.75
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.21; acc: 0.69
Batch: 200; loss: 1.24; acc: 0.69
Batch: 220; loss: 1.27; acc: 0.62
Batch: 240; loss: 1.31; acc: 0.69
Batch: 260; loss: 1.19; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.78
Batch: 300; loss: 1.3; acc: 0.56
Batch: 320; loss: 1.22; acc: 0.62
Batch: 340; loss: 1.12; acc: 0.72
Batch: 360; loss: 1.15; acc: 0.72
Batch: 380; loss: 1.16; acc: 0.73
Batch: 400; loss: 1.38; acc: 0.55
Batch: 420; loss: 1.34; acc: 0.62
Batch: 440; loss: 1.31; acc: 0.56
Batch: 460; loss: 1.17; acc: 0.7
Batch: 480; loss: 1.28; acc: 0.59
Batch: 500; loss: 1.32; acc: 0.62
Batch: 520; loss: 1.11; acc: 0.7
Batch: 540; loss: 1.38; acc: 0.64
Batch: 560; loss: 1.32; acc: 0.61
Batch: 580; loss: 1.34; acc: 0.62
Batch: 600; loss: 1.17; acc: 0.69
Batch: 620; loss: 1.27; acc: 0.61
Batch: 640; loss: 1.23; acc: 0.66
Batch: 660; loss: 1.31; acc: 0.59
Batch: 680; loss: 1.29; acc: 0.64
Batch: 700; loss: 1.31; acc: 0.58
Batch: 720; loss: 1.17; acc: 0.69
Batch: 740; loss: 1.28; acc: 0.67
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.16; acc: 0.64
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.00016079324996098876
0.0001561992394272238
Batch: 0; loss: 1.27; acc: 0.55
Batch: 20; loss: 1.33; acc: 0.56
Batch: 40; loss: 0.93; acc: 0.78
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.77
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 1.02; acc: 0.75
Val Epoch over. val_loss: 1.1941276234426317; val_accuracy: 0.6808320063694268 

The current subspace-distance is: 0.0001561992394272238 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.18; acc: 0.72
Batch: 80; loss: 0.95; acc: 0.86
Batch: 100; loss: 1.41; acc: 0.56
Batch: 120; loss: 1.15; acc: 0.7
Batch: 140; loss: 1.12; acc: 0.75
Batch: 160; loss: 1.24; acc: 0.64
Batch: 180; loss: 1.08; acc: 0.78
Batch: 200; loss: 1.07; acc: 0.78
Batch: 220; loss: 1.24; acc: 0.58
Batch: 240; loss: 1.29; acc: 0.66
Batch: 260; loss: 1.14; acc: 0.7
Batch: 280; loss: 1.5; acc: 0.5
Batch: 300; loss: 1.27; acc: 0.59
Batch: 320; loss: 1.46; acc: 0.52
Batch: 340; loss: 1.36; acc: 0.62
Batch: 360; loss: 1.21; acc: 0.66
Batch: 380; loss: 1.19; acc: 0.67
Batch: 400; loss: 1.38; acc: 0.55
Batch: 420; loss: 1.22; acc: 0.66
Batch: 440; loss: 1.27; acc: 0.66
Batch: 460; loss: 1.27; acc: 0.62
Batch: 480; loss: 1.3; acc: 0.53
Batch: 500; loss: 1.2; acc: 0.7
Batch: 520; loss: 1.1; acc: 0.77
Batch: 540; loss: 1.07; acc: 0.78
Batch: 560; loss: 1.21; acc: 0.7
Batch: 580; loss: 1.26; acc: 0.72
Batch: 600; loss: 1.14; acc: 0.7
Batch: 620; loss: 1.32; acc: 0.56
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 1.38; acc: 0.66
Batch: 680; loss: 1.22; acc: 0.61
Batch: 700; loss: 1.47; acc: 0.53
Batch: 720; loss: 1.38; acc: 0.5
Batch: 740; loss: 1.29; acc: 0.64
Batch: 760; loss: 1.25; acc: 0.7
Batch: 780; loss: 1.31; acc: 0.58
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00016666633018758148
0.00016144447727128863
Batch: 0; loss: 1.26; acc: 0.58
Batch: 20; loss: 1.3; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.78
Batch: 60; loss: 1.14; acc: 0.72
Batch: 80; loss: 1.04; acc: 0.8
Batch: 100; loss: 1.2; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.73
Val Epoch over. val_loss: 1.1768096305762128; val_accuracy: 0.6928742038216561 

The current subspace-distance is: 0.00016144447727128863 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 1.13; acc: 0.73
Batch: 60; loss: 1.22; acc: 0.7
Batch: 80; loss: 1.22; acc: 0.73
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.04; acc: 0.75
Batch: 140; loss: 1.24; acc: 0.62
Batch: 160; loss: 1.26; acc: 0.64
Batch: 180; loss: 1.29; acc: 0.69
Batch: 200; loss: 1.12; acc: 0.77
Batch: 220; loss: 1.12; acc: 0.72
Batch: 240; loss: 1.34; acc: 0.58
Batch: 260; loss: 1.3; acc: 0.62
Batch: 280; loss: 1.44; acc: 0.56
Batch: 300; loss: 1.13; acc: 0.72
Batch: 320; loss: 1.23; acc: 0.67
Batch: 340; loss: 1.1; acc: 0.7
Batch: 360; loss: 1.07; acc: 0.72
Batch: 380; loss: 1.12; acc: 0.73
Batch: 400; loss: 1.39; acc: 0.55
Batch: 420; loss: 1.32; acc: 0.59
Batch: 440; loss: 1.34; acc: 0.61
Batch: 460; loss: 1.3; acc: 0.59
Batch: 480; loss: 1.1; acc: 0.72
Batch: 500; loss: 1.08; acc: 0.73
Batch: 520; loss: 1.26; acc: 0.7
Batch: 540; loss: 1.33; acc: 0.66
Batch: 560; loss: 1.11; acc: 0.72
Batch: 580; loss: 1.19; acc: 0.7
Batch: 600; loss: 1.08; acc: 0.75
Batch: 620; loss: 1.28; acc: 0.61
Batch: 640; loss: 1.15; acc: 0.67
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.2; acc: 0.7
Batch: 700; loss: 1.19; acc: 0.69
Batch: 720; loss: 1.32; acc: 0.62
Batch: 740; loss: 1.29; acc: 0.66
Batch: 760; loss: 1.23; acc: 0.66
Batch: 780; loss: 0.98; acc: 0.83
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.0001659370173001662
0.0001584102283231914
Batch: 0; loss: 1.25; acc: 0.64
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 0.91; acc: 0.78
Batch: 60; loss: 1.12; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 1.0; acc: 0.77
Val Epoch over. val_loss: 1.1641866553361249; val_accuracy: 0.6987460191082803 

The current subspace-distance is: 0.0001584102283231914 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.69
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 1.26; acc: 0.62
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.58
Batch: 100; loss: 1.41; acc: 0.56
Batch: 120; loss: 1.27; acc: 0.69
Batch: 140; loss: 1.16; acc: 0.7
Batch: 160; loss: 1.24; acc: 0.64
Batch: 180; loss: 1.16; acc: 0.69
Batch: 200; loss: 1.24; acc: 0.67
Batch: 220; loss: 1.29; acc: 0.67
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 1.34; acc: 0.58
Batch: 280; loss: 1.15; acc: 0.7
Batch: 300; loss: 1.22; acc: 0.67
Batch: 320; loss: 1.17; acc: 0.73
Batch: 340; loss: 1.01; acc: 0.8
Batch: 360; loss: 1.1; acc: 0.75
Batch: 380; loss: 1.22; acc: 0.64
Batch: 400; loss: 1.24; acc: 0.62
Batch: 420; loss: 1.08; acc: 0.77
Batch: 440; loss: 1.29; acc: 0.58
Batch: 460; loss: 1.42; acc: 0.58
Batch: 480; loss: 1.26; acc: 0.66
Batch: 500; loss: 1.37; acc: 0.58
Batch: 520; loss: 1.21; acc: 0.7
Batch: 540; loss: 1.2; acc: 0.69
Batch: 560; loss: 1.14; acc: 0.69
Batch: 580; loss: 1.22; acc: 0.66
Batch: 600; loss: 1.24; acc: 0.61
Batch: 620; loss: 1.2; acc: 0.7
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 1.18; acc: 0.61
Batch: 680; loss: 1.32; acc: 0.59
Batch: 700; loss: 1.08; acc: 0.78
Batch: 720; loss: 1.16; acc: 0.7
Batch: 740; loss: 1.17; acc: 0.69
Batch: 760; loss: 1.27; acc: 0.61
Batch: 780; loss: 1.15; acc: 0.7
Train Epoch over. train_loss: 1.22; train_accuracy: 0.66 

0.0001678495027590543
0.0001608343154657632
Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.32; acc: 0.52
Batch: 40; loss: 0.89; acc: 0.81
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.78
Batch: 100; loss: 1.2; acc: 0.69
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.73
Val Epoch over. val_loss: 1.1607074809681839; val_accuracy: 0.6941679936305732 

The current subspace-distance is: 0.0001608343154657632 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 1.15; acc: 0.62
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.22; acc: 0.62
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.22; acc: 0.73
Batch: 140; loss: 1.21; acc: 0.59
Batch: 160; loss: 1.1; acc: 0.75
Batch: 180; loss: 1.28; acc: 0.62
Batch: 200; loss: 1.29; acc: 0.67
Batch: 220; loss: 1.25; acc: 0.64
Batch: 240; loss: 1.13; acc: 0.7
Batch: 260; loss: 1.14; acc: 0.72
Batch: 280; loss: 1.31; acc: 0.59
Batch: 300; loss: 1.16; acc: 0.72
Batch: 320; loss: 1.17; acc: 0.7
Batch: 340; loss: 1.24; acc: 0.7
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.26; acc: 0.69
Batch: 400; loss: 1.01; acc: 0.81
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.15; acc: 0.72
Batch: 460; loss: 1.12; acc: 0.62
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.21; acc: 0.66
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.29; acc: 0.59
Batch: 580; loss: 1.36; acc: 0.67
Batch: 600; loss: 1.09; acc: 0.73
Batch: 620; loss: 1.06; acc: 0.77
Batch: 640; loss: 1.2; acc: 0.64
Batch: 660; loss: 1.34; acc: 0.56
Batch: 680; loss: 1.24; acc: 0.69
Batch: 700; loss: 1.17; acc: 0.64
Batch: 720; loss: 1.16; acc: 0.7
Batch: 740; loss: 1.22; acc: 0.67
Batch: 760; loss: 1.24; acc: 0.61
Batch: 780; loss: 1.19; acc: 0.66
Train Epoch over. train_loss: 1.22; train_accuracy: 0.66 

0.00016819681331980973
0.0001611615443835035
Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.33; acc: 0.52
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 1.21; acc: 0.66
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.75
Val Epoch over. val_loss: 1.1649414057944232; val_accuracy: 0.6948646496815286 

The current subspace-distance is: 0.0001611615443835035 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.72
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 1.18; acc: 0.64
Batch: 60; loss: 1.18; acc: 0.73
Batch: 80; loss: 1.21; acc: 0.64
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 1.3; acc: 0.59
Batch: 160; loss: 1.13; acc: 0.7
Batch: 180; loss: 1.27; acc: 0.67
Batch: 200; loss: 1.2; acc: 0.64
Batch: 220; loss: 1.22; acc: 0.7
Batch: 240; loss: 1.18; acc: 0.72
Batch: 260; loss: 1.37; acc: 0.59
Batch: 280; loss: 1.26; acc: 0.66
Batch: 300; loss: 1.21; acc: 0.64
Batch: 320; loss: 1.19; acc: 0.67
Batch: 340; loss: 1.22; acc: 0.62
Batch: 360; loss: 1.23; acc: 0.64
Batch: 380; loss: 1.17; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.69
Batch: 420; loss: 1.18; acc: 0.67
Batch: 440; loss: 1.16; acc: 0.62
Batch: 460; loss: 1.07; acc: 0.75
Batch: 480; loss: 1.15; acc: 0.72
Batch: 500; loss: 1.29; acc: 0.64
Batch: 520; loss: 1.31; acc: 0.64
Batch: 540; loss: 1.33; acc: 0.64
Batch: 560; loss: 1.26; acc: 0.61
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.24; acc: 0.69
Batch: 620; loss: 1.14; acc: 0.75
Batch: 640; loss: 1.23; acc: 0.7
Batch: 660; loss: 1.15; acc: 0.69
Batch: 680; loss: 1.05; acc: 0.75
Batch: 700; loss: 1.41; acc: 0.56
Batch: 720; loss: 1.04; acc: 0.75
Batch: 740; loss: 1.32; acc: 0.59
Batch: 760; loss: 1.33; acc: 0.58
Batch: 780; loss: 1.42; acc: 0.56
Train Epoch over. train_loss: 1.21; train_accuracy: 0.66 

0.00017151772044599056
0.0001667698088567704
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 1.31; acc: 0.53
Batch: 40; loss: 0.89; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.75
Batch: 80; loss: 1.01; acc: 0.78
Batch: 100; loss: 1.18; acc: 0.66
Batch: 120; loss: 1.33; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.75
Val Epoch over. val_loss: 1.153501150714364; val_accuracy: 0.6980493630573248 

The current subspace-distance is: 0.0001667698088567704 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.64
Batch: 20; loss: 1.36; acc: 0.56
Batch: 40; loss: 1.05; acc: 0.73
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.33; acc: 0.59
Batch: 100; loss: 1.28; acc: 0.61
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 1.14; acc: 0.7
Batch: 160; loss: 1.3; acc: 0.62
Batch: 180; loss: 1.18; acc: 0.73
Batch: 200; loss: 1.27; acc: 0.69
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 1.32; acc: 0.59
Batch: 260; loss: 1.2; acc: 0.64
Batch: 280; loss: 1.22; acc: 0.72
Batch: 300; loss: 1.17; acc: 0.62
Batch: 320; loss: 1.34; acc: 0.53
Batch: 340; loss: 1.03; acc: 0.75
Batch: 360; loss: 1.2; acc: 0.64
Batch: 380; loss: 1.42; acc: 0.59
Batch: 400; loss: 1.16; acc: 0.67
Batch: 420; loss: 1.16; acc: 0.7
Batch: 440; loss: 1.19; acc: 0.59
Batch: 460; loss: 1.23; acc: 0.66
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.22; acc: 0.64
Batch: 520; loss: 1.2; acc: 0.69
Batch: 540; loss: 1.25; acc: 0.61
Batch: 560; loss: 1.01; acc: 0.78
Batch: 580; loss: 1.3; acc: 0.58
Batch: 600; loss: 1.07; acc: 0.81
Batch: 620; loss: 1.07; acc: 0.73
Batch: 640; loss: 1.18; acc: 0.62
Batch: 660; loss: 1.26; acc: 0.66
Batch: 680; loss: 0.99; acc: 0.86
Batch: 700; loss: 1.17; acc: 0.64
Batch: 720; loss: 1.32; acc: 0.55
Batch: 740; loss: 1.3; acc: 0.56
Batch: 760; loss: 1.13; acc: 0.67
Batch: 780; loss: 1.14; acc: 0.69
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

0.00017497219960205257
0.00016791671805549413
Batch: 0; loss: 1.22; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 1.18; acc: 0.67
Batch: 120; loss: 1.32; acc: 0.62
Batch: 140; loss: 0.99; acc: 0.75
Val Epoch over. val_loss: 1.1488006088384397; val_accuracy: 0.6975517515923567 

The current subspace-distance is: 0.00016791671805549413 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.14; acc: 0.75
Batch: 20; loss: 0.96; acc: 0.77
Batch: 40; loss: 1.17; acc: 0.64
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.25; acc: 0.66
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 1.08; acc: 0.73
Batch: 160; loss: 1.24; acc: 0.7
Batch: 180; loss: 1.37; acc: 0.58
Batch: 200; loss: 1.2; acc: 0.61
Batch: 220; loss: 1.1; acc: 0.72
Batch: 240; loss: 1.11; acc: 0.64
Batch: 260; loss: 1.06; acc: 0.73
Batch: 280; loss: 1.09; acc: 0.73
Batch: 300; loss: 1.07; acc: 0.7
Batch: 320; loss: 1.15; acc: 0.66
Batch: 340; loss: 1.01; acc: 0.72
Batch: 360; loss: 1.19; acc: 0.69
Batch: 380; loss: 1.29; acc: 0.56
Batch: 400; loss: 1.46; acc: 0.5
Batch: 420; loss: 1.21; acc: 0.7
Batch: 440; loss: 1.23; acc: 0.66
Batch: 460; loss: 1.13; acc: 0.69
Batch: 480; loss: 1.16; acc: 0.69
Batch: 500; loss: 1.2; acc: 0.66
Batch: 520; loss: 1.07; acc: 0.7
Batch: 540; loss: 1.22; acc: 0.64
Batch: 560; loss: 1.25; acc: 0.62
Batch: 580; loss: 1.47; acc: 0.47
Batch: 600; loss: 1.24; acc: 0.69
Batch: 620; loss: 1.31; acc: 0.58
Batch: 640; loss: 1.26; acc: 0.69
Batch: 660; loss: 1.38; acc: 0.66
Batch: 680; loss: 1.18; acc: 0.67
Batch: 700; loss: 1.23; acc: 0.69
Batch: 720; loss: 1.2; acc: 0.62
Batch: 740; loss: 1.46; acc: 0.55
Batch: 760; loss: 1.28; acc: 0.59
Batch: 780; loss: 1.21; acc: 0.7
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

0.00017659802688285708
0.00017119603580795228
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.31; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.73
Batch: 100; loss: 1.19; acc: 0.66
Batch: 120; loss: 1.33; acc: 0.61
Batch: 140; loss: 0.99; acc: 0.77
Val Epoch over. val_loss: 1.1484605438390356; val_accuracy: 0.6978503184713376 

The current subspace-distance is: 0.00017119603580795228 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.64
Batch: 20; loss: 1.25; acc: 0.64
Batch: 40; loss: 1.18; acc: 0.66
Batch: 60; loss: 1.31; acc: 0.62
Batch: 80; loss: 1.16; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.17; acc: 0.7
Batch: 160; loss: 1.0; acc: 0.75
Batch: 180; loss: 1.19; acc: 0.66
Batch: 200; loss: 1.07; acc: 0.72
Batch: 220; loss: 1.14; acc: 0.72
Batch: 240; loss: 1.19; acc: 0.69
Batch: 260; loss: 1.25; acc: 0.61
Batch: 280; loss: 1.13; acc: 0.69
Batch: 300; loss: 1.41; acc: 0.5
Batch: 320; loss: 1.25; acc: 0.58
Batch: 340; loss: 1.13; acc: 0.75
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.11; acc: 0.75
Batch: 400; loss: 1.15; acc: 0.72
Batch: 420; loss: 1.19; acc: 0.66
Batch: 440; loss: 1.11; acc: 0.78
Batch: 460; loss: 1.15; acc: 0.66
Batch: 480; loss: 1.47; acc: 0.55
Batch: 500; loss: 1.08; acc: 0.8
Batch: 520; loss: 1.09; acc: 0.75
Batch: 540; loss: 1.16; acc: 0.75
Batch: 560; loss: 1.17; acc: 0.72
Batch: 580; loss: 1.22; acc: 0.62
Batch: 600; loss: 1.29; acc: 0.62
Batch: 620; loss: 1.12; acc: 0.69
Batch: 640; loss: 1.45; acc: 0.59
Batch: 660; loss: 1.07; acc: 0.77
Batch: 680; loss: 1.15; acc: 0.67
Batch: 700; loss: 1.13; acc: 0.7
Batch: 720; loss: 1.15; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.67
Batch: 760; loss: 1.19; acc: 0.75
Batch: 780; loss: 1.18; acc: 0.67
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

0.00017773405124899
0.0001727285998640582
Batch: 0; loss: 1.22; acc: 0.69
Batch: 20; loss: 1.3; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.75
Batch: 100; loss: 1.18; acc: 0.64
Batch: 120; loss: 1.32; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.75
Val Epoch over. val_loss: 1.1420668853316338; val_accuracy: 0.6995421974522293 

The current subspace-distance is: 0.0001727285998640582 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 1.43; acc: 0.55
Batch: 60; loss: 1.27; acc: 0.66
Batch: 80; loss: 1.3; acc: 0.61
Batch: 100; loss: 1.13; acc: 0.69
Batch: 120; loss: 1.07; acc: 0.72
Batch: 140; loss: 1.16; acc: 0.67
Batch: 160; loss: 1.11; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.19; acc: 0.66
Batch: 220; loss: 1.31; acc: 0.64
Batch: 240; loss: 1.2; acc: 0.62
Batch: 260; loss: 1.12; acc: 0.7
Batch: 280; loss: 1.34; acc: 0.62
Batch: 300; loss: 1.16; acc: 0.62
Batch: 320; loss: 1.21; acc: 0.66
Batch: 340; loss: 1.2; acc: 0.69
Batch: 360; loss: 1.08; acc: 0.75
Batch: 380; loss: 1.12; acc: 0.73
Batch: 400; loss: 1.27; acc: 0.62
Batch: 420; loss: 1.14; acc: 0.72
Batch: 440; loss: 1.2; acc: 0.67
Batch: 460; loss: 1.15; acc: 0.67
Batch: 480; loss: 1.23; acc: 0.58
Batch: 500; loss: 1.15; acc: 0.67
Batch: 520; loss: 1.19; acc: 0.69
Batch: 540; loss: 1.13; acc: 0.62
Batch: 560; loss: 1.38; acc: 0.59
Batch: 580; loss: 1.05; acc: 0.73
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.29; acc: 0.61
Batch: 640; loss: 1.04; acc: 0.7
Batch: 660; loss: 1.23; acc: 0.67
Batch: 680; loss: 1.13; acc: 0.72
Batch: 700; loss: 1.43; acc: 0.56
Batch: 720; loss: 1.17; acc: 0.61
Batch: 740; loss: 1.06; acc: 0.8
Batch: 760; loss: 1.23; acc: 0.61
Batch: 780; loss: 1.31; acc: 0.64
Train Epoch over. train_loss: 1.19; train_accuracy: 0.66 

0.00018260098295286298
0.0001744488690746948
Batch: 0; loss: 1.2; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 0.97; acc: 0.77
Val Epoch over. val_loss: 1.1316356810794514; val_accuracy: 0.6951632165605095 

The current subspace-distance is: 0.0001744488690746948 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.15; acc: 0.7
Batch: 20; loss: 1.45; acc: 0.52
Batch: 40; loss: 1.09; acc: 0.73
Batch: 60; loss: 1.06; acc: 0.77
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 1.3; acc: 0.66
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 1.26; acc: 0.61
Batch: 160; loss: 1.2; acc: 0.67
Batch: 180; loss: 1.16; acc: 0.67
Batch: 200; loss: 1.09; acc: 0.7
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.19; acc: 0.59
Batch: 280; loss: 1.22; acc: 0.62
Batch: 300; loss: 1.29; acc: 0.64
Batch: 320; loss: 1.29; acc: 0.62
Batch: 340; loss: 1.17; acc: 0.69
Batch: 360; loss: 1.29; acc: 0.62
Batch: 380; loss: 1.26; acc: 0.53
Batch: 400; loss: 1.31; acc: 0.64
Batch: 420; loss: 1.33; acc: 0.55
Batch: 440; loss: 1.34; acc: 0.59
Batch: 460; loss: 1.16; acc: 0.7
Batch: 480; loss: 1.29; acc: 0.62
Batch: 500; loss: 1.04; acc: 0.77
Batch: 520; loss: 1.19; acc: 0.66
Batch: 540; loss: 1.24; acc: 0.64
Batch: 560; loss: 1.14; acc: 0.64
Batch: 580; loss: 1.28; acc: 0.59
Batch: 600; loss: 1.27; acc: 0.58
Batch: 620; loss: 1.21; acc: 0.61
Batch: 640; loss: 1.28; acc: 0.56
Batch: 660; loss: 1.44; acc: 0.53
Batch: 680; loss: 1.12; acc: 0.73
Batch: 700; loss: 1.44; acc: 0.48
Batch: 720; loss: 1.17; acc: 0.72
Batch: 740; loss: 1.07; acc: 0.73
Batch: 760; loss: 1.13; acc: 0.69
Batch: 780; loss: 1.15; acc: 0.67
Train Epoch over. train_loss: 1.19; train_accuracy: 0.66 

0.00018115279090125114
0.0001735498517518863
Batch: 0; loss: 1.2; acc: 0.7
Batch: 20; loss: 1.28; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.64
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 0.98; acc: 0.73
Val Epoch over. val_loss: 1.1296620513223539; val_accuracy: 0.6956608280254777 

The current subspace-distance is: 0.0001735498517518863 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 1.1; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 1.11; acc: 0.69
Batch: 120; loss: 1.08; acc: 0.77
Batch: 140; loss: 1.25; acc: 0.69
Batch: 160; loss: 1.21; acc: 0.64
Batch: 180; loss: 1.4; acc: 0.45
Batch: 200; loss: 1.18; acc: 0.62
Batch: 220; loss: 1.15; acc: 0.69
Batch: 240; loss: 1.29; acc: 0.61
Batch: 260; loss: 1.29; acc: 0.62
Batch: 280; loss: 1.13; acc: 0.7
Batch: 300; loss: 1.19; acc: 0.62
Batch: 320; loss: 1.38; acc: 0.58
Batch: 340; loss: 1.07; acc: 0.72
Batch: 360; loss: 1.29; acc: 0.66
Batch: 380; loss: 1.09; acc: 0.69
Batch: 400; loss: 1.2; acc: 0.59
Batch: 420; loss: 1.07; acc: 0.7
Batch: 440; loss: 1.18; acc: 0.7
Batch: 460; loss: 1.3; acc: 0.62
Batch: 480; loss: 1.03; acc: 0.73
Batch: 500; loss: 1.1; acc: 0.69
Batch: 520; loss: 1.24; acc: 0.66
Batch: 540; loss: 1.22; acc: 0.7
Batch: 560; loss: 1.28; acc: 0.66
Batch: 580; loss: 1.22; acc: 0.64
Batch: 600; loss: 1.07; acc: 0.75
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.26; acc: 0.64
Batch: 660; loss: 1.24; acc: 0.61
Batch: 680; loss: 1.15; acc: 0.69
Batch: 700; loss: 1.16; acc: 0.72
Batch: 720; loss: 1.13; acc: 0.75
Batch: 740; loss: 1.34; acc: 0.59
Batch: 760; loss: 1.28; acc: 0.59
Batch: 780; loss: 1.33; acc: 0.62
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.00018199777696281672
0.00017612258670851588
Batch: 0; loss: 1.2; acc: 0.73
Batch: 20; loss: 1.28; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 0.97; acc: 0.78
Val Epoch over. val_loss: 1.12717178369024; val_accuracy: 0.7032245222929936 

The current subspace-distance is: 0.00017612258670851588 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 1.05; acc: 0.69
Batch: 40; loss: 1.24; acc: 0.67
Batch: 60; loss: 1.29; acc: 0.62
Batch: 80; loss: 1.28; acc: 0.66
Batch: 100; loss: 1.12; acc: 0.67
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 1.08; acc: 0.7
Batch: 160; loss: 1.2; acc: 0.62
Batch: 180; loss: 1.07; acc: 0.67
Batch: 200; loss: 1.06; acc: 0.81
Batch: 220; loss: 1.4; acc: 0.59
Batch: 240; loss: 1.02; acc: 0.67
Batch: 260; loss: 1.28; acc: 0.56
Batch: 280; loss: 1.06; acc: 0.78
Batch: 300; loss: 1.1; acc: 0.66
Batch: 320; loss: 1.33; acc: 0.61
Batch: 340; loss: 1.1; acc: 0.75
Batch: 360; loss: 1.16; acc: 0.7
Batch: 380; loss: 1.36; acc: 0.72
Batch: 400; loss: 1.25; acc: 0.58
Batch: 420; loss: 1.14; acc: 0.66
Batch: 440; loss: 1.23; acc: 0.67
Batch: 460; loss: 1.15; acc: 0.72
Batch: 480; loss: 1.14; acc: 0.67
Batch: 500; loss: 1.27; acc: 0.62
Batch: 520; loss: 1.17; acc: 0.7
Batch: 540; loss: 1.21; acc: 0.55
Batch: 560; loss: 1.18; acc: 0.66
Batch: 580; loss: 1.06; acc: 0.73
Batch: 600; loss: 1.27; acc: 0.66
Batch: 620; loss: 1.06; acc: 0.69
Batch: 640; loss: 1.04; acc: 0.75
Batch: 660; loss: 1.25; acc: 0.61
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.19; acc: 0.66
Batch: 720; loss: 1.4; acc: 0.52
Batch: 740; loss: 1.16; acc: 0.66
Batch: 760; loss: 1.26; acc: 0.61
Batch: 780; loss: 1.15; acc: 0.72
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.00018263740639667958
0.0001749193324940279
Batch: 0; loss: 1.21; acc: 0.7
Batch: 20; loss: 1.29; acc: 0.53
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.67
Batch: 100; loss: 1.16; acc: 0.62
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.75
Val Epoch over. val_loss: 1.1312732100486755; val_accuracy: 0.6903861464968153 

The current subspace-distance is: 0.0001749193324940279 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.12; acc: 0.66
Batch: 40; loss: 1.13; acc: 0.72
Batch: 60; loss: 1.26; acc: 0.7
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 1.05; acc: 0.77
Batch: 160; loss: 1.19; acc: 0.64
Batch: 180; loss: 1.21; acc: 0.67
Batch: 200; loss: 1.24; acc: 0.69
Batch: 220; loss: 1.37; acc: 0.53
Batch: 240; loss: 1.41; acc: 0.5
Batch: 260; loss: 1.08; acc: 0.78
Batch: 280; loss: 1.13; acc: 0.7
Batch: 300; loss: 1.25; acc: 0.67
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.11; acc: 0.67
Batch: 360; loss: 1.15; acc: 0.72
Batch: 380; loss: 1.2; acc: 0.7
Batch: 400; loss: 1.19; acc: 0.66
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.11; acc: 0.78
Batch: 460; loss: 1.3; acc: 0.53
Batch: 480; loss: 1.07; acc: 0.66
Batch: 500; loss: 1.32; acc: 0.62
Batch: 520; loss: 1.3; acc: 0.56
Batch: 540; loss: 1.25; acc: 0.61
Batch: 560; loss: 1.07; acc: 0.69
Batch: 580; loss: 1.43; acc: 0.58
Batch: 600; loss: 1.21; acc: 0.62
Batch: 620; loss: 1.11; acc: 0.73
Batch: 640; loss: 1.14; acc: 0.61
Batch: 660; loss: 1.11; acc: 0.75
Batch: 680; loss: 1.16; acc: 0.69
Batch: 700; loss: 1.23; acc: 0.66
Batch: 720; loss: 1.14; acc: 0.66
Batch: 740; loss: 1.18; acc: 0.77
Batch: 760; loss: 1.24; acc: 0.7
Batch: 780; loss: 1.01; acc: 0.78
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.00018080815789289773
0.00017537908570375293
Batch: 0; loss: 1.2; acc: 0.7
Batch: 20; loss: 1.28; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.73
Batch: 100; loss: 1.17; acc: 0.66
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.78
Val Epoch over. val_loss: 1.125421582513554; val_accuracy: 0.7008359872611465 

The current subspace-distance is: 0.00017537908570375293 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.72
Batch: 40; loss: 1.34; acc: 0.61
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.07; acc: 0.72
Batch: 160; loss: 1.46; acc: 0.58
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.27; acc: 0.61
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 1.36; acc: 0.55
Batch: 260; loss: 1.2; acc: 0.61
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 1.24; acc: 0.66
Batch: 320; loss: 1.16; acc: 0.61
Batch: 340; loss: 1.1; acc: 0.73
Batch: 360; loss: 1.22; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.69
Batch: 400; loss: 1.29; acc: 0.69
Batch: 420; loss: 1.36; acc: 0.59
Batch: 440; loss: 1.17; acc: 0.69
Batch: 460; loss: 1.25; acc: 0.67
Batch: 480; loss: 1.13; acc: 0.7
Batch: 500; loss: 1.17; acc: 0.72
Batch: 520; loss: 1.22; acc: 0.69
Batch: 540; loss: 1.08; acc: 0.7
Batch: 560; loss: 1.3; acc: 0.55
Batch: 580; loss: 1.14; acc: 0.72
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.1; acc: 0.72
Batch: 660; loss: 1.27; acc: 0.58
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.25; acc: 0.64
Batch: 720; loss: 1.22; acc: 0.61
Batch: 740; loss: 1.14; acc: 0.73
Batch: 760; loss: 1.17; acc: 0.7
Batch: 780; loss: 1.09; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.0001855548907769844
0.00018001734861172736
Batch: 0; loss: 1.2; acc: 0.7
Batch: 20; loss: 1.28; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.64
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.77
Val Epoch over. val_loss: 1.1260105348696374; val_accuracy: 0.6973527070063694 

The current subspace-distance is: 0.00018001734861172736 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.0; acc: 0.75
Batch: 40; loss: 1.37; acc: 0.58
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.14; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.7
Batch: 140; loss: 1.35; acc: 0.62
Batch: 160; loss: 1.3; acc: 0.62
Batch: 180; loss: 1.21; acc: 0.61
Batch: 200; loss: 0.91; acc: 0.8
Batch: 220; loss: 1.11; acc: 0.75
Batch: 240; loss: 1.32; acc: 0.64
Batch: 260; loss: 1.19; acc: 0.64
Batch: 280; loss: 1.05; acc: 0.77
Batch: 300; loss: 1.05; acc: 0.77
Batch: 320; loss: 1.1; acc: 0.75
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.07; acc: 0.73
Batch: 380; loss: 1.14; acc: 0.67
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 1.15; acc: 0.64
Batch: 440; loss: 1.33; acc: 0.62
Batch: 460; loss: 1.09; acc: 0.64
Batch: 480; loss: 1.2; acc: 0.64
Batch: 500; loss: 1.23; acc: 0.59
Batch: 520; loss: 0.97; acc: 0.8
Batch: 540; loss: 1.09; acc: 0.7
Batch: 560; loss: 1.1; acc: 0.75
Batch: 580; loss: 1.29; acc: 0.58
Batch: 600; loss: 1.22; acc: 0.67
Batch: 620; loss: 1.28; acc: 0.67
Batch: 640; loss: 1.11; acc: 0.72
Batch: 660; loss: 1.19; acc: 0.61
Batch: 680; loss: 1.04; acc: 0.7
Batch: 700; loss: 1.14; acc: 0.72
Batch: 720; loss: 1.14; acc: 0.7
Batch: 740; loss: 1.14; acc: 0.69
Batch: 760; loss: 1.24; acc: 0.59
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.0001847286184784025
0.00017846339324023575
Batch: 0; loss: 1.2; acc: 0.73
Batch: 20; loss: 1.3; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.73
Batch: 100; loss: 1.17; acc: 0.64
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 0.97; acc: 0.75
Val Epoch over. val_loss: 1.1220026464219306; val_accuracy: 0.6976512738853503 

The current subspace-distance is: 0.00017846339324023575 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.7
Batch: 40; loss: 1.19; acc: 0.77
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 1.14; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.67
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 1.1; acc: 0.72
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 1.19; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.69
Batch: 220; loss: 1.18; acc: 0.61
Batch: 240; loss: 1.17; acc: 0.69
Batch: 260; loss: 0.92; acc: 0.83
Batch: 280; loss: 1.01; acc: 0.72
Batch: 300; loss: 1.25; acc: 0.55
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.14; acc: 0.66
Batch: 360; loss: 1.08; acc: 0.73
Batch: 380; loss: 1.15; acc: 0.64
Batch: 400; loss: 1.16; acc: 0.72
Batch: 420; loss: 1.17; acc: 0.67
Batch: 440; loss: 1.37; acc: 0.59
Batch: 460; loss: 1.12; acc: 0.69
Batch: 480; loss: 1.04; acc: 0.7
Batch: 500; loss: 1.22; acc: 0.66
Batch: 520; loss: 1.26; acc: 0.58
Batch: 540; loss: 1.2; acc: 0.64
Batch: 560; loss: 1.18; acc: 0.69
Batch: 580; loss: 1.16; acc: 0.69
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.78
Batch: 640; loss: 1.1; acc: 0.69
Batch: 660; loss: 1.3; acc: 0.64
Batch: 680; loss: 1.29; acc: 0.55
Batch: 700; loss: 1.19; acc: 0.61
Batch: 720; loss: 1.22; acc: 0.61
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 1.17; acc: 0.64
Batch: 780; loss: 1.06; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.00018606730736792088
0.00018012009968515486
Batch: 0; loss: 1.18; acc: 0.75
Batch: 20; loss: 1.26; acc: 0.53
Batch: 40; loss: 0.85; acc: 0.81
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 1.1132435456962342; val_accuracy: 0.7043192675159236 

The current subspace-distance is: 0.00018012009968515486 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.27; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 1.14; acc: 0.73
Batch: 60; loss: 1.17; acc: 0.78
Batch: 80; loss: 1.24; acc: 0.61
Batch: 100; loss: 1.2; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.58
Batch: 140; loss: 1.16; acc: 0.77
Batch: 160; loss: 1.2; acc: 0.62
Batch: 180; loss: 1.2; acc: 0.62
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 1.3; acc: 0.59
Batch: 240; loss: 1.23; acc: 0.7
Batch: 260; loss: 0.98; acc: 0.81
Batch: 280; loss: 1.02; acc: 0.7
Batch: 300; loss: 1.11; acc: 0.77
Batch: 320; loss: 1.34; acc: 0.62
Batch: 340; loss: 1.17; acc: 0.67
Batch: 360; loss: 1.21; acc: 0.72
Batch: 380; loss: 1.27; acc: 0.7
Batch: 400; loss: 1.22; acc: 0.64
Batch: 420; loss: 1.18; acc: 0.64
Batch: 440; loss: 1.23; acc: 0.67
Batch: 460; loss: 1.16; acc: 0.72
Batch: 480; loss: 1.13; acc: 0.73
Batch: 500; loss: 1.19; acc: 0.66
Batch: 520; loss: 1.39; acc: 0.59
Batch: 540; loss: 1.12; acc: 0.67
Batch: 560; loss: 1.31; acc: 0.62
Batch: 580; loss: 1.16; acc: 0.69
Batch: 600; loss: 1.18; acc: 0.64
Batch: 620; loss: 1.02; acc: 0.75
Batch: 640; loss: 0.97; acc: 0.83
Batch: 660; loss: 1.17; acc: 0.59
Batch: 680; loss: 1.2; acc: 0.61
Batch: 700; loss: 1.22; acc: 0.62
Batch: 720; loss: 1.17; acc: 0.67
Batch: 740; loss: 1.19; acc: 0.67
Batch: 760; loss: 1.23; acc: 0.62
Batch: 780; loss: 1.27; acc: 0.61
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.00018611905397847295
0.0001790333044482395
Batch: 0; loss: 1.2; acc: 0.72
Batch: 20; loss: 1.29; acc: 0.53
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.73
Batch: 100; loss: 1.16; acc: 0.61
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.97; acc: 0.78
Val Epoch over. val_loss: 1.1228379398394541; val_accuracy: 0.6986464968152867 

The current subspace-distance is: 0.0001790333044482395 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 1.07; acc: 0.72
Batch: 40; loss: 1.21; acc: 0.67
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.18; acc: 0.64
Batch: 120; loss: 1.19; acc: 0.61
Batch: 140; loss: 1.27; acc: 0.64
Batch: 160; loss: 1.06; acc: 0.75
Batch: 180; loss: 1.14; acc: 0.64
Batch: 200; loss: 1.31; acc: 0.61
Batch: 220; loss: 1.2; acc: 0.62
Batch: 240; loss: 1.14; acc: 0.67
Batch: 260; loss: 1.17; acc: 0.7
Batch: 280; loss: 1.13; acc: 0.67
Batch: 300; loss: 1.16; acc: 0.72
Batch: 320; loss: 1.22; acc: 0.62
Batch: 340; loss: 1.19; acc: 0.67
Batch: 360; loss: 1.31; acc: 0.59
Batch: 380; loss: 1.18; acc: 0.72
Batch: 400; loss: 1.12; acc: 0.66
Batch: 420; loss: 1.12; acc: 0.73
Batch: 440; loss: 1.1; acc: 0.67
Batch: 460; loss: 1.39; acc: 0.61
Batch: 480; loss: 1.12; acc: 0.62
Batch: 500; loss: 0.98; acc: 0.75
Batch: 520; loss: 1.23; acc: 0.62
Batch: 540; loss: 1.02; acc: 0.72
Batch: 560; loss: 1.12; acc: 0.75
Batch: 580; loss: 1.04; acc: 0.73
Batch: 600; loss: 1.17; acc: 0.64
Batch: 620; loss: 1.21; acc: 0.67
Batch: 640; loss: 1.1; acc: 0.7
Batch: 660; loss: 1.17; acc: 0.67
Batch: 680; loss: 1.16; acc: 0.67
Batch: 700; loss: 1.02; acc: 0.75
Batch: 720; loss: 1.23; acc: 0.61
Batch: 740; loss: 1.16; acc: 0.69
Batch: 760; loss: 1.3; acc: 0.61
Batch: 780; loss: 1.33; acc: 0.56
Train Epoch over. train_loss: 1.17; train_accuracy: 0.67 

0.00018776040815282613
0.00018090649973601103
Batch: 0; loss: 1.18; acc: 0.75
Batch: 20; loss: 1.26; acc: 0.53
Batch: 40; loss: 0.84; acc: 0.81
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 1.27; acc: 0.61
Batch: 140; loss: 0.95; acc: 0.77
Val Epoch over. val_loss: 1.1090690755540398; val_accuracy: 0.6998407643312102 

The current subspace-distance is: 0.00018090649973601103 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 1.17; acc: 0.61
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 1.33; acc: 0.66
Batch: 100; loss: 1.2; acc: 0.66
Batch: 120; loss: 1.07; acc: 0.66
Batch: 140; loss: 1.27; acc: 0.58
Batch: 160; loss: 1.23; acc: 0.61
Batch: 180; loss: 1.19; acc: 0.64
Batch: 200; loss: 1.12; acc: 0.66
Batch: 220; loss: 1.37; acc: 0.55
Batch: 240; loss: 1.14; acc: 0.73
Batch: 260; loss: 1.09; acc: 0.7
Batch: 280; loss: 1.12; acc: 0.75
Batch: 300; loss: 1.25; acc: 0.61
Batch: 320; loss: 1.24; acc: 0.67
Batch: 340; loss: 1.1; acc: 0.72
Batch: 360; loss: 1.07; acc: 0.73
Batch: 380; loss: 1.18; acc: 0.61
Batch: 400; loss: 1.11; acc: 0.72
Batch: 420; loss: 1.23; acc: 0.64
Batch: 440; loss: 1.12; acc: 0.67
Batch: 460; loss: 1.12; acc: 0.73
Batch: 480; loss: 1.11; acc: 0.75
Batch: 500; loss: 1.11; acc: 0.77
Batch: 520; loss: 1.06; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.44; acc: 0.55
Batch: 580; loss: 1.14; acc: 0.69
Batch: 600; loss: 1.26; acc: 0.66
Batch: 620; loss: 1.32; acc: 0.61
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.13; acc: 0.7
Batch: 700; loss: 1.16; acc: 0.73
Batch: 720; loss: 1.09; acc: 0.7
Batch: 740; loss: 1.26; acc: 0.64
Batch: 760; loss: 1.39; acc: 0.52
Batch: 780; loss: 1.4; acc: 0.58
Train Epoch over. train_loss: 1.17; train_accuracy: 0.67 

0.0001901736977742985
0.00018295309564564377
Batch: 0; loss: 1.21; acc: 0.69
Batch: 20; loss: 1.29; acc: 0.53
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.69
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 1.123729882726244; val_accuracy: 0.6969546178343949 

The current subspace-distance is: 0.00018295309564564377 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_16_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1

The number of parameters is: 272670

The number of individual parameters is:

8
144
8
8
12
29952
12
12
24
89856
24
24
64
147456
64
64
4096
64
640
10
64
64

nonzero elements in E: 54533996
elements in E: 54534000
fraction nonzero: 0.9999999266512635
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.11; acc: 0.28
Batch: 40; loss: 2.0; acc: 0.36
Batch: 60; loss: 1.92; acc: 0.42
Batch: 80; loss: 1.8; acc: 0.52
Batch: 100; loss: 1.83; acc: 0.45
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.72; acc: 0.58
Batch: 160; loss: 1.74; acc: 0.53
Batch: 180; loss: 1.62; acc: 0.5
Batch: 200; loss: 1.71; acc: 0.5
Batch: 220; loss: 1.59; acc: 0.58
Batch: 240; loss: 1.61; acc: 0.59
Batch: 260; loss: 1.5; acc: 0.66
Batch: 280; loss: 1.48; acc: 0.7
Batch: 300; loss: 1.43; acc: 0.73
Batch: 320; loss: 1.53; acc: 0.61
Batch: 340; loss: 1.57; acc: 0.66
Batch: 360; loss: 1.48; acc: 0.69
Batch: 380; loss: 1.47; acc: 0.67
Batch: 400; loss: 1.48; acc: 0.66
Batch: 420; loss: 1.39; acc: 0.78
Batch: 440; loss: 1.45; acc: 0.67
Batch: 460; loss: 1.4; acc: 0.75
Batch: 480; loss: 1.34; acc: 0.77
Batch: 500; loss: 1.46; acc: 0.73
Batch: 520; loss: 1.38; acc: 0.77
Batch: 540; loss: 1.51; acc: 0.61
Batch: 560; loss: 1.28; acc: 0.75
Batch: 580; loss: 1.33; acc: 0.8
Batch: 600; loss: 1.34; acc: 0.72
Batch: 620; loss: 1.43; acc: 0.7
Batch: 640; loss: 1.45; acc: 0.66
Batch: 660; loss: 1.35; acc: 0.69
Batch: 680; loss: 1.39; acc: 0.73
Batch: 700; loss: 1.31; acc: 0.77
Batch: 720; loss: 1.39; acc: 0.67
Batch: 740; loss: 1.33; acc: 0.75
Batch: 760; loss: 1.27; acc: 0.78
Batch: 780; loss: 1.4; acc: 0.67
Train Epoch over. train_loss: 1.53; train_accuracy: 0.63 

5.996179606881924e-05
5.4115942475618795e-05
Batch: 0; loss: 1.31; acc: 0.77
Batch: 20; loss: 1.43; acc: 0.67
Batch: 40; loss: 1.05; acc: 0.86
Batch: 60; loss: 1.21; acc: 0.77
Batch: 80; loss: 1.18; acc: 0.81
Batch: 100; loss: 1.32; acc: 0.8
Batch: 120; loss: 1.44; acc: 0.59
Batch: 140; loss: 1.09; acc: 0.88
Val Epoch over. val_loss: 1.2832703298064554; val_accuracy: 0.7576632165605095 

The current subspace-distance is: 5.4115942475618795e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.59
Batch: 20; loss: 1.39; acc: 0.7
Batch: 40; loss: 1.37; acc: 0.7
Batch: 60; loss: 1.28; acc: 0.78
Batch: 80; loss: 1.23; acc: 0.8
Batch: 100; loss: 1.2; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.69
Batch: 140; loss: 1.32; acc: 0.78
Batch: 160; loss: 1.29; acc: 0.77
Batch: 180; loss: 1.24; acc: 0.75
Batch: 200; loss: 1.21; acc: 0.83
Batch: 220; loss: 1.27; acc: 0.75
Batch: 240; loss: 1.23; acc: 0.7
Batch: 260; loss: 1.26; acc: 0.75
Batch: 280; loss: 1.38; acc: 0.62
Batch: 300; loss: 1.23; acc: 0.78
Batch: 320; loss: 1.23; acc: 0.81
Batch: 340; loss: 1.54; acc: 0.53
Batch: 360; loss: 1.35; acc: 0.67
Batch: 380; loss: 1.31; acc: 0.64
Batch: 400; loss: 1.25; acc: 0.78
Batch: 420; loss: 1.28; acc: 0.69
Batch: 440; loss: 1.35; acc: 0.75
Batch: 460; loss: 1.36; acc: 0.69
Batch: 480; loss: 1.3; acc: 0.7
Batch: 500; loss: 1.34; acc: 0.77
Batch: 520; loss: 1.24; acc: 0.77
Batch: 540; loss: 1.24; acc: 0.73
Batch: 560; loss: 1.42; acc: 0.62
Batch: 580; loss: 1.39; acc: 0.67
Batch: 600; loss: 1.26; acc: 0.73
Batch: 620; loss: 1.18; acc: 0.77
Batch: 640; loss: 1.44; acc: 0.62
Batch: 660; loss: 1.46; acc: 0.62
Batch: 680; loss: 1.22; acc: 0.7
Batch: 700; loss: 1.33; acc: 0.69
Batch: 720; loss: 1.26; acc: 0.77
Batch: 740; loss: 1.21; acc: 0.75
Batch: 760; loss: 1.28; acc: 0.73
Batch: 780; loss: 1.27; acc: 0.69
Train Epoch over. train_loss: 1.28; train_accuracy: 0.74 

7.820982864359394e-05
7.27054793969728e-05
Batch: 0; loss: 1.2; acc: 0.75
Batch: 20; loss: 1.43; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.86
Batch: 60; loss: 1.15; acc: 0.75
Batch: 80; loss: 1.07; acc: 0.88
Batch: 100; loss: 1.21; acc: 0.81
Batch: 120; loss: 1.41; acc: 0.53
Batch: 140; loss: 1.0; acc: 0.88
Val Epoch over. val_loss: 1.190980298883596; val_accuracy: 0.773984872611465 

The current subspace-distance is: 7.27054793969728e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.8
Batch: 20; loss: 1.2; acc: 0.72
Batch: 40; loss: 1.17; acc: 0.81
Batch: 60; loss: 1.22; acc: 0.75
Batch: 80; loss: 1.22; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.78
Batch: 120; loss: 1.15; acc: 0.8
Batch: 140; loss: 1.21; acc: 0.8
Batch: 160; loss: 1.21; acc: 0.73
Batch: 180; loss: 1.32; acc: 0.69
Batch: 200; loss: 1.34; acc: 0.7
Batch: 220; loss: 1.21; acc: 0.75
Batch: 240; loss: 1.11; acc: 0.8
Batch: 260; loss: 1.24; acc: 0.75
Batch: 280; loss: 1.16; acc: 0.78
Batch: 300; loss: 1.1; acc: 0.86
Batch: 320; loss: 1.34; acc: 0.67
Batch: 340; loss: 1.28; acc: 0.72
Batch: 360; loss: 1.27; acc: 0.77
Batch: 380; loss: 1.14; acc: 0.84
Batch: 400; loss: 1.16; acc: 0.78
Batch: 420; loss: 1.2; acc: 0.73
Batch: 440; loss: 1.16; acc: 0.77
Batch: 460; loss: 1.18; acc: 0.72
Batch: 480; loss: 1.25; acc: 0.73
Batch: 500; loss: 1.13; acc: 0.75
Batch: 520; loss: 1.14; acc: 0.75
Batch: 540; loss: 1.17; acc: 0.73
Batch: 560; loss: 1.29; acc: 0.73
Batch: 580; loss: 1.06; acc: 0.83
Batch: 600; loss: 1.24; acc: 0.72
Batch: 620; loss: 1.02; acc: 0.84
Batch: 640; loss: 1.2; acc: 0.72
Batch: 660; loss: 1.2; acc: 0.77
Batch: 680; loss: 1.3; acc: 0.64
Batch: 700; loss: 1.24; acc: 0.72
Batch: 720; loss: 1.22; acc: 0.69
Batch: 740; loss: 1.21; acc: 0.7
Batch: 760; loss: 1.29; acc: 0.66
Batch: 780; loss: 1.15; acc: 0.75
Train Epoch over. train_loss: 1.21; train_accuracy: 0.74 

9.160207991953939e-05
8.664997585583478e-05
Batch: 0; loss: 1.15; acc: 0.77
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.91
Batch: 60; loss: 1.09; acc: 0.77
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.12; acc: 0.83
Batch: 120; loss: 1.35; acc: 0.56
Batch: 140; loss: 0.98; acc: 0.86
Val Epoch over. val_loss: 1.1357889046334917; val_accuracy: 0.7753781847133758 

The current subspace-distance is: 8.664997585583478e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.21; acc: 0.78
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.7
Batch: 80; loss: 1.1; acc: 0.78
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.22; acc: 0.72
Batch: 140; loss: 1.03; acc: 0.81
Batch: 160; loss: 1.16; acc: 0.73
Batch: 180; loss: 1.13; acc: 0.83
Batch: 200; loss: 1.14; acc: 0.72
Batch: 220; loss: 1.14; acc: 0.81
Batch: 240; loss: 1.21; acc: 0.75
Batch: 260; loss: 1.18; acc: 0.8
Batch: 280; loss: 1.2; acc: 0.7
Batch: 300; loss: 1.17; acc: 0.67
Batch: 320; loss: 1.09; acc: 0.8
Batch: 340; loss: 1.11; acc: 0.77
Batch: 360; loss: 1.04; acc: 0.83
Batch: 380; loss: 1.22; acc: 0.72
Batch: 400; loss: 1.22; acc: 0.66
Batch: 420; loss: 1.19; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.73
Batch: 460; loss: 1.16; acc: 0.72
Batch: 480; loss: 1.16; acc: 0.77
Batch: 500; loss: 1.18; acc: 0.8
Batch: 520; loss: 1.25; acc: 0.7
Batch: 540; loss: 1.23; acc: 0.7
Batch: 560; loss: 1.19; acc: 0.77
Batch: 580; loss: 1.24; acc: 0.66
Batch: 600; loss: 1.13; acc: 0.73
Batch: 620; loss: 1.24; acc: 0.72
Batch: 640; loss: 1.18; acc: 0.69
Batch: 660; loss: 1.05; acc: 0.8
Batch: 680; loss: 1.21; acc: 0.72
Batch: 700; loss: 1.11; acc: 0.69
Batch: 720; loss: 1.19; acc: 0.75
Batch: 740; loss: 1.18; acc: 0.66
Batch: 760; loss: 1.17; acc: 0.7
Batch: 780; loss: 1.06; acc: 0.77
Train Epoch over. train_loss: 1.17; train_accuracy: 0.74 

0.0001024565426632762
9.735814819578081e-05
Batch: 0; loss: 1.11; acc: 0.77
Batch: 20; loss: 1.36; acc: 0.58
Batch: 40; loss: 0.81; acc: 0.91
Batch: 60; loss: 1.05; acc: 0.75
Batch: 80; loss: 0.96; acc: 0.84
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 0.92; acc: 0.88
Val Epoch over. val_loss: 1.0832731807307832; val_accuracy: 0.772093949044586 

The current subspace-distance is: 9.735814819578081e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.58
Batch: 20; loss: 1.31; acc: 0.64
Batch: 40; loss: 1.11; acc: 0.77
Batch: 60; loss: 1.1; acc: 0.78
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.05; acc: 0.81
Batch: 120; loss: 1.16; acc: 0.73
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 1.14; acc: 0.78
Batch: 180; loss: 1.1; acc: 0.73
Batch: 200; loss: 1.08; acc: 0.8
Batch: 220; loss: 1.14; acc: 0.72
Batch: 240; loss: 1.21; acc: 0.72
Batch: 260; loss: 1.06; acc: 0.77
Batch: 280; loss: 1.2; acc: 0.7
Batch: 300; loss: 1.2; acc: 0.72
Batch: 320; loss: 1.12; acc: 0.72
Batch: 340; loss: 1.27; acc: 0.66
Batch: 360; loss: 0.99; acc: 0.8
Batch: 380; loss: 1.26; acc: 0.7
Batch: 400; loss: 1.15; acc: 0.72
Batch: 420; loss: 1.28; acc: 0.69
Batch: 440; loss: 1.15; acc: 0.72
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.09; acc: 0.72
Batch: 500; loss: 1.02; acc: 0.81
Batch: 520; loss: 1.18; acc: 0.72
Batch: 540; loss: 0.98; acc: 0.78
Batch: 560; loss: 1.2; acc: 0.73
Batch: 580; loss: 0.99; acc: 0.83
Batch: 600; loss: 1.26; acc: 0.64
Batch: 620; loss: 1.13; acc: 0.75
Batch: 640; loss: 1.28; acc: 0.64
Batch: 660; loss: 1.01; acc: 0.84
Batch: 680; loss: 0.96; acc: 0.84
Batch: 700; loss: 1.02; acc: 0.86
Batch: 720; loss: 1.05; acc: 0.77
Batch: 740; loss: 1.14; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.86
Batch: 780; loss: 0.99; acc: 0.8
Train Epoch over. train_loss: 1.11; train_accuracy: 0.75 

0.00011516545055201277
0.00010939248750219122
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.91
Batch: 60; loss: 1.0; acc: 0.75
Batch: 80; loss: 0.89; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 0.88; acc: 0.88
Val Epoch over. val_loss: 1.0258069437020902; val_accuracy: 0.7786624203821656 

The current subspace-distance is: 0.00010939248750219122 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.73
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 1.04; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.75
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 1.04; acc: 0.73
Batch: 140; loss: 1.08; acc: 0.7
Batch: 160; loss: 1.12; acc: 0.7
Batch: 180; loss: 1.15; acc: 0.72
Batch: 200; loss: 1.11; acc: 0.72
Batch: 220; loss: 1.0; acc: 0.81
Batch: 240; loss: 0.97; acc: 0.8
Batch: 260; loss: 1.06; acc: 0.75
Batch: 280; loss: 1.06; acc: 0.8
Batch: 300; loss: 1.05; acc: 0.78
Batch: 320; loss: 1.0; acc: 0.83
Batch: 340; loss: 1.16; acc: 0.64
Batch: 360; loss: 1.1; acc: 0.73
Batch: 380; loss: 1.02; acc: 0.72
Batch: 400; loss: 1.04; acc: 0.75
Batch: 420; loss: 1.1; acc: 0.75
Batch: 440; loss: 0.97; acc: 0.8
Batch: 460; loss: 1.05; acc: 0.69
Batch: 480; loss: 1.0; acc: 0.81
Batch: 500; loss: 1.19; acc: 0.72
Batch: 520; loss: 0.9; acc: 0.83
Batch: 540; loss: 0.95; acc: 0.8
Batch: 560; loss: 1.12; acc: 0.73
Batch: 580; loss: 1.26; acc: 0.69
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 0.9; acc: 0.81
Batch: 640; loss: 0.95; acc: 0.78
Batch: 660; loss: 0.98; acc: 0.8
Batch: 680; loss: 0.92; acc: 0.81
Batch: 700; loss: 0.9; acc: 0.84
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 0.85; acc: 0.78
Batch: 760; loss: 0.96; acc: 0.75
Batch: 780; loss: 0.89; acc: 0.8
Train Epoch over. train_loss: 1.04; train_accuracy: 0.76 

0.00013116354239173234
0.00012538759619928896
Batch: 0; loss: 1.0; acc: 0.77
Batch: 20; loss: 1.23; acc: 0.67
Batch: 40; loss: 0.67; acc: 0.91
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.82; acc: 0.88
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 0.8; acc: 0.88
Val Epoch over. val_loss: 0.9580757116815847; val_accuracy: 0.7867237261146497 

The current subspace-distance is: 0.00012538759619928896 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.77
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 1.03; acc: 0.78
Batch: 60; loss: 0.87; acc: 0.86
Batch: 80; loss: 0.8; acc: 0.81
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.04; acc: 0.78
Batch: 140; loss: 1.21; acc: 0.67
Batch: 160; loss: 0.98; acc: 0.83
Batch: 180; loss: 1.04; acc: 0.72
Batch: 200; loss: 0.98; acc: 0.75
Batch: 220; loss: 1.13; acc: 0.73
Batch: 240; loss: 0.94; acc: 0.81
Batch: 260; loss: 0.96; acc: 0.8
Batch: 280; loss: 0.9; acc: 0.78
Batch: 300; loss: 1.08; acc: 0.8
Batch: 320; loss: 1.13; acc: 0.69
Batch: 340; loss: 0.9; acc: 0.81
Batch: 360; loss: 0.9; acc: 0.81
Batch: 380; loss: 1.03; acc: 0.75
Batch: 400; loss: 1.0; acc: 0.78
Batch: 420; loss: 0.95; acc: 0.8
Batch: 440; loss: 0.96; acc: 0.7
Batch: 460; loss: 0.84; acc: 0.8
Batch: 480; loss: 0.99; acc: 0.73
Batch: 500; loss: 1.01; acc: 0.72
Batch: 520; loss: 0.94; acc: 0.78
Batch: 540; loss: 0.9; acc: 0.81
Batch: 560; loss: 0.94; acc: 0.77
Batch: 580; loss: 0.98; acc: 0.77
Batch: 600; loss: 1.11; acc: 0.75
Batch: 620; loss: 0.96; acc: 0.8
Batch: 640; loss: 0.9; acc: 0.72
Batch: 660; loss: 0.83; acc: 0.81
Batch: 680; loss: 0.88; acc: 0.78
Batch: 700; loss: 1.04; acc: 0.72
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 1.0; acc: 0.75
Batch: 760; loss: 0.87; acc: 0.86
Batch: 780; loss: 1.02; acc: 0.69
Train Epoch over. train_loss: 0.97; train_accuracy: 0.77 

0.0001450662239221856
0.00013958883937448263
Batch: 0; loss: 0.9; acc: 0.78
Batch: 20; loss: 1.15; acc: 0.72
Batch: 40; loss: 0.59; acc: 0.92
Batch: 60; loss: 0.87; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 1.1; acc: 0.67
Batch: 140; loss: 0.7; acc: 0.88
Val Epoch over. val_loss: 0.8740886322631958; val_accuracy: 0.8031449044585988 

The current subspace-distance is: 0.00013958883937448263 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.75
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 1.02; acc: 0.73
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 0.96; acc: 0.77
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.9; acc: 0.78
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 0.92; acc: 0.81
Batch: 200; loss: 0.86; acc: 0.77
Batch: 220; loss: 0.95; acc: 0.77
Batch: 240; loss: 0.83; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.77
Batch: 280; loss: 0.82; acc: 0.81
Batch: 300; loss: 0.95; acc: 0.75
Batch: 320; loss: 0.87; acc: 0.83
Batch: 340; loss: 0.82; acc: 0.78
Batch: 360; loss: 0.99; acc: 0.77
Batch: 380; loss: 0.95; acc: 0.78
Batch: 400; loss: 0.92; acc: 0.83
Batch: 420; loss: 0.96; acc: 0.72
Batch: 440; loss: 0.91; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.73
Batch: 480; loss: 0.87; acc: 0.78
Batch: 500; loss: 0.8; acc: 0.86
Batch: 520; loss: 0.88; acc: 0.83
Batch: 540; loss: 1.0; acc: 0.77
Batch: 560; loss: 0.87; acc: 0.75
Batch: 580; loss: 0.77; acc: 0.91
Batch: 600; loss: 0.8; acc: 0.81
Batch: 620; loss: 0.87; acc: 0.78
Batch: 640; loss: 0.85; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.84
Batch: 680; loss: 0.93; acc: 0.8
Batch: 700; loss: 0.91; acc: 0.75
Batch: 720; loss: 0.82; acc: 0.8
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.78; acc: 0.84
Batch: 780; loss: 0.99; acc: 0.73
Train Epoch over. train_loss: 0.9; train_accuracy: 0.78 

0.00015882057778071612
0.0001533300383016467
Batch: 0; loss: 0.83; acc: 0.8
Batch: 20; loss: 1.07; acc: 0.73
Batch: 40; loss: 0.53; acc: 0.94
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.64; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.88
Batch: 120; loss: 1.05; acc: 0.69
Batch: 140; loss: 0.64; acc: 0.83
Val Epoch over. val_loss: 0.814824438208987; val_accuracy: 0.814390923566879 

The current subspace-distance is: 0.0001533300383016467 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.73
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.93; acc: 0.78
Batch: 60; loss: 0.86; acc: 0.78
Batch: 80; loss: 0.88; acc: 0.8
Batch: 100; loss: 0.72; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.77; acc: 0.88
Batch: 160; loss: 0.81; acc: 0.8
Batch: 180; loss: 0.86; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.86
Batch: 220; loss: 0.79; acc: 0.8
Batch: 240; loss: 0.82; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.84
Batch: 280; loss: 0.91; acc: 0.77
Batch: 300; loss: 0.97; acc: 0.75
Batch: 320; loss: 0.99; acc: 0.66
Batch: 340; loss: 0.82; acc: 0.83
Batch: 360; loss: 1.01; acc: 0.75
Batch: 380; loss: 1.0; acc: 0.8
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.9; acc: 0.72
Batch: 440; loss: 0.82; acc: 0.75
Batch: 460; loss: 0.73; acc: 0.86
Batch: 480; loss: 0.68; acc: 0.88
Batch: 500; loss: 0.96; acc: 0.7
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.93; acc: 0.77
Batch: 560; loss: 0.77; acc: 0.81
Batch: 580; loss: 0.84; acc: 0.8
Batch: 600; loss: 0.85; acc: 0.8
Batch: 620; loss: 0.89; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.88
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.95; acc: 0.77
Batch: 700; loss: 0.89; acc: 0.75
Batch: 720; loss: 0.81; acc: 0.84
Batch: 740; loss: 0.71; acc: 0.88
Batch: 760; loss: 0.85; acc: 0.78
Batch: 780; loss: 0.76; acc: 0.84
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

0.00017223383474629372
0.0001655839296290651
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.89
Batch: 120; loss: 0.99; acc: 0.72
Batch: 140; loss: 0.6; acc: 0.84
Val Epoch over. val_loss: 0.7729265424096661; val_accuracy: 0.8214570063694268 

The current subspace-distance is: 0.0001655839296290651 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 0.78; acc: 0.86
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.92
Batch: 160; loss: 0.81; acc: 0.8
Batch: 180; loss: 0.69; acc: 0.91
Batch: 200; loss: 0.93; acc: 0.77
Batch: 220; loss: 0.67; acc: 0.88
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.81; acc: 0.8
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.88; acc: 0.73
Batch: 340; loss: 0.82; acc: 0.75
Batch: 360; loss: 0.86; acc: 0.78
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.74; acc: 0.83
Batch: 420; loss: 0.81; acc: 0.81
Batch: 440; loss: 0.9; acc: 0.78
Batch: 460; loss: 0.86; acc: 0.73
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.81; acc: 0.78
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.9; acc: 0.73
Batch: 560; loss: 1.11; acc: 0.62
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.82; acc: 0.77
Batch: 660; loss: 1.02; acc: 0.7
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.87; acc: 0.77
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.87; acc: 0.75
Train Epoch over. train_loss: 0.81; train_accuracy: 0.8 

0.0001833228743635118
0.00017595106328371912
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.97; acc: 0.75
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.89
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.58; acc: 0.83
Val Epoch over. val_loss: 0.7266339942528184; val_accuracy: 0.8260350318471338 

The current subspace-distance is: 0.00017595106328371912 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.77; acc: 0.83
Batch: 80; loss: 0.83; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.97; acc: 0.75
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.65; acc: 0.91
Batch: 200; loss: 0.76; acc: 0.8
Batch: 220; loss: 0.79; acc: 0.73
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.89
Batch: 280; loss: 0.78; acc: 0.78
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.62; acc: 0.89
Batch: 340; loss: 0.89; acc: 0.73
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.9; acc: 0.69
Batch: 400; loss: 0.94; acc: 0.7
Batch: 420; loss: 0.96; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.76; acc: 0.8
Batch: 480; loss: 0.66; acc: 0.89
Batch: 500; loss: 0.72; acc: 0.78
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.81
Batch: 560; loss: 0.81; acc: 0.8
Batch: 580; loss: 0.94; acc: 0.78
Batch: 600; loss: 0.94; acc: 0.75
Batch: 620; loss: 0.86; acc: 0.75
Batch: 640; loss: 0.77; acc: 0.8
Batch: 660; loss: 0.87; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.89
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.84; acc: 0.77
Batch: 740; loss: 0.95; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.79; train_accuracy: 0.8 

0.00018462773005012423
0.00017811630095820874
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.97; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 0.57; acc: 0.83
Val Epoch over. val_loss: 0.7231354462872644; val_accuracy: 0.8247412420382165 

The current subspace-distance is: 0.00017811630095820874 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.86
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.88; acc: 0.78
Batch: 180; loss: 0.83; acc: 0.81
Batch: 200; loss: 0.82; acc: 0.77
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.78; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.79; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.9; acc: 0.7
Batch: 400; loss: 0.97; acc: 0.75
Batch: 420; loss: 0.89; acc: 0.75
Batch: 440; loss: 0.81; acc: 0.77
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.67; acc: 0.88
Batch: 500; loss: 0.75; acc: 0.81
Batch: 520; loss: 0.82; acc: 0.73
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.77; acc: 0.78
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.84; acc: 0.73
Batch: 620; loss: 0.85; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.84; acc: 0.75
Batch: 680; loss: 0.74; acc: 0.8
Batch: 700; loss: 0.87; acc: 0.83
Batch: 720; loss: 0.76; acc: 0.81
Batch: 740; loss: 0.85; acc: 0.77
Batch: 760; loss: 0.74; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.81
Train Epoch over. train_loss: 0.78; train_accuracy: 0.8 

0.0001909914135467261
0.0001840599870774895
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.57; acc: 0.83
Val Epoch over. val_loss: 0.7120173770910615; val_accuracy: 0.8253383757961783 

The current subspace-distance is: 0.0001840599870774895 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.7
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.85; acc: 0.75
Batch: 200; loss: 0.92; acc: 0.72
Batch: 220; loss: 0.86; acc: 0.67
Batch: 240; loss: 0.96; acc: 0.72
Batch: 260; loss: 0.74; acc: 0.8
Batch: 280; loss: 0.88; acc: 0.73
Batch: 300; loss: 0.72; acc: 0.77
Batch: 320; loss: 0.8; acc: 0.84
Batch: 340; loss: 0.96; acc: 0.69
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.74; acc: 0.83
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 0.98; acc: 0.77
Batch: 440; loss: 0.77; acc: 0.75
Batch: 460; loss: 0.81; acc: 0.8
Batch: 480; loss: 0.7; acc: 0.78
Batch: 500; loss: 0.83; acc: 0.8
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.76; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.87; acc: 0.73
Batch: 640; loss: 0.73; acc: 0.73
Batch: 660; loss: 0.9; acc: 0.77
Batch: 680; loss: 0.67; acc: 0.84
Batch: 700; loss: 0.76; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.83
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.84; acc: 0.8
Batch: 780; loss: 0.83; acc: 0.81
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00019135471666231751
0.00018664645904209465
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.95; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.56; acc: 0.84
Val Epoch over. val_loss: 0.7057554774982914; val_accuracy: 0.8279259554140127 

The current subspace-distance is: 0.00018664645904209465 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.77; acc: 0.86
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.94; acc: 0.8
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.81; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.76; acc: 0.77
Batch: 220; loss: 0.84; acc: 0.75
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.9; acc: 0.72
Batch: 360; loss: 0.82; acc: 0.75
Batch: 380; loss: 0.76; acc: 0.75
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.94; acc: 0.77
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 0.84; acc: 0.78
Batch: 480; loss: 0.8; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.91
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.79; acc: 0.75
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 0.79; acc: 0.81
Batch: 620; loss: 0.94; acc: 0.72
Batch: 640; loss: 0.75; acc: 0.77
Batch: 660; loss: 0.72; acc: 0.86
Batch: 680; loss: 0.85; acc: 0.78
Batch: 700; loss: 0.84; acc: 0.73
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.74; acc: 0.84
Batch: 760; loss: 0.82; acc: 0.78
Batch: 780; loss: 0.63; acc: 0.91
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00019499614427331835
0.00018580042524263263
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.56; acc: 0.83
Val Epoch over. val_loss: 0.697174364973785; val_accuracy: 0.8290207006369427 

The current subspace-distance is: 0.00018580042524263263 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.89; acc: 0.73
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.84
Batch: 160; loss: 0.73; acc: 0.88
Batch: 180; loss: 0.97; acc: 0.7
Batch: 200; loss: 0.8; acc: 0.81
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.73; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.89
Batch: 380; loss: 0.78; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.88
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.66; acc: 0.86
Batch: 540; loss: 0.88; acc: 0.77
Batch: 560; loss: 0.68; acc: 0.88
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.76; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.89; acc: 0.73
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.75; acc: 0.75
Batch: 760; loss: 0.79; acc: 0.8
Batch: 780; loss: 0.82; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00019927385437767953
0.00019157491624355316
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.55; acc: 0.83
Val Epoch over. val_loss: 0.6886800730684001; val_accuracy: 0.82921974522293 

The current subspace-distance is: 0.00019157491624355316 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.77; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 1.01; acc: 0.75
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.78; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.89
Batch: 300; loss: 0.88; acc: 0.77
Batch: 320; loss: 0.74; acc: 0.86
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.8; acc: 0.78
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.79; acc: 0.78
Batch: 420; loss: 0.87; acc: 0.73
Batch: 440; loss: 0.81; acc: 0.77
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.83; acc: 0.8
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.78; acc: 0.72
Batch: 580; loss: 0.65; acc: 0.88
Batch: 600; loss: 0.84; acc: 0.73
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.77; acc: 0.78
Batch: 700; loss: 0.93; acc: 0.7
Batch: 720; loss: 0.77; acc: 0.81
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.8; acc: 0.78
Train Epoch over. train_loss: 0.74; train_accuracy: 0.81 

0.0001995213533518836
0.00019257488020230085
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.54; acc: 0.86
Val Epoch over. val_loss: 0.6777149725491833; val_accuracy: 0.832703025477707 

The current subspace-distance is: 0.00019257488020230085 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.75
Batch: 200; loss: 0.72; acc: 0.86
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.8; acc: 0.78
Batch: 320; loss: 0.7; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.81
Batch: 360; loss: 0.87; acc: 0.77
Batch: 380; loss: 0.72; acc: 0.8
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.96; acc: 0.67
Batch: 460; loss: 0.66; acc: 0.86
Batch: 480; loss: 0.76; acc: 0.8
Batch: 500; loss: 0.77; acc: 0.81
Batch: 520; loss: 0.82; acc: 0.72
Batch: 540; loss: 0.8; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.88
Batch: 580; loss: 0.71; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.8
Batch: 620; loss: 0.75; acc: 0.8
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.67; acc: 0.88
Batch: 720; loss: 0.7; acc: 0.81
Batch: 740; loss: 0.5; acc: 0.94
Batch: 760; loss: 0.76; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.88
Train Epoch over. train_loss: 0.73; train_accuracy: 0.81 

0.00020181870786473155
0.00019502388022374362
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.94
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.6611405265559057; val_accuracy: 0.8376791401273885 

The current subspace-distance is: 0.00019502388022374362 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.81
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.79; acc: 0.75
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.89
Batch: 160; loss: 0.83; acc: 0.8
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.9; acc: 0.78
Batch: 220; loss: 0.74; acc: 0.81
Batch: 240; loss: 0.78; acc: 0.81
Batch: 260; loss: 0.73; acc: 0.8
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.75; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.78
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.75; acc: 0.84
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 0.76; acc: 0.84
Batch: 480; loss: 0.77; acc: 0.81
Batch: 500; loss: 0.78; acc: 0.75
Batch: 520; loss: 0.83; acc: 0.75
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.86
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.94; acc: 0.67
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.85; acc: 0.7
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.8; acc: 0.78
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.81 

0.00020436401246115565
0.00019783641619142145
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.84
Val Epoch over. val_loss: 0.6648767393106109; val_accuracy: 0.836484872611465 

The current subspace-distance is: 0.00019783641619142145 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 0.9; acc: 0.66
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.93; acc: 0.75
Batch: 380; loss: 0.67; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.73; acc: 0.84
Batch: 440; loss: 0.84; acc: 0.73
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.81
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.84; acc: 0.8
Batch: 740; loss: 0.85; acc: 0.72
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.91
Train Epoch over. train_loss: 0.72; train_accuracy: 0.81 

0.00020792112627532333
0.0001989395241253078
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.94
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.89
Val Epoch over. val_loss: 0.6560597848740353; val_accuracy: 0.8395700636942676 

The current subspace-distance is: 0.0001989395241253078 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.73
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.89
Batch: 160; loss: 0.85; acc: 0.8
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.7; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.77; acc: 0.81
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.8; acc: 0.78
Batch: 660; loss: 0.62; acc: 0.91
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.87; acc: 0.75
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.68; acc: 0.78
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.000208150057005696
0.00020339734328445047
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.6553074514410299; val_accuracy: 0.8369824840764332 

The current subspace-distance is: 0.00020339734328445047 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.5; acc: 0.94
Batch: 140; loss: 0.74; acc: 0.78
Batch: 160; loss: 0.77; acc: 0.75
Batch: 180; loss: 0.94; acc: 0.64
Batch: 200; loss: 0.88; acc: 0.7
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.83; acc: 0.77
Batch: 340; loss: 0.74; acc: 0.81
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.81; acc: 0.8
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.89
Batch: 480; loss: 0.76; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.73
Batch: 520; loss: 0.74; acc: 0.8
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.83
Batch: 580; loss: 0.79; acc: 0.78
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.52; acc: 0.92
Batch: 640; loss: 0.83; acc: 0.83
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.73; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.8
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00021163640485610813
0.00020423129899427295
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.84
Val Epoch over. val_loss: 0.6502629476747696; val_accuracy: 0.8382762738853503 

The current subspace-distance is: 0.00020423129899427295 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.89; acc: 0.77
Batch: 180; loss: 0.64; acc: 0.88
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.88
Batch: 240; loss: 0.74; acc: 0.78
Batch: 260; loss: 0.73; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.8; acc: 0.73
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.85; acc: 0.75
Batch: 380; loss: 0.71; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.99; acc: 0.69
Batch: 440; loss: 0.85; acc: 0.72
Batch: 460; loss: 0.84; acc: 0.77
Batch: 480; loss: 0.74; acc: 0.86
Batch: 500; loss: 0.91; acc: 0.77
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.85; acc: 0.77
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.73; acc: 0.83
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 0.77; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.94
Batch: 700; loss: 0.81; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.0002108369953930378
0.00020372499420773238
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.6510855337237097; val_accuracy: 0.8382762738853503 

The current subspace-distance is: 0.00020372499420773238 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.8; acc: 0.78
Batch: 60; loss: 0.55; acc: 0.95
Batch: 80; loss: 0.91; acc: 0.73
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.75; acc: 0.8
Batch: 220; loss: 0.76; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.7; acc: 0.83
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.64; acc: 0.89
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.81; acc: 0.78
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.81; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.86
Batch: 520; loss: 0.81; acc: 0.78
Batch: 540; loss: 0.82; acc: 0.8
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.91
Batch: 680; loss: 0.72; acc: 0.84
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.74; acc: 0.84
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.78; acc: 0.73
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.0002139473072020337
0.00020753673743456602
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.6474257950570174; val_accuracy: 0.8382762738853503 

The current subspace-distance is: 0.00020753673743456602 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.87; acc: 0.78
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.85; acc: 0.77
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.8; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.8; acc: 0.77
Batch: 160; loss: 0.77; acc: 0.77
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.67; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.73; acc: 0.83
Batch: 260; loss: 0.8; acc: 0.72
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.7; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.69; acc: 0.86
Batch: 440; loss: 0.96; acc: 0.66
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.65; acc: 0.84
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.72; acc: 0.86
Batch: 540; loss: 0.86; acc: 0.73
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.79; acc: 0.73
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.81
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.66; acc: 0.77
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.8; acc: 0.81
Batch: 740; loss: 0.89; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.88
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00021436563110910356
0.00020579136617016047
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.6404298891307442; val_accuracy: 0.8409633757961783 

The current subspace-distance is: 0.00020579136617016047 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.78; acc: 0.81
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.83; acc: 0.78
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.89
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.85; acc: 0.75
Batch: 320; loss: 0.54; acc: 0.92
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.76; acc: 0.75
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.83; acc: 0.77
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.73; acc: 0.81
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.82; acc: 0.73
Batch: 600; loss: 0.74; acc: 0.8
Batch: 620; loss: 0.79; acc: 0.78
Batch: 640; loss: 0.83; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.75; acc: 0.84
Batch: 700; loss: 0.67; acc: 0.84
Batch: 720; loss: 0.87; acc: 0.77
Batch: 740; loss: 0.92; acc: 0.69
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.00021346870926208794
0.0002070915070362389
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.88
Val Epoch over. val_loss: 0.6381206018909528; val_accuracy: 0.8431528662420382 

The current subspace-distance is: 0.0002070915070362389 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.95
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.78
Batch: 180; loss: 0.77; acc: 0.75
Batch: 200; loss: 0.75; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.71; acc: 0.88
Batch: 260; loss: 0.72; acc: 0.84
Batch: 280; loss: 0.85; acc: 0.77
Batch: 300; loss: 0.8; acc: 0.75
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.82; acc: 0.77
Batch: 400; loss: 0.77; acc: 0.77
Batch: 420; loss: 0.66; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.82; acc: 0.77
Batch: 480; loss: 0.72; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.88
Batch: 520; loss: 0.81; acc: 0.8
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.86; acc: 0.78
Batch: 580; loss: 0.86; acc: 0.69
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.78
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.81; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.78
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.75; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.00021446324535645545
0.00020663479517679662
Batch: 0; loss: 0.6; acc: 0.92
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.86
Val Epoch over. val_loss: 0.6410775909757918; val_accuracy: 0.8416600318471338 

The current subspace-distance is: 0.00020663479517679662 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.57; acc: 0.91
Batch: 180; loss: 0.64; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.78; acc: 0.75
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.84
Batch: 280; loss: 0.61; acc: 0.89
Batch: 300; loss: 0.72; acc: 0.75
Batch: 320; loss: 0.78; acc: 0.73
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.73; acc: 0.78
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.8; acc: 0.77
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.73
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.79; acc: 0.75
Batch: 600; loss: 0.65; acc: 0.88
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 0.57; acc: 0.94
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.73; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.91
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.0002140438591595739
0.00020825488900300115
Batch: 0; loss: 0.59; acc: 0.92
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.6335999823299943; val_accuracy: 0.841062898089172 

The current subspace-distance is: 0.00020825488900300115 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.77
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.88
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.78; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.79; acc: 0.73
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.78; acc: 0.83
Batch: 460; loss: 0.83; acc: 0.83
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.83; acc: 0.73
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.9; acc: 0.78
Batch: 620; loss: 0.74; acc: 0.81
Batch: 640; loss: 0.66; acc: 0.88
Batch: 660; loss: 0.69; acc: 0.78
Batch: 680; loss: 0.71; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.79; acc: 0.77
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.00021629974071402103
0.0002091969217872247
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.88
Val Epoch over. val_loss: 0.6379600028703167; val_accuracy: 0.8382762738853503 

The current subspace-distance is: 0.0002091969217872247 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.75; acc: 0.72
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.78
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.62; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.62; acc: 0.81
Batch: 280; loss: 0.79; acc: 0.78
Batch: 300; loss: 0.75; acc: 0.75
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.78
Batch: 360; loss: 0.72; acc: 0.8
Batch: 380; loss: 0.63; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.66; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 1.0; acc: 0.66
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.6; acc: 0.89
Batch: 600; loss: 0.84; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.73
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.7; acc: 0.88
Batch: 700; loss: 0.86; acc: 0.8
Batch: 720; loss: 0.56; acc: 0.92
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.00021717431081924587
0.00021127695799805224
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.88; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.5; acc: 0.89
Val Epoch over. val_loss: 0.6282766132977358; val_accuracy: 0.8424562101910829 

The current subspace-distance is: 0.00021127695799805224 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.77; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.74; acc: 0.77
Batch: 160; loss: 0.84; acc: 0.77
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.72; acc: 0.8
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.74; acc: 0.8
Batch: 380; loss: 0.82; acc: 0.73
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.81; acc: 0.78
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.76; acc: 0.78
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.95; acc: 0.69
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.98; acc: 0.73
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.0002178180729970336
0.0002083106228383258
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.94
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.52; acc: 0.83
Val Epoch over. val_loss: 0.6389107387157003; val_accuracy: 0.8377786624203821 

The current subspace-distance is: 0.0002083106228383258 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_16_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1

The number of parameters is: 272670

The number of individual parameters is:

8
144
8
8
12
29952
12
12
24
89856
24
24
64
147456
64
64
4096
64
640
10
64
64

nonzero elements in E: 81800992
elements in E: 81801000
fraction nonzero: 0.9999999022016846
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.05
Batch: 20; loss: 2.04; acc: 0.31
Batch: 40; loss: 1.89; acc: 0.45
Batch: 60; loss: 1.71; acc: 0.58
Batch: 80; loss: 1.62; acc: 0.62
Batch: 100; loss: 1.59; acc: 0.61
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.57; acc: 0.61
Batch: 160; loss: 1.51; acc: 0.69
Batch: 180; loss: 1.46; acc: 0.69
Batch: 200; loss: 1.45; acc: 0.69
Batch: 220; loss: 1.57; acc: 0.56
Batch: 240; loss: 1.52; acc: 0.64
Batch: 260; loss: 1.49; acc: 0.64
Batch: 280; loss: 1.46; acc: 0.69
Batch: 300; loss: 1.55; acc: 0.64
Batch: 320; loss: 1.5; acc: 0.66
Batch: 340; loss: 1.4; acc: 0.73
Batch: 360; loss: 1.3; acc: 0.78
Batch: 380; loss: 1.43; acc: 0.69
Batch: 400; loss: 1.32; acc: 0.78
Batch: 420; loss: 1.3; acc: 0.83
Batch: 440; loss: 1.46; acc: 0.62
Batch: 460; loss: 1.26; acc: 0.77
Batch: 480; loss: 1.43; acc: 0.67
Batch: 500; loss: 1.3; acc: 0.77
Batch: 520; loss: 1.34; acc: 0.72
Batch: 540; loss: 1.15; acc: 0.8
Batch: 560; loss: 1.29; acc: 0.73
Batch: 580; loss: 1.38; acc: 0.72
Batch: 600; loss: 1.27; acc: 0.75
Batch: 620; loss: 1.21; acc: 0.83
Batch: 640; loss: 1.24; acc: 0.77
Batch: 660; loss: 1.23; acc: 0.78
Batch: 680; loss: 1.28; acc: 0.77
Batch: 700; loss: 1.26; acc: 0.77
Batch: 720; loss: 1.18; acc: 0.78
Batch: 740; loss: 1.16; acc: 0.81
Batch: 760; loss: 1.27; acc: 0.73
Batch: 780; loss: 1.19; acc: 0.81
Train Epoch over. train_loss: 1.45; train_accuracy: 0.67 

6.35479373158887e-05
5.850422166986391e-05
Batch: 0; loss: 1.15; acc: 0.8
Batch: 20; loss: 1.36; acc: 0.62
Batch: 40; loss: 0.97; acc: 0.89
Batch: 60; loss: 1.08; acc: 0.83
Batch: 80; loss: 1.03; acc: 0.84
Batch: 100; loss: 1.19; acc: 0.84
Batch: 120; loss: 1.29; acc: 0.72
Batch: 140; loss: 1.08; acc: 0.88
Val Epoch over. val_loss: 1.1806270043561413; val_accuracy: 0.792296974522293 

The current subspace-distance is: 5.850422166986391e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.72
Batch: 20; loss: 1.27; acc: 0.73
Batch: 40; loss: 1.15; acc: 0.8
Batch: 60; loss: 1.27; acc: 0.73
Batch: 80; loss: 1.18; acc: 0.83
Batch: 100; loss: 1.33; acc: 0.64
Batch: 120; loss: 1.15; acc: 0.81
Batch: 140; loss: 1.16; acc: 0.77
Batch: 160; loss: 1.12; acc: 0.84
Batch: 180; loss: 1.23; acc: 0.75
Batch: 200; loss: 1.28; acc: 0.69
Batch: 220; loss: 1.15; acc: 0.81
Batch: 240; loss: 1.22; acc: 0.77
Batch: 260; loss: 1.08; acc: 0.81
Batch: 280; loss: 1.1; acc: 0.81
Batch: 300; loss: 1.06; acc: 0.89
Batch: 320; loss: 1.12; acc: 0.81
Batch: 340; loss: 1.22; acc: 0.69
Batch: 360; loss: 1.08; acc: 0.81
Batch: 380; loss: 1.14; acc: 0.77
Batch: 400; loss: 1.04; acc: 0.77
Batch: 420; loss: 1.11; acc: 0.8
Batch: 440; loss: 1.14; acc: 0.8
Batch: 460; loss: 1.05; acc: 0.81
Batch: 480; loss: 1.13; acc: 0.75
Batch: 500; loss: 1.02; acc: 0.86
Batch: 520; loss: 1.09; acc: 0.81
Batch: 540; loss: 1.12; acc: 0.77
Batch: 560; loss: 1.15; acc: 0.73
Batch: 580; loss: 1.12; acc: 0.78
Batch: 600; loss: 1.06; acc: 0.8
Batch: 620; loss: 1.15; acc: 0.77
Batch: 640; loss: 1.01; acc: 0.84
Batch: 660; loss: 1.05; acc: 0.78
Batch: 680; loss: 1.05; acc: 0.83
Batch: 700; loss: 1.11; acc: 0.81
Batch: 720; loss: 1.11; acc: 0.73
Batch: 740; loss: 1.16; acc: 0.77
Batch: 760; loss: 1.08; acc: 0.8
Batch: 780; loss: 1.06; acc: 0.83
Train Epoch over. train_loss: 1.12; train_accuracy: 0.8 

9.021452569868416e-05
8.510160114383325e-05
Batch: 0; loss: 0.98; acc: 0.88
Batch: 20; loss: 1.17; acc: 0.7
Batch: 40; loss: 0.76; acc: 0.92
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 0.85; acc: 0.88
Batch: 100; loss: 1.03; acc: 0.86
Batch: 120; loss: 1.08; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.94
Val Epoch over. val_loss: 1.0034529597136626; val_accuracy: 0.8320063694267515 

The current subspace-distance is: 8.510160114383325e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.89
Batch: 20; loss: 1.02; acc: 0.84
Batch: 40; loss: 1.06; acc: 0.78
Batch: 60; loss: 1.04; acc: 0.73
Batch: 80; loss: 1.2; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.89
Batch: 120; loss: 1.09; acc: 0.78
Batch: 140; loss: 0.95; acc: 0.91
Batch: 160; loss: 1.16; acc: 0.72
Batch: 180; loss: 0.89; acc: 0.81
Batch: 200; loss: 1.07; acc: 0.8
Batch: 220; loss: 1.0; acc: 0.8
Batch: 240; loss: 1.24; acc: 0.69
Batch: 260; loss: 0.93; acc: 0.86
Batch: 280; loss: 1.13; acc: 0.72
Batch: 300; loss: 1.03; acc: 0.8
Batch: 320; loss: 0.94; acc: 0.89
Batch: 340; loss: 0.86; acc: 0.84
Batch: 360; loss: 1.03; acc: 0.8
Batch: 380; loss: 1.0; acc: 0.81
Batch: 400; loss: 1.01; acc: 0.78
Batch: 420; loss: 1.06; acc: 0.73
Batch: 440; loss: 0.95; acc: 0.81
Batch: 460; loss: 0.92; acc: 0.89
Batch: 480; loss: 1.03; acc: 0.81
Batch: 500; loss: 0.93; acc: 0.83
Batch: 520; loss: 0.89; acc: 0.81
Batch: 540; loss: 1.09; acc: 0.7
Batch: 560; loss: 0.91; acc: 0.84
Batch: 580; loss: 0.97; acc: 0.75
Batch: 600; loss: 0.98; acc: 0.84
Batch: 620; loss: 0.97; acc: 0.77
Batch: 640; loss: 1.04; acc: 0.8
Batch: 660; loss: 0.9; acc: 0.89
Batch: 680; loss: 0.92; acc: 0.81
Batch: 700; loss: 0.93; acc: 0.89
Batch: 720; loss: 0.98; acc: 0.77
Batch: 740; loss: 1.06; acc: 0.8
Batch: 760; loss: 0.91; acc: 0.8
Batch: 780; loss: 0.88; acc: 0.83
Train Epoch over. train_loss: 0.99; train_accuracy: 0.81 

0.00010761099838418886
0.00010321362788090482
Batch: 0; loss: 0.86; acc: 0.88
Batch: 20; loss: 1.06; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.94
Batch: 60; loss: 0.89; acc: 0.75
Batch: 80; loss: 0.72; acc: 0.92
Batch: 100; loss: 0.93; acc: 0.86
Batch: 120; loss: 0.99; acc: 0.78
Batch: 140; loss: 0.73; acc: 0.92
Val Epoch over. val_loss: 0.8882873635383168; val_accuracy: 0.8403662420382165 

The current subspace-distance is: 0.00010321362788090482 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.88
Batch: 20; loss: 1.0; acc: 0.83
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 0.94; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.86
Batch: 140; loss: 0.85; acc: 0.88
Batch: 160; loss: 0.91; acc: 0.81
Batch: 180; loss: 1.0; acc: 0.75
Batch: 200; loss: 1.01; acc: 0.81
Batch: 220; loss: 1.0; acc: 0.8
Batch: 240; loss: 0.93; acc: 0.78
Batch: 260; loss: 0.85; acc: 0.84
Batch: 280; loss: 0.87; acc: 0.81
Batch: 300; loss: 0.94; acc: 0.81
Batch: 320; loss: 0.96; acc: 0.86
Batch: 340; loss: 0.87; acc: 0.84
Batch: 360; loss: 0.99; acc: 0.77
Batch: 380; loss: 0.9; acc: 0.81
Batch: 400; loss: 0.78; acc: 0.88
Batch: 420; loss: 0.9; acc: 0.88
Batch: 440; loss: 0.82; acc: 0.91
Batch: 460; loss: 0.85; acc: 0.86
Batch: 480; loss: 0.87; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.88
Batch: 520; loss: 0.95; acc: 0.81
Batch: 540; loss: 0.97; acc: 0.78
Batch: 560; loss: 0.99; acc: 0.77
Batch: 580; loss: 0.84; acc: 0.92
Batch: 600; loss: 0.88; acc: 0.86
Batch: 620; loss: 0.87; acc: 0.78
Batch: 640; loss: 1.08; acc: 0.7
Batch: 660; loss: 0.95; acc: 0.84
Batch: 680; loss: 0.96; acc: 0.75
Batch: 700; loss: 0.82; acc: 0.86
Batch: 720; loss: 0.8; acc: 0.83
Batch: 740; loss: 1.01; acc: 0.73
Batch: 760; loss: 0.9; acc: 0.84
Batch: 780; loss: 0.86; acc: 0.81
Train Epoch over. train_loss: 0.91; train_accuracy: 0.82 

0.00012424320448189974
0.00011839703074656427
Batch: 0; loss: 0.78; acc: 0.91
Batch: 20; loss: 0.99; acc: 0.75
Batch: 40; loss: 0.57; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.92
Batch: 100; loss: 0.84; acc: 0.88
Batch: 120; loss: 0.94; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.91
Val Epoch over. val_loss: 0.8043587412803795; val_accuracy: 0.8505175159235668 

The current subspace-distance is: 0.00011839703074656427 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.8; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.88
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 0.81; acc: 0.86
Batch: 120; loss: 0.96; acc: 0.81
Batch: 140; loss: 1.16; acc: 0.66
Batch: 160; loss: 0.76; acc: 0.86
Batch: 180; loss: 0.88; acc: 0.83
Batch: 200; loss: 0.85; acc: 0.83
Batch: 220; loss: 0.85; acc: 0.8
Batch: 240; loss: 0.78; acc: 0.86
Batch: 260; loss: 0.96; acc: 0.8
Batch: 280; loss: 0.86; acc: 0.91
Batch: 300; loss: 0.89; acc: 0.83
Batch: 320; loss: 0.85; acc: 0.75
Batch: 340; loss: 0.87; acc: 0.84
Batch: 360; loss: 0.78; acc: 0.91
Batch: 380; loss: 0.96; acc: 0.83
Batch: 400; loss: 0.78; acc: 0.89
Batch: 420; loss: 0.95; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.88
Batch: 460; loss: 0.84; acc: 0.78
Batch: 480; loss: 0.74; acc: 0.88
Batch: 500; loss: 0.77; acc: 0.88
Batch: 520; loss: 0.85; acc: 0.86
Batch: 540; loss: 0.83; acc: 0.86
Batch: 560; loss: 0.86; acc: 0.83
Batch: 580; loss: 0.81; acc: 0.86
Batch: 600; loss: 0.82; acc: 0.81
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.84; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.89
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.86; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.86
Batch: 740; loss: 0.93; acc: 0.8
Batch: 760; loss: 0.79; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.85; train_accuracy: 0.83 

0.00013688851322513074
0.00013173736806493253
Batch: 0; loss: 0.72; acc: 0.92
Batch: 20; loss: 0.95; acc: 0.77
Batch: 40; loss: 0.52; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.57; acc: 0.94
Batch: 100; loss: 0.75; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.61; acc: 0.91
Val Epoch over. val_loss: 0.7581809231430102; val_accuracy: 0.8544984076433121 

The current subspace-distance is: 0.00013173736806493253 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.81
Batch: 40; loss: 0.77; acc: 0.89
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 0.77; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.88; acc: 0.83
Batch: 180; loss: 0.86; acc: 0.81
Batch: 200; loss: 0.79; acc: 0.8
Batch: 220; loss: 0.7; acc: 0.92
Batch: 240; loss: 0.75; acc: 0.89
Batch: 260; loss: 0.78; acc: 0.88
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.98; acc: 0.73
Batch: 320; loss: 0.71; acc: 0.91
Batch: 340; loss: 0.8; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.72; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.86
Batch: 440; loss: 0.77; acc: 0.83
Batch: 460; loss: 0.82; acc: 0.81
Batch: 480; loss: 0.81; acc: 0.83
Batch: 500; loss: 1.0; acc: 0.72
Batch: 520; loss: 0.83; acc: 0.8
Batch: 540; loss: 0.78; acc: 0.84
Batch: 560; loss: 0.78; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 0.83; acc: 0.84
Batch: 640; loss: 0.74; acc: 0.89
Batch: 660; loss: 0.76; acc: 0.84
Batch: 680; loss: 0.8; acc: 0.84
Batch: 700; loss: 0.84; acc: 0.78
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.7; acc: 0.81
Batch: 760; loss: 0.76; acc: 0.83
Batch: 780; loss: 0.83; acc: 0.78
Train Epoch over. train_loss: 0.79; train_accuracy: 0.83 

0.00014813100278843194
0.00014371365250553936
Batch: 0; loss: 0.67; acc: 0.94
Batch: 20; loss: 0.95; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.69; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.86
Val Epoch over. val_loss: 0.7055777506843494; val_accuracy: 0.8603702229299363 

The current subspace-distance is: 0.00014371365250553936 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.92
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.87; acc: 0.83
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.84
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.92
Batch: 160; loss: 0.88; acc: 0.77
Batch: 180; loss: 0.76; acc: 0.86
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.95; acc: 0.72
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 0.86; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.88
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.77; acc: 0.83
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.8; acc: 0.81
Batch: 400; loss: 0.97; acc: 0.73
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.88
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.91; acc: 0.78
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.89
Batch: 660; loss: 0.66; acc: 0.88
Batch: 680; loss: 0.7; acc: 0.91
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.76; acc: 0.84
Batch: 780; loss: 0.77; acc: 0.83
Train Epoch over. train_loss: 0.75; train_accuracy: 0.84 

0.00016024989599827677
0.0001544485567137599
Batch: 0; loss: 0.63; acc: 0.95
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.89
Val Epoch over. val_loss: 0.6537802378842785; val_accuracy: 0.8692277070063694 

The current subspace-distance is: 0.0001544485567137599 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.83
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.84; acc: 0.77
Batch: 180; loss: 0.65; acc: 0.91
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 1.0; acc: 0.78
Batch: 260; loss: 0.73; acc: 0.86
Batch: 280; loss: 0.88; acc: 0.8
Batch: 300; loss: 0.72; acc: 0.81
Batch: 320; loss: 0.62; acc: 0.91
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.85; acc: 0.78
Batch: 380; loss: 0.83; acc: 0.83
Batch: 400; loss: 0.78; acc: 0.88
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.7; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.78; acc: 0.75
Batch: 600; loss: 0.67; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.75; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.76; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.84 

0.00017171341460198164
0.000167193342349492
Batch: 0; loss: 0.61; acc: 0.94
Batch: 20; loss: 0.89; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.86
Val Epoch over. val_loss: 0.6139684828223696; val_accuracy: 0.8717157643312102 

The current subspace-distance is: 0.000167193342349492 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.84
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.85; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.78; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.88
Batch: 220; loss: 0.8; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.87; acc: 0.8
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.6; acc: 0.91
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.72; acc: 0.86
Batch: 400; loss: 0.72; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.94
Batch: 440; loss: 0.93; acc: 0.77
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 0.8; acc: 0.78
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.89
Batch: 580; loss: 0.58; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.89
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.91
Batch: 700; loss: 0.82; acc: 0.8
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.68; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.89
Batch: 780; loss: 0.85; acc: 0.78
Train Epoch over. train_loss: 0.67; train_accuracy: 0.85 

0.00018542571342550218
0.00017955288058146834
Batch: 0; loss: 0.57; acc: 0.94
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.89
Val Epoch over. val_loss: 0.5866902766713671; val_accuracy: 0.87390525477707 

The current subspace-distance is: 0.00017955288058146834 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.89
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.88
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.85; acc: 0.72
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.68; acc: 0.86
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.66; acc: 0.81
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.78; acc: 0.81
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.95
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.74; acc: 0.83
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.61; acc: 0.89
Batch: 760; loss: 0.79; acc: 0.75
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00019474989676382393
0.00018851389177143574
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.89
Val Epoch over. val_loss: 0.5610562692022627; val_accuracy: 0.8794785031847133 

The current subspace-distance is: 0.00018851389177143574 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.97
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.71; acc: 0.81
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.67; acc: 0.88
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.97
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.91
Batch: 620; loss: 0.72; acc: 0.83
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.91
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.89
Batch: 740; loss: 0.78; acc: 0.77
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.67; acc: 0.78
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00019944414088968188
0.00019313821394462138
Batch: 0; loss: 0.53; acc: 0.94
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.5458054178079982; val_accuracy: 0.8782842356687898 

The current subspace-distance is: 0.00019313821394462138 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.66; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.95
Batch: 140; loss: 0.65; acc: 0.89
Batch: 160; loss: 0.63; acc: 0.89
Batch: 180; loss: 0.64; acc: 0.86
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.57; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.74; acc: 0.77
Batch: 300; loss: 0.82; acc: 0.77
Batch: 320; loss: 0.66; acc: 0.86
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.82; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.7; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.97
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.92
Batch: 620; loss: 0.63; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.62; train_accuracy: 0.86 

0.00020044409029651433
0.00019519514171406627
Batch: 0; loss: 0.52; acc: 0.94
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.5400105589514326; val_accuracy: 0.8826632165605095 

The current subspace-distance is: 0.00019519514171406627 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.95
Batch: 340; loss: 0.61; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.77; acc: 0.77
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.81
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.49; acc: 0.92
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.61; train_accuracy: 0.86 

0.0002069858746835962
0.00019834392878692597
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.91
Val Epoch over. val_loss: 0.5385344275243723; val_accuracy: 0.8850517515923567 

The current subspace-distance is: 0.00019834392878692597 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.78; acc: 0.77
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.52; acc: 0.91
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.92
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.77
Batch: 580; loss: 0.65; acc: 0.86
Batch: 600; loss: 0.8; acc: 0.8
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.7; acc: 0.89
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.58; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.00020746410882566124
0.00020217073324602097
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5330542899241113; val_accuracy: 0.8837579617834395 

The current subspace-distance is: 0.00020217073324602097 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.94
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.6; acc: 0.89
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.92
Batch: 380; loss: 0.76; acc: 0.77
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.92
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.91
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 0.71; acc: 0.78
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.00021010467025917023
0.00020327397214714438
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.89
Val Epoch over. val_loss: 0.521625379088578; val_accuracy: 0.8831608280254777 

The current subspace-distance is: 0.00020327397214714438 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.59; acc: 0.92
Batch: 200; loss: 0.77; acc: 0.75
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.94
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.59; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.91
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00021295322221703827
0.00020581501303240657
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.5159774181569458; val_accuracy: 0.886046974522293 

The current subspace-distance is: 0.00020581501303240657 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.77
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.77; acc: 0.78
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.97
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.75; acc: 0.8
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.86
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.91
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

0.00021717220079153776
0.0002124334714608267
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.91
Val Epoch over. val_loss: 0.5137239440231566; val_accuracy: 0.8859474522292994 

The current subspace-distance is: 0.0002124334714608267 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.72; acc: 0.83
Batch: 260; loss: 0.7; acc: 0.77
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.71; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.75; acc: 0.83
Batch: 520; loss: 0.83; acc: 0.72
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.8
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

0.00021943356841802597
0.00021207524696365
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.5002774511742744; val_accuracy: 0.8874402866242038 

The current subspace-distance is: 0.00021207524696365 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.77
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.83
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.76; acc: 0.78
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.86 

0.00022050997358746827
0.00021499156719073653
Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.49669370292478304; val_accuracy: 0.8884355095541401 

The current subspace-distance is: 0.00021499156719073653 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.91
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.94
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.95
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.72; acc: 0.77
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.75; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.56; train_accuracy: 0.86 

0.0002248946693725884
0.00021674347226507962
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.4920852549706295; val_accuracy: 0.8890326433121019 

The current subspace-distance is: 0.00021674347226507962 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.76; acc: 0.77
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.56; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.97
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.66; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.7; acc: 0.8
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.59; acc: 0.8
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.94
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.95
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.58; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.95
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.000223219147301279
0.00021849495533388108
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.4923360497708533; val_accuracy: 0.8876393312101911 

The current subspace-distance is: 0.00021849495533388108 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.79; acc: 0.8
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.92
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.95
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.61; acc: 0.75
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.67; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.94
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.95
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00022858141164761037
0.0002190372470067814
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.48987890656586663; val_accuracy: 0.8889331210191083 

The current subspace-distance is: 0.0002190372470067814 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.55; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.95
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.62; acc: 0.78
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.0002268995449412614
0.00021916757395956665
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.92
Val Epoch over. val_loss: 0.4818467507316808; val_accuracy: 0.8913216560509554 

The current subspace-distance is: 0.00021916757395956665 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.52; acc: 0.94
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.75; acc: 0.77
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.55; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00022822379833087325
0.0002207099605584517
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.4915074425138486; val_accuracy: 0.8880374203821656 

The current subspace-distance is: 0.0002207099605584517 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.91
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.95
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.78
Batch: 560; loss: 0.6; acc: 0.8
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.71; acc: 0.81
Batch: 640; loss: 0.68; acc: 0.83
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.77
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0002300870546605438
0.00022117330809123814
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.4916939124180253; val_accuracy: 0.8907245222929936 

The current subspace-distance is: 0.00022117330809123814 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.76; acc: 0.77
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.55; acc: 0.92
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.76; acc: 0.75
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.92
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00022779687424190342
0.00022157176863402128
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Val Epoch over. val_loss: 0.4791534997665199; val_accuracy: 0.8915207006369427 

The current subspace-distance is: 0.00022157176863402128 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.92
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.47; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.92
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00023038050858303905
0.0002235776191810146
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.92
Val Epoch over. val_loss: 0.47623311770949395; val_accuracy: 0.8922173566878981 

The current subspace-distance is: 0.0002235776191810146 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.86
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.74; acc: 0.73
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.88; acc: 0.75
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.8
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.72; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.89
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00023201899603009224
0.000224785297177732
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.92
Val Epoch over. val_loss: 0.48173597967548737; val_accuracy: 0.8907245222929936 

The current subspace-distance is: 0.000224785297177732 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.75; acc: 0.73
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.65; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.66; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00023133904323913157
0.00022319686831906438
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.4808290741246217; val_accuracy: 0.8911226114649682 

The current subspace-distance is: 0.00022319686831906438 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.81
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.6; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.75
Batch: 420; loss: 0.4; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.52; acc: 0.91
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00023120037803892046
0.0002249658718938008
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.4782245683062608; val_accuracy: 0.8908240445859873 

The current subspace-distance is: 0.0002249658718938008 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_16_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1

The number of parameters is: 272670

The number of individual parameters is:

8
144
8
8
12
29952
12
12
24
89856
24
24
64
147456
64
64
4096
64
640
10
64
64

nonzero elements in E: 109067989
elements in E: 109068000
fraction nonzero: 0.9999998991454873
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.38; acc: 0.08
Batch: 20; loss: 2.03; acc: 0.34
Batch: 40; loss: 1.72; acc: 0.59
Batch: 60; loss: 1.69; acc: 0.58
Batch: 80; loss: 1.56; acc: 0.66
Batch: 100; loss: 1.61; acc: 0.7
Batch: 120; loss: 1.56; acc: 0.64
Batch: 140; loss: 1.43; acc: 0.73
Batch: 160; loss: 1.39; acc: 0.77
Batch: 180; loss: 1.41; acc: 0.72
Batch: 200; loss: 1.47; acc: 0.69
Batch: 220; loss: 1.27; acc: 0.77
Batch: 240; loss: 1.28; acc: 0.81
Batch: 260; loss: 1.27; acc: 0.75
Batch: 280; loss: 1.29; acc: 0.77
Batch: 300; loss: 1.19; acc: 0.77
Batch: 320; loss: 1.29; acc: 0.72
Batch: 340; loss: 1.32; acc: 0.77
Batch: 360; loss: 1.37; acc: 0.66
Batch: 380; loss: 1.11; acc: 0.84
Batch: 400; loss: 1.13; acc: 0.86
Batch: 420; loss: 1.19; acc: 0.83
Batch: 440; loss: 1.14; acc: 0.84
Batch: 460; loss: 1.24; acc: 0.77
Batch: 480; loss: 1.18; acc: 0.75
Batch: 500; loss: 1.08; acc: 0.84
Batch: 520; loss: 1.21; acc: 0.81
Batch: 540; loss: 1.16; acc: 0.81
Batch: 560; loss: 1.03; acc: 0.84
Batch: 580; loss: 1.22; acc: 0.7
Batch: 600; loss: 1.22; acc: 0.72
Batch: 620; loss: 1.05; acc: 0.83
Batch: 640; loss: 1.17; acc: 0.8
Batch: 660; loss: 1.24; acc: 0.78
Batch: 680; loss: 1.0; acc: 0.88
Batch: 700; loss: 1.11; acc: 0.81
Batch: 720; loss: 1.12; acc: 0.75
Batch: 740; loss: 0.98; acc: 0.88
Batch: 760; loss: 0.92; acc: 0.89
Batch: 780; loss: 1.04; acc: 0.84
Train Epoch over. train_loss: 1.3; train_accuracy: 0.73 

2.6358497052569874e-05
7.742661182419397e-06
Batch: 0; loss: 1.08; acc: 0.81
Batch: 20; loss: 1.19; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.91
Batch: 60; loss: 1.0; acc: 0.78
Batch: 80; loss: 0.89; acc: 0.86
Batch: 100; loss: 0.93; acc: 0.88
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 0.94; acc: 0.91
Val Epoch over. val_loss: 1.00725930777325; val_accuracy: 0.8201632165605095 

The current subspace-distance is: 7.742661182419397e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.81
Batch: 20; loss: 1.04; acc: 0.83
Batch: 40; loss: 1.05; acc: 0.8
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.81
Batch: 100; loss: 0.99; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.89
Batch: 140; loss: 1.04; acc: 0.8
Batch: 160; loss: 0.88; acc: 0.86
Batch: 180; loss: 0.92; acc: 0.81
Batch: 200; loss: 0.99; acc: 0.78
Batch: 220; loss: 0.96; acc: 0.86
Batch: 240; loss: 1.14; acc: 0.75
Batch: 260; loss: 1.08; acc: 0.78
Batch: 280; loss: 1.03; acc: 0.8
Batch: 300; loss: 0.92; acc: 0.86
Batch: 320; loss: 0.94; acc: 0.8
Batch: 340; loss: 0.92; acc: 0.78
Batch: 360; loss: 0.95; acc: 0.86
Batch: 380; loss: 0.91; acc: 0.88
Batch: 400; loss: 1.03; acc: 0.73
Batch: 420; loss: 0.85; acc: 0.88
Batch: 440; loss: 0.95; acc: 0.84
Batch: 460; loss: 1.0; acc: 0.78
Batch: 480; loss: 0.98; acc: 0.81
Batch: 500; loss: 0.89; acc: 0.84
Batch: 520; loss: 0.85; acc: 0.86
Batch: 540; loss: 0.96; acc: 0.83
Batch: 560; loss: 1.02; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.84
Batch: 600; loss: 0.88; acc: 0.89
Batch: 620; loss: 0.79; acc: 0.91
Batch: 640; loss: 1.11; acc: 0.8
Batch: 660; loss: 0.87; acc: 0.81
Batch: 680; loss: 0.91; acc: 0.84
Batch: 700; loss: 0.79; acc: 0.88
Batch: 720; loss: 0.84; acc: 0.88
Batch: 740; loss: 0.95; acc: 0.78
Batch: 760; loss: 0.92; acc: 0.86
Batch: 780; loss: 0.9; acc: 0.81
Train Epoch over. train_loss: 0.97; train_accuracy: 0.81 

3.100300091318786e-05
1.1454688319645356e-05
Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 1.0; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.95
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.91
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.91
Val Epoch over. val_loss: 0.8200597087289118; val_accuracy: 0.8469347133757962 

The current subspace-distance is: 1.1454688319645356e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.78
Batch: 20; loss: 0.91; acc: 0.86
Batch: 40; loss: 0.77; acc: 0.89
Batch: 60; loss: 0.9; acc: 0.77
Batch: 80; loss: 0.79; acc: 0.86
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 0.91; acc: 0.81
Batch: 140; loss: 0.95; acc: 0.81
Batch: 160; loss: 0.89; acc: 0.81
Batch: 180; loss: 0.83; acc: 0.89
Batch: 200; loss: 0.86; acc: 0.86
Batch: 220; loss: 0.76; acc: 0.88
Batch: 240; loss: 0.8; acc: 0.86
Batch: 260; loss: 0.78; acc: 0.86
Batch: 280; loss: 0.72; acc: 0.86
Batch: 300; loss: 0.7; acc: 0.92
Batch: 320; loss: 0.83; acc: 0.84
Batch: 340; loss: 0.73; acc: 0.94
Batch: 360; loss: 0.9; acc: 0.8
Batch: 380; loss: 0.84; acc: 0.8
Batch: 400; loss: 0.82; acc: 0.78
Batch: 420; loss: 0.76; acc: 0.86
Batch: 440; loss: 0.9; acc: 0.78
Batch: 460; loss: 0.87; acc: 0.84
Batch: 480; loss: 0.95; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.88
Batch: 540; loss: 0.73; acc: 0.84
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.91
Batch: 640; loss: 0.76; acc: 0.83
Batch: 660; loss: 0.7; acc: 0.89
Batch: 680; loss: 0.69; acc: 0.91
Batch: 700; loss: 0.85; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.84; acc: 0.8
Train Epoch over. train_loss: 0.81; train_accuracy: 0.84 

3.614766683313064e-05
1.3942971236247104e-05
Batch: 0; loss: 0.77; acc: 0.88
Batch: 20; loss: 0.86; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.98
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.52; acc: 0.97
Val Epoch over. val_loss: 0.7000789492373254; val_accuracy: 0.8701234076433121 

The current subspace-distance is: 1.3942971236247104e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.94
Batch: 160; loss: 0.66; acc: 0.92
Batch: 180; loss: 0.66; acc: 0.92
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.88
Batch: 240; loss: 0.67; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.89
Batch: 280; loss: 0.8; acc: 0.77
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.81; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.84
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.8; acc: 0.8
Batch: 460; loss: 0.65; acc: 0.91
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.77; acc: 0.8
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.71; acc: 0.81
Batch: 600; loss: 0.85; acc: 0.75
Batch: 620; loss: 0.87; acc: 0.78
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.69; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.73; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.84
Batch: 780; loss: 0.68; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.86 

3.988854950875975e-05
1.605328725418076e-05
Batch: 0; loss: 0.68; acc: 0.95
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.6107473922003607; val_accuracy: 0.8801751592356688 

The current subspace-distance is: 1.605328725418076e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.47; acc: 0.97
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.97
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.8; acc: 0.78
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.83
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.69; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.63; train_accuracy: 0.87 

4.321832238929346e-05
1.8212593204225414e-05
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.97
Val Epoch over. val_loss: 0.5604443540618678; val_accuracy: 0.8854498407643312 

The current subspace-distance is: 1.8212593204225414e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.95
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.66; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.91
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.76; acc: 0.8
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

4.689873458119109e-05
2.0565459635690786e-05
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.5126008674217637; val_accuracy: 0.8915207006369427 

The current subspace-distance is: 2.0565459635690786e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.95
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.65; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.43; acc: 0.94
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.95
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

5.045014404458925e-05
2.3067985239322297e-05
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.47267792701341543; val_accuracy: 0.8979896496815286 

The current subspace-distance is: 2.3067985239322297e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.97
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.92
Batch: 320; loss: 0.54; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.8
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.54; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

5.26011353940703e-05
2.313393088115845e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.44823713211496924; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 2.313393088115845e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.98
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.98
Batch: 340; loss: 0.47; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.81
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

5.468916424433701e-05
2.3549631805508398e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.41413747618912133; val_accuracy: 0.9053542993630573 

The current subspace-distance is: 2.3549631805508398e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.97
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.95
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.89 

5.7330969866598025e-05
2.4881430363166146e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3992539900503341; val_accuracy: 0.9086385350318471 

The current subspace-distance is: 2.4881430363166146e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.8783265558304265e-05
2.5948957045329735e-05
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.3897761578203007; val_accuracy: 0.910031847133758 

The current subspace-distance is: 2.5948957045329735e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.63; acc: 0.8
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.8
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.9014582802774385e-05
2.504402982594911e-05
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.38660801434592834; val_accuracy: 0.9099323248407644 

The current subspace-distance is: 2.504402982594911e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.97
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.58; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.36; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.01268548052758e-05
2.7474266971694306e-05
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.37885371524437217; val_accuracy: 0.9107285031847133 

The current subspace-distance is: 2.7474266971694306e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.98
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.59; acc: 0.77
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.78
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.0583261074498296e-05
2.6926181817543693e-05
Batch: 0; loss: 0.36; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3832464711681293; val_accuracy: 0.9099323248407644 

The current subspace-distance is: 2.6926181817543693e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.042215318302624e-05
2.595843398012221e-05
Batch: 0; loss: 0.35; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3726879512048831; val_accuracy: 0.9125199044585988 

The current subspace-distance is: 2.595843398012221e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.19576676399447e-05
2.8371010557748377e-05
Batch: 0; loss: 0.34; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.36709296921635887; val_accuracy: 0.911922770700637 

The current subspace-distance is: 2.8371010557748377e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.285560084506869e-05
2.9345645089051686e-05
Batch: 0; loss: 0.34; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3667934384125813; val_accuracy: 0.9114251592356688 

The current subspace-distance is: 2.9345645089051686e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.78
Batch: 560; loss: 0.39; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.257524364627898e-05
2.7311001758789644e-05
Batch: 0; loss: 0.33; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3602003500719739; val_accuracy: 0.9132165605095541 

The current subspace-distance is: 2.7311001758789644e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.81
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.260735244723037e-05
2.6082356271217577e-05
Batch: 0; loss: 0.33; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3557739557733961; val_accuracy: 0.9141122611464968 

The current subspace-distance is: 2.6082356271217577e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.97
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.9 

6.285465497057885e-05
2.6434639949002303e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3488337387135074; val_accuracy: 0.9143113057324841 

The current subspace-distance is: 2.6434639949002303e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.9 

6.308178853942081e-05
2.6525694920565e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.35006666658030955; val_accuracy: 0.9144108280254777 

The current subspace-distance is: 2.6525694920565e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.9 

6.410954665625468e-05
2.7785799829871394e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.34576166321517554; val_accuracy: 0.9159036624203821 

The current subspace-distance is: 2.7785799829871394e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.95
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.9 

6.347116868710145e-05
2.7661533749778755e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.35121864668882574; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.7661533749778755e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.81
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.395785021595657e-05
2.783277159323916e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.34635725047937627; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.783277159323916e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.81
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.8
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.98
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.98
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.400329584721476e-05
2.8552807634696364e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.34404378702306443; val_accuracy: 0.9155055732484076 

The current subspace-distance is: 2.8552807634696364e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.21; acc: 1.0
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.418672273866832e-05
2.8156831831438467e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.348414008784446; val_accuracy: 0.9145103503184714 

The current subspace-distance is: 2.8156831831438467e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.400974962161854e-05
2.7512131055118516e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3427406166009842; val_accuracy: 0.9159036624203821 

The current subspace-distance is: 2.7512131055118516e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.58; acc: 0.78
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.449139618780464e-05
2.744512130448129e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.34573584690595127; val_accuracy: 0.9155055732484076 

The current subspace-distance is: 2.744512130448129e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.98
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.98
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.97
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.570255209226161e-05
2.9258762879180722e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3432678745430746; val_accuracy: 0.9148089171974523 

The current subspace-distance is: 2.9258762879180722e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.459269934566692e-05
2.9770220862701535e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.33726970073144147; val_accuracy: 0.9166003184713376 

The current subspace-distance is: 2.9770220862701535e-05 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_16_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1

The number of parameters is: 272670

The number of individual parameters is:

8
144
8
8
12
29952
12
12
24
89856
24
24
64
147456
64
64
4096
64
640
10
64
64

nonzero elements in E: 136334989
elements in E: 136335000
fraction nonzero: 0.9999999193163898
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.39; acc: 0.09
Batch: 20; loss: 1.93; acc: 0.48
Batch: 40; loss: 1.69; acc: 0.58
Batch: 60; loss: 1.51; acc: 0.7
Batch: 80; loss: 1.52; acc: 0.61
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.27; acc: 0.81
Batch: 140; loss: 1.31; acc: 0.75
Batch: 160; loss: 1.39; acc: 0.62
Batch: 180; loss: 1.26; acc: 0.8
Batch: 200; loss: 1.29; acc: 0.7
Batch: 220; loss: 1.24; acc: 0.77
Batch: 240; loss: 1.29; acc: 0.72
Batch: 260; loss: 1.31; acc: 0.75
Batch: 280; loss: 1.18; acc: 0.75
Batch: 300; loss: 1.18; acc: 0.77
Batch: 320; loss: 1.3; acc: 0.66
Batch: 340; loss: 1.04; acc: 0.8
Batch: 360; loss: 1.07; acc: 0.77
Batch: 380; loss: 1.22; acc: 0.72
Batch: 400; loss: 1.14; acc: 0.8
Batch: 420; loss: 0.98; acc: 0.8
Batch: 440; loss: 0.87; acc: 0.91
Batch: 460; loss: 1.02; acc: 0.83
Batch: 480; loss: 1.0; acc: 0.88
Batch: 500; loss: 1.07; acc: 0.78
Batch: 520; loss: 0.99; acc: 0.81
Batch: 540; loss: 1.04; acc: 0.81
Batch: 560; loss: 0.87; acc: 0.86
Batch: 580; loss: 0.89; acc: 0.84
Batch: 600; loss: 0.96; acc: 0.8
Batch: 620; loss: 0.96; acc: 0.81
Batch: 640; loss: 0.87; acc: 0.91
Batch: 660; loss: 0.81; acc: 0.91
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.81; acc: 0.88
Batch: 720; loss: 0.93; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.86
Batch: 760; loss: 0.83; acc: 0.88
Batch: 780; loss: 0.86; acc: 0.81
Train Epoch over. train_loss: 1.14; train_accuracy: 0.77 

2.6544250431470573e-05
9.022282029036433e-06
Batch: 0; loss: 0.91; acc: 0.83
Batch: 20; loss: 0.99; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.97
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.71; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.88
Batch: 120; loss: 0.99; acc: 0.7
Batch: 140; loss: 0.75; acc: 0.86
Val Epoch over. val_loss: 0.796516653838431; val_accuracy: 0.8592754777070064 

The current subspace-distance is: 9.022282029036433e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.84
Batch: 20; loss: 0.89; acc: 0.81
Batch: 40; loss: 0.9; acc: 0.84
Batch: 60; loss: 0.78; acc: 0.86
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.79; acc: 0.91
Batch: 160; loss: 0.79; acc: 0.88
Batch: 180; loss: 0.78; acc: 0.84
Batch: 200; loss: 0.8; acc: 0.84
Batch: 220; loss: 0.89; acc: 0.78
Batch: 240; loss: 0.72; acc: 0.88
Batch: 260; loss: 0.78; acc: 0.86
Batch: 280; loss: 0.72; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.91
Batch: 320; loss: 0.71; acc: 0.91
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.95
Batch: 380; loss: 0.76; acc: 0.89
Batch: 400; loss: 0.72; acc: 0.88
Batch: 420; loss: 0.71; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.91
Batch: 460; loss: 0.9; acc: 0.81
Batch: 480; loss: 0.79; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 0.87; acc: 0.8
Batch: 540; loss: 0.73; acc: 0.94
Batch: 560; loss: 0.67; acc: 0.89
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.76; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.91
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.63; acc: 0.89
Batch: 680; loss: 0.77; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.89
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.84; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.91
Train Epoch over. train_loss: 0.76; train_accuracy: 0.86 

3.277008363511413e-05
1.3008228052058257e-05
Batch: 0; loss: 0.75; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.56; acc: 0.91
Val Epoch over. val_loss: 0.6238520952167025; val_accuracy: 0.8864450636942676 

The current subspace-distance is: 1.3008228052058257e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.76; acc: 0.77
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.68; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.94
Batch: 200; loss: 0.62; acc: 0.92
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.73; acc: 0.8
Batch: 260; loss: 0.61; acc: 0.91
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.92
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.92
Batch: 480; loss: 0.65; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.94
Batch: 540; loss: 0.55; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.94
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.7; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.92
Batch: 660; loss: 0.56; acc: 0.91
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.94
Train Epoch over. train_loss: 0.62; train_accuracy: 0.88 

3.743648630916141e-05
1.6105646864161827e-05
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.5208645939447318; val_accuracy: 0.9053542993630573 

The current subspace-distance is: 1.6105646864161827e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.56; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.95
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.94
Batch: 400; loss: 0.7; acc: 0.8
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.97
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.95
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.67; acc: 0.78
Batch: 620; loss: 0.62; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.1215964301954955e-05
1.7446182027924806e-05
