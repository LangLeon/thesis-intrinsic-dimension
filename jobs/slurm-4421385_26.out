model : table13slim
N : 10
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:52

Channel scaling factor: 1.35

The number of parameters is: 266871

The number of individual parameters is:

11
198
11
11
17
33660
17
17
33
100980
33
33
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 13343548
elements in E: 13343550
fraction nonzero: 0.9999998501148495
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.4; acc: 0.12
Batch: 20; loss: 2.31; acc: 0.11
Batch: 40; loss: 2.25; acc: 0.23
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.26; acc: 0.17
Batch: 120; loss: 2.22; acc: 0.23
Batch: 140; loss: 2.14; acc: 0.25
Batch: 160; loss: 2.11; acc: 0.3
Batch: 180; loss: 2.13; acc: 0.27
Batch: 200; loss: 2.1; acc: 0.31
Batch: 220; loss: 2.25; acc: 0.27
Batch: 240; loss: 2.2; acc: 0.25
Batch: 260; loss: 2.15; acc: 0.17
Batch: 280; loss: 2.17; acc: 0.23
Batch: 300; loss: 2.13; acc: 0.27
Batch: 320; loss: 2.07; acc: 0.34
Batch: 340; loss: 2.22; acc: 0.16
Batch: 360; loss: 2.09; acc: 0.25
Batch: 380; loss: 2.02; acc: 0.34
Batch: 400; loss: 2.09; acc: 0.22
Batch: 420; loss: 2.19; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.36
Batch: 460; loss: 2.08; acc: 0.3
Batch: 480; loss: 2.0; acc: 0.38
Batch: 500; loss: 2.02; acc: 0.31
Batch: 520; loss: 1.89; acc: 0.45
Batch: 540; loss: 1.95; acc: 0.41
Batch: 560; loss: 1.99; acc: 0.36
Batch: 580; loss: 2.0; acc: 0.31
Batch: 600; loss: 2.03; acc: 0.36
Batch: 620; loss: 1.93; acc: 0.39
Batch: 640; loss: 1.86; acc: 0.52
Batch: 660; loss: 1.96; acc: 0.39
Batch: 680; loss: 2.03; acc: 0.31
Batch: 700; loss: 2.1; acc: 0.3
Batch: 720; loss: 2.02; acc: 0.28
Batch: 740; loss: 1.9; acc: 0.47
Batch: 760; loss: 1.98; acc: 0.3
Batch: 780; loss: 1.93; acc: 0.38
Train Epoch over. train_loss: 2.1; train_accuracy: 0.29 

2.392301757936366e-05
5.314132067724131e-06
Batch: 0; loss: 2.0; acc: 0.28
Batch: 20; loss: 1.88; acc: 0.39
Batch: 40; loss: 1.81; acc: 0.53
Batch: 60; loss: 1.87; acc: 0.45
Batch: 80; loss: 1.81; acc: 0.5
Batch: 100; loss: 1.86; acc: 0.48
Batch: 120; loss: 1.95; acc: 0.38
Batch: 140; loss: 1.85; acc: 0.52
Val Epoch over. val_loss: 1.9002795462395734; val_accuracy: 0.44158041401273884 

The current subspace-distance is: 5.314132067724131e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.84; acc: 0.44
Batch: 20; loss: 1.94; acc: 0.45
Batch: 40; loss: 1.88; acc: 0.42
Batch: 60; loss: 1.88; acc: 0.41
Batch: 80; loss: 1.89; acc: 0.39
Batch: 100; loss: 1.96; acc: 0.41
Batch: 120; loss: 1.88; acc: 0.41
Batch: 140; loss: 1.86; acc: 0.5
Batch: 160; loss: 2.0; acc: 0.33
Batch: 180; loss: 1.89; acc: 0.44
Batch: 200; loss: 1.88; acc: 0.45
Batch: 220; loss: 1.72; acc: 0.53
Batch: 240; loss: 1.87; acc: 0.38
Batch: 260; loss: 1.82; acc: 0.48
Batch: 280; loss: 1.95; acc: 0.42
Batch: 300; loss: 1.83; acc: 0.39
Batch: 320; loss: 1.75; acc: 0.52
Batch: 340; loss: 1.91; acc: 0.44
Batch: 360; loss: 1.67; acc: 0.58
Batch: 380; loss: 1.77; acc: 0.58
Batch: 400; loss: 1.87; acc: 0.44
Batch: 420; loss: 1.93; acc: 0.44
Batch: 440; loss: 1.97; acc: 0.3
Batch: 460; loss: 1.85; acc: 0.44
Batch: 480; loss: 1.87; acc: 0.41
Batch: 500; loss: 1.86; acc: 0.45
Batch: 520; loss: 1.89; acc: 0.48
Batch: 540; loss: 1.79; acc: 0.48
Batch: 560; loss: 1.83; acc: 0.45
Batch: 580; loss: 1.85; acc: 0.47
Batch: 600; loss: 1.91; acc: 0.36
Batch: 620; loss: 1.9; acc: 0.48
Batch: 640; loss: 1.88; acc: 0.42
Batch: 660; loss: 1.77; acc: 0.56
Batch: 680; loss: 1.87; acc: 0.44
Batch: 700; loss: 1.87; acc: 0.42
Batch: 720; loss: 1.87; acc: 0.45
Batch: 740; loss: 1.85; acc: 0.47
Batch: 760; loss: 1.86; acc: 0.41
Batch: 780; loss: 1.69; acc: 0.62
Train Epoch over. train_loss: 1.85; train_accuracy: 0.45 

2.7997184588457458e-05
8.735790288483258e-06
Batch: 0; loss: 1.89; acc: 0.38
Batch: 20; loss: 1.84; acc: 0.38
Batch: 40; loss: 1.66; acc: 0.56
Batch: 60; loss: 1.74; acc: 0.45
Batch: 80; loss: 1.72; acc: 0.55
Batch: 100; loss: 1.74; acc: 0.53
Batch: 120; loss: 1.84; acc: 0.5
Batch: 140; loss: 1.74; acc: 0.55
Val Epoch over. val_loss: 1.7806276751172012; val_accuracy: 0.5031847133757962 

The current subspace-distance is: 8.735790288483258e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.81; acc: 0.47
Batch: 20; loss: 1.84; acc: 0.42
Batch: 40; loss: 1.85; acc: 0.41
Batch: 60; loss: 1.85; acc: 0.44
Batch: 80; loss: 1.81; acc: 0.44
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 1.8; acc: 0.45
Batch: 140; loss: 1.81; acc: 0.34
Batch: 160; loss: 1.82; acc: 0.48
Batch: 180; loss: 1.86; acc: 0.42
Batch: 200; loss: 1.91; acc: 0.41
Batch: 220; loss: 1.82; acc: 0.5
Batch: 240; loss: 1.77; acc: 0.42
Batch: 260; loss: 1.84; acc: 0.44
Batch: 280; loss: 1.89; acc: 0.44
Batch: 300; loss: 1.9; acc: 0.44
Batch: 320; loss: 1.76; acc: 0.5
Batch: 340; loss: 1.78; acc: 0.48
Batch: 360; loss: 1.82; acc: 0.45
Batch: 380; loss: 1.83; acc: 0.47
Batch: 400; loss: 1.91; acc: 0.42
Batch: 420; loss: 1.77; acc: 0.5
Batch: 440; loss: 1.69; acc: 0.53
Batch: 460; loss: 1.72; acc: 0.52
Batch: 480; loss: 1.81; acc: 0.55
Batch: 500; loss: 1.77; acc: 0.53
Batch: 520; loss: 1.83; acc: 0.5
Batch: 540; loss: 1.71; acc: 0.52
Batch: 560; loss: 1.74; acc: 0.45
Batch: 580; loss: 1.8; acc: 0.48
Batch: 600; loss: 1.81; acc: 0.44
Batch: 620; loss: 1.71; acc: 0.52
Batch: 640; loss: 1.84; acc: 0.47
Batch: 660; loss: 1.68; acc: 0.55
Batch: 680; loss: 1.82; acc: 0.52
Batch: 700; loss: 1.95; acc: 0.38
Batch: 720; loss: 1.8; acc: 0.42
Batch: 740; loss: 1.88; acc: 0.41
Batch: 760; loss: 1.78; acc: 0.45
Batch: 780; loss: 1.75; acc: 0.5
Train Epoch over. train_loss: 1.79; train_accuracy: 0.48 

2.9645671020261943e-05
9.558038982504513e-06
Batch: 0; loss: 1.88; acc: 0.44
Batch: 20; loss: 1.84; acc: 0.39
Batch: 40; loss: 1.62; acc: 0.56
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.58
Batch: 100; loss: 1.71; acc: 0.48
Batch: 120; loss: 1.8; acc: 0.5
Batch: 140; loss: 1.71; acc: 0.47
Val Epoch over. val_loss: 1.7437344425043482; val_accuracy: 0.5173168789808917 

The current subspace-distance is: 9.558038982504513e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.52
Batch: 20; loss: 1.83; acc: 0.47
Batch: 40; loss: 1.73; acc: 0.52
Batch: 60; loss: 1.81; acc: 0.45
Batch: 80; loss: 1.76; acc: 0.53
Batch: 100; loss: 1.83; acc: 0.44
Batch: 120; loss: 1.7; acc: 0.55
Batch: 140; loss: 1.7; acc: 0.53
Batch: 160; loss: 1.76; acc: 0.47
Batch: 180; loss: 1.66; acc: 0.55
Batch: 200; loss: 1.75; acc: 0.55
Batch: 220; loss: 1.69; acc: 0.5
Batch: 240; loss: 1.77; acc: 0.52
Batch: 260; loss: 1.81; acc: 0.47
Batch: 280; loss: 1.65; acc: 0.56
Batch: 300; loss: 1.85; acc: 0.53
Batch: 320; loss: 1.74; acc: 0.52
Batch: 340; loss: 1.77; acc: 0.45
Batch: 360; loss: 1.74; acc: 0.53
Batch: 380; loss: 1.84; acc: 0.42
Batch: 400; loss: 1.68; acc: 0.5
Batch: 420; loss: 1.81; acc: 0.44
Batch: 440; loss: 1.85; acc: 0.44
Batch: 460; loss: 1.74; acc: 0.52
Batch: 480; loss: 1.74; acc: 0.55
Batch: 500; loss: 1.69; acc: 0.53
Batch: 520; loss: 1.64; acc: 0.58
Batch: 540; loss: 1.7; acc: 0.55
Batch: 560; loss: 1.8; acc: 0.44
Batch: 580; loss: 1.62; acc: 0.53
Batch: 600; loss: 1.77; acc: 0.44
Batch: 620; loss: 1.83; acc: 0.41
Batch: 640; loss: 1.72; acc: 0.38
Batch: 660; loss: 1.71; acc: 0.48
Batch: 680; loss: 1.66; acc: 0.55
Batch: 700; loss: 1.65; acc: 0.61
Batch: 720; loss: 1.64; acc: 0.58
Batch: 740; loss: 1.71; acc: 0.48
Batch: 760; loss: 1.76; acc: 0.48
Batch: 780; loss: 1.74; acc: 0.5
Train Epoch over. train_loss: 1.75; train_accuracy: 0.5 

3.289733285782859e-05
1.209088986797724e-05
Batch: 0; loss: 1.84; acc: 0.45
Batch: 20; loss: 1.79; acc: 0.41
Batch: 40; loss: 1.54; acc: 0.64
Batch: 60; loss: 1.62; acc: 0.55
Batch: 80; loss: 1.59; acc: 0.59
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.74; acc: 0.5
Batch: 140; loss: 1.68; acc: 0.53
Val Epoch over. val_loss: 1.6911739703196629; val_accuracy: 0.5267714968152867 

The current subspace-distance is: 1.209088986797724e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.47
Batch: 20; loss: 1.71; acc: 0.45
Batch: 40; loss: 1.7; acc: 0.55
Batch: 60; loss: 1.78; acc: 0.47
Batch: 80; loss: 1.71; acc: 0.44
Batch: 100; loss: 1.58; acc: 0.59
Batch: 120; loss: 1.71; acc: 0.56
Batch: 140; loss: 1.8; acc: 0.44
Batch: 160; loss: 1.78; acc: 0.45
Batch: 180; loss: 1.68; acc: 0.47
Batch: 200; loss: 1.79; acc: 0.41
Batch: 220; loss: 1.74; acc: 0.47
Batch: 240; loss: 1.7; acc: 0.5
Batch: 260; loss: 1.68; acc: 0.5
Batch: 280; loss: 1.72; acc: 0.52
Batch: 300; loss: 1.71; acc: 0.42
Batch: 320; loss: 1.66; acc: 0.55
Batch: 340; loss: 1.69; acc: 0.5
Batch: 360; loss: 1.8; acc: 0.48
Batch: 380; loss: 1.74; acc: 0.42
Batch: 400; loss: 1.81; acc: 0.52
Batch: 420; loss: 1.69; acc: 0.47
Batch: 440; loss: 1.65; acc: 0.58
Batch: 460; loss: 1.69; acc: 0.56
Batch: 480; loss: 1.64; acc: 0.53
Batch: 500; loss: 1.58; acc: 0.58
Batch: 520; loss: 1.68; acc: 0.5
Batch: 540; loss: 1.6; acc: 0.58
Batch: 560; loss: 1.6; acc: 0.61
Batch: 580; loss: 1.79; acc: 0.45
Batch: 600; loss: 1.67; acc: 0.56
Batch: 620; loss: 1.71; acc: 0.5
Batch: 640; loss: 1.75; acc: 0.45
Batch: 660; loss: 1.76; acc: 0.47
Batch: 680; loss: 1.62; acc: 0.53
Batch: 700; loss: 1.6; acc: 0.52
Batch: 720; loss: 1.53; acc: 0.64
Batch: 740; loss: 1.58; acc: 0.5
Batch: 760; loss: 1.71; acc: 0.5
Batch: 780; loss: 1.56; acc: 0.61
Train Epoch over. train_loss: 1.69; train_accuracy: 0.51 

3.494400152703747e-05
1.2144373613409698e-05
Batch: 0; loss: 1.8; acc: 0.48
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.47; acc: 0.67
Batch: 60; loss: 1.56; acc: 0.58
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.66; acc: 0.53
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.64; acc: 0.59
Val Epoch over. val_loss: 1.6375603379717298; val_accuracy: 0.5416003184713376 

The current subspace-distance is: 1.2144373613409698e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.58
Batch: 20; loss: 1.79; acc: 0.45
Batch: 40; loss: 1.89; acc: 0.48
Batch: 60; loss: 1.63; acc: 0.58
Batch: 80; loss: 1.67; acc: 0.53
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.63; acc: 0.55
Batch: 180; loss: 1.57; acc: 0.58
Batch: 200; loss: 1.58; acc: 0.59
Batch: 220; loss: 1.75; acc: 0.42
Batch: 240; loss: 1.69; acc: 0.56
Batch: 260; loss: 1.61; acc: 0.55
Batch: 280; loss: 1.59; acc: 0.48
Batch: 300; loss: 1.73; acc: 0.5
Batch: 320; loss: 1.66; acc: 0.52
Batch: 340; loss: 1.69; acc: 0.5
Batch: 360; loss: 1.68; acc: 0.47
Batch: 380; loss: 1.62; acc: 0.53
Batch: 400; loss: 1.57; acc: 0.58
Batch: 420; loss: 1.66; acc: 0.52
Batch: 440; loss: 1.58; acc: 0.56
Batch: 460; loss: 1.53; acc: 0.69
Batch: 480; loss: 1.55; acc: 0.61
Batch: 500; loss: 1.73; acc: 0.48
Batch: 520; loss: 1.64; acc: 0.52
Batch: 540; loss: 1.7; acc: 0.44
Batch: 560; loss: 1.82; acc: 0.45
Batch: 580; loss: 1.51; acc: 0.56
Batch: 600; loss: 1.62; acc: 0.53
Batch: 620; loss: 1.61; acc: 0.61
Batch: 640; loss: 1.49; acc: 0.62
Batch: 660; loss: 1.7; acc: 0.53
Batch: 680; loss: 1.6; acc: 0.55
Batch: 700; loss: 1.62; acc: 0.45
Batch: 720; loss: 1.61; acc: 0.55
Batch: 740; loss: 1.58; acc: 0.61
Batch: 760; loss: 1.71; acc: 0.45
Batch: 780; loss: 1.59; acc: 0.53
Train Epoch over. train_loss: 1.65; train_accuracy: 0.53 

3.591080894693732e-05
1.2763795893988572e-05
Batch: 0; loss: 1.77; acc: 0.55
Batch: 20; loss: 1.72; acc: 0.48
Batch: 40; loss: 1.42; acc: 0.69
Batch: 60; loss: 1.52; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.59
Batch: 100; loss: 1.63; acc: 0.53
Batch: 120; loss: 1.67; acc: 0.53
Batch: 140; loss: 1.59; acc: 0.55
Val Epoch over. val_loss: 1.6035560703581306; val_accuracy: 0.5485668789808917 

The current subspace-distance is: 1.2763795893988572e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.56; acc: 0.61
Batch: 40; loss: 1.64; acc: 0.55
Batch: 60; loss: 1.64; acc: 0.48
Batch: 80; loss: 1.63; acc: 0.61
Batch: 100; loss: 1.69; acc: 0.5
Batch: 120; loss: 1.65; acc: 0.58
Batch: 140; loss: 1.6; acc: 0.61
Batch: 160; loss: 1.66; acc: 0.58
Batch: 180; loss: 1.69; acc: 0.48
Batch: 200; loss: 1.55; acc: 0.59
Batch: 220; loss: 1.7; acc: 0.48
Batch: 240; loss: 1.63; acc: 0.5
Batch: 260; loss: 1.47; acc: 0.59
Batch: 280; loss: 1.63; acc: 0.55
Batch: 300; loss: 1.72; acc: 0.41
Batch: 320; loss: 1.5; acc: 0.61
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.7; acc: 0.48
Batch: 380; loss: 1.67; acc: 0.5
Batch: 400; loss: 1.57; acc: 0.48
Batch: 420; loss: 1.66; acc: 0.55
Batch: 440; loss: 1.7; acc: 0.53
Batch: 460; loss: 1.71; acc: 0.47
Batch: 480; loss: 1.63; acc: 0.53
Batch: 500; loss: 1.7; acc: 0.45
Batch: 520; loss: 1.56; acc: 0.56
Batch: 540; loss: 1.6; acc: 0.55
Batch: 560; loss: 1.52; acc: 0.58
Batch: 580; loss: 1.68; acc: 0.52
Batch: 600; loss: 1.52; acc: 0.5
Batch: 620; loss: 1.69; acc: 0.48
Batch: 640; loss: 1.61; acc: 0.56
Batch: 660; loss: 1.69; acc: 0.44
Batch: 680; loss: 1.53; acc: 0.61
Batch: 700; loss: 1.63; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.52
Batch: 740; loss: 1.49; acc: 0.62
Batch: 760; loss: 1.62; acc: 0.52
Batch: 780; loss: 1.57; acc: 0.55
Train Epoch over. train_loss: 1.62; train_accuracy: 0.53 

3.9783444663044065e-05
1.7237578504136764e-05
Batch: 0; loss: 1.76; acc: 0.5
Batch: 20; loss: 1.68; acc: 0.5
Batch: 40; loss: 1.39; acc: 0.67
Batch: 60; loss: 1.48; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.56
Batch: 100; loss: 1.62; acc: 0.53
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.57; acc: 0.53
Val Epoch over. val_loss: 1.5824713046383705; val_accuracy: 0.5452826433121019 

The current subspace-distance is: 1.7237578504136764e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.44
Batch: 20; loss: 1.72; acc: 0.48
Batch: 40; loss: 1.6; acc: 0.52
Batch: 60; loss: 1.57; acc: 0.55
Batch: 80; loss: 1.53; acc: 0.56
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.62; acc: 0.59
Batch: 140; loss: 1.65; acc: 0.55
Batch: 160; loss: 1.6; acc: 0.59
Batch: 180; loss: 1.76; acc: 0.47
Batch: 200; loss: 1.57; acc: 0.55
Batch: 220; loss: 1.64; acc: 0.44
Batch: 240; loss: 1.53; acc: 0.58
Batch: 260; loss: 1.6; acc: 0.53
Batch: 280; loss: 1.43; acc: 0.61
Batch: 300; loss: 1.53; acc: 0.56
Batch: 320; loss: 1.67; acc: 0.45
Batch: 340; loss: 1.72; acc: 0.44
Batch: 360; loss: 1.63; acc: 0.53
Batch: 380; loss: 1.57; acc: 0.55
Batch: 400; loss: 1.49; acc: 0.59
Batch: 420; loss: 1.6; acc: 0.5
Batch: 440; loss: 1.64; acc: 0.53
Batch: 460; loss: 1.72; acc: 0.45
Batch: 480; loss: 1.73; acc: 0.42
Batch: 500; loss: 1.62; acc: 0.44
Batch: 520; loss: 1.68; acc: 0.47
Batch: 540; loss: 1.49; acc: 0.58
Batch: 560; loss: 1.55; acc: 0.59
Batch: 580; loss: 1.4; acc: 0.62
Batch: 600; loss: 1.56; acc: 0.48
Batch: 620; loss: 1.81; acc: 0.41
Batch: 640; loss: 1.57; acc: 0.59
Batch: 660; loss: 1.66; acc: 0.53
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.56; acc: 0.56
Batch: 720; loss: 1.79; acc: 0.48
Batch: 740; loss: 1.65; acc: 0.5
Batch: 760; loss: 1.68; acc: 0.5
Batch: 780; loss: 1.68; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.53 

4.130577144678682e-05
1.5861631254665554e-05
Batch: 0; loss: 1.73; acc: 0.45
Batch: 20; loss: 1.63; acc: 0.55
Batch: 40; loss: 1.35; acc: 0.69
Batch: 60; loss: 1.43; acc: 0.56
Batch: 80; loss: 1.48; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.64; acc: 0.5
Batch: 140; loss: 1.54; acc: 0.52
Val Epoch over. val_loss: 1.5580764097772586; val_accuracy: 0.5477707006369427 

The current subspace-distance is: 1.5861631254665554e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.54; acc: 0.62
Batch: 40; loss: 1.59; acc: 0.52
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.59; acc: 0.5
Batch: 100; loss: 1.64; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.59
Batch: 160; loss: 1.53; acc: 0.5
Batch: 180; loss: 1.56; acc: 0.47
Batch: 200; loss: 1.53; acc: 0.5
Batch: 220; loss: 1.51; acc: 0.59
Batch: 240; loss: 1.48; acc: 0.62
Batch: 260; loss: 1.61; acc: 0.44
Batch: 280; loss: 1.59; acc: 0.48
Batch: 300; loss: 1.59; acc: 0.52
Batch: 320; loss: 1.48; acc: 0.56
Batch: 340; loss: 1.59; acc: 0.55
Batch: 360; loss: 1.67; acc: 0.53
Batch: 380; loss: 1.67; acc: 0.52
Batch: 400; loss: 1.72; acc: 0.55
Batch: 420; loss: 1.69; acc: 0.52
Batch: 440; loss: 1.7; acc: 0.47
Batch: 460; loss: 1.66; acc: 0.48
Batch: 480; loss: 1.59; acc: 0.53
Batch: 500; loss: 1.67; acc: 0.45
Batch: 520; loss: 1.56; acc: 0.5
Batch: 540; loss: 1.51; acc: 0.53
Batch: 560; loss: 1.56; acc: 0.55
Batch: 580; loss: 1.77; acc: 0.36
Batch: 600; loss: 1.67; acc: 0.52
Batch: 620; loss: 1.47; acc: 0.61
Batch: 640; loss: 1.42; acc: 0.66
Batch: 660; loss: 1.69; acc: 0.48
Batch: 680; loss: 1.59; acc: 0.56
Batch: 700; loss: 1.53; acc: 0.56
Batch: 720; loss: 1.72; acc: 0.44
Batch: 740; loss: 1.6; acc: 0.47
Batch: 760; loss: 1.58; acc: 0.55
Batch: 780; loss: 1.54; acc: 0.45
Train Epoch over. train_loss: 1.59; train_accuracy: 0.53 

4.3622734665405005e-05
1.9650638932944275e-05
Batch: 0; loss: 1.73; acc: 0.48
Batch: 20; loss: 1.59; acc: 0.53
Batch: 40; loss: 1.33; acc: 0.69
Batch: 60; loss: 1.4; acc: 0.58
Batch: 80; loss: 1.47; acc: 0.59
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.63; acc: 0.53
Batch: 140; loss: 1.51; acc: 0.47
Val Epoch over. val_loss: 1.5423437448064232; val_accuracy: 0.5518511146496815 

The current subspace-distance is: 1.9650638932944275e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.41
Batch: 20; loss: 1.61; acc: 0.53
Batch: 40; loss: 1.54; acc: 0.61
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.48
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.54; acc: 0.56
Batch: 160; loss: 1.68; acc: 0.47
Batch: 180; loss: 1.59; acc: 0.52
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.67; acc: 0.5
Batch: 240; loss: 1.59; acc: 0.62
Batch: 260; loss: 1.6; acc: 0.5
Batch: 280; loss: 1.61; acc: 0.53
Batch: 300; loss: 1.68; acc: 0.44
Batch: 320; loss: 1.74; acc: 0.47
Batch: 340; loss: 1.57; acc: 0.5
Batch: 360; loss: 1.65; acc: 0.45
Batch: 380; loss: 1.49; acc: 0.59
Batch: 400; loss: 1.51; acc: 0.56
Batch: 420; loss: 1.51; acc: 0.61
Batch: 440; loss: 1.47; acc: 0.58
Batch: 460; loss: 1.57; acc: 0.52
Batch: 480; loss: 1.6; acc: 0.53
Batch: 500; loss: 1.7; acc: 0.41
Batch: 520; loss: 1.48; acc: 0.53
Batch: 540; loss: 1.66; acc: 0.48
Batch: 560; loss: 1.57; acc: 0.48
Batch: 580; loss: 1.58; acc: 0.56
Batch: 600; loss: 1.66; acc: 0.48
Batch: 620; loss: 1.56; acc: 0.5
Batch: 640; loss: 1.43; acc: 0.56
Batch: 660; loss: 1.52; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.61
Batch: 700; loss: 1.65; acc: 0.53
Batch: 720; loss: 1.57; acc: 0.52
Batch: 740; loss: 1.55; acc: 0.56
Batch: 760; loss: 1.44; acc: 0.61
Batch: 780; loss: 1.39; acc: 0.61
Train Epoch over. train_loss: 1.58; train_accuracy: 0.53 

4.533792525762692e-05
2.289637632202357e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.57; acc: 0.48
Batch: 40; loss: 1.31; acc: 0.7
Batch: 60; loss: 1.38; acc: 0.61
Batch: 80; loss: 1.47; acc: 0.55
Batch: 100; loss: 1.6; acc: 0.5
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.47; acc: 0.47
Val Epoch over. val_loss: 1.5264060542841626; val_accuracy: 0.5504578025477707 

The current subspace-distance is: 2.289637632202357e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.55
Batch: 20; loss: 1.63; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.61
Batch: 60; loss: 1.35; acc: 0.66
Batch: 80; loss: 1.44; acc: 0.53
Batch: 100; loss: 1.54; acc: 0.52
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.64; acc: 0.42
Batch: 160; loss: 1.58; acc: 0.52
Batch: 180; loss: 1.49; acc: 0.64
Batch: 200; loss: 1.61; acc: 0.52
Batch: 220; loss: 1.47; acc: 0.55
Batch: 240; loss: 1.42; acc: 0.59
Batch: 260; loss: 1.56; acc: 0.5
Batch: 280; loss: 1.55; acc: 0.55
Batch: 300; loss: 1.48; acc: 0.56
Batch: 320; loss: 1.55; acc: 0.52
Batch: 340; loss: 1.48; acc: 0.48
Batch: 360; loss: 1.5; acc: 0.52
Batch: 380; loss: 1.64; acc: 0.44
Batch: 400; loss: 1.49; acc: 0.61
Batch: 420; loss: 1.57; acc: 0.47
Batch: 440; loss: 1.46; acc: 0.69
Batch: 460; loss: 1.47; acc: 0.56
Batch: 480; loss: 1.48; acc: 0.52
Batch: 500; loss: 1.55; acc: 0.58
Batch: 520; loss: 1.59; acc: 0.52
Batch: 540; loss: 1.51; acc: 0.58
Batch: 560; loss: 1.63; acc: 0.44
Batch: 580; loss: 1.4; acc: 0.61
Batch: 600; loss: 1.58; acc: 0.55
Batch: 620; loss: 1.53; acc: 0.55
Batch: 640; loss: 1.82; acc: 0.45
Batch: 660; loss: 1.51; acc: 0.52
Batch: 680; loss: 1.64; acc: 0.47
Batch: 700; loss: 1.54; acc: 0.58
Batch: 720; loss: 1.46; acc: 0.59
Batch: 740; loss: 1.65; acc: 0.5
Batch: 760; loss: 1.56; acc: 0.52
Batch: 780; loss: 1.68; acc: 0.45
Train Epoch over. train_loss: 1.56; train_accuracy: 0.53 

4.6112541895126924e-05
2.002824294322636e-05
Batch: 0; loss: 1.7; acc: 0.48
Batch: 20; loss: 1.56; acc: 0.48
Batch: 40; loss: 1.3; acc: 0.72
Batch: 60; loss: 1.37; acc: 0.62
Batch: 80; loss: 1.46; acc: 0.55
Batch: 100; loss: 1.6; acc: 0.53
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.45; acc: 0.47
Val Epoch over. val_loss: 1.5203391176879786; val_accuracy: 0.5492635350318471 

The current subspace-distance is: 2.002824294322636e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.53
Batch: 20; loss: 1.56; acc: 0.55
Batch: 40; loss: 1.43; acc: 0.64
Batch: 60; loss: 1.55; acc: 0.56
Batch: 80; loss: 1.5; acc: 0.55
Batch: 100; loss: 1.64; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.44
Batch: 140; loss: 1.45; acc: 0.58
Batch: 160; loss: 1.6; acc: 0.52
Batch: 180; loss: 1.58; acc: 0.56
Batch: 200; loss: 1.55; acc: 0.5
Batch: 220; loss: 1.48; acc: 0.61
Batch: 240; loss: 1.53; acc: 0.56
Batch: 260; loss: 1.75; acc: 0.44
Batch: 280; loss: 1.62; acc: 0.58
Batch: 300; loss: 1.52; acc: 0.5
Batch: 320; loss: 1.43; acc: 0.61
Batch: 340; loss: 1.71; acc: 0.42
Batch: 360; loss: 1.67; acc: 0.5
Batch: 380; loss: 1.56; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.45
Batch: 420; loss: 1.32; acc: 0.69
Batch: 440; loss: 1.53; acc: 0.56
Batch: 460; loss: 1.67; acc: 0.5
Batch: 480; loss: 1.54; acc: 0.55
Batch: 500; loss: 1.45; acc: 0.62
Batch: 520; loss: 1.44; acc: 0.64
Batch: 540; loss: 1.5; acc: 0.55
Batch: 560; loss: 1.5; acc: 0.58
Batch: 580; loss: 1.32; acc: 0.66
Batch: 600; loss: 1.59; acc: 0.45
Batch: 620; loss: 1.5; acc: 0.56
Batch: 640; loss: 1.6; acc: 0.56
Batch: 660; loss: 1.47; acc: 0.53
Batch: 680; loss: 1.77; acc: 0.48
Batch: 700; loss: 1.5; acc: 0.5
Batch: 720; loss: 1.5; acc: 0.55
Batch: 740; loss: 1.48; acc: 0.5
Batch: 760; loss: 1.5; acc: 0.59
Batch: 780; loss: 1.54; acc: 0.53
Train Epoch over. train_loss: 1.56; train_accuracy: 0.53 

4.841717236558907e-05
2.2499907572637312e-05
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.55; acc: 0.47
Batch: 40; loss: 1.29; acc: 0.7
Batch: 60; loss: 1.37; acc: 0.64
Batch: 80; loss: 1.46; acc: 0.55
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.44; acc: 0.45
Val Epoch over. val_loss: 1.5144312510824507; val_accuracy: 0.5510549363057324 

The current subspace-distance is: 2.2499907572637312e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.45
Batch: 20; loss: 1.52; acc: 0.53
Batch: 40; loss: 1.71; acc: 0.5
Batch: 60; loss: 1.51; acc: 0.48
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.51; acc: 0.56
Batch: 120; loss: 1.44; acc: 0.59
Batch: 140; loss: 1.64; acc: 0.42
Batch: 160; loss: 1.48; acc: 0.53
Batch: 180; loss: 1.48; acc: 0.64
Batch: 200; loss: 1.55; acc: 0.59
Batch: 220; loss: 1.68; acc: 0.47
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.49; acc: 0.56
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.44; acc: 0.48
Batch: 320; loss: 1.39; acc: 0.59
Batch: 340; loss: 1.52; acc: 0.55
Batch: 360; loss: 1.56; acc: 0.55
Batch: 380; loss: 1.51; acc: 0.53
Batch: 400; loss: 1.56; acc: 0.47
Batch: 420; loss: 1.41; acc: 0.66
Batch: 440; loss: 1.77; acc: 0.38
Batch: 460; loss: 1.54; acc: 0.56
Batch: 480; loss: 1.52; acc: 0.62
Batch: 500; loss: 1.52; acc: 0.53
Batch: 520; loss: 1.48; acc: 0.58
Batch: 540; loss: 1.52; acc: 0.59
Batch: 560; loss: 1.49; acc: 0.58
Batch: 580; loss: 1.58; acc: 0.53
Batch: 600; loss: 1.55; acc: 0.52
Batch: 620; loss: 1.52; acc: 0.5
Batch: 640; loss: 1.64; acc: 0.48
Batch: 660; loss: 1.55; acc: 0.58
Batch: 680; loss: 1.59; acc: 0.45
Batch: 700; loss: 1.63; acc: 0.42
Batch: 720; loss: 1.59; acc: 0.55
Batch: 740; loss: 1.43; acc: 0.55
Batch: 760; loss: 1.51; acc: 0.58
Batch: 780; loss: 1.57; acc: 0.47
Train Epoch over. train_loss: 1.55; train_accuracy: 0.53 

4.799081580131315e-05
2.2894146241014823e-05
Batch: 0; loss: 1.7; acc: 0.5
Batch: 20; loss: 1.53; acc: 0.5
Batch: 40; loss: 1.3; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.62
Batch: 80; loss: 1.47; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.53
Batch: 140; loss: 1.43; acc: 0.52
Val Epoch over. val_loss: 1.5171350962037493; val_accuracy: 0.5544386942675159 

The current subspace-distance is: 2.2894146241014823e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.56
Batch: 20; loss: 1.58; acc: 0.58
Batch: 40; loss: 1.6; acc: 0.44
Batch: 60; loss: 1.58; acc: 0.52
Batch: 80; loss: 1.58; acc: 0.56
Batch: 100; loss: 1.52; acc: 0.58
Batch: 120; loss: 1.46; acc: 0.58
Batch: 140; loss: 1.7; acc: 0.48
Batch: 160; loss: 1.6; acc: 0.48
Batch: 180; loss: 1.49; acc: 0.53
Batch: 200; loss: 1.58; acc: 0.58
Batch: 220; loss: 1.54; acc: 0.48
Batch: 240; loss: 1.61; acc: 0.47
Batch: 260; loss: 1.56; acc: 0.48
Batch: 280; loss: 1.62; acc: 0.45
Batch: 300; loss: 1.58; acc: 0.44
Batch: 320; loss: 1.5; acc: 0.56
Batch: 340; loss: 1.52; acc: 0.58
Batch: 360; loss: 1.53; acc: 0.48
Batch: 380; loss: 1.55; acc: 0.53
Batch: 400; loss: 1.49; acc: 0.61
Batch: 420; loss: 1.49; acc: 0.55
Batch: 440; loss: 1.54; acc: 0.55
Batch: 460; loss: 1.56; acc: 0.52
Batch: 480; loss: 1.75; acc: 0.48
Batch: 500; loss: 1.53; acc: 0.56
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.73; acc: 0.45
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.64; acc: 0.48
Batch: 620; loss: 1.46; acc: 0.58
Batch: 640; loss: 1.76; acc: 0.45
Batch: 660; loss: 1.74; acc: 0.45
Batch: 680; loss: 1.57; acc: 0.58
Batch: 700; loss: 1.5; acc: 0.61
Batch: 720; loss: 1.57; acc: 0.5
Batch: 740; loss: 1.51; acc: 0.56
Batch: 760; loss: 1.63; acc: 0.53
Batch: 780; loss: 1.51; acc: 0.59
Train Epoch over. train_loss: 1.55; train_accuracy: 0.53 

4.738589996122755e-05
2.2491178242489696e-05
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.54; acc: 0.52
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.35; acc: 0.61
Batch: 80; loss: 1.47; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.53
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.41; acc: 0.53
Val Epoch over. val_loss: 1.5030504358801873; val_accuracy: 0.5554339171974523 

The current subspace-distance is: 2.2491178242489696e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.52
Batch: 40; loss: 1.67; acc: 0.44
Batch: 60; loss: 1.21; acc: 0.7
Batch: 80; loss: 1.48; acc: 0.64
Batch: 100; loss: 1.56; acc: 0.56
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.46; acc: 0.55
Batch: 160; loss: 1.6; acc: 0.47
Batch: 180; loss: 1.56; acc: 0.55
Batch: 200; loss: 1.49; acc: 0.55
Batch: 220; loss: 1.62; acc: 0.47
Batch: 240; loss: 1.39; acc: 0.7
Batch: 260; loss: 1.47; acc: 0.59
Batch: 280; loss: 1.52; acc: 0.55
Batch: 300; loss: 1.47; acc: 0.56
Batch: 320; loss: 1.68; acc: 0.48
Batch: 340; loss: 1.44; acc: 0.56
Batch: 360; loss: 1.57; acc: 0.52
Batch: 380; loss: 1.69; acc: 0.44
Batch: 400; loss: 1.43; acc: 0.61
Batch: 420; loss: 1.5; acc: 0.55
Batch: 440; loss: 1.64; acc: 0.5
Batch: 460; loss: 1.58; acc: 0.5
Batch: 480; loss: 1.61; acc: 0.55
Batch: 500; loss: 1.72; acc: 0.38
Batch: 520; loss: 1.43; acc: 0.62
Batch: 540; loss: 1.51; acc: 0.52
Batch: 560; loss: 1.56; acc: 0.48
Batch: 580; loss: 1.52; acc: 0.59
Batch: 600; loss: 1.46; acc: 0.55
Batch: 620; loss: 1.42; acc: 0.5
Batch: 640; loss: 1.47; acc: 0.56
Batch: 660; loss: 1.55; acc: 0.48
Batch: 680; loss: 1.49; acc: 0.53
Batch: 700; loss: 1.61; acc: 0.44
Batch: 720; loss: 1.55; acc: 0.48
Batch: 740; loss: 1.55; acc: 0.47
Batch: 760; loss: 1.58; acc: 0.52
Batch: 780; loss: 1.38; acc: 0.62
Train Epoch over. train_loss: 1.54; train_accuracy: 0.53 

4.853124846704304e-05
2.3384662199532613e-05
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.52; acc: 0.52
Batch: 40; loss: 1.26; acc: 0.69
Batch: 60; loss: 1.34; acc: 0.64
Batch: 80; loss: 1.45; acc: 0.55
Batch: 100; loss: 1.56; acc: 0.53
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.4; acc: 0.52
Val Epoch over. val_loss: 1.492724431548149; val_accuracy: 0.5599124203821656 

The current subspace-distance is: 2.3384662199532613e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.43; acc: 0.62
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 1.51; acc: 0.55
Batch: 60; loss: 1.51; acc: 0.55
Batch: 80; loss: 1.53; acc: 0.5
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.56; acc: 0.48
Batch: 140; loss: 1.6; acc: 0.55
Batch: 160; loss: 1.48; acc: 0.58
Batch: 180; loss: 1.65; acc: 0.48
Batch: 200; loss: 1.61; acc: 0.56
Batch: 220; loss: 1.55; acc: 0.5
Batch: 240; loss: 1.42; acc: 0.59
Batch: 260; loss: 1.5; acc: 0.64
Batch: 280; loss: 1.84; acc: 0.38
Batch: 300; loss: 1.48; acc: 0.61
Batch: 320; loss: 1.71; acc: 0.47
Batch: 340; loss: 1.41; acc: 0.69
Batch: 360; loss: 1.34; acc: 0.62
Batch: 380; loss: 1.49; acc: 0.52
Batch: 400; loss: 1.5; acc: 0.59
Batch: 420; loss: 1.5; acc: 0.52
Batch: 440; loss: 1.49; acc: 0.59
Batch: 460; loss: 1.61; acc: 0.53
Batch: 480; loss: 1.46; acc: 0.53
Batch: 500; loss: 1.6; acc: 0.5
Batch: 520; loss: 1.59; acc: 0.48
Batch: 540; loss: 1.46; acc: 0.58
Batch: 560; loss: 1.58; acc: 0.52
Batch: 580; loss: 1.6; acc: 0.45
Batch: 600; loss: 1.45; acc: 0.53
Batch: 620; loss: 1.45; acc: 0.55
Batch: 640; loss: 1.38; acc: 0.62
Batch: 660; loss: 1.52; acc: 0.55
Batch: 680; loss: 1.55; acc: 0.48
Batch: 700; loss: 1.53; acc: 0.52
Batch: 720; loss: 1.6; acc: 0.5
Batch: 740; loss: 1.37; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.56
Batch: 780; loss: 1.57; acc: 0.58
Train Epoch over. train_loss: 1.54; train_accuracy: 0.53 

4.668971450882964e-05
2.021302680077497e-05
Batch: 0; loss: 1.66; acc: 0.45
Batch: 20; loss: 1.53; acc: 0.5
Batch: 40; loss: 1.26; acc: 0.69
Batch: 60; loss: 1.35; acc: 0.64
Batch: 80; loss: 1.45; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.55
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.39; acc: 0.53
Val Epoch over. val_loss: 1.4965771816338702; val_accuracy: 0.557921974522293 

The current subspace-distance is: 2.021302680077497e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.48
Batch: 20; loss: 1.54; acc: 0.48
Batch: 40; loss: 1.52; acc: 0.5
Batch: 60; loss: 1.58; acc: 0.47
Batch: 80; loss: 1.6; acc: 0.52
Batch: 100; loss: 1.6; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.45
Batch: 140; loss: 1.52; acc: 0.55
Batch: 160; loss: 1.63; acc: 0.44
Batch: 180; loss: 1.45; acc: 0.53
Batch: 200; loss: 1.56; acc: 0.47
Batch: 220; loss: 1.69; acc: 0.45
Batch: 240; loss: 1.58; acc: 0.55
Batch: 260; loss: 1.52; acc: 0.62
Batch: 280; loss: 1.65; acc: 0.47
Batch: 300; loss: 1.45; acc: 0.61
Batch: 320; loss: 1.36; acc: 0.62
Batch: 340; loss: 1.53; acc: 0.61
Batch: 360; loss: 1.61; acc: 0.53
Batch: 380; loss: 1.52; acc: 0.56
Batch: 400; loss: 1.69; acc: 0.5
Batch: 420; loss: 1.37; acc: 0.59
Batch: 440; loss: 1.63; acc: 0.47
Batch: 460; loss: 1.48; acc: 0.47
Batch: 480; loss: 1.61; acc: 0.5
Batch: 500; loss: 1.49; acc: 0.56
Batch: 520; loss: 1.61; acc: 0.44
Batch: 540; loss: 1.51; acc: 0.48
Batch: 560; loss: 1.47; acc: 0.52
Batch: 580; loss: 1.36; acc: 0.66
Batch: 600; loss: 1.42; acc: 0.55
Batch: 620; loss: 1.49; acc: 0.53
Batch: 640; loss: 1.41; acc: 0.61
Batch: 660; loss: 1.59; acc: 0.44
Batch: 680; loss: 1.49; acc: 0.66
Batch: 700; loss: 1.51; acc: 0.47
Batch: 720; loss: 1.38; acc: 0.59
Batch: 740; loss: 1.57; acc: 0.53
Batch: 760; loss: 1.63; acc: 0.45
Batch: 780; loss: 1.51; acc: 0.53
Train Epoch over. train_loss: 1.53; train_accuracy: 0.53 

4.81195020256564e-05
2.0323883290984668e-05
Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.24; acc: 0.67
Batch: 60; loss: 1.33; acc: 0.62
Batch: 80; loss: 1.43; acc: 0.56
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.36; acc: 0.53
Val Epoch over. val_loss: 1.4783324314530488; val_accuracy: 0.5584195859872612 

The current subspace-distance is: 2.0323883290984668e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.53
Batch: 20; loss: 1.72; acc: 0.45
Batch: 40; loss: 1.46; acc: 0.52
Batch: 60; loss: 1.49; acc: 0.53
Batch: 80; loss: 1.5; acc: 0.52
Batch: 100; loss: 1.54; acc: 0.53
Batch: 120; loss: 1.58; acc: 0.47
Batch: 140; loss: 1.44; acc: 0.56
Batch: 160; loss: 1.63; acc: 0.52
Batch: 180; loss: 1.54; acc: 0.55
Batch: 200; loss: 1.52; acc: 0.58
Batch: 220; loss: 1.59; acc: 0.53
Batch: 240; loss: 1.54; acc: 0.52
Batch: 260; loss: 1.65; acc: 0.48
Batch: 280; loss: 1.62; acc: 0.48
Batch: 300; loss: 1.58; acc: 0.47
Batch: 320; loss: 1.53; acc: 0.47
Batch: 340; loss: 1.42; acc: 0.61
Batch: 360; loss: 1.65; acc: 0.48
Batch: 380; loss: 1.51; acc: 0.56
Batch: 400; loss: 1.86; acc: 0.42
Batch: 420; loss: 1.62; acc: 0.52
Batch: 440; loss: 1.47; acc: 0.53
Batch: 460; loss: 1.54; acc: 0.5
Batch: 480; loss: 1.45; acc: 0.58
Batch: 500; loss: 1.52; acc: 0.55
Batch: 520; loss: 1.4; acc: 0.58
Batch: 540; loss: 1.43; acc: 0.5
Batch: 560; loss: 1.8; acc: 0.39
Batch: 580; loss: 1.34; acc: 0.64
Batch: 600; loss: 1.58; acc: 0.5
Batch: 620; loss: 1.45; acc: 0.52
Batch: 640; loss: 1.63; acc: 0.5
Batch: 660; loss: 1.59; acc: 0.5
Batch: 680; loss: 1.49; acc: 0.5
Batch: 700; loss: 1.54; acc: 0.55
Batch: 720; loss: 1.55; acc: 0.53
Batch: 740; loss: 1.68; acc: 0.52
Batch: 760; loss: 1.54; acc: 0.53
Batch: 780; loss: 1.37; acc: 0.58
Train Epoch over. train_loss: 1.53; train_accuracy: 0.53 

4.9040911108022556e-05
1.9536657418939285e-05
Batch: 0; loss: 1.64; acc: 0.44
Batch: 20; loss: 1.51; acc: 0.5
Batch: 40; loss: 1.25; acc: 0.69
Batch: 60; loss: 1.34; acc: 0.62
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.37; acc: 0.53
Val Epoch over. val_loss: 1.4841899089752488; val_accuracy: 0.5600119426751592 

The current subspace-distance is: 1.9536657418939285e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.32; acc: 0.64
Batch: 20; loss: 1.54; acc: 0.52
Batch: 40; loss: 1.54; acc: 0.52
Batch: 60; loss: 1.6; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.55
Batch: 100; loss: 1.52; acc: 0.53
Batch: 120; loss: 1.49; acc: 0.53
Batch: 140; loss: 1.51; acc: 0.53
Batch: 160; loss: 1.68; acc: 0.45
Batch: 180; loss: 1.46; acc: 0.59
Batch: 200; loss: 1.69; acc: 0.48
Batch: 220; loss: 1.59; acc: 0.47
Batch: 240; loss: 1.48; acc: 0.56
Batch: 260; loss: 1.47; acc: 0.52
Batch: 280; loss: 1.56; acc: 0.47
Batch: 300; loss: 1.46; acc: 0.53
Batch: 320; loss: 1.4; acc: 0.58
Batch: 340; loss: 1.49; acc: 0.53
Batch: 360; loss: 1.61; acc: 0.53
Batch: 380; loss: 1.73; acc: 0.44
Batch: 400; loss: 1.52; acc: 0.53
Batch: 420; loss: 1.55; acc: 0.48
Batch: 440; loss: 1.45; acc: 0.55
Batch: 460; loss: 1.51; acc: 0.55
Batch: 480; loss: 1.6; acc: 0.52
Batch: 500; loss: 1.38; acc: 0.61
Batch: 520; loss: 1.53; acc: 0.58
Batch: 540; loss: 1.54; acc: 0.52
Batch: 560; loss: 1.44; acc: 0.56
Batch: 580; loss: 1.49; acc: 0.56
Batch: 600; loss: 1.55; acc: 0.45
Batch: 620; loss: 1.71; acc: 0.52
Batch: 640; loss: 1.56; acc: 0.52
Batch: 660; loss: 1.67; acc: 0.5
Batch: 680; loss: 1.58; acc: 0.5
Batch: 700; loss: 1.62; acc: 0.44
Batch: 720; loss: 1.66; acc: 0.52
Batch: 740; loss: 1.65; acc: 0.44
Batch: 760; loss: 1.47; acc: 0.56
Batch: 780; loss: 1.59; acc: 0.47
Train Epoch over. train_loss: 1.53; train_accuracy: 0.53 

4.989441731595434e-05
2.6957430236507207e-05
Batch: 0; loss: 1.64; acc: 0.44
Batch: 20; loss: 1.51; acc: 0.5
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.34; acc: 0.66
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.56; acc: 0.55
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.35; acc: 0.55
Val Epoch over. val_loss: 1.4799113273620605; val_accuracy: 0.5572253184713376 

The current subspace-distance is: 2.6957430236507207e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.48
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 1.83; acc: 0.39
Batch: 60; loss: 1.42; acc: 0.61
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.61; acc: 0.5
Batch: 120; loss: 1.65; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.5
Batch: 160; loss: 1.67; acc: 0.42
Batch: 180; loss: 1.48; acc: 0.55
Batch: 200; loss: 1.47; acc: 0.55
Batch: 220; loss: 1.75; acc: 0.39
Batch: 240; loss: 1.49; acc: 0.59
Batch: 260; loss: 1.24; acc: 0.69
Batch: 280; loss: 1.54; acc: 0.53
Batch: 300; loss: 1.52; acc: 0.58
Batch: 320; loss: 1.52; acc: 0.53
Batch: 340; loss: 1.53; acc: 0.56
Batch: 360; loss: 1.66; acc: 0.56
Batch: 380; loss: 1.49; acc: 0.56
Batch: 400; loss: 1.52; acc: 0.48
Batch: 420; loss: 1.58; acc: 0.48
Batch: 440; loss: 1.56; acc: 0.52
Batch: 460; loss: 1.65; acc: 0.41
Batch: 480; loss: 1.43; acc: 0.55
Batch: 500; loss: 1.45; acc: 0.61
Batch: 520; loss: 1.38; acc: 0.59
Batch: 540; loss: 1.61; acc: 0.45
Batch: 560; loss: 1.5; acc: 0.52
Batch: 580; loss: 1.54; acc: 0.55
Batch: 600; loss: 1.68; acc: 0.47
Batch: 620; loss: 1.65; acc: 0.44
Batch: 640; loss: 1.61; acc: 0.47
Batch: 660; loss: 1.62; acc: 0.48
Batch: 680; loss: 1.62; acc: 0.52
Batch: 700; loss: 1.47; acc: 0.59
Batch: 720; loss: 1.56; acc: 0.52
Batch: 740; loss: 1.62; acc: 0.5
Batch: 760; loss: 1.48; acc: 0.59
Batch: 780; loss: 1.69; acc: 0.45
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

4.947750858264044e-05
2.12316863326123e-05
Batch: 0; loss: 1.63; acc: 0.47
Batch: 20; loss: 1.52; acc: 0.5
Batch: 40; loss: 1.23; acc: 0.66
Batch: 60; loss: 1.32; acc: 0.67
Batch: 80; loss: 1.42; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.5
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.35; acc: 0.55
Val Epoch over. val_loss: 1.4735620970938617; val_accuracy: 0.5612062101910829 

The current subspace-distance is: 2.12316863326123e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.41; acc: 0.58
Batch: 20; loss: 1.46; acc: 0.62
Batch: 40; loss: 1.57; acc: 0.58
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.49; acc: 0.59
Batch: 100; loss: 1.4; acc: 0.58
Batch: 120; loss: 1.5; acc: 0.53
Batch: 140; loss: 1.42; acc: 0.61
Batch: 160; loss: 1.49; acc: 0.56
Batch: 180; loss: 1.54; acc: 0.53
Batch: 200; loss: 1.51; acc: 0.55
Batch: 220; loss: 1.44; acc: 0.53
Batch: 240; loss: 1.52; acc: 0.59
Batch: 260; loss: 1.49; acc: 0.55
Batch: 280; loss: 1.67; acc: 0.41
Batch: 300; loss: 1.61; acc: 0.52
Batch: 320; loss: 1.58; acc: 0.52
Batch: 340; loss: 1.64; acc: 0.48
Batch: 360; loss: 1.52; acc: 0.59
Batch: 380; loss: 1.53; acc: 0.56
Batch: 400; loss: 1.41; acc: 0.66
Batch: 420; loss: 1.34; acc: 0.61
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.42; acc: 0.58
Batch: 480; loss: 1.5; acc: 0.47
Batch: 500; loss: 1.54; acc: 0.5
Batch: 520; loss: 1.48; acc: 0.5
Batch: 540; loss: 1.35; acc: 0.59
Batch: 560; loss: 1.63; acc: 0.5
Batch: 580; loss: 1.75; acc: 0.44
Batch: 600; loss: 1.56; acc: 0.52
Batch: 620; loss: 1.4; acc: 0.64
Batch: 640; loss: 1.48; acc: 0.56
Batch: 660; loss: 1.67; acc: 0.41
Batch: 680; loss: 1.5; acc: 0.5
Batch: 700; loss: 1.62; acc: 0.41
Batch: 720; loss: 1.41; acc: 0.61
Batch: 740; loss: 1.49; acc: 0.52
Batch: 760; loss: 1.46; acc: 0.62
Batch: 780; loss: 1.53; acc: 0.53
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

5.024862912250683e-05
2.225736534455791e-05
Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.51; acc: 0.52
Batch: 40; loss: 1.23; acc: 0.67
Batch: 60; loss: 1.31; acc: 0.67
Batch: 80; loss: 1.42; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.52
Batch: 140; loss: 1.33; acc: 0.56
Val Epoch over. val_loss: 1.4642311820558682; val_accuracy: 0.5623009554140127 

The current subspace-distance is: 2.225736534455791e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.58
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.42; acc: 0.58
Batch: 60; loss: 1.75; acc: 0.45
Batch: 80; loss: 1.6; acc: 0.53
Batch: 100; loss: 1.46; acc: 0.59
Batch: 120; loss: 1.48; acc: 0.55
Batch: 140; loss: 1.52; acc: 0.58
Batch: 160; loss: 1.58; acc: 0.5
Batch: 180; loss: 1.29; acc: 0.67
Batch: 200; loss: 1.62; acc: 0.5
Batch: 220; loss: 1.63; acc: 0.44
Batch: 240; loss: 1.37; acc: 0.53
Batch: 260; loss: 1.49; acc: 0.58
Batch: 280; loss: 1.43; acc: 0.58
Batch: 300; loss: 1.36; acc: 0.69
Batch: 320; loss: 1.62; acc: 0.48
Batch: 340; loss: 1.53; acc: 0.53
Batch: 360; loss: 1.59; acc: 0.5
Batch: 380; loss: 1.64; acc: 0.48
Batch: 400; loss: 1.74; acc: 0.47
Batch: 420; loss: 1.44; acc: 0.52
Batch: 440; loss: 1.57; acc: 0.53
Batch: 460; loss: 1.56; acc: 0.56
Batch: 480; loss: 1.59; acc: 0.52
Batch: 500; loss: 1.57; acc: 0.48
Batch: 520; loss: 1.43; acc: 0.52
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.55; acc: 0.47
Batch: 580; loss: 1.48; acc: 0.59
Batch: 600; loss: 1.49; acc: 0.5
Batch: 620; loss: 1.53; acc: 0.58
Batch: 640; loss: 1.74; acc: 0.42
Batch: 660; loss: 1.57; acc: 0.48
Batch: 680; loss: 1.39; acc: 0.5
Batch: 700; loss: 1.61; acc: 0.47
Batch: 720; loss: 1.62; acc: 0.44
Batch: 740; loss: 1.42; acc: 0.61
Batch: 760; loss: 1.48; acc: 0.61
Batch: 780; loss: 1.5; acc: 0.52
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

4.908932896796614e-05
1.9298377083032392e-05
Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.51; acc: 0.52
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.32; acc: 0.67
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.33; acc: 0.59
Val Epoch over. val_loss: 1.4648704111196433; val_accuracy: 0.5617038216560509 

The current subspace-distance is: 1.9298377083032392e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.48
Batch: 20; loss: 1.38; acc: 0.58
Batch: 40; loss: 1.54; acc: 0.53
Batch: 60; loss: 1.65; acc: 0.48
Batch: 80; loss: 1.52; acc: 0.47
Batch: 100; loss: 1.49; acc: 0.58
Batch: 120; loss: 1.56; acc: 0.48
Batch: 140; loss: 1.5; acc: 0.53
Batch: 160; loss: 1.72; acc: 0.44
Batch: 180; loss: 1.52; acc: 0.48
Batch: 200; loss: 1.61; acc: 0.44
Batch: 220; loss: 1.52; acc: 0.48
Batch: 240; loss: 1.47; acc: 0.53
Batch: 260; loss: 1.49; acc: 0.55
Batch: 280; loss: 1.44; acc: 0.55
Batch: 300; loss: 1.61; acc: 0.45
Batch: 320; loss: 1.43; acc: 0.61
Batch: 340; loss: 1.58; acc: 0.53
Batch: 360; loss: 1.6; acc: 0.48
Batch: 380; loss: 1.62; acc: 0.47
Batch: 400; loss: 1.48; acc: 0.59
Batch: 420; loss: 1.38; acc: 0.55
Batch: 440; loss: 1.49; acc: 0.58
Batch: 460; loss: 1.6; acc: 0.48
Batch: 480; loss: 1.49; acc: 0.53
Batch: 500; loss: 1.72; acc: 0.44
Batch: 520; loss: 1.55; acc: 0.53
Batch: 540; loss: 1.57; acc: 0.48
Batch: 560; loss: 1.65; acc: 0.47
Batch: 580; loss: 1.39; acc: 0.56
Batch: 600; loss: 1.46; acc: 0.55
Batch: 620; loss: 1.63; acc: 0.41
Batch: 640; loss: 1.46; acc: 0.55
Batch: 660; loss: 1.61; acc: 0.53
Batch: 680; loss: 1.38; acc: 0.59
Batch: 700; loss: 1.35; acc: 0.61
Batch: 720; loss: 1.59; acc: 0.53
Batch: 740; loss: 1.42; acc: 0.58
Batch: 760; loss: 1.6; acc: 0.48
Batch: 780; loss: 1.45; acc: 0.56
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

5.025742939324118e-05
2.490499900886789e-05
Batch: 0; loss: 1.64; acc: 0.47
Batch: 20; loss: 1.51; acc: 0.5
Batch: 40; loss: 1.23; acc: 0.67
Batch: 60; loss: 1.34; acc: 0.64
Batch: 80; loss: 1.42; acc: 0.55
Batch: 100; loss: 1.56; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.34; acc: 0.55
Val Epoch over. val_loss: 1.4728751182556152; val_accuracy: 0.5601114649681529 

The current subspace-distance is: 2.490499900886789e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.55
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.61; acc: 0.45
Batch: 60; loss: 1.43; acc: 0.58
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 1.37; acc: 0.59
Batch: 120; loss: 1.56; acc: 0.56
Batch: 140; loss: 1.53; acc: 0.52
Batch: 160; loss: 1.33; acc: 0.62
Batch: 180; loss: 1.62; acc: 0.42
Batch: 200; loss: 1.69; acc: 0.5
Batch: 220; loss: 1.48; acc: 0.52
Batch: 240; loss: 1.57; acc: 0.5
Batch: 260; loss: 1.37; acc: 0.58
Batch: 280; loss: 1.46; acc: 0.59
Batch: 300; loss: 1.56; acc: 0.47
Batch: 320; loss: 1.55; acc: 0.56
Batch: 340; loss: 1.48; acc: 0.53
Batch: 360; loss: 1.5; acc: 0.58
Batch: 380; loss: 1.56; acc: 0.5
Batch: 400; loss: 1.32; acc: 0.64
Batch: 420; loss: 1.64; acc: 0.48
Batch: 440; loss: 1.52; acc: 0.59
Batch: 460; loss: 1.66; acc: 0.44
Batch: 480; loss: 1.57; acc: 0.47
Batch: 500; loss: 1.49; acc: 0.56
Batch: 520; loss: 1.54; acc: 0.53
Batch: 540; loss: 1.43; acc: 0.61
Batch: 560; loss: 1.46; acc: 0.55
Batch: 580; loss: 1.51; acc: 0.52
Batch: 600; loss: 1.51; acc: 0.58
Batch: 620; loss: 1.44; acc: 0.52
Batch: 640; loss: 1.54; acc: 0.48
Batch: 660; loss: 1.38; acc: 0.64
Batch: 680; loss: 1.44; acc: 0.52
Batch: 700; loss: 1.31; acc: 0.62
Batch: 720; loss: 1.57; acc: 0.56
Batch: 740; loss: 1.53; acc: 0.52
Batch: 760; loss: 1.37; acc: 0.62
Batch: 780; loss: 1.61; acc: 0.5
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

5.0215770897921175e-05
2.45565297518624e-05
Batch: 0; loss: 1.63; acc: 0.42
Batch: 20; loss: 1.49; acc: 0.5
Batch: 40; loss: 1.22; acc: 0.69
Batch: 60; loss: 1.3; acc: 0.69
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.53; acc: 0.53
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.32; acc: 0.56
Val Epoch over. val_loss: 1.4619070162439043; val_accuracy: 0.5627985668789809 

The current subspace-distance is: 2.45565297518624e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.67; acc: 0.53
Batch: 40; loss: 1.34; acc: 0.61
Batch: 60; loss: 1.67; acc: 0.5
Batch: 80; loss: 1.5; acc: 0.53
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 1.48; acc: 0.5
Batch: 160; loss: 1.52; acc: 0.48
Batch: 180; loss: 1.51; acc: 0.58
Batch: 200; loss: 1.4; acc: 0.61
Batch: 220; loss: 1.68; acc: 0.5
Batch: 240; loss: 1.58; acc: 0.47
Batch: 260; loss: 1.65; acc: 0.48
Batch: 280; loss: 1.49; acc: 0.56
Batch: 300; loss: 1.5; acc: 0.47
Batch: 320; loss: 1.41; acc: 0.56
Batch: 340; loss: 1.52; acc: 0.48
Batch: 360; loss: 1.38; acc: 0.64
Batch: 380; loss: 1.52; acc: 0.48
Batch: 400; loss: 1.65; acc: 0.48
Batch: 420; loss: 1.47; acc: 0.56
Batch: 440; loss: 1.59; acc: 0.48
Batch: 460; loss: 1.45; acc: 0.56
Batch: 480; loss: 1.37; acc: 0.62
Batch: 500; loss: 1.47; acc: 0.58
Batch: 520; loss: 1.45; acc: 0.62
Batch: 540; loss: 1.44; acc: 0.62
Batch: 560; loss: 1.45; acc: 0.53
Batch: 580; loss: 1.41; acc: 0.55
Batch: 600; loss: 1.59; acc: 0.55
Batch: 620; loss: 1.5; acc: 0.5
Batch: 640; loss: 1.58; acc: 0.5
Batch: 660; loss: 1.35; acc: 0.61
Batch: 680; loss: 1.61; acc: 0.48
Batch: 700; loss: 1.45; acc: 0.61
Batch: 720; loss: 1.54; acc: 0.56
Batch: 740; loss: 1.52; acc: 0.55
Batch: 760; loss: 1.6; acc: 0.56
Batch: 780; loss: 1.51; acc: 0.5
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

4.978228753316216e-05
1.7695891074254178e-05
Batch: 0; loss: 1.61; acc: 0.45
Batch: 20; loss: 1.49; acc: 0.53
Batch: 40; loss: 1.21; acc: 0.64
Batch: 60; loss: 1.3; acc: 0.7
Batch: 80; loss: 1.4; acc: 0.59
Batch: 100; loss: 1.53; acc: 0.58
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.31; acc: 0.58
Val Epoch over. val_loss: 1.4547031867276332; val_accuracy: 0.5683718152866242 

The current subspace-distance is: 1.7695891074254178e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.36; acc: 0.58
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.59; acc: 0.47
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 1.49; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.58
Batch: 120; loss: 1.56; acc: 0.52
Batch: 140; loss: 1.62; acc: 0.53
Batch: 160; loss: 1.43; acc: 0.62
Batch: 180; loss: 1.52; acc: 0.53
Batch: 200; loss: 1.63; acc: 0.47
Batch: 220; loss: 1.57; acc: 0.55
Batch: 240; loss: 1.66; acc: 0.44
Batch: 260; loss: 1.64; acc: 0.48
Batch: 280; loss: 1.46; acc: 0.56
Batch: 300; loss: 1.63; acc: 0.5
Batch: 320; loss: 1.67; acc: 0.44
Batch: 340; loss: 1.61; acc: 0.47
Batch: 360; loss: 1.47; acc: 0.66
Batch: 380; loss: 1.41; acc: 0.53
Batch: 400; loss: 1.44; acc: 0.67
Batch: 420; loss: 1.42; acc: 0.52
Batch: 440; loss: 1.48; acc: 0.52
Batch: 460; loss: 1.58; acc: 0.53
Batch: 480; loss: 1.45; acc: 0.56
Batch: 500; loss: 1.76; acc: 0.42
Batch: 520; loss: 1.38; acc: 0.64
Batch: 540; loss: 1.67; acc: 0.52
Batch: 560; loss: 1.4; acc: 0.59
Batch: 580; loss: 1.44; acc: 0.56
Batch: 600; loss: 1.52; acc: 0.52
Batch: 620; loss: 1.56; acc: 0.48
Batch: 640; loss: 1.65; acc: 0.45
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.64; acc: 0.48
Batch: 700; loss: 1.63; acc: 0.52
Batch: 720; loss: 1.37; acc: 0.59
Batch: 740; loss: 1.52; acc: 0.48
Batch: 760; loss: 1.5; acc: 0.53
Batch: 780; loss: 1.28; acc: 0.61
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

5.158039493835531e-05
2.4518920326954685e-05
Batch: 0; loss: 1.64; acc: 0.47
Batch: 20; loss: 1.51; acc: 0.5
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.31; acc: 0.67
Batch: 80; loss: 1.41; acc: 0.59
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.33; acc: 0.56
Val Epoch over. val_loss: 1.4650929194347115; val_accuracy: 0.558718152866242 

The current subspace-distance is: 2.4518920326954685e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.53; acc: 0.61
Batch: 40; loss: 1.53; acc: 0.55
Batch: 60; loss: 1.42; acc: 0.55
Batch: 80; loss: 1.33; acc: 0.62
Batch: 100; loss: 1.64; acc: 0.41
Batch: 120; loss: 1.52; acc: 0.52
Batch: 140; loss: 1.59; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.5
Batch: 180; loss: 1.48; acc: 0.59
Batch: 200; loss: 1.57; acc: 0.55
Batch: 220; loss: 1.53; acc: 0.52
Batch: 240; loss: 1.31; acc: 0.64
Batch: 260; loss: 1.64; acc: 0.47
Batch: 280; loss: 1.56; acc: 0.48
Batch: 300; loss: 1.69; acc: 0.5
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.52
Batch: 360; loss: 1.58; acc: 0.5
Batch: 380; loss: 1.43; acc: 0.61
Batch: 400; loss: 1.53; acc: 0.55
Batch: 420; loss: 1.55; acc: 0.53
Batch: 440; loss: 1.54; acc: 0.61
Batch: 460; loss: 1.57; acc: 0.58
Batch: 480; loss: 1.57; acc: 0.55
Batch: 500; loss: 1.55; acc: 0.53
Batch: 520; loss: 1.46; acc: 0.53
Batch: 540; loss: 1.59; acc: 0.47
Batch: 560; loss: 1.47; acc: 0.53
Batch: 580; loss: 1.4; acc: 0.67
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.47; acc: 0.56
Batch: 640; loss: 1.56; acc: 0.55
Batch: 660; loss: 1.42; acc: 0.53
Batch: 680; loss: 1.41; acc: 0.62
Batch: 700; loss: 1.54; acc: 0.55
Batch: 720; loss: 1.39; acc: 0.59
Batch: 740; loss: 1.45; acc: 0.61
Batch: 760; loss: 1.53; acc: 0.48
Batch: 780; loss: 1.4; acc: 0.58
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

4.948740024701692e-05
1.9754999811993912e-05
Batch: 0; loss: 1.63; acc: 0.47
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.23; acc: 0.7
Batch: 60; loss: 1.34; acc: 0.64
Batch: 80; loss: 1.44; acc: 0.61
Batch: 100; loss: 1.56; acc: 0.53
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.34; acc: 0.56
Val Epoch over. val_loss: 1.473126576964263; val_accuracy: 0.5620023885350318 

The current subspace-distance is: 1.9754999811993912e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.47; acc: 0.53
Batch: 40; loss: 1.43; acc: 0.53
Batch: 60; loss: 1.53; acc: 0.56
Batch: 80; loss: 1.42; acc: 0.61
Batch: 100; loss: 1.52; acc: 0.48
Batch: 120; loss: 1.49; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.53
Batch: 160; loss: 1.41; acc: 0.55
Batch: 180; loss: 1.64; acc: 0.52
Batch: 200; loss: 1.42; acc: 0.58
Batch: 220; loss: 1.33; acc: 0.61
Batch: 240; loss: 1.5; acc: 0.55
Batch: 260; loss: 1.38; acc: 0.58
Batch: 280; loss: 1.41; acc: 0.56
Batch: 300; loss: 1.49; acc: 0.58
Batch: 320; loss: 1.49; acc: 0.53
Batch: 340; loss: 1.47; acc: 0.58
Batch: 360; loss: 1.54; acc: 0.52
Batch: 380; loss: 1.69; acc: 0.45
Batch: 400; loss: 1.39; acc: 0.61
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.59; acc: 0.52
Batch: 460; loss: 1.58; acc: 0.47
Batch: 480; loss: 1.45; acc: 0.53
Batch: 500; loss: 1.66; acc: 0.47
Batch: 520; loss: 1.54; acc: 0.56
Batch: 540; loss: 1.58; acc: 0.5
Batch: 560; loss: 1.61; acc: 0.45
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.61; acc: 0.53
Batch: 620; loss: 1.57; acc: 0.48
Batch: 640; loss: 1.67; acc: 0.45
Batch: 660; loss: 1.6; acc: 0.47
Batch: 680; loss: 1.65; acc: 0.42
Batch: 700; loss: 1.44; acc: 0.64
Batch: 720; loss: 1.42; acc: 0.59
Batch: 740; loss: 1.62; acc: 0.5
Batch: 760; loss: 1.44; acc: 0.53
Batch: 780; loss: 1.66; acc: 0.44
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

5.119753768667579e-05
1.8965087292599492e-05
Batch: 0; loss: 1.62; acc: 0.44
Batch: 20; loss: 1.49; acc: 0.55
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.32; acc: 0.59
Val Epoch over. val_loss: 1.463524383344468; val_accuracy: 0.5621019108280255 

The current subspace-distance is: 1.8965087292599492e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.4; acc: 0.66
Batch: 20; loss: 1.61; acc: 0.45
Batch: 40; loss: 1.59; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.61
Batch: 80; loss: 1.47; acc: 0.55
Batch: 100; loss: 1.62; acc: 0.44
Batch: 120; loss: 1.47; acc: 0.53
Batch: 140; loss: 1.49; acc: 0.5
Batch: 160; loss: 1.4; acc: 0.56
Batch: 180; loss: 1.7; acc: 0.44
Batch: 200; loss: 1.65; acc: 0.45
Batch: 220; loss: 1.55; acc: 0.52
Batch: 240; loss: 1.41; acc: 0.72
Batch: 260; loss: 1.66; acc: 0.47
Batch: 280; loss: 1.56; acc: 0.52
Batch: 300; loss: 1.48; acc: 0.52
Batch: 320; loss: 1.42; acc: 0.48
Batch: 340; loss: 1.46; acc: 0.52
Batch: 360; loss: 1.5; acc: 0.52
Batch: 380; loss: 1.45; acc: 0.64
Batch: 400; loss: 1.55; acc: 0.47
Batch: 420; loss: 1.58; acc: 0.58
Batch: 440; loss: 1.54; acc: 0.56
Batch: 460; loss: 1.48; acc: 0.55
Batch: 480; loss: 1.71; acc: 0.44
Batch: 500; loss: 1.47; acc: 0.52
Batch: 520; loss: 1.47; acc: 0.52
Batch: 540; loss: 1.53; acc: 0.59
Batch: 560; loss: 1.53; acc: 0.47
Batch: 580; loss: 1.56; acc: 0.52
Batch: 600; loss: 1.39; acc: 0.59
Batch: 620; loss: 1.56; acc: 0.44
Batch: 640; loss: 1.33; acc: 0.66
Batch: 660; loss: 1.52; acc: 0.53
Batch: 680; loss: 1.65; acc: 0.5
Batch: 700; loss: 1.48; acc: 0.53
Batch: 720; loss: 1.48; acc: 0.55
Batch: 740; loss: 1.49; acc: 0.52
Batch: 760; loss: 1.49; acc: 0.48
Batch: 780; loss: 1.72; acc: 0.42
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

5.213010445004329e-05
2.3361759303952567e-05
Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.51; acc: 0.5
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.33; acc: 0.66
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.5
Batch: 140; loss: 1.32; acc: 0.55
Val Epoch over. val_loss: 1.4688397782623388; val_accuracy: 0.5575238853503185 

The current subspace-distance is: 2.3361759303952567e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.55
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.52; acc: 0.53
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.46; acc: 0.56
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 1.56; acc: 0.48
Batch: 160; loss: 1.35; acc: 0.56
Batch: 180; loss: 1.59; acc: 0.55
Batch: 200; loss: 1.57; acc: 0.5
Batch: 220; loss: 1.34; acc: 0.59
Batch: 240; loss: 1.4; acc: 0.62
Batch: 260; loss: 1.55; acc: 0.52
Batch: 280; loss: 1.6; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.53
Batch: 320; loss: 1.45; acc: 0.58
Batch: 340; loss: 1.53; acc: 0.59
Batch: 360; loss: 1.56; acc: 0.44
Batch: 380; loss: 1.36; acc: 0.59
Batch: 400; loss: 1.65; acc: 0.45
Batch: 420; loss: 1.38; acc: 0.55
Batch: 440; loss: 1.82; acc: 0.38
Batch: 460; loss: 1.57; acc: 0.64
Batch: 480; loss: 1.58; acc: 0.47
Batch: 500; loss: 1.35; acc: 0.58
Batch: 520; loss: 1.59; acc: 0.58
Batch: 540; loss: 1.52; acc: 0.55
Batch: 560; loss: 1.63; acc: 0.47
Batch: 580; loss: 1.5; acc: 0.62
Batch: 600; loss: 1.51; acc: 0.55
Batch: 620; loss: 1.39; acc: 0.61
Batch: 640; loss: 1.45; acc: 0.55
Batch: 660; loss: 1.56; acc: 0.5
Batch: 680; loss: 1.41; acc: 0.53
Batch: 700; loss: 1.74; acc: 0.39
Batch: 720; loss: 1.46; acc: 0.61
Batch: 740; loss: 1.51; acc: 0.52
Batch: 760; loss: 1.36; acc: 0.69
Batch: 780; loss: 1.51; acc: 0.55
Train Epoch over. train_loss: 1.52; train_accuracy: 0.53 

5.305236845742911e-05
2.6909876396530308e-05
Batch: 0; loss: 1.64; acc: 0.47
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.35; acc: 0.66
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.57; acc: 0.5
Batch: 120; loss: 1.63; acc: 0.5
Batch: 140; loss: 1.33; acc: 0.59
Val Epoch over. val_loss: 1.4746885724887726; val_accuracy: 0.5637937898089171 

The current subspace-distance is: 2.6909876396530308e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_10_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.35

The number of parameters is: 266871

The number of individual parameters is:

11
198
11
11
17
33660
17
17
33
100980
33
33
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 26687097
elements in E: 26687100
fraction nonzero: 0.9999998875861371
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.45; acc: 0.09
Batch: 20; loss: 2.16; acc: 0.31
Batch: 40; loss: 2.18; acc: 0.28
Batch: 60; loss: 2.19; acc: 0.22
Batch: 80; loss: 2.19; acc: 0.23
Batch: 100; loss: 2.09; acc: 0.3
Batch: 120; loss: 1.99; acc: 0.3
Batch: 140; loss: 1.97; acc: 0.36
Batch: 160; loss: 1.91; acc: 0.42
Batch: 180; loss: 1.92; acc: 0.39
Batch: 200; loss: 1.97; acc: 0.3
Batch: 220; loss: 1.86; acc: 0.41
Batch: 240; loss: 1.86; acc: 0.44
Batch: 260; loss: 1.85; acc: 0.42
Batch: 280; loss: 1.83; acc: 0.42
Batch: 300; loss: 1.78; acc: 0.53
Batch: 320; loss: 1.84; acc: 0.38
Batch: 340; loss: 1.78; acc: 0.45
Batch: 360; loss: 1.83; acc: 0.41
Batch: 380; loss: 1.75; acc: 0.47
Batch: 400; loss: 1.83; acc: 0.39
Batch: 420; loss: 1.8; acc: 0.53
Batch: 440; loss: 1.76; acc: 0.48
Batch: 460; loss: 1.83; acc: 0.41
Batch: 480; loss: 1.7; acc: 0.53
Batch: 500; loss: 1.79; acc: 0.52
Batch: 520; loss: 1.71; acc: 0.48
Batch: 540; loss: 1.7; acc: 0.53
Batch: 560; loss: 1.69; acc: 0.56
Batch: 580; loss: 1.56; acc: 0.67
Batch: 600; loss: 1.78; acc: 0.55
Batch: 620; loss: 1.77; acc: 0.44
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.71; acc: 0.52
Batch: 680; loss: 1.69; acc: 0.5
Batch: 700; loss: 1.67; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.45
Batch: 740; loss: 1.59; acc: 0.61
Batch: 760; loss: 1.69; acc: 0.58
Batch: 780; loss: 1.52; acc: 0.67
Train Epoch over. train_loss: 1.83; train_accuracy: 0.45 

5.276332740322687e-05
4.680876372731291e-05
Batch: 0; loss: 1.66; acc: 0.55
Batch: 20; loss: 1.67; acc: 0.58
Batch: 40; loss: 1.34; acc: 0.73
Batch: 60; loss: 1.6; acc: 0.56
Batch: 80; loss: 1.5; acc: 0.66
Batch: 100; loss: 1.63; acc: 0.56
Batch: 120; loss: 1.71; acc: 0.56
Batch: 140; loss: 1.53; acc: 0.66
Val Epoch over. val_loss: 1.6114258804139059; val_accuracy: 0.5710589171974523 

The current subspace-distance is: 4.680876372731291e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.58; acc: 0.62
Batch: 40; loss: 1.58; acc: 0.59
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.62
Batch: 100; loss: 1.47; acc: 0.72
Batch: 120; loss: 1.71; acc: 0.53
Batch: 140; loss: 1.48; acc: 0.67
Batch: 160; loss: 1.54; acc: 0.59
Batch: 180; loss: 1.68; acc: 0.55
Batch: 200; loss: 1.74; acc: 0.42
Batch: 220; loss: 1.56; acc: 0.62
Batch: 240; loss: 1.67; acc: 0.5
Batch: 260; loss: 1.69; acc: 0.53
Batch: 280; loss: 1.54; acc: 0.56
Batch: 300; loss: 1.56; acc: 0.64
Batch: 320; loss: 1.6; acc: 0.61
Batch: 340; loss: 1.53; acc: 0.56
Batch: 360; loss: 1.5; acc: 0.67
Batch: 380; loss: 1.57; acc: 0.62
Batch: 400; loss: 1.53; acc: 0.59
Batch: 420; loss: 1.73; acc: 0.48
Batch: 440; loss: 1.51; acc: 0.61
Batch: 460; loss: 1.47; acc: 0.62
Batch: 480; loss: 1.47; acc: 0.67
Batch: 500; loss: 1.46; acc: 0.7
Batch: 520; loss: 1.66; acc: 0.48
Batch: 540; loss: 1.55; acc: 0.56
Batch: 560; loss: 1.54; acc: 0.66
Batch: 580; loss: 1.59; acc: 0.48
Batch: 600; loss: 1.59; acc: 0.55
Batch: 620; loss: 1.52; acc: 0.66
Batch: 640; loss: 1.56; acc: 0.62
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.61; acc: 0.48
Batch: 700; loss: 1.59; acc: 0.62
Batch: 720; loss: 1.55; acc: 0.55
Batch: 740; loss: 1.51; acc: 0.64
Batch: 760; loss: 1.56; acc: 0.55
Batch: 780; loss: 1.5; acc: 0.67
Train Epoch over. train_loss: 1.58; train_accuracy: 0.58 

6.626228423556313e-05
6.0797538026236e-05
Batch: 0; loss: 1.54; acc: 0.52
Batch: 20; loss: 1.59; acc: 0.53
Batch: 40; loss: 1.3; acc: 0.7
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.4; acc: 0.66
Batch: 100; loss: 1.48; acc: 0.67
Batch: 120; loss: 1.69; acc: 0.59
Batch: 140; loss: 1.42; acc: 0.69
Val Epoch over. val_loss: 1.5136946052502676; val_accuracy: 0.6159434713375797 

The current subspace-distance is: 6.0797538026236e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.58; acc: 0.58
Batch: 40; loss: 1.6; acc: 0.56
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.56; acc: 0.58
Batch: 100; loss: 1.46; acc: 0.7
Batch: 120; loss: 1.7; acc: 0.5
Batch: 140; loss: 1.42; acc: 0.73
Batch: 160; loss: 1.54; acc: 0.61
Batch: 180; loss: 1.61; acc: 0.58
Batch: 200; loss: 1.63; acc: 0.53
Batch: 220; loss: 1.59; acc: 0.58
Batch: 240; loss: 1.58; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.53
Batch: 280; loss: 1.49; acc: 0.67
Batch: 300; loss: 1.59; acc: 0.62
Batch: 320; loss: 1.53; acc: 0.53
Batch: 340; loss: 1.57; acc: 0.52
Batch: 360; loss: 1.54; acc: 0.59
Batch: 380; loss: 1.48; acc: 0.59
Batch: 400; loss: 1.57; acc: 0.61
Batch: 420; loss: 1.42; acc: 0.72
Batch: 440; loss: 1.49; acc: 0.59
Batch: 460; loss: 1.46; acc: 0.67
Batch: 480; loss: 1.51; acc: 0.66
Batch: 500; loss: 1.45; acc: 0.69
Batch: 520; loss: 1.5; acc: 0.61
Batch: 540; loss: 1.48; acc: 0.69
Batch: 560; loss: 1.48; acc: 0.62
Batch: 580; loss: 1.48; acc: 0.61
Batch: 600; loss: 1.42; acc: 0.67
Batch: 620; loss: 1.53; acc: 0.59
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.48; acc: 0.66
Batch: 680; loss: 1.61; acc: 0.53
Batch: 700; loss: 1.54; acc: 0.56
Batch: 720; loss: 1.5; acc: 0.59
Batch: 740; loss: 1.48; acc: 0.61
Batch: 760; loss: 1.49; acc: 0.64
Batch: 780; loss: 1.47; acc: 0.69
Train Epoch over. train_loss: 1.52; train_accuracy: 0.61 

7.842114428058267e-05
7.296416879398748e-05
Batch: 0; loss: 1.48; acc: 0.56
Batch: 20; loss: 1.55; acc: 0.58
Batch: 40; loss: 1.26; acc: 0.72
Batch: 60; loss: 1.49; acc: 0.59
Batch: 80; loss: 1.35; acc: 0.66
Batch: 100; loss: 1.4; acc: 0.7
Batch: 120; loss: 1.65; acc: 0.64
Batch: 140; loss: 1.36; acc: 0.64
Val Epoch over. val_loss: 1.457486937759788; val_accuracy: 0.6415207006369427 

The current subspace-distance is: 7.296416879398748e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.64
Batch: 20; loss: 1.39; acc: 0.75
Batch: 40; loss: 1.55; acc: 0.56
Batch: 60; loss: 1.45; acc: 0.66
Batch: 80; loss: 1.46; acc: 0.62
Batch: 100; loss: 1.51; acc: 0.61
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.42; acc: 0.62
Batch: 160; loss: 1.38; acc: 0.72
Batch: 180; loss: 1.42; acc: 0.69
Batch: 200; loss: 1.48; acc: 0.58
Batch: 220; loss: 1.44; acc: 0.59
Batch: 240; loss: 1.38; acc: 0.66
Batch: 260; loss: 1.49; acc: 0.58
Batch: 280; loss: 1.39; acc: 0.69
Batch: 300; loss: 1.38; acc: 0.7
Batch: 320; loss: 1.49; acc: 0.56
Batch: 340; loss: 1.39; acc: 0.67
Batch: 360; loss: 1.43; acc: 0.66
Batch: 380; loss: 1.47; acc: 0.61
Batch: 400; loss: 1.36; acc: 0.72
Batch: 420; loss: 1.46; acc: 0.67
Batch: 440; loss: 1.43; acc: 0.67
Batch: 460; loss: 1.41; acc: 0.66
Batch: 480; loss: 1.43; acc: 0.64
Batch: 500; loss: 1.43; acc: 0.64
Batch: 520; loss: 1.39; acc: 0.64
Batch: 540; loss: 1.47; acc: 0.58
Batch: 560; loss: 1.71; acc: 0.47
Batch: 580; loss: 1.52; acc: 0.55
Batch: 600; loss: 1.49; acc: 0.69
Batch: 620; loss: 1.47; acc: 0.62
Batch: 640; loss: 1.47; acc: 0.64
Batch: 660; loss: 1.35; acc: 0.78
Batch: 680; loss: 1.4; acc: 0.62
Batch: 700; loss: 1.34; acc: 0.77
Batch: 720; loss: 1.36; acc: 0.7
Batch: 740; loss: 1.51; acc: 0.61
Batch: 760; loss: 1.34; acc: 0.77
Batch: 780; loss: 1.46; acc: 0.56
Train Epoch over. train_loss: 1.46; train_accuracy: 0.63 

8.983399311546236e-05
8.498569513903931e-05
Batch: 0; loss: 1.41; acc: 0.64
Batch: 20; loss: 1.53; acc: 0.58
Batch: 40; loss: 1.2; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.66
Batch: 80; loss: 1.25; acc: 0.72
Batch: 100; loss: 1.37; acc: 0.73
Batch: 120; loss: 1.6; acc: 0.62
Batch: 140; loss: 1.25; acc: 0.73
Val Epoch over. val_loss: 1.3907575220059438; val_accuracy: 0.6826234076433121 

The current subspace-distance is: 8.498569513903931e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.72
Batch: 20; loss: 1.41; acc: 0.61
Batch: 40; loss: 1.47; acc: 0.62
Batch: 60; loss: 1.32; acc: 0.69
Batch: 80; loss: 1.35; acc: 0.72
Batch: 100; loss: 1.46; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.69
Batch: 140; loss: 1.41; acc: 0.67
Batch: 160; loss: 1.54; acc: 0.55
Batch: 180; loss: 1.51; acc: 0.64
Batch: 200; loss: 1.33; acc: 0.7
Batch: 220; loss: 1.46; acc: 0.62
Batch: 240; loss: 1.28; acc: 0.7
Batch: 260; loss: 1.28; acc: 0.84
Batch: 280; loss: 1.43; acc: 0.67
Batch: 300; loss: 1.36; acc: 0.61
Batch: 320; loss: 1.31; acc: 0.73
Batch: 340; loss: 1.46; acc: 0.56
Batch: 360; loss: 1.4; acc: 0.62
Batch: 380; loss: 1.29; acc: 0.7
Batch: 400; loss: 1.36; acc: 0.7
Batch: 420; loss: 1.49; acc: 0.58
Batch: 440; loss: 1.51; acc: 0.56
Batch: 460; loss: 1.45; acc: 0.66
Batch: 480; loss: 1.4; acc: 0.59
Batch: 500; loss: 1.46; acc: 0.62
Batch: 520; loss: 1.31; acc: 0.77
Batch: 540; loss: 1.39; acc: 0.69
Batch: 560; loss: 1.47; acc: 0.59
Batch: 580; loss: 1.47; acc: 0.64
Batch: 600; loss: 1.35; acc: 0.64
Batch: 620; loss: 1.39; acc: 0.64
Batch: 640; loss: 1.42; acc: 0.59
Batch: 660; loss: 1.32; acc: 0.67
Batch: 680; loss: 1.32; acc: 0.72
Batch: 700; loss: 1.37; acc: 0.66
Batch: 720; loss: 1.54; acc: 0.55
Batch: 740; loss: 1.38; acc: 0.58
Batch: 760; loss: 1.28; acc: 0.75
Batch: 780; loss: 1.35; acc: 0.67
Train Epoch over. train_loss: 1.41; train_accuracy: 0.64 

0.00010074891906697303
9.517920989310369e-05
Batch: 0; loss: 1.38; acc: 0.62
Batch: 20; loss: 1.5; acc: 0.62
Batch: 40; loss: 1.15; acc: 0.77
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.22; acc: 0.77
Batch: 100; loss: 1.34; acc: 0.78
Batch: 120; loss: 1.56; acc: 0.59
Batch: 140; loss: 1.2; acc: 0.73
Val Epoch over. val_loss: 1.3521642517891659; val_accuracy: 0.6883957006369427 

The current subspace-distance is: 9.517920989310369e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.39; acc: 0.61
Batch: 20; loss: 1.35; acc: 0.59
Batch: 40; loss: 1.42; acc: 0.66
Batch: 60; loss: 1.36; acc: 0.75
Batch: 80; loss: 1.37; acc: 0.62
Batch: 100; loss: 1.43; acc: 0.64
Batch: 120; loss: 1.46; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.66
Batch: 160; loss: 1.36; acc: 0.64
Batch: 180; loss: 1.45; acc: 0.58
Batch: 200; loss: 1.25; acc: 0.75
Batch: 220; loss: 1.36; acc: 0.67
Batch: 240; loss: 1.38; acc: 0.66
Batch: 260; loss: 1.4; acc: 0.62
Batch: 280; loss: 1.31; acc: 0.77
Batch: 300; loss: 1.41; acc: 0.59
Batch: 320; loss: 1.27; acc: 0.69
Batch: 340; loss: 1.3; acc: 0.72
Batch: 360; loss: 1.44; acc: 0.67
Batch: 380; loss: 1.53; acc: 0.59
Batch: 400; loss: 1.47; acc: 0.61
Batch: 420; loss: 1.39; acc: 0.67
Batch: 440; loss: 1.56; acc: 0.52
Batch: 460; loss: 1.31; acc: 0.7
Batch: 480; loss: 1.26; acc: 0.73
Batch: 500; loss: 1.39; acc: 0.69
Batch: 520; loss: 1.38; acc: 0.61
Batch: 540; loss: 1.4; acc: 0.66
Batch: 560; loss: 1.4; acc: 0.56
Batch: 580; loss: 1.38; acc: 0.56
Batch: 600; loss: 1.38; acc: 0.66
Batch: 620; loss: 1.41; acc: 0.69
Batch: 640; loss: 1.15; acc: 0.77
Batch: 660; loss: 1.44; acc: 0.61
Batch: 680; loss: 1.34; acc: 0.75
Batch: 700; loss: 1.43; acc: 0.67
Batch: 720; loss: 1.39; acc: 0.66
Batch: 740; loss: 1.31; acc: 0.77
Batch: 760; loss: 1.5; acc: 0.5
Batch: 780; loss: 1.34; acc: 0.61
Train Epoch over. train_loss: 1.38; train_accuracy: 0.65 

0.00010766470222733915
0.00010081535583594814
Batch: 0; loss: 1.35; acc: 0.62
Batch: 20; loss: 1.51; acc: 0.59
Batch: 40; loss: 1.1; acc: 0.75
Batch: 60; loss: 1.34; acc: 0.67
Batch: 80; loss: 1.18; acc: 0.8
Batch: 100; loss: 1.34; acc: 0.75
Batch: 120; loss: 1.54; acc: 0.56
Batch: 140; loss: 1.15; acc: 0.75
Val Epoch over. val_loss: 1.3240610118124896; val_accuracy: 0.6955613057324841 

The current subspace-distance is: 0.00010081535583594814 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.7
Batch: 20; loss: 1.44; acc: 0.56
Batch: 40; loss: 1.41; acc: 0.62
Batch: 60; loss: 1.36; acc: 0.7
Batch: 80; loss: 1.32; acc: 0.61
Batch: 100; loss: 1.32; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.61
Batch: 140; loss: 1.43; acc: 0.58
Batch: 160; loss: 1.42; acc: 0.62
Batch: 180; loss: 1.32; acc: 0.72
Batch: 200; loss: 1.45; acc: 0.62
Batch: 220; loss: 1.33; acc: 0.66
Batch: 240; loss: 1.39; acc: 0.61
Batch: 260; loss: 1.38; acc: 0.67
Batch: 280; loss: 1.38; acc: 0.62
Batch: 300; loss: 1.38; acc: 0.64
Batch: 320; loss: 1.35; acc: 0.62
Batch: 340; loss: 1.28; acc: 0.7
Batch: 360; loss: 1.23; acc: 0.72
Batch: 380; loss: 1.49; acc: 0.59
Batch: 400; loss: 1.34; acc: 0.67
Batch: 420; loss: 1.39; acc: 0.64
Batch: 440; loss: 1.42; acc: 0.62
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.26; acc: 0.69
Batch: 500; loss: 1.39; acc: 0.64
Batch: 520; loss: 1.37; acc: 0.64
Batch: 540; loss: 1.35; acc: 0.69
Batch: 560; loss: 1.29; acc: 0.75
Batch: 580; loss: 1.44; acc: 0.55
Batch: 600; loss: 1.36; acc: 0.67
Batch: 620; loss: 1.37; acc: 0.64
Batch: 640; loss: 1.25; acc: 0.75
Batch: 660; loss: 1.33; acc: 0.7
Batch: 680; loss: 1.38; acc: 0.56
Batch: 700; loss: 1.37; acc: 0.73
Batch: 720; loss: 1.38; acc: 0.69
Batch: 740; loss: 1.35; acc: 0.61
Batch: 760; loss: 1.35; acc: 0.72
Batch: 780; loss: 1.38; acc: 0.67
Train Epoch over. train_loss: 1.37; train_accuracy: 0.65 

0.00011330687266308814
0.00010781854507513344
Batch: 0; loss: 1.34; acc: 0.59
Batch: 20; loss: 1.52; acc: 0.62
Batch: 40; loss: 1.08; acc: 0.75
Batch: 60; loss: 1.34; acc: 0.72
Batch: 80; loss: 1.17; acc: 0.81
Batch: 100; loss: 1.33; acc: 0.72
Batch: 120; loss: 1.55; acc: 0.58
Batch: 140; loss: 1.1; acc: 0.78
Val Epoch over. val_loss: 1.3181272016209402; val_accuracy: 0.6952627388535032 

The current subspace-distance is: 0.00010781854507513344 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.75
Batch: 20; loss: 1.43; acc: 0.55
Batch: 40; loss: 1.4; acc: 0.67
Batch: 60; loss: 1.33; acc: 0.7
Batch: 80; loss: 1.37; acc: 0.61
Batch: 100; loss: 1.43; acc: 0.53
Batch: 120; loss: 1.44; acc: 0.61
Batch: 140; loss: 1.33; acc: 0.62
Batch: 160; loss: 1.42; acc: 0.56
Batch: 180; loss: 1.3; acc: 0.67
Batch: 200; loss: 1.4; acc: 0.67
Batch: 220; loss: 1.28; acc: 0.69
Batch: 240; loss: 1.31; acc: 0.69
Batch: 260; loss: 1.35; acc: 0.66
Batch: 280; loss: 1.34; acc: 0.64
Batch: 300; loss: 1.54; acc: 0.55
Batch: 320; loss: 1.44; acc: 0.59
Batch: 340; loss: 1.44; acc: 0.56
Batch: 360; loss: 1.29; acc: 0.8
Batch: 380; loss: 1.33; acc: 0.58
Batch: 400; loss: 1.32; acc: 0.73
Batch: 420; loss: 1.39; acc: 0.7
Batch: 440; loss: 1.21; acc: 0.69
Batch: 460; loss: 1.37; acc: 0.62
Batch: 480; loss: 1.38; acc: 0.59
Batch: 500; loss: 1.45; acc: 0.62
Batch: 520; loss: 1.54; acc: 0.52
Batch: 540; loss: 1.4; acc: 0.64
Batch: 560; loss: 1.35; acc: 0.66
Batch: 580; loss: 1.38; acc: 0.58
Batch: 600; loss: 1.25; acc: 0.72
Batch: 620; loss: 1.39; acc: 0.67
Batch: 640; loss: 1.4; acc: 0.59
Batch: 660; loss: 1.5; acc: 0.5
Batch: 680; loss: 1.34; acc: 0.61
Batch: 700; loss: 1.6; acc: 0.48
Batch: 720; loss: 1.33; acc: 0.69
Batch: 740; loss: 1.32; acc: 0.59
Batch: 760; loss: 1.33; acc: 0.66
Batch: 780; loss: 1.39; acc: 0.61
Train Epoch over. train_loss: 1.35; train_accuracy: 0.65 

0.00011617987183853984
0.00011080256081186235
Batch: 0; loss: 1.34; acc: 0.58
Batch: 20; loss: 1.53; acc: 0.56
Batch: 40; loss: 1.03; acc: 0.78
Batch: 60; loss: 1.31; acc: 0.73
Batch: 80; loss: 1.14; acc: 0.83
Batch: 100; loss: 1.31; acc: 0.7
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.06; acc: 0.78
Val Epoch over. val_loss: 1.2966271813508052; val_accuracy: 0.6952627388535032 

The current subspace-distance is: 0.00011080256081186235 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.66
Batch: 20; loss: 1.47; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.67
Batch: 60; loss: 1.3; acc: 0.75
Batch: 80; loss: 1.42; acc: 0.62
Batch: 100; loss: 1.37; acc: 0.62
Batch: 120; loss: 1.38; acc: 0.66
Batch: 140; loss: 1.31; acc: 0.66
Batch: 160; loss: 1.39; acc: 0.59
Batch: 180; loss: 1.39; acc: 0.67
Batch: 200; loss: 1.44; acc: 0.62
Batch: 220; loss: 1.2; acc: 0.81
Batch: 240; loss: 1.31; acc: 0.7
Batch: 260; loss: 1.24; acc: 0.72
Batch: 280; loss: 1.32; acc: 0.66
Batch: 300; loss: 1.48; acc: 0.62
Batch: 320; loss: 1.39; acc: 0.59
Batch: 340; loss: 1.23; acc: 0.62
Batch: 360; loss: 1.23; acc: 0.75
Batch: 380; loss: 1.36; acc: 0.62
Batch: 400; loss: 1.29; acc: 0.7
Batch: 420; loss: 1.37; acc: 0.66
Batch: 440; loss: 1.39; acc: 0.61
Batch: 460; loss: 1.26; acc: 0.67
Batch: 480; loss: 1.19; acc: 0.8
Batch: 500; loss: 1.42; acc: 0.61
Batch: 520; loss: 1.33; acc: 0.62
Batch: 540; loss: 1.23; acc: 0.73
Batch: 560; loss: 1.3; acc: 0.69
Batch: 580; loss: 1.36; acc: 0.67
Batch: 600; loss: 1.35; acc: 0.56
Batch: 620; loss: 1.44; acc: 0.59
Batch: 640; loss: 1.28; acc: 0.69
Batch: 660; loss: 1.26; acc: 0.7
Batch: 680; loss: 1.41; acc: 0.64
Batch: 700; loss: 1.33; acc: 0.66
Batch: 720; loss: 1.28; acc: 0.67
Batch: 740; loss: 1.34; acc: 0.66
Batch: 760; loss: 1.34; acc: 0.64
Batch: 780; loss: 1.22; acc: 0.72
Train Epoch over. train_loss: 1.33; train_accuracy: 0.66 

0.00012357767263893038
0.00011590775829972699
Batch: 0; loss: 1.3; acc: 0.64
Batch: 20; loss: 1.52; acc: 0.58
Batch: 40; loss: 0.98; acc: 0.84
Batch: 60; loss: 1.28; acc: 0.75
Batch: 80; loss: 1.11; acc: 0.81
Batch: 100; loss: 1.26; acc: 0.72
Batch: 120; loss: 1.49; acc: 0.59
Batch: 140; loss: 1.01; acc: 0.78
Val Epoch over. val_loss: 1.2677970426097798; val_accuracy: 0.7006369426751592 

The current subspace-distance is: 0.00011590775829972699 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.59
Batch: 20; loss: 1.33; acc: 0.66
Batch: 40; loss: 1.43; acc: 0.58
Batch: 60; loss: 1.38; acc: 0.69
Batch: 80; loss: 1.27; acc: 0.69
Batch: 100; loss: 1.36; acc: 0.64
Batch: 120; loss: 1.48; acc: 0.56
Batch: 140; loss: 1.26; acc: 0.69
Batch: 160; loss: 1.23; acc: 0.73
Batch: 180; loss: 1.2; acc: 0.69
Batch: 200; loss: 1.32; acc: 0.67
Batch: 220; loss: 1.31; acc: 0.69
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.33; acc: 0.62
Batch: 280; loss: 1.36; acc: 0.59
Batch: 300; loss: 1.3; acc: 0.7
Batch: 320; loss: 1.34; acc: 0.62
Batch: 340; loss: 1.26; acc: 0.66
Batch: 360; loss: 1.24; acc: 0.66
Batch: 380; loss: 1.26; acc: 0.66
Batch: 400; loss: 1.31; acc: 0.64
Batch: 420; loss: 1.23; acc: 0.7
Batch: 440; loss: 1.4; acc: 0.66
Batch: 460; loss: 1.33; acc: 0.59
Batch: 480; loss: 1.37; acc: 0.62
Batch: 500; loss: 1.27; acc: 0.61
Batch: 520; loss: 1.22; acc: 0.67
Batch: 540; loss: 1.23; acc: 0.67
Batch: 560; loss: 1.25; acc: 0.73
Batch: 580; loss: 1.31; acc: 0.64
Batch: 600; loss: 1.39; acc: 0.66
Batch: 620; loss: 1.17; acc: 0.75
Batch: 640; loss: 1.15; acc: 0.7
Batch: 660; loss: 1.29; acc: 0.64
Batch: 680; loss: 1.16; acc: 0.73
Batch: 700; loss: 1.32; acc: 0.56
Batch: 720; loss: 1.28; acc: 0.62
Batch: 740; loss: 1.36; acc: 0.59
Batch: 760; loss: 1.35; acc: 0.62
Batch: 780; loss: 1.43; acc: 0.52
Train Epoch over. train_loss: 1.29; train_accuracy: 0.66 

0.00012978386075701565
0.00012275399058125913
Batch: 0; loss: 1.27; acc: 0.69
Batch: 20; loss: 1.5; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.81
Batch: 60; loss: 1.25; acc: 0.7
Batch: 80; loss: 1.07; acc: 0.83
Batch: 100; loss: 1.19; acc: 0.78
Batch: 120; loss: 1.45; acc: 0.61
Batch: 140; loss: 0.97; acc: 0.81
Val Epoch over. val_loss: 1.2282541546092671; val_accuracy: 0.6998407643312102 

The current subspace-distance is: 0.00012275399058125913 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.66
Batch: 20; loss: 1.27; acc: 0.69
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.21; acc: 0.69
Batch: 100; loss: 1.29; acc: 0.62
Batch: 120; loss: 1.42; acc: 0.61
Batch: 140; loss: 1.22; acc: 0.72
Batch: 160; loss: 1.3; acc: 0.62
Batch: 180; loss: 1.33; acc: 0.66
Batch: 200; loss: 1.21; acc: 0.77
Batch: 220; loss: 1.23; acc: 0.66
Batch: 240; loss: 1.34; acc: 0.64
Batch: 260; loss: 1.2; acc: 0.69
Batch: 280; loss: 1.25; acc: 0.69
Batch: 300; loss: 1.25; acc: 0.72
Batch: 320; loss: 1.28; acc: 0.59
Batch: 340; loss: 1.42; acc: 0.64
Batch: 360; loss: 1.32; acc: 0.62
Batch: 380; loss: 1.35; acc: 0.67
Batch: 400; loss: 1.31; acc: 0.66
Batch: 420; loss: 1.3; acc: 0.66
Batch: 440; loss: 1.44; acc: 0.55
Batch: 460; loss: 1.17; acc: 0.73
Batch: 480; loss: 1.39; acc: 0.61
Batch: 500; loss: 1.26; acc: 0.69
Batch: 520; loss: 1.44; acc: 0.64
Batch: 540; loss: 1.17; acc: 0.7
Batch: 560; loss: 1.31; acc: 0.61
Batch: 580; loss: 1.33; acc: 0.64
Batch: 600; loss: 1.25; acc: 0.72
Batch: 620; loss: 1.22; acc: 0.7
Batch: 640; loss: 1.33; acc: 0.61
Batch: 660; loss: 1.28; acc: 0.66
Batch: 680; loss: 1.39; acc: 0.61
Batch: 700; loss: 1.23; acc: 0.66
Batch: 720; loss: 1.32; acc: 0.64
Batch: 740; loss: 1.29; acc: 0.64
Batch: 760; loss: 1.35; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.66 

0.00013127039710525423
0.00012415391393005848
Batch: 0; loss: 1.26; acc: 0.67
Batch: 20; loss: 1.48; acc: 0.56
Batch: 40; loss: 0.9; acc: 0.8
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.83
Batch: 100; loss: 1.15; acc: 0.81
Batch: 120; loss: 1.43; acc: 0.61
Batch: 140; loss: 0.96; acc: 0.81
Val Epoch over. val_loss: 1.214813183447358; val_accuracy: 0.7000398089171974 

The current subspace-distance is: 0.00012415391393005848 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.69
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 1.31; acc: 0.75
Batch: 60; loss: 1.26; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.83
Batch: 100; loss: 1.35; acc: 0.61
Batch: 120; loss: 1.29; acc: 0.59
Batch: 140; loss: 1.3; acc: 0.66
Batch: 160; loss: 1.38; acc: 0.59
Batch: 180; loss: 1.2; acc: 0.7
Batch: 200; loss: 1.55; acc: 0.48
Batch: 220; loss: 1.3; acc: 0.59
Batch: 240; loss: 1.19; acc: 0.69
Batch: 260; loss: 1.26; acc: 0.64
Batch: 280; loss: 1.17; acc: 0.75
Batch: 300; loss: 1.33; acc: 0.59
Batch: 320; loss: 1.34; acc: 0.59
Batch: 340; loss: 1.17; acc: 0.7
Batch: 360; loss: 1.22; acc: 0.72
Batch: 380; loss: 1.17; acc: 0.75
Batch: 400; loss: 1.4; acc: 0.56
Batch: 420; loss: 1.41; acc: 0.59
Batch: 440; loss: 1.29; acc: 0.64
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.25; acc: 0.75
Batch: 500; loss: 1.12; acc: 0.73
Batch: 520; loss: 1.22; acc: 0.72
Batch: 540; loss: 1.3; acc: 0.56
Batch: 560; loss: 1.33; acc: 0.64
Batch: 580; loss: 1.4; acc: 0.58
Batch: 600; loss: 1.22; acc: 0.69
Batch: 620; loss: 1.37; acc: 0.55
Batch: 640; loss: 1.22; acc: 0.75
Batch: 660; loss: 1.14; acc: 0.72
Batch: 680; loss: 1.3; acc: 0.61
Batch: 700; loss: 1.29; acc: 0.66
Batch: 720; loss: 1.4; acc: 0.64
Batch: 740; loss: 1.27; acc: 0.64
Batch: 760; loss: 1.22; acc: 0.69
Batch: 780; loss: 1.34; acc: 0.58
Train Epoch over. train_loss: 1.26; train_accuracy: 0.66 

0.0001338785805273801
0.00012738131044898182
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.48; acc: 0.56
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 1.25; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.77
Batch: 100; loss: 1.15; acc: 0.8
Batch: 120; loss: 1.42; acc: 0.62
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.2089358340403078; val_accuracy: 0.6984474522292994 

The current subspace-distance is: 0.00012738131044898182 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 1.25; acc: 0.75
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.21; acc: 0.69
Batch: 100; loss: 1.22; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.73
Batch: 140; loss: 1.28; acc: 0.67
Batch: 160; loss: 1.21; acc: 0.66
Batch: 180; loss: 1.16; acc: 0.75
Batch: 200; loss: 1.19; acc: 0.61
Batch: 220; loss: 1.35; acc: 0.67
Batch: 240; loss: 1.31; acc: 0.66
Batch: 260; loss: 1.4; acc: 0.56
Batch: 280; loss: 1.29; acc: 0.64
Batch: 300; loss: 1.33; acc: 0.7
Batch: 320; loss: 1.23; acc: 0.66
Batch: 340; loss: 1.35; acc: 0.62
Batch: 360; loss: 1.31; acc: 0.64
Batch: 380; loss: 1.15; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.25; acc: 0.64
Batch: 440; loss: 1.2; acc: 0.69
Batch: 460; loss: 1.3; acc: 0.66
Batch: 480; loss: 1.16; acc: 0.69
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 1.32; acc: 0.64
Batch: 540; loss: 1.22; acc: 0.67
Batch: 560; loss: 1.19; acc: 0.64
Batch: 580; loss: 1.22; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.72
Batch: 620; loss: 1.2; acc: 0.67
Batch: 640; loss: 1.2; acc: 0.67
Batch: 660; loss: 1.25; acc: 0.64
Batch: 680; loss: 1.14; acc: 0.75
Batch: 700; loss: 1.23; acc: 0.64
Batch: 720; loss: 1.21; acc: 0.66
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.51; acc: 0.52
Batch: 780; loss: 1.2; acc: 0.67
Train Epoch over. train_loss: 1.25; train_accuracy: 0.66 

0.00013663120626006275
0.0001302074670093134
Batch: 0; loss: 1.24; acc: 0.69
Batch: 20; loss: 1.46; acc: 0.55
Batch: 40; loss: 0.89; acc: 0.78
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.78
Batch: 100; loss: 1.13; acc: 0.8
Batch: 120; loss: 1.4; acc: 0.61
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.2025824238540261; val_accuracy: 0.700437898089172 

The current subspace-distance is: 0.0001302074670093134 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.72
Batch: 20; loss: 1.05; acc: 0.77
Batch: 40; loss: 1.32; acc: 0.59
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.21; acc: 0.64
Batch: 100; loss: 1.34; acc: 0.62
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.64
Batch: 160; loss: 1.29; acc: 0.64
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.1; acc: 0.78
Batch: 220; loss: 1.38; acc: 0.62
Batch: 240; loss: 1.18; acc: 0.7
Batch: 260; loss: 1.25; acc: 0.67
Batch: 280; loss: 1.3; acc: 0.56
Batch: 300; loss: 1.28; acc: 0.69
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.29; acc: 0.58
Batch: 360; loss: 1.26; acc: 0.66
Batch: 380; loss: 1.29; acc: 0.66
Batch: 400; loss: 1.24; acc: 0.67
Batch: 420; loss: 1.17; acc: 0.61
Batch: 440; loss: 1.29; acc: 0.66
Batch: 460; loss: 1.37; acc: 0.59
Batch: 480; loss: 1.21; acc: 0.69
Batch: 500; loss: 1.23; acc: 0.67
Batch: 520; loss: 1.11; acc: 0.77
Batch: 540; loss: 1.19; acc: 0.72
Batch: 560; loss: 1.35; acc: 0.66
Batch: 580; loss: 1.29; acc: 0.67
Batch: 600; loss: 1.16; acc: 0.7
Batch: 620; loss: 1.23; acc: 0.66
Batch: 640; loss: 1.11; acc: 0.75
Batch: 660; loss: 1.34; acc: 0.55
Batch: 680; loss: 1.34; acc: 0.64
Batch: 700; loss: 1.32; acc: 0.59
Batch: 720; loss: 1.28; acc: 0.69
Batch: 740; loss: 1.23; acc: 0.66
Batch: 760; loss: 1.25; acc: 0.66
Batch: 780; loss: 1.28; acc: 0.61
Train Epoch over. train_loss: 1.25; train_accuracy: 0.67 

0.00013928076077718288
0.00013245413720142096
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.45; acc: 0.55
Batch: 40; loss: 0.88; acc: 0.84
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.03; acc: 0.78
Batch: 100; loss: 1.12; acc: 0.8
Batch: 120; loss: 1.39; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.1923328668448576; val_accuracy: 0.701234076433121 

The current subspace-distance is: 0.00013245413720142096 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.69
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.59
Batch: 80; loss: 1.28; acc: 0.59
Batch: 100; loss: 1.24; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.35; acc: 0.64
Batch: 160; loss: 1.16; acc: 0.7
Batch: 180; loss: 1.2; acc: 0.73
Batch: 200; loss: 1.19; acc: 0.66
Batch: 220; loss: 1.31; acc: 0.59
Batch: 240; loss: 1.27; acc: 0.62
Batch: 260; loss: 1.3; acc: 0.66
Batch: 280; loss: 1.17; acc: 0.75
Batch: 300; loss: 1.21; acc: 0.69
Batch: 320; loss: 1.25; acc: 0.66
Batch: 340; loss: 1.38; acc: 0.56
Batch: 360; loss: 1.3; acc: 0.66
Batch: 380; loss: 1.37; acc: 0.61
Batch: 400; loss: 1.33; acc: 0.64
Batch: 420; loss: 1.17; acc: 0.7
Batch: 440; loss: 1.28; acc: 0.69
Batch: 460; loss: 1.27; acc: 0.69
Batch: 480; loss: 1.23; acc: 0.64
Batch: 500; loss: 1.22; acc: 0.75
Batch: 520; loss: 1.09; acc: 0.78
Batch: 540; loss: 1.27; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.67
Batch: 580; loss: 1.47; acc: 0.55
Batch: 600; loss: 1.2; acc: 0.64
Batch: 620; loss: 1.28; acc: 0.61
Batch: 640; loss: 1.19; acc: 0.77
Batch: 660; loss: 1.07; acc: 0.75
Batch: 680; loss: 1.24; acc: 0.59
Batch: 700; loss: 1.21; acc: 0.64
Batch: 720; loss: 1.18; acc: 0.73
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.25; acc: 0.67
Batch: 780; loss: 1.08; acc: 0.73
Train Epoch over. train_loss: 1.24; train_accuracy: 0.67 

0.00014049590390641242
0.0001331851672148332
Batch: 0; loss: 1.24; acc: 0.69
Batch: 20; loss: 1.44; acc: 0.58
Batch: 40; loss: 0.87; acc: 0.83
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 1.11; acc: 0.8
Batch: 120; loss: 1.38; acc: 0.62
Batch: 140; loss: 0.94; acc: 0.78
Val Epoch over. val_loss: 1.1818304927485763; val_accuracy: 0.6963574840764332 

The current subspace-distance is: 0.0001331851672148332 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.29; acc: 0.66
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 1.32; acc: 0.56
Batch: 160; loss: 1.28; acc: 0.59
Batch: 180; loss: 1.19; acc: 0.72
Batch: 200; loss: 1.42; acc: 0.58
Batch: 220; loss: 1.38; acc: 0.62
Batch: 240; loss: 1.1; acc: 0.75
Batch: 260; loss: 1.31; acc: 0.61
Batch: 280; loss: 1.11; acc: 0.73
Batch: 300; loss: 1.17; acc: 0.73
Batch: 320; loss: 1.15; acc: 0.7
Batch: 340; loss: 1.22; acc: 0.7
Batch: 360; loss: 1.29; acc: 0.64
Batch: 380; loss: 1.4; acc: 0.62
Batch: 400; loss: 1.13; acc: 0.7
Batch: 420; loss: 1.29; acc: 0.66
Batch: 440; loss: 1.26; acc: 0.66
Batch: 460; loss: 1.25; acc: 0.7
Batch: 480; loss: 1.21; acc: 0.69
Batch: 500; loss: 1.43; acc: 0.47
Batch: 520; loss: 1.33; acc: 0.62
Batch: 540; loss: 1.36; acc: 0.52
Batch: 560; loss: 1.16; acc: 0.72
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.22; acc: 0.69
Batch: 620; loss: 1.18; acc: 0.72
Batch: 640; loss: 1.27; acc: 0.62
Batch: 660; loss: 1.15; acc: 0.67
Batch: 680; loss: 1.34; acc: 0.56
Batch: 700; loss: 1.27; acc: 0.62
Batch: 720; loss: 1.12; acc: 0.78
Batch: 740; loss: 1.22; acc: 0.7
Batch: 760; loss: 1.36; acc: 0.61
Batch: 780; loss: 1.22; acc: 0.67
Train Epoch over. train_loss: 1.23; train_accuracy: 0.67 

0.00014356004248838872
0.00013698256225325167
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.43; acc: 0.59
Batch: 40; loss: 0.86; acc: 0.84
Batch: 60; loss: 1.22; acc: 0.66
Batch: 80; loss: 1.01; acc: 0.8
Batch: 100; loss: 1.11; acc: 0.8
Batch: 120; loss: 1.38; acc: 0.64
Batch: 140; loss: 0.93; acc: 0.78
Val Epoch over. val_loss: 1.1748892330819634; val_accuracy: 0.7001393312101911 

The current subspace-distance is: 0.00013698256225325167 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.72
Batch: 20; loss: 1.28; acc: 0.64
Batch: 40; loss: 1.39; acc: 0.62
Batch: 60; loss: 1.17; acc: 0.77
Batch: 80; loss: 1.11; acc: 0.78
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 1.26; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.66
Batch: 160; loss: 1.24; acc: 0.73
Batch: 180; loss: 1.38; acc: 0.66
Batch: 200; loss: 1.13; acc: 0.66
Batch: 220; loss: 1.29; acc: 0.67
Batch: 240; loss: 1.24; acc: 0.62
Batch: 260; loss: 1.41; acc: 0.48
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.11; acc: 0.78
Batch: 320; loss: 1.3; acc: 0.64
Batch: 340; loss: 1.22; acc: 0.69
Batch: 360; loss: 1.34; acc: 0.56
Batch: 380; loss: 1.2; acc: 0.7
Batch: 400; loss: 1.43; acc: 0.5
Batch: 420; loss: 1.11; acc: 0.75
Batch: 440; loss: 1.06; acc: 0.73
Batch: 460; loss: 1.2; acc: 0.66
Batch: 480; loss: 1.2; acc: 0.67
Batch: 500; loss: 1.36; acc: 0.61
Batch: 520; loss: 1.29; acc: 0.67
Batch: 540; loss: 1.3; acc: 0.62
Batch: 560; loss: 1.26; acc: 0.66
Batch: 580; loss: 1.08; acc: 0.73
Batch: 600; loss: 1.31; acc: 0.55
Batch: 620; loss: 1.27; acc: 0.64
Batch: 640; loss: 1.29; acc: 0.58
Batch: 660; loss: 1.47; acc: 0.5
Batch: 680; loss: 1.16; acc: 0.7
Batch: 700; loss: 1.08; acc: 0.75
Batch: 720; loss: 1.14; acc: 0.7
Batch: 740; loss: 1.28; acc: 0.69
Batch: 760; loss: 1.08; acc: 0.75
Batch: 780; loss: 1.19; acc: 0.62
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.00014477205695584416
0.0001372666738461703
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.43; acc: 0.58
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 1.2; acc: 0.67
Batch: 80; loss: 1.0; acc: 0.83
Batch: 100; loss: 1.09; acc: 0.81
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 0.92; acc: 0.77
Val Epoch over. val_loss: 1.1637411368121007; val_accuracy: 0.7033240445859873 

The current subspace-distance is: 0.0001372666738461703 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.05; acc: 0.86
Batch: 20; loss: 1.17; acc: 0.62
Batch: 40; loss: 1.17; acc: 0.75
Batch: 60; loss: 1.27; acc: 0.7
Batch: 80; loss: 1.36; acc: 0.55
Batch: 100; loss: 1.18; acc: 0.66
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 1.12; acc: 0.72
Batch: 160; loss: 1.26; acc: 0.64
Batch: 180; loss: 1.16; acc: 0.78
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 1.11; acc: 0.72
Batch: 240; loss: 1.33; acc: 0.61
Batch: 260; loss: 1.22; acc: 0.67
Batch: 280; loss: 1.27; acc: 0.59
Batch: 300; loss: 1.29; acc: 0.64
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 1.13; acc: 0.77
Batch: 360; loss: 1.25; acc: 0.66
Batch: 380; loss: 1.24; acc: 0.66
Batch: 400; loss: 1.13; acc: 0.72
Batch: 420; loss: 1.22; acc: 0.66
Batch: 440; loss: 1.24; acc: 0.64
Batch: 460; loss: 1.31; acc: 0.61
Batch: 480; loss: 1.18; acc: 0.66
Batch: 500; loss: 1.27; acc: 0.62
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 1.1; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.7
Batch: 580; loss: 1.3; acc: 0.62
Batch: 600; loss: 1.26; acc: 0.61
Batch: 620; loss: 1.09; acc: 0.69
Batch: 640; loss: 1.14; acc: 0.72
Batch: 660; loss: 1.28; acc: 0.66
Batch: 680; loss: 1.23; acc: 0.66
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.23; acc: 0.64
Batch: 740; loss: 1.08; acc: 0.78
Batch: 760; loss: 1.26; acc: 0.67
Batch: 780; loss: 1.08; acc: 0.77
Train Epoch over. train_loss: 1.21; train_accuracy: 0.67 

0.00014605144679080695
0.00014013072359375656
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 0.84; acc: 0.86
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 1.06; acc: 0.81
Batch: 120; loss: 1.34; acc: 0.64
Batch: 140; loss: 0.93; acc: 0.75
Val Epoch over. val_loss: 1.1579051974472727; val_accuracy: 0.7038216560509554 

The current subspace-distance is: 0.00014013072359375656 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.13; acc: 0.75
Batch: 40; loss: 1.15; acc: 0.72
Batch: 60; loss: 1.08; acc: 0.73
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.19; acc: 0.61
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 1.14; acc: 0.69
Batch: 160; loss: 1.22; acc: 0.67
Batch: 180; loss: 1.15; acc: 0.61
Batch: 200; loss: 1.28; acc: 0.56
Batch: 220; loss: 1.26; acc: 0.64
Batch: 240; loss: 1.41; acc: 0.48
Batch: 260; loss: 1.43; acc: 0.56
Batch: 280; loss: 1.25; acc: 0.73
Batch: 300; loss: 1.12; acc: 0.75
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.19; acc: 0.69
Batch: 360; loss: 1.24; acc: 0.62
Batch: 380; loss: 1.28; acc: 0.64
Batch: 400; loss: 1.01; acc: 0.75
Batch: 420; loss: 1.11; acc: 0.75
Batch: 440; loss: 1.32; acc: 0.55
Batch: 460; loss: 1.28; acc: 0.7
Batch: 480; loss: 1.08; acc: 0.84
Batch: 500; loss: 1.31; acc: 0.62
Batch: 520; loss: 1.29; acc: 0.59
Batch: 540; loss: 1.23; acc: 0.67
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.25; acc: 0.66
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.11; acc: 0.67
Batch: 640; loss: 1.22; acc: 0.66
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.34; acc: 0.61
Batch: 700; loss: 1.22; acc: 0.67
Batch: 720; loss: 1.08; acc: 0.75
Batch: 740; loss: 1.19; acc: 0.67
Batch: 760; loss: 1.29; acc: 0.66
Batch: 780; loss: 1.23; acc: 0.64
Train Epoch over. train_loss: 1.2; train_accuracy: 0.68 

0.000145489233545959
0.00014053317136131227
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.41; acc: 0.58
Batch: 40; loss: 0.82; acc: 0.88
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 1.06; acc: 0.8
Batch: 120; loss: 1.32; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.75
Val Epoch over. val_loss: 1.1452436519276565; val_accuracy: 0.7051154458598726 

The current subspace-distance is: 0.00014053317136131227 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.18; acc: 0.62
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 1.23; acc: 0.66
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 1.15; acc: 0.73
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 1.23; acc: 0.73
Batch: 220; loss: 1.16; acc: 0.64
Batch: 240; loss: 1.2; acc: 0.69
Batch: 260; loss: 1.21; acc: 0.69
Batch: 280; loss: 1.15; acc: 0.7
Batch: 300; loss: 1.18; acc: 0.7
Batch: 320; loss: 1.12; acc: 0.67
Batch: 340; loss: 1.19; acc: 0.67
Batch: 360; loss: 1.23; acc: 0.66
Batch: 380; loss: 1.24; acc: 0.67
Batch: 400; loss: 1.09; acc: 0.72
Batch: 420; loss: 1.21; acc: 0.73
Batch: 440; loss: 1.28; acc: 0.64
Batch: 460; loss: 1.0; acc: 0.8
Batch: 480; loss: 1.18; acc: 0.75
Batch: 500; loss: 1.22; acc: 0.7
Batch: 520; loss: 1.25; acc: 0.59
Batch: 540; loss: 1.11; acc: 0.72
Batch: 560; loss: 1.26; acc: 0.73
Batch: 580; loss: 1.25; acc: 0.72
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 1.26; acc: 0.64
Batch: 660; loss: 1.04; acc: 0.81
Batch: 680; loss: 1.25; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.72
Batch: 720; loss: 1.08; acc: 0.72
Batch: 740; loss: 1.09; acc: 0.8
Batch: 760; loss: 1.14; acc: 0.72
Batch: 780; loss: 1.17; acc: 0.73
Train Epoch over. train_loss: 1.19; train_accuracy: 0.68 

0.00015033806266728789
0.0001424376096110791
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.39; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.88
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 1.05; acc: 0.78
Batch: 120; loss: 1.32; acc: 0.64
Batch: 140; loss: 0.9; acc: 0.77
Val Epoch over. val_loss: 1.1389707903953115; val_accuracy: 0.7088972929936306 

The current subspace-distance is: 0.0001424376096110791 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.17; acc: 0.66
Batch: 20; loss: 1.34; acc: 0.58
Batch: 40; loss: 1.3; acc: 0.7
Batch: 60; loss: 1.25; acc: 0.61
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.24; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 1.15; acc: 0.67
Batch: 160; loss: 1.2; acc: 0.62
Batch: 180; loss: 1.13; acc: 0.7
Batch: 200; loss: 1.14; acc: 0.66
Batch: 220; loss: 1.25; acc: 0.7
Batch: 240; loss: 1.11; acc: 0.77
Batch: 260; loss: 1.24; acc: 0.73
Batch: 280; loss: 1.09; acc: 0.7
Batch: 300; loss: 0.98; acc: 0.89
Batch: 320; loss: 1.13; acc: 0.72
Batch: 340; loss: 1.18; acc: 0.64
Batch: 360; loss: 1.05; acc: 0.75
Batch: 380; loss: 1.05; acc: 0.81
Batch: 400; loss: 1.05; acc: 0.7
Batch: 420; loss: 1.14; acc: 0.69
Batch: 440; loss: 1.23; acc: 0.62
Batch: 460; loss: 1.19; acc: 0.7
Batch: 480; loss: 1.19; acc: 0.64
Batch: 500; loss: 1.25; acc: 0.69
Batch: 520; loss: 1.17; acc: 0.62
Batch: 540; loss: 1.27; acc: 0.67
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.06; acc: 0.77
Batch: 600; loss: 1.03; acc: 0.73
Batch: 620; loss: 1.17; acc: 0.72
Batch: 640; loss: 1.11; acc: 0.73
Batch: 660; loss: 1.45; acc: 0.47
Batch: 680; loss: 1.19; acc: 0.66
Batch: 700; loss: 1.16; acc: 0.73
Batch: 720; loss: 1.43; acc: 0.53
Batch: 740; loss: 1.33; acc: 0.61
Batch: 760; loss: 1.18; acc: 0.73
Batch: 780; loss: 1.13; acc: 0.73
Train Epoch over. train_loss: 1.19; train_accuracy: 0.68 

0.0001506452535977587
0.00014180652215145528
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.39; acc: 0.58
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 0.97; acc: 0.8
Batch: 100; loss: 1.03; acc: 0.78
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 0.9; acc: 0.75
Val Epoch over. val_loss: 1.1329992880487139; val_accuracy: 0.7066082802547771 

The current subspace-distance is: 0.00014180652215145528 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.3; acc: 0.62
Batch: 40; loss: 1.12; acc: 0.75
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.21; acc: 0.62
Batch: 100; loss: 1.26; acc: 0.62
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 1.13; acc: 0.75
Batch: 160; loss: 1.28; acc: 0.56
Batch: 180; loss: 1.14; acc: 0.72
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 1.21; acc: 0.69
Batch: 240; loss: 1.21; acc: 0.62
Batch: 260; loss: 0.96; acc: 0.77
Batch: 280; loss: 1.11; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.67
Batch: 320; loss: 1.12; acc: 0.69
Batch: 340; loss: 1.11; acc: 0.75
Batch: 360; loss: 1.16; acc: 0.73
Batch: 380; loss: 1.35; acc: 0.62
Batch: 400; loss: 1.09; acc: 0.7
Batch: 420; loss: 1.14; acc: 0.75
Batch: 440; loss: 1.16; acc: 0.69
Batch: 460; loss: 1.26; acc: 0.59
Batch: 480; loss: 1.14; acc: 0.7
Batch: 500; loss: 1.17; acc: 0.67
Batch: 520; loss: 1.2; acc: 0.69
Batch: 540; loss: 1.29; acc: 0.69
Batch: 560; loss: 1.19; acc: 0.72
Batch: 580; loss: 1.33; acc: 0.59
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.19; acc: 0.62
Batch: 640; loss: 1.23; acc: 0.67
Batch: 660; loss: 1.06; acc: 0.7
Batch: 680; loss: 1.18; acc: 0.64
Batch: 700; loss: 1.28; acc: 0.61
Batch: 720; loss: 1.1; acc: 0.75
Batch: 740; loss: 1.13; acc: 0.64
Batch: 760; loss: 1.14; acc: 0.77
Batch: 780; loss: 1.24; acc: 0.62
Train Epoch over. train_loss: 1.19; train_accuracy: 0.68 

0.0001508699351688847
0.0001443282817490399
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.37; acc: 0.59
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 0.96; acc: 0.81
Batch: 100; loss: 1.03; acc: 0.81
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1218257489477752; val_accuracy: 0.7098925159235668 

The current subspace-distance is: 0.0001443282817490399 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 1.15; acc: 0.73
Batch: 40; loss: 1.01; acc: 0.75
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.14; acc: 0.69
Batch: 100; loss: 1.18; acc: 0.67
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 1.19; acc: 0.64
Batch: 160; loss: 1.21; acc: 0.64
Batch: 180; loss: 1.11; acc: 0.75
Batch: 200; loss: 1.01; acc: 0.75
Batch: 220; loss: 1.15; acc: 0.66
Batch: 240; loss: 1.05; acc: 0.8
Batch: 260; loss: 1.25; acc: 0.64
Batch: 280; loss: 0.99; acc: 0.75
Batch: 300; loss: 1.08; acc: 0.73
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 1.18; acc: 0.66
Batch: 360; loss: 1.25; acc: 0.67
Batch: 380; loss: 1.3; acc: 0.62
Batch: 400; loss: 1.0; acc: 0.8
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 1.06; acc: 0.73
Batch: 460; loss: 1.13; acc: 0.69
Batch: 480; loss: 1.23; acc: 0.62
Batch: 500; loss: 1.11; acc: 0.72
Batch: 520; loss: 1.2; acc: 0.69
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 1.16; acc: 0.61
Batch: 580; loss: 1.3; acc: 0.61
Batch: 600; loss: 1.21; acc: 0.66
Batch: 620; loss: 1.05; acc: 0.8
Batch: 640; loss: 1.05; acc: 0.77
Batch: 660; loss: 1.08; acc: 0.7
Batch: 680; loss: 1.1; acc: 0.7
Batch: 700; loss: 1.03; acc: 0.77
Batch: 720; loss: 1.23; acc: 0.64
Batch: 740; loss: 1.3; acc: 0.66
Batch: 760; loss: 1.16; acc: 0.72
Batch: 780; loss: 1.25; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.68 

0.0001513458846602589
0.00014560791896656156
Batch: 0; loss: 1.21; acc: 0.69
Batch: 20; loss: 1.37; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.88
Batch: 60; loss: 1.17; acc: 0.69
Batch: 80; loss: 0.97; acc: 0.8
Batch: 100; loss: 1.03; acc: 0.81
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 0.9; acc: 0.75
Val Epoch over. val_loss: 1.1310135475389518; val_accuracy: 0.713077229299363 

The current subspace-distance is: 0.00014560791896656156 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 1.39; acc: 0.61
Batch: 60; loss: 1.02; acc: 0.73
Batch: 80; loss: 1.15; acc: 0.72
Batch: 100; loss: 1.25; acc: 0.61
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.06; acc: 0.73
Batch: 160; loss: 1.13; acc: 0.72
Batch: 180; loss: 1.2; acc: 0.67
Batch: 200; loss: 1.22; acc: 0.67
Batch: 220; loss: 1.19; acc: 0.69
Batch: 240; loss: 1.08; acc: 0.72
Batch: 260; loss: 1.2; acc: 0.72
Batch: 280; loss: 1.17; acc: 0.77
Batch: 300; loss: 1.19; acc: 0.64
Batch: 320; loss: 1.17; acc: 0.72
Batch: 340; loss: 1.15; acc: 0.69
Batch: 360; loss: 1.0; acc: 0.77
Batch: 380; loss: 1.05; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.78
Batch: 420; loss: 1.18; acc: 0.73
Batch: 440; loss: 1.19; acc: 0.73
Batch: 460; loss: 1.22; acc: 0.58
Batch: 480; loss: 1.32; acc: 0.59
Batch: 500; loss: 1.1; acc: 0.72
Batch: 520; loss: 1.44; acc: 0.52
Batch: 540; loss: 1.28; acc: 0.66
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.22; acc: 0.66
Batch: 600; loss: 1.02; acc: 0.7
Batch: 620; loss: 1.18; acc: 0.66
Batch: 640; loss: 1.14; acc: 0.75
Batch: 660; loss: 1.19; acc: 0.72
Batch: 680; loss: 1.0; acc: 0.78
Batch: 700; loss: 1.17; acc: 0.7
Batch: 720; loss: 1.12; acc: 0.69
Batch: 740; loss: 1.19; acc: 0.67
Batch: 760; loss: 1.05; acc: 0.8
Batch: 780; loss: 1.22; acc: 0.64
Train Epoch over. train_loss: 1.18; train_accuracy: 0.68 

0.00015078860451467335
0.00014530375483445823
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.38; acc: 0.61
Batch: 40; loss: 0.79; acc: 0.86
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.83
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 0.89; acc: 0.77
Val Epoch over. val_loss: 1.1250509152746504; val_accuracy: 0.7067078025477707 

The current subspace-distance is: 0.00014530375483445823 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.72
Batch: 20; loss: 1.11; acc: 0.75
Batch: 40; loss: 1.17; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.02; acc: 0.78
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.26; acc: 0.62
Batch: 180; loss: 1.28; acc: 0.59
Batch: 200; loss: 1.07; acc: 0.72
Batch: 220; loss: 0.99; acc: 0.81
Batch: 240; loss: 1.11; acc: 0.69
Batch: 260; loss: 1.12; acc: 0.67
Batch: 280; loss: 1.07; acc: 0.72
Batch: 300; loss: 1.09; acc: 0.75
Batch: 320; loss: 0.99; acc: 0.78
Batch: 340; loss: 1.03; acc: 0.84
Batch: 360; loss: 1.33; acc: 0.62
Batch: 380; loss: 1.15; acc: 0.66
Batch: 400; loss: 1.09; acc: 0.67
Batch: 420; loss: 1.2; acc: 0.69
Batch: 440; loss: 1.27; acc: 0.67
Batch: 460; loss: 1.03; acc: 0.72
Batch: 480; loss: 1.08; acc: 0.69
Batch: 500; loss: 1.21; acc: 0.62
Batch: 520; loss: 1.17; acc: 0.7
Batch: 540; loss: 1.13; acc: 0.77
Batch: 560; loss: 1.12; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.72
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.1; acc: 0.8
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.15; acc: 0.69
Batch: 680; loss: 1.1; acc: 0.72
Batch: 700; loss: 1.28; acc: 0.58
Batch: 720; loss: 1.21; acc: 0.72
Batch: 740; loss: 1.27; acc: 0.62
Batch: 760; loss: 1.09; acc: 0.77
Batch: 780; loss: 1.26; acc: 0.7
Train Epoch over. train_loss: 1.18; train_accuracy: 0.68 

0.00015315588098019361
0.0001448926777811721
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.38; acc: 0.62
Batch: 40; loss: 0.78; acc: 0.88
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 0.87; acc: 0.77
Val Epoch over. val_loss: 1.1215729003499268; val_accuracy: 0.7108877388535032 

The current subspace-distance is: 0.0001448926777811721 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.73
Batch: 20; loss: 1.18; acc: 0.61
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 1.15; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.1; acc: 0.69
Batch: 140; loss: 1.23; acc: 0.62
Batch: 160; loss: 1.28; acc: 0.56
Batch: 180; loss: 1.16; acc: 0.7
Batch: 200; loss: 1.22; acc: 0.61
Batch: 220; loss: 1.13; acc: 0.66
Batch: 240; loss: 1.23; acc: 0.62
Batch: 260; loss: 1.18; acc: 0.73
Batch: 280; loss: 1.38; acc: 0.55
Batch: 300; loss: 1.16; acc: 0.73
Batch: 320; loss: 1.13; acc: 0.69
Batch: 340; loss: 1.34; acc: 0.62
Batch: 360; loss: 1.15; acc: 0.69
Batch: 380; loss: 1.24; acc: 0.64
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 1.2; acc: 0.66
Batch: 440; loss: 1.26; acc: 0.69
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.23; acc: 0.69
Batch: 500; loss: 1.17; acc: 0.77
Batch: 520; loss: 1.19; acc: 0.66
Batch: 540; loss: 1.18; acc: 0.73
Batch: 560; loss: 1.25; acc: 0.62
Batch: 580; loss: 1.21; acc: 0.59
Batch: 600; loss: 1.17; acc: 0.64
Batch: 620; loss: 1.06; acc: 0.75
Batch: 640; loss: 1.0; acc: 0.8
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 1.21; acc: 0.67
Batch: 700; loss: 1.16; acc: 0.7
Batch: 720; loss: 1.21; acc: 0.66
Batch: 740; loss: 1.03; acc: 0.69
Batch: 760; loss: 1.22; acc: 0.64
Batch: 780; loss: 1.12; acc: 0.7
Train Epoch over. train_loss: 1.17; train_accuracy: 0.68 

0.00015229932614602149
0.00014621940499637276
Batch: 0; loss: 1.21; acc: 0.67
Batch: 20; loss: 1.38; acc: 0.61
Batch: 40; loss: 0.78; acc: 0.91
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 0.96; acc: 0.81
Batch: 100; loss: 1.02; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.88; acc: 0.78
Val Epoch over. val_loss: 1.1186008912742518; val_accuracy: 0.7133757961783439 

The current subspace-distance is: 0.00014621940499637276 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.15; acc: 0.7
Batch: 20; loss: 1.17; acc: 0.73
Batch: 40; loss: 1.11; acc: 0.72
Batch: 60; loss: 1.18; acc: 0.69
Batch: 80; loss: 1.27; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.77
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.23; acc: 0.67
Batch: 160; loss: 1.1; acc: 0.7
Batch: 180; loss: 1.11; acc: 0.67
Batch: 200; loss: 1.1; acc: 0.75
Batch: 220; loss: 1.35; acc: 0.59
Batch: 240; loss: 1.09; acc: 0.73
Batch: 260; loss: 1.16; acc: 0.7
Batch: 280; loss: 1.24; acc: 0.66
Batch: 300; loss: 1.28; acc: 0.62
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.12; acc: 0.7
Batch: 360; loss: 1.18; acc: 0.7
Batch: 380; loss: 1.27; acc: 0.58
Batch: 400; loss: 1.22; acc: 0.66
Batch: 420; loss: 1.24; acc: 0.64
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.7
Batch: 480; loss: 1.37; acc: 0.58
Batch: 500; loss: 1.29; acc: 0.62
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.16; acc: 0.69
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.02; acc: 0.8
Batch: 600; loss: 1.08; acc: 0.72
Batch: 620; loss: 1.21; acc: 0.69
Batch: 640; loss: 1.31; acc: 0.62
Batch: 660; loss: 1.27; acc: 0.69
Batch: 680; loss: 1.21; acc: 0.7
Batch: 700; loss: 1.22; acc: 0.64
Batch: 720; loss: 1.19; acc: 0.7
Batch: 740; loss: 1.08; acc: 0.77
Batch: 760; loss: 1.13; acc: 0.73
Batch: 780; loss: 1.37; acc: 0.58
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.00015453180822078139
0.00014397219638340175
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 1.38; acc: 0.59
Batch: 40; loss: 0.77; acc: 0.88
Batch: 60; loss: 1.14; acc: 0.69
Batch: 80; loss: 0.94; acc: 0.83
Batch: 100; loss: 1.01; acc: 0.81
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.78
Val Epoch over. val_loss: 1.111125753943328; val_accuracy: 0.7100915605095541 

The current subspace-distance is: 0.00014397219638340175 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.7
Batch: 20; loss: 1.08; acc: 0.73
Batch: 40; loss: 1.18; acc: 0.67
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.22; acc: 0.61
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.18; acc: 0.66
Batch: 160; loss: 1.36; acc: 0.59
Batch: 180; loss: 1.01; acc: 0.77
Batch: 200; loss: 1.21; acc: 0.66
Batch: 220; loss: 1.24; acc: 0.67
Batch: 240; loss: 1.23; acc: 0.69
Batch: 260; loss: 1.09; acc: 0.69
Batch: 280; loss: 1.06; acc: 0.75
Batch: 300; loss: 1.23; acc: 0.67
Batch: 320; loss: 1.01; acc: 0.73
Batch: 340; loss: 1.32; acc: 0.64
Batch: 360; loss: 1.21; acc: 0.73
Batch: 380; loss: 1.05; acc: 0.77
Batch: 400; loss: 1.15; acc: 0.7
Batch: 420; loss: 1.32; acc: 0.67
Batch: 440; loss: 1.1; acc: 0.7
Batch: 460; loss: 1.09; acc: 0.67
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 1.15; acc: 0.75
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 1.13; acc: 0.66
Batch: 560; loss: 1.22; acc: 0.69
Batch: 580; loss: 1.08; acc: 0.75
Batch: 600; loss: 1.18; acc: 0.59
Batch: 620; loss: 1.22; acc: 0.64
Batch: 640; loss: 1.1; acc: 0.75
Batch: 660; loss: 1.36; acc: 0.58
Batch: 680; loss: 1.13; acc: 0.67
Batch: 700; loss: 1.25; acc: 0.67
Batch: 720; loss: 1.12; acc: 0.75
Batch: 740; loss: 1.17; acc: 0.67
Batch: 760; loss: 1.2; acc: 0.69
Batch: 780; loss: 1.2; acc: 0.64
Train Epoch over. train_loss: 1.17; train_accuracy: 0.68 

0.00015614283620379865
0.00015016307588666677
Batch: 0; loss: 1.2; acc: 0.7
Batch: 20; loss: 1.37; acc: 0.59
Batch: 40; loss: 0.77; acc: 0.89
Batch: 60; loss: 1.14; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.84
Batch: 100; loss: 1.02; acc: 0.81
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.86; acc: 0.78
Val Epoch over. val_loss: 1.1080757975578308; val_accuracy: 0.716062898089172 

The current subspace-distance is: 0.00015016307588666677 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.29; acc: 0.64
Batch: 40; loss: 1.17; acc: 0.64
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 1.17; acc: 0.62
Batch: 100; loss: 1.16; acc: 0.69
Batch: 120; loss: 1.26; acc: 0.62
Batch: 140; loss: 1.18; acc: 0.69
Batch: 160; loss: 1.09; acc: 0.72
Batch: 180; loss: 1.06; acc: 0.8
Batch: 200; loss: 1.13; acc: 0.66
Batch: 220; loss: 1.13; acc: 0.67
Batch: 240; loss: 1.12; acc: 0.66
Batch: 260; loss: 1.03; acc: 0.77
Batch: 280; loss: 1.19; acc: 0.64
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.08; acc: 0.78
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.18; acc: 0.64
Batch: 400; loss: 0.98; acc: 0.81
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 1.1; acc: 0.69
Batch: 460; loss: 1.24; acc: 0.61
Batch: 480; loss: 1.12; acc: 0.73
Batch: 500; loss: 1.25; acc: 0.64
Batch: 520; loss: 1.13; acc: 0.69
Batch: 540; loss: 1.2; acc: 0.62
Batch: 560; loss: 1.11; acc: 0.73
Batch: 580; loss: 1.02; acc: 0.78
Batch: 600; loss: 1.09; acc: 0.75
Batch: 620; loss: 1.13; acc: 0.73
Batch: 640; loss: 1.09; acc: 0.69
Batch: 660; loss: 1.28; acc: 0.66
Batch: 680; loss: 1.03; acc: 0.75
Batch: 700; loss: 1.19; acc: 0.67
Batch: 720; loss: 1.16; acc: 0.67
Batch: 740; loss: 1.15; acc: 0.69
Batch: 760; loss: 1.06; acc: 0.77
Batch: 780; loss: 1.31; acc: 0.64
Train Epoch over. train_loss: 1.17; train_accuracy: 0.68 

0.00015588542737532407
0.0001495533506385982
Batch: 0; loss: 1.21; acc: 0.7
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 0.77; acc: 0.88
Batch: 60; loss: 1.14; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.83
Batch: 100; loss: 1.01; acc: 0.81
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.86; acc: 0.77
Val Epoch over. val_loss: 1.1080421516849737; val_accuracy: 0.7158638535031847 

The current subspace-distance is: 0.0001495533506385982 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.72
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 1.04; acc: 0.81
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 1.25; acc: 0.62
Batch: 100; loss: 1.27; acc: 0.61
Batch: 120; loss: 1.06; acc: 0.72
Batch: 140; loss: 1.38; acc: 0.53
Batch: 160; loss: 1.21; acc: 0.7
Batch: 180; loss: 1.17; acc: 0.66
Batch: 200; loss: 1.02; acc: 0.77
Batch: 220; loss: 1.18; acc: 0.69
Batch: 240; loss: 1.36; acc: 0.62
Batch: 260; loss: 1.16; acc: 0.75
Batch: 280; loss: 1.13; acc: 0.62
Batch: 300; loss: 1.27; acc: 0.67
Batch: 320; loss: 1.17; acc: 0.64
Batch: 340; loss: 1.13; acc: 0.75
Batch: 360; loss: 1.14; acc: 0.69
Batch: 380; loss: 1.03; acc: 0.72
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.18; acc: 0.77
Batch: 440; loss: 1.23; acc: 0.66
Batch: 460; loss: 1.06; acc: 0.72
Batch: 480; loss: 1.28; acc: 0.69
Batch: 500; loss: 1.05; acc: 0.78
Batch: 520; loss: 1.14; acc: 0.64
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.11; acc: 0.73
Batch: 580; loss: 1.15; acc: 0.75
Batch: 600; loss: 1.1; acc: 0.69
Batch: 620; loss: 1.27; acc: 0.58
Batch: 640; loss: 1.07; acc: 0.7
Batch: 660; loss: 1.3; acc: 0.69
Batch: 680; loss: 1.06; acc: 0.73
Batch: 700; loss: 1.21; acc: 0.66
Batch: 720; loss: 1.02; acc: 0.7
Batch: 740; loss: 0.99; acc: 0.78
Batch: 760; loss: 1.33; acc: 0.66
Batch: 780; loss: 1.07; acc: 0.67
Train Epoch over. train_loss: 1.16; train_accuracy: 0.69 

0.00015511490346398205
0.00014732551062479615
Batch: 0; loss: 1.22; acc: 0.72
Batch: 20; loss: 1.36; acc: 0.61
Batch: 40; loss: 0.77; acc: 0.88
Batch: 60; loss: 1.14; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.83
Batch: 100; loss: 1.01; acc: 0.81
Batch: 120; loss: 1.29; acc: 0.59
Batch: 140; loss: 0.86; acc: 0.78
Val Epoch over. val_loss: 1.1077058744278683; val_accuracy: 0.7151671974522293 

The current subspace-distance is: 0.00014732551062479615 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_10_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.35

The number of parameters is: 266871

The number of individual parameters is:

11
198
11
11
17
33660
17
17
33
100980
33
33
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 53374195
elements in E: 53374200
fraction nonzero: 0.9999999063217809
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.17
Batch: 20; loss: 2.29; acc: 0.19
Batch: 40; loss: 2.06; acc: 0.31
Batch: 60; loss: 2.03; acc: 0.39
Batch: 80; loss: 2.04; acc: 0.36
Batch: 100; loss: 1.89; acc: 0.5
Batch: 120; loss: 1.86; acc: 0.38
Batch: 140; loss: 1.79; acc: 0.58
Batch: 160; loss: 1.83; acc: 0.52
Batch: 180; loss: 1.82; acc: 0.48
Batch: 200; loss: 1.73; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.56
Batch: 240; loss: 1.65; acc: 0.62
Batch: 260; loss: 1.71; acc: 0.53
Batch: 280; loss: 1.7; acc: 0.64
Batch: 300; loss: 1.63; acc: 0.52
Batch: 320; loss: 1.69; acc: 0.55
Batch: 340; loss: 1.69; acc: 0.67
Batch: 360; loss: 1.64; acc: 0.62
Batch: 380; loss: 1.57; acc: 0.67
Batch: 400; loss: 1.55; acc: 0.64
Batch: 420; loss: 1.54; acc: 0.62
Batch: 440; loss: 1.47; acc: 0.73
Batch: 460; loss: 1.54; acc: 0.64
Batch: 480; loss: 1.5; acc: 0.66
Batch: 500; loss: 1.64; acc: 0.58
Batch: 520; loss: 1.58; acc: 0.48
Batch: 540; loss: 1.59; acc: 0.61
Batch: 560; loss: 1.55; acc: 0.72
Batch: 580; loss: 1.44; acc: 0.66
Batch: 600; loss: 1.57; acc: 0.66
Batch: 620; loss: 1.47; acc: 0.69
Batch: 640; loss: 1.52; acc: 0.69
Batch: 660; loss: 1.46; acc: 0.72
Batch: 680; loss: 1.47; acc: 0.66
Batch: 700; loss: 1.45; acc: 0.62
Batch: 720; loss: 1.51; acc: 0.58
Batch: 740; loss: 1.5; acc: 0.61
Batch: 760; loss: 1.47; acc: 0.67
Batch: 780; loss: 1.43; acc: 0.7
Train Epoch over. train_loss: 1.67; train_accuracy: 0.56 

6.0624890465987846e-05
5.516492456081323e-05
Batch: 0; loss: 1.46; acc: 0.67
Batch: 20; loss: 1.58; acc: 0.62
Batch: 40; loss: 1.08; acc: 0.94
Batch: 60; loss: 1.35; acc: 0.83
Batch: 80; loss: 1.26; acc: 0.83
Batch: 100; loss: 1.46; acc: 0.7
Batch: 120; loss: 1.48; acc: 0.67
Batch: 140; loss: 1.27; acc: 0.81
Val Epoch over. val_loss: 1.3851997267668414; val_accuracy: 0.7296974522292994 

The current subspace-distance is: 5.516492456081323e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.5; acc: 0.66
Batch: 20; loss: 1.49; acc: 0.64
Batch: 40; loss: 1.55; acc: 0.52
Batch: 60; loss: 1.43; acc: 0.69
Batch: 80; loss: 1.37; acc: 0.69
Batch: 100; loss: 1.36; acc: 0.72
Batch: 120; loss: 1.31; acc: 0.77
Batch: 140; loss: 1.29; acc: 0.83
Batch: 160; loss: 1.32; acc: 0.72
Batch: 180; loss: 1.25; acc: 0.81
Batch: 200; loss: 1.38; acc: 0.7
Batch: 220; loss: 1.45; acc: 0.59
Batch: 240; loss: 1.41; acc: 0.67
Batch: 260; loss: 1.4; acc: 0.69
Batch: 280; loss: 1.23; acc: 0.75
Batch: 300; loss: 1.29; acc: 0.78
Batch: 320; loss: 1.31; acc: 0.75
Batch: 340; loss: 1.35; acc: 0.69
Batch: 360; loss: 1.29; acc: 0.73
Batch: 380; loss: 1.3; acc: 0.73
Batch: 400; loss: 1.4; acc: 0.64
Batch: 420; loss: 1.3; acc: 0.78
Batch: 440; loss: 1.27; acc: 0.7
Batch: 460; loss: 1.35; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.77
Batch: 500; loss: 1.23; acc: 0.77
Batch: 520; loss: 1.28; acc: 0.77
Batch: 540; loss: 1.19; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.7
Batch: 580; loss: 1.19; acc: 0.72
Batch: 600; loss: 1.22; acc: 0.7
Batch: 620; loss: 1.17; acc: 0.77
Batch: 640; loss: 1.15; acc: 0.77
Batch: 660; loss: 1.25; acc: 0.75
Batch: 680; loss: 1.2; acc: 0.73
Batch: 700; loss: 1.29; acc: 0.72
Batch: 720; loss: 1.24; acc: 0.73
Batch: 740; loss: 1.19; acc: 0.78
Batch: 760; loss: 1.22; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.83
Train Epoch over. train_loss: 1.29; train_accuracy: 0.73 

8.893731865100563e-05
8.503492426825687e-05
Batch: 0; loss: 1.17; acc: 0.72
Batch: 20; loss: 1.34; acc: 0.67
Batch: 40; loss: 0.85; acc: 0.92
Batch: 60; loss: 1.05; acc: 0.88
Batch: 80; loss: 1.04; acc: 0.86
Batch: 100; loss: 1.23; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.72
Batch: 140; loss: 0.99; acc: 0.89
Val Epoch over. val_loss: 1.1365954093872361; val_accuracy: 0.7830414012738853 

The current subspace-distance is: 8.503492426825687e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.64
Batch: 20; loss: 1.12; acc: 0.78
Batch: 40; loss: 1.24; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.84
Batch: 80; loss: 1.17; acc: 0.73
Batch: 100; loss: 1.07; acc: 0.81
Batch: 120; loss: 1.08; acc: 0.83
Batch: 140; loss: 1.17; acc: 0.78
Batch: 160; loss: 1.02; acc: 0.84
Batch: 180; loss: 1.16; acc: 0.72
Batch: 200; loss: 1.15; acc: 0.77
Batch: 220; loss: 1.2; acc: 0.72
Batch: 240; loss: 1.09; acc: 0.8
Batch: 260; loss: 1.01; acc: 0.84
Batch: 280; loss: 1.14; acc: 0.75
Batch: 300; loss: 1.14; acc: 0.75
Batch: 320; loss: 1.15; acc: 0.75
Batch: 340; loss: 1.27; acc: 0.72
Batch: 360; loss: 1.02; acc: 0.86
Batch: 380; loss: 1.13; acc: 0.81
Batch: 400; loss: 1.05; acc: 0.8
Batch: 420; loss: 1.1; acc: 0.8
Batch: 440; loss: 1.18; acc: 0.72
Batch: 460; loss: 1.02; acc: 0.77
Batch: 480; loss: 1.09; acc: 0.81
Batch: 500; loss: 1.06; acc: 0.81
Batch: 520; loss: 1.18; acc: 0.69
Batch: 540; loss: 1.26; acc: 0.66
Batch: 560; loss: 1.16; acc: 0.73
Batch: 580; loss: 1.13; acc: 0.78
Batch: 600; loss: 1.29; acc: 0.62
Batch: 620; loss: 1.09; acc: 0.73
Batch: 640; loss: 1.02; acc: 0.83
Batch: 660; loss: 1.04; acc: 0.77
Batch: 680; loss: 1.07; acc: 0.75
Batch: 700; loss: 1.08; acc: 0.73
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 1.04; acc: 0.8
Batch: 760; loss: 1.09; acc: 0.78
Batch: 780; loss: 1.07; acc: 0.83
Train Epoch over. train_loss: 1.11; train_accuracy: 0.77 

0.00010951320291496813
0.00010393836419098079
Batch: 0; loss: 0.96; acc: 0.84
Batch: 20; loss: 1.21; acc: 0.73
Batch: 40; loss: 0.72; acc: 0.92
Batch: 60; loss: 0.93; acc: 0.86
Batch: 80; loss: 0.86; acc: 0.91
Batch: 100; loss: 1.08; acc: 0.81
Batch: 120; loss: 1.16; acc: 0.73
Batch: 140; loss: 0.85; acc: 0.92
Val Epoch over. val_loss: 0.9904606425838106; val_accuracy: 0.8119028662420382 

The current subspace-distance is: 0.00010393836419098079 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.8
Batch: 20; loss: 1.03; acc: 0.83
Batch: 40; loss: 1.01; acc: 0.83
Batch: 60; loss: 1.0; acc: 0.77
Batch: 80; loss: 1.09; acc: 0.8
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 0.84; acc: 0.86
Batch: 140; loss: 1.07; acc: 0.72
Batch: 160; loss: 1.0; acc: 0.78
Batch: 180; loss: 1.03; acc: 0.75
Batch: 200; loss: 1.04; acc: 0.78
Batch: 220; loss: 0.94; acc: 0.84
Batch: 240; loss: 1.1; acc: 0.73
Batch: 260; loss: 1.1; acc: 0.73
Batch: 280; loss: 0.98; acc: 0.86
Batch: 300; loss: 1.04; acc: 0.72
Batch: 320; loss: 0.97; acc: 0.84
Batch: 340; loss: 0.95; acc: 0.8
Batch: 360; loss: 0.99; acc: 0.83
Batch: 380; loss: 0.93; acc: 0.8
Batch: 400; loss: 1.07; acc: 0.77
Batch: 420; loss: 1.08; acc: 0.77
Batch: 440; loss: 0.96; acc: 0.8
Batch: 460; loss: 0.91; acc: 0.86
Batch: 480; loss: 0.97; acc: 0.8
Batch: 500; loss: 0.91; acc: 0.84
Batch: 520; loss: 0.98; acc: 0.78
Batch: 540; loss: 0.96; acc: 0.78
Batch: 560; loss: 0.97; acc: 0.8
Batch: 580; loss: 1.0; acc: 0.78
Batch: 600; loss: 0.85; acc: 0.88
Batch: 620; loss: 0.95; acc: 0.77
Batch: 640; loss: 0.99; acc: 0.73
Batch: 660; loss: 0.93; acc: 0.84
Batch: 680; loss: 0.96; acc: 0.78
Batch: 700; loss: 1.08; acc: 0.78
Batch: 720; loss: 0.96; acc: 0.83
Batch: 740; loss: 0.96; acc: 0.73
Batch: 760; loss: 0.8; acc: 0.89
Batch: 780; loss: 0.99; acc: 0.81
Train Epoch over. train_loss: 1.0; train_accuracy: 0.79 

0.00012674200115725398
0.00012139896716689691
Batch: 0; loss: 0.83; acc: 0.89
Batch: 20; loss: 1.17; acc: 0.73
Batch: 40; loss: 0.63; acc: 0.94
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.75; acc: 0.92
Batch: 100; loss: 0.99; acc: 0.8
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.77; acc: 0.92
Val Epoch over. val_loss: 0.8962316277680124; val_accuracy: 0.8287221337579618 

The current subspace-distance is: 0.00012139896716689691 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.84
Batch: 20; loss: 1.0; acc: 0.77
Batch: 40; loss: 0.94; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 0.94; acc: 0.81
Batch: 100; loss: 0.96; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.88
Batch: 140; loss: 1.08; acc: 0.69
Batch: 160; loss: 1.0; acc: 0.75
Batch: 180; loss: 0.89; acc: 0.83
Batch: 200; loss: 0.83; acc: 0.88
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 0.91; acc: 0.81
Batch: 260; loss: 0.88; acc: 0.89
Batch: 280; loss: 0.95; acc: 0.78
Batch: 300; loss: 0.91; acc: 0.8
Batch: 320; loss: 0.9; acc: 0.83
Batch: 340; loss: 0.88; acc: 0.84
Batch: 360; loss: 0.98; acc: 0.78
Batch: 380; loss: 0.78; acc: 0.88
Batch: 400; loss: 0.95; acc: 0.81
Batch: 420; loss: 1.06; acc: 0.75
Batch: 440; loss: 0.69; acc: 0.92
Batch: 460; loss: 1.02; acc: 0.73
Batch: 480; loss: 0.87; acc: 0.83
Batch: 500; loss: 0.97; acc: 0.75
Batch: 520; loss: 0.8; acc: 0.86
Batch: 540; loss: 0.93; acc: 0.81
Batch: 560; loss: 0.89; acc: 0.88
Batch: 580; loss: 0.83; acc: 0.83
Batch: 600; loss: 0.93; acc: 0.84
Batch: 620; loss: 0.88; acc: 0.8
Batch: 640; loss: 0.87; acc: 0.8
Batch: 660; loss: 0.91; acc: 0.8
Batch: 680; loss: 0.84; acc: 0.88
Batch: 700; loss: 0.84; acc: 0.86
Batch: 720; loss: 0.86; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.91
Batch: 760; loss: 0.85; acc: 0.83
Batch: 780; loss: 0.81; acc: 0.84
Train Epoch over. train_loss: 0.91; train_accuracy: 0.81 

0.00014278416347224265
0.00013702758587896824
Batch: 0; loss: 0.75; acc: 0.91
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.8; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.91
Batch: 100; loss: 0.91; acc: 0.81
Batch: 120; loss: 1.02; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.94
Val Epoch over. val_loss: 0.8223940528881778; val_accuracy: 0.8385748407643312 

The current subspace-distance is: 0.00013702758587896824 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.84
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.93; acc: 0.8
Batch: 60; loss: 0.81; acc: 0.84
Batch: 80; loss: 0.87; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.91
Batch: 140; loss: 0.93; acc: 0.77
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.85; acc: 0.83
Batch: 200; loss: 0.86; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.88
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 0.99; acc: 0.72
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.86; acc: 0.81
Batch: 320; loss: 0.85; acc: 0.77
Batch: 340; loss: 0.87; acc: 0.83
Batch: 360; loss: 0.89; acc: 0.77
Batch: 380; loss: 0.83; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.91
Batch: 420; loss: 0.79; acc: 0.86
Batch: 440; loss: 0.86; acc: 0.8
Batch: 460; loss: 0.85; acc: 0.8
Batch: 480; loss: 0.92; acc: 0.77
Batch: 500; loss: 0.79; acc: 0.81
Batch: 520; loss: 1.05; acc: 0.7
Batch: 540; loss: 0.94; acc: 0.78
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.75; acc: 0.88
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.75; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.88
Batch: 660; loss: 0.82; acc: 0.84
Batch: 680; loss: 0.91; acc: 0.8
Batch: 700; loss: 0.75; acc: 0.91
Batch: 720; loss: 0.92; acc: 0.77
Batch: 740; loss: 0.84; acc: 0.88
Batch: 760; loss: 0.7; acc: 0.89
Batch: 780; loss: 0.93; acc: 0.8
Train Epoch over. train_loss: 0.86; train_accuracy: 0.81 

0.00015460772556252778
0.0001503418607171625
Batch: 0; loss: 0.72; acc: 0.88
Batch: 20; loss: 1.12; acc: 0.72
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.87; acc: 0.83
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.91
Val Epoch over. val_loss: 0.7836879708205059; val_accuracy: 0.8387738853503185 

The current subspace-distance is: 0.0001503418607171625 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.92; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.86; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.81
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 0.81; acc: 0.77
Batch: 180; loss: 1.01; acc: 0.83
Batch: 200; loss: 0.97; acc: 0.69
Batch: 220; loss: 0.79; acc: 0.86
Batch: 240; loss: 0.82; acc: 0.86
Batch: 260; loss: 0.75; acc: 0.84
Batch: 280; loss: 0.92; acc: 0.72
Batch: 300; loss: 0.99; acc: 0.69
Batch: 320; loss: 0.79; acc: 0.83
Batch: 340; loss: 0.69; acc: 0.88
Batch: 360; loss: 0.78; acc: 0.86
Batch: 380; loss: 0.7; acc: 0.92
Batch: 400; loss: 0.73; acc: 0.89
Batch: 420; loss: 0.9; acc: 0.8
Batch: 440; loss: 0.94; acc: 0.8
Batch: 460; loss: 0.92; acc: 0.8
Batch: 480; loss: 0.92; acc: 0.78
Batch: 500; loss: 0.87; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.91
Batch: 540; loss: 0.85; acc: 0.77
Batch: 560; loss: 0.78; acc: 0.83
Batch: 580; loss: 0.7; acc: 0.88
Batch: 600; loss: 0.82; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.73; acc: 0.88
Batch: 660; loss: 0.76; acc: 0.88
Batch: 680; loss: 0.85; acc: 0.75
Batch: 700; loss: 0.81; acc: 0.83
Batch: 720; loss: 0.83; acc: 0.78
Batch: 740; loss: 0.85; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.91
Batch: 780; loss: 0.87; acc: 0.81
Train Epoch over. train_loss: 0.81; train_accuracy: 0.82 

0.00016830256208777428
0.00016104693349916488
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 1.1; acc: 0.7
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.91
Batch: 100; loss: 0.8; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.64; acc: 0.88
Val Epoch over. val_loss: 0.7391388930712536; val_accuracy: 0.8436504777070064 

The current subspace-distance is: 0.00016104693349916488 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.8
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.91; acc: 0.78
Batch: 60; loss: 0.82; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.94
Batch: 100; loss: 0.86; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.86
Batch: 160; loss: 0.84; acc: 0.8
Batch: 180; loss: 0.64; acc: 0.89
Batch: 200; loss: 0.83; acc: 0.84
Batch: 220; loss: 0.83; acc: 0.81
Batch: 240; loss: 0.95; acc: 0.69
Batch: 260; loss: 0.83; acc: 0.81
Batch: 280; loss: 0.8; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.91
Batch: 340; loss: 0.83; acc: 0.81
Batch: 360; loss: 0.79; acc: 0.78
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.84
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 0.71; acc: 0.81
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.91
Batch: 520; loss: 0.75; acc: 0.84
Batch: 540; loss: 0.69; acc: 0.91
Batch: 560; loss: 0.8; acc: 0.83
Batch: 580; loss: 0.87; acc: 0.75
Batch: 600; loss: 0.92; acc: 0.77
Batch: 620; loss: 0.75; acc: 0.81
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.89
Batch: 680; loss: 0.69; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.89
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 0.77; train_accuracy: 0.83 

0.00017663807375356555
0.00017149785708170384
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 1.06; acc: 0.69
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.79; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.61; acc: 0.92
Val Epoch over. val_loss: 0.7040976880082659; val_accuracy: 0.851015127388535 

The current subspace-distance is: 0.00017149785708170384 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.73
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.76; acc: 0.83
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.66; acc: 0.89
Batch: 300; loss: 0.89; acc: 0.75
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.59; acc: 0.94
Batch: 380; loss: 0.73; acc: 0.8
Batch: 400; loss: 0.83; acc: 0.8
Batch: 420; loss: 0.88; acc: 0.72
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.66; acc: 0.88
Batch: 480; loss: 0.77; acc: 0.81
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.72; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 0.82; acc: 0.84
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.86; acc: 0.72
Batch: 620; loss: 0.73; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.89
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.65; acc: 0.94
Batch: 720; loss: 0.79; acc: 0.81
Batch: 740; loss: 0.82; acc: 0.77
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.83 

0.0001838750613387674
0.0001780431339284405
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.89
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.6720286613437021; val_accuracy: 0.8515127388535032 

The current subspace-distance is: 0.0001780431339284405 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.56; acc: 0.94
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.94
Batch: 160; loss: 0.65; acc: 0.86
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.69; acc: 0.91
Batch: 220; loss: 0.88; acc: 0.75
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.85; acc: 0.67
Batch: 280; loss: 0.77; acc: 0.8
Batch: 300; loss: 0.86; acc: 0.73
Batch: 320; loss: 0.66; acc: 0.86
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.77; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.79; acc: 0.78
Batch: 460; loss: 0.71; acc: 0.88
Batch: 480; loss: 0.83; acc: 0.8
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.61; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.95
Batch: 560; loss: 0.77; acc: 0.84
Batch: 580; loss: 0.82; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.94
Batch: 620; loss: 0.67; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.88
Batch: 680; loss: 0.78; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.73
Batch: 720; loss: 0.8; acc: 0.8
Batch: 740; loss: 0.83; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.00019199714006390423
0.0001844042126322165
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 1.0; acc: 0.7
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.6533491527958281; val_accuracy: 0.856687898089172 

The current subspace-distance is: 0.0001844042126322165 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 0.65; acc: 0.89
Batch: 40; loss: 0.79; acc: 0.78
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.77; acc: 0.77
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.86
Batch: 240; loss: 0.84; acc: 0.72
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.76; acc: 0.84
Batch: 340; loss: 0.89; acc: 0.77
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.8; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.73; acc: 0.75
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.79; acc: 0.83
Batch: 600; loss: 0.67; acc: 0.91
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.89; acc: 0.77
Batch: 680; loss: 0.7; acc: 0.89
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.86
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.63; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00019584878464229405
0.0001879548217402771
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.6466471391498663; val_accuracy: 0.8568869426751592 

The current subspace-distance is: 0.0001879548217402771 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.57; acc: 0.92
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.86; acc: 0.75
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.69; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.83; acc: 0.84
Batch: 360; loss: 0.82; acc: 0.83
Batch: 380; loss: 0.52; acc: 0.92
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 0.79; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.69
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.64; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.78; acc: 0.84
Batch: 540; loss: 0.78; acc: 0.83
Batch: 560; loss: 0.83; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.75; acc: 0.77
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.67; acc: 0.89
Batch: 700; loss: 0.66; acc: 0.88
Batch: 720; loss: 0.84; acc: 0.78
Batch: 740; loss: 0.78; acc: 0.8
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.7; acc: 0.88
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00019624968990683556
0.00018902626470662653
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.88
Val Epoch over. val_loss: 0.6453199868748902; val_accuracy: 0.8557921974522293 

The current subspace-distance is: 0.00018902626470662653 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.78
Batch: 80; loss: 0.89; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.72; acc: 0.81
Batch: 200; loss: 0.85; acc: 0.8
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.86
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.91
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.74; acc: 0.81
Batch: 360; loss: 0.68; acc: 0.83
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.91
Batch: 440; loss: 0.74; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.72; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.89
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.89; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.73
Batch: 600; loss: 0.72; acc: 0.81
Batch: 620; loss: 0.6; acc: 0.89
Batch: 640; loss: 0.83; acc: 0.77
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.77; acc: 0.77
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.75; acc: 0.77
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00019995849288534373
0.0001920205686474219
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.96; acc: 0.7
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.86
Val Epoch over. val_loss: 0.6449872727986354; val_accuracy: 0.8560907643312102 

The current subspace-distance is: 0.0001920205686474219 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.73; acc: 0.81
Batch: 180; loss: 0.62; acc: 0.91
Batch: 200; loss: 0.77; acc: 0.77
Batch: 220; loss: 0.79; acc: 0.77
Batch: 240; loss: 0.73; acc: 0.78
Batch: 260; loss: 0.81; acc: 0.77
Batch: 280; loss: 0.65; acc: 0.89
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.61; acc: 0.91
Batch: 380; loss: 0.78; acc: 0.8
Batch: 400; loss: 0.75; acc: 0.73
Batch: 420; loss: 0.81; acc: 0.83
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.72; acc: 0.77
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.79; acc: 0.81
Batch: 600; loss: 0.88; acc: 0.73
Batch: 620; loss: 0.75; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.76; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 0.66; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00020005766418762505
0.00019408225489314646
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.57; acc: 0.89
Val Epoch over. val_loss: 0.6332292733298746; val_accuracy: 0.8575835987261147 

The current subspace-distance is: 0.00019408225489314646 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.63; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.88
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.73; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.91
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.47; acc: 0.97
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.72; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.95
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.71; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.69; acc: 0.88
Batch: 600; loss: 0.81; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.91
Batch: 640; loss: 0.75; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.83; acc: 0.72
Batch: 700; loss: 0.59; acc: 0.92
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 0.79; acc: 0.8
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.0002025761641561985
0.0001969359436770901
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.88
Val Epoch over. val_loss: 0.632782073916903; val_accuracy: 0.8576831210191083 

The current subspace-distance is: 0.0001969359436770901 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.74; acc: 0.84
Batch: 160; loss: 1.01; acc: 0.73
Batch: 180; loss: 0.63; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.77; acc: 0.78
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.92
Batch: 300; loss: 0.69; acc: 0.78
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.72; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.91
Batch: 420; loss: 0.77; acc: 0.78
Batch: 440; loss: 0.78; acc: 0.84
Batch: 460; loss: 0.74; acc: 0.83
Batch: 480; loss: 0.65; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.89
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.8; acc: 0.78
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.65; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.74; acc: 0.77
Batch: 740; loss: 0.69; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00020181782019790262
0.00019333773525431752
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.88
Val Epoch over. val_loss: 0.6324224458758239; val_accuracy: 0.857484076433121 

The current subspace-distance is: 0.00019333773525431752 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.78
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.73; acc: 0.77
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.81; acc: 0.83
Batch: 200; loss: 0.86; acc: 0.81
Batch: 220; loss: 0.87; acc: 0.77
Batch: 240; loss: 0.69; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.88
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.77
Batch: 400; loss: 0.74; acc: 0.83
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.86; acc: 0.77
Batch: 560; loss: 0.75; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.86
Batch: 640; loss: 0.76; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.72; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00020774896256625652
0.00020206556655466557
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.54; acc: 0.89
Val Epoch over. val_loss: 0.6168965906094593; val_accuracy: 0.8603702229299363 

The current subspace-distance is: 0.00020206556655466557 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.75; acc: 0.77
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 0.83; acc: 0.81
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.78
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.82; acc: 0.78
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 0.57; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.86; acc: 0.77
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.59; acc: 0.91
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.79; acc: 0.75
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.75; acc: 0.78
Batch: 780; loss: 0.59; acc: 0.91
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00020674706320278347
0.00020019567455165088
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.86
Val Epoch over. val_loss: 0.6187439183141016; val_accuracy: 0.8607683121019108 

The current subspace-distance is: 0.00020019567455165088 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.72
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 0.74; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.74; acc: 0.77
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.88
Batch: 300; loss: 0.98; acc: 0.72
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.98; acc: 0.69
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.89
Batch: 500; loss: 1.0; acc: 0.61
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.86
Batch: 580; loss: 0.76; acc: 0.75
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.79; acc: 0.75
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.8
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00020962701819371432
0.00020134465012233704
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.88
Val Epoch over. val_loss: 0.6105271767658792; val_accuracy: 0.8583797770700637 

The current subspace-distance is: 0.00020134465012233704 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.58; acc: 0.89
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.81; acc: 0.77
Batch: 320; loss: 0.61; acc: 0.89
Batch: 340; loss: 0.77; acc: 0.77
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.72; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.91
Batch: 420; loss: 0.77; acc: 0.81
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.78
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.89
Batch: 680; loss: 0.77; acc: 0.84
Batch: 700; loss: 0.71; acc: 0.8
Batch: 720; loss: 0.73; acc: 0.8
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00021258083870634437
0.00020399085769895464
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.54; acc: 0.84
Val Epoch over. val_loss: 0.6031699936101391; val_accuracy: 0.8634554140127388 

The current subspace-distance is: 0.00020399085769895464 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.66; acc: 0.8
Batch: 160; loss: 0.83; acc: 0.77
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.94
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.78; acc: 0.78
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.82; acc: 0.83
Batch: 440; loss: 0.69; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.79; acc: 0.75
Batch: 560; loss: 0.73; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.92
Batch: 640; loss: 0.63; acc: 0.8
Batch: 660; loss: 0.68; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.91
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.71; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.79; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00021337528596632183
0.000201981354621239
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.6022471857678359; val_accuracy: 0.8605692675159236 

The current subspace-distance is: 0.000201981354621239 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.77; acc: 0.8
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 0.8; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.68; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.77
Batch: 420; loss: 0.68; acc: 0.8
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.91
Batch: 520; loss: 0.67; acc: 0.86
Batch: 540; loss: 0.92; acc: 0.72
Batch: 560; loss: 0.79; acc: 0.83
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.91; acc: 0.72
Batch: 620; loss: 0.62; acc: 0.91
Batch: 640; loss: 0.85; acc: 0.75
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.84
Batch: 720; loss: 0.77; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.88
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00021519622532650828
0.00020685738127212971
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.88
Val Epoch over. val_loss: 0.5980401544054602; val_accuracy: 0.8634554140127388 

The current subspace-distance is: 0.00020685738127212971 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.91
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.78; acc: 0.8
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.83; acc: 0.73
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.7; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.75; acc: 0.77
Batch: 620; loss: 0.68; acc: 0.83
Batch: 640; loss: 0.61; acc: 0.89
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.88
Batch: 720; loss: 0.79; acc: 0.77
Batch: 740; loss: 0.78; acc: 0.75
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.91
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.0002157604758394882
0.0002057343372143805
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.91
Val Epoch over. val_loss: 0.5997136248524781; val_accuracy: 0.8621616242038217 

The current subspace-distance is: 0.0002057343372143805 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.54; acc: 0.94
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.62; acc: 0.78
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.84; acc: 0.77
Batch: 380; loss: 0.74; acc: 0.78
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.7; acc: 0.86
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.67; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.78; acc: 0.78
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.75; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00021860786364413798
0.00021113725961185992
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.89
Val Epoch over. val_loss: 0.6019370104097257; val_accuracy: 0.861265923566879 

The current subspace-distance is: 0.00021113725961185992 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.91; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.87; acc: 0.77
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.72; acc: 0.88
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.78
Batch: 440; loss: 0.58; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.81; acc: 0.75
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.76; acc: 0.77
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.81
Batch: 660; loss: 0.76; acc: 0.78
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.74; acc: 0.81
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.83; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00021652065333910286
0.00020886554557364434
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.5932547653176982; val_accuracy: 0.8643511146496815 

The current subspace-distance is: 0.00020886554557364434 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.75; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.78; acc: 0.78
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.73; acc: 0.8
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.71; acc: 0.84
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.54; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.84; acc: 0.75
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.82; acc: 0.75
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00021267870033625513
0.00020611222134903073
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.5990375722669492; val_accuracy: 0.8632563694267515 

The current subspace-distance is: 0.00020611222134903073 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.81
Batch: 200; loss: 0.7; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.92
Batch: 260; loss: 0.68; acc: 0.81
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.74; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.63; acc: 0.84
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.91
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.89
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.88
Batch: 620; loss: 0.82; acc: 0.77
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.7; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.55; acc: 0.89
Batch: 760; loss: 0.63; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00021514002582989633
0.00020873422909062356
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.88; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.592274824335317; val_accuracy: 0.8645501592356688 

The current subspace-distance is: 0.00020873422909062356 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.66; acc: 0.89
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.85; acc: 0.8
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.73; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.73; acc: 0.78
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.83
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.92; acc: 0.78
Batch: 640; loss: 0.55; acc: 0.92
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.84
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00021987169748172164
0.00021248105622362345
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.5955028347908311; val_accuracy: 0.8622611464968153 

The current subspace-distance is: 0.00021248105622362345 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.94
Batch: 20; loss: 0.81; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.89
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.92
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.89
Batch: 440; loss: 0.77; acc: 0.83
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.75; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.81
Batch: 640; loss: 0.85; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.92
Batch: 700; loss: 0.66; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.83
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00021738438226748258
0.0002119976852554828
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.88; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.5982401346325115; val_accuracy: 0.8629578025477707 

The current subspace-distance is: 0.0002119976852554828 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.48; acc: 0.94
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.77; acc: 0.77
Batch: 380; loss: 0.62; acc: 0.88
Batch: 400; loss: 0.72; acc: 0.77
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.94
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.59; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.92
Batch: 660; loss: 0.63; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00021819326502736658
0.00021197339810896665
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.589586954018113; val_accuracy: 0.8647492038216561 

The current subspace-distance is: 0.00021197339810896665 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_10_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.35

The number of parameters is: 266871

The number of individual parameters is:

11
198
11
11
17
33660
17
17
33
100980
33
33
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 80061293
elements in E: 80061300
fraction nonzero: 0.9999999125669955
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.12
Batch: 20; loss: 2.19; acc: 0.25
Batch: 40; loss: 1.99; acc: 0.44
Batch: 60; loss: 1.89; acc: 0.47
Batch: 80; loss: 1.79; acc: 0.47
Batch: 100; loss: 1.71; acc: 0.53
Batch: 120; loss: 1.56; acc: 0.69
Batch: 140; loss: 1.56; acc: 0.7
Batch: 160; loss: 1.59; acc: 0.66
Batch: 180; loss: 1.49; acc: 0.72
Batch: 200; loss: 1.45; acc: 0.73
Batch: 220; loss: 1.53; acc: 0.61
Batch: 240; loss: 1.47; acc: 0.66
Batch: 260; loss: 1.48; acc: 0.75
Batch: 280; loss: 1.46; acc: 0.7
Batch: 300; loss: 1.54; acc: 0.64
Batch: 320; loss: 1.38; acc: 0.73
Batch: 340; loss: 1.31; acc: 0.73
Batch: 360; loss: 1.46; acc: 0.64
Batch: 380; loss: 1.35; acc: 0.67
Batch: 400; loss: 1.4; acc: 0.7
Batch: 420; loss: 1.32; acc: 0.72
Batch: 440; loss: 1.4; acc: 0.67
Batch: 460; loss: 1.3; acc: 0.77
Batch: 480; loss: 1.31; acc: 0.73
Batch: 500; loss: 1.34; acc: 0.72
Batch: 520; loss: 1.33; acc: 0.72
Batch: 540; loss: 1.35; acc: 0.69
Batch: 560; loss: 1.18; acc: 0.83
Batch: 580; loss: 1.34; acc: 0.66
Batch: 600; loss: 1.18; acc: 0.81
Batch: 620; loss: 1.21; acc: 0.75
Batch: 640; loss: 1.19; acc: 0.81
Batch: 660; loss: 1.41; acc: 0.62
Batch: 680; loss: 1.22; acc: 0.77
Batch: 700; loss: 1.15; acc: 0.77
Batch: 720; loss: 1.25; acc: 0.67
Batch: 740; loss: 1.19; acc: 0.73
Batch: 760; loss: 1.28; acc: 0.77
Batch: 780; loss: 1.26; acc: 0.73
Train Epoch over. train_loss: 1.44; train_accuracy: 0.67 

6.458551797550172e-05
5.9615002101054415e-05
Batch: 0; loss: 1.14; acc: 0.78
Batch: 20; loss: 1.27; acc: 0.62
Batch: 40; loss: 0.88; acc: 0.91
Batch: 60; loss: 1.04; acc: 0.77
Batch: 80; loss: 0.98; acc: 0.86
Batch: 100; loss: 1.06; acc: 0.81
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 1.08; acc: 0.83
Val Epoch over. val_loss: 1.0963935096552417; val_accuracy: 0.7970740445859873 

The current subspace-distance is: 5.9615002101054415e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.8
Batch: 20; loss: 1.09; acc: 0.84
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 1.14; acc: 0.77
Batch: 80; loss: 1.05; acc: 0.81
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.77
Batch: 140; loss: 1.22; acc: 0.73
Batch: 160; loss: 1.15; acc: 0.75
Batch: 180; loss: 1.12; acc: 0.78
Batch: 200; loss: 1.03; acc: 0.84
Batch: 220; loss: 1.03; acc: 0.88
Batch: 240; loss: 1.12; acc: 0.78
Batch: 260; loss: 1.06; acc: 0.83
Batch: 280; loss: 1.09; acc: 0.77
Batch: 300; loss: 0.89; acc: 0.91
Batch: 320; loss: 1.06; acc: 0.8
Batch: 340; loss: 1.03; acc: 0.8
Batch: 360; loss: 0.93; acc: 0.88
Batch: 380; loss: 1.03; acc: 0.77
Batch: 400; loss: 0.99; acc: 0.84
Batch: 420; loss: 1.17; acc: 0.7
Batch: 440; loss: 1.03; acc: 0.83
Batch: 460; loss: 1.09; acc: 0.72
Batch: 480; loss: 0.94; acc: 0.86
Batch: 500; loss: 0.95; acc: 0.83
Batch: 520; loss: 1.13; acc: 0.73
Batch: 540; loss: 1.03; acc: 0.77
Batch: 560; loss: 0.96; acc: 0.81
Batch: 580; loss: 1.15; acc: 0.75
Batch: 600; loss: 1.01; acc: 0.83
Batch: 620; loss: 1.14; acc: 0.73
Batch: 640; loss: 1.1; acc: 0.77
Batch: 660; loss: 1.07; acc: 0.72
Batch: 680; loss: 0.92; acc: 0.83
Batch: 700; loss: 1.05; acc: 0.8
Batch: 720; loss: 1.16; acc: 0.72
Batch: 740; loss: 1.02; acc: 0.8
Batch: 760; loss: 0.98; acc: 0.88
Batch: 780; loss: 1.06; acc: 0.78
Train Epoch over. train_loss: 1.07; train_accuracy: 0.79 

8.520732080796733e-05
8.004288247320801e-05
Batch: 0; loss: 0.98; acc: 0.83
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 0.7; acc: 0.91
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 0.77; acc: 0.91
Batch: 100; loss: 0.9; acc: 0.91
Batch: 120; loss: 1.08; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.86
Val Epoch over. val_loss: 0.923801954384822; val_accuracy: 0.8323049363057324 

The current subspace-distance is: 8.004288247320801e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.72
Batch: 40; loss: 1.04; acc: 0.78
Batch: 60; loss: 0.94; acc: 0.81
Batch: 80; loss: 0.98; acc: 0.81
Batch: 100; loss: 0.97; acc: 0.81
Batch: 120; loss: 1.03; acc: 0.75
Batch: 140; loss: 0.91; acc: 0.78
Batch: 160; loss: 1.03; acc: 0.77
Batch: 180; loss: 1.0; acc: 0.78
Batch: 200; loss: 0.93; acc: 0.78
Batch: 220; loss: 1.16; acc: 0.7
Batch: 240; loss: 1.1; acc: 0.73
Batch: 260; loss: 0.97; acc: 0.8
Batch: 280; loss: 1.07; acc: 0.78
Batch: 300; loss: 0.85; acc: 0.84
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 0.97; acc: 0.83
Batch: 360; loss: 0.87; acc: 0.81
Batch: 380; loss: 0.92; acc: 0.83
Batch: 400; loss: 0.99; acc: 0.8
Batch: 420; loss: 0.93; acc: 0.86
Batch: 440; loss: 0.88; acc: 0.84
Batch: 460; loss: 0.82; acc: 0.89
Batch: 480; loss: 0.87; acc: 0.8
Batch: 500; loss: 0.9; acc: 0.88
Batch: 520; loss: 0.82; acc: 0.86
Batch: 540; loss: 0.88; acc: 0.8
Batch: 560; loss: 0.94; acc: 0.81
Batch: 580; loss: 0.8; acc: 0.84
Batch: 600; loss: 0.87; acc: 0.86
Batch: 620; loss: 0.87; acc: 0.81
Batch: 640; loss: 0.96; acc: 0.8
Batch: 660; loss: 0.81; acc: 0.86
Batch: 680; loss: 1.0; acc: 0.8
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.9; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.84
Batch: 760; loss: 0.91; acc: 0.84
Batch: 780; loss: 0.78; acc: 0.89
Train Epoch over. train_loss: 0.93; train_accuracy: 0.82 

9.898099960992113e-05
9.420261631021276e-05
Batch: 0; loss: 0.9; acc: 0.8
Batch: 20; loss: 1.01; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.92
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.94
Batch: 100; loss: 0.82; acc: 0.89
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.91
Val Epoch over. val_loss: 0.821539363853491; val_accuracy: 0.8541003184713376 

The current subspace-distance is: 9.420261631021276e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.91
Batch: 20; loss: 0.93; acc: 0.84
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 0.92; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.91
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.88
Batch: 160; loss: 0.82; acc: 0.88
Batch: 180; loss: 0.87; acc: 0.84
Batch: 200; loss: 0.86; acc: 0.83
Batch: 220; loss: 0.78; acc: 0.88
Batch: 240; loss: 0.88; acc: 0.88
Batch: 260; loss: 0.85; acc: 0.86
Batch: 280; loss: 0.7; acc: 0.92
Batch: 300; loss: 0.95; acc: 0.84
Batch: 320; loss: 0.91; acc: 0.8
Batch: 340; loss: 0.85; acc: 0.86
Batch: 360; loss: 0.9; acc: 0.83
Batch: 380; loss: 0.85; acc: 0.83
Batch: 400; loss: 0.81; acc: 0.88
Batch: 420; loss: 0.89; acc: 0.81
Batch: 440; loss: 0.78; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.91
Batch: 480; loss: 0.9; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.89
Batch: 520; loss: 0.83; acc: 0.78
Batch: 540; loss: 0.85; acc: 0.83
Batch: 560; loss: 0.85; acc: 0.83
Batch: 580; loss: 0.88; acc: 0.81
Batch: 600; loss: 0.81; acc: 0.83
Batch: 620; loss: 0.82; acc: 0.8
Batch: 640; loss: 0.84; acc: 0.83
Batch: 660; loss: 0.88; acc: 0.81
Batch: 680; loss: 0.9; acc: 0.8
Batch: 700; loss: 0.76; acc: 0.81
Batch: 720; loss: 0.74; acc: 0.86
Batch: 740; loss: 0.9; acc: 0.83
Batch: 760; loss: 0.83; acc: 0.83
Batch: 780; loss: 0.79; acc: 0.84
Train Epoch over. train_loss: 0.84; train_accuracy: 0.84 

0.00011360198550391942
0.00010807114449562505
Batch: 0; loss: 0.84; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.81
Batch: 40; loss: 0.54; acc: 0.92
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.77; acc: 0.91
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.6; acc: 0.92
Val Epoch over. val_loss: 0.749748637531973; val_accuracy: 0.8656449044585988 

The current subspace-distance is: 0.00010807114449562505 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.88
Batch: 20; loss: 0.98; acc: 0.78
Batch: 40; loss: 0.72; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.86
Batch: 80; loss: 0.8; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.84
Batch: 140; loss: 0.8; acc: 0.88
Batch: 160; loss: 0.84; acc: 0.8
Batch: 180; loss: 0.75; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.77
Batch: 220; loss: 0.75; acc: 0.89
Batch: 240; loss: 0.93; acc: 0.8
Batch: 260; loss: 0.77; acc: 0.88
Batch: 280; loss: 0.84; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.89
Batch: 320; loss: 0.79; acc: 0.84
Batch: 340; loss: 0.71; acc: 0.92
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.72; acc: 0.95
Batch: 400; loss: 0.82; acc: 0.84
Batch: 420; loss: 0.8; acc: 0.84
Batch: 440; loss: 0.84; acc: 0.81
Batch: 460; loss: 0.67; acc: 0.91
Batch: 480; loss: 0.68; acc: 0.89
Batch: 500; loss: 0.92; acc: 0.75
Batch: 520; loss: 0.76; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.89
Batch: 560; loss: 0.91; acc: 0.8
Batch: 580; loss: 0.73; acc: 0.84
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 0.82; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.81
Batch: 660; loss: 0.86; acc: 0.81
Batch: 680; loss: 0.61; acc: 0.92
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 0.84; acc: 0.78
Batch: 740; loss: 0.78; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.89
Batch: 780; loss: 0.86; acc: 0.81
Train Epoch over. train_loss: 0.77; train_accuracy: 0.85 

0.00012511246313806623
0.00011969436309300363
Batch: 0; loss: 0.78; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.83
Batch: 40; loss: 0.48; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.6875546811872227; val_accuracy: 0.8778861464968153 

The current subspace-distance is: 0.00011969436309300363 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.92
Batch: 20; loss: 0.8; acc: 0.86
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.86
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.89; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.71; acc: 0.88
Batch: 160; loss: 0.77; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.95
Batch: 200; loss: 0.79; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.88
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.83
Batch: 300; loss: 0.82; acc: 0.81
Batch: 320; loss: 0.8; acc: 0.81
Batch: 340; loss: 0.73; acc: 0.81
Batch: 360; loss: 0.81; acc: 0.77
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.72; acc: 0.88
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.88
Batch: 460; loss: 0.72; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.88
Batch: 500; loss: 0.73; acc: 0.86
Batch: 520; loss: 0.82; acc: 0.83
Batch: 540; loss: 0.78; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.91
Batch: 580; loss: 0.74; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.91
Batch: 620; loss: 0.74; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.78; acc: 0.81
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.82; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.89
Batch: 760; loss: 0.69; acc: 0.89
Batch: 780; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.86 

0.00013670996122527868
0.00013125650002621114
Batch: 0; loss: 0.71; acc: 0.92
Batch: 20; loss: 0.78; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.94
Val Epoch over. val_loss: 0.6334182012612652; val_accuracy: 0.8823646496815286 

The current subspace-distance is: 0.00013125650002621114 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.79; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.91
Batch: 140; loss: 0.76; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.67; acc: 0.88
Batch: 220; loss: 0.85; acc: 0.78
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.88
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.61; acc: 0.94
Batch: 360; loss: 0.56; acc: 0.94
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.54; acc: 0.94
Batch: 440; loss: 0.67; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.94
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.91
Batch: 540; loss: 0.67; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.86
Batch: 620; loss: 0.63; acc: 0.91
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.63; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.84
Batch: 700; loss: 0.83; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.86
Batch: 740; loss: 0.68; acc: 0.88
Batch: 760; loss: 0.63; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.67; train_accuracy: 0.86 

0.00014784481027163565
0.0001411640550941229
Batch: 0; loss: 0.67; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.5903033160479965; val_accuracy: 0.884156050955414 

The current subspace-distance is: 0.0001411640550941229 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.91
Batch: 100; loss: 0.84; acc: 0.7
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.94
Batch: 160; loss: 0.66; acc: 0.89
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.69; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.92
Batch: 420; loss: 0.58; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.97
Batch: 500; loss: 0.64; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.89
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.63; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.92
Batch: 640; loss: 0.59; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.92
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.89
Train Epoch over. train_loss: 0.64; train_accuracy: 0.87 

0.00015628908295184374
0.0001488902053097263
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.94
Val Epoch over. val_loss: 0.567726725415819; val_accuracy: 0.8821656050955414 

The current subspace-distance is: 0.0001488902053097263 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.94
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.94
Batch: 420; loss: 0.58; acc: 0.91
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.8
Batch: 560; loss: 0.71; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.65; acc: 0.88
Batch: 640; loss: 0.66; acc: 0.89
Batch: 660; loss: 0.58; acc: 0.94
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.61; train_accuracy: 0.87 

0.00016525277169421315
0.00015791511395946145
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5488135934255685; val_accuracy: 0.887937898089172 

The current subspace-distance is: 0.00015791511395946145 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.7; acc: 0.88
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.63; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 0.71; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.94
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.94
Batch: 720; loss: 0.67; acc: 0.8
Batch: 740; loss: 0.48; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.59; train_accuracy: 0.87 

0.00017337777535431087
0.00016670767217874527
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.5221237048601649; val_accuracy: 0.8901273885350318 

The current subspace-distance is: 0.00016670767217874527 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.94
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.94
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.53; acc: 0.94
Batch: 480; loss: 0.59; acc: 0.91
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.56; acc: 0.88
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.67; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.0001748799259075895
0.00016713410150259733
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.5097071928962781; val_accuracy: 0.8908240445859873 

The current subspace-distance is: 0.00016713410150259733 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.69; acc: 0.8
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.66; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.63; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.6; acc: 0.89
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00017509970348328352
0.00016768604109529406
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.5074486770447652; val_accuracy: 0.8919187898089171 

The current subspace-distance is: 0.00016768604109529406 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.72; acc: 0.8
Batch: 220; loss: 0.42; acc: 0.97
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.94
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.91
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00018103713227901608
0.00017213656974490732
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.5043046719329373; val_accuracy: 0.8919187898089171 

The current subspace-distance is: 0.00017213656974490732 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.92
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.0001806594227673486
0.00017337684403173625
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.5000473988853442; val_accuracy: 0.8926154458598726 

The current subspace-distance is: 0.00017337684403173625 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00018341999384574592
0.00017590781499166042
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49279658486888667; val_accuracy: 0.8921178343949044 

The current subspace-distance is: 0.00017590781499166042 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.97
Batch: 220; loss: 0.67; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.94
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.54; acc: 0.94
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.64; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0001848218817031011
0.00017969409236684442
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4919101260839754; val_accuracy: 0.8954020700636943 

The current subspace-distance is: 0.00017969409236684442 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.97
Batch: 140; loss: 0.61; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.95
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.65; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.94
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00018663701484911144
0.00017831804871093482
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.69; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4904124622891663; val_accuracy: 0.8926154458598726 

The current subspace-distance is: 0.00017831804871093482 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.95
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.46; acc: 0.94
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.92
Batch: 500; loss: 0.54; acc: 0.92
Batch: 520; loss: 0.58; acc: 0.8
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.65; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.92
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00018809210450854152
0.00018051607185043395
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.47771664201074343; val_accuracy: 0.8976910828025477 

The current subspace-distance is: 0.00018051607185043395 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.74; acc: 0.77
Batch: 780; loss: 0.43; acc: 0.95
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00019017746672034264
0.00018245184037368745
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4778300146957871; val_accuracy: 0.8957006369426752 

The current subspace-distance is: 0.00018245184037368745 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.89
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.97
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.46; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00018942615133710206
0.00018148284289054573
Batch: 0; loss: 0.5; acc: 0.95
Batch: 20; loss: 0.66; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4730822610057843; val_accuracy: 0.895203025477707 

The current subspace-distance is: 0.00018148284289054573 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.78
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.95
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00019199626694899052
0.0001839923206716776
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.47642308407148737; val_accuracy: 0.8951035031847133 

The current subspace-distance is: 0.0001839923206716776 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.92
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.78; acc: 0.72
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.71; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.0001919027854455635
0.00018568756058812141
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4771195136627574; val_accuracy: 0.8957006369426752 

The current subspace-distance is: 0.00018568756058812141 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.65; acc: 0.78
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.65; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.78
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.81
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.6; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00019533056183718145
0.00018828269094228745
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.473891272001965; val_accuracy: 0.8958996815286624 

The current subspace-distance is: 0.00018828269094228745 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.97
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.78
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.6; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.8
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00019684733706526458
0.00018950135563500226
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.65; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.4706997204168587; val_accuracy: 0.8955015923566879 

The current subspace-distance is: 0.00018950135563500226 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.63; acc: 0.78
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.72; acc: 0.81
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.95
Batch: 420; loss: 0.62; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.95
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.52; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0001944751711562276
0.00018576740694697946
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.26; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.47741369931561173; val_accuracy: 0.8975915605095541 

The current subspace-distance is: 0.00018576740694697946 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.95
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.95
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.74; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.94
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00019674522627610713
0.00018915778491646051
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.47538029739431514; val_accuracy: 0.8920183121019108 

The current subspace-distance is: 0.00018915778491646051 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.92
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.97
Batch: 340; loss: 0.43; acc: 0.95
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.95
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.92
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.76; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00019540444191079587
0.00018743108375929296
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.25; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.46587847875561683; val_accuracy: 0.8988853503184714 

The current subspace-distance is: 0.00018743108375929296 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.95
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019646628061309457
0.00019046290253754705
Batch: 0; loss: 0.49; acc: 0.95
Batch: 20; loss: 0.67; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.47058584280074783; val_accuracy: 0.8958001592356688 

The current subspace-distance is: 0.00019046290253754705 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.95
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.92
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.77
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00019672793860081583
0.00018867352628149092
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.466555986244967; val_accuracy: 0.8987858280254777 

The current subspace-distance is: 0.00018867352628149092 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.8
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.78
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020006713748443872
0.00019195197091903538
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4628903743377916; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.00019195197091903538 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_10_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.35

The number of parameters is: 266871

The number of individual parameters is:

11
198
11
11
17
33660
17
17
33
100980
33
33
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 106748391
elements in E: 106748400
fraction nonzero: 0.9999999156896029
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.5; acc: 0.09
Batch: 20; loss: 2.12; acc: 0.28
Batch: 40; loss: 1.89; acc: 0.44
Batch: 60; loss: 1.78; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.67
Batch: 100; loss: 1.59; acc: 0.62
Batch: 120; loss: 1.61; acc: 0.59
Batch: 140; loss: 1.51; acc: 0.7
Batch: 160; loss: 1.42; acc: 0.73
Batch: 180; loss: 1.4; acc: 0.73
Batch: 200; loss: 1.35; acc: 0.73
Batch: 220; loss: 1.5; acc: 0.64
Batch: 240; loss: 1.29; acc: 0.78
Batch: 260; loss: 1.42; acc: 0.67
Batch: 280; loss: 1.3; acc: 0.73
Batch: 300; loss: 1.32; acc: 0.77
Batch: 320; loss: 1.13; acc: 0.81
Batch: 340; loss: 1.17; acc: 0.86
Batch: 360; loss: 1.16; acc: 0.84
Batch: 380; loss: 1.3; acc: 0.73
Batch: 400; loss: 1.15; acc: 0.83
Batch: 420; loss: 1.15; acc: 0.81
Batch: 440; loss: 1.1; acc: 0.83
Batch: 460; loss: 1.11; acc: 0.83
Batch: 480; loss: 1.2; acc: 0.8
Batch: 500; loss: 1.14; acc: 0.78
Batch: 520; loss: 1.13; acc: 0.8
Batch: 540; loss: 1.17; acc: 0.78
Batch: 560; loss: 1.18; acc: 0.75
Batch: 580; loss: 1.02; acc: 0.84
Batch: 600; loss: 1.12; acc: 0.83
Batch: 620; loss: 1.0; acc: 0.91
Batch: 640; loss: 1.14; acc: 0.8
Batch: 660; loss: 1.04; acc: 0.81
Batch: 680; loss: 0.95; acc: 0.89
Batch: 700; loss: 1.13; acc: 0.84
Batch: 720; loss: 0.95; acc: 0.81
Batch: 740; loss: 0.98; acc: 0.86
Batch: 760; loss: 0.97; acc: 0.88
Batch: 780; loss: 1.1; acc: 0.77
Train Epoch over. train_loss: 1.3; train_accuracy: 0.74 

2.637763464008458e-05
7.689667654631194e-06
Batch: 0; loss: 1.05; acc: 0.86
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 0.74; acc: 0.94
Batch: 60; loss: 0.98; acc: 0.86
Batch: 80; loss: 0.81; acc: 0.94
Batch: 100; loss: 0.98; acc: 0.84
Batch: 120; loss: 1.13; acc: 0.75
Batch: 140; loss: 0.85; acc: 0.91
Val Epoch over. val_loss: 0.9629240108143752; val_accuracy: 0.8627587579617835 

The current subspace-distance is: 7.689667654631194e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.86
Batch: 20; loss: 1.03; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.88
Batch: 60; loss: 1.03; acc: 0.78
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 1.08; acc: 0.77
Batch: 120; loss: 1.04; acc: 0.77
Batch: 140; loss: 1.12; acc: 0.75
Batch: 160; loss: 1.01; acc: 0.86
Batch: 180; loss: 0.93; acc: 0.91
Batch: 200; loss: 0.91; acc: 0.89
Batch: 220; loss: 0.91; acc: 0.88
Batch: 240; loss: 0.92; acc: 0.88
Batch: 260; loss: 0.9; acc: 0.88
Batch: 280; loss: 1.06; acc: 0.77
Batch: 300; loss: 0.8; acc: 0.91
Batch: 320; loss: 0.85; acc: 0.84
Batch: 340; loss: 0.85; acc: 0.89
Batch: 360; loss: 0.9; acc: 0.86
Batch: 380; loss: 0.89; acc: 0.84
Batch: 400; loss: 0.98; acc: 0.8
Batch: 420; loss: 0.99; acc: 0.78
Batch: 440; loss: 0.81; acc: 0.91
Batch: 460; loss: 0.9; acc: 0.88
Batch: 480; loss: 0.85; acc: 0.84
Batch: 500; loss: 0.98; acc: 0.81
Batch: 520; loss: 0.78; acc: 0.92
Batch: 540; loss: 0.81; acc: 0.83
Batch: 560; loss: 0.86; acc: 0.86
Batch: 580; loss: 0.83; acc: 0.8
Batch: 600; loss: 0.93; acc: 0.83
Batch: 620; loss: 0.78; acc: 0.91
Batch: 640; loss: 0.84; acc: 0.89
Batch: 660; loss: 0.82; acc: 0.84
Batch: 680; loss: 0.81; acc: 0.91
Batch: 700; loss: 0.81; acc: 0.88
Batch: 720; loss: 0.83; acc: 0.84
Batch: 740; loss: 0.83; acc: 0.83
Batch: 760; loss: 0.84; acc: 0.84
Batch: 780; loss: 0.89; acc: 0.88
Train Epoch over. train_loss: 0.91; train_accuracy: 0.85 

3.1809300708118826e-05
1.1943645404244307e-05
Batch: 0; loss: 0.8; acc: 0.92
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.55; acc: 0.97
Batch: 60; loss: 0.76; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.95
Batch: 100; loss: 0.77; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.94
Val Epoch over. val_loss: 0.7546252251430682; val_accuracy: 0.8866441082802548 

The current subspace-distance is: 1.1943645404244307e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 0.91; acc: 0.84
Batch: 60; loss: 0.85; acc: 0.86
Batch: 80; loss: 0.82; acc: 0.83
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.84
Batch: 140; loss: 0.84; acc: 0.88
Batch: 160; loss: 0.87; acc: 0.81
Batch: 180; loss: 0.77; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.89
Batch: 220; loss: 0.74; acc: 0.89
Batch: 240; loss: 0.86; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.89
Batch: 280; loss: 0.71; acc: 0.89
Batch: 300; loss: 0.9; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.83
Batch: 340; loss: 0.79; acc: 0.84
Batch: 360; loss: 0.76; acc: 0.91
Batch: 380; loss: 0.7; acc: 0.92
Batch: 400; loss: 0.77; acc: 0.91
Batch: 420; loss: 0.68; acc: 0.88
Batch: 440; loss: 0.75; acc: 0.86
Batch: 460; loss: 0.78; acc: 0.89
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.91
Batch: 540; loss: 0.68; acc: 0.86
Batch: 560; loss: 0.73; acc: 0.86
Batch: 580; loss: 0.82; acc: 0.8
Batch: 600; loss: 0.61; acc: 0.92
Batch: 620; loss: 0.84; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.88
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.65; acc: 0.91
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.85; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.91
Batch: 780; loss: 0.71; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.87 

3.681516682263464e-05
1.4683305380458478e-05
Batch: 0; loss: 0.66; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.98
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.94
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.54; acc: 0.94
Val Epoch over. val_loss: 0.6395900202025274; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 1.4683305380458478e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.91
Batch: 40; loss: 0.7; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.92
Batch: 80; loss: 0.64; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.67; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.91
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.73; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.98
Batch: 260; loss: 0.66; acc: 0.89
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.89
Batch: 320; loss: 0.76; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.92
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.59; acc: 0.91
Batch: 440; loss: 0.74; acc: 0.81
Batch: 460; loss: 0.67; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.92
Batch: 500; loss: 0.59; acc: 0.92
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.94
Batch: 560; loss: 0.67; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.73; acc: 0.81
Batch: 660; loss: 0.66; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.92
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.66; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.88 

4.056938996654935e-05
1.5254950994858518e-05
Batch: 0; loss: 0.56; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.43; acc: 0.97
Val Epoch over. val_loss: 0.5485011585958445; val_accuracy: 0.9061504777070064 

The current subspace-distance is: 1.5254950994858518e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.92
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.89
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.57; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.94
Batch: 280; loss: 0.53; acc: 0.94
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.95
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.92
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.94
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.94
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.95
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.89 

4.419821925694123e-05
1.8325006749364547e-05
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.487143157394069; val_accuracy: 0.9112261146496815 

The current subspace-distance is: 1.8325006749364547e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.98
Batch: 540; loss: 0.4; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

4.716027615359053e-05
1.905540375446435e-05
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4410596037176764; val_accuracy: 0.9163017515923567 

The current subspace-distance is: 1.905540375446435e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.94
Batch: 300; loss: 0.52; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.94
Batch: 500; loss: 0.61; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

5.0732578529277816e-05
2.240579851786606e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.4177354186014005; val_accuracy: 0.9185907643312102 

The current subspace-distance is: 2.240579851786606e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.95
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.39; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.97
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.295174923958257e-05
2.230139944003895e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.3930418192391183; val_accuracy: 0.9176950636942676 

The current subspace-distance is: 2.230139944003895e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.95
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.97
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.591084118350409e-05
2.454618334013503e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.3641097650975938; val_accuracy: 0.9197850318471338 

The current subspace-distance is: 2.454618334013503e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.97
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.97
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.95
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.729242548113689e-05
2.527347533032298e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3533205023616742; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.527347533032298e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.98
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.97
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.98
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.8148234529653564e-05
2.446510552545078e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3489952838155115; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.446510552545078e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.8874931710306555e-05
2.470485014782753e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.34149273185972956; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.470485014782753e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.98
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.893827255931683e-05
2.4982089598779567e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3427609187212719; val_accuracy: 0.9241640127388535 

The current subspace-distance is: 2.4982089598779567e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.8
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.98
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.002221562084742e-05
2.4575878342147917e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.3370772540379482; val_accuracy: 0.9258558917197452 

The current subspace-distance is: 2.4575878342147917e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.020845103194006e-05
2.575185862951912e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.3322504391525961; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 2.575185862951912e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.21; acc: 1.0
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.137274613138288e-05
2.6864152459893376e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3256804296734986; val_accuracy: 0.9253582802547771 

The current subspace-distance is: 2.6864152459893376e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.98
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.21; acc: 0.98
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.97
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.141283665783703e-05
2.6478866857360117e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.32224305676426857; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.6478866857360117e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.198751361807808e-05
2.6829549824469723e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3194247889480773; val_accuracy: 0.9261544585987261 

The current subspace-distance is: 2.6829549824469723e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.97
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.98
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.276253407122567e-05
2.708403735596221e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3164519921989198; val_accuracy: 0.9282444267515924 

The current subspace-distance is: 2.708403735596221e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.98
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.97
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.29003916401416e-05
2.7165568099007942e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.30846251841563327; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.7165568099007942e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.98
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.393293006112799e-05
2.8570966605911963e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3108458851173425; val_accuracy: 0.9287420382165605 

The current subspace-distance is: 2.8570966605911963e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.369401671690866e-05
2.8040663892170414e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3052375071747288; val_accuracy: 0.9297372611464968 

The current subspace-distance is: 2.8040663892170414e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.98
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.98
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.354118522722274e-05
2.710357148316689e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3088617297304664; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.710357148316689e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.35247488389723e-05
2.788314850477036e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3076100699651014; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.788314850477036e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.98
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.77
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.38488054391928e-05
2.6538829843048006e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3040221702710838; val_accuracy: 0.9287420382165605 

The current subspace-distance is: 2.6538829843048006e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.98
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.98
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.415278039639816e-05
2.812648926919792e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.30613375981901864; val_accuracy: 0.9294386942675159 

The current subspace-distance is: 2.812648926919792e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.444918108172715e-05
2.8065234801033512e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.30272530598245606; val_accuracy: 0.9280453821656051 

The current subspace-distance is: 2.8065234801033512e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.47975830361247e-05
2.846671486622654e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2999358500834483; val_accuracy: 0.9317277070063694 

The current subspace-distance is: 2.846671486622654e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.55; acc: 0.8
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.22; acc: 1.0
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.98
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.488035432994366e-05
2.812254024320282e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.30726034094573584; val_accuracy: 0.9287420382165605 

The current subspace-distance is: 2.812254024320282e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.42995655653067e-05
2.900898107327521e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2987002579934278; val_accuracy: 0.9299363057324841 

The current subspace-distance is: 2.900898107327521e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_10_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.35

The number of parameters is: 266871

The number of individual parameters is:

11
198
11
11
17
33660
17
17
33
100980
33
33
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 133435491
elements in E: 133435500
fraction nonzero: 0.9999999325516823
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.37; acc: 0.12
Batch: 20; loss: 2.02; acc: 0.41
Batch: 40; loss: 1.88; acc: 0.5
Batch: 60; loss: 1.78; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.53
Batch: 100; loss: 1.51; acc: 0.75
Batch: 120; loss: 1.37; acc: 0.77
Batch: 140; loss: 1.5; acc: 0.66
Batch: 160; loss: 1.34; acc: 0.73
Batch: 180; loss: 1.33; acc: 0.75
Batch: 200; loss: 1.32; acc: 0.78
Batch: 220; loss: 1.28; acc: 0.81
Batch: 240; loss: 1.19; acc: 0.86
Batch: 260; loss: 1.15; acc: 0.8
Batch: 280; loss: 1.17; acc: 0.84
Batch: 300; loss: 1.21; acc: 0.81
Batch: 320; loss: 1.16; acc: 0.86
Batch: 340; loss: 1.08; acc: 0.88
Batch: 360; loss: 1.09; acc: 0.8
Batch: 380; loss: 1.13; acc: 0.8
Batch: 400; loss: 1.23; acc: 0.72
Batch: 420; loss: 1.08; acc: 0.78
Batch: 440; loss: 1.03; acc: 0.84
Batch: 460; loss: 0.97; acc: 0.86
Batch: 480; loss: 1.08; acc: 0.77
Batch: 500; loss: 1.06; acc: 0.78
Batch: 520; loss: 0.99; acc: 0.84
Batch: 540; loss: 1.04; acc: 0.75
Batch: 560; loss: 1.0; acc: 0.81
Batch: 580; loss: 0.92; acc: 0.88
Batch: 600; loss: 1.0; acc: 0.88
Batch: 620; loss: 1.06; acc: 0.78
Batch: 640; loss: 0.9; acc: 0.88
Batch: 660; loss: 0.99; acc: 0.81
Batch: 680; loss: 0.9; acc: 0.83
Batch: 700; loss: 1.01; acc: 0.8
Batch: 720; loss: 0.85; acc: 0.88
Batch: 740; loss: 1.06; acc: 0.8
Batch: 760; loss: 0.91; acc: 0.8
Batch: 780; loss: 0.86; acc: 0.84
Train Epoch over. train_loss: 1.19; train_accuracy: 0.77 

2.6493658879189752e-05
9.1849342425121e-06
Batch: 0; loss: 0.91; acc: 0.86
Batch: 20; loss: 1.05; acc: 0.69
Batch: 40; loss: 0.55; acc: 0.98
Batch: 60; loss: 0.84; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.95
Batch: 100; loss: 0.86; acc: 0.89
Batch: 120; loss: 0.93; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.92
Val Epoch over. val_loss: 0.8288707269984446; val_accuracy: 0.8663415605095541 

The current subspace-distance is: 9.1849342425121e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.86
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.89
Batch: 80; loss: 0.92; acc: 0.84
Batch: 100; loss: 0.83; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.89
Batch: 140; loss: 0.83; acc: 0.84
Batch: 160; loss: 0.82; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.94
Batch: 200; loss: 0.78; acc: 0.86
Batch: 220; loss: 0.74; acc: 0.89
Batch: 240; loss: 0.73; acc: 0.89
Batch: 260; loss: 0.79; acc: 0.88
Batch: 280; loss: 0.77; acc: 0.86
Batch: 300; loss: 0.78; acc: 0.88
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.83
Batch: 360; loss: 0.75; acc: 0.88
Batch: 380; loss: 0.86; acc: 0.84
Batch: 400; loss: 0.7; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.89
Batch: 440; loss: 0.64; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.91
Batch: 480; loss: 0.77; acc: 0.84
Batch: 500; loss: 0.75; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.91
Batch: 540; loss: 0.7; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.95
Batch: 600; loss: 0.74; acc: 0.84
Batch: 620; loss: 0.76; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.92
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.74; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.56; acc: 0.95
Batch: 780; loss: 0.59; acc: 0.92
Train Epoch over. train_loss: 0.77; train_accuracy: 0.86 

3.261830715928227e-05
1.3104094250593334e-05
Batch: 0; loss: 0.67; acc: 0.94
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.95
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.618209891448355; val_accuracy: 0.8891321656050956 

The current subspace-distance is: 1.3104094250593334e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 0.76; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.92
Batch: 100; loss: 0.77; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.76; acc: 0.84
Batch: 200; loss: 0.68; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.92
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.94
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.92
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 0.56; acc: 0.95
Batch: 440; loss: 0.56; acc: 0.92
Batch: 460; loss: 0.65; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.94
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.95
Batch: 540; loss: 0.52; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.95
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.92
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.45; acc: 0.94
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.68; acc: 0.78
Train Epoch over. train_loss: 0.62; train_accuracy: 0.88 

3.7454294215422124e-05
1.5216528481687419e-05
Batch: 0; loss: 0.53; acc: 0.97
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5112887003998847; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 1.5216528481687419e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.92
Batch: 40; loss: 0.66; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.65; acc: 0.83
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.53; acc: 0.91
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.95
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.94
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.97
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

4.185151919955388e-05
1.801132930268068e-05
Batch: 0; loss: 0.44; acc: 0.98
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.44545109997129745; val_accuracy: 0.9158041401273885 

The current subspace-distance is: 1.801132930268068e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.92
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.95
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.61; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

4.456453461898491e-05
1.928933670569677e-05
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.3967144617419334; val_accuracy: 0.9211783439490446 

The current subspace-distance is: 1.928933670569677e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.55; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.97
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.97
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.95
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

4.741564407595433e-05
2.1687737898901105e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.35146795246441653; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 2.1687737898901105e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.81
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.97
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.98
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.0450187700334936e-05
2.2457170416601002e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3335991963079781; val_accuracy: 0.9270501592356688 

The current subspace-distance is: 2.2457170416601002e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.98
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.341507130651735e-05
2.4024760932661593e-05
Batch: 0; loss: 0.28; acc: 1.0
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3106550787379787; val_accuracy: 0.9304339171974523 

The current subspace-distance is: 2.4024760932661593e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.25; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.98
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.485194924403913e-05
2.36554114962928e-05
Batch: 0; loss: 0.26; acc: 1.0
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.29247394194648524; val_accuracy: 0.9328224522292994 

The current subspace-distance is: 2.36554114962928e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.98
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.98
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.6936372857308015e-05
2.61438235611422e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2782142390586009; val_accuracy: 0.9354100318471338 

The current subspace-distance is: 2.61438235611422e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.7831417507259175e-05
2.6827119654626586e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2767650774519914; val_accuracy: 0.9357085987261147 

The current subspace-distance is: 2.6827119654626586e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.98
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

5.832777969771996e-05
2.62525263678981e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.27423593354452946; val_accuracy: 0.9343152866242038 

The current subspace-distance is: 2.62525263678981e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.28; acc: 1.0
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.98
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.98
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.873483314644545e-05
2.68857620540075e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.26790420345629856; val_accuracy: 0.9360071656050956 

The current subspace-distance is: 2.68857620540075e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.944092481513508e-05
2.8284866857575253e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.26155324930408197; val_accuracy: 0.9370023885350318 

The current subspace-distance is: 2.8284866857575253e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.16; acc: 1.0
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.13; acc: 1.0
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.81
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.951941784587689e-05
2.793792009470053e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.25970244882213084; val_accuracy: 0.9373009554140127 

The current subspace-distance is: 2.793792009470053e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.16; acc: 1.0
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.996052277623676e-05
2.8353215384413488e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.254264404059975; val_accuracy: 0.9382961783439491 

The current subspace-distance is: 2.8353215384413488e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.084485721657984e-05
2.8491334887803532e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.25748326615163475; val_accuracy: 0.9377985668789809 

The current subspace-distance is: 2.8491334887803532e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.21; acc: 0.98
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.46; acc: 0.83
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.98
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.114802818046883e-05
2.794546162476763e-05
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.25178579335949225; val_accuracy: 0.9377985668789809 

The current subspace-distance is: 2.794546162476763e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.2; acc: 1.0
Batch: 160; loss: 0.18; acc: 1.0
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.98
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.13; acc: 1.0
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.98
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.238515925360844e-05
2.9658429411938414e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.25108405012803475; val_accuracy: 0.9382961783439491 

The current subspace-distance is: 2.9658429411938414e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.125766230979934e-05
2.83265981124714e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2489780304348393; val_accuracy: 0.9391918789808917 

The current subspace-distance is: 2.83265981124714e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.98
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.191650754772127e-05
2.886033325921744e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.24324953437420974; val_accuracy: 0.9389928343949044 

The current subspace-distance is: 2.886033325921744e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.19; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.98
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.81
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.26; acc: 0.97
Batch: 540; loss: 0.17; acc: 1.0
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.208274135133252e-05
2.7921701985178515e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2450867732334289; val_accuracy: 0.9403861464968153 

The current subspace-distance is: 2.7921701985178515e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.246213160920888e-05
2.832090831361711e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.24721818735265427; val_accuracy: 0.939390923566879 

The current subspace-distance is: 2.832090831361711e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.2; acc: 1.0
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.257007044041529e-05
2.901258267229423e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.24788210147125705; val_accuracy: 0.9389928343949044 

The current subspace-distance is: 2.901258267229423e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.86
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.359310646075755e-05
3.0814560886938125e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2468927301419009; val_accuracy: 0.9397890127388535 

The current subspace-distance is: 3.0814560886938125e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.247699639061466e-05
2.8654190828092396e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.24442143648103543; val_accuracy: 0.9382961783439491 

The current subspace-distance is: 2.8654190828092396e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.274010956985876e-05
2.8363941964926198e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.24556796162561245; val_accuracy: 0.9397890127388535 

The current subspace-distance is: 2.8363941964926198e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.304508860921487e-05
2.813980245264247e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.24413285589521858; val_accuracy: 0.939390923566879 

The current subspace-distance is: 2.813980245264247e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.19; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.18; acc: 1.0
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.18; acc: 1.0
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.16; acc: 1.0
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.98
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.271262827794999e-05
2.8921194825670682e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2430231521368786; val_accuracy: 0.9398885350318471 

The current subspace-distance is: 2.8921194825670682e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.30805516266264e-05
2.9412138246698305e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.23981464198630326; val_accuracy: 0.9414808917197452 

The current subspace-distance is: 2.9412138246698305e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_10_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:52/N_10_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
