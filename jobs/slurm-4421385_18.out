model : table13slim
N : 2
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:47

Channel scaling factor: 4.75

The number of parameters is: 266969

The number of individual parameters is:

38
380
38
38
57
43320
57
57
114
129960
114
114
64
87552
64
64
4096
64
640
10
64
64

nonzero elements in E: 13348449
elements in E: 13348450
fraction nonzero: 0.999999925084935
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.34; acc: 0.12
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.18; acc: 0.2
Batch: 60; loss: 2.23; acc: 0.25
Batch: 80; loss: 2.13; acc: 0.27
Batch: 100; loss: 2.11; acc: 0.3
Batch: 120; loss: 1.96; acc: 0.44
Batch: 140; loss: 2.08; acc: 0.27
Batch: 160; loss: 2.13; acc: 0.25
Batch: 180; loss: 2.1; acc: 0.3
Batch: 200; loss: 2.06; acc: 0.31
Batch: 220; loss: 2.07; acc: 0.34
Batch: 240; loss: 1.98; acc: 0.42
Batch: 260; loss: 1.91; acc: 0.45
Batch: 280; loss: 2.11; acc: 0.3
Batch: 300; loss: 2.04; acc: 0.42
Batch: 320; loss: 1.97; acc: 0.38
Batch: 340; loss: 1.99; acc: 0.38
Batch: 360; loss: 1.98; acc: 0.33
Batch: 380; loss: 1.95; acc: 0.33
Batch: 400; loss: 1.95; acc: 0.45
Batch: 420; loss: 2.02; acc: 0.41
Batch: 440; loss: 2.01; acc: 0.31
Batch: 460; loss: 1.9; acc: 0.41
Batch: 480; loss: 1.9; acc: 0.52
Batch: 500; loss: 2.02; acc: 0.31
Batch: 520; loss: 2.03; acc: 0.34
Batch: 540; loss: 2.1; acc: 0.28
Batch: 560; loss: 1.99; acc: 0.38
Batch: 580; loss: 1.99; acc: 0.38
Batch: 600; loss: 1.9; acc: 0.42
Batch: 620; loss: 2.01; acc: 0.31
Batch: 640; loss: 1.92; acc: 0.41
Batch: 660; loss: 1.86; acc: 0.48
Batch: 680; loss: 2.07; acc: 0.34
Batch: 700; loss: 1.94; acc: 0.38
Batch: 720; loss: 1.98; acc: 0.34
Batch: 740; loss: 1.86; acc: 0.45
Batch: 760; loss: 1.94; acc: 0.42
Batch: 780; loss: 1.88; acc: 0.48
Train Epoch over. train_loss: 2.02; train_accuracy: 0.35 

2.6128374884137884e-05
4.3769937292381655e-06
Batch: 0; loss: 1.98; acc: 0.33
Batch: 20; loss: 2.01; acc: 0.34
Batch: 40; loss: 1.78; acc: 0.59
Batch: 60; loss: 1.96; acc: 0.34
Batch: 80; loss: 1.89; acc: 0.48
Batch: 100; loss: 1.91; acc: 0.41
Batch: 120; loss: 1.86; acc: 0.48
Batch: 140; loss: 1.92; acc: 0.44
Val Epoch over. val_loss: 1.9181103478571413; val_accuracy: 0.4293391719745223 

The current subspace-distance is: 4.3769937292381655e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.95; acc: 0.38
Batch: 20; loss: 1.9; acc: 0.42
Batch: 40; loss: 1.84; acc: 0.53
Batch: 60; loss: 1.84; acc: 0.53
Batch: 80; loss: 1.85; acc: 0.44
Batch: 100; loss: 1.99; acc: 0.45
Batch: 120; loss: 1.81; acc: 0.53
Batch: 140; loss: 1.85; acc: 0.48
Batch: 160; loss: 1.95; acc: 0.42
Batch: 180; loss: 2.03; acc: 0.34
Batch: 200; loss: 1.93; acc: 0.33
Batch: 220; loss: 1.92; acc: 0.44
Batch: 240; loss: 1.86; acc: 0.44
Batch: 260; loss: 1.95; acc: 0.44
Batch: 280; loss: 1.88; acc: 0.42
Batch: 300; loss: 1.95; acc: 0.38
Batch: 320; loss: 1.85; acc: 0.44
Batch: 340; loss: 1.85; acc: 0.48
Batch: 360; loss: 1.94; acc: 0.41
Batch: 380; loss: 1.86; acc: 0.39
Batch: 400; loss: 1.94; acc: 0.36
Batch: 420; loss: 1.96; acc: 0.36
Batch: 440; loss: 1.94; acc: 0.39
Batch: 460; loss: 1.92; acc: 0.45
Batch: 480; loss: 1.81; acc: 0.52
Batch: 500; loss: 1.9; acc: 0.41
Batch: 520; loss: 1.88; acc: 0.47
Batch: 540; loss: 1.94; acc: 0.38
Batch: 560; loss: 1.99; acc: 0.38
Batch: 580; loss: 1.98; acc: 0.39
Batch: 600; loss: 1.87; acc: 0.42
Batch: 620; loss: 1.95; acc: 0.38
Batch: 640; loss: 1.84; acc: 0.41
Batch: 660; loss: 1.85; acc: 0.44
Batch: 680; loss: 1.88; acc: 0.45
Batch: 700; loss: 1.87; acc: 0.45
Batch: 720; loss: 1.82; acc: 0.48
Batch: 740; loss: 1.91; acc: 0.38
Batch: 760; loss: 1.92; acc: 0.45
Batch: 780; loss: 1.82; acc: 0.52
Train Epoch over. train_loss: 1.9; train_accuracy: 0.43 

2.7947766284341924e-05
7.062697477522306e-06
Batch: 0; loss: 1.9; acc: 0.41
Batch: 20; loss: 1.94; acc: 0.41
Batch: 40; loss: 1.78; acc: 0.55
Batch: 60; loss: 1.92; acc: 0.36
Batch: 80; loss: 1.83; acc: 0.44
Batch: 100; loss: 1.87; acc: 0.47
Batch: 120; loss: 1.83; acc: 0.47
Batch: 140; loss: 1.84; acc: 0.47
Val Epoch over. val_loss: 1.8525616332983514; val_accuracy: 0.46894904458598724 

The current subspace-distance is: 7.062697477522306e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.85; acc: 0.52
Batch: 20; loss: 1.85; acc: 0.45
Batch: 40; loss: 1.84; acc: 0.45
Batch: 60; loss: 1.85; acc: 0.44
Batch: 80; loss: 1.92; acc: 0.36
Batch: 100; loss: 1.88; acc: 0.44
Batch: 120; loss: 1.98; acc: 0.33
Batch: 140; loss: 1.76; acc: 0.61
Batch: 160; loss: 1.97; acc: 0.33
Batch: 180; loss: 1.97; acc: 0.44
Batch: 200; loss: 1.73; acc: 0.61
Batch: 220; loss: 1.85; acc: 0.47
Batch: 240; loss: 1.89; acc: 0.45
Batch: 260; loss: 1.83; acc: 0.5
Batch: 280; loss: 1.74; acc: 0.56
Batch: 300; loss: 1.88; acc: 0.41
Batch: 320; loss: 1.76; acc: 0.5
Batch: 340; loss: 1.85; acc: 0.47
Batch: 360; loss: 1.77; acc: 0.59
Batch: 380; loss: 1.79; acc: 0.5
Batch: 400; loss: 1.81; acc: 0.45
Batch: 420; loss: 1.76; acc: 0.58
Batch: 440; loss: 1.81; acc: 0.53
Batch: 460; loss: 1.73; acc: 0.56
Batch: 480; loss: 1.92; acc: 0.3
Batch: 500; loss: 1.86; acc: 0.47
Batch: 520; loss: 1.89; acc: 0.38
Batch: 540; loss: 1.91; acc: 0.41
Batch: 560; loss: 1.71; acc: 0.58
Batch: 580; loss: 1.77; acc: 0.48
Batch: 600; loss: 1.89; acc: 0.33
Batch: 620; loss: 1.79; acc: 0.56
Batch: 640; loss: 1.78; acc: 0.56
Batch: 660; loss: 1.86; acc: 0.47
Batch: 680; loss: 1.89; acc: 0.42
Batch: 700; loss: 1.86; acc: 0.42
Batch: 720; loss: 1.81; acc: 0.45
Batch: 740; loss: 1.88; acc: 0.41
Batch: 760; loss: 1.77; acc: 0.53
Batch: 780; loss: 1.89; acc: 0.39
Train Epoch over. train_loss: 1.84; train_accuracy: 0.46 

2.9157914468669333e-05
6.047459010005696e-06
Batch: 0; loss: 1.87; acc: 0.38
Batch: 20; loss: 1.89; acc: 0.45
Batch: 40; loss: 1.74; acc: 0.61
Batch: 60; loss: 1.84; acc: 0.38
Batch: 80; loss: 1.78; acc: 0.5
Batch: 100; loss: 1.85; acc: 0.42
Batch: 120; loss: 1.82; acc: 0.44
Batch: 140; loss: 1.75; acc: 0.56
Val Epoch over. val_loss: 1.805153316752926; val_accuracy: 0.4749203821656051 

The current subspace-distance is: 6.047459010005696e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.7; acc: 0.55
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.74; acc: 0.48
Batch: 60; loss: 1.74; acc: 0.58
Batch: 80; loss: 1.82; acc: 0.45
Batch: 100; loss: 1.82; acc: 0.5
Batch: 120; loss: 1.88; acc: 0.38
Batch: 140; loss: 1.77; acc: 0.41
Batch: 160; loss: 1.76; acc: 0.52
Batch: 180; loss: 1.75; acc: 0.5
Batch: 200; loss: 1.74; acc: 0.48
Batch: 220; loss: 1.7; acc: 0.64
Batch: 240; loss: 1.82; acc: 0.5
Batch: 260; loss: 1.77; acc: 0.48
Batch: 280; loss: 1.85; acc: 0.44
Batch: 300; loss: 1.88; acc: 0.45
Batch: 320; loss: 1.83; acc: 0.47
Batch: 340; loss: 1.89; acc: 0.33
Batch: 360; loss: 1.68; acc: 0.55
Batch: 380; loss: 1.85; acc: 0.41
Batch: 400; loss: 1.74; acc: 0.58
Batch: 420; loss: 1.73; acc: 0.53
Batch: 440; loss: 1.84; acc: 0.39
Batch: 460; loss: 1.79; acc: 0.45
Batch: 480; loss: 1.81; acc: 0.45
Batch: 500; loss: 1.78; acc: 0.45
Batch: 520; loss: 1.74; acc: 0.5
Batch: 540; loss: 1.82; acc: 0.39
Batch: 560; loss: 1.77; acc: 0.48
Batch: 580; loss: 1.7; acc: 0.55
Batch: 600; loss: 1.86; acc: 0.38
Batch: 620; loss: 1.82; acc: 0.34
Batch: 640; loss: 1.66; acc: 0.47
Batch: 660; loss: 1.8; acc: 0.47
Batch: 680; loss: 2.0; acc: 0.3
Batch: 700; loss: 1.7; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.53
Batch: 740; loss: 1.74; acc: 0.42
Batch: 760; loss: 1.7; acc: 0.53
Batch: 780; loss: 1.66; acc: 0.56
Train Epoch over. train_loss: 1.79; train_accuracy: 0.48 

3.103975541307591e-05
1.0400383871456143e-05
Batch: 0; loss: 1.86; acc: 0.34
Batch: 20; loss: 1.8; acc: 0.42
Batch: 40; loss: 1.66; acc: 0.58
Batch: 60; loss: 1.75; acc: 0.45
Batch: 80; loss: 1.68; acc: 0.55
Batch: 100; loss: 1.78; acc: 0.45
Batch: 120; loss: 1.8; acc: 0.41
Batch: 140; loss: 1.7; acc: 0.55
Val Epoch over. val_loss: 1.7612274885177612; val_accuracy: 0.4793988853503185 

The current subspace-distance is: 1.0400383871456143e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.56
Batch: 20; loss: 1.71; acc: 0.55
Batch: 40; loss: 1.81; acc: 0.39
Batch: 60; loss: 1.72; acc: 0.5
Batch: 80; loss: 1.74; acc: 0.5
Batch: 100; loss: 1.86; acc: 0.42
Batch: 120; loss: 1.82; acc: 0.41
Batch: 140; loss: 1.7; acc: 0.52
Batch: 160; loss: 1.73; acc: 0.53
Batch: 180; loss: 1.71; acc: 0.61
Batch: 200; loss: 1.74; acc: 0.52
Batch: 220; loss: 1.87; acc: 0.33
Batch: 240; loss: 1.82; acc: 0.42
Batch: 260; loss: 1.67; acc: 0.53
Batch: 280; loss: 1.68; acc: 0.52
Batch: 300; loss: 1.73; acc: 0.47
Batch: 320; loss: 1.7; acc: 0.53
Batch: 340; loss: 1.77; acc: 0.5
Batch: 360; loss: 1.74; acc: 0.48
Batch: 380; loss: 1.82; acc: 0.42
Batch: 400; loss: 1.74; acc: 0.55
Batch: 420; loss: 1.76; acc: 0.52
Batch: 440; loss: 1.83; acc: 0.39
Batch: 460; loss: 1.64; acc: 0.58
Batch: 480; loss: 1.68; acc: 0.59
Batch: 500; loss: 1.86; acc: 0.44
Batch: 520; loss: 1.88; acc: 0.39
Batch: 540; loss: 1.67; acc: 0.55
Batch: 560; loss: 1.57; acc: 0.61
Batch: 580; loss: 1.77; acc: 0.55
Batch: 600; loss: 1.74; acc: 0.52
Batch: 620; loss: 1.68; acc: 0.58
Batch: 640; loss: 1.76; acc: 0.5
Batch: 660; loss: 1.79; acc: 0.45
Batch: 680; loss: 1.82; acc: 0.39
Batch: 700; loss: 1.76; acc: 0.5
Batch: 720; loss: 1.85; acc: 0.42
Batch: 740; loss: 1.75; acc: 0.36
Batch: 760; loss: 1.77; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.39
Train Epoch over. train_loss: 1.75; train_accuracy: 0.47 

3.2879095670068637e-05
8.86094494489953e-06
Batch: 0; loss: 1.84; acc: 0.41
Batch: 20; loss: 1.77; acc: 0.48
Batch: 40; loss: 1.61; acc: 0.56
Batch: 60; loss: 1.69; acc: 0.47
Batch: 80; loss: 1.64; acc: 0.5
Batch: 100; loss: 1.74; acc: 0.44
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.66; acc: 0.52
Val Epoch over. val_loss: 1.7345066116114332; val_accuracy: 0.48318073248407645 

The current subspace-distance is: 8.86094494489953e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.64; acc: 0.62
Batch: 40; loss: 1.79; acc: 0.5
Batch: 60; loss: 1.8; acc: 0.44
Batch: 80; loss: 1.77; acc: 0.44
Batch: 100; loss: 1.89; acc: 0.34
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.6; acc: 0.58
Batch: 160; loss: 1.8; acc: 0.44
Batch: 180; loss: 1.68; acc: 0.58
Batch: 200; loss: 1.7; acc: 0.5
Batch: 220; loss: 1.83; acc: 0.39
Batch: 240; loss: 1.77; acc: 0.45
Batch: 260; loss: 1.83; acc: 0.39
Batch: 280; loss: 1.71; acc: 0.48
Batch: 300; loss: 1.73; acc: 0.47
Batch: 320; loss: 1.83; acc: 0.41
Batch: 340; loss: 1.73; acc: 0.47
Batch: 360; loss: 1.86; acc: 0.33
Batch: 380; loss: 1.73; acc: 0.44
Batch: 400; loss: 1.87; acc: 0.42
Batch: 420; loss: 1.71; acc: 0.44
Batch: 440; loss: 1.8; acc: 0.42
Batch: 460; loss: 1.8; acc: 0.39
Batch: 480; loss: 1.66; acc: 0.52
Batch: 500; loss: 1.76; acc: 0.5
Batch: 520; loss: 1.69; acc: 0.5
Batch: 540; loss: 1.82; acc: 0.38
Batch: 560; loss: 1.66; acc: 0.48
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.75; acc: 0.47
Batch: 620; loss: 1.64; acc: 0.58
Batch: 640; loss: 1.62; acc: 0.58
Batch: 660; loss: 1.72; acc: 0.47
Batch: 680; loss: 1.64; acc: 0.55
Batch: 700; loss: 1.77; acc: 0.44
Batch: 720; loss: 1.64; acc: 0.55
Batch: 740; loss: 1.61; acc: 0.53
Batch: 760; loss: 1.72; acc: 0.48
Batch: 780; loss: 1.74; acc: 0.45
Train Epoch over. train_loss: 1.72; train_accuracy: 0.49 

3.479333463474177e-05
1.282516859646421e-05
Batch: 0; loss: 1.83; acc: 0.34
Batch: 20; loss: 1.76; acc: 0.44
Batch: 40; loss: 1.55; acc: 0.58
Batch: 60; loss: 1.64; acc: 0.59
Batch: 80; loss: 1.61; acc: 0.52
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.75; acc: 0.5
Batch: 140; loss: 1.63; acc: 0.5
Val Epoch over. val_loss: 1.7114829113528987; val_accuracy: 0.5119426751592356 

The current subspace-distance is: 1.282516859646421e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.74; acc: 0.5
Batch: 40; loss: 1.76; acc: 0.41
Batch: 60; loss: 1.72; acc: 0.45
Batch: 80; loss: 1.79; acc: 0.45
Batch: 100; loss: 1.8; acc: 0.45
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.71; acc: 0.42
Batch: 160; loss: 1.66; acc: 0.61
Batch: 180; loss: 1.66; acc: 0.5
Batch: 200; loss: 1.72; acc: 0.45
Batch: 220; loss: 1.83; acc: 0.34
Batch: 240; loss: 1.62; acc: 0.53
Batch: 260; loss: 1.68; acc: 0.48
Batch: 280; loss: 1.66; acc: 0.58
Batch: 300; loss: 1.69; acc: 0.48
Batch: 320; loss: 1.74; acc: 0.47
Batch: 340; loss: 1.61; acc: 0.58
Batch: 360; loss: 1.84; acc: 0.42
Batch: 380; loss: 1.71; acc: 0.53
Batch: 400; loss: 1.8; acc: 0.48
Batch: 420; loss: 1.6; acc: 0.56
Batch: 440; loss: 1.68; acc: 0.53
Batch: 460; loss: 1.77; acc: 0.39
Batch: 480; loss: 1.77; acc: 0.45
Batch: 500; loss: 1.68; acc: 0.48
Batch: 520; loss: 1.67; acc: 0.5
Batch: 540; loss: 1.73; acc: 0.44
Batch: 560; loss: 1.72; acc: 0.48
Batch: 580; loss: 1.57; acc: 0.59
Batch: 600; loss: 1.64; acc: 0.53
Batch: 620; loss: 1.75; acc: 0.48
Batch: 640; loss: 1.7; acc: 0.53
Batch: 660; loss: 1.62; acc: 0.5
Batch: 680; loss: 1.61; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.58
Batch: 720; loss: 1.72; acc: 0.45
Batch: 740; loss: 1.74; acc: 0.38
Batch: 760; loss: 1.72; acc: 0.48
Batch: 780; loss: 1.6; acc: 0.59
Train Epoch over. train_loss: 1.69; train_accuracy: 0.51 

3.566295708878897e-05
9.707686331239529e-06
Batch: 0; loss: 1.78; acc: 0.44
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.44; acc: 0.67
Batch: 60; loss: 1.57; acc: 0.61
Batch: 80; loss: 1.57; acc: 0.53
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.72; acc: 0.53
Batch: 140; loss: 1.57; acc: 0.55
Val Epoch over. val_loss: 1.6634017882073762; val_accuracy: 0.5389132165605095 

The current subspace-distance is: 9.707686331239529e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.59
Batch: 20; loss: 1.55; acc: 0.58
Batch: 40; loss: 1.66; acc: 0.5
Batch: 60; loss: 1.66; acc: 0.48
Batch: 80; loss: 1.6; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.76; acc: 0.52
Batch: 160; loss: 1.67; acc: 0.44
Batch: 180; loss: 1.55; acc: 0.58
Batch: 200; loss: 1.77; acc: 0.42
Batch: 220; loss: 1.63; acc: 0.48
Batch: 240; loss: 1.6; acc: 0.61
Batch: 260; loss: 1.65; acc: 0.53
Batch: 280; loss: 1.5; acc: 0.72
Batch: 300; loss: 1.62; acc: 0.56
Batch: 320; loss: 1.8; acc: 0.42
Batch: 340; loss: 1.72; acc: 0.5
Batch: 360; loss: 1.69; acc: 0.47
Batch: 380; loss: 1.52; acc: 0.58
Batch: 400; loss: 1.65; acc: 0.47
Batch: 420; loss: 1.7; acc: 0.45
Batch: 440; loss: 1.69; acc: 0.53
Batch: 460; loss: 1.67; acc: 0.55
Batch: 480; loss: 1.67; acc: 0.48
Batch: 500; loss: 1.64; acc: 0.5
Batch: 520; loss: 1.52; acc: 0.61
Batch: 540; loss: 1.64; acc: 0.52
Batch: 560; loss: 1.58; acc: 0.58
Batch: 580; loss: 1.63; acc: 0.48
Batch: 600; loss: 1.7; acc: 0.44
Batch: 620; loss: 1.73; acc: 0.39
Batch: 640; loss: 1.69; acc: 0.52
Batch: 660; loss: 1.72; acc: 0.47
Batch: 680; loss: 1.63; acc: 0.56
Batch: 700; loss: 1.59; acc: 0.62
Batch: 720; loss: 1.72; acc: 0.47
Batch: 740; loss: 1.63; acc: 0.53
Batch: 760; loss: 1.51; acc: 0.66
Batch: 780; loss: 1.68; acc: 0.48
Train Epoch over. train_loss: 1.66; train_accuracy: 0.52 

3.755224315682426e-05
1.134209924202878e-05
Batch: 0; loss: 1.75; acc: 0.53
Batch: 20; loss: 1.78; acc: 0.45
Batch: 40; loss: 1.37; acc: 0.67
Batch: 60; loss: 1.55; acc: 0.59
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.55; acc: 0.62
Batch: 120; loss: 1.71; acc: 0.47
Batch: 140; loss: 1.52; acc: 0.58
Val Epoch over. val_loss: 1.6333361924833554; val_accuracy: 0.5403065286624203 

The current subspace-distance is: 1.134209924202878e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.52
Batch: 40; loss: 1.67; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.62
Batch: 80; loss: 1.63; acc: 0.56
Batch: 100; loss: 1.57; acc: 0.62
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.67; acc: 0.53
Batch: 160; loss: 1.55; acc: 0.61
Batch: 180; loss: 1.69; acc: 0.53
Batch: 200; loss: 1.74; acc: 0.5
Batch: 220; loss: 1.65; acc: 0.58
Batch: 240; loss: 1.76; acc: 0.44
Batch: 260; loss: 1.74; acc: 0.48
Batch: 280; loss: 1.5; acc: 0.61
Batch: 300; loss: 1.73; acc: 0.44
Batch: 320; loss: 1.68; acc: 0.55
Batch: 340; loss: 1.81; acc: 0.44
Batch: 360; loss: 1.66; acc: 0.5
Batch: 380; loss: 1.77; acc: 0.48
Batch: 400; loss: 1.69; acc: 0.44
Batch: 420; loss: 1.63; acc: 0.56
Batch: 440; loss: 1.53; acc: 0.56
Batch: 460; loss: 1.71; acc: 0.45
Batch: 480; loss: 1.47; acc: 0.53
Batch: 500; loss: 1.76; acc: 0.47
Batch: 520; loss: 1.63; acc: 0.52
Batch: 540; loss: 1.64; acc: 0.56
Batch: 560; loss: 1.71; acc: 0.44
Batch: 580; loss: 1.52; acc: 0.61
Batch: 600; loss: 1.56; acc: 0.64
Batch: 620; loss: 1.63; acc: 0.55
Batch: 640; loss: 1.7; acc: 0.45
Batch: 660; loss: 1.43; acc: 0.66
Batch: 680; loss: 1.69; acc: 0.48
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.59; acc: 0.56
Batch: 740; loss: 1.75; acc: 0.42
Batch: 760; loss: 1.57; acc: 0.52
Batch: 780; loss: 1.69; acc: 0.48
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.890488005708903e-05
1.4987776921771001e-05
Batch: 0; loss: 1.73; acc: 0.53
Batch: 20; loss: 1.77; acc: 0.45
Batch: 40; loss: 1.32; acc: 0.72
Batch: 60; loss: 1.54; acc: 0.61
Batch: 80; loss: 1.56; acc: 0.53
Batch: 100; loss: 1.53; acc: 0.61
Batch: 120; loss: 1.68; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.59
Val Epoch over. val_loss: 1.6118071443715674; val_accuracy: 0.5446855095541401 

The current subspace-distance is: 1.4987776921771001e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.62
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.62; acc: 0.58
Batch: 60; loss: 1.65; acc: 0.45
Batch: 80; loss: 1.61; acc: 0.55
Batch: 100; loss: 1.72; acc: 0.48
Batch: 120; loss: 1.57; acc: 0.56
Batch: 140; loss: 1.77; acc: 0.45
Batch: 160; loss: 1.63; acc: 0.52
Batch: 180; loss: 1.67; acc: 0.53
Batch: 200; loss: 1.55; acc: 0.52
Batch: 220; loss: 1.64; acc: 0.55
Batch: 240; loss: 1.5; acc: 0.61
Batch: 260; loss: 1.68; acc: 0.52
Batch: 280; loss: 1.66; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.52
Batch: 320; loss: 1.65; acc: 0.48
Batch: 340; loss: 1.74; acc: 0.5
Batch: 360; loss: 1.77; acc: 0.45
Batch: 380; loss: 1.73; acc: 0.48
Batch: 400; loss: 1.59; acc: 0.52
Batch: 420; loss: 1.58; acc: 0.53
Batch: 440; loss: 1.57; acc: 0.55
Batch: 460; loss: 1.55; acc: 0.56
Batch: 480; loss: 1.65; acc: 0.45
Batch: 500; loss: 1.68; acc: 0.56
Batch: 520; loss: 1.6; acc: 0.56
Batch: 540; loss: 1.52; acc: 0.62
Batch: 560; loss: 1.59; acc: 0.58
Batch: 580; loss: 1.64; acc: 0.48
Batch: 600; loss: 1.6; acc: 0.56
Batch: 620; loss: 1.63; acc: 0.45
Batch: 640; loss: 1.7; acc: 0.44
Batch: 660; loss: 1.57; acc: 0.48
Batch: 680; loss: 1.68; acc: 0.47
Batch: 700; loss: 1.57; acc: 0.62
Batch: 720; loss: 1.62; acc: 0.52
Batch: 740; loss: 1.58; acc: 0.48
Batch: 760; loss: 1.67; acc: 0.48
Batch: 780; loss: 1.63; acc: 0.59
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.971131445723586e-05
1.3570396731665824e-05
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.78; acc: 0.45
Batch: 40; loss: 1.29; acc: 0.73
Batch: 60; loss: 1.52; acc: 0.62
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.68; acc: 0.47
Batch: 140; loss: 1.47; acc: 0.62
Val Epoch over. val_loss: 1.602175600969108; val_accuracy: 0.5456807324840764 

The current subspace-distance is: 1.3570396731665824e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.78; acc: 0.42
Batch: 40; loss: 1.59; acc: 0.55
Batch: 60; loss: 1.69; acc: 0.48
Batch: 80; loss: 1.58; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.47
Batch: 120; loss: 1.49; acc: 0.61
Batch: 140; loss: 1.54; acc: 0.55
Batch: 160; loss: 1.62; acc: 0.55
Batch: 180; loss: 1.58; acc: 0.61
Batch: 200; loss: 1.78; acc: 0.44
Batch: 220; loss: 1.5; acc: 0.62
Batch: 240; loss: 1.66; acc: 0.52
Batch: 260; loss: 1.72; acc: 0.5
Batch: 280; loss: 1.67; acc: 0.55
Batch: 300; loss: 1.6; acc: 0.52
Batch: 320; loss: 1.62; acc: 0.53
Batch: 340; loss: 1.57; acc: 0.58
Batch: 360; loss: 1.68; acc: 0.44
Batch: 380; loss: 1.63; acc: 0.53
Batch: 400; loss: 1.69; acc: 0.5
Batch: 420; loss: 1.71; acc: 0.5
Batch: 440; loss: 1.61; acc: 0.53
Batch: 460; loss: 1.75; acc: 0.42
Batch: 480; loss: 1.55; acc: 0.61
Batch: 500; loss: 1.71; acc: 0.5
Batch: 520; loss: 1.57; acc: 0.61
Batch: 540; loss: 1.59; acc: 0.5
Batch: 560; loss: 1.61; acc: 0.52
Batch: 580; loss: 1.65; acc: 0.52
Batch: 600; loss: 1.57; acc: 0.53
Batch: 620; loss: 1.68; acc: 0.45
Batch: 640; loss: 1.56; acc: 0.5
Batch: 660; loss: 1.55; acc: 0.64
Batch: 680; loss: 1.65; acc: 0.45
Batch: 700; loss: 1.63; acc: 0.52
Batch: 720; loss: 1.67; acc: 0.47
Batch: 740; loss: 1.69; acc: 0.47
Batch: 760; loss: 1.56; acc: 0.56
Batch: 780; loss: 1.54; acc: 0.56
Train Epoch over. train_loss: 1.62; train_accuracy: 0.53 

4.046388858114369e-05
1.4483493032457773e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.79; acc: 0.47
Batch: 40; loss: 1.29; acc: 0.73
Batch: 60; loss: 1.52; acc: 0.62
Batch: 80; loss: 1.57; acc: 0.5
Batch: 100; loss: 1.51; acc: 0.58
Batch: 120; loss: 1.68; acc: 0.47
Batch: 140; loss: 1.48; acc: 0.58
Val Epoch over. val_loss: 1.6022104920854994; val_accuracy: 0.5460788216560509 

The current subspace-distance is: 1.4483493032457773e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.62
Batch: 20; loss: 1.64; acc: 0.48
Batch: 40; loss: 1.61; acc: 0.53
Batch: 60; loss: 1.53; acc: 0.48
Batch: 80; loss: 1.56; acc: 0.62
Batch: 100; loss: 1.63; acc: 0.48
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.64; acc: 0.55
Batch: 160; loss: 1.67; acc: 0.55
Batch: 180; loss: 1.58; acc: 0.56
Batch: 200; loss: 1.77; acc: 0.47
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.49; acc: 0.59
Batch: 260; loss: 1.52; acc: 0.56
Batch: 280; loss: 1.51; acc: 0.59
Batch: 300; loss: 1.63; acc: 0.55
Batch: 320; loss: 1.65; acc: 0.5
Batch: 340; loss: 1.57; acc: 0.59
Batch: 360; loss: 1.56; acc: 0.58
Batch: 380; loss: 1.8; acc: 0.44
Batch: 400; loss: 1.58; acc: 0.58
Batch: 420; loss: 1.66; acc: 0.47
Batch: 440; loss: 1.65; acc: 0.53
Batch: 460; loss: 1.58; acc: 0.52
Batch: 480; loss: 1.64; acc: 0.44
Batch: 500; loss: 1.42; acc: 0.66
Batch: 520; loss: 1.55; acc: 0.56
Batch: 540; loss: 1.64; acc: 0.56
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.69; acc: 0.5
Batch: 600; loss: 1.58; acc: 0.62
Batch: 620; loss: 1.51; acc: 0.56
Batch: 640; loss: 1.62; acc: 0.5
Batch: 660; loss: 1.75; acc: 0.47
Batch: 680; loss: 1.57; acc: 0.58
Batch: 700; loss: 1.55; acc: 0.61
Batch: 720; loss: 1.57; acc: 0.52
Batch: 740; loss: 1.76; acc: 0.42
Batch: 760; loss: 1.42; acc: 0.66
Batch: 780; loss: 1.49; acc: 0.58
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.03611920773983e-05
1.25957549244049e-05
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.79; acc: 0.45
Batch: 40; loss: 1.29; acc: 0.7
Batch: 60; loss: 1.52; acc: 0.61
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.68; acc: 0.47
Batch: 140; loss: 1.48; acc: 0.59
Val Epoch over. val_loss: 1.601149765549192; val_accuracy: 0.5416003184713376 

The current subspace-distance is: 1.25957549244049e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.47; acc: 0.64
Batch: 20; loss: 1.69; acc: 0.45
Batch: 40; loss: 1.64; acc: 0.53
Batch: 60; loss: 1.49; acc: 0.64
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.77; acc: 0.41
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.64; acc: 0.58
Batch: 160; loss: 1.68; acc: 0.5
Batch: 180; loss: 1.6; acc: 0.56
Batch: 200; loss: 1.69; acc: 0.47
Batch: 220; loss: 1.61; acc: 0.52
Batch: 240; loss: 1.67; acc: 0.45
Batch: 260; loss: 1.58; acc: 0.58
Batch: 280; loss: 1.55; acc: 0.55
Batch: 300; loss: 1.68; acc: 0.44
Batch: 320; loss: 1.66; acc: 0.56
Batch: 340; loss: 1.68; acc: 0.48
Batch: 360; loss: 1.69; acc: 0.45
Batch: 380; loss: 1.53; acc: 0.58
Batch: 400; loss: 1.65; acc: 0.47
Batch: 420; loss: 1.71; acc: 0.53
Batch: 440; loss: 1.62; acc: 0.53
Batch: 460; loss: 1.6; acc: 0.55
Batch: 480; loss: 1.75; acc: 0.38
Batch: 500; loss: 1.55; acc: 0.55
Batch: 520; loss: 1.79; acc: 0.42
Batch: 540; loss: 1.63; acc: 0.48
Batch: 560; loss: 1.61; acc: 0.5
Batch: 580; loss: 1.6; acc: 0.55
Batch: 600; loss: 1.61; acc: 0.5
Batch: 620; loss: 1.53; acc: 0.58
Batch: 640; loss: 1.8; acc: 0.39
Batch: 660; loss: 1.5; acc: 0.64
Batch: 680; loss: 1.72; acc: 0.47
Batch: 700; loss: 1.59; acc: 0.53
Batch: 720; loss: 1.51; acc: 0.56
Batch: 740; loss: 1.55; acc: 0.53
Batch: 760; loss: 1.68; acc: 0.53
Batch: 780; loss: 1.61; acc: 0.44
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.0533697756472975e-05
1.3157264220353682e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.29; acc: 0.73
Batch: 60; loss: 1.52; acc: 0.62
Batch: 80; loss: 1.58; acc: 0.47
Batch: 100; loss: 1.51; acc: 0.58
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.47; acc: 0.59
Val Epoch over. val_loss: 1.598679659472909; val_accuracy: 0.5426950636942676 

The current subspace-distance is: 1.3157264220353682e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.52
Batch: 20; loss: 1.6; acc: 0.56
Batch: 40; loss: 1.55; acc: 0.61
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.53; acc: 0.58
Batch: 100; loss: 1.74; acc: 0.45
Batch: 120; loss: 1.55; acc: 0.59
Batch: 140; loss: 1.59; acc: 0.52
Batch: 160; loss: 1.65; acc: 0.48
Batch: 180; loss: 1.56; acc: 0.59
Batch: 200; loss: 1.73; acc: 0.52
Batch: 220; loss: 1.59; acc: 0.53
Batch: 240; loss: 1.73; acc: 0.55
Batch: 260; loss: 1.58; acc: 0.52
Batch: 280; loss: 1.54; acc: 0.53
Batch: 300; loss: 1.51; acc: 0.58
Batch: 320; loss: 1.55; acc: 0.53
Batch: 340; loss: 1.68; acc: 0.48
Batch: 360; loss: 1.67; acc: 0.5
Batch: 380; loss: 1.45; acc: 0.62
Batch: 400; loss: 1.62; acc: 0.55
Batch: 420; loss: 1.6; acc: 0.5
Batch: 440; loss: 1.6; acc: 0.48
Batch: 460; loss: 1.76; acc: 0.45
Batch: 480; loss: 1.71; acc: 0.5
Batch: 500; loss: 1.6; acc: 0.55
Batch: 520; loss: 1.58; acc: 0.5
Batch: 540; loss: 1.65; acc: 0.52
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.75; acc: 0.47
Batch: 600; loss: 1.6; acc: 0.47
Batch: 620; loss: 1.61; acc: 0.47
Batch: 640; loss: 1.67; acc: 0.45
Batch: 660; loss: 1.48; acc: 0.66
Batch: 680; loss: 1.65; acc: 0.55
Batch: 700; loss: 1.64; acc: 0.52
Batch: 720; loss: 1.61; acc: 0.55
Batch: 740; loss: 1.49; acc: 0.58
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.66; acc: 0.41
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.068541602464393e-05
1.1024400009773672e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.78; acc: 0.44
Batch: 40; loss: 1.28; acc: 0.73
Batch: 60; loss: 1.51; acc: 0.64
Batch: 80; loss: 1.56; acc: 0.52
Batch: 100; loss: 1.51; acc: 0.56
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.47; acc: 0.59
Val Epoch over. val_loss: 1.5954288517593578; val_accuracy: 0.5440883757961783 

The current subspace-distance is: 1.1024400009773672e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.57; acc: 0.53
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.62; acc: 0.55
Batch: 60; loss: 1.64; acc: 0.47
Batch: 80; loss: 1.7; acc: 0.47
Batch: 100; loss: 1.64; acc: 0.5
Batch: 120; loss: 1.73; acc: 0.45
Batch: 140; loss: 1.84; acc: 0.33
Batch: 160; loss: 1.6; acc: 0.52
Batch: 180; loss: 1.54; acc: 0.58
Batch: 200; loss: 1.57; acc: 0.58
Batch: 220; loss: 1.51; acc: 0.64
Batch: 240; loss: 1.61; acc: 0.48
Batch: 260; loss: 1.58; acc: 0.52
Batch: 280; loss: 1.64; acc: 0.45
Batch: 300; loss: 1.57; acc: 0.56
Batch: 320; loss: 1.35; acc: 0.72
Batch: 340; loss: 1.59; acc: 0.55
Batch: 360; loss: 1.7; acc: 0.48
Batch: 380; loss: 1.73; acc: 0.39
Batch: 400; loss: 1.75; acc: 0.48
Batch: 420; loss: 1.71; acc: 0.48
Batch: 440; loss: 1.69; acc: 0.42
Batch: 460; loss: 1.64; acc: 0.53
Batch: 480; loss: 1.65; acc: 0.47
Batch: 500; loss: 1.69; acc: 0.39
Batch: 520; loss: 1.55; acc: 0.55
Batch: 540; loss: 1.6; acc: 0.53
Batch: 560; loss: 1.61; acc: 0.48
Batch: 580; loss: 1.78; acc: 0.45
Batch: 600; loss: 1.69; acc: 0.5
Batch: 620; loss: 1.63; acc: 0.52
Batch: 640; loss: 1.53; acc: 0.55
Batch: 660; loss: 1.63; acc: 0.61
Batch: 680; loss: 1.66; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.53
Batch: 720; loss: 1.62; acc: 0.45
Batch: 740; loss: 1.61; acc: 0.56
Batch: 760; loss: 1.61; acc: 0.52
Batch: 780; loss: 1.62; acc: 0.56
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.104374602320604e-05
1.4295033906819299e-05
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 1.5; acc: 0.64
Batch: 80; loss: 1.56; acc: 0.52
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.47; acc: 0.61
Val Epoch over. val_loss: 1.5935825420792695; val_accuracy: 0.5409036624203821 

The current subspace-distance is: 1.4295033906819299e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.55
Batch: 20; loss: 1.41; acc: 0.69
Batch: 40; loss: 1.76; acc: 0.48
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.64
Batch: 120; loss: 1.6; acc: 0.5
Batch: 140; loss: 1.61; acc: 0.5
Batch: 160; loss: 1.64; acc: 0.55
Batch: 180; loss: 1.8; acc: 0.41
Batch: 200; loss: 1.64; acc: 0.5
Batch: 220; loss: 1.64; acc: 0.42
Batch: 240; loss: 1.62; acc: 0.55
Batch: 260; loss: 1.58; acc: 0.52
Batch: 280; loss: 1.53; acc: 0.56
Batch: 300; loss: 1.58; acc: 0.53
Batch: 320; loss: 1.64; acc: 0.52
Batch: 340; loss: 1.69; acc: 0.42
Batch: 360; loss: 1.46; acc: 0.59
Batch: 380; loss: 1.57; acc: 0.59
Batch: 400; loss: 1.78; acc: 0.41
Batch: 420; loss: 1.59; acc: 0.61
Batch: 440; loss: 1.67; acc: 0.48
Batch: 460; loss: 1.66; acc: 0.5
Batch: 480; loss: 1.74; acc: 0.47
Batch: 500; loss: 1.63; acc: 0.5
Batch: 520; loss: 1.66; acc: 0.42
Batch: 540; loss: 1.63; acc: 0.47
Batch: 560; loss: 1.72; acc: 0.44
Batch: 580; loss: 1.62; acc: 0.47
Batch: 600; loss: 1.51; acc: 0.56
Batch: 620; loss: 1.59; acc: 0.52
Batch: 640; loss: 1.6; acc: 0.53
Batch: 660; loss: 1.67; acc: 0.47
Batch: 680; loss: 1.61; acc: 0.59
Batch: 700; loss: 1.76; acc: 0.44
Batch: 720; loss: 1.8; acc: 0.42
Batch: 740; loss: 1.67; acc: 0.45
Batch: 760; loss: 1.58; acc: 0.56
Batch: 780; loss: 1.53; acc: 0.58
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.1923023673007265e-05
1.3814440535497852e-05
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.76; acc: 0.44
Batch: 40; loss: 1.27; acc: 0.77
Batch: 60; loss: 1.5; acc: 0.66
Batch: 80; loss: 1.57; acc: 0.53
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.68; acc: 0.44
Batch: 140; loss: 1.46; acc: 0.59
Val Epoch over. val_loss: 1.5942055062883218; val_accuracy: 0.5390127388535032 

The current subspace-distance is: 1.3814440535497852e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.57; acc: 0.56
Batch: 20; loss: 1.51; acc: 0.66
Batch: 40; loss: 1.5; acc: 0.55
Batch: 60; loss: 1.5; acc: 0.62
Batch: 80; loss: 1.53; acc: 0.58
Batch: 100; loss: 1.56; acc: 0.52
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.57; acc: 0.48
Batch: 160; loss: 1.45; acc: 0.7
Batch: 180; loss: 1.57; acc: 0.5
Batch: 200; loss: 1.58; acc: 0.56
Batch: 220; loss: 1.59; acc: 0.48
Batch: 240; loss: 1.7; acc: 0.5
Batch: 260; loss: 1.85; acc: 0.38
Batch: 280; loss: 1.67; acc: 0.47
Batch: 300; loss: 1.62; acc: 0.52
Batch: 320; loss: 1.71; acc: 0.47
Batch: 340; loss: 1.65; acc: 0.5
Batch: 360; loss: 1.46; acc: 0.62
Batch: 380; loss: 1.62; acc: 0.53
Batch: 400; loss: 1.6; acc: 0.5
Batch: 420; loss: 1.63; acc: 0.47
Batch: 440; loss: 1.38; acc: 0.67
Batch: 460; loss: 1.73; acc: 0.39
Batch: 480; loss: 1.65; acc: 0.5
Batch: 500; loss: 1.71; acc: 0.48
Batch: 520; loss: 1.63; acc: 0.53
Batch: 540; loss: 1.61; acc: 0.45
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.75; acc: 0.41
Batch: 600; loss: 1.64; acc: 0.45
Batch: 620; loss: 1.55; acc: 0.55
Batch: 640; loss: 1.62; acc: 0.53
Batch: 660; loss: 1.61; acc: 0.58
Batch: 680; loss: 1.63; acc: 0.53
Batch: 700; loss: 1.79; acc: 0.36
Batch: 720; loss: 1.53; acc: 0.62
Batch: 740; loss: 1.61; acc: 0.59
Batch: 760; loss: 1.5; acc: 0.58
Batch: 780; loss: 1.54; acc: 0.56
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.083810927113518e-05
1.3524094356398564e-05
Batch: 0; loss: 1.7; acc: 0.42
Batch: 20; loss: 1.76; acc: 0.42
Batch: 40; loss: 1.24; acc: 0.75
Batch: 60; loss: 1.48; acc: 0.66
Batch: 80; loss: 1.56; acc: 0.48
Batch: 100; loss: 1.5; acc: 0.59
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.45; acc: 0.64
Val Epoch over. val_loss: 1.5839041752420413; val_accuracy: 0.5366242038216561 

The current subspace-distance is: 1.3524094356398564e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.7; acc: 0.48
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.62
Batch: 100; loss: 1.6; acc: 0.56
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.55; acc: 0.59
Batch: 160; loss: 1.64; acc: 0.5
Batch: 180; loss: 1.55; acc: 0.59
Batch: 200; loss: 1.71; acc: 0.45
Batch: 220; loss: 1.56; acc: 0.55
Batch: 240; loss: 1.61; acc: 0.55
Batch: 260; loss: 1.51; acc: 0.53
Batch: 280; loss: 1.67; acc: 0.48
Batch: 300; loss: 1.56; acc: 0.56
Batch: 320; loss: 1.47; acc: 0.73
Batch: 340; loss: 1.56; acc: 0.61
Batch: 360; loss: 1.6; acc: 0.42
Batch: 380; loss: 1.54; acc: 0.53
Batch: 400; loss: 1.84; acc: 0.41
Batch: 420; loss: 1.54; acc: 0.52
Batch: 440; loss: 1.6; acc: 0.55
Batch: 460; loss: 1.62; acc: 0.48
Batch: 480; loss: 1.62; acc: 0.5
Batch: 500; loss: 1.54; acc: 0.56
Batch: 520; loss: 1.6; acc: 0.53
Batch: 540; loss: 1.71; acc: 0.47
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.61; acc: 0.47
Batch: 600; loss: 1.62; acc: 0.48
Batch: 620; loss: 1.59; acc: 0.53
Batch: 640; loss: 1.56; acc: 0.56
Batch: 660; loss: 1.67; acc: 0.53
Batch: 680; loss: 1.65; acc: 0.55
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.54; acc: 0.52
Batch: 760; loss: 1.54; acc: 0.56
Batch: 780; loss: 1.67; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.17882292822469e-05
1.3812754332320765e-05
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.26; acc: 0.72
Batch: 60; loss: 1.49; acc: 0.67
Batch: 80; loss: 1.57; acc: 0.5
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.46; acc: 0.64
Val Epoch over. val_loss: 1.594448986326813; val_accuracy: 0.533937101910828 

The current subspace-distance is: 1.3812754332320765e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.48
Batch: 20; loss: 1.71; acc: 0.5
Batch: 40; loss: 1.63; acc: 0.59
Batch: 60; loss: 1.53; acc: 0.62
Batch: 80; loss: 1.68; acc: 0.42
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.55; acc: 0.61
Batch: 140; loss: 1.54; acc: 0.56
Batch: 160; loss: 1.62; acc: 0.55
Batch: 180; loss: 1.55; acc: 0.53
Batch: 200; loss: 1.72; acc: 0.45
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.61; acc: 0.48
Batch: 260; loss: 1.69; acc: 0.45
Batch: 280; loss: 1.51; acc: 0.56
Batch: 300; loss: 1.69; acc: 0.52
Batch: 320; loss: 1.75; acc: 0.44
Batch: 340; loss: 1.52; acc: 0.61
Batch: 360; loss: 1.51; acc: 0.61
Batch: 380; loss: 1.71; acc: 0.44
Batch: 400; loss: 1.56; acc: 0.59
Batch: 420; loss: 1.6; acc: 0.5
Batch: 440; loss: 1.61; acc: 0.52
Batch: 460; loss: 1.62; acc: 0.44
Batch: 480; loss: 1.67; acc: 0.45
Batch: 500; loss: 1.61; acc: 0.47
Batch: 520; loss: 1.53; acc: 0.5
Batch: 540; loss: 1.62; acc: 0.55
Batch: 560; loss: 1.63; acc: 0.48
Batch: 580; loss: 1.66; acc: 0.52
Batch: 600; loss: 1.54; acc: 0.52
Batch: 620; loss: 1.67; acc: 0.5
Batch: 640; loss: 1.62; acc: 0.48
Batch: 660; loss: 1.63; acc: 0.58
Batch: 680; loss: 1.64; acc: 0.52
Batch: 700; loss: 1.56; acc: 0.52
Batch: 720; loss: 1.64; acc: 0.44
Batch: 740; loss: 1.58; acc: 0.47
Batch: 760; loss: 1.56; acc: 0.53
Batch: 780; loss: 1.72; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.1678034904180095e-05
1.448106559109874e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.75
Batch: 60; loss: 1.48; acc: 0.69
Batch: 80; loss: 1.56; acc: 0.48
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.46; acc: 0.64
Val Epoch over. val_loss: 1.584064971109864; val_accuracy: 0.5380175159235668 

The current subspace-distance is: 1.448106559109874e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.69; acc: 0.48
Batch: 20; loss: 1.73; acc: 0.48
Batch: 40; loss: 1.76; acc: 0.42
Batch: 60; loss: 1.69; acc: 0.45
Batch: 80; loss: 1.74; acc: 0.44
Batch: 100; loss: 1.58; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.47
Batch: 140; loss: 1.64; acc: 0.52
Batch: 160; loss: 1.6; acc: 0.52
Batch: 180; loss: 1.57; acc: 0.53
Batch: 200; loss: 1.49; acc: 0.56
Batch: 220; loss: 1.41; acc: 0.67
Batch: 240; loss: 1.58; acc: 0.48
Batch: 260; loss: 1.8; acc: 0.41
Batch: 280; loss: 1.58; acc: 0.55
Batch: 300; loss: 1.49; acc: 0.61
Batch: 320; loss: 1.47; acc: 0.64
Batch: 340; loss: 1.79; acc: 0.36
Batch: 360; loss: 1.69; acc: 0.44
Batch: 380; loss: 1.5; acc: 0.58
Batch: 400; loss: 1.67; acc: 0.45
Batch: 420; loss: 1.73; acc: 0.38
Batch: 440; loss: 1.41; acc: 0.62
Batch: 460; loss: 1.76; acc: 0.36
Batch: 480; loss: 1.67; acc: 0.42
Batch: 500; loss: 1.71; acc: 0.44
Batch: 520; loss: 1.66; acc: 0.44
Batch: 540; loss: 1.51; acc: 0.62
Batch: 560; loss: 1.64; acc: 0.55
Batch: 580; loss: 1.59; acc: 0.55
Batch: 600; loss: 1.51; acc: 0.56
Batch: 620; loss: 1.61; acc: 0.58
Batch: 640; loss: 1.52; acc: 0.5
Batch: 660; loss: 1.6; acc: 0.47
Batch: 680; loss: 1.71; acc: 0.45
Batch: 700; loss: 1.64; acc: 0.48
Batch: 720; loss: 1.47; acc: 0.56
Batch: 740; loss: 1.58; acc: 0.56
Batch: 760; loss: 1.61; acc: 0.48
Batch: 780; loss: 1.63; acc: 0.47
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.216793968225829e-05
1.4633836144639645e-05
Batch: 0; loss: 1.7; acc: 0.41
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.24; acc: 0.73
Batch: 60; loss: 1.47; acc: 0.66
Batch: 80; loss: 1.56; acc: 0.5
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.68; acc: 0.48
Batch: 140; loss: 1.45; acc: 0.64
Val Epoch over. val_loss: 1.5877083190687142; val_accuracy: 0.5297571656050956 

The current subspace-distance is: 1.4633836144639645e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.64; acc: 0.5
Batch: 20; loss: 1.57; acc: 0.58
Batch: 40; loss: 1.65; acc: 0.45
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.47
Batch: 100; loss: 1.83; acc: 0.36
Batch: 120; loss: 1.61; acc: 0.5
Batch: 140; loss: 1.62; acc: 0.53
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.67; acc: 0.44
Batch: 200; loss: 1.52; acc: 0.48
Batch: 220; loss: 1.73; acc: 0.44
Batch: 240; loss: 1.67; acc: 0.48
Batch: 260; loss: 1.64; acc: 0.53
Batch: 280; loss: 1.46; acc: 0.58
Batch: 300; loss: 1.69; acc: 0.5
Batch: 320; loss: 1.7; acc: 0.39
Batch: 340; loss: 1.61; acc: 0.55
Batch: 360; loss: 1.65; acc: 0.42
Batch: 380; loss: 1.66; acc: 0.45
Batch: 400; loss: 1.45; acc: 0.58
Batch: 420; loss: 1.66; acc: 0.44
Batch: 440; loss: 1.57; acc: 0.61
Batch: 460; loss: 1.5; acc: 0.66
Batch: 480; loss: 1.65; acc: 0.56
Batch: 500; loss: 1.57; acc: 0.5
Batch: 520; loss: 1.51; acc: 0.56
Batch: 540; loss: 1.73; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.66
Batch: 580; loss: 1.5; acc: 0.55
Batch: 600; loss: 1.71; acc: 0.38
Batch: 620; loss: 1.63; acc: 0.56
Batch: 640; loss: 1.65; acc: 0.48
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.62; acc: 0.52
Batch: 700; loss: 1.67; acc: 0.47
Batch: 720; loss: 1.53; acc: 0.62
Batch: 740; loss: 1.66; acc: 0.45
Batch: 760; loss: 1.57; acc: 0.52
Batch: 780; loss: 1.51; acc: 0.58
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.188155071460642e-05
1.4653949619969353e-05
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.75; acc: 0.44
Batch: 40; loss: 1.24; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.69
Batch: 80; loss: 1.55; acc: 0.5
Batch: 100; loss: 1.5; acc: 0.59
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.45; acc: 0.61
Val Epoch over. val_loss: 1.5814649102034841; val_accuracy: 0.5373208598726115 

The current subspace-distance is: 1.4653949619969353e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.52
Batch: 40; loss: 1.68; acc: 0.45
Batch: 60; loss: 1.69; acc: 0.42
Batch: 80; loss: 1.55; acc: 0.53
Batch: 100; loss: 1.71; acc: 0.41
Batch: 120; loss: 1.58; acc: 0.53
Batch: 140; loss: 1.78; acc: 0.42
Batch: 160; loss: 1.48; acc: 0.59
Batch: 180; loss: 1.66; acc: 0.52
Batch: 200; loss: 1.51; acc: 0.55
Batch: 220; loss: 1.75; acc: 0.44
Batch: 240; loss: 1.62; acc: 0.45
Batch: 260; loss: 1.77; acc: 0.39
Batch: 280; loss: 1.51; acc: 0.53
Batch: 300; loss: 1.45; acc: 0.56
Batch: 320; loss: 1.49; acc: 0.56
Batch: 340; loss: 1.64; acc: 0.48
Batch: 360; loss: 1.71; acc: 0.47
Batch: 380; loss: 1.71; acc: 0.45
Batch: 400; loss: 1.65; acc: 0.47
Batch: 420; loss: 1.49; acc: 0.58
Batch: 440; loss: 1.53; acc: 0.52
Batch: 460; loss: 1.52; acc: 0.48
Batch: 480; loss: 1.76; acc: 0.44
Batch: 500; loss: 1.57; acc: 0.56
Batch: 520; loss: 1.65; acc: 0.56
Batch: 540; loss: 1.58; acc: 0.48
Batch: 560; loss: 1.56; acc: 0.58
Batch: 580; loss: 1.65; acc: 0.42
Batch: 600; loss: 1.81; acc: 0.42
Batch: 620; loss: 1.75; acc: 0.41
Batch: 640; loss: 1.88; acc: 0.36
Batch: 660; loss: 1.51; acc: 0.59
Batch: 680; loss: 1.69; acc: 0.5
Batch: 700; loss: 1.71; acc: 0.44
Batch: 720; loss: 1.57; acc: 0.52
Batch: 740; loss: 1.56; acc: 0.56
Batch: 760; loss: 1.67; acc: 0.47
Batch: 780; loss: 1.56; acc: 0.53
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.173963679932058e-05
1.409632295690244e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.24; acc: 0.73
Batch: 60; loss: 1.46; acc: 0.69
Batch: 80; loss: 1.56; acc: 0.47
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.46; acc: 0.62
Val Epoch over. val_loss: 1.5823402495900536; val_accuracy: 0.5378184713375797 

The current subspace-distance is: 1.409632295690244e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.64; acc: 0.52
Batch: 20; loss: 1.63; acc: 0.56
Batch: 40; loss: 1.54; acc: 0.55
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.71; acc: 0.41
Batch: 100; loss: 1.69; acc: 0.47
Batch: 120; loss: 1.56; acc: 0.56
Batch: 140; loss: 1.54; acc: 0.55
Batch: 160; loss: 1.45; acc: 0.56
Batch: 180; loss: 1.79; acc: 0.47
Batch: 200; loss: 1.61; acc: 0.53
Batch: 220; loss: 1.72; acc: 0.56
Batch: 240; loss: 1.56; acc: 0.59
Batch: 260; loss: 1.55; acc: 0.53
Batch: 280; loss: 1.76; acc: 0.5
Batch: 300; loss: 1.63; acc: 0.55
Batch: 320; loss: 1.61; acc: 0.52
Batch: 340; loss: 1.6; acc: 0.44
Batch: 360; loss: 1.56; acc: 0.58
Batch: 380; loss: 1.7; acc: 0.44
Batch: 400; loss: 1.57; acc: 0.55
Batch: 420; loss: 1.68; acc: 0.52
Batch: 440; loss: 1.79; acc: 0.36
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.73; acc: 0.42
Batch: 500; loss: 1.78; acc: 0.45
Batch: 520; loss: 1.53; acc: 0.52
Batch: 540; loss: 1.66; acc: 0.48
Batch: 560; loss: 1.67; acc: 0.5
Batch: 580; loss: 1.67; acc: 0.47
Batch: 600; loss: 1.51; acc: 0.52
Batch: 620; loss: 1.58; acc: 0.48
Batch: 640; loss: 1.51; acc: 0.59
Batch: 660; loss: 1.63; acc: 0.47
Batch: 680; loss: 1.62; acc: 0.52
Batch: 700; loss: 1.59; acc: 0.61
Batch: 720; loss: 1.69; acc: 0.44
Batch: 740; loss: 1.64; acc: 0.48
Batch: 760; loss: 1.53; acc: 0.56
Batch: 780; loss: 1.58; acc: 0.55
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.230688136885874e-05
1.4978216313465964e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.74; acc: 0.41
Batch: 40; loss: 1.23; acc: 0.75
Batch: 60; loss: 1.46; acc: 0.69
Batch: 80; loss: 1.56; acc: 0.52
Batch: 100; loss: 1.48; acc: 0.61
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.45; acc: 0.62
Val Epoch over. val_loss: 1.5772560979150663; val_accuracy: 0.5351313694267515 

The current subspace-distance is: 1.4978216313465964e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.63; acc: 0.47
Batch: 20; loss: 1.48; acc: 0.58
Batch: 40; loss: 1.56; acc: 0.45
Batch: 60; loss: 1.7; acc: 0.44
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.68; acc: 0.5
Batch: 120; loss: 1.63; acc: 0.53
Batch: 140; loss: 1.59; acc: 0.53
Batch: 160; loss: 1.64; acc: 0.56
Batch: 180; loss: 1.62; acc: 0.52
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.64; acc: 0.5
Batch: 240; loss: 1.53; acc: 0.56
Batch: 260; loss: 1.47; acc: 0.62
Batch: 280; loss: 1.54; acc: 0.59
Batch: 300; loss: 1.5; acc: 0.61
Batch: 320; loss: 1.63; acc: 0.53
Batch: 340; loss: 1.63; acc: 0.48
Batch: 360; loss: 1.52; acc: 0.56
Batch: 380; loss: 1.62; acc: 0.5
Batch: 400; loss: 1.55; acc: 0.56
Batch: 420; loss: 1.72; acc: 0.41
Batch: 440; loss: 1.74; acc: 0.44
Batch: 460; loss: 1.82; acc: 0.44
Batch: 480; loss: 1.69; acc: 0.53
Batch: 500; loss: 1.59; acc: 0.48
Batch: 520; loss: 1.57; acc: 0.5
Batch: 540; loss: 1.59; acc: 0.53
Batch: 560; loss: 1.58; acc: 0.56
Batch: 580; loss: 1.83; acc: 0.34
Batch: 600; loss: 1.52; acc: 0.56
Batch: 620; loss: 1.58; acc: 0.52
Batch: 640; loss: 1.5; acc: 0.58
Batch: 660; loss: 1.73; acc: 0.36
Batch: 680; loss: 1.7; acc: 0.41
Batch: 700; loss: 1.57; acc: 0.52
Batch: 720; loss: 1.66; acc: 0.5
Batch: 740; loss: 1.49; acc: 0.61
Batch: 760; loss: 1.67; acc: 0.45
Batch: 780; loss: 1.71; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.1590457840356976e-05
1.2502900062827393e-05
Batch: 0; loss: 1.69; acc: 0.44
Batch: 20; loss: 1.75; acc: 0.44
Batch: 40; loss: 1.23; acc: 0.75
Batch: 60; loss: 1.45; acc: 0.69
Batch: 80; loss: 1.56; acc: 0.48
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.45; acc: 0.62
Val Epoch over. val_loss: 1.5806808782990571; val_accuracy: 0.5336385350318471 

The current subspace-distance is: 1.2502900062827393e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.47
Batch: 20; loss: 1.55; acc: 0.53
Batch: 40; loss: 1.66; acc: 0.5
Batch: 60; loss: 1.57; acc: 0.58
Batch: 80; loss: 1.72; acc: 0.48
Batch: 100; loss: 1.58; acc: 0.56
Batch: 120; loss: 1.63; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.52
Batch: 160; loss: 1.63; acc: 0.48
Batch: 180; loss: 1.64; acc: 0.53
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.7; acc: 0.42
Batch: 240; loss: 1.57; acc: 0.56
Batch: 260; loss: 1.58; acc: 0.5
Batch: 280; loss: 1.59; acc: 0.5
Batch: 300; loss: 1.59; acc: 0.47
Batch: 320; loss: 1.52; acc: 0.52
Batch: 340; loss: 1.5; acc: 0.52
Batch: 360; loss: 1.62; acc: 0.52
Batch: 380; loss: 1.77; acc: 0.38
Batch: 400; loss: 1.59; acc: 0.5
Batch: 420; loss: 1.6; acc: 0.5
Batch: 440; loss: 1.66; acc: 0.47
Batch: 460; loss: 1.51; acc: 0.56
Batch: 480; loss: 1.51; acc: 0.62
Batch: 500; loss: 1.57; acc: 0.52
Batch: 520; loss: 1.69; acc: 0.48
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.5; acc: 0.55
Batch: 580; loss: 1.63; acc: 0.5
Batch: 600; loss: 1.54; acc: 0.53
Batch: 620; loss: 1.52; acc: 0.56
Batch: 640; loss: 1.71; acc: 0.52
Batch: 660; loss: 1.59; acc: 0.53
Batch: 680; loss: 1.54; acc: 0.5
Batch: 700; loss: 1.7; acc: 0.47
Batch: 720; loss: 1.46; acc: 0.62
Batch: 740; loss: 1.61; acc: 0.52
Batch: 760; loss: 1.63; acc: 0.5
Batch: 780; loss: 1.51; acc: 0.62
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.322479071561247e-05
1.635291482671164e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.75; acc: 0.44
Batch: 40; loss: 1.24; acc: 0.73
Batch: 60; loss: 1.46; acc: 0.69
Batch: 80; loss: 1.55; acc: 0.5
Batch: 100; loss: 1.5; acc: 0.58
Batch: 120; loss: 1.67; acc: 0.45
Batch: 140; loss: 1.46; acc: 0.61
Val Epoch over. val_loss: 1.5835057140155961; val_accuracy: 0.537718949044586 

The current subspace-distance is: 1.635291482671164e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.68; acc: 0.53
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.64; acc: 0.52
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.44; acc: 0.67
Batch: 160; loss: 1.65; acc: 0.52
Batch: 180; loss: 1.67; acc: 0.48
Batch: 200; loss: 1.68; acc: 0.48
Batch: 220; loss: 1.7; acc: 0.44
Batch: 240; loss: 1.59; acc: 0.52
Batch: 260; loss: 1.52; acc: 0.59
Batch: 280; loss: 1.62; acc: 0.56
Batch: 300; loss: 1.65; acc: 0.52
Batch: 320; loss: 1.62; acc: 0.62
Batch: 340; loss: 1.69; acc: 0.45
Batch: 360; loss: 1.63; acc: 0.53
Batch: 380; loss: 1.72; acc: 0.44
Batch: 400; loss: 1.65; acc: 0.5
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.66; acc: 0.44
Batch: 460; loss: 1.46; acc: 0.61
Batch: 480; loss: 1.63; acc: 0.53
Batch: 500; loss: 1.78; acc: 0.41
Batch: 520; loss: 1.62; acc: 0.59
Batch: 540; loss: 1.59; acc: 0.58
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.42; acc: 0.62
Batch: 600; loss: 1.66; acc: 0.44
Batch: 620; loss: 1.63; acc: 0.55
Batch: 640; loss: 1.6; acc: 0.52
Batch: 660; loss: 1.56; acc: 0.52
Batch: 680; loss: 1.43; acc: 0.61
Batch: 700; loss: 1.66; acc: 0.48
Batch: 720; loss: 1.69; acc: 0.47
Batch: 740; loss: 1.61; acc: 0.52
Batch: 760; loss: 1.64; acc: 0.47
Batch: 780; loss: 1.59; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.258251283317804e-05
1.577952025400009e-05
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.75; acc: 0.42
Batch: 40; loss: 1.24; acc: 0.73
Batch: 60; loss: 1.45; acc: 0.67
Batch: 80; loss: 1.56; acc: 0.5
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.45; acc: 0.62
Val Epoch over. val_loss: 1.5783220461219738; val_accuracy: 0.5408041401273885 

The current subspace-distance is: 1.577952025400009e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.45
Batch: 20; loss: 1.68; acc: 0.45
Batch: 40; loss: 1.55; acc: 0.59
Batch: 60; loss: 1.64; acc: 0.53
Batch: 80; loss: 1.68; acc: 0.44
Batch: 100; loss: 1.59; acc: 0.55
Batch: 120; loss: 1.77; acc: 0.42
Batch: 140; loss: 1.66; acc: 0.52
Batch: 160; loss: 1.72; acc: 0.44
Batch: 180; loss: 1.64; acc: 0.48
Batch: 200; loss: 1.6; acc: 0.56
Batch: 220; loss: 1.52; acc: 0.59
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.73; acc: 0.45
Batch: 280; loss: 1.67; acc: 0.45
Batch: 300; loss: 1.54; acc: 0.58
Batch: 320; loss: 1.6; acc: 0.52
Batch: 340; loss: 1.49; acc: 0.55
Batch: 360; loss: 1.48; acc: 0.5
Batch: 380; loss: 1.43; acc: 0.69
Batch: 400; loss: 1.61; acc: 0.45
Batch: 420; loss: 1.61; acc: 0.56
Batch: 440; loss: 1.53; acc: 0.59
Batch: 460; loss: 1.54; acc: 0.5
Batch: 480; loss: 1.75; acc: 0.41
Batch: 500; loss: 1.52; acc: 0.62
Batch: 520; loss: 1.49; acc: 0.52
Batch: 540; loss: 1.62; acc: 0.5
Batch: 560; loss: 1.78; acc: 0.39
Batch: 580; loss: 1.77; acc: 0.42
Batch: 600; loss: 1.53; acc: 0.61
Batch: 620; loss: 1.55; acc: 0.52
Batch: 640; loss: 1.63; acc: 0.47
Batch: 660; loss: 1.71; acc: 0.45
Batch: 680; loss: 1.81; acc: 0.38
Batch: 700; loss: 1.65; acc: 0.5
Batch: 720; loss: 1.43; acc: 0.64
Batch: 740; loss: 1.64; acc: 0.55
Batch: 760; loss: 1.59; acc: 0.59
Batch: 780; loss: 1.61; acc: 0.53
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.1844668885460123e-05
1.4100224689173046e-05
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.75; acc: 0.42
Batch: 40; loss: 1.24; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.66
Batch: 80; loss: 1.56; acc: 0.5
Batch: 100; loss: 1.5; acc: 0.62
Batch: 120; loss: 1.68; acc: 0.44
Batch: 140; loss: 1.45; acc: 0.62
Val Epoch over. val_loss: 1.5830473277219541; val_accuracy: 0.5348328025477707 

The current subspace-distance is: 1.4100224689173046e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.68; acc: 0.42
Batch: 20; loss: 1.47; acc: 0.62
Batch: 40; loss: 1.78; acc: 0.41
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.56
Batch: 100; loss: 1.45; acc: 0.62
Batch: 120; loss: 1.55; acc: 0.58
Batch: 140; loss: 1.47; acc: 0.58
Batch: 160; loss: 1.73; acc: 0.42
Batch: 180; loss: 1.63; acc: 0.52
Batch: 200; loss: 1.68; acc: 0.45
Batch: 220; loss: 1.7; acc: 0.44
Batch: 240; loss: 1.74; acc: 0.5
Batch: 260; loss: 1.69; acc: 0.41
Batch: 280; loss: 1.63; acc: 0.5
Batch: 300; loss: 1.52; acc: 0.61
Batch: 320; loss: 1.57; acc: 0.58
Batch: 340; loss: 1.51; acc: 0.62
Batch: 360; loss: 1.73; acc: 0.41
Batch: 380; loss: 1.54; acc: 0.55
Batch: 400; loss: 1.72; acc: 0.44
Batch: 420; loss: 1.67; acc: 0.48
Batch: 440; loss: 1.5; acc: 0.56
Batch: 460; loss: 1.76; acc: 0.47
Batch: 480; loss: 1.53; acc: 0.58
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.6; acc: 0.52
Batch: 540; loss: 1.52; acc: 0.5
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.77; acc: 0.42
Batch: 600; loss: 1.53; acc: 0.52
Batch: 620; loss: 1.52; acc: 0.61
Batch: 640; loss: 1.46; acc: 0.61
Batch: 660; loss: 1.46; acc: 0.61
Batch: 680; loss: 1.6; acc: 0.48
Batch: 700; loss: 1.69; acc: 0.38
Batch: 720; loss: 1.61; acc: 0.45
Batch: 740; loss: 1.44; acc: 0.53
Batch: 760; loss: 1.53; acc: 0.56
Batch: 780; loss: 1.51; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.243212242727168e-05
1.594803688931279e-05
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.74; acc: 0.41
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.66
Batch: 80; loss: 1.55; acc: 0.48
Batch: 100; loss: 1.48; acc: 0.61
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.44; acc: 0.64
Val Epoch over. val_loss: 1.5748855422256858; val_accuracy: 0.5368232484076433 

The current subspace-distance is: 1.594803688931279e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.68; acc: 0.5
Batch: 40; loss: 1.57; acc: 0.48
Batch: 60; loss: 1.73; acc: 0.48
Batch: 80; loss: 1.63; acc: 0.48
Batch: 100; loss: 1.57; acc: 0.55
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.54; acc: 0.55
Batch: 160; loss: 1.74; acc: 0.39
Batch: 180; loss: 1.57; acc: 0.47
Batch: 200; loss: 1.65; acc: 0.48
Batch: 220; loss: 1.55; acc: 0.52
Batch: 240; loss: 1.57; acc: 0.53
Batch: 260; loss: 1.6; acc: 0.48
Batch: 280; loss: 1.57; acc: 0.52
Batch: 300; loss: 1.47; acc: 0.58
Batch: 320; loss: 1.56; acc: 0.5
Batch: 340; loss: 1.71; acc: 0.44
Batch: 360; loss: 1.57; acc: 0.55
Batch: 380; loss: 1.76; acc: 0.38
Batch: 400; loss: 1.63; acc: 0.5
Batch: 420; loss: 1.55; acc: 0.53
Batch: 440; loss: 1.64; acc: 0.48
Batch: 460; loss: 1.61; acc: 0.48
Batch: 480; loss: 1.62; acc: 0.53
Batch: 500; loss: 1.72; acc: 0.42
Batch: 520; loss: 1.76; acc: 0.39
Batch: 540; loss: 1.53; acc: 0.53
Batch: 560; loss: 1.61; acc: 0.61
Batch: 580; loss: 1.74; acc: 0.39
Batch: 600; loss: 1.73; acc: 0.47
Batch: 620; loss: 1.64; acc: 0.5
Batch: 640; loss: 1.69; acc: 0.48
Batch: 660; loss: 1.65; acc: 0.53
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.58
Batch: 720; loss: 1.6; acc: 0.45
Batch: 740; loss: 1.69; acc: 0.52
Batch: 760; loss: 1.75; acc: 0.39
Batch: 780; loss: 1.6; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.1560026147635654e-05
1.3121766642143484e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.74; acc: 0.44
Batch: 40; loss: 1.24; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.69
Batch: 80; loss: 1.56; acc: 0.5
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.45; acc: 0.62
Val Epoch over. val_loss: 1.5816868763820382; val_accuracy: 0.5361265923566879 

The current subspace-distance is: 1.3121766642143484e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.45
Batch: 40; loss: 1.54; acc: 0.48
Batch: 60; loss: 1.75; acc: 0.45
Batch: 80; loss: 1.59; acc: 0.56
Batch: 100; loss: 1.56; acc: 0.55
Batch: 120; loss: 1.63; acc: 0.47
Batch: 140; loss: 1.8; acc: 0.33
Batch: 160; loss: 1.67; acc: 0.52
Batch: 180; loss: 1.71; acc: 0.48
Batch: 200; loss: 1.49; acc: 0.59
Batch: 220; loss: 1.71; acc: 0.42
Batch: 240; loss: 1.61; acc: 0.55
Batch: 260; loss: 1.6; acc: 0.47
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.59; acc: 0.45
Batch: 320; loss: 1.76; acc: 0.42
Batch: 340; loss: 1.69; acc: 0.45
Batch: 360; loss: 1.58; acc: 0.61
Batch: 380; loss: 1.6; acc: 0.59
Batch: 400; loss: 1.67; acc: 0.52
Batch: 420; loss: 1.58; acc: 0.56
Batch: 440; loss: 1.6; acc: 0.53
Batch: 460; loss: 1.53; acc: 0.62
Batch: 480; loss: 1.51; acc: 0.56
Batch: 500; loss: 1.56; acc: 0.52
Batch: 520; loss: 1.42; acc: 0.58
Batch: 540; loss: 1.65; acc: 0.48
Batch: 560; loss: 1.53; acc: 0.59
Batch: 580; loss: 1.57; acc: 0.56
Batch: 600; loss: 1.74; acc: 0.42
Batch: 620; loss: 1.69; acc: 0.47
Batch: 640; loss: 1.62; acc: 0.52
Batch: 660; loss: 1.64; acc: 0.53
Batch: 680; loss: 1.69; acc: 0.44
Batch: 700; loss: 1.6; acc: 0.55
Batch: 720; loss: 1.48; acc: 0.61
Batch: 740; loss: 1.49; acc: 0.64
Batch: 760; loss: 1.6; acc: 0.5
Batch: 780; loss: 1.65; acc: 0.42
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.220027767587453e-05
1.4264602214097977e-05
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.74; acc: 0.42
Batch: 40; loss: 1.23; acc: 0.73
Batch: 60; loss: 1.46; acc: 0.67
Batch: 80; loss: 1.55; acc: 0.5
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.44; acc: 0.62
Val Epoch over. val_loss: 1.5777009027019429; val_accuracy: 0.5321457006369427 

The current subspace-distance is: 1.4264602214097977e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.75

The number of parameters is: 266969

The number of individual parameters is:

38
380
38
38
57
43320
57
57
114
129960
114
114
64
87552
64
64
4096
64
640
10
64
64

nonzero elements in E: 26696897
elements in E: 26696900
fraction nonzero: 0.9999998876274024
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.39; acc: 0.11
Batch: 20; loss: 2.24; acc: 0.23
Batch: 40; loss: 2.23; acc: 0.14
Batch: 60; loss: 2.08; acc: 0.31
Batch: 80; loss: 1.98; acc: 0.44
Batch: 100; loss: 2.07; acc: 0.33
Batch: 120; loss: 1.95; acc: 0.36
Batch: 140; loss: 1.94; acc: 0.41
Batch: 160; loss: 1.87; acc: 0.45
Batch: 180; loss: 1.93; acc: 0.41
Batch: 200; loss: 1.9; acc: 0.47
Batch: 220; loss: 1.89; acc: 0.44
Batch: 240; loss: 2.08; acc: 0.27
Batch: 260; loss: 1.87; acc: 0.48
Batch: 280; loss: 1.92; acc: 0.44
Batch: 300; loss: 1.88; acc: 0.45
Batch: 320; loss: 1.82; acc: 0.44
Batch: 340; loss: 1.8; acc: 0.47
Batch: 360; loss: 1.84; acc: 0.39
Batch: 380; loss: 1.86; acc: 0.41
Batch: 400; loss: 1.8; acc: 0.45
Batch: 420; loss: 1.8; acc: 0.52
Batch: 440; loss: 1.77; acc: 0.44
Batch: 460; loss: 1.86; acc: 0.39
Batch: 480; loss: 1.77; acc: 0.56
Batch: 500; loss: 1.84; acc: 0.45
Batch: 520; loss: 1.87; acc: 0.45
Batch: 540; loss: 1.64; acc: 0.67
Batch: 560; loss: 1.76; acc: 0.53
Batch: 580; loss: 1.81; acc: 0.53
Batch: 600; loss: 1.78; acc: 0.48
Batch: 620; loss: 1.73; acc: 0.5
Batch: 640; loss: 1.71; acc: 0.56
Batch: 660; loss: 1.75; acc: 0.55
Batch: 680; loss: 1.77; acc: 0.41
Batch: 700; loss: 1.68; acc: 0.55
Batch: 720; loss: 1.71; acc: 0.61
Batch: 740; loss: 1.72; acc: 0.52
Batch: 760; loss: 1.79; acc: 0.48
Batch: 780; loss: 1.76; acc: 0.5
Train Epoch over. train_loss: 1.87; train_accuracy: 0.44 

4.829598401556723e-05
4.0782353607937694e-05
Batch: 0; loss: 1.76; acc: 0.52
Batch: 20; loss: 1.89; acc: 0.39
Batch: 40; loss: 1.59; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.66
Batch: 80; loss: 1.57; acc: 0.67
Batch: 100; loss: 1.74; acc: 0.5
Batch: 120; loss: 1.77; acc: 0.52
Batch: 140; loss: 1.66; acc: 0.55
Val Epoch over. val_loss: 1.7272556686097649; val_accuracy: 0.5421974522292994 

The current subspace-distance is: 4.0782353607937694e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.55
Batch: 40; loss: 1.8; acc: 0.47
Batch: 60; loss: 1.69; acc: 0.55
Batch: 80; loss: 1.78; acc: 0.42
Batch: 100; loss: 1.64; acc: 0.59
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.73; acc: 0.5
Batch: 160; loss: 1.7; acc: 0.53
Batch: 180; loss: 1.92; acc: 0.41
Batch: 200; loss: 1.69; acc: 0.58
Batch: 220; loss: 1.77; acc: 0.5
Batch: 240; loss: 1.72; acc: 0.58
Batch: 260; loss: 1.71; acc: 0.5
Batch: 280; loss: 1.67; acc: 0.61
Batch: 300; loss: 1.62; acc: 0.61
Batch: 320; loss: 1.53; acc: 0.7
Batch: 340; loss: 1.68; acc: 0.55
Batch: 360; loss: 1.74; acc: 0.52
Batch: 380; loss: 1.57; acc: 0.59
Batch: 400; loss: 1.62; acc: 0.59
Batch: 420; loss: 1.68; acc: 0.56
Batch: 440; loss: 1.72; acc: 0.5
Batch: 460; loss: 1.67; acc: 0.56
Batch: 480; loss: 1.73; acc: 0.5
Batch: 500; loss: 1.62; acc: 0.56
Batch: 520; loss: 1.75; acc: 0.47
Batch: 540; loss: 1.61; acc: 0.64
Batch: 560; loss: 1.57; acc: 0.61
Batch: 580; loss: 1.54; acc: 0.55
Batch: 600; loss: 1.62; acc: 0.5
Batch: 620; loss: 1.64; acc: 0.56
Batch: 640; loss: 1.66; acc: 0.55
Batch: 660; loss: 1.55; acc: 0.64
Batch: 680; loss: 1.65; acc: 0.55
Batch: 700; loss: 1.61; acc: 0.61
Batch: 720; loss: 1.53; acc: 0.67
Batch: 740; loss: 1.66; acc: 0.55
Batch: 760; loss: 1.68; acc: 0.61
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.66; train_accuracy: 0.56 

6.334242789307609e-05
5.704123395844363e-05
Batch: 0; loss: 1.54; acc: 0.66
Batch: 20; loss: 1.77; acc: 0.47
Batch: 40; loss: 1.38; acc: 0.67
Batch: 60; loss: 1.5; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.62
Batch: 100; loss: 1.54; acc: 0.64
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.61
Val Epoch over. val_loss: 1.5509046399669282; val_accuracy: 0.6107683121019108 

The current subspace-distance is: 5.704123395844363e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.55; acc: 0.64
Batch: 20; loss: 1.57; acc: 0.61
Batch: 40; loss: 1.66; acc: 0.52
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.39; acc: 0.75
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.52; acc: 0.59
Batch: 140; loss: 1.56; acc: 0.67
Batch: 160; loss: 1.6; acc: 0.53
Batch: 180; loss: 1.64; acc: 0.48
Batch: 200; loss: 1.69; acc: 0.47
Batch: 220; loss: 1.56; acc: 0.59
Batch: 240; loss: 1.46; acc: 0.66
Batch: 260; loss: 1.45; acc: 0.73
Batch: 280; loss: 1.49; acc: 0.62
Batch: 300; loss: 1.52; acc: 0.52
Batch: 320; loss: 1.48; acc: 0.66
Batch: 340; loss: 1.54; acc: 0.61
Batch: 360; loss: 1.46; acc: 0.69
Batch: 380; loss: 1.48; acc: 0.64
Batch: 400; loss: 1.57; acc: 0.53
Batch: 420; loss: 1.64; acc: 0.5
Batch: 440; loss: 1.52; acc: 0.59
Batch: 460; loss: 1.53; acc: 0.56
Batch: 480; loss: 1.59; acc: 0.55
Batch: 500; loss: 1.51; acc: 0.53
Batch: 520; loss: 1.37; acc: 0.73
Batch: 540; loss: 1.51; acc: 0.55
Batch: 560; loss: 1.6; acc: 0.53
Batch: 580; loss: 1.43; acc: 0.66
Batch: 600; loss: 1.54; acc: 0.69
Batch: 620; loss: 1.4; acc: 0.69
Batch: 640; loss: 1.36; acc: 0.66
Batch: 660; loss: 1.52; acc: 0.62
Batch: 680; loss: 1.42; acc: 0.67
Batch: 700; loss: 1.5; acc: 0.66
Batch: 720; loss: 1.48; acc: 0.59
Batch: 740; loss: 1.42; acc: 0.66
Batch: 760; loss: 1.4; acc: 0.69
Batch: 780; loss: 1.34; acc: 0.64
Train Epoch over. train_loss: 1.52; train_accuracy: 0.61 

8.036091458052397e-05
7.397474109893665e-05
Batch: 0; loss: 1.41; acc: 0.64
Batch: 20; loss: 1.68; acc: 0.45
Batch: 40; loss: 1.24; acc: 0.77
Batch: 60; loss: 1.35; acc: 0.69
Batch: 80; loss: 1.33; acc: 0.69
Batch: 100; loss: 1.45; acc: 0.64
Batch: 120; loss: 1.51; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.67
Val Epoch over. val_loss: 1.4240617926713008; val_accuracy: 0.6496815286624203 

The current subspace-distance is: 7.397474109893665e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.39; acc: 0.64
Batch: 20; loss: 1.44; acc: 0.64
Batch: 40; loss: 1.47; acc: 0.58
Batch: 60; loss: 1.35; acc: 0.62
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.43; acc: 0.66
Batch: 120; loss: 1.45; acc: 0.62
Batch: 140; loss: 1.49; acc: 0.59
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.41; acc: 0.67
Batch: 200; loss: 1.48; acc: 0.61
Batch: 220; loss: 1.36; acc: 0.62
Batch: 240; loss: 1.3; acc: 0.75
Batch: 260; loss: 1.63; acc: 0.53
Batch: 280; loss: 1.38; acc: 0.72
Batch: 300; loss: 1.39; acc: 0.66
Batch: 320; loss: 1.46; acc: 0.58
Batch: 340; loss: 1.48; acc: 0.58
Batch: 360; loss: 1.43; acc: 0.59
Batch: 380; loss: 1.38; acc: 0.7
Batch: 400; loss: 1.54; acc: 0.52
Batch: 420; loss: 1.43; acc: 0.62
Batch: 440; loss: 1.36; acc: 0.66
Batch: 460; loss: 1.36; acc: 0.62
Batch: 480; loss: 1.37; acc: 0.62
Batch: 500; loss: 1.3; acc: 0.77
Batch: 520; loss: 1.49; acc: 0.59
Batch: 540; loss: 1.54; acc: 0.53
Batch: 560; loss: 1.29; acc: 0.67
Batch: 580; loss: 1.3; acc: 0.73
Batch: 600; loss: 1.47; acc: 0.59
Batch: 620; loss: 1.39; acc: 0.56
Batch: 640; loss: 1.31; acc: 0.66
Batch: 660; loss: 1.44; acc: 0.59
Batch: 680; loss: 1.28; acc: 0.67
Batch: 700; loss: 1.48; acc: 0.61
Batch: 720; loss: 1.39; acc: 0.64
Batch: 740; loss: 1.49; acc: 0.62
Batch: 760; loss: 1.42; acc: 0.62
Batch: 780; loss: 1.36; acc: 0.7
Train Epoch over. train_loss: 1.41; train_accuracy: 0.64 

9.604015940567479e-05
9.002580918604508e-05
Batch: 0; loss: 1.31; acc: 0.7
Batch: 20; loss: 1.56; acc: 0.52
Batch: 40; loss: 1.08; acc: 0.78
Batch: 60; loss: 1.27; acc: 0.72
Batch: 80; loss: 1.24; acc: 0.73
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.43; acc: 0.64
Batch: 140; loss: 1.19; acc: 0.66
Val Epoch over. val_loss: 1.3174851282387023; val_accuracy: 0.6860071656050956 

The current subspace-distance is: 9.002580918604508e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.72
Batch: 20; loss: 1.36; acc: 0.66
Batch: 40; loss: 1.48; acc: 0.55
Batch: 60; loss: 1.29; acc: 0.67
Batch: 80; loss: 1.36; acc: 0.64
Batch: 100; loss: 1.41; acc: 0.59
Batch: 120; loss: 1.42; acc: 0.67
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.34; acc: 0.7
Batch: 180; loss: 1.31; acc: 0.69
Batch: 200; loss: 1.17; acc: 0.77
Batch: 220; loss: 1.45; acc: 0.61
Batch: 240; loss: 1.37; acc: 0.61
Batch: 260; loss: 1.41; acc: 0.69
Batch: 280; loss: 1.33; acc: 0.67
Batch: 300; loss: 1.34; acc: 0.67
Batch: 320; loss: 1.37; acc: 0.62
Batch: 340; loss: 1.38; acc: 0.62
Batch: 360; loss: 1.19; acc: 0.75
Batch: 380; loss: 1.29; acc: 0.69
Batch: 400; loss: 1.39; acc: 0.58
Batch: 420; loss: 1.41; acc: 0.56
Batch: 440; loss: 1.48; acc: 0.53
Batch: 460; loss: 1.45; acc: 0.56
Batch: 480; loss: 1.37; acc: 0.59
Batch: 500; loss: 1.44; acc: 0.56
Batch: 520; loss: 1.36; acc: 0.64
Batch: 540; loss: 1.31; acc: 0.67
Batch: 560; loss: 1.36; acc: 0.64
Batch: 580; loss: 1.27; acc: 0.67
Batch: 600; loss: 1.3; acc: 0.66
Batch: 620; loss: 1.28; acc: 0.67
Batch: 640; loss: 1.37; acc: 0.62
Batch: 660; loss: 1.32; acc: 0.64
Batch: 680; loss: 1.39; acc: 0.64
Batch: 700; loss: 1.4; acc: 0.67
Batch: 720; loss: 1.31; acc: 0.69
Batch: 740; loss: 1.36; acc: 0.62
Batch: 760; loss: 1.29; acc: 0.69
Batch: 780; loss: 1.36; acc: 0.66
Train Epoch over. train_loss: 1.33; train_accuracy: 0.65 

0.00010660341649781913
9.902608871925622e-05
Batch: 0; loss: 1.27; acc: 0.7
Batch: 20; loss: 1.52; acc: 0.55
Batch: 40; loss: 1.02; acc: 0.77
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.19; acc: 0.75
Batch: 100; loss: 1.33; acc: 0.64
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.67
Val Epoch over. val_loss: 1.275392441992547; val_accuracy: 0.6833200636942676 

The current subspace-distance is: 9.902608871925622e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.69
Batch: 20; loss: 1.23; acc: 0.75
Batch: 40; loss: 1.36; acc: 0.59
Batch: 60; loss: 1.32; acc: 0.67
Batch: 80; loss: 1.36; acc: 0.62
Batch: 100; loss: 1.26; acc: 0.67
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 1.22; acc: 0.75
Batch: 160; loss: 1.26; acc: 0.64
Batch: 180; loss: 1.3; acc: 0.67
Batch: 200; loss: 1.2; acc: 0.7
Batch: 220; loss: 1.32; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.72
Batch: 260; loss: 1.2; acc: 0.73
Batch: 280; loss: 1.54; acc: 0.53
Batch: 300; loss: 1.34; acc: 0.56
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.28; acc: 0.61
Batch: 360; loss: 1.2; acc: 0.67
Batch: 380; loss: 1.28; acc: 0.69
Batch: 400; loss: 1.4; acc: 0.58
Batch: 420; loss: 1.35; acc: 0.64
Batch: 440; loss: 1.3; acc: 0.7
Batch: 460; loss: 1.26; acc: 0.69
Batch: 480; loss: 1.27; acc: 0.64
Batch: 500; loss: 1.52; acc: 0.53
Batch: 520; loss: 1.49; acc: 0.58
Batch: 540; loss: 1.23; acc: 0.72
Batch: 560; loss: 1.28; acc: 0.7
Batch: 580; loss: 1.19; acc: 0.66
Batch: 600; loss: 1.32; acc: 0.66
Batch: 620; loss: 1.36; acc: 0.59
Batch: 640; loss: 1.24; acc: 0.66
Batch: 660; loss: 1.24; acc: 0.7
Batch: 680; loss: 1.2; acc: 0.67
Batch: 700; loss: 1.38; acc: 0.59
Batch: 720; loss: 1.42; acc: 0.58
Batch: 740; loss: 1.29; acc: 0.59
Batch: 760; loss: 1.22; acc: 0.78
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.29; train_accuracy: 0.66 

0.00011577318946365267
0.00010915042366832495
Batch: 0; loss: 1.24; acc: 0.67
Batch: 20; loss: 1.49; acc: 0.48
Batch: 40; loss: 1.0; acc: 0.77
Batch: 60; loss: 1.21; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.75
Batch: 100; loss: 1.32; acc: 0.64
Batch: 120; loss: 1.38; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.67
Val Epoch over. val_loss: 1.253515931831044; val_accuracy: 0.6768511146496815 

The current subspace-distance is: 0.00010915042366832495 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.66
Batch: 20; loss: 1.32; acc: 0.7
Batch: 40; loss: 1.31; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.22; acc: 0.73
Batch: 100; loss: 1.29; acc: 0.61
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 1.19; acc: 0.72
Batch: 160; loss: 1.31; acc: 0.62
Batch: 180; loss: 1.17; acc: 0.72
Batch: 200; loss: 1.33; acc: 0.61
Batch: 220; loss: 1.27; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.7
Batch: 260; loss: 1.34; acc: 0.59
Batch: 280; loss: 1.2; acc: 0.77
Batch: 300; loss: 1.28; acc: 0.66
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 1.46; acc: 0.52
Batch: 360; loss: 1.13; acc: 0.72
Batch: 380; loss: 1.42; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.69
Batch: 420; loss: 1.15; acc: 0.73
Batch: 440; loss: 1.3; acc: 0.61
Batch: 460; loss: 1.42; acc: 0.58
Batch: 480; loss: 1.14; acc: 0.72
Batch: 500; loss: 1.25; acc: 0.66
Batch: 520; loss: 1.17; acc: 0.78
Batch: 540; loss: 1.28; acc: 0.64
Batch: 560; loss: 1.25; acc: 0.61
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.31; acc: 0.64
Batch: 620; loss: 1.23; acc: 0.7
Batch: 640; loss: 1.31; acc: 0.69
Batch: 660; loss: 1.17; acc: 0.72
Batch: 680; loss: 1.16; acc: 0.7
Batch: 700; loss: 1.37; acc: 0.62
Batch: 720; loss: 1.18; acc: 0.77
Batch: 740; loss: 1.13; acc: 0.77
Batch: 760; loss: 1.36; acc: 0.59
Batch: 780; loss: 1.39; acc: 0.59
Train Epoch over. train_loss: 1.27; train_accuracy: 0.66 

0.00012628070544451475
0.00011792764416895807
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.46; acc: 0.48
Batch: 40; loss: 0.98; acc: 0.81
Batch: 60; loss: 1.18; acc: 0.78
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.28; acc: 0.66
Batch: 120; loss: 1.35; acc: 0.56
Batch: 140; loss: 1.12; acc: 0.72
Val Epoch over. val_loss: 1.2252969111606573; val_accuracy: 0.6826234076433121 

The current subspace-distance is: 0.00011792764416895807 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 1.12; acc: 0.69
Batch: 60; loss: 1.43; acc: 0.61
Batch: 80; loss: 1.16; acc: 0.77
Batch: 100; loss: 1.24; acc: 0.62
Batch: 120; loss: 1.26; acc: 0.7
Batch: 140; loss: 1.29; acc: 0.61
Batch: 160; loss: 1.12; acc: 0.8
Batch: 180; loss: 1.24; acc: 0.69
Batch: 200; loss: 1.35; acc: 0.62
Batch: 220; loss: 1.24; acc: 0.67
Batch: 240; loss: 1.21; acc: 0.69
Batch: 260; loss: 1.2; acc: 0.69
Batch: 280; loss: 1.31; acc: 0.56
Batch: 300; loss: 1.41; acc: 0.52
Batch: 320; loss: 1.28; acc: 0.61
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 1.22; acc: 0.69
Batch: 380; loss: 1.38; acc: 0.62
Batch: 400; loss: 1.4; acc: 0.53
Batch: 420; loss: 1.17; acc: 0.73
Batch: 440; loss: 1.32; acc: 0.64
Batch: 460; loss: 1.32; acc: 0.64
Batch: 480; loss: 1.19; acc: 0.64
Batch: 500; loss: 1.15; acc: 0.75
Batch: 520; loss: 1.31; acc: 0.66
Batch: 540; loss: 1.25; acc: 0.66
Batch: 560; loss: 1.3; acc: 0.62
Batch: 580; loss: 1.13; acc: 0.62
Batch: 600; loss: 1.13; acc: 0.69
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.23; acc: 0.67
Batch: 660; loss: 1.44; acc: 0.55
Batch: 680; loss: 1.21; acc: 0.67
Batch: 700; loss: 1.17; acc: 0.73
Batch: 720; loss: 1.23; acc: 0.66
Batch: 740; loss: 1.27; acc: 0.61
Batch: 760; loss: 1.35; acc: 0.61
Batch: 780; loss: 1.24; acc: 0.69
Train Epoch over. train_loss: 1.25; train_accuracy: 0.66 

0.00012712397438008338
0.00012126946967327967
Batch: 0; loss: 1.21; acc: 0.64
Batch: 20; loss: 1.44; acc: 0.52
Batch: 40; loss: 0.96; acc: 0.8
Batch: 60; loss: 1.17; acc: 0.78
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.28; acc: 0.67
Batch: 120; loss: 1.35; acc: 0.59
Batch: 140; loss: 1.09; acc: 0.72
Val Epoch over. val_loss: 1.2048576151489452; val_accuracy: 0.6776472929936306 

The current subspace-distance is: 0.00012126946967327967 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.66
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 1.22; acc: 0.61
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.22; acc: 0.7
Batch: 100; loss: 1.4; acc: 0.58
Batch: 120; loss: 1.33; acc: 0.58
Batch: 140; loss: 1.31; acc: 0.62
Batch: 160; loss: 1.13; acc: 0.64
Batch: 180; loss: 1.17; acc: 0.64
Batch: 200; loss: 1.34; acc: 0.59
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 1.26; acc: 0.7
Batch: 260; loss: 1.44; acc: 0.55
Batch: 280; loss: 1.23; acc: 0.67
Batch: 300; loss: 1.05; acc: 0.77
Batch: 320; loss: 1.27; acc: 0.64
Batch: 340; loss: 1.34; acc: 0.61
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.43; acc: 0.55
Batch: 400; loss: 1.19; acc: 0.67
Batch: 420; loss: 1.29; acc: 0.62
Batch: 440; loss: 1.19; acc: 0.67
Batch: 460; loss: 1.28; acc: 0.59
Batch: 480; loss: 1.16; acc: 0.69
Batch: 500; loss: 1.26; acc: 0.59
Batch: 520; loss: 1.21; acc: 0.7
Batch: 540; loss: 1.13; acc: 0.67
Batch: 560; loss: 1.12; acc: 0.7
Batch: 580; loss: 1.36; acc: 0.61
Batch: 600; loss: 1.15; acc: 0.73
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.23; acc: 0.67
Batch: 660; loss: 1.37; acc: 0.67
Batch: 680; loss: 1.2; acc: 0.67
Batch: 700; loss: 1.34; acc: 0.58
Batch: 720; loss: 1.15; acc: 0.73
Batch: 740; loss: 1.07; acc: 0.75
Batch: 760; loss: 1.11; acc: 0.7
Batch: 780; loss: 1.23; acc: 0.62
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.0001341334282187745
0.00012794499343726784
Batch: 0; loss: 1.19; acc: 0.62
Batch: 20; loss: 1.42; acc: 0.52
Batch: 40; loss: 0.94; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.72
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 1.08; acc: 0.73
Val Epoch over. val_loss: 1.1928220622858423; val_accuracy: 0.6806329617834395 

The current subspace-distance is: 0.00012794499343726784 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.14; acc: 0.7
Batch: 40; loss: 1.4; acc: 0.56
Batch: 60; loss: 1.2; acc: 0.62
Batch: 80; loss: 1.2; acc: 0.78
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 1.34; acc: 0.67
Batch: 160; loss: 1.3; acc: 0.62
Batch: 180; loss: 1.33; acc: 0.62
Batch: 200; loss: 1.24; acc: 0.61
Batch: 220; loss: 1.22; acc: 0.69
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.2; acc: 0.66
Batch: 280; loss: 1.19; acc: 0.64
Batch: 300; loss: 1.18; acc: 0.69
Batch: 320; loss: 1.46; acc: 0.52
Batch: 340; loss: 1.35; acc: 0.58
Batch: 360; loss: 1.25; acc: 0.66
Batch: 380; loss: 1.07; acc: 0.78
Batch: 400; loss: 1.2; acc: 0.62
Batch: 420; loss: 1.31; acc: 0.48
Batch: 440; loss: 1.13; acc: 0.72
Batch: 460; loss: 1.14; acc: 0.69
Batch: 480; loss: 1.27; acc: 0.64
Batch: 500; loss: 1.19; acc: 0.69
Batch: 520; loss: 1.22; acc: 0.7
Batch: 540; loss: 1.12; acc: 0.69
Batch: 560; loss: 1.33; acc: 0.66
Batch: 580; loss: 1.32; acc: 0.59
Batch: 600; loss: 1.33; acc: 0.61
Batch: 620; loss: 1.44; acc: 0.55
Batch: 640; loss: 1.41; acc: 0.5
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.37; acc: 0.56
Batch: 700; loss: 1.16; acc: 0.66
Batch: 720; loss: 1.12; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.77
Batch: 760; loss: 1.19; acc: 0.66
Batch: 780; loss: 1.13; acc: 0.73
Train Epoch over. train_loss: 1.21; train_accuracy: 0.66 

0.000140501739224419
0.0001327376812696457
Batch: 0; loss: 1.17; acc: 0.61
Batch: 20; loss: 1.39; acc: 0.56
Batch: 40; loss: 0.91; acc: 0.8
Batch: 60; loss: 1.15; acc: 0.73
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.23; acc: 0.7
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 1.04; acc: 0.75
Val Epoch over. val_loss: 1.1677205376564317; val_accuracy: 0.6846138535031847 

The current subspace-distance is: 0.0001327376812696457 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.78
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.19; acc: 0.66
Batch: 80; loss: 1.0; acc: 0.72
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.09; acc: 0.67
Batch: 160; loss: 1.25; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.66
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 1.21; acc: 0.66
Batch: 240; loss: 1.21; acc: 0.59
Batch: 260; loss: 1.12; acc: 0.7
Batch: 280; loss: 1.07; acc: 0.77
Batch: 300; loss: 1.26; acc: 0.56
Batch: 320; loss: 1.34; acc: 0.56
Batch: 340; loss: 1.22; acc: 0.67
Batch: 360; loss: 1.19; acc: 0.72
Batch: 380; loss: 1.11; acc: 0.73
Batch: 400; loss: 1.31; acc: 0.66
Batch: 420; loss: 1.31; acc: 0.62
Batch: 440; loss: 1.07; acc: 0.77
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.19; acc: 0.69
Batch: 500; loss: 1.17; acc: 0.66
Batch: 520; loss: 1.42; acc: 0.44
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.2; acc: 0.64
Batch: 580; loss: 1.17; acc: 0.64
Batch: 600; loss: 1.46; acc: 0.58
Batch: 620; loss: 1.29; acc: 0.62
Batch: 640; loss: 1.28; acc: 0.59
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.2; acc: 0.66
Batch: 700; loss: 1.07; acc: 0.69
Batch: 720; loss: 1.24; acc: 0.64
Batch: 740; loss: 1.15; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.69
Batch: 780; loss: 1.21; acc: 0.61
Train Epoch over. train_loss: 1.19; train_accuracy: 0.67 

0.00014236911374609917
0.0001348852674709633
Batch: 0; loss: 1.15; acc: 0.69
Batch: 20; loss: 1.39; acc: 0.58
Batch: 40; loss: 0.9; acc: 0.77
Batch: 60; loss: 1.14; acc: 0.75
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.2; acc: 0.73
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.04; acc: 0.75
Val Epoch over. val_loss: 1.157850293976486; val_accuracy: 0.6809315286624203 

The current subspace-distance is: 0.0001348852674709633 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.31; acc: 0.56
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 1.36; acc: 0.55
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 1.37; acc: 0.59
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.31; acc: 0.59
Batch: 180; loss: 1.14; acc: 0.77
Batch: 200; loss: 1.23; acc: 0.59
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 1.23; acc: 0.61
Batch: 260; loss: 1.23; acc: 0.61
Batch: 280; loss: 1.31; acc: 0.59
Batch: 300; loss: 1.08; acc: 0.7
Batch: 320; loss: 1.22; acc: 0.69
Batch: 340; loss: 1.22; acc: 0.61
Batch: 360; loss: 1.13; acc: 0.7
Batch: 380; loss: 1.15; acc: 0.67
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 1.24; acc: 0.58
Batch: 440; loss: 1.23; acc: 0.7
Batch: 460; loss: 0.97; acc: 0.83
Batch: 480; loss: 1.21; acc: 0.64
Batch: 500; loss: 1.11; acc: 0.7
Batch: 520; loss: 1.27; acc: 0.61
Batch: 540; loss: 1.11; acc: 0.73
Batch: 560; loss: 1.28; acc: 0.69
Batch: 580; loss: 1.17; acc: 0.69
Batch: 600; loss: 1.15; acc: 0.66
Batch: 620; loss: 1.16; acc: 0.67
Batch: 640; loss: 1.12; acc: 0.72
Batch: 660; loss: 1.17; acc: 0.67
Batch: 680; loss: 1.3; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.32; acc: 0.61
Batch: 740; loss: 1.13; acc: 0.66
Batch: 760; loss: 1.26; acc: 0.7
Batch: 780; loss: 1.22; acc: 0.62
Train Epoch over. train_loss: 1.19; train_accuracy: 0.67 

0.00014496795483864844
0.0001359266898361966
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 1.14; acc: 0.73
Batch: 80; loss: 1.09; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.77
Val Epoch over. val_loss: 1.15859943210699; val_accuracy: 0.6902866242038217 

The current subspace-distance is: 0.0001359266898361966 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.31; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.7
Batch: 40; loss: 1.2; acc: 0.72
Batch: 60; loss: 1.29; acc: 0.62
Batch: 80; loss: 1.38; acc: 0.52
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.18; acc: 0.7
Batch: 140; loss: 1.26; acc: 0.69
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.22; acc: 0.64
Batch: 200; loss: 1.23; acc: 0.62
Batch: 220; loss: 1.26; acc: 0.66
Batch: 240; loss: 1.18; acc: 0.72
Batch: 260; loss: 1.1; acc: 0.78
Batch: 280; loss: 1.21; acc: 0.69
Batch: 300; loss: 1.27; acc: 0.61
Batch: 320; loss: 1.19; acc: 0.67
Batch: 340; loss: 1.01; acc: 0.77
Batch: 360; loss: 1.34; acc: 0.58
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 1.31; acc: 0.62
Batch: 420; loss: 1.24; acc: 0.67
Batch: 440; loss: 1.07; acc: 0.72
Batch: 460; loss: 1.18; acc: 0.62
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 1.05; acc: 0.72
Batch: 520; loss: 1.11; acc: 0.7
Batch: 540; loss: 1.12; acc: 0.75
Batch: 560; loss: 1.35; acc: 0.61
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.15; acc: 0.7
Batch: 620; loss: 1.08; acc: 0.78
Batch: 640; loss: 1.37; acc: 0.48
Batch: 660; loss: 1.07; acc: 0.75
Batch: 680; loss: 1.35; acc: 0.64
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 1.15; acc: 0.7
Batch: 740; loss: 1.38; acc: 0.64
Batch: 760; loss: 1.17; acc: 0.72
Batch: 780; loss: 1.15; acc: 0.7
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.0001465402019675821
0.00013622594997286797
Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 1.37; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.77
Batch: 60; loss: 1.11; acc: 0.77
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 1.03; acc: 0.75
Val Epoch over. val_loss: 1.1406051459585784; val_accuracy: 0.6889928343949044 

The current subspace-distance is: 0.00013622594997286797 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.41; acc: 0.5
Batch: 20; loss: 1.1; acc: 0.77
Batch: 40; loss: 1.21; acc: 0.64
Batch: 60; loss: 0.88; acc: 0.89
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.21; acc: 0.62
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 1.05; acc: 0.72
Batch: 160; loss: 1.25; acc: 0.67
Batch: 180; loss: 1.23; acc: 0.58
Batch: 200; loss: 1.32; acc: 0.59
Batch: 220; loss: 1.14; acc: 0.72
Batch: 240; loss: 1.14; acc: 0.69
Batch: 260; loss: 1.12; acc: 0.69
Batch: 280; loss: 1.17; acc: 0.64
Batch: 300; loss: 1.27; acc: 0.66
Batch: 320; loss: 1.33; acc: 0.58
Batch: 340; loss: 1.17; acc: 0.62
Batch: 360; loss: 1.08; acc: 0.77
Batch: 380; loss: 1.08; acc: 0.7
Batch: 400; loss: 1.14; acc: 0.66
Batch: 420; loss: 1.03; acc: 0.77
Batch: 440; loss: 1.12; acc: 0.69
Batch: 460; loss: 1.19; acc: 0.62
Batch: 480; loss: 1.18; acc: 0.66
Batch: 500; loss: 1.22; acc: 0.59
Batch: 520; loss: 1.21; acc: 0.66
Batch: 540; loss: 1.15; acc: 0.67
Batch: 560; loss: 1.26; acc: 0.61
Batch: 580; loss: 1.18; acc: 0.64
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.1; acc: 0.75
Batch: 640; loss: 1.2; acc: 0.69
Batch: 660; loss: 1.13; acc: 0.67
Batch: 680; loss: 1.01; acc: 0.77
Batch: 700; loss: 1.17; acc: 0.69
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.29; acc: 0.62
Batch: 760; loss: 1.17; acc: 0.62
Batch: 780; loss: 1.02; acc: 0.75
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.0001478854101151228
0.0001403808273607865
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 1.11; acc: 0.77
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.02; acc: 0.8
Val Epoch over. val_loss: 1.141961923070774; val_accuracy: 0.6908837579617835 

The current subspace-distance is: 0.0001403808273607865 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.99; acc: 0.78
Batch: 20; loss: 0.99; acc: 0.77
Batch: 40; loss: 1.18; acc: 0.66
Batch: 60; loss: 1.2; acc: 0.61
Batch: 80; loss: 1.19; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.5
Batch: 140; loss: 1.08; acc: 0.77
Batch: 160; loss: 1.13; acc: 0.7
Batch: 180; loss: 1.31; acc: 0.59
Batch: 200; loss: 1.18; acc: 0.61
Batch: 220; loss: 1.34; acc: 0.62
Batch: 240; loss: 1.26; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.64
Batch: 280; loss: 1.23; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.62
Batch: 320; loss: 1.03; acc: 0.78
Batch: 340; loss: 1.08; acc: 0.73
Batch: 360; loss: 1.24; acc: 0.66
Batch: 380; loss: 1.2; acc: 0.69
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 1.16; acc: 0.64
Batch: 440; loss: 0.93; acc: 0.81
Batch: 460; loss: 1.09; acc: 0.75
Batch: 480; loss: 1.2; acc: 0.58
Batch: 500; loss: 1.12; acc: 0.7
Batch: 520; loss: 1.04; acc: 0.73
Batch: 540; loss: 1.19; acc: 0.66
Batch: 560; loss: 1.13; acc: 0.66
Batch: 580; loss: 1.18; acc: 0.64
Batch: 600; loss: 1.22; acc: 0.67
Batch: 620; loss: 1.25; acc: 0.67
Batch: 640; loss: 1.17; acc: 0.62
Batch: 660; loss: 1.22; acc: 0.66
Batch: 680; loss: 1.25; acc: 0.62
Batch: 700; loss: 1.18; acc: 0.62
Batch: 720; loss: 1.33; acc: 0.61
Batch: 740; loss: 1.27; acc: 0.56
Batch: 760; loss: 0.93; acc: 0.78
Batch: 780; loss: 1.0; acc: 0.78
Train Epoch over. train_loss: 1.17; train_accuracy: 0.67 

0.00015138692106120288
0.00014392782759387046
Batch: 0; loss: 1.1; acc: 0.69
Batch: 20; loss: 1.36; acc: 0.56
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.77
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 1.02; acc: 0.78
Val Epoch over. val_loss: 1.1290395457273836; val_accuracy: 0.691281847133758 

The current subspace-distance is: 0.00014392782759387046 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.23; acc: 0.64
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.75
Batch: 100; loss: 1.35; acc: 0.58
Batch: 120; loss: 1.27; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.78
Batch: 160; loss: 1.19; acc: 0.7
Batch: 180; loss: 1.17; acc: 0.66
Batch: 200; loss: 1.15; acc: 0.69
Batch: 220; loss: 1.06; acc: 0.7
Batch: 240; loss: 1.29; acc: 0.62
Batch: 260; loss: 1.08; acc: 0.7
Batch: 280; loss: 1.26; acc: 0.61
Batch: 300; loss: 1.07; acc: 0.75
Batch: 320; loss: 0.99; acc: 0.81
Batch: 340; loss: 1.17; acc: 0.64
Batch: 360; loss: 1.24; acc: 0.59
Batch: 380; loss: 1.31; acc: 0.69
Batch: 400; loss: 1.08; acc: 0.7
Batch: 420; loss: 1.16; acc: 0.69
Batch: 440; loss: 1.1; acc: 0.69
Batch: 460; loss: 1.13; acc: 0.69
Batch: 480; loss: 1.14; acc: 0.69
Batch: 500; loss: 1.14; acc: 0.7
Batch: 520; loss: 1.18; acc: 0.69
Batch: 540; loss: 1.03; acc: 0.78
Batch: 560; loss: 1.27; acc: 0.66
Batch: 580; loss: 1.16; acc: 0.67
Batch: 600; loss: 1.17; acc: 0.67
Batch: 620; loss: 1.02; acc: 0.77
Batch: 640; loss: 1.29; acc: 0.61
Batch: 660; loss: 1.46; acc: 0.56
Batch: 680; loss: 1.12; acc: 0.7
Batch: 700; loss: 1.2; acc: 0.67
Batch: 720; loss: 1.17; acc: 0.64
Batch: 740; loss: 1.26; acc: 0.64
Batch: 760; loss: 1.17; acc: 0.73
Batch: 780; loss: 1.08; acc: 0.73
Train Epoch over. train_loss: 1.17; train_accuracy: 0.67 

0.0001527759595774114
0.00014224740152712911
Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.35; acc: 0.56
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 1.09; acc: 0.75
Batch: 80; loss: 1.03; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 1.01; acc: 0.83
Val Epoch over. val_loss: 1.1222768930872535; val_accuracy: 0.6968550955414012 

The current subspace-distance is: 0.00014224740152712911 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 1.15; acc: 0.67
Batch: 60; loss: 1.1; acc: 0.66
Batch: 80; loss: 0.89; acc: 0.8
Batch: 100; loss: 1.34; acc: 0.53
Batch: 120; loss: 1.17; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.62
Batch: 160; loss: 1.24; acc: 0.58
Batch: 180; loss: 1.06; acc: 0.8
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 1.16; acc: 0.67
Batch: 240; loss: 1.22; acc: 0.69
Batch: 260; loss: 1.09; acc: 0.66
Batch: 280; loss: 1.22; acc: 0.61
Batch: 300; loss: 1.03; acc: 0.72
Batch: 320; loss: 1.32; acc: 0.56
Batch: 340; loss: 1.06; acc: 0.7
Batch: 360; loss: 1.14; acc: 0.7
Batch: 380; loss: 1.11; acc: 0.7
Batch: 400; loss: 1.38; acc: 0.52
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 1.1; acc: 0.72
Batch: 460; loss: 1.27; acc: 0.66
Batch: 480; loss: 1.12; acc: 0.73
Batch: 500; loss: 1.18; acc: 0.59
Batch: 520; loss: 1.02; acc: 0.77
Batch: 540; loss: 1.17; acc: 0.66
Batch: 560; loss: 1.26; acc: 0.64
Batch: 580; loss: 1.34; acc: 0.64
Batch: 600; loss: 1.26; acc: 0.59
Batch: 620; loss: 1.28; acc: 0.62
Batch: 640; loss: 1.08; acc: 0.72
Batch: 660; loss: 1.33; acc: 0.62
Batch: 680; loss: 1.04; acc: 0.75
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.12; acc: 0.73
Batch: 740; loss: 1.17; acc: 0.69
Batch: 760; loss: 1.09; acc: 0.72
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 1.16; train_accuracy: 0.67 

0.00015410307969432324
0.00014496524818241596
Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.36; acc: 0.58
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 1.1; acc: 0.75
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.16; acc: 0.66
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 1.01; acc: 0.83
Val Epoch over. val_loss: 1.129075399249982; val_accuracy: 0.6946656050955414 

The current subspace-distance is: 0.00014496524818241596 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.27; acc: 0.62
Batch: 40; loss: 1.1; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 1.25; acc: 0.61
Batch: 100; loss: 1.25; acc: 0.53
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.12; acc: 0.62
Batch: 160; loss: 1.05; acc: 0.77
Batch: 180; loss: 1.33; acc: 0.64
Batch: 200; loss: 1.25; acc: 0.7
Batch: 220; loss: 1.12; acc: 0.72
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.1; acc: 0.72
Batch: 280; loss: 1.15; acc: 0.72
Batch: 300; loss: 1.05; acc: 0.73
Batch: 320; loss: 1.06; acc: 0.7
Batch: 340; loss: 1.43; acc: 0.55
Batch: 360; loss: 1.17; acc: 0.67
Batch: 380; loss: 1.0; acc: 0.72
Batch: 400; loss: 1.21; acc: 0.66
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.13; acc: 0.69
Batch: 460; loss: 1.04; acc: 0.73
Batch: 480; loss: 1.13; acc: 0.69
Batch: 500; loss: 1.12; acc: 0.73
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.28; acc: 0.58
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 1.19; acc: 0.62
Batch: 600; loss: 1.27; acc: 0.62
Batch: 620; loss: 1.14; acc: 0.7
Batch: 640; loss: 1.01; acc: 0.77
Batch: 660; loss: 1.14; acc: 0.73
Batch: 680; loss: 1.04; acc: 0.72
Batch: 700; loss: 1.24; acc: 0.61
Batch: 720; loss: 1.01; acc: 0.75
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 1.18; acc: 0.72
Batch: 780; loss: 1.06; acc: 0.73
Train Epoch over. train_loss: 1.15; train_accuracy: 0.67 

0.00015549352974630892
0.0001476783218095079
Batch: 0; loss: 1.12; acc: 0.69
Batch: 20; loss: 1.36; acc: 0.56
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.73
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 1.01; acc: 0.83
Val Epoch over. val_loss: 1.1213259708349872; val_accuracy: 0.6956608280254777 

The current subspace-distance is: 0.0001476783218095079 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.61
Batch: 40; loss: 1.19; acc: 0.66
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.19; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.55
Batch: 140; loss: 1.08; acc: 0.75
Batch: 160; loss: 1.3; acc: 0.53
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 1.1; acc: 0.7
Batch: 220; loss: 1.04; acc: 0.75
Batch: 240; loss: 0.96; acc: 0.77
Batch: 260; loss: 1.24; acc: 0.55
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.16; acc: 0.66
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.11; acc: 0.73
Batch: 360; loss: 1.27; acc: 0.58
Batch: 380; loss: 1.23; acc: 0.59
Batch: 400; loss: 1.13; acc: 0.67
Batch: 420; loss: 1.16; acc: 0.73
Batch: 440; loss: 1.19; acc: 0.66
Batch: 460; loss: 1.12; acc: 0.67
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.09; acc: 0.72
Batch: 520; loss: 1.11; acc: 0.69
Batch: 540; loss: 1.16; acc: 0.64
Batch: 560; loss: 1.18; acc: 0.64
Batch: 580; loss: 1.13; acc: 0.7
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.16; acc: 0.67
Batch: 640; loss: 1.04; acc: 0.69
Batch: 660; loss: 1.14; acc: 0.67
Batch: 680; loss: 1.19; acc: 0.7
Batch: 700; loss: 1.2; acc: 0.73
Batch: 720; loss: 1.18; acc: 0.61
Batch: 740; loss: 1.32; acc: 0.59
Batch: 760; loss: 1.21; acc: 0.67
Batch: 780; loss: 1.11; acc: 0.69
Train Epoch over. train_loss: 1.15; train_accuracy: 0.67 

0.00015422362776007503
0.00014542342978529632
Batch: 0; loss: 1.07; acc: 0.77
Batch: 20; loss: 1.35; acc: 0.55
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.26; acc: 0.62
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.1069629431530168; val_accuracy: 0.6975517515923567 

The current subspace-distance is: 0.00014542342978529632 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.69
Batch: 40; loss: 1.24; acc: 0.53
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 1.16; acc: 0.67
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.73
Batch: 140; loss: 1.1; acc: 0.7
Batch: 160; loss: 1.19; acc: 0.72
Batch: 180; loss: 1.1; acc: 0.69
Batch: 200; loss: 1.23; acc: 0.64
Batch: 220; loss: 1.14; acc: 0.67
Batch: 240; loss: 1.16; acc: 0.67
Batch: 260; loss: 1.16; acc: 0.64
Batch: 280; loss: 1.24; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.7
Batch: 320; loss: 0.99; acc: 0.77
Batch: 340; loss: 1.22; acc: 0.67
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.11; acc: 0.72
Batch: 420; loss: 1.37; acc: 0.62
Batch: 440; loss: 1.16; acc: 0.69
Batch: 460; loss: 1.25; acc: 0.64
Batch: 480; loss: 1.09; acc: 0.73
Batch: 500; loss: 1.22; acc: 0.62
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 1.12; acc: 0.67
Batch: 560; loss: 1.08; acc: 0.73
Batch: 580; loss: 1.34; acc: 0.62
Batch: 600; loss: 1.26; acc: 0.62
Batch: 620; loss: 1.21; acc: 0.58
Batch: 640; loss: 1.35; acc: 0.56
Batch: 660; loss: 1.03; acc: 0.78
Batch: 680; loss: 1.21; acc: 0.64
Batch: 700; loss: 1.17; acc: 0.67
Batch: 720; loss: 1.25; acc: 0.61
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.15; acc: 0.67
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

0.0001577095390530303
0.0001492241135565564
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.35; acc: 0.53
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.09; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.69
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.1028749103758746; val_accuracy: 0.7015326433121019 

The current subspace-distance is: 0.0001492241135565564 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.61
Batch: 40; loss: 1.37; acc: 0.59
Batch: 60; loss: 1.29; acc: 0.61
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 1.01; acc: 0.78
Batch: 140; loss: 1.25; acc: 0.62
Batch: 160; loss: 1.19; acc: 0.59
Batch: 180; loss: 1.07; acc: 0.66
Batch: 200; loss: 1.14; acc: 0.69
Batch: 220; loss: 1.01; acc: 0.73
Batch: 240; loss: 1.14; acc: 0.75
Batch: 260; loss: 1.07; acc: 0.7
Batch: 280; loss: 0.96; acc: 0.83
Batch: 300; loss: 1.18; acc: 0.64
Batch: 320; loss: 1.05; acc: 0.75
Batch: 340; loss: 1.19; acc: 0.66
Batch: 360; loss: 1.1; acc: 0.72
Batch: 380; loss: 1.03; acc: 0.72
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 0.98; acc: 0.7
Batch: 440; loss: 1.06; acc: 0.72
Batch: 460; loss: 1.2; acc: 0.69
Batch: 480; loss: 0.99; acc: 0.72
Batch: 500; loss: 1.01; acc: 0.7
Batch: 520; loss: 1.27; acc: 0.61
Batch: 540; loss: 1.03; acc: 0.72
Batch: 560; loss: 1.05; acc: 0.7
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.04; acc: 0.77
Batch: 620; loss: 1.1; acc: 0.7
Batch: 640; loss: 1.15; acc: 0.67
Batch: 660; loss: 1.25; acc: 0.61
Batch: 680; loss: 0.97; acc: 0.84
Batch: 700; loss: 1.22; acc: 0.67
Batch: 720; loss: 1.24; acc: 0.56
Batch: 740; loss: 1.14; acc: 0.67
Batch: 760; loss: 1.15; acc: 0.66
Batch: 780; loss: 1.3; acc: 0.61
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00016118885832838714
0.00015417278336826712
Batch: 0; loss: 1.07; acc: 0.77
Batch: 20; loss: 1.35; acc: 0.53
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.0978961985581999; val_accuracy: 0.7025278662420382 

The current subspace-distance is: 0.00015417278336826712 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.81
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.16; acc: 0.62
Batch: 80; loss: 1.18; acc: 0.69
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 1.15; acc: 0.66
Batch: 160; loss: 1.11; acc: 0.64
Batch: 180; loss: 1.22; acc: 0.59
Batch: 200; loss: 1.17; acc: 0.66
Batch: 220; loss: 1.25; acc: 0.56
Batch: 240; loss: 0.98; acc: 0.78
Batch: 260; loss: 1.29; acc: 0.61
Batch: 280; loss: 1.09; acc: 0.69
Batch: 300; loss: 1.3; acc: 0.67
Batch: 320; loss: 1.07; acc: 0.77
Batch: 340; loss: 1.25; acc: 0.59
Batch: 360; loss: 1.07; acc: 0.72
Batch: 380; loss: 1.2; acc: 0.61
Batch: 400; loss: 1.16; acc: 0.67
Batch: 420; loss: 1.21; acc: 0.67
Batch: 440; loss: 1.03; acc: 0.67
Batch: 460; loss: 1.08; acc: 0.67
Batch: 480; loss: 1.13; acc: 0.64
Batch: 500; loss: 1.16; acc: 0.67
Batch: 520; loss: 0.99; acc: 0.75
Batch: 540; loss: 0.91; acc: 0.83
Batch: 560; loss: 1.19; acc: 0.64
Batch: 580; loss: 1.22; acc: 0.69
Batch: 600; loss: 1.04; acc: 0.69
Batch: 620; loss: 1.15; acc: 0.62
Batch: 640; loss: 1.03; acc: 0.72
Batch: 660; loss: 1.18; acc: 0.7
Batch: 680; loss: 1.09; acc: 0.67
Batch: 700; loss: 1.17; acc: 0.62
Batch: 720; loss: 1.11; acc: 0.72
Batch: 740; loss: 1.27; acc: 0.56
Batch: 760; loss: 0.96; acc: 0.78
Batch: 780; loss: 1.13; acc: 0.59
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.000162602387717925
0.00015354578499682248
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.35; acc: 0.56
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 1.1; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.72
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.99; acc: 0.81
Val Epoch over. val_loss: 1.1082509535892753; val_accuracy: 0.7027269108280255 

The current subspace-distance is: 0.00015354578499682248 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.69
Batch: 20; loss: 1.04; acc: 0.73
Batch: 40; loss: 1.06; acc: 0.8
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 0.99; acc: 0.72
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 1.08; acc: 0.66
Batch: 160; loss: 1.18; acc: 0.61
Batch: 180; loss: 1.24; acc: 0.64
Batch: 200; loss: 1.38; acc: 0.5
Batch: 220; loss: 1.04; acc: 0.69
Batch: 240; loss: 1.15; acc: 0.66
Batch: 260; loss: 1.07; acc: 0.75
Batch: 280; loss: 1.07; acc: 0.72
Batch: 300; loss: 1.15; acc: 0.69
Batch: 320; loss: 1.18; acc: 0.59
Batch: 340; loss: 1.08; acc: 0.72
Batch: 360; loss: 0.91; acc: 0.83
Batch: 380; loss: 1.17; acc: 0.59
Batch: 400; loss: 1.2; acc: 0.66
Batch: 420; loss: 1.25; acc: 0.59
Batch: 440; loss: 1.09; acc: 0.67
Batch: 460; loss: 1.07; acc: 0.67
Batch: 480; loss: 1.24; acc: 0.64
Batch: 500; loss: 1.06; acc: 0.72
Batch: 520; loss: 1.1; acc: 0.72
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 0.9; acc: 0.83
Batch: 580; loss: 1.13; acc: 0.67
Batch: 600; loss: 1.11; acc: 0.7
Batch: 620; loss: 1.03; acc: 0.7
Batch: 640; loss: 1.07; acc: 0.7
Batch: 660; loss: 1.07; acc: 0.72
Batch: 680; loss: 1.22; acc: 0.62
Batch: 700; loss: 1.11; acc: 0.61
Batch: 720; loss: 1.24; acc: 0.62
Batch: 740; loss: 1.08; acc: 0.7
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.28; acc: 0.58
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.0001600606628926471
0.00015102693578228354
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.34; acc: 0.56
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.02; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.81
Val Epoch over. val_loss: 1.100494582182283; val_accuracy: 0.7037221337579618 

The current subspace-distance is: 0.00015102693578228354 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 1.14; acc: 0.7
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.64
Batch: 100; loss: 1.09; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.67
Batch: 140; loss: 1.02; acc: 0.72
Batch: 160; loss: 1.23; acc: 0.66
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.15; acc: 0.7
Batch: 220; loss: 1.26; acc: 0.58
Batch: 240; loss: 1.2; acc: 0.66
Batch: 260; loss: 1.28; acc: 0.64
Batch: 280; loss: 1.2; acc: 0.64
Batch: 300; loss: 1.07; acc: 0.72
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.06; acc: 0.73
Batch: 360; loss: 1.08; acc: 0.67
Batch: 380; loss: 1.22; acc: 0.66
Batch: 400; loss: 1.16; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.72
Batch: 440; loss: 1.18; acc: 0.67
Batch: 460; loss: 1.21; acc: 0.62
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.15; acc: 0.64
Batch: 520; loss: 1.03; acc: 0.67
Batch: 540; loss: 1.18; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.03; acc: 0.72
Batch: 620; loss: 1.18; acc: 0.61
Batch: 640; loss: 1.18; acc: 0.73
Batch: 660; loss: 1.01; acc: 0.69
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.08; acc: 0.7
Batch: 720; loss: 1.18; acc: 0.64
Batch: 740; loss: 1.12; acc: 0.78
Batch: 760; loss: 1.02; acc: 0.77
Batch: 780; loss: 1.02; acc: 0.78
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00016267455066554248
0.00015484918549191207
Batch: 0; loss: 1.06; acc: 0.77
Batch: 20; loss: 1.34; acc: 0.56
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.7
Batch: 100; loss: 1.12; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.97; acc: 0.81
Val Epoch over. val_loss: 1.094198357527423; val_accuracy: 0.7043192675159236 

The current subspace-distance is: 0.00015484918549191207 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.18; acc: 0.61
Batch: 40; loss: 1.09; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 1.17; acc: 0.67
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 1.27; acc: 0.59
Batch: 160; loss: 1.26; acc: 0.64
Batch: 180; loss: 1.24; acc: 0.67
Batch: 200; loss: 1.18; acc: 0.69
Batch: 220; loss: 1.15; acc: 0.7
Batch: 240; loss: 1.08; acc: 0.69
Batch: 260; loss: 1.17; acc: 0.7
Batch: 280; loss: 1.15; acc: 0.64
Batch: 300; loss: 1.22; acc: 0.67
Batch: 320; loss: 1.05; acc: 0.7
Batch: 340; loss: 1.25; acc: 0.55
Batch: 360; loss: 1.17; acc: 0.7
Batch: 380; loss: 1.07; acc: 0.77
Batch: 400; loss: 1.08; acc: 0.67
Batch: 420; loss: 1.0; acc: 0.72
Batch: 440; loss: 1.16; acc: 0.67
Batch: 460; loss: 1.18; acc: 0.67
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 1.2; acc: 0.66
Batch: 520; loss: 1.26; acc: 0.59
Batch: 540; loss: 1.19; acc: 0.58
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.14; acc: 0.69
Batch: 600; loss: 1.12; acc: 0.69
Batch: 620; loss: 1.21; acc: 0.69
Batch: 640; loss: 1.26; acc: 0.59
Batch: 660; loss: 0.97; acc: 0.75
Batch: 680; loss: 1.2; acc: 0.62
Batch: 700; loss: 1.02; acc: 0.69
Batch: 720; loss: 1.06; acc: 0.69
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.07; acc: 0.77
Batch: 780; loss: 1.32; acc: 0.69
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00016047849203459918
0.0001518672361271456
Batch: 0; loss: 1.09; acc: 0.77
Batch: 20; loss: 1.38; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.99; acc: 0.81
Val Epoch over. val_loss: 1.1107654237443474; val_accuracy: 0.6965565286624203 

The current subspace-distance is: 0.0001518672361271456 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.06; acc: 0.73
Batch: 20; loss: 1.17; acc: 0.56
Batch: 40; loss: 1.38; acc: 0.64
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.1; acc: 0.72
Batch: 160; loss: 1.3; acc: 0.56
Batch: 180; loss: 1.13; acc: 0.73
Batch: 200; loss: 1.3; acc: 0.64
Batch: 220; loss: 1.14; acc: 0.67
Batch: 240; loss: 1.14; acc: 0.69
Batch: 260; loss: 1.09; acc: 0.75
Batch: 280; loss: 1.13; acc: 0.75
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.15; acc: 0.66
Batch: 340; loss: 0.97; acc: 0.78
Batch: 360; loss: 1.07; acc: 0.69
Batch: 380; loss: 1.46; acc: 0.53
Batch: 400; loss: 1.34; acc: 0.61
Batch: 420; loss: 1.11; acc: 0.67
Batch: 440; loss: 1.02; acc: 0.81
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 0.92; acc: 0.73
Batch: 500; loss: 1.05; acc: 0.7
Batch: 520; loss: 1.23; acc: 0.62
Batch: 540; loss: 1.1; acc: 0.73
Batch: 560; loss: 1.26; acc: 0.64
Batch: 580; loss: 1.19; acc: 0.59
Batch: 600; loss: 1.47; acc: 0.47
Batch: 620; loss: 1.1; acc: 0.69
Batch: 640; loss: 1.17; acc: 0.66
Batch: 660; loss: 1.19; acc: 0.62
Batch: 680; loss: 1.09; acc: 0.8
Batch: 700; loss: 1.27; acc: 0.59
Batch: 720; loss: 1.25; acc: 0.62
Batch: 740; loss: 1.05; acc: 0.72
Batch: 760; loss: 0.97; acc: 0.72
Batch: 780; loss: 1.1; acc: 0.64
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00016148018767125905
0.00015473051462322474
Batch: 0; loss: 1.08; acc: 0.77
Batch: 20; loss: 1.34; acc: 0.53
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.7
Batch: 100; loss: 1.12; acc: 0.67
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.81
Val Epoch over. val_loss: 1.1030058465945494; val_accuracy: 0.7046178343949044 

The current subspace-distance is: 0.00015473051462322474 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.2; acc: 0.62
Batch: 40; loss: 1.21; acc: 0.64
Batch: 60; loss: 1.1; acc: 0.69
Batch: 80; loss: 1.16; acc: 0.72
Batch: 100; loss: 1.18; acc: 0.62
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 1.08; acc: 0.67
Batch: 160; loss: 1.18; acc: 0.67
Batch: 180; loss: 1.04; acc: 0.77
Batch: 200; loss: 1.11; acc: 0.7
Batch: 220; loss: 1.23; acc: 0.66
Batch: 240; loss: 1.28; acc: 0.56
Batch: 260; loss: 1.16; acc: 0.66
Batch: 280; loss: 1.0; acc: 0.67
Batch: 300; loss: 1.31; acc: 0.52
Batch: 320; loss: 1.05; acc: 0.73
Batch: 340; loss: 1.06; acc: 0.75
Batch: 360; loss: 1.06; acc: 0.72
Batch: 380; loss: 1.15; acc: 0.62
Batch: 400; loss: 1.2; acc: 0.66
Batch: 420; loss: 1.17; acc: 0.67
Batch: 440; loss: 1.12; acc: 0.7
Batch: 460; loss: 0.95; acc: 0.77
Batch: 480; loss: 0.94; acc: 0.73
Batch: 500; loss: 1.11; acc: 0.7
Batch: 520; loss: 1.15; acc: 0.69
Batch: 540; loss: 1.07; acc: 0.69
Batch: 560; loss: 1.09; acc: 0.7
Batch: 580; loss: 1.15; acc: 0.64
Batch: 600; loss: 1.24; acc: 0.62
Batch: 620; loss: 1.03; acc: 0.75
Batch: 640; loss: 1.26; acc: 0.62
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 1.02; acc: 0.73
Batch: 700; loss: 1.06; acc: 0.73
Batch: 720; loss: 1.21; acc: 0.66
Batch: 740; loss: 0.91; acc: 0.75
Batch: 760; loss: 1.1; acc: 0.62
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00016242732817772776
0.00015719611837994307
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 1.35; acc: 0.55
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.81
Val Epoch over. val_loss: 1.0913932934688155; val_accuracy: 0.7046178343949044 

The current subspace-distance is: 0.00015719611837994307 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.7
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 1.11; acc: 0.73
Batch: 60; loss: 1.03; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.62
Batch: 100; loss: 1.25; acc: 0.67
Batch: 120; loss: 1.06; acc: 0.72
Batch: 140; loss: 1.27; acc: 0.59
Batch: 160; loss: 1.11; acc: 0.69
Batch: 180; loss: 0.96; acc: 0.75
Batch: 200; loss: 1.0; acc: 0.73
Batch: 220; loss: 1.21; acc: 0.62
Batch: 240; loss: 1.14; acc: 0.64
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.27; acc: 0.61
Batch: 320; loss: 1.23; acc: 0.61
Batch: 340; loss: 1.16; acc: 0.69
Batch: 360; loss: 1.06; acc: 0.7
Batch: 380; loss: 1.05; acc: 0.67
Batch: 400; loss: 1.05; acc: 0.75
Batch: 420; loss: 1.06; acc: 0.73
Batch: 440; loss: 1.07; acc: 0.72
Batch: 460; loss: 1.2; acc: 0.64
Batch: 480; loss: 1.29; acc: 0.56
Batch: 500; loss: 1.14; acc: 0.69
Batch: 520; loss: 1.14; acc: 0.75
Batch: 540; loss: 1.15; acc: 0.64
Batch: 560; loss: 1.14; acc: 0.72
Batch: 580; loss: 1.06; acc: 0.73
Batch: 600; loss: 1.14; acc: 0.69
Batch: 620; loss: 1.27; acc: 0.62
Batch: 640; loss: 1.17; acc: 0.72
Batch: 660; loss: 1.13; acc: 0.69
Batch: 680; loss: 1.15; acc: 0.64
Batch: 700; loss: 1.05; acc: 0.7
Batch: 720; loss: 0.99; acc: 0.78
Batch: 740; loss: 1.08; acc: 0.7
Batch: 760; loss: 1.23; acc: 0.64
Batch: 780; loss: 1.2; acc: 0.59
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00016490292910020798
0.00015457750123459846
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.37; acc: 0.52
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.66
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.1025525582064488; val_accuracy: 0.7026273885350318 

The current subspace-distance is: 0.00015457750123459846 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.67
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 1.09; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.12; acc: 0.67
Batch: 100; loss: 1.24; acc: 0.64
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 1.04; acc: 0.73
Batch: 160; loss: 0.95; acc: 0.83
Batch: 180; loss: 1.1; acc: 0.8
Batch: 200; loss: 0.94; acc: 0.78
Batch: 220; loss: 1.25; acc: 0.61
Batch: 240; loss: 0.98; acc: 0.8
Batch: 260; loss: 1.32; acc: 0.59
Batch: 280; loss: 1.03; acc: 0.72
Batch: 300; loss: 1.07; acc: 0.8
Batch: 320; loss: 1.13; acc: 0.8
Batch: 340; loss: 1.17; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.61
Batch: 380; loss: 1.07; acc: 0.69
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 1.1; acc: 0.66
Batch: 440; loss: 1.07; acc: 0.69
Batch: 460; loss: 1.08; acc: 0.69
Batch: 480; loss: 1.14; acc: 0.66
Batch: 500; loss: 1.3; acc: 0.69
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 1.16; acc: 0.67
Batch: 560; loss: 1.21; acc: 0.61
Batch: 580; loss: 1.26; acc: 0.59
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.22; acc: 0.61
Batch: 640; loss: 1.03; acc: 0.7
Batch: 660; loss: 1.2; acc: 0.69
Batch: 680; loss: 1.04; acc: 0.73
Batch: 700; loss: 1.31; acc: 0.61
Batch: 720; loss: 1.12; acc: 0.69
Batch: 740; loss: 0.94; acc: 0.8
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.02; acc: 0.73
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00016514475282747298
0.00015449376951437443
Batch: 0; loss: 1.05; acc: 0.78
Batch: 20; loss: 1.34; acc: 0.56
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.1; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.0906202739970698; val_accuracy: 0.7089968152866242 

The current subspace-distance is: 0.00015449376951437443 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.08; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.58
Batch: 100; loss: 1.21; acc: 0.66
Batch: 120; loss: 0.99; acc: 0.8
Batch: 140; loss: 1.05; acc: 0.72
Batch: 160; loss: 1.21; acc: 0.62
Batch: 180; loss: 1.11; acc: 0.7
Batch: 200; loss: 1.25; acc: 0.66
Batch: 220; loss: 1.14; acc: 0.64
Batch: 240; loss: 1.02; acc: 0.8
Batch: 260; loss: 1.18; acc: 0.55
Batch: 280; loss: 1.22; acc: 0.61
Batch: 300; loss: 1.07; acc: 0.69
Batch: 320; loss: 1.05; acc: 0.75
Batch: 340; loss: 1.06; acc: 0.72
Batch: 360; loss: 1.12; acc: 0.66
Batch: 380; loss: 1.15; acc: 0.61
Batch: 400; loss: 1.16; acc: 0.72
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.25; acc: 0.62
Batch: 480; loss: 1.22; acc: 0.67
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 1.17; acc: 0.7
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 1.09; acc: 0.7
Batch: 580; loss: 1.04; acc: 0.69
Batch: 600; loss: 1.2; acc: 0.64
Batch: 620; loss: 1.16; acc: 0.64
Batch: 640; loss: 1.13; acc: 0.61
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 1.18; acc: 0.58
Batch: 700; loss: 1.13; acc: 0.69
Batch: 720; loss: 1.41; acc: 0.5
Batch: 740; loss: 0.99; acc: 0.72
Batch: 760; loss: 1.09; acc: 0.73
Batch: 780; loss: 1.16; acc: 0.62
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00016490121197421104
0.00015693438763264567
Batch: 0; loss: 1.06; acc: 0.78
Batch: 20; loss: 1.35; acc: 0.53
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.69
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 0.96; acc: 0.81
Val Epoch over. val_loss: 1.0916477851806932; val_accuracy: 0.7079020700636943 

The current subspace-distance is: 0.00015693438763264567 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.75

The number of parameters is: 266969

The number of individual parameters is:

38
380
38
38
57
43320
57
57
114
129960
114
114
64
87552
64
64
4096
64
640
10
64
64

nonzero elements in E: 53393796
elements in E: 53393800
fraction nonzero: 0.999999925084935
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.09
Batch: 20; loss: 2.26; acc: 0.2
Batch: 40; loss: 2.09; acc: 0.38
Batch: 60; loss: 1.78; acc: 0.52
Batch: 80; loss: 1.93; acc: 0.33
Batch: 100; loss: 1.82; acc: 0.42
Batch: 120; loss: 1.91; acc: 0.41
Batch: 140; loss: 1.74; acc: 0.47
Batch: 160; loss: 1.77; acc: 0.52
Batch: 180; loss: 1.69; acc: 0.5
Batch: 200; loss: 1.74; acc: 0.52
Batch: 220; loss: 1.63; acc: 0.53
Batch: 240; loss: 1.63; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.5
Batch: 280; loss: 1.63; acc: 0.61
Batch: 300; loss: 1.59; acc: 0.56
Batch: 320; loss: 1.59; acc: 0.53
Batch: 340; loss: 1.49; acc: 0.66
Batch: 360; loss: 1.51; acc: 0.59
Batch: 380; loss: 1.54; acc: 0.59
Batch: 400; loss: 1.42; acc: 0.64
Batch: 420; loss: 1.55; acc: 0.53
Batch: 440; loss: 1.45; acc: 0.66
Batch: 460; loss: 1.48; acc: 0.62
Batch: 480; loss: 1.35; acc: 0.8
Batch: 500; loss: 1.51; acc: 0.67
Batch: 520; loss: 1.52; acc: 0.62
Batch: 540; loss: 1.45; acc: 0.62
Batch: 560; loss: 1.59; acc: 0.59
Batch: 580; loss: 1.51; acc: 0.59
Batch: 600; loss: 1.51; acc: 0.67
Batch: 620; loss: 1.4; acc: 0.69
Batch: 640; loss: 1.42; acc: 0.66
Batch: 660; loss: 1.36; acc: 0.69
Batch: 680; loss: 1.54; acc: 0.56
Batch: 700; loss: 1.42; acc: 0.67
Batch: 720; loss: 1.36; acc: 0.72
Batch: 740; loss: 1.37; acc: 0.69
Batch: 760; loss: 1.43; acc: 0.59
Batch: 780; loss: 1.44; acc: 0.66
Train Epoch over. train_loss: 1.61; train_accuracy: 0.57 

6.058435610611923e-05
5.3965570259606466e-05
Batch: 0; loss: 1.38; acc: 0.72
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.16; acc: 0.75
Batch: 60; loss: 1.32; acc: 0.66
Batch: 80; loss: 1.21; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.73
Batch: 120; loss: 1.45; acc: 0.64
Batch: 140; loss: 1.31; acc: 0.7
Val Epoch over. val_loss: 1.3649906117445345; val_accuracy: 0.6942675159235668 

The current subspace-distance is: 5.3965570259606466e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.75
Batch: 20; loss: 1.37; acc: 0.69
Batch: 40; loss: 1.43; acc: 0.62
Batch: 60; loss: 1.27; acc: 0.73
Batch: 80; loss: 1.31; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.8
Batch: 120; loss: 1.41; acc: 0.7
Batch: 140; loss: 1.46; acc: 0.66
Batch: 160; loss: 1.4; acc: 0.69
Batch: 180; loss: 1.36; acc: 0.66
Batch: 200; loss: 1.32; acc: 0.7
Batch: 220; loss: 1.35; acc: 0.7
Batch: 240; loss: 1.29; acc: 0.77
Batch: 260; loss: 1.28; acc: 0.75
Batch: 280; loss: 1.37; acc: 0.62
Batch: 300; loss: 1.22; acc: 0.8
Batch: 320; loss: 1.27; acc: 0.73
Batch: 340; loss: 1.27; acc: 0.77
Batch: 360; loss: 1.35; acc: 0.58
Batch: 380; loss: 1.22; acc: 0.8
Batch: 400; loss: 1.27; acc: 0.72
Batch: 420; loss: 1.42; acc: 0.59
Batch: 440; loss: 1.19; acc: 0.8
Batch: 460; loss: 1.39; acc: 0.66
Batch: 480; loss: 1.2; acc: 0.78
Batch: 500; loss: 1.31; acc: 0.7
Batch: 520; loss: 1.3; acc: 0.67
Batch: 540; loss: 1.38; acc: 0.64
Batch: 560; loss: 1.44; acc: 0.61
Batch: 580; loss: 1.33; acc: 0.7
Batch: 600; loss: 1.35; acc: 0.72
Batch: 620; loss: 1.38; acc: 0.64
Batch: 640; loss: 1.4; acc: 0.61
Batch: 660; loss: 1.24; acc: 0.75
Batch: 680; loss: 1.21; acc: 0.72
Batch: 700; loss: 1.4; acc: 0.59
Batch: 720; loss: 1.35; acc: 0.62
Batch: 740; loss: 1.33; acc: 0.66
Batch: 760; loss: 1.21; acc: 0.72
Batch: 780; loss: 1.42; acc: 0.58
Train Epoch over. train_loss: 1.34; train_accuracy: 0.68 

8.216140122385696e-05
7.698918489040807e-05
Batch: 0; loss: 1.28; acc: 0.73
Batch: 20; loss: 1.56; acc: 0.5
Batch: 40; loss: 1.01; acc: 0.8
Batch: 60; loss: 1.16; acc: 0.75
Batch: 80; loss: 1.06; acc: 0.78
Batch: 100; loss: 1.25; acc: 0.75
Batch: 120; loss: 1.39; acc: 0.62
Batch: 140; loss: 1.14; acc: 0.72
Val Epoch over. val_loss: 1.2252831318575865; val_accuracy: 0.7296974522292994 

The current subspace-distance is: 7.698918489040807e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.72
Batch: 20; loss: 1.25; acc: 0.72
Batch: 40; loss: 1.23; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.69
Batch: 80; loss: 1.29; acc: 0.72
Batch: 100; loss: 1.21; acc: 0.78
Batch: 120; loss: 1.34; acc: 0.66
Batch: 140; loss: 1.23; acc: 0.77
Batch: 160; loss: 1.07; acc: 0.83
Batch: 180; loss: 1.09; acc: 0.84
Batch: 200; loss: 1.2; acc: 0.67
Batch: 220; loss: 1.33; acc: 0.61
Batch: 240; loss: 1.29; acc: 0.64
Batch: 260; loss: 1.27; acc: 0.73
Batch: 280; loss: 1.27; acc: 0.7
Batch: 300; loss: 1.32; acc: 0.64
Batch: 320; loss: 1.19; acc: 0.72
Batch: 340; loss: 1.23; acc: 0.66
Batch: 360; loss: 1.02; acc: 0.92
Batch: 380; loss: 1.26; acc: 0.67
Batch: 400; loss: 1.15; acc: 0.78
Batch: 420; loss: 1.17; acc: 0.73
Batch: 440; loss: 1.23; acc: 0.75
Batch: 460; loss: 1.18; acc: 0.73
Batch: 480; loss: 1.13; acc: 0.78
Batch: 500; loss: 1.15; acc: 0.78
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 1.42; acc: 0.62
Batch: 560; loss: 1.22; acc: 0.72
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.77
Batch: 620; loss: 1.18; acc: 0.72
Batch: 640; loss: 1.11; acc: 0.75
Batch: 660; loss: 1.32; acc: 0.62
Batch: 680; loss: 1.13; acc: 0.77
Batch: 700; loss: 1.18; acc: 0.73
Batch: 720; loss: 1.15; acc: 0.75
Batch: 740; loss: 1.23; acc: 0.64
Batch: 760; loss: 1.13; acc: 0.78
Batch: 780; loss: 1.21; acc: 0.66
Train Epoch over. train_loss: 1.23; train_accuracy: 0.72 

9.738713561091572e-05
9.184450755128637e-05
Batch: 0; loss: 1.27; acc: 0.69
Batch: 20; loss: 1.44; acc: 0.56
Batch: 40; loss: 0.91; acc: 0.86
Batch: 60; loss: 1.05; acc: 0.8
Batch: 80; loss: 0.97; acc: 0.88
Batch: 100; loss: 1.16; acc: 0.78
Batch: 120; loss: 1.36; acc: 0.62
Batch: 140; loss: 1.06; acc: 0.78
Val Epoch over. val_loss: 1.1348229141751671; val_accuracy: 0.7608479299363057 

The current subspace-distance is: 9.184450755128637e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.72
Batch: 20; loss: 1.15; acc: 0.73
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.75
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.77
Batch: 140; loss: 1.18; acc: 0.67
Batch: 160; loss: 1.12; acc: 0.8
Batch: 180; loss: 1.16; acc: 0.72
Batch: 200; loss: 1.09; acc: 0.78
Batch: 220; loss: 1.21; acc: 0.66
Batch: 240; loss: 1.18; acc: 0.73
Batch: 260; loss: 1.07; acc: 0.83
Batch: 280; loss: 1.12; acc: 0.69
Batch: 300; loss: 1.17; acc: 0.72
Batch: 320; loss: 1.19; acc: 0.67
Batch: 340; loss: 1.11; acc: 0.83
Batch: 360; loss: 1.19; acc: 0.67
Batch: 380; loss: 1.13; acc: 0.69
Batch: 400; loss: 1.13; acc: 0.69
Batch: 420; loss: 1.18; acc: 0.69
Batch: 440; loss: 1.03; acc: 0.8
Batch: 460; loss: 1.19; acc: 0.69
Batch: 480; loss: 0.99; acc: 0.86
Batch: 500; loss: 1.1; acc: 0.73
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.15; acc: 0.72
Batch: 560; loss: 1.05; acc: 0.8
Batch: 580; loss: 1.04; acc: 0.77
Batch: 600; loss: 1.09; acc: 0.73
Batch: 620; loss: 1.04; acc: 0.78
Batch: 640; loss: 1.03; acc: 0.78
Batch: 660; loss: 1.02; acc: 0.77
Batch: 680; loss: 1.22; acc: 0.7
Batch: 700; loss: 1.0; acc: 0.8
Batch: 720; loss: 0.95; acc: 0.86
Batch: 740; loss: 1.18; acc: 0.69
Batch: 760; loss: 1.28; acc: 0.64
Batch: 780; loss: 1.02; acc: 0.77
Train Epoch over. train_loss: 1.14; train_accuracy: 0.74 

0.000113595204311423
0.00010769884829642251
Batch: 0; loss: 1.18; acc: 0.73
Batch: 20; loss: 1.33; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.84
Batch: 80; loss: 0.9; acc: 0.89
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.06886686185363; val_accuracy: 0.7684116242038217 

The current subspace-distance is: 0.00010769884829642251 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 1.03; acc: 0.75
Batch: 60; loss: 1.23; acc: 0.72
Batch: 80; loss: 1.14; acc: 0.64
Batch: 100; loss: 1.21; acc: 0.7
Batch: 120; loss: 1.06; acc: 0.84
Batch: 140; loss: 1.13; acc: 0.75
Batch: 160; loss: 1.09; acc: 0.8
Batch: 180; loss: 1.19; acc: 0.62
Batch: 200; loss: 1.05; acc: 0.75
Batch: 220; loss: 1.22; acc: 0.61
Batch: 240; loss: 1.3; acc: 0.62
Batch: 260; loss: 1.09; acc: 0.78
Batch: 280; loss: 1.06; acc: 0.81
Batch: 300; loss: 1.08; acc: 0.81
Batch: 320; loss: 1.04; acc: 0.8
Batch: 340; loss: 0.96; acc: 0.84
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.22; acc: 0.7
Batch: 400; loss: 1.11; acc: 0.7
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 0.97; acc: 0.77
Batch: 460; loss: 1.13; acc: 0.72
Batch: 480; loss: 1.05; acc: 0.81
Batch: 500; loss: 1.06; acc: 0.78
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.02; acc: 0.81
Batch: 560; loss: 0.9; acc: 0.84
Batch: 580; loss: 1.04; acc: 0.77
Batch: 600; loss: 0.95; acc: 0.83
Batch: 620; loss: 1.13; acc: 0.73
Batch: 640; loss: 1.03; acc: 0.77
Batch: 660; loss: 1.03; acc: 0.8
Batch: 680; loss: 1.05; acc: 0.72
Batch: 700; loss: 1.09; acc: 0.73
Batch: 720; loss: 1.12; acc: 0.69
Batch: 740; loss: 1.04; acc: 0.81
Batch: 760; loss: 0.96; acc: 0.78
Batch: 780; loss: 0.99; acc: 0.75
Train Epoch over. train_loss: 1.08; train_accuracy: 0.75 

0.00012308119039516896
0.00011770451965276152
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 0.75; acc: 0.91
Batch: 60; loss: 0.92; acc: 0.8
Batch: 80; loss: 0.81; acc: 0.88
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.61
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.0153860203020133; val_accuracy: 0.7784633757961783 

The current subspace-distance is: 0.00011770451965276152 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.75
Batch: 20; loss: 1.09; acc: 0.77
Batch: 40; loss: 1.13; acc: 0.72
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 0.98; acc: 0.81
Batch: 140; loss: 0.93; acc: 0.81
Batch: 160; loss: 1.14; acc: 0.7
Batch: 180; loss: 1.07; acc: 0.7
Batch: 200; loss: 1.1; acc: 0.77
Batch: 220; loss: 1.08; acc: 0.73
Batch: 240; loss: 1.02; acc: 0.8
Batch: 260; loss: 1.11; acc: 0.73
Batch: 280; loss: 0.99; acc: 0.8
Batch: 300; loss: 1.1; acc: 0.66
Batch: 320; loss: 1.18; acc: 0.7
Batch: 340; loss: 1.11; acc: 0.7
Batch: 360; loss: 1.04; acc: 0.77
Batch: 380; loss: 0.98; acc: 0.84
Batch: 400; loss: 1.14; acc: 0.78
Batch: 420; loss: 0.96; acc: 0.81
Batch: 440; loss: 1.1; acc: 0.72
Batch: 460; loss: 1.14; acc: 0.67
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 0.96; acc: 0.8
Batch: 520; loss: 0.94; acc: 0.86
Batch: 540; loss: 0.9; acc: 0.89
Batch: 560; loss: 0.95; acc: 0.78
Batch: 580; loss: 1.12; acc: 0.73
Batch: 600; loss: 1.05; acc: 0.75
Batch: 620; loss: 1.02; acc: 0.78
Batch: 640; loss: 1.03; acc: 0.78
Batch: 660; loss: 1.12; acc: 0.72
Batch: 680; loss: 1.02; acc: 0.75
Batch: 700; loss: 1.04; acc: 0.75
Batch: 720; loss: 1.01; acc: 0.77
Batch: 740; loss: 0.98; acc: 0.78
Batch: 760; loss: 1.05; acc: 0.73
Batch: 780; loss: 1.06; acc: 0.75
Train Epoch over. train_loss: 1.04; train_accuracy: 0.76 

0.00013310194481164217
0.00012708724534604698
Batch: 0; loss: 1.03; acc: 0.8
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 0.71; acc: 0.89
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 0.75; acc: 0.89
Batch: 100; loss: 1.05; acc: 0.8
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 0.89; acc: 0.8
Val Epoch over. val_loss: 0.9794045644960586; val_accuracy: 0.7859275477707006 

The current subspace-distance is: 0.00012708724534604698 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.77
Batch: 20; loss: 0.94; acc: 0.81
Batch: 40; loss: 1.06; acc: 0.72
Batch: 60; loss: 1.04; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.73
Batch: 120; loss: 0.98; acc: 0.81
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 0.91; acc: 0.83
Batch: 180; loss: 0.93; acc: 0.81
Batch: 200; loss: 1.13; acc: 0.73
Batch: 220; loss: 1.06; acc: 0.72
Batch: 240; loss: 0.93; acc: 0.8
Batch: 260; loss: 1.0; acc: 0.78
Batch: 280; loss: 0.93; acc: 0.84
Batch: 300; loss: 0.9; acc: 0.81
Batch: 320; loss: 0.86; acc: 0.81
Batch: 340; loss: 0.93; acc: 0.84
Batch: 360; loss: 0.96; acc: 0.81
Batch: 380; loss: 1.04; acc: 0.72
Batch: 400; loss: 0.76; acc: 0.95
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 0.99; acc: 0.78
Batch: 460; loss: 1.09; acc: 0.67
Batch: 480; loss: 0.94; acc: 0.83
Batch: 500; loss: 0.94; acc: 0.81
Batch: 520; loss: 0.91; acc: 0.83
Batch: 540; loss: 1.1; acc: 0.7
Batch: 560; loss: 1.03; acc: 0.73
Batch: 580; loss: 1.04; acc: 0.78
Batch: 600; loss: 0.86; acc: 0.83
Batch: 620; loss: 1.02; acc: 0.75
Batch: 640; loss: 0.97; acc: 0.75
Batch: 660; loss: 1.05; acc: 0.77
Batch: 680; loss: 0.98; acc: 0.77
Batch: 700; loss: 0.9; acc: 0.84
Batch: 720; loss: 0.96; acc: 0.8
Batch: 740; loss: 0.88; acc: 0.83
Batch: 760; loss: 0.95; acc: 0.81
Batch: 780; loss: 1.01; acc: 0.78
Train Epoch over. train_loss: 1.01; train_accuracy: 0.77 

0.00014254740381147712
0.00013480403868015856
Batch: 0; loss: 0.96; acc: 0.84
Batch: 20; loss: 1.16; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.88; acc: 0.81
Batch: 80; loss: 0.72; acc: 0.92
Batch: 100; loss: 0.99; acc: 0.83
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 0.84; acc: 0.81
Val Epoch over. val_loss: 0.9452933615939633; val_accuracy: 0.7989649681528662 

The current subspace-distance is: 0.00013480403868015856 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.75
Batch: 20; loss: 1.2; acc: 0.64
Batch: 40; loss: 1.03; acc: 0.78
Batch: 60; loss: 0.87; acc: 0.8
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 0.96; acc: 0.78
Batch: 120; loss: 1.03; acc: 0.75
Batch: 140; loss: 0.98; acc: 0.73
Batch: 160; loss: 0.99; acc: 0.78
Batch: 180; loss: 0.88; acc: 0.84
Batch: 200; loss: 1.09; acc: 0.69
Batch: 220; loss: 1.01; acc: 0.78
Batch: 240; loss: 1.01; acc: 0.77
Batch: 260; loss: 1.11; acc: 0.72
Batch: 280; loss: 0.88; acc: 0.8
Batch: 300; loss: 0.83; acc: 0.83
Batch: 320; loss: 1.02; acc: 0.8
Batch: 340; loss: 0.98; acc: 0.78
Batch: 360; loss: 1.16; acc: 0.72
Batch: 380; loss: 0.77; acc: 0.89
Batch: 400; loss: 0.92; acc: 0.78
Batch: 420; loss: 1.0; acc: 0.78
Batch: 440; loss: 0.87; acc: 0.8
Batch: 460; loss: 0.89; acc: 0.78
Batch: 480; loss: 0.95; acc: 0.81
Batch: 500; loss: 1.03; acc: 0.75
Batch: 520; loss: 0.96; acc: 0.83
Batch: 540; loss: 0.95; acc: 0.78
Batch: 560; loss: 0.83; acc: 0.83
Batch: 580; loss: 0.95; acc: 0.8
Batch: 600; loss: 0.97; acc: 0.77
Batch: 620; loss: 0.87; acc: 0.84
Batch: 640; loss: 0.89; acc: 0.84
Batch: 660; loss: 0.85; acc: 0.81
Batch: 680; loss: 0.9; acc: 0.77
Batch: 700; loss: 0.8; acc: 0.89
Batch: 720; loss: 0.88; acc: 0.8
Batch: 740; loss: 1.05; acc: 0.77
Batch: 760; loss: 1.07; acc: 0.77
Batch: 780; loss: 0.91; acc: 0.8
Train Epoch over. train_loss: 0.99; train_accuracy: 0.77 

0.00014708780508954078
0.00013950189168099314
Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 1.14; acc: 0.7
Batch: 40; loss: 0.64; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.92
Batch: 100; loss: 0.99; acc: 0.84
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.81; acc: 0.81
Val Epoch over. val_loss: 0.9207371473312378; val_accuracy: 0.79578025477707 

The current subspace-distance is: 0.00013950189168099314 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.86
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.93; acc: 0.8
Batch: 60; loss: 1.12; acc: 0.69
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.75; acc: 0.89
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 1.05; acc: 0.72
Batch: 160; loss: 0.88; acc: 0.8
Batch: 180; loss: 0.76; acc: 0.91
Batch: 200; loss: 1.19; acc: 0.62
Batch: 220; loss: 0.83; acc: 0.86
Batch: 240; loss: 0.86; acc: 0.81
Batch: 260; loss: 0.87; acc: 0.84
Batch: 280; loss: 1.01; acc: 0.73
Batch: 300; loss: 0.91; acc: 0.83
Batch: 320; loss: 0.99; acc: 0.78
Batch: 340; loss: 0.88; acc: 0.8
Batch: 360; loss: 1.01; acc: 0.73
Batch: 380; loss: 0.97; acc: 0.81
Batch: 400; loss: 1.06; acc: 0.69
Batch: 420; loss: 0.95; acc: 0.78
Batch: 440; loss: 1.12; acc: 0.73
Batch: 460; loss: 1.14; acc: 0.7
Batch: 480; loss: 0.85; acc: 0.84
Batch: 500; loss: 1.01; acc: 0.78
Batch: 520; loss: 0.95; acc: 0.77
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 0.92; acc: 0.83
Batch: 580; loss: 0.83; acc: 0.86
Batch: 600; loss: 0.94; acc: 0.78
Batch: 620; loss: 1.16; acc: 0.67
Batch: 640; loss: 1.15; acc: 0.64
Batch: 660; loss: 1.28; acc: 0.59
Batch: 680; loss: 0.9; acc: 0.77
Batch: 700; loss: 1.07; acc: 0.7
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 0.92; acc: 0.73
Batch: 760; loss: 1.02; acc: 0.8
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 0.97; train_accuracy: 0.77 

0.00015550768875982612
0.0001494915341027081
Batch: 0; loss: 0.9; acc: 0.84
Batch: 20; loss: 1.17; acc: 0.67
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.87; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.92
Batch: 100; loss: 1.0; acc: 0.81
Batch: 120; loss: 1.13; acc: 0.7
Batch: 140; loss: 0.81; acc: 0.81
Val Epoch over. val_loss: 0.9162514190764943; val_accuracy: 0.7980692675159236 

The current subspace-distance is: 0.0001494915341027081 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.84
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 0.95; acc: 0.75
Batch: 80; loss: 0.95; acc: 0.73
Batch: 100; loss: 0.86; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.94; acc: 0.77
Batch: 160; loss: 0.95; acc: 0.8
Batch: 180; loss: 1.01; acc: 0.8
Batch: 200; loss: 1.05; acc: 0.7
Batch: 220; loss: 0.97; acc: 0.73
Batch: 240; loss: 0.91; acc: 0.8
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 1.01; acc: 0.75
Batch: 300; loss: 0.88; acc: 0.77
Batch: 320; loss: 0.95; acc: 0.78
Batch: 340; loss: 0.95; acc: 0.77
Batch: 360; loss: 1.0; acc: 0.75
Batch: 380; loss: 0.79; acc: 0.83
Batch: 400; loss: 1.01; acc: 0.69
Batch: 420; loss: 0.91; acc: 0.81
Batch: 440; loss: 0.82; acc: 0.83
Batch: 460; loss: 0.96; acc: 0.75
Batch: 480; loss: 0.99; acc: 0.8
Batch: 500; loss: 1.01; acc: 0.72
Batch: 520; loss: 0.83; acc: 0.81
Batch: 540; loss: 1.0; acc: 0.81
Batch: 560; loss: 0.94; acc: 0.81
Batch: 580; loss: 0.9; acc: 0.75
Batch: 600; loss: 0.92; acc: 0.72
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 0.92; acc: 0.78
Batch: 660; loss: 1.04; acc: 0.7
Batch: 680; loss: 0.98; acc: 0.78
Batch: 700; loss: 0.93; acc: 0.78
Batch: 720; loss: 0.99; acc: 0.75
Batch: 740; loss: 0.9; acc: 0.75
Batch: 760; loss: 0.86; acc: 0.8
Batch: 780; loss: 0.82; acc: 0.78
Train Epoch over. train_loss: 0.94; train_accuracy: 0.77 

0.00016265503654722124
0.00015648895350750536
Batch: 0; loss: 0.83; acc: 0.88
Batch: 20; loss: 1.13; acc: 0.67
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.81
Batch: 80; loss: 0.64; acc: 0.92
Batch: 100; loss: 0.96; acc: 0.77
Batch: 120; loss: 1.07; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.84
Val Epoch over. val_loss: 0.8796035290523699; val_accuracy: 0.8046377388535032 

The current subspace-distance is: 0.00015648895350750536 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.86; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.83
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.81
Batch: 80; loss: 0.83; acc: 0.88
Batch: 100; loss: 1.02; acc: 0.7
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 1.03; acc: 0.7
Batch: 160; loss: 1.0; acc: 0.77
Batch: 180; loss: 0.83; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.92
Batch: 220; loss: 1.05; acc: 0.72
Batch: 240; loss: 0.99; acc: 0.8
Batch: 260; loss: 0.84; acc: 0.86
Batch: 280; loss: 0.79; acc: 0.89
Batch: 300; loss: 0.88; acc: 0.81
Batch: 320; loss: 1.0; acc: 0.75
Batch: 340; loss: 1.04; acc: 0.72
Batch: 360; loss: 0.93; acc: 0.78
Batch: 380; loss: 0.95; acc: 0.72
Batch: 400; loss: 0.96; acc: 0.81
Batch: 420; loss: 1.02; acc: 0.8
Batch: 440; loss: 0.91; acc: 0.86
Batch: 460; loss: 1.01; acc: 0.73
Batch: 480; loss: 0.91; acc: 0.72
Batch: 500; loss: 0.87; acc: 0.83
Batch: 520; loss: 0.91; acc: 0.73
Batch: 540; loss: 0.94; acc: 0.75
Batch: 560; loss: 1.04; acc: 0.75
Batch: 580; loss: 0.88; acc: 0.83
Batch: 600; loss: 0.98; acc: 0.77
Batch: 620; loss: 1.0; acc: 0.73
Batch: 640; loss: 0.94; acc: 0.75
Batch: 660; loss: 0.94; acc: 0.72
Batch: 680; loss: 0.97; acc: 0.7
Batch: 700; loss: 0.88; acc: 0.77
Batch: 720; loss: 0.95; acc: 0.8
Batch: 740; loss: 0.86; acc: 0.84
Batch: 760; loss: 0.86; acc: 0.75
Batch: 780; loss: 1.15; acc: 0.7
Train Epoch over. train_loss: 0.92; train_accuracy: 0.78 

0.0001675000530667603
0.0001602949487278238
Batch: 0; loss: 0.83; acc: 0.89
Batch: 20; loss: 1.14; acc: 0.67
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.64; acc: 0.91
Batch: 100; loss: 0.98; acc: 0.78
Batch: 120; loss: 1.07; acc: 0.75
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.883216458900719; val_accuracy: 0.8013535031847133 

The current subspace-distance is: 0.0001602949487278238 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.85; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.86
Batch: 40; loss: 0.82; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.84
Batch: 80; loss: 0.98; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.87; acc: 0.78
Batch: 160; loss: 0.95; acc: 0.77
Batch: 180; loss: 0.96; acc: 0.78
Batch: 200; loss: 0.92; acc: 0.75
Batch: 220; loss: 0.88; acc: 0.77
Batch: 240; loss: 0.87; acc: 0.77
Batch: 260; loss: 0.91; acc: 0.78
Batch: 280; loss: 0.83; acc: 0.8
Batch: 300; loss: 0.87; acc: 0.86
Batch: 320; loss: 0.96; acc: 0.73
Batch: 340; loss: 0.86; acc: 0.81
Batch: 360; loss: 0.97; acc: 0.8
Batch: 380; loss: 0.96; acc: 0.77
Batch: 400; loss: 0.89; acc: 0.75
Batch: 420; loss: 0.9; acc: 0.77
Batch: 440; loss: 1.12; acc: 0.75
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 0.94; acc: 0.75
Batch: 500; loss: 0.99; acc: 0.67
Batch: 520; loss: 0.92; acc: 0.78
Batch: 540; loss: 0.92; acc: 0.77
Batch: 560; loss: 0.83; acc: 0.84
Batch: 580; loss: 0.89; acc: 0.75
Batch: 600; loss: 0.99; acc: 0.8
Batch: 620; loss: 0.91; acc: 0.77
Batch: 640; loss: 1.05; acc: 0.78
Batch: 660; loss: 1.06; acc: 0.73
Batch: 680; loss: 0.87; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.83
Batch: 720; loss: 0.85; acc: 0.78
Batch: 740; loss: 0.88; acc: 0.8
Batch: 760; loss: 0.85; acc: 0.78
Batch: 780; loss: 0.93; acc: 0.75
Train Epoch over. train_loss: 0.92; train_accuracy: 0.78 

0.00016884042997844517
0.0001620030525373295
Batch: 0; loss: 0.79; acc: 0.91
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.94
Batch: 100; loss: 0.94; acc: 0.77
Batch: 120; loss: 1.04; acc: 0.73
Batch: 140; loss: 0.73; acc: 0.86
Val Epoch over. val_loss: 0.8589883613738285; val_accuracy: 0.8061305732484076 

The current subspace-distance is: 0.0001620030525373295 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.86; acc: 0.8
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.97; acc: 0.72
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 0.96; acc: 0.78
Batch: 100; loss: 1.01; acc: 0.7
Batch: 120; loss: 0.97; acc: 0.83
Batch: 140; loss: 0.86; acc: 0.83
Batch: 160; loss: 0.95; acc: 0.72
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.83; acc: 0.83
Batch: 220; loss: 1.09; acc: 0.77
Batch: 240; loss: 0.95; acc: 0.86
Batch: 260; loss: 1.03; acc: 0.69
Batch: 280; loss: 0.95; acc: 0.81
Batch: 300; loss: 0.87; acc: 0.75
Batch: 320; loss: 0.96; acc: 0.8
Batch: 340; loss: 1.07; acc: 0.66
Batch: 360; loss: 0.86; acc: 0.8
Batch: 380; loss: 0.8; acc: 0.78
Batch: 400; loss: 0.84; acc: 0.8
Batch: 420; loss: 1.08; acc: 0.72
Batch: 440; loss: 0.75; acc: 0.91
Batch: 460; loss: 1.03; acc: 0.7
Batch: 480; loss: 0.98; acc: 0.78
Batch: 500; loss: 0.82; acc: 0.84
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.78; acc: 0.8
Batch: 560; loss: 0.95; acc: 0.75
Batch: 580; loss: 1.13; acc: 0.62
Batch: 600; loss: 0.91; acc: 0.77
Batch: 620; loss: 0.77; acc: 0.81
Batch: 640; loss: 1.05; acc: 0.7
Batch: 660; loss: 0.9; acc: 0.83
Batch: 680; loss: 0.79; acc: 0.83
Batch: 700; loss: 1.01; acc: 0.7
Batch: 720; loss: 1.0; acc: 0.7
Batch: 740; loss: 0.68; acc: 0.88
Batch: 760; loss: 0.76; acc: 0.86
Batch: 780; loss: 0.91; acc: 0.77
Train Epoch over. train_loss: 0.91; train_accuracy: 0.78 

0.00016963179223239422
0.00016429064271505922
Batch: 0; loss: 0.78; acc: 0.89
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.92
Batch: 100; loss: 0.94; acc: 0.77
Batch: 120; loss: 1.02; acc: 0.73
Batch: 140; loss: 0.72; acc: 0.86
Val Epoch over. val_loss: 0.8519933576796465; val_accuracy: 0.8067277070063694 

The current subspace-distance is: 0.00016429064271505922 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.91; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.84
Batch: 40; loss: 0.94; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.75
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 0.88; acc: 0.83
Batch: 160; loss: 0.95; acc: 0.81
Batch: 180; loss: 0.97; acc: 0.75
Batch: 200; loss: 0.93; acc: 0.77
Batch: 220; loss: 0.78; acc: 0.91
Batch: 240; loss: 0.81; acc: 0.78
Batch: 260; loss: 0.79; acc: 0.83
Batch: 280; loss: 1.03; acc: 0.7
Batch: 300; loss: 0.85; acc: 0.83
Batch: 320; loss: 0.89; acc: 0.77
Batch: 340; loss: 0.87; acc: 0.81
Batch: 360; loss: 0.82; acc: 0.83
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.84; acc: 0.84
Batch: 420; loss: 0.83; acc: 0.81
Batch: 440; loss: 0.97; acc: 0.75
Batch: 460; loss: 0.84; acc: 0.84
Batch: 480; loss: 0.99; acc: 0.73
Batch: 500; loss: 0.96; acc: 0.73
Batch: 520; loss: 0.88; acc: 0.8
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 0.73; acc: 0.88
Batch: 580; loss: 0.77; acc: 0.86
Batch: 600; loss: 0.93; acc: 0.78
Batch: 620; loss: 0.92; acc: 0.81
Batch: 640; loss: 0.81; acc: 0.86
Batch: 660; loss: 0.84; acc: 0.73
Batch: 680; loss: 0.96; acc: 0.75
Batch: 700; loss: 0.88; acc: 0.81
Batch: 720; loss: 0.77; acc: 0.84
Batch: 740; loss: 0.93; acc: 0.73
Batch: 760; loss: 0.76; acc: 0.83
Batch: 780; loss: 0.86; acc: 0.78
Train Epoch over. train_loss: 0.9; train_accuracy: 0.78 

0.0001723950554151088
0.00016575002518948168
Batch: 0; loss: 0.78; acc: 0.91
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.91
Batch: 100; loss: 0.95; acc: 0.78
Batch: 120; loss: 1.03; acc: 0.75
Batch: 140; loss: 0.73; acc: 0.84
Val Epoch over. val_loss: 0.8581872612807402; val_accuracy: 0.7999601910828026 

The current subspace-distance is: 0.00016575002518948168 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.83
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.83
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.88; acc: 0.77
Batch: 180; loss: 0.95; acc: 0.78
Batch: 200; loss: 0.75; acc: 0.88
Batch: 220; loss: 0.97; acc: 0.78
Batch: 240; loss: 1.0; acc: 0.75
Batch: 260; loss: 0.82; acc: 0.84
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 0.85; acc: 0.81
Batch: 320; loss: 0.84; acc: 0.77
Batch: 340; loss: 0.87; acc: 0.81
Batch: 360; loss: 0.96; acc: 0.77
Batch: 380; loss: 0.87; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 1.16; acc: 0.67
Batch: 440; loss: 0.9; acc: 0.8
Batch: 460; loss: 1.04; acc: 0.7
Batch: 480; loss: 0.91; acc: 0.8
Batch: 500; loss: 0.83; acc: 0.84
Batch: 520; loss: 1.08; acc: 0.69
Batch: 540; loss: 0.96; acc: 0.78
Batch: 560; loss: 0.89; acc: 0.8
Batch: 580; loss: 0.83; acc: 0.8
Batch: 600; loss: 0.92; acc: 0.78
Batch: 620; loss: 0.91; acc: 0.77
Batch: 640; loss: 0.94; acc: 0.73
Batch: 660; loss: 0.91; acc: 0.77
Batch: 680; loss: 0.97; acc: 0.67
Batch: 700; loss: 0.85; acc: 0.88
Batch: 720; loss: 1.0; acc: 0.78
Batch: 740; loss: 0.8; acc: 0.84
Batch: 760; loss: 0.89; acc: 0.75
Batch: 780; loss: 0.94; acc: 0.75
Train Epoch over. train_loss: 0.89; train_accuracy: 0.78 

0.0001752315874909982
0.0001691238139756024
Batch: 0; loss: 0.74; acc: 0.91
Batch: 20; loss: 1.09; acc: 0.66
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 0.7; acc: 0.86
Val Epoch over. val_loss: 0.8268692614925894; val_accuracy: 0.8058320063694268 

The current subspace-distance is: 0.0001691238139756024 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.98; acc: 0.75
Batch: 20; loss: 1.01; acc: 0.77
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.98; acc: 0.73
Batch: 100; loss: 0.84; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.91; acc: 0.78
Batch: 160; loss: 0.8; acc: 0.81
Batch: 180; loss: 1.04; acc: 0.75
Batch: 200; loss: 0.85; acc: 0.83
Batch: 220; loss: 0.94; acc: 0.78
Batch: 240; loss: 0.8; acc: 0.83
Batch: 260; loss: 0.86; acc: 0.81
Batch: 280; loss: 0.77; acc: 0.86
Batch: 300; loss: 0.89; acc: 0.75
Batch: 320; loss: 0.95; acc: 0.77
Batch: 340; loss: 1.16; acc: 0.66
Batch: 360; loss: 0.82; acc: 0.83
Batch: 380; loss: 0.9; acc: 0.83
Batch: 400; loss: 0.94; acc: 0.75
Batch: 420; loss: 0.81; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.86
Batch: 460; loss: 0.87; acc: 0.75
Batch: 480; loss: 0.81; acc: 0.86
Batch: 500; loss: 1.02; acc: 0.7
Batch: 520; loss: 1.08; acc: 0.7
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 0.96; acc: 0.77
Batch: 580; loss: 0.89; acc: 0.8
Batch: 600; loss: 0.96; acc: 0.72
Batch: 620; loss: 0.88; acc: 0.8
Batch: 640; loss: 0.84; acc: 0.81
Batch: 660; loss: 0.81; acc: 0.81
Batch: 680; loss: 0.85; acc: 0.84
Batch: 700; loss: 0.79; acc: 0.83
Batch: 720; loss: 0.95; acc: 0.78
Batch: 740; loss: 0.92; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.91
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 0.88; train_accuracy: 0.78 

0.00017523253336548805
0.00016950961435213685
Batch: 0; loss: 0.76; acc: 0.89
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.76; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.93; acc: 0.78
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.86
Val Epoch over. val_loss: 0.82991415452046; val_accuracy: 0.805234872611465 

The current subspace-distance is: 0.00016950961435213685 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.78
Batch: 80; loss: 0.83; acc: 0.78
Batch: 100; loss: 0.92; acc: 0.75
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.7; acc: 0.92
Batch: 160; loss: 0.87; acc: 0.77
Batch: 180; loss: 0.95; acc: 0.78
Batch: 200; loss: 0.87; acc: 0.83
Batch: 220; loss: 0.95; acc: 0.69
Batch: 240; loss: 0.83; acc: 0.83
Batch: 260; loss: 0.88; acc: 0.73
Batch: 280; loss: 0.84; acc: 0.83
Batch: 300; loss: 0.8; acc: 0.84
Batch: 320; loss: 0.84; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 0.96; acc: 0.77
Batch: 420; loss: 0.81; acc: 0.84
Batch: 440; loss: 0.85; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.8
Batch: 480; loss: 1.0; acc: 0.73
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.8; acc: 0.88
Batch: 540; loss: 0.95; acc: 0.78
Batch: 560; loss: 0.99; acc: 0.66
Batch: 580; loss: 0.87; acc: 0.78
Batch: 600; loss: 0.79; acc: 0.81
Batch: 620; loss: 0.93; acc: 0.77
Batch: 640; loss: 0.97; acc: 0.75
Batch: 660; loss: 0.94; acc: 0.69
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 0.87; acc: 0.77
Batch: 720; loss: 0.81; acc: 0.81
Batch: 740; loss: 0.97; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.92; acc: 0.77
Train Epoch over. train_loss: 0.87; train_accuracy: 0.78 

0.00018255972827319056
0.00017300245235674083
Batch: 0; loss: 0.73; acc: 0.92
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.88
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 0.67; acc: 0.86
Val Epoch over. val_loss: 0.8122257039805126; val_accuracy: 0.8118033439490446 

The current subspace-distance is: 0.00017300245235674083 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 0.87; acc: 0.8
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 0.72; acc: 0.86
Batch: 100; loss: 0.93; acc: 0.69
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.81
Batch: 160; loss: 0.93; acc: 0.77
Batch: 180; loss: 0.94; acc: 0.75
Batch: 200; loss: 1.02; acc: 0.64
Batch: 220; loss: 0.63; acc: 0.89
Batch: 240; loss: 0.83; acc: 0.77
Batch: 260; loss: 1.05; acc: 0.67
Batch: 280; loss: 0.94; acc: 0.69
Batch: 300; loss: 0.85; acc: 0.75
Batch: 320; loss: 0.87; acc: 0.83
Batch: 340; loss: 0.77; acc: 0.81
Batch: 360; loss: 1.01; acc: 0.72
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.88; acc: 0.73
Batch: 420; loss: 0.92; acc: 0.7
Batch: 440; loss: 1.01; acc: 0.69
Batch: 460; loss: 0.92; acc: 0.75
Batch: 480; loss: 0.81; acc: 0.8
Batch: 500; loss: 0.68; acc: 0.91
Batch: 520; loss: 0.96; acc: 0.75
Batch: 540; loss: 0.93; acc: 0.77
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.89; acc: 0.77
Batch: 600; loss: 0.91; acc: 0.73
Batch: 620; loss: 0.84; acc: 0.81
Batch: 640; loss: 0.97; acc: 0.8
Batch: 660; loss: 0.83; acc: 0.83
Batch: 680; loss: 0.83; acc: 0.77
Batch: 700; loss: 0.92; acc: 0.75
Batch: 720; loss: 0.86; acc: 0.8
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.89; acc: 0.8
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.87; train_accuracy: 0.78 

0.0001808284578146413
0.00017585462774150074
Batch: 0; loss: 0.71; acc: 0.91
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.86
Val Epoch over. val_loss: 0.8136501889319936; val_accuracy: 0.8072253184713376 

The current subspace-distance is: 0.00017585462774150074 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.05; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 1.12; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.95; acc: 0.7
Batch: 160; loss: 0.84; acc: 0.78
Batch: 180; loss: 0.83; acc: 0.8
Batch: 200; loss: 1.03; acc: 0.64
Batch: 220; loss: 0.96; acc: 0.69
Batch: 240; loss: 0.68; acc: 0.91
Batch: 260; loss: 0.85; acc: 0.78
Batch: 280; loss: 0.83; acc: 0.72
Batch: 300; loss: 1.01; acc: 0.75
Batch: 320; loss: 0.85; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.81
Batch: 360; loss: 0.84; acc: 0.83
Batch: 380; loss: 0.89; acc: 0.81
Batch: 400; loss: 0.96; acc: 0.73
Batch: 420; loss: 0.83; acc: 0.73
Batch: 440; loss: 0.68; acc: 0.88
Batch: 460; loss: 0.87; acc: 0.8
Batch: 480; loss: 0.91; acc: 0.77
Batch: 500; loss: 0.98; acc: 0.78
Batch: 520; loss: 0.87; acc: 0.77
Batch: 540; loss: 0.91; acc: 0.72
Batch: 560; loss: 0.64; acc: 0.89
Batch: 580; loss: 0.86; acc: 0.83
Batch: 600; loss: 0.9; acc: 0.78
Batch: 620; loss: 0.79; acc: 0.78
Batch: 640; loss: 0.88; acc: 0.75
Batch: 660; loss: 0.84; acc: 0.83
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 0.88; acc: 0.7
Batch: 720; loss: 0.88; acc: 0.86
Batch: 740; loss: 0.88; acc: 0.73
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 0.95; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.79 

0.00018686622206587344
0.00017989780462812632
Batch: 0; loss: 0.69; acc: 0.91
Batch: 20; loss: 1.08; acc: 0.67
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.86
Val Epoch over. val_loss: 0.7970903876480783; val_accuracy: 0.8132961783439491 

The current subspace-distance is: 0.00017989780462812632 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 0.91; acc: 0.75
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.77
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.71; acc: 0.89
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 0.8; acc: 0.78
Batch: 240; loss: 0.83; acc: 0.8
Batch: 260; loss: 0.91; acc: 0.77
Batch: 280; loss: 0.95; acc: 0.75
Batch: 300; loss: 0.84; acc: 0.78
Batch: 320; loss: 0.79; acc: 0.86
Batch: 340; loss: 0.93; acc: 0.77
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.87; acc: 0.75
Batch: 400; loss: 0.9; acc: 0.72
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.99; acc: 0.69
Batch: 460; loss: 0.92; acc: 0.73
Batch: 480; loss: 0.89; acc: 0.78
Batch: 500; loss: 0.84; acc: 0.77
Batch: 520; loss: 0.66; acc: 0.86
Batch: 540; loss: 0.83; acc: 0.81
Batch: 560; loss: 0.89; acc: 0.72
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.85; acc: 0.78
Batch: 620; loss: 0.83; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.94
Batch: 660; loss: 0.72; acc: 0.84
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.98; acc: 0.72
Batch: 720; loss: 0.89; acc: 0.81
Batch: 740; loss: 0.86; acc: 0.77
Batch: 760; loss: 1.01; acc: 0.72
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

0.00018609898688737303
0.00017924887652043253
Batch: 0; loss: 0.69; acc: 0.91
Batch: 20; loss: 1.08; acc: 0.69
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.67; acc: 0.86
Val Epoch over. val_loss: 0.7971625240745058; val_accuracy: 0.8123009554140127 

The current subspace-distance is: 0.00017924887652043253 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.79; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.92; acc: 0.75
Batch: 60; loss: 0.86; acc: 0.72
Batch: 80; loss: 0.84; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.86; acc: 0.8
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.83
Batch: 200; loss: 0.79; acc: 0.83
Batch: 220; loss: 0.85; acc: 0.73
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.81; acc: 0.77
Batch: 280; loss: 0.91; acc: 0.75
Batch: 300; loss: 0.8; acc: 0.83
Batch: 320; loss: 0.85; acc: 0.81
Batch: 340; loss: 0.73; acc: 0.81
Batch: 360; loss: 0.99; acc: 0.75
Batch: 380; loss: 0.81; acc: 0.78
Batch: 400; loss: 1.01; acc: 0.73
Batch: 420; loss: 0.87; acc: 0.78
Batch: 440; loss: 0.84; acc: 0.83
Batch: 460; loss: 0.82; acc: 0.77
Batch: 480; loss: 0.82; acc: 0.83
Batch: 500; loss: 0.82; acc: 0.77
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.86; acc: 0.83
Batch: 580; loss: 0.79; acc: 0.83
Batch: 600; loss: 0.98; acc: 0.73
Batch: 620; loss: 0.83; acc: 0.78
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.96; acc: 0.75
Batch: 680; loss: 1.0; acc: 0.75
Batch: 700; loss: 0.81; acc: 0.81
Batch: 720; loss: 0.83; acc: 0.83
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.9; acc: 0.73
Batch: 780; loss: 0.85; acc: 0.75
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

0.00018740784435067326
0.00018012466898653656
Batch: 0; loss: 0.7; acc: 0.91
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.77
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.88
Val Epoch over. val_loss: 0.7920933798619896; val_accuracy: 0.8125 

The current subspace-distance is: 0.00018012466898653656 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.86
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.86; acc: 0.77
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 0.85; acc: 0.73
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 0.86; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.84
Batch: 160; loss: 0.79; acc: 0.75
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.86; acc: 0.78
Batch: 220; loss: 1.0; acc: 0.78
Batch: 240; loss: 0.73; acc: 0.91
Batch: 260; loss: 0.83; acc: 0.8
Batch: 280; loss: 0.85; acc: 0.77
Batch: 300; loss: 0.86; acc: 0.7
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.86; acc: 0.77
Batch: 360; loss: 0.87; acc: 0.81
Batch: 380; loss: 0.98; acc: 0.75
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.85; acc: 0.73
Batch: 440; loss: 0.8; acc: 0.84
Batch: 460; loss: 0.81; acc: 0.81
Batch: 480; loss: 0.89; acc: 0.77
Batch: 500; loss: 1.01; acc: 0.72
Batch: 520; loss: 0.9; acc: 0.69
Batch: 540; loss: 0.82; acc: 0.81
Batch: 560; loss: 0.93; acc: 0.78
Batch: 580; loss: 0.89; acc: 0.81
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.79; acc: 0.84
Batch: 640; loss: 0.99; acc: 0.69
Batch: 660; loss: 0.96; acc: 0.8
Batch: 680; loss: 0.82; acc: 0.81
Batch: 700; loss: 0.79; acc: 0.73
Batch: 720; loss: 0.86; acc: 0.8
Batch: 740; loss: 1.16; acc: 0.72
Batch: 760; loss: 0.76; acc: 0.84
Batch: 780; loss: 0.88; acc: 0.8
Train Epoch over. train_loss: 0.84; train_accuracy: 0.79 

0.0001904410746647045
0.00018261971126776189
Batch: 0; loss: 0.69; acc: 0.91
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.89; acc: 0.77
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 0.67; acc: 0.86
Val Epoch over. val_loss: 0.7979349163687153; val_accuracy: 0.8088176751592356 

The current subspace-distance is: 0.00018261971126776189 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.94; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.7
Batch: 40; loss: 0.86; acc: 0.83
Batch: 60; loss: 0.94; acc: 0.75
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.86
Batch: 140; loss: 0.99; acc: 0.72
Batch: 160; loss: 0.76; acc: 0.84
Batch: 180; loss: 0.86; acc: 0.8
Batch: 200; loss: 0.72; acc: 0.86
Batch: 220; loss: 0.96; acc: 0.72
Batch: 240; loss: 0.82; acc: 0.81
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.83; acc: 0.8
Batch: 300; loss: 0.82; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.83
Batch: 340; loss: 0.78; acc: 0.84
Batch: 360; loss: 0.84; acc: 0.78
Batch: 380; loss: 0.92; acc: 0.75
Batch: 400; loss: 0.91; acc: 0.81
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.73; acc: 0.89
Batch: 460; loss: 0.81; acc: 0.78
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 0.94; acc: 0.72
Batch: 520; loss: 0.85; acc: 0.72
Batch: 540; loss: 1.0; acc: 0.7
Batch: 560; loss: 0.96; acc: 0.78
Batch: 580; loss: 0.9; acc: 0.75
Batch: 600; loss: 0.94; acc: 0.84
Batch: 620; loss: 0.89; acc: 0.81
Batch: 640; loss: 0.81; acc: 0.77
Batch: 660; loss: 0.79; acc: 0.77
Batch: 680; loss: 0.78; acc: 0.8
Batch: 700; loss: 0.85; acc: 0.72
Batch: 720; loss: 1.0; acc: 0.67
Batch: 740; loss: 0.88; acc: 0.75
Batch: 760; loss: 0.84; acc: 0.81
Batch: 780; loss: 0.85; acc: 0.83
Train Epoch over. train_loss: 0.84; train_accuracy: 0.79 

0.00018829274631571025
0.00018214256851933897
Batch: 0; loss: 0.69; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.86
Val Epoch over. val_loss: 0.7885294336422234; val_accuracy: 0.8149880573248408 

The current subspace-distance is: 0.00018214256851933897 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.93; acc: 0.7
Batch: 20; loss: 0.97; acc: 0.75
Batch: 40; loss: 0.97; acc: 0.72
Batch: 60; loss: 0.92; acc: 0.7
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 0.89; acc: 0.77
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.84; acc: 0.77
Batch: 160; loss: 0.71; acc: 0.91
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.89
Batch: 240; loss: 0.82; acc: 0.84
Batch: 260; loss: 0.91; acc: 0.75
Batch: 280; loss: 0.95; acc: 0.69
Batch: 300; loss: 0.8; acc: 0.86
Batch: 320; loss: 0.93; acc: 0.72
Batch: 340; loss: 0.72; acc: 0.84
Batch: 360; loss: 0.89; acc: 0.75
Batch: 380; loss: 0.89; acc: 0.78
Batch: 400; loss: 0.78; acc: 0.77
Batch: 420; loss: 0.65; acc: 0.88
Batch: 440; loss: 0.86; acc: 0.84
Batch: 460; loss: 0.71; acc: 0.86
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.88; acc: 0.8
Batch: 580; loss: 0.82; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.88
Batch: 620; loss: 0.89; acc: 0.78
Batch: 640; loss: 0.89; acc: 0.73
Batch: 660; loss: 0.94; acc: 0.75
Batch: 680; loss: 1.16; acc: 0.69
Batch: 700; loss: 0.88; acc: 0.78
Batch: 720; loss: 0.96; acc: 0.75
Batch: 740; loss: 0.9; acc: 0.73
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.84; train_accuracy: 0.79 

0.00018965669733006507
0.0001820167526602745
Batch: 0; loss: 0.69; acc: 0.91
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.86
Val Epoch over. val_loss: 0.7949356336122865; val_accuracy: 0.8120023885350318 

The current subspace-distance is: 0.0001820167526602745 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 0.8; acc: 0.8
Batch: 40; loss: 0.84; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.75; acc: 0.86
Batch: 100; loss: 0.93; acc: 0.69
Batch: 120; loss: 0.88; acc: 0.81
Batch: 140; loss: 0.99; acc: 0.7
Batch: 160; loss: 0.79; acc: 0.84
Batch: 180; loss: 0.79; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.83
Batch: 220; loss: 0.82; acc: 0.77
Batch: 240; loss: 0.8; acc: 0.83
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.91; acc: 0.77
Batch: 320; loss: 1.01; acc: 0.73
Batch: 340; loss: 0.87; acc: 0.77
Batch: 360; loss: 0.87; acc: 0.78
Batch: 380; loss: 0.9; acc: 0.8
Batch: 400; loss: 0.8; acc: 0.81
Batch: 420; loss: 0.74; acc: 0.86
Batch: 440; loss: 0.98; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.84; acc: 0.8
Batch: 500; loss: 0.96; acc: 0.75
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.91; acc: 0.77
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.82; acc: 0.8
Batch: 600; loss: 0.88; acc: 0.77
Batch: 620; loss: 0.88; acc: 0.78
Batch: 640; loss: 0.85; acc: 0.75
Batch: 660; loss: 0.75; acc: 0.75
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 0.74; acc: 0.88
Batch: 720; loss: 0.85; acc: 0.73
Batch: 740; loss: 0.87; acc: 0.75
Batch: 760; loss: 0.91; acc: 0.78
Batch: 780; loss: 0.88; acc: 0.81
Train Epoch over. train_loss: 0.84; train_accuracy: 0.79 

0.00019080747733823955
0.0001823689672164619
Batch: 0; loss: 0.68; acc: 0.91
Batch: 20; loss: 1.06; acc: 0.69
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.88; acc: 0.77
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.86
Val Epoch over. val_loss: 0.7859224925754936; val_accuracy: 0.8124004777070064 

The current subspace-distance is: 0.0001823689672164619 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.89
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 0.96; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.73
Batch: 160; loss: 0.86; acc: 0.75
Batch: 180; loss: 0.81; acc: 0.8
Batch: 200; loss: 0.79; acc: 0.84
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.8
Batch: 260; loss: 0.8; acc: 0.75
Batch: 280; loss: 0.87; acc: 0.75
Batch: 300; loss: 0.96; acc: 0.72
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 0.82; acc: 0.8
Batch: 380; loss: 0.83; acc: 0.78
Batch: 400; loss: 0.72; acc: 0.88
Batch: 420; loss: 0.82; acc: 0.8
Batch: 440; loss: 0.73; acc: 0.8
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.81; acc: 0.86
Batch: 500; loss: 0.68; acc: 0.89
Batch: 520; loss: 0.86; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.83; acc: 0.77
Batch: 580; loss: 1.0; acc: 0.7
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 1.01; acc: 0.66
Batch: 640; loss: 0.91; acc: 0.8
Batch: 660; loss: 0.74; acc: 0.81
Batch: 680; loss: 0.91; acc: 0.72
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.86; acc: 0.81
Batch: 760; loss: 0.85; acc: 0.75
Batch: 780; loss: 0.95; acc: 0.7
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

0.00018977755098603666
0.0001832385896705091
Batch: 0; loss: 0.68; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.86
Val Epoch over. val_loss: 0.7876770898794673; val_accuracy: 0.8117038216560509 

The current subspace-distance is: 0.0001832385896705091 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.87; acc: 0.8
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.88; acc: 0.78
Batch: 100; loss: 0.89; acc: 0.81
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 0.88; acc: 0.7
Batch: 160; loss: 0.8; acc: 0.81
Batch: 180; loss: 1.02; acc: 0.77
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.89; acc: 0.78
Batch: 240; loss: 0.89; acc: 0.72
Batch: 260; loss: 0.83; acc: 0.83
Batch: 280; loss: 0.73; acc: 0.84
Batch: 300; loss: 0.98; acc: 0.7
Batch: 320; loss: 0.86; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.65; acc: 0.94
Batch: 400; loss: 0.69; acc: 0.88
Batch: 420; loss: 0.75; acc: 0.89
Batch: 440; loss: 0.93; acc: 0.77
Batch: 460; loss: 0.9; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.83
Batch: 500; loss: 1.0; acc: 0.73
Batch: 520; loss: 0.7; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.91
Batch: 560; loss: 0.81; acc: 0.81
Batch: 580; loss: 0.8; acc: 0.89
Batch: 600; loss: 0.86; acc: 0.77
Batch: 620; loss: 0.79; acc: 0.84
Batch: 640; loss: 0.84; acc: 0.83
Batch: 660; loss: 0.93; acc: 0.72
Batch: 680; loss: 0.84; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.84
Batch: 740; loss: 0.88; acc: 0.72
Batch: 760; loss: 0.84; acc: 0.81
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

0.00019245375005993992
0.0001857328024925664
Batch: 0; loss: 0.68; acc: 0.91
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.86
Val Epoch over. val_loss: 0.7829387258192536; val_accuracy: 0.8101114649681529 

The current subspace-distance is: 0.0001857328024925664 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.74; acc: 0.89
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.83
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.78; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 1.0; acc: 0.72
Batch: 200; loss: 0.87; acc: 0.73
Batch: 220; loss: 0.83; acc: 0.8
Batch: 240; loss: 0.9; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 0.79; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.88
Batch: 340; loss: 1.04; acc: 0.69
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.89; acc: 0.78
Batch: 400; loss: 0.83; acc: 0.78
Batch: 420; loss: 0.86; acc: 0.8
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.8; acc: 0.77
Batch: 480; loss: 0.88; acc: 0.78
Batch: 500; loss: 0.83; acc: 0.78
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.76; acc: 0.8
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.89; acc: 0.75
Batch: 600; loss: 0.78; acc: 0.81
Batch: 620; loss: 0.85; acc: 0.78
Batch: 640; loss: 0.8; acc: 0.86
Batch: 660; loss: 0.9; acc: 0.78
Batch: 680; loss: 0.95; acc: 0.75
Batch: 700; loss: 0.77; acc: 0.8
Batch: 720; loss: 0.85; acc: 0.83
Batch: 740; loss: 0.91; acc: 0.78
Batch: 760; loss: 0.83; acc: 0.78
Batch: 780; loss: 0.84; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

0.00019583047833293676
0.00018962714239023626
Batch: 0; loss: 0.68; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.67; acc: 0.86
Val Epoch over. val_loss: 0.7859924048375172; val_accuracy: 0.8104100318471338 

The current subspace-distance is: 0.00018962714239023626 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 1.03; acc: 0.7
Batch: 80; loss: 0.88; acc: 0.81
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 1.04; acc: 0.67
Batch: 160; loss: 0.84; acc: 0.8
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.85; acc: 0.78
Batch: 220; loss: 0.86; acc: 0.72
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 1.02; acc: 0.73
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.92; acc: 0.75
Batch: 360; loss: 0.72; acc: 0.88
Batch: 380; loss: 0.86; acc: 0.78
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.91; acc: 0.75
Batch: 440; loss: 0.76; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.81
Batch: 480; loss: 0.65; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.92
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 0.91; acc: 0.75
Batch: 560; loss: 0.76; acc: 0.86
Batch: 580; loss: 0.92; acc: 0.78
Batch: 600; loss: 0.8; acc: 0.81
Batch: 620; loss: 0.76; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.91
Batch: 660; loss: 0.83; acc: 0.81
Batch: 680; loss: 0.88; acc: 0.77
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.95; acc: 0.73
Batch: 760; loss: 1.07; acc: 0.7
Batch: 780; loss: 0.88; acc: 0.8
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

0.0001922681840369478
0.00018754448683466762
Batch: 0; loss: 0.67; acc: 0.91
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.86
Val Epoch over. val_loss: 0.7778845245291472; val_accuracy: 0.8123009554140127 

The current subspace-distance is: 0.00018754448683466762 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 0.88; acc: 0.73
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.92; acc: 0.73
Batch: 160; loss: 0.8; acc: 0.8
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.74; acc: 0.83
Batch: 220; loss: 0.9; acc: 0.78
Batch: 240; loss: 0.77; acc: 0.77
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 0.81; acc: 0.75
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.93; acc: 0.8
Batch: 360; loss: 0.86; acc: 0.77
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.99; acc: 0.72
Batch: 420; loss: 0.75; acc: 0.83
Batch: 440; loss: 0.87; acc: 0.77
Batch: 460; loss: 0.76; acc: 0.84
Batch: 480; loss: 0.95; acc: 0.69
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.95; acc: 0.78
Batch: 580; loss: 0.89; acc: 0.8
Batch: 600; loss: 0.87; acc: 0.73
Batch: 620; loss: 0.83; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.86
Batch: 660; loss: 0.89; acc: 0.8
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.82; acc: 0.83
Batch: 720; loss: 0.82; acc: 0.77
Batch: 740; loss: 0.77; acc: 0.84
Batch: 760; loss: 0.89; acc: 0.78
Batch: 780; loss: 0.78; acc: 0.83
Train Epoch over. train_loss: 0.82; train_accuracy: 0.79 

0.000194201449630782
0.00018626541714183986
Batch: 0; loss: 0.66; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.89
Val Epoch over. val_loss: 0.7758108692564023; val_accuracy: 0.8148885350318471 

The current subspace-distance is: 0.00018626541714183986 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.75

The number of parameters is: 266969

The number of individual parameters is:

38
380
38
38
57
43320
57
57
114
129960
114
114
64
87552
64
64
4096
64
640
10
64
64

nonzero elements in E: 80090694
elements in E: 80090700
fraction nonzero: 0.999999925084935
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.11
Batch: 20; loss: 2.11; acc: 0.25
Batch: 40; loss: 1.91; acc: 0.39
Batch: 60; loss: 1.84; acc: 0.45
Batch: 80; loss: 1.85; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.58
Batch: 120; loss: 1.72; acc: 0.55
Batch: 140; loss: 1.76; acc: 0.45
Batch: 160; loss: 1.74; acc: 0.52
Batch: 180; loss: 1.59; acc: 0.58
Batch: 200; loss: 1.64; acc: 0.53
Batch: 220; loss: 1.44; acc: 0.64
Batch: 240; loss: 1.5; acc: 0.64
Batch: 260; loss: 1.54; acc: 0.64
Batch: 280; loss: 1.44; acc: 0.62
Batch: 300; loss: 1.56; acc: 0.61
Batch: 320; loss: 1.49; acc: 0.61
Batch: 340; loss: 1.57; acc: 0.59
Batch: 360; loss: 1.45; acc: 0.72
Batch: 380; loss: 1.41; acc: 0.7
Batch: 400; loss: 1.36; acc: 0.66
Batch: 420; loss: 1.47; acc: 0.59
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.46; acc: 0.61
Batch: 480; loss: 1.41; acc: 0.64
Batch: 500; loss: 1.3; acc: 0.64
Batch: 520; loss: 1.32; acc: 0.73
Batch: 540; loss: 1.19; acc: 0.77
Batch: 560; loss: 1.41; acc: 0.66
Batch: 580; loss: 1.4; acc: 0.69
Batch: 600; loss: 1.24; acc: 0.73
Batch: 620; loss: 1.32; acc: 0.66
Batch: 640; loss: 1.34; acc: 0.7
Batch: 660; loss: 1.23; acc: 0.77
Batch: 680; loss: 1.23; acc: 0.75
Batch: 700; loss: 1.26; acc: 0.77
Batch: 720; loss: 1.2; acc: 0.69
Batch: 740; loss: 1.25; acc: 0.7
Batch: 760; loss: 1.19; acc: 0.81
Batch: 780; loss: 1.14; acc: 0.81
Train Epoch over. train_loss: 1.49; train_accuracy: 0.63 

6.288301665335894e-05
5.7138291595038027e-05
Batch: 0; loss: 1.19; acc: 0.8
Batch: 20; loss: 1.47; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.88
Batch: 60; loss: 1.15; acc: 0.8
Batch: 80; loss: 1.05; acc: 0.84
Batch: 100; loss: 1.13; acc: 0.84
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 1.08; acc: 0.75
Val Epoch over. val_loss: 1.168311795231643; val_accuracy: 0.7661226114649682 

The current subspace-distance is: 5.7138291595038027e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.77
Batch: 20; loss: 1.27; acc: 0.69
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.31; acc: 0.67
Batch: 80; loss: 1.14; acc: 0.83
Batch: 100; loss: 1.18; acc: 0.73
Batch: 120; loss: 1.06; acc: 0.81
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.03; acc: 0.86
Batch: 180; loss: 1.17; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.84
Batch: 220; loss: 1.02; acc: 0.84
Batch: 240; loss: 1.08; acc: 0.81
Batch: 260; loss: 1.01; acc: 0.84
Batch: 280; loss: 1.13; acc: 0.78
Batch: 300; loss: 1.21; acc: 0.72
Batch: 320; loss: 1.06; acc: 0.72
Batch: 340; loss: 1.04; acc: 0.83
Batch: 360; loss: 1.12; acc: 0.8
Batch: 380; loss: 1.02; acc: 0.83
Batch: 400; loss: 1.04; acc: 0.77
Batch: 420; loss: 1.21; acc: 0.72
Batch: 440; loss: 0.92; acc: 0.84
Batch: 460; loss: 1.19; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.78
Batch: 500; loss: 1.0; acc: 0.73
Batch: 520; loss: 0.99; acc: 0.84
Batch: 540; loss: 1.02; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.78
Batch: 580; loss: 1.22; acc: 0.73
Batch: 600; loss: 1.02; acc: 0.75
Batch: 620; loss: 1.06; acc: 0.77
Batch: 640; loss: 0.81; acc: 0.89
Batch: 660; loss: 0.98; acc: 0.75
Batch: 680; loss: 0.99; acc: 0.84
Batch: 700; loss: 0.86; acc: 0.86
Batch: 720; loss: 0.91; acc: 0.81
Batch: 740; loss: 1.13; acc: 0.7
Batch: 760; loss: 0.93; acc: 0.84
Batch: 780; loss: 1.1; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.77 

8.768975385464728e-05
8.152353257173672e-05
Batch: 0; loss: 0.92; acc: 0.83
Batch: 20; loss: 1.23; acc: 0.64
Batch: 40; loss: 0.68; acc: 0.94
Batch: 60; loss: 0.93; acc: 0.83
Batch: 80; loss: 0.86; acc: 0.88
Batch: 100; loss: 0.9; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.72
Batch: 140; loss: 0.83; acc: 0.91
Val Epoch over. val_loss: 0.942490938362802; val_accuracy: 0.8201632165605095 

The current subspace-distance is: 8.152353257173672e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.77
Batch: 20; loss: 1.0; acc: 0.81
Batch: 40; loss: 1.0; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 1.1; acc: 0.77
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 1.01; acc: 0.81
Batch: 140; loss: 1.03; acc: 0.73
Batch: 160; loss: 1.09; acc: 0.75
Batch: 180; loss: 0.9; acc: 0.8
Batch: 200; loss: 0.84; acc: 0.84
Batch: 220; loss: 0.91; acc: 0.83
Batch: 240; loss: 1.0; acc: 0.8
Batch: 260; loss: 0.85; acc: 0.86
Batch: 280; loss: 0.98; acc: 0.78
Batch: 300; loss: 0.93; acc: 0.77
Batch: 320; loss: 1.06; acc: 0.8
Batch: 340; loss: 0.84; acc: 0.81
Batch: 360; loss: 0.92; acc: 0.81
Batch: 380; loss: 0.91; acc: 0.83
Batch: 400; loss: 1.05; acc: 0.75
Batch: 420; loss: 0.86; acc: 0.86
Batch: 440; loss: 0.97; acc: 0.77
Batch: 460; loss: 0.95; acc: 0.83
Batch: 480; loss: 0.89; acc: 0.84
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.9; acc: 0.8
Batch: 540; loss: 0.88; acc: 0.77
Batch: 560; loss: 0.99; acc: 0.77
Batch: 580; loss: 0.91; acc: 0.78
Batch: 600; loss: 0.98; acc: 0.83
Batch: 620; loss: 0.86; acc: 0.83
Batch: 640; loss: 0.98; acc: 0.77
Batch: 660; loss: 0.96; acc: 0.73
Batch: 680; loss: 0.76; acc: 0.89
Batch: 700; loss: 0.93; acc: 0.83
Batch: 720; loss: 0.92; acc: 0.77
Batch: 740; loss: 0.96; acc: 0.73
Batch: 760; loss: 0.85; acc: 0.86
Batch: 780; loss: 0.79; acc: 0.89
Train Epoch over. train_loss: 0.94; train_accuracy: 0.8 

0.00010433317947899923
9.839992708293721e-05
Batch: 0; loss: 0.82; acc: 0.84
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.56; acc: 0.95
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.91
Batch: 100; loss: 0.81; acc: 0.95
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.7; acc: 0.91
Val Epoch over. val_loss: 0.8413992736749588; val_accuracy: 0.8317078025477707 

The current subspace-distance is: 9.839992708293721e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.77
Batch: 20; loss: 0.89; acc: 0.83
Batch: 40; loss: 0.94; acc: 0.81
Batch: 60; loss: 0.88; acc: 0.84
Batch: 80; loss: 0.85; acc: 0.84
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.78
Batch: 160; loss: 0.91; acc: 0.8
Batch: 180; loss: 0.99; acc: 0.75
Batch: 200; loss: 1.03; acc: 0.72
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 1.0; acc: 0.77
Batch: 260; loss: 0.75; acc: 0.88
Batch: 280; loss: 0.93; acc: 0.8
Batch: 300; loss: 0.89; acc: 0.83
Batch: 320; loss: 0.8; acc: 0.84
Batch: 340; loss: 0.83; acc: 0.86
Batch: 360; loss: 0.84; acc: 0.8
Batch: 380; loss: 0.93; acc: 0.75
Batch: 400; loss: 0.77; acc: 0.84
Batch: 420; loss: 0.73; acc: 0.86
Batch: 440; loss: 0.82; acc: 0.84
Batch: 460; loss: 0.76; acc: 0.83
Batch: 480; loss: 0.81; acc: 0.84
Batch: 500; loss: 0.92; acc: 0.77
Batch: 520; loss: 0.71; acc: 0.86
Batch: 540; loss: 0.76; acc: 0.83
Batch: 560; loss: 0.94; acc: 0.75
Batch: 580; loss: 0.89; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.78
Batch: 620; loss: 0.9; acc: 0.78
Batch: 640; loss: 0.95; acc: 0.75
Batch: 660; loss: 0.78; acc: 0.83
Batch: 680; loss: 0.84; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.77
Batch: 720; loss: 1.06; acc: 0.75
Batch: 740; loss: 0.62; acc: 0.92
Batch: 760; loss: 0.85; acc: 0.77
Batch: 780; loss: 0.77; acc: 0.86
Train Epoch over. train_loss: 0.87; train_accuracy: 0.81 

0.00011702554911607876
0.00011101415293524042
Batch: 0; loss: 0.76; acc: 0.89
Batch: 20; loss: 1.08; acc: 0.66
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.95
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.91
Val Epoch over. val_loss: 0.7876194434560788; val_accuracy: 0.839968152866242 

The current subspace-distance is: 0.00011101415293524042 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.83
Batch: 20; loss: 0.86; acc: 0.8
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 0.89; acc: 0.78
Batch: 80; loss: 0.82; acc: 0.81
Batch: 100; loss: 0.8; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.85; acc: 0.75
Batch: 160; loss: 0.81; acc: 0.81
Batch: 180; loss: 0.83; acc: 0.88
Batch: 200; loss: 0.78; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.84
Batch: 240; loss: 0.78; acc: 0.86
Batch: 260; loss: 0.74; acc: 0.86
Batch: 280; loss: 0.84; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.86
Batch: 320; loss: 0.8; acc: 0.84
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 0.82; acc: 0.86
Batch: 380; loss: 0.85; acc: 0.83
Batch: 400; loss: 0.69; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.86
Batch: 440; loss: 0.79; acc: 0.86
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.88
Batch: 520; loss: 0.81; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.84
Batch: 560; loss: 0.8; acc: 0.84
Batch: 580; loss: 0.74; acc: 0.89
Batch: 600; loss: 0.83; acc: 0.8
Batch: 620; loss: 0.84; acc: 0.77
Batch: 640; loss: 0.77; acc: 0.84
Batch: 660; loss: 0.73; acc: 0.84
Batch: 680; loss: 0.88; acc: 0.8
Batch: 700; loss: 0.73; acc: 0.88
Batch: 720; loss: 0.85; acc: 0.83
Batch: 740; loss: 0.81; acc: 0.84
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 0.65; acc: 0.89
Train Epoch over. train_loss: 0.81; train_accuracy: 0.82 

0.0001282918092329055
0.00012287599383853376
Batch: 0; loss: 0.72; acc: 0.89
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.7; acc: 0.94
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.55; acc: 0.92
Val Epoch over. val_loss: 0.730903344739015; val_accuracy: 0.847531847133758 

The current subspace-distance is: 0.00012287599383853376 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.86
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 0.75; acc: 0.91
Batch: 80; loss: 0.78; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.91
Batch: 180; loss: 0.71; acc: 0.89
Batch: 200; loss: 0.82; acc: 0.81
Batch: 220; loss: 0.84; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.85; acc: 0.8
Batch: 300; loss: 0.57; acc: 0.92
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.92; acc: 0.77
Batch: 380; loss: 0.79; acc: 0.8
Batch: 400; loss: 0.81; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.83
Batch: 460; loss: 0.82; acc: 0.78
Batch: 480; loss: 0.71; acc: 0.86
Batch: 500; loss: 0.78; acc: 0.8
Batch: 520; loss: 0.74; acc: 0.83
Batch: 540; loss: 0.7; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.88
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.82; acc: 0.75
Batch: 640; loss: 0.82; acc: 0.84
Batch: 660; loss: 0.84; acc: 0.81
Batch: 680; loss: 0.86; acc: 0.78
Batch: 700; loss: 0.93; acc: 0.72
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.83
Batch: 760; loss: 0.87; acc: 0.77
Batch: 780; loss: 0.67; acc: 0.91
Train Epoch over. train_loss: 0.77; train_accuracy: 0.83 

0.0001400236797053367
0.00013327726628631353
Batch: 0; loss: 0.67; acc: 0.89
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.92
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.6852971167321418; val_accuracy: 0.8556926751592356 

The current subspace-distance is: 0.00013327726628631353 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.75
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.76; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.83; acc: 0.77
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.87; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.81
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.66; acc: 0.88
Batch: 260; loss: 0.77; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.89
Batch: 300; loss: 0.74; acc: 0.83
Batch: 320; loss: 0.66; acc: 0.91
Batch: 340; loss: 0.76; acc: 0.81
Batch: 360; loss: 0.75; acc: 0.83
Batch: 380; loss: 0.78; acc: 0.86
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 0.85; acc: 0.8
Batch: 440; loss: 0.69; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.94
Batch: 500; loss: 0.6; acc: 0.89
Batch: 520; loss: 0.71; acc: 0.88
Batch: 540; loss: 0.72; acc: 0.86
Batch: 560; loss: 0.79; acc: 0.8
Batch: 580; loss: 0.66; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.92
Batch: 620; loss: 0.75; acc: 0.8
Batch: 640; loss: 0.83; acc: 0.8
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.84; acc: 0.78
Batch: 700; loss: 0.83; acc: 0.73
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.84; acc: 0.78
Batch: 760; loss: 0.75; acc: 0.83
Batch: 780; loss: 0.82; acc: 0.8
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.0001489458081778139
0.00014356426254380494
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.93; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.43; acc: 0.97
Val Epoch over. val_loss: 0.6462489909427181; val_accuracy: 0.8617635350318471 

The current subspace-distance is: 0.00014356426254380494 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.81; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.81; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.91
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.92
Batch: 440; loss: 0.67; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.72; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.72; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.88
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.58; acc: 0.89
Batch: 620; loss: 0.75; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.00015876309771556407
0.0001528838329249993
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.6117582926704626; val_accuracy: 0.8635549363057324 

The current subspace-distance is: 0.0001528838329249993 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.67; acc: 0.91
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.71; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.63; acc: 0.84
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.69; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.82; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.81; acc: 0.78
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.95
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.79; acc: 0.73
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.7; acc: 0.88
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.91
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.85 

0.0001687313342699781
0.00015877919213380665
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.97
Val Epoch over. val_loss: 0.5928347275894918; val_accuracy: 0.8676353503184714 

The current subspace-distance is: 0.00015877919213380665 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.92
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.95
Batch: 360; loss: 0.48; acc: 0.94
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.77; acc: 0.77
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.94
Batch: 620; loss: 0.69; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.92
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.88
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.0001760890008881688
0.0001696395775070414
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.569308089033054; val_accuracy: 0.8700238853503185 

The current subspace-distance is: 0.0001696395775070414 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 0.59; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.7; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.85; acc: 0.73
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.66; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.8
Batch: 560; loss: 0.52; acc: 0.94
Batch: 580; loss: 0.6; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.92
Batch: 640; loss: 0.63; acc: 0.8
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.85 

0.00017829571152105927
0.00017064614803530276
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.5573643969882066; val_accuracy: 0.8718152866242038 

The current subspace-distance is: 0.00017064614803530276 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.78
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.6; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.94
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.81; acc: 0.8
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.95
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.61; train_accuracy: 0.85 

0.00018222065409645438
0.00017480437236372381
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5565868821113732; val_accuracy: 0.8692277070063694 

The current subspace-distance is: 0.00017480437236372381 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.67; acc: 0.77
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.65; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.46; acc: 0.94
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.91
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.68; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.79; acc: 0.81
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.00018413522047922015
0.00017615272372495383
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5457322426662323; val_accuracy: 0.8764928343949044 

The current subspace-distance is: 0.00017615272372495383 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.75
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.61; acc: 0.77
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.0001873413275461644
0.0001794826239347458
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5406581303875917; val_accuracy: 0.8733081210191083 

The current subspace-distance is: 0.0001794826239347458 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.61; acc: 0.89
Batch: 160; loss: 0.73; acc: 0.78
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.75; acc: 0.77
Batch: 260; loss: 0.74; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 0.74; acc: 0.75
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.94
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.77
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.94
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.67; acc: 0.73
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00018878387345466763
0.0001839055330492556
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.5329550624273385; val_accuracy: 0.8786823248407644 

The current subspace-distance is: 0.0001839055330492556 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.65; acc: 0.81
Batch: 220; loss: 0.54; acc: 0.92
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.92
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.67; acc: 0.78
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.65; acc: 0.88
Batch: 640; loss: 0.67; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.65; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.94
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

0.0001899614289868623
0.0001823225902626291
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.5292662995256436; val_accuracy: 0.8773885350318471 

The current subspace-distance is: 0.0001823225902626291 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.92
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.78
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.83
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.63; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.86 

0.000191995786735788
0.00018379813991487026
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.5184727013111115; val_accuracy: 0.8766918789808917 

The current subspace-distance is: 0.00018379813991487026 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.61; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.94
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.79; acc: 0.75
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.86 

0.00019495101878419518
0.0001872321154223755
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.5108019565321078; val_accuracy: 0.8796775477707006 

The current subspace-distance is: 0.0001872321154223755 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.72; acc: 0.75
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.78
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.62; acc: 0.77
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.41; acc: 0.95
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.56; train_accuracy: 0.86 

0.0001988004078157246
0.00019257073290646076
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.518442050097095; val_accuracy: 0.8764928343949044 

The current subspace-distance is: 0.00019257073290646076 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.71; acc: 0.77
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.83
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.81
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.86 

0.00020268093794584274
0.00019511398568283767
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.5004525539601684; val_accuracy: 0.8812699044585988 

The current subspace-distance is: 0.00019511398568283767 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.94
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.44; acc: 0.94
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.77
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.78
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.65; acc: 0.77
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.86 

0.0002028724702540785
0.00019358743156772107
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.5051711776833625; val_accuracy: 0.8784832802547771 

The current subspace-distance is: 0.00019358743156772107 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.71; acc: 0.75
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.92
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.95
Batch: 780; loss: 0.6; acc: 0.81
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0002022969420067966
0.0001949868310475722
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.5040908091387172; val_accuracy: 0.8809713375796179 

The current subspace-distance is: 0.0001949868310475722 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.8
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.78
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.86 

0.00020381306239869446
0.00019676581723615527
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4979279727502993; val_accuracy: 0.8805732484076433 

The current subspace-distance is: 0.00019676581723615527 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.43; acc: 0.95
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.41; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.94
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020330536062829196
0.00019448612874839455
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.49638864625791074; val_accuracy: 0.8799761146496815 

The current subspace-distance is: 0.00019448612874839455 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.4; acc: 0.97
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.97
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.33; acc: 1.0
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.78
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.79; acc: 0.77
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020285965001676232
0.00019593053730204701
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.49332594767117954; val_accuracy: 0.882265127388535 

The current subspace-distance is: 0.00019593053730204701 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.74; acc: 0.78
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020423690148163587
0.00019848422380164266
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.48869926022116544; val_accuracy: 0.881468949044586 

The current subspace-distance is: 0.00019848422380164266 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.95
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.78
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.92
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.7; acc: 0.81
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.78
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.95
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020486576249822974
0.00019875318685080856
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4909662582502244; val_accuracy: 0.8818670382165605 

The current subspace-distance is: 0.00019875318685080856 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.63; acc: 0.77
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.7; acc: 0.78
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.41; acc: 0.95
Batch: 400; loss: 0.49; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.94
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.8
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.52; acc: 0.91
Batch: 780; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020575548114720732
0.00019890406110789627
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.49399049124520295; val_accuracy: 0.8788813694267515 

The current subspace-distance is: 0.00019890406110789627 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.57; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.94
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.78; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020590204803738743
0.0001972834434127435
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.49009421448798696; val_accuracy: 0.8803742038216561 

The current subspace-distance is: 0.0001972834434127435 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.66; acc: 0.77
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.6; acc: 0.77
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020661536836996675
0.0001984361733775586
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.98
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.48223984412326937; val_accuracy: 0.8830613057324841 

The current subspace-distance is: 0.0001984361733775586 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.75

The number of parameters is: 266969

The number of individual parameters is:

38
380
38
38
57
43320
57
57
114
129960
114
114
64
87552
64
64
4096
64
640
10
64
64

nonzero elements in E: 106787590
elements in E: 106787600
fraction nonzero: 0.9999999063561686
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.05; acc: 0.33
Batch: 40; loss: 1.81; acc: 0.53
Batch: 60; loss: 1.84; acc: 0.44
Batch: 80; loss: 1.75; acc: 0.44
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.55; acc: 0.62
Batch: 140; loss: 1.46; acc: 0.7
Batch: 160; loss: 1.54; acc: 0.61
Batch: 180; loss: 1.39; acc: 0.69
Batch: 200; loss: 1.59; acc: 0.52
Batch: 220; loss: 1.42; acc: 0.69
Batch: 240; loss: 1.45; acc: 0.69
Batch: 260; loss: 1.35; acc: 0.64
Batch: 280; loss: 1.34; acc: 0.7
Batch: 300; loss: 1.43; acc: 0.61
Batch: 320; loss: 1.31; acc: 0.77
Batch: 340; loss: 1.4; acc: 0.64
Batch: 360; loss: 1.32; acc: 0.69
Batch: 380; loss: 1.28; acc: 0.67
Batch: 400; loss: 1.28; acc: 0.73
Batch: 420; loss: 1.25; acc: 0.78
Batch: 440; loss: 1.23; acc: 0.77
Batch: 460; loss: 1.09; acc: 0.8
Batch: 480; loss: 1.27; acc: 0.67
Batch: 500; loss: 1.31; acc: 0.72
Batch: 520; loss: 1.16; acc: 0.75
Batch: 540; loss: 1.19; acc: 0.81
Batch: 560; loss: 1.26; acc: 0.7
Batch: 580; loss: 1.32; acc: 0.75
Batch: 600; loss: 1.19; acc: 0.7
Batch: 620; loss: 1.2; acc: 0.75
Batch: 640; loss: 1.17; acc: 0.8
Batch: 660; loss: 1.09; acc: 0.8
Batch: 680; loss: 1.13; acc: 0.8
Batch: 700; loss: 1.06; acc: 0.84
Batch: 720; loss: 1.19; acc: 0.77
Batch: 740; loss: 1.11; acc: 0.8
Batch: 760; loss: 1.27; acc: 0.75
Batch: 780; loss: 1.01; acc: 0.83
Train Epoch over. train_loss: 1.38; train_accuracy: 0.67 

2.7822037736768834e-05
7.2995303526113275e-06
Batch: 0; loss: 1.04; acc: 0.89
Batch: 20; loss: 1.35; acc: 0.67
Batch: 40; loss: 0.82; acc: 0.91
Batch: 60; loss: 1.01; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.21; acc: 0.78
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.0793776056569093; val_accuracy: 0.7984673566878981 

The current subspace-distance is: 7.2995303526113275e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.83
Batch: 20; loss: 1.1; acc: 0.78
Batch: 40; loss: 1.31; acc: 0.72
Batch: 60; loss: 1.07; acc: 0.83
Batch: 80; loss: 1.05; acc: 0.86
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.83
Batch: 140; loss: 1.01; acc: 0.8
Batch: 160; loss: 0.95; acc: 0.84
Batch: 180; loss: 1.03; acc: 0.81
Batch: 200; loss: 1.18; acc: 0.67
Batch: 220; loss: 1.11; acc: 0.77
Batch: 240; loss: 1.07; acc: 0.81
Batch: 260; loss: 1.01; acc: 0.83
Batch: 280; loss: 1.07; acc: 0.75
Batch: 300; loss: 0.93; acc: 0.89
Batch: 320; loss: 0.93; acc: 0.83
Batch: 340; loss: 1.0; acc: 0.83
Batch: 360; loss: 1.16; acc: 0.73
Batch: 380; loss: 0.96; acc: 0.81
Batch: 400; loss: 1.06; acc: 0.75
Batch: 420; loss: 0.88; acc: 0.91
Batch: 440; loss: 1.18; acc: 0.72
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 1.04; acc: 0.77
Batch: 500; loss: 1.08; acc: 0.77
Batch: 520; loss: 1.01; acc: 0.78
Batch: 540; loss: 1.02; acc: 0.77
Batch: 560; loss: 1.07; acc: 0.7
Batch: 580; loss: 1.0; acc: 0.84
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 0.93; acc: 0.81
Batch: 640; loss: 0.95; acc: 0.77
Batch: 660; loss: 0.79; acc: 0.89
Batch: 680; loss: 0.99; acc: 0.77
Batch: 700; loss: 1.05; acc: 0.75
Batch: 720; loss: 0.95; acc: 0.86
Batch: 740; loss: 0.9; acc: 0.83
Batch: 760; loss: 0.96; acc: 0.86
Batch: 780; loss: 1.07; acc: 0.66
Train Epoch over. train_loss: 1.02; train_accuracy: 0.8 

3.2627678592689335e-05
1.0824556738953106e-05
Batch: 0; loss: 0.79; acc: 0.91
Batch: 20; loss: 1.18; acc: 0.7
Batch: 40; loss: 0.6; acc: 0.97
Batch: 60; loss: 0.86; acc: 0.88
Batch: 80; loss: 0.79; acc: 0.91
Batch: 100; loss: 0.9; acc: 0.83
Batch: 120; loss: 1.06; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.88
Val Epoch over. val_loss: 0.8733408025875213; val_accuracy: 0.8401671974522293 

The current subspace-distance is: 1.0824556738953106e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.8
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.89; acc: 0.83
Batch: 60; loss: 0.95; acc: 0.81
Batch: 80; loss: 0.9; acc: 0.84
Batch: 100; loss: 0.85; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.89
Batch: 140; loss: 0.97; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 0.81; acc: 0.86
Batch: 200; loss: 0.89; acc: 0.8
Batch: 220; loss: 0.76; acc: 0.92
Batch: 240; loss: 0.94; acc: 0.86
Batch: 260; loss: 0.82; acc: 0.88
Batch: 280; loss: 0.84; acc: 0.81
Batch: 300; loss: 0.96; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.91
Batch: 340; loss: 0.9; acc: 0.8
Batch: 360; loss: 0.81; acc: 0.88
Batch: 380; loss: 0.88; acc: 0.84
Batch: 400; loss: 0.7; acc: 0.88
Batch: 420; loss: 0.96; acc: 0.77
Batch: 440; loss: 0.77; acc: 0.84
Batch: 460; loss: 0.79; acc: 0.89
Batch: 480; loss: 0.85; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.89
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.91
Batch: 580; loss: 0.75; acc: 0.88
Batch: 600; loss: 0.9; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.88
Batch: 640; loss: 0.75; acc: 0.86
Batch: 660; loss: 0.71; acc: 0.89
Batch: 680; loss: 0.85; acc: 0.86
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.75; acc: 0.88
Batch: 740; loss: 0.89; acc: 0.78
Batch: 760; loss: 0.82; acc: 0.86
Batch: 780; loss: 0.82; acc: 0.84
Train Epoch over. train_loss: 0.85; train_accuracy: 0.83 

3.724062844412401e-05
1.346753560937941e-05
Batch: 0; loss: 0.62; acc: 0.97
Batch: 20; loss: 1.0; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.64; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.53; acc: 0.95
Val Epoch over. val_loss: 0.7382289957089029; val_accuracy: 0.8649482484076433 

The current subspace-distance is: 1.346753560937941e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.83
Batch: 20; loss: 0.9; acc: 0.78
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.82; acc: 0.84
Batch: 80; loss: 0.84; acc: 0.81
Batch: 100; loss: 0.85; acc: 0.78
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.91
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 0.81; acc: 0.86
Batch: 200; loss: 0.72; acc: 0.88
Batch: 220; loss: 0.78; acc: 0.84
Batch: 240; loss: 0.72; acc: 0.88
Batch: 260; loss: 0.62; acc: 0.94
Batch: 280; loss: 0.8; acc: 0.8
Batch: 300; loss: 0.88; acc: 0.83
Batch: 320; loss: 0.89; acc: 0.8
Batch: 340; loss: 0.67; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.71; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.68; acc: 0.89
Batch: 440; loss: 0.65; acc: 0.89
Batch: 460; loss: 0.75; acc: 0.84
Batch: 480; loss: 0.74; acc: 0.89
Batch: 500; loss: 0.76; acc: 0.83
Batch: 520; loss: 0.58; acc: 0.92
Batch: 540; loss: 0.73; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.81
Batch: 580; loss: 0.69; acc: 0.89
Batch: 600; loss: 0.62; acc: 0.91
Batch: 620; loss: 0.95; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.89
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.87; acc: 0.75
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.58; acc: 0.94
Batch: 760; loss: 0.69; acc: 0.92
Batch: 780; loss: 0.8; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.85 

4.067357076564804e-05
1.4426773304876406e-05
Batch: 0; loss: 0.54; acc: 0.97
Batch: 20; loss: 0.9; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.94
Batch: 100; loss: 0.69; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.43; acc: 0.95
Val Epoch over. val_loss: 0.6517051987966914; val_accuracy: 0.8711186305732485 

The current subspace-distance is: 1.4426773304876406e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.94
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.75; acc: 0.8
Batch: 200; loss: 0.7; acc: 0.88
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.89
Batch: 260; loss: 0.83; acc: 0.77
Batch: 280; loss: 0.61; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.92
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.89
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.91
Batch: 540; loss: 0.67; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.92
Batch: 660; loss: 0.68; acc: 0.89
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.6; acc: 0.91
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.86 

4.392491973703727e-05
1.6752746887505054e-05
Batch: 0; loss: 0.5; acc: 0.97
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.5849864363290702; val_accuracy: 0.8845541401273885 

The current subspace-distance is: 1.6752746887505054e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.98
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.83; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.95
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.8; acc: 0.75
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.68; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.94
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.92
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.64; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.91
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.91
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.92
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.89
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.91
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.62; train_accuracy: 0.87 

4.7259192797355354e-05
1.8393731807009317e-05
Batch: 0; loss: 0.45; acc: 0.97
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.97
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.5376071555978933; val_accuracy: 0.8911226114649682 

The current subspace-distance is: 1.8393731807009317e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.94
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.63; acc: 0.8
Batch: 320; loss: 0.5; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.97
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.94
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.94
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.95
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

4.953401366947219e-05
1.9172210159013048e-05
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.4959549792822759; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 1.9172210159013048e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.97
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.95
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.95
Batch: 760; loss: 0.54; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

5.193562537897378e-05
2.0710660464828834e-05
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.4522106291572; val_accuracy: 0.9038614649681529 

The current subspace-distance is: 2.0710660464828834e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.97
Batch: 140; loss: 0.67; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.94
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.97
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.95
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.97
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.95
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

5.4092601203592494e-05
2.1687466869479977e-05
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.439323014799197; val_accuracy: 0.9077428343949044 

The current subspace-distance is: 2.1687466869479977e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.92
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.97
Batch: 180; loss: 0.57; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.94
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.72; acc: 0.72
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

5.5888329370645806e-05
2.2583340978599153e-05
Batch: 0; loss: 0.35; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.40713336864474475; val_accuracy: 0.9123208598726115 

The current subspace-distance is: 2.2583340978599153e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.56; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

5.782075459137559e-05
2.361582846788224e-05
Batch: 0; loss: 0.35; acc: 1.0
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4073257505134412; val_accuracy: 0.9117237261146497 

The current subspace-distance is: 2.361582846788224e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.62; acc: 0.8
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.84
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.59; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

5.838486322318204e-05
2.477672569511924e-05
Batch: 0; loss: 0.35; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.404824847343621; val_accuracy: 0.9131170382165605 

The current subspace-distance is: 2.477672569511924e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.61; acc: 0.77
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

5.83119472139515e-05
2.4533750547561795e-05
Batch: 0; loss: 0.35; acc: 1.0
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.39859585388071217; val_accuracy: 0.912718949044586 

The current subspace-distance is: 2.4533750547561795e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.97
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.95
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.97
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.850423258380033e-05
2.4256258257082663e-05
Batch: 0; loss: 0.34; acc: 1.0
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.3879464909339407; val_accuracy: 0.9167993630573248 

The current subspace-distance is: 2.4256258257082663e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.98
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.39; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.98
Batch: 600; loss: 0.66; acc: 0.78
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.9331734519219026e-05
2.342345760553144e-05
Batch: 0; loss: 0.34; acc: 1.0
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.39332099619564737; val_accuracy: 0.9140127388535032 

The current subspace-distance is: 2.342345760553144e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

6.0957831010455266e-05
2.7550866434467025e-05
Batch: 0; loss: 0.34; acc: 0.98
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.38326239187246675; val_accuracy: 0.9183917197452229 

The current subspace-distance is: 2.7550866434467025e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.97
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.97
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.95
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

6.097701407270506e-05
2.5431683752685785e-05
Batch: 0; loss: 0.33; acc: 0.98
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3772015432073812; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.5431683752685785e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.97
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.95
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.97
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.97
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.078098886064254e-05
2.4046978069236502e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.37344876691034645; val_accuracy: 0.9175955414012739 

The current subspace-distance is: 2.4046978069236502e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.95
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.81
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.198212940944359e-05
2.6148340111831203e-05
Batch: 0; loss: 0.31; acc: 1.0
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.37533846744306526; val_accuracy: 0.9168988853503185 

The current subspace-distance is: 2.6148340111831203e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.97
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.126163498265669e-05
2.7015525120077655e-05
Batch: 0; loss: 0.31; acc: 1.0
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3683326716066166; val_accuracy: 0.9175955414012739 

The current subspace-distance is: 2.7015525120077655e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.206999387359247e-05
2.553272497607395e-05
Batch: 0; loss: 0.3; acc: 1.0
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.365648409648306; val_accuracy: 0.9193869426751592 

The current subspace-distance is: 2.553272497607395e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.97
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.279500667005777e-05
2.6778159735840745e-05
Batch: 0; loss: 0.31; acc: 1.0
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.36220901521148197; val_accuracy: 0.9203821656050956 

The current subspace-distance is: 2.6778159735840745e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.95
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.97
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.273059989325702e-05
2.699793185456656e-05
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.35548266597614164; val_accuracy: 0.921875 

The current subspace-distance is: 2.699793185456656e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.97
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.97
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.98
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.67; acc: 0.77
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.218822818482295e-05
2.472989035595674e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.35846968488708425; val_accuracy: 0.9201831210191083 

The current subspace-distance is: 2.472989035595674e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.95
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.29586647846736e-05
2.6518349841353483e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.35489111616732966; val_accuracy: 0.9209792993630573 

The current subspace-distance is: 2.6518349841353483e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.7; acc: 0.73
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.83
Batch: 440; loss: 0.37; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.25; acc: 1.0
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.310002208920196e-05
2.645043241500389e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3647498996204631; val_accuracy: 0.9188893312101911 

The current subspace-distance is: 2.645043241500389e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.97
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.275030318647623e-05
2.6150935809710063e-05
Batch: 0; loss: 0.31; acc: 1.0
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3585607328802157; val_accuracy: 0.919187898089172 

The current subspace-distance is: 2.6150935809710063e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.97
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.81
Batch: 680; loss: 0.29; acc: 0.98
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.2609113228973e-05
2.54614442383172e-05
Batch: 0; loss: 0.3; acc: 1.0
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.35644311082970564; val_accuracy: 0.9208797770700637 

The current subspace-distance is: 2.54614442383172e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.307977309916168e-05
2.5806653866311535e-05
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3525288267310258; val_accuracy: 0.9220740445859873 

The current subspace-distance is: 2.5806653866311535e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.98
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.98
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.98
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.300063250819221e-05
2.600579318823293e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.36073653760609353; val_accuracy: 0.917296974522293 

The current subspace-distance is: 2.600579318823293e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.75

The number of parameters is: 266969

The number of individual parameters is:

38
380
38
38
57
43320
57
57
114
129960
114
114
64
87552
64
64
4096
64
640
10
64
64

nonzero elements in E: 133484490
elements in E: 133484500
fraction nonzero: 0.999999925084935
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.4; acc: 0.08
Batch: 20; loss: 1.88; acc: 0.47
Batch: 40; loss: 1.72; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.64
Batch: 80; loss: 1.64; acc: 0.55
Batch: 100; loss: 1.49; acc: 0.7
Batch: 120; loss: 1.6; acc: 0.5
Batch: 140; loss: 1.6; acc: 0.53
Batch: 160; loss: 1.44; acc: 0.66
Batch: 180; loss: 1.35; acc: 0.64
Batch: 200; loss: 1.32; acc: 0.66
Batch: 220; loss: 1.39; acc: 0.69
Batch: 240; loss: 1.18; acc: 0.8
Batch: 260; loss: 1.32; acc: 0.73
Batch: 280; loss: 1.31; acc: 0.69
Batch: 300; loss: 1.28; acc: 0.72
Batch: 320; loss: 1.14; acc: 0.8
Batch: 340; loss: 1.19; acc: 0.69
Batch: 360; loss: 1.14; acc: 0.78
Batch: 380; loss: 1.02; acc: 0.84
Batch: 400; loss: 1.0; acc: 0.83
Batch: 420; loss: 1.11; acc: 0.75
Batch: 440; loss: 1.12; acc: 0.7
Batch: 460; loss: 1.11; acc: 0.73
Batch: 480; loss: 1.18; acc: 0.73
Batch: 500; loss: 1.09; acc: 0.73
Batch: 520; loss: 1.08; acc: 0.73
Batch: 540; loss: 0.92; acc: 0.8
Batch: 560; loss: 1.19; acc: 0.73
Batch: 580; loss: 0.95; acc: 0.78
Batch: 600; loss: 1.15; acc: 0.75
Batch: 620; loss: 1.14; acc: 0.75
Batch: 640; loss: 1.02; acc: 0.81
Batch: 660; loss: 0.92; acc: 0.81
Batch: 680; loss: 0.92; acc: 0.84
Batch: 700; loss: 0.82; acc: 0.84
Batch: 720; loss: 0.93; acc: 0.8
Batch: 740; loss: 0.98; acc: 0.7
Batch: 760; loss: 0.98; acc: 0.81
Batch: 780; loss: 0.89; acc: 0.83
Train Epoch over. train_loss: 1.23; train_accuracy: 0.72 

2.8262018531677313e-05
8.660383173264563e-06
Batch: 0; loss: 0.87; acc: 0.83
Batch: 20; loss: 1.12; acc: 0.78
Batch: 40; loss: 0.63; acc: 0.95
Batch: 60; loss: 0.85; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.88
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.82; acc: 0.8
Val Epoch over. val_loss: 0.8852845400002352; val_accuracy: 0.836484872611465 

The current subspace-distance is: 8.660383173264563e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 0.92; acc: 0.83
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 0.96; acc: 0.8
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 0.86; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.86
Batch: 140; loss: 0.87; acc: 0.84
Batch: 160; loss: 0.88; acc: 0.83
Batch: 180; loss: 0.92; acc: 0.81
Batch: 200; loss: 0.86; acc: 0.8
Batch: 220; loss: 0.91; acc: 0.81
Batch: 240; loss: 0.8; acc: 0.84
Batch: 260; loss: 0.9; acc: 0.83
Batch: 280; loss: 0.76; acc: 0.81
Batch: 300; loss: 0.94; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.84
Batch: 340; loss: 0.83; acc: 0.86
Batch: 360; loss: 0.85; acc: 0.81
Batch: 380; loss: 0.78; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.86
Batch: 420; loss: 0.84; acc: 0.86
Batch: 440; loss: 0.7; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.95
Batch: 480; loss: 0.97; acc: 0.75
Batch: 500; loss: 0.77; acc: 0.84
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.75; acc: 0.84
Batch: 600; loss: 0.8; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.86
Batch: 640; loss: 0.73; acc: 0.86
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 0.61; acc: 0.92
Batch: 700; loss: 0.6; acc: 0.94
Batch: 720; loss: 0.75; acc: 0.83
Batch: 740; loss: 0.62; acc: 0.92
Batch: 760; loss: 0.75; acc: 0.83
Batch: 780; loss: 0.8; acc: 0.83
Train Epoch over. train_loss: 0.82; train_accuracy: 0.84 

3.368653415236622e-05
1.1848001122416463e-05
Batch: 0; loss: 0.64; acc: 0.97
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.6778083485402878; val_accuracy: 0.87390525477707 

The current subspace-distance is: 1.1848001122416463e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.89
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 0.7; acc: 0.89
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.79; acc: 0.84
Batch: 160; loss: 0.63; acc: 0.92
Batch: 180; loss: 0.67; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.89
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.79; acc: 0.84
Batch: 300; loss: 0.74; acc: 0.8
Batch: 320; loss: 0.58; acc: 0.92
Batch: 340; loss: 0.6; acc: 0.89
Batch: 360; loss: 0.67; acc: 0.88
Batch: 380; loss: 0.68; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.66; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.74; acc: 0.88
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.87; acc: 0.75
Batch: 520; loss: 0.72; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.89
Batch: 620; loss: 0.64; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.92
Batch: 660; loss: 0.7; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.95
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.94
Train Epoch over. train_loss: 0.67; train_accuracy: 0.87 

3.727757211891003e-05
1.3944159945822321e-05
Batch: 0; loss: 0.49; acc: 0.97
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.5560161513128098; val_accuracy: 0.8910230891719745 

The current subspace-distance is: 1.3944159945822321e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.94
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.92
Batch: 300; loss: 0.52; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.91
Batch: 380; loss: 0.62; acc: 0.81
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.62; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.74; acc: 0.8
Batch: 500; loss: 0.49; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.6; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.57; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.94
Batch: 780; loss: 0.6; acc: 0.91
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

4.1045841498998925e-05
1.596683796378784e-05
Batch: 0; loss: 0.4; acc: 0.97
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.97
Val Epoch over. val_loss: 0.4891680989675461; val_accuracy: 0.9031648089171974 

The current subspace-distance is: 1.596683796378784e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.44; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.61; acc: 0.84
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.97
Batch: 300; loss: 0.48; acc: 0.92
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

4.372237162897363e-05
1.673469705565367e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4441867449860664; val_accuracy: 0.9029657643312102 

The current subspace-distance is: 1.673469705565367e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.95
Batch: 560; loss: 0.51; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.95
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

4.597429506247863e-05
1.8212875147582963e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.4157874050793374; val_accuracy: 0.9111265923566879 

The current subspace-distance is: 1.8212875147582963e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.64; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.92
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.97
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.97
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.97
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

4.850533150602132e-05
1.8754553821054287e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.3878709604595877; val_accuracy: 0.9112261146496815 

The current subspace-distance is: 1.8754553821054287e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.95
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

4.976934360456653e-05
2.0164992747595534e-05
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.36563496747214325; val_accuracy: 0.9174960191082803 

The current subspace-distance is: 2.0164992747595534e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.98
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.98
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

5.172976307221688e-05
2.145778671547305e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.3481393996507499; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.145778671547305e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.279996548779309e-05
2.2681913833366707e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.33520453380551307; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 2.2681913833366707e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.98
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.98
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.416756903287023e-05
2.182741809519939e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.33082137123984134; val_accuracy: 0.9240644904458599 

The current subspace-distance is: 2.182741809519939e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.95
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.97
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.42590678378474e-05
2.190174382121768e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.32385141364518244; val_accuracy: 0.9263535031847133 

The current subspace-distance is: 2.190174382121768e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.4497380915563554e-05
2.185439007007517e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3203888355641608; val_accuracy: 0.9276472929936306 

The current subspace-distance is: 2.185439007007517e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.97
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.98
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.97
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.5379052355419844e-05
2.290708653163165e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3228805614220109; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 2.290708653163165e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.97
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.98
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.97
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.592770685325377e-05
2.2464018911705352e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3152707531858402; val_accuracy: 0.9288415605095541 

The current subspace-distance is: 2.2464018911705352e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.97
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.97
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.6814911658875644e-05
2.4670796847203746e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.31448553426630177; val_accuracy: 0.9284434713375797 

The current subspace-distance is: 2.4670796847203746e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.98
Batch: 620; loss: 0.55; acc: 0.81
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.7390938309254125e-05
2.4039742129389197e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3139955693749106; val_accuracy: 0.9269506369426752 

The current subspace-distance is: 2.4039742129389197e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.701288682757877e-05
2.3777962269377895e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3042503434476579; val_accuracy: 0.9305334394904459 

The current subspace-distance is: 2.3777962269377895e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.19; acc: 1.0
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.743669316871092e-05
2.4515597033314407e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.30745215581101215; val_accuracy: 0.9299363057324841 

The current subspace-distance is: 2.4515597033314407e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.27; acc: 0.98
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

5.817991041112691e-05
2.4517212295904756e-05
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.30568443267208756; val_accuracy: 0.9289410828025477 

The current subspace-distance is: 2.4517212295904756e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.17; acc: 1.0
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.98
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

5.861068711965345e-05
2.4411870981566608e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2982118967801902; val_accuracy: 0.9307324840764332 

The current subspace-distance is: 2.4411870981566608e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.98
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.810658694826998e-05
2.513538311177399e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.29973737207377793; val_accuracy: 0.9314291401273885 

The current subspace-distance is: 2.513538311177399e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.98
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.98
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.8110392274102196e-05
2.328125628991984e-05
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.81
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3000591466096556; val_accuracy: 0.9314291401273885 

The current subspace-distance is: 2.328125628991984e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.98
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.892201079404913e-05
2.499423317203764e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2997062896277494; val_accuracy: 0.9316281847133758 

The current subspace-distance is: 2.499423317203764e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.98
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.98
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.80461164645385e-05
2.3612410586792976e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2950901267160276; val_accuracy: 0.931827229299363 

The current subspace-distance is: 2.3612410586792976e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.95
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.52; acc: 0.81
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.92060387134552e-05
2.5517107133055106e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.29775065615488466; val_accuracy: 0.9309315286624203 

The current subspace-distance is: 2.5517107133055106e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.97
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.8333265769761056e-05
2.3964090360095724e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2992035547734066; val_accuracy: 0.931827229299363 

The current subspace-distance is: 2.3964090360095724e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.97
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.8
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.8958528825314716e-05
2.602900167403277e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2978204037448403; val_accuracy: 0.930234872611465 

The current subspace-distance is: 2.602900167403277e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.918118404224515e-05
2.4865374143701047e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.29519335569659616; val_accuracy: 0.9331210191082803 

The current subspace-distance is: 2.4865374143701047e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.936210436630063e-05
2.552904697949998e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.29423688290415295; val_accuracy: 0.9321257961783439 

The current subspace-distance is: 2.552904697949998e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
