model : table13slim
N : 6
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 3.1089789320611354

The number of parameters is: 267194

The number of individual parameters is:

25
450
25
25
38
43700
38
38
75
131100
75
75
64
86400
64
64
4096
64
640
10
64
64

nonzero elements in E: 13359698
elements in E: 13359700
fraction nonzero: 0.9999998502960395
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.38; acc: 0.09
Batch: 20; loss: 2.36; acc: 0.11
Batch: 40; loss: 2.36; acc: 0.16
Batch: 60; loss: 2.22; acc: 0.11
Batch: 80; loss: 2.24; acc: 0.12
Batch: 100; loss: 2.12; acc: 0.19
Batch: 120; loss: 2.15; acc: 0.27
Batch: 140; loss: 2.15; acc: 0.19
Batch: 160; loss: 2.19; acc: 0.22
Batch: 180; loss: 2.11; acc: 0.25
Batch: 200; loss: 2.06; acc: 0.33
Batch: 220; loss: 2.08; acc: 0.31
Batch: 240; loss: 2.08; acc: 0.31
Batch: 260; loss: 1.94; acc: 0.36
Batch: 280; loss: 2.01; acc: 0.28
Batch: 300; loss: 2.07; acc: 0.22
Batch: 320; loss: 2.04; acc: 0.31
Batch: 340; loss: 1.96; acc: 0.33
Batch: 360; loss: 1.95; acc: 0.39
Batch: 380; loss: 2.0; acc: 0.39
Batch: 400; loss: 1.99; acc: 0.36
Batch: 420; loss: 2.06; acc: 0.33
Batch: 440; loss: 2.02; acc: 0.36
Batch: 460; loss: 1.94; acc: 0.41
Batch: 480; loss: 1.97; acc: 0.33
Batch: 500; loss: 1.9; acc: 0.45
Batch: 520; loss: 1.91; acc: 0.5
Batch: 540; loss: 1.92; acc: 0.41
Batch: 560; loss: 1.9; acc: 0.42
Batch: 580; loss: 1.93; acc: 0.39
Batch: 600; loss: 1.96; acc: 0.36
Batch: 620; loss: 1.98; acc: 0.36
Batch: 640; loss: 1.91; acc: 0.36
Batch: 660; loss: 1.79; acc: 0.53
Batch: 680; loss: 1.83; acc: 0.45
Batch: 700; loss: 1.78; acc: 0.47
Batch: 720; loss: 1.93; acc: 0.48
Batch: 740; loss: 1.87; acc: 0.42
Batch: 760; loss: 1.92; acc: 0.45
Batch: 780; loss: 1.82; acc: 0.48
Train Epoch over. train_loss: 2.02; train_accuracy: 0.32 

2.368412970099598e-05
4.2120627767872065e-06
Batch: 0; loss: 1.88; acc: 0.39
Batch: 20; loss: 1.83; acc: 0.47
Batch: 40; loss: 1.76; acc: 0.56
Batch: 60; loss: 1.81; acc: 0.48
Batch: 80; loss: 1.81; acc: 0.48
Batch: 100; loss: 1.82; acc: 0.55
Batch: 120; loss: 1.91; acc: 0.44
Batch: 140; loss: 1.79; acc: 0.5
Val Epoch over. val_loss: 1.848274672107332; val_accuracy: 0.4583996815286624 

The current subspace-distance is: 4.2120627767872065e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.55
Batch: 20; loss: 1.81; acc: 0.36
Batch: 40; loss: 1.78; acc: 0.48
Batch: 60; loss: 1.91; acc: 0.44
Batch: 80; loss: 1.76; acc: 0.52
Batch: 100; loss: 1.94; acc: 0.44
Batch: 120; loss: 1.96; acc: 0.39
Batch: 140; loss: 1.88; acc: 0.47
Batch: 160; loss: 2.03; acc: 0.27
Batch: 180; loss: 1.72; acc: 0.62
Batch: 200; loss: 1.86; acc: 0.47
Batch: 220; loss: 1.74; acc: 0.55
Batch: 240; loss: 1.73; acc: 0.53
Batch: 260; loss: 1.78; acc: 0.48
Batch: 280; loss: 1.85; acc: 0.48
Batch: 300; loss: 1.76; acc: 0.47
Batch: 320; loss: 1.9; acc: 0.44
Batch: 340; loss: 1.77; acc: 0.47
Batch: 360; loss: 1.77; acc: 0.55
Batch: 380; loss: 1.69; acc: 0.55
Batch: 400; loss: 1.75; acc: 0.52
Batch: 420; loss: 1.79; acc: 0.5
Batch: 440; loss: 1.8; acc: 0.48
Batch: 460; loss: 1.81; acc: 0.48
Batch: 480; loss: 1.73; acc: 0.58
Batch: 500; loss: 1.72; acc: 0.53
Batch: 520; loss: 1.93; acc: 0.39
Batch: 540; loss: 1.68; acc: 0.58
Batch: 560; loss: 1.75; acc: 0.52
Batch: 580; loss: 1.8; acc: 0.52
Batch: 600; loss: 1.75; acc: 0.52
Batch: 620; loss: 1.72; acc: 0.58
Batch: 640; loss: 1.75; acc: 0.55
Batch: 660; loss: 1.82; acc: 0.47
Batch: 680; loss: 1.71; acc: 0.59
Batch: 700; loss: 1.74; acc: 0.58
Batch: 720; loss: 1.79; acc: 0.48
Batch: 740; loss: 1.74; acc: 0.47
Batch: 760; loss: 1.7; acc: 0.47
Batch: 780; loss: 1.73; acc: 0.55
Train Epoch over. train_loss: 1.79; train_accuracy: 0.48 

2.749007944657933e-05
6.63543005430256e-06
Batch: 0; loss: 1.81; acc: 0.5
Batch: 20; loss: 1.71; acc: 0.55
Batch: 40; loss: 1.56; acc: 0.7
Batch: 60; loss: 1.71; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.53
Batch: 100; loss: 1.72; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.44
Batch: 140; loss: 1.72; acc: 0.47
Val Epoch over. val_loss: 1.7192979899181682; val_accuracy: 0.5222929936305732 

The current subspace-distance is: 6.63543005430256e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.56
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.71; acc: 0.48
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.79; acc: 0.44
Batch: 120; loss: 1.82; acc: 0.48
Batch: 140; loss: 1.73; acc: 0.52
Batch: 160; loss: 1.82; acc: 0.44
Batch: 180; loss: 1.67; acc: 0.56
Batch: 200; loss: 1.76; acc: 0.48
Batch: 220; loss: 1.69; acc: 0.5
Batch: 240; loss: 1.69; acc: 0.56
Batch: 260; loss: 1.72; acc: 0.52
Batch: 280; loss: 1.69; acc: 0.53
Batch: 300; loss: 1.75; acc: 0.52
Batch: 320; loss: 1.78; acc: 0.52
Batch: 340; loss: 1.68; acc: 0.44
Batch: 360; loss: 1.8; acc: 0.39
Batch: 380; loss: 1.71; acc: 0.5
Batch: 400; loss: 1.7; acc: 0.58
Batch: 420; loss: 1.69; acc: 0.55
Batch: 440; loss: 1.69; acc: 0.5
Batch: 460; loss: 1.82; acc: 0.39
Batch: 480; loss: 1.7; acc: 0.5
Batch: 500; loss: 1.76; acc: 0.52
Batch: 520; loss: 1.61; acc: 0.58
Batch: 540; loss: 1.71; acc: 0.53
Batch: 560; loss: 1.72; acc: 0.58
Batch: 580; loss: 1.64; acc: 0.58
Batch: 600; loss: 1.67; acc: 0.55
Batch: 620; loss: 1.74; acc: 0.42
Batch: 640; loss: 1.72; acc: 0.52
Batch: 660; loss: 1.7; acc: 0.52
Batch: 680; loss: 1.62; acc: 0.59
Batch: 700; loss: 1.76; acc: 0.5
Batch: 720; loss: 1.61; acc: 0.52
Batch: 740; loss: 1.73; acc: 0.5
Batch: 760; loss: 1.75; acc: 0.48
Batch: 780; loss: 1.59; acc: 0.66
Train Epoch over. train_loss: 1.7; train_accuracy: 0.52 

3.016173286596313e-05
9.346668775833678e-06
Batch: 0; loss: 1.79; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.59
Batch: 40; loss: 1.44; acc: 0.77
Batch: 60; loss: 1.62; acc: 0.56
Batch: 80; loss: 1.6; acc: 0.59
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.71; acc: 0.48
Batch: 140; loss: 1.62; acc: 0.61
Val Epoch over. val_loss: 1.6441770108642093; val_accuracy: 0.5522492038216561 

The current subspace-distance is: 9.346668775833678e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.62
Batch: 20; loss: 1.68; acc: 0.53
Batch: 40; loss: 1.57; acc: 0.61
Batch: 60; loss: 1.81; acc: 0.42
Batch: 80; loss: 1.61; acc: 0.55
Batch: 100; loss: 1.64; acc: 0.52
Batch: 120; loss: 1.62; acc: 0.61
Batch: 140; loss: 1.7; acc: 0.45
Batch: 160; loss: 1.53; acc: 0.67
Batch: 180; loss: 1.61; acc: 0.53
Batch: 200; loss: 1.6; acc: 0.59
Batch: 220; loss: 1.63; acc: 0.53
Batch: 240; loss: 1.72; acc: 0.52
Batch: 260; loss: 1.57; acc: 0.64
Batch: 280; loss: 1.72; acc: 0.52
Batch: 300; loss: 1.57; acc: 0.66
Batch: 320; loss: 1.64; acc: 0.58
Batch: 340; loss: 1.61; acc: 0.53
Batch: 360; loss: 1.65; acc: 0.64
Batch: 380; loss: 1.69; acc: 0.56
Batch: 400; loss: 1.66; acc: 0.52
Batch: 420; loss: 1.58; acc: 0.58
Batch: 440; loss: 1.63; acc: 0.52
Batch: 460; loss: 1.63; acc: 0.53
Batch: 480; loss: 1.58; acc: 0.64
Batch: 500; loss: 1.7; acc: 0.52
Batch: 520; loss: 1.61; acc: 0.53
Batch: 540; loss: 1.78; acc: 0.44
Batch: 560; loss: 1.66; acc: 0.52
Batch: 580; loss: 1.66; acc: 0.44
Batch: 600; loss: 1.69; acc: 0.52
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.75; acc: 0.48
Batch: 660; loss: 1.63; acc: 0.58
Batch: 680; loss: 1.62; acc: 0.58
Batch: 700; loss: 1.65; acc: 0.5
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.62; acc: 0.53
Batch: 760; loss: 1.69; acc: 0.53
Batch: 780; loss: 1.69; acc: 0.48
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.218428901163861e-05
8.048077688727062e-06
Batch: 0; loss: 1.73; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.35; acc: 0.77
Batch: 60; loss: 1.54; acc: 0.61
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.55; acc: 0.55
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.53; acc: 0.69
Val Epoch over. val_loss: 1.5797729932578506; val_accuracy: 0.5780254777070064 

The current subspace-distance is: 8.048077688727062e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.57; acc: 0.59
Batch: 20; loss: 1.55; acc: 0.64
Batch: 40; loss: 1.59; acc: 0.56
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.61; acc: 0.59
Batch: 100; loss: 1.54; acc: 0.58
Batch: 120; loss: 1.66; acc: 0.55
Batch: 140; loss: 1.5; acc: 0.69
Batch: 160; loss: 1.55; acc: 0.64
Batch: 180; loss: 1.51; acc: 0.62
Batch: 200; loss: 1.75; acc: 0.52
Batch: 220; loss: 1.84; acc: 0.38
Batch: 240; loss: 1.58; acc: 0.52
Batch: 260; loss: 1.61; acc: 0.64
Batch: 280; loss: 1.69; acc: 0.53
Batch: 300; loss: 1.65; acc: 0.52
Batch: 320; loss: 1.58; acc: 0.55
Batch: 340; loss: 1.6; acc: 0.52
Batch: 360; loss: 1.57; acc: 0.55
Batch: 380; loss: 1.56; acc: 0.56
Batch: 400; loss: 1.7; acc: 0.53
Batch: 420; loss: 1.6; acc: 0.5
Batch: 440; loss: 1.54; acc: 0.61
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.75; acc: 0.5
Batch: 500; loss: 1.6; acc: 0.55
Batch: 520; loss: 1.62; acc: 0.53
Batch: 540; loss: 1.69; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.58
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.57; acc: 0.62
Batch: 620; loss: 1.65; acc: 0.45
Batch: 640; loss: 1.75; acc: 0.47
Batch: 660; loss: 1.69; acc: 0.48
Batch: 680; loss: 1.59; acc: 0.62
Batch: 700; loss: 1.53; acc: 0.58
Batch: 720; loss: 1.62; acc: 0.44
Batch: 740; loss: 1.56; acc: 0.61
Batch: 760; loss: 1.56; acc: 0.55
Batch: 780; loss: 1.42; acc: 0.66
Train Epoch over. train_loss: 1.6; train_accuracy: 0.56 

3.515142452670261e-05
1.3308032976055983e-05
Batch: 0; loss: 1.69; acc: 0.5
Batch: 20; loss: 1.54; acc: 0.62
Batch: 40; loss: 1.3; acc: 0.83
Batch: 60; loss: 1.5; acc: 0.58
Batch: 80; loss: 1.45; acc: 0.66
Batch: 100; loss: 1.49; acc: 0.58
Batch: 120; loss: 1.52; acc: 0.61
Batch: 140; loss: 1.49; acc: 0.7
Val Epoch over. val_loss: 1.5361462550558103; val_accuracy: 0.59265525477707 

The current subspace-distance is: 1.3308032976055983e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.61
Batch: 20; loss: 1.61; acc: 0.58
Batch: 40; loss: 1.56; acc: 0.56
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.61; acc: 0.55
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.46; acc: 0.69
Batch: 160; loss: 1.51; acc: 0.62
Batch: 180; loss: 1.5; acc: 0.56
Batch: 200; loss: 1.5; acc: 0.59
Batch: 220; loss: 1.49; acc: 0.58
Batch: 240; loss: 1.63; acc: 0.58
Batch: 260; loss: 1.74; acc: 0.45
Batch: 280; loss: 1.6; acc: 0.55
Batch: 300; loss: 1.52; acc: 0.59
Batch: 320; loss: 1.57; acc: 0.56
Batch: 340; loss: 1.54; acc: 0.62
Batch: 360; loss: 1.61; acc: 0.52
Batch: 380; loss: 1.55; acc: 0.56
Batch: 400; loss: 1.58; acc: 0.55
Batch: 420; loss: 1.65; acc: 0.48
Batch: 440; loss: 1.44; acc: 0.67
Batch: 460; loss: 1.43; acc: 0.72
Batch: 480; loss: 1.53; acc: 0.61
Batch: 500; loss: 1.54; acc: 0.56
Batch: 520; loss: 1.65; acc: 0.58
Batch: 540; loss: 1.63; acc: 0.45
Batch: 560; loss: 1.64; acc: 0.52
Batch: 580; loss: 1.51; acc: 0.59
Batch: 600; loss: 1.52; acc: 0.59
Batch: 620; loss: 1.55; acc: 0.59
Batch: 640; loss: 1.59; acc: 0.56
Batch: 660; loss: 1.55; acc: 0.56
Batch: 680; loss: 1.47; acc: 0.59
Batch: 700; loss: 1.57; acc: 0.58
Batch: 720; loss: 1.54; acc: 0.62
Batch: 740; loss: 1.57; acc: 0.53
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.5; acc: 0.61
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

3.706438292283565e-05
1.1388220627850387e-05
Batch: 0; loss: 1.67; acc: 0.53
Batch: 20; loss: 1.49; acc: 0.62
Batch: 40; loss: 1.25; acc: 0.78
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.4; acc: 0.69
Batch: 100; loss: 1.47; acc: 0.55
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.45; acc: 0.64
Val Epoch over. val_loss: 1.4986813053203996; val_accuracy: 0.6012141719745223 

The current subspace-distance is: 1.1388220627850387e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.62
Batch: 40; loss: 1.53; acc: 0.58
Batch: 60; loss: 1.52; acc: 0.62
Batch: 80; loss: 1.56; acc: 0.58
Batch: 100; loss: 1.47; acc: 0.62
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.49; acc: 0.58
Batch: 160; loss: 1.43; acc: 0.69
Batch: 180; loss: 1.48; acc: 0.58
Batch: 200; loss: 1.5; acc: 0.59
Batch: 220; loss: 1.56; acc: 0.55
Batch: 240; loss: 1.49; acc: 0.53
Batch: 260; loss: 1.51; acc: 0.58
Batch: 280; loss: 1.44; acc: 0.66
Batch: 300; loss: 1.51; acc: 0.53
Batch: 320; loss: 1.61; acc: 0.53
Batch: 340; loss: 1.66; acc: 0.48
Batch: 360; loss: 1.5; acc: 0.61
Batch: 380; loss: 1.61; acc: 0.5
Batch: 400; loss: 1.4; acc: 0.66
Batch: 420; loss: 1.43; acc: 0.62
Batch: 440; loss: 1.52; acc: 0.58
Batch: 460; loss: 1.39; acc: 0.61
Batch: 480; loss: 1.61; acc: 0.59
Batch: 500; loss: 1.46; acc: 0.67
Batch: 520; loss: 1.43; acc: 0.66
Batch: 540; loss: 1.6; acc: 0.56
Batch: 560; loss: 1.5; acc: 0.58
Batch: 580; loss: 1.61; acc: 0.52
Batch: 600; loss: 1.46; acc: 0.55
Batch: 620; loss: 1.53; acc: 0.53
Batch: 640; loss: 1.68; acc: 0.52
Batch: 660; loss: 1.53; acc: 0.55
Batch: 680; loss: 1.64; acc: 0.5
Batch: 700; loss: 1.5; acc: 0.62
Batch: 720; loss: 1.55; acc: 0.55
Batch: 740; loss: 1.47; acc: 0.61
Batch: 760; loss: 1.61; acc: 0.45
Batch: 780; loss: 1.59; acc: 0.55
Train Epoch over. train_loss: 1.52; train_accuracy: 0.58 

3.885116530000232e-05
1.0613123777147848e-05
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.44; acc: 0.66
Batch: 40; loss: 1.2; acc: 0.78
Batch: 60; loss: 1.43; acc: 0.62
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.42; acc: 0.58
Batch: 120; loss: 1.41; acc: 0.72
Batch: 140; loss: 1.42; acc: 0.7
Val Epoch over. val_loss: 1.456911667137389; val_accuracy: 0.6173367834394905 

The current subspace-distance is: 1.0613123777147848e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.67
Batch: 20; loss: 1.43; acc: 0.62
Batch: 40; loss: 1.55; acc: 0.55
Batch: 60; loss: 1.47; acc: 0.66
Batch: 80; loss: 1.57; acc: 0.59
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.4; acc: 0.69
Batch: 140; loss: 1.49; acc: 0.59
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.61; acc: 0.52
Batch: 200; loss: 1.51; acc: 0.5
Batch: 220; loss: 1.57; acc: 0.59
Batch: 240; loss: 1.51; acc: 0.58
Batch: 260; loss: 1.46; acc: 0.56
Batch: 280; loss: 1.46; acc: 0.56
Batch: 300; loss: 1.57; acc: 0.5
Batch: 320; loss: 1.45; acc: 0.64
Batch: 340; loss: 1.51; acc: 0.52
Batch: 360; loss: 1.44; acc: 0.61
Batch: 380; loss: 1.46; acc: 0.59
Batch: 400; loss: 1.39; acc: 0.67
Batch: 420; loss: 1.5; acc: 0.61
Batch: 440; loss: 1.51; acc: 0.58
Batch: 460; loss: 1.64; acc: 0.5
Batch: 480; loss: 1.67; acc: 0.45
Batch: 500; loss: 1.57; acc: 0.56
Batch: 520; loss: 1.35; acc: 0.59
Batch: 540; loss: 1.47; acc: 0.62
Batch: 560; loss: 1.53; acc: 0.56
Batch: 580; loss: 1.58; acc: 0.48
Batch: 600; loss: 1.48; acc: 0.67
Batch: 620; loss: 1.73; acc: 0.44
Batch: 640; loss: 1.45; acc: 0.58
Batch: 660; loss: 1.53; acc: 0.61
Batch: 680; loss: 1.51; acc: 0.59
Batch: 700; loss: 1.53; acc: 0.55
Batch: 720; loss: 1.5; acc: 0.58
Batch: 740; loss: 1.41; acc: 0.56
Batch: 760; loss: 1.58; acc: 0.56
Batch: 780; loss: 1.47; acc: 0.66
Train Epoch over. train_loss: 1.49; train_accuracy: 0.59 

4.01555844291579e-05
1.1025728781532962e-05
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.41; acc: 0.61
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.35; acc: 0.67
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.42; acc: 0.66
Batch: 140; loss: 1.4; acc: 0.69
Val Epoch over. val_loss: 1.4531290720982157; val_accuracy: 0.6025079617834395 

The current subspace-distance is: 1.1025728781532962e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.66
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 1.45; acc: 0.61
Batch: 60; loss: 1.47; acc: 0.5
Batch: 80; loss: 1.46; acc: 0.56
Batch: 100; loss: 1.46; acc: 0.58
Batch: 120; loss: 1.5; acc: 0.52
Batch: 140; loss: 1.55; acc: 0.5
Batch: 160; loss: 1.48; acc: 0.58
Batch: 180; loss: 1.54; acc: 0.56
Batch: 200; loss: 1.33; acc: 0.64
Batch: 220; loss: 1.54; acc: 0.5
Batch: 240; loss: 1.47; acc: 0.62
Batch: 260; loss: 1.38; acc: 0.64
Batch: 280; loss: 1.43; acc: 0.59
Batch: 300; loss: 1.41; acc: 0.64
Batch: 320; loss: 1.61; acc: 0.59
Batch: 340; loss: 1.61; acc: 0.5
Batch: 360; loss: 1.38; acc: 0.69
Batch: 380; loss: 1.51; acc: 0.56
Batch: 400; loss: 1.52; acc: 0.52
Batch: 420; loss: 1.64; acc: 0.38
Batch: 440; loss: 1.6; acc: 0.5
Batch: 460; loss: 1.44; acc: 0.66
Batch: 480; loss: 1.43; acc: 0.53
Batch: 500; loss: 1.28; acc: 0.66
Batch: 520; loss: 1.53; acc: 0.56
Batch: 540; loss: 1.33; acc: 0.7
Batch: 560; loss: 1.47; acc: 0.59
Batch: 580; loss: 1.49; acc: 0.61
Batch: 600; loss: 1.42; acc: 0.61
Batch: 620; loss: 1.39; acc: 0.62
Batch: 640; loss: 1.53; acc: 0.55
Batch: 660; loss: 1.32; acc: 0.69
Batch: 680; loss: 1.54; acc: 0.52
Batch: 700; loss: 1.71; acc: 0.5
Batch: 720; loss: 1.39; acc: 0.64
Batch: 740; loss: 1.43; acc: 0.59
Batch: 760; loss: 1.49; acc: 0.58
Batch: 780; loss: 1.4; acc: 0.67
Train Epoch over. train_loss: 1.48; train_accuracy: 0.59 

4.1573221096768975e-05
1.2766178770107217e-05
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.39; acc: 0.66
Batch: 40; loss: 1.19; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.58
Batch: 80; loss: 1.32; acc: 0.69
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.42; acc: 0.66
Batch: 140; loss: 1.36; acc: 0.7
Val Epoch over. val_loss: 1.4321816696482859; val_accuracy: 0.6071855095541401 

The current subspace-distance is: 1.2766178770107217e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.48; acc: 0.58
Batch: 20; loss: 1.47; acc: 0.61
Batch: 40; loss: 1.46; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.38; acc: 0.64
Batch: 120; loss: 1.43; acc: 0.59
Batch: 140; loss: 1.47; acc: 0.61
Batch: 160; loss: 1.39; acc: 0.64
Batch: 180; loss: 1.47; acc: 0.62
Batch: 200; loss: 1.39; acc: 0.66
Batch: 220; loss: 1.38; acc: 0.58
Batch: 240; loss: 1.48; acc: 0.56
Batch: 260; loss: 1.4; acc: 0.64
Batch: 280; loss: 1.35; acc: 0.67
Batch: 300; loss: 1.49; acc: 0.58
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.64; acc: 0.52
Batch: 360; loss: 1.5; acc: 0.55
Batch: 380; loss: 1.34; acc: 0.69
Batch: 400; loss: 1.39; acc: 0.64
Batch: 420; loss: 1.5; acc: 0.62
Batch: 440; loss: 1.49; acc: 0.61
Batch: 460; loss: 1.58; acc: 0.52
Batch: 480; loss: 1.4; acc: 0.62
Batch: 500; loss: 1.44; acc: 0.66
Batch: 520; loss: 1.52; acc: 0.59
Batch: 540; loss: 1.38; acc: 0.69
Batch: 560; loss: 1.39; acc: 0.62
Batch: 580; loss: 1.61; acc: 0.5
Batch: 600; loss: 1.56; acc: 0.53
Batch: 620; loss: 1.58; acc: 0.52
Batch: 640; loss: 1.28; acc: 0.75
Batch: 660; loss: 1.36; acc: 0.69
Batch: 680; loss: 1.25; acc: 0.72
Batch: 700; loss: 1.5; acc: 0.59
Batch: 720; loss: 1.41; acc: 0.62
Batch: 740; loss: 1.37; acc: 0.69
Batch: 760; loss: 1.42; acc: 0.62
Batch: 780; loss: 1.35; acc: 0.69
Train Epoch over. train_loss: 1.46; train_accuracy: 0.59 

4.312000601203181e-05
1.3228301213530358e-05
Batch: 0; loss: 1.62; acc: 0.47
Batch: 20; loss: 1.35; acc: 0.62
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.3; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.64
Batch: 140; loss: 1.31; acc: 0.72
Val Epoch over. val_loss: 1.4076795092054233; val_accuracy: 0.613156847133758 

The current subspace-distance is: 1.3228301213530358e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.49; acc: 0.66
Batch: 20; loss: 1.48; acc: 0.61
Batch: 40; loss: 1.44; acc: 0.67
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.53
Batch: 100; loss: 1.49; acc: 0.52
Batch: 120; loss: 1.42; acc: 0.52
Batch: 140; loss: 1.48; acc: 0.67
Batch: 160; loss: 1.45; acc: 0.61
Batch: 180; loss: 1.36; acc: 0.67
Batch: 200; loss: 1.58; acc: 0.52
Batch: 220; loss: 1.6; acc: 0.53
Batch: 240; loss: 1.46; acc: 0.53
Batch: 260; loss: 1.6; acc: 0.42
Batch: 280; loss: 1.52; acc: 0.55
Batch: 300; loss: 1.46; acc: 0.62
Batch: 320; loss: 1.36; acc: 0.64
Batch: 340; loss: 1.44; acc: 0.58
Batch: 360; loss: 1.53; acc: 0.58
Batch: 380; loss: 1.39; acc: 0.58
Batch: 400; loss: 1.37; acc: 0.69
Batch: 420; loss: 1.41; acc: 0.58
Batch: 440; loss: 1.43; acc: 0.64
Batch: 460; loss: 1.38; acc: 0.66
Batch: 480; loss: 1.53; acc: 0.5
Batch: 500; loss: 1.49; acc: 0.56
Batch: 520; loss: 1.48; acc: 0.53
Batch: 540; loss: 1.35; acc: 0.62
Batch: 560; loss: 1.61; acc: 0.52
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.3; acc: 0.7
Batch: 620; loss: 1.51; acc: 0.55
Batch: 640; loss: 1.38; acc: 0.69
Batch: 660; loss: 1.43; acc: 0.61
Batch: 680; loss: 1.51; acc: 0.5
Batch: 700; loss: 1.39; acc: 0.59
Batch: 720; loss: 1.4; acc: 0.62
Batch: 740; loss: 1.43; acc: 0.62
Batch: 760; loss: 1.42; acc: 0.59
Batch: 780; loss: 1.46; acc: 0.59
Train Epoch over. train_loss: 1.46; train_accuracy: 0.59 

4.4094085751567036e-05
1.1528137292771135e-05
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.35; acc: 0.62
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.31; acc: 0.69
Batch: 100; loss: 1.39; acc: 0.64
Batch: 120; loss: 1.42; acc: 0.66
Batch: 140; loss: 1.32; acc: 0.69
Val Epoch over. val_loss: 1.4173540734941033; val_accuracy: 0.6068869426751592 

The current subspace-distance is: 1.1528137292771135e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.43; acc: 0.56
Batch: 20; loss: 1.59; acc: 0.56
Batch: 40; loss: 1.45; acc: 0.59
Batch: 60; loss: 1.26; acc: 0.69
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.53; acc: 0.55
Batch: 120; loss: 1.54; acc: 0.56
Batch: 140; loss: 1.44; acc: 0.56
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.47; acc: 0.55
Batch: 200; loss: 1.42; acc: 0.61
Batch: 220; loss: 1.37; acc: 0.64
Batch: 240; loss: 1.35; acc: 0.66
Batch: 260; loss: 1.39; acc: 0.61
Batch: 280; loss: 1.57; acc: 0.59
Batch: 300; loss: 1.38; acc: 0.66
Batch: 320; loss: 1.42; acc: 0.61
Batch: 340; loss: 1.47; acc: 0.62
Batch: 360; loss: 1.5; acc: 0.56
Batch: 380; loss: 1.46; acc: 0.59
Batch: 400; loss: 1.4; acc: 0.64
Batch: 420; loss: 1.29; acc: 0.75
Batch: 440; loss: 1.44; acc: 0.55
Batch: 460; loss: 1.47; acc: 0.58
Batch: 480; loss: 1.56; acc: 0.58
Batch: 500; loss: 1.26; acc: 0.81
Batch: 520; loss: 1.28; acc: 0.7
Batch: 540; loss: 1.53; acc: 0.55
Batch: 560; loss: 1.61; acc: 0.52
Batch: 580; loss: 1.46; acc: 0.55
Batch: 600; loss: 1.51; acc: 0.58
Batch: 620; loss: 1.44; acc: 0.58
Batch: 640; loss: 1.42; acc: 0.62
Batch: 660; loss: 1.44; acc: 0.56
Batch: 680; loss: 1.54; acc: 0.58
Batch: 700; loss: 1.56; acc: 0.53
Batch: 720; loss: 1.48; acc: 0.61
Batch: 740; loss: 1.42; acc: 0.56
Batch: 760; loss: 1.54; acc: 0.47
Batch: 780; loss: 1.37; acc: 0.66
Train Epoch over. train_loss: 1.46; train_accuracy: 0.59 

4.418550815898925e-05
1.407222225680016e-05
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.35; acc: 0.61
Batch: 40; loss: 1.17; acc: 0.73
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.29; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.66
Batch: 120; loss: 1.42; acc: 0.59
Batch: 140; loss: 1.3; acc: 0.7
Val Epoch over. val_loss: 1.4073946073556403; val_accuracy: 0.6105692675159236 

The current subspace-distance is: 1.407222225680016e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.4; acc: 0.59
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.42; acc: 0.59
Batch: 60; loss: 1.43; acc: 0.53
Batch: 80; loss: 1.41; acc: 0.67
Batch: 100; loss: 1.48; acc: 0.55
Batch: 120; loss: 1.49; acc: 0.53
Batch: 140; loss: 1.48; acc: 0.53
Batch: 160; loss: 1.43; acc: 0.61
Batch: 180; loss: 1.46; acc: 0.61
Batch: 200; loss: 1.57; acc: 0.53
Batch: 220; loss: 1.4; acc: 0.67
Batch: 240; loss: 1.41; acc: 0.66
Batch: 260; loss: 1.43; acc: 0.56
Batch: 280; loss: 1.51; acc: 0.59
Batch: 300; loss: 1.51; acc: 0.56
Batch: 320; loss: 1.62; acc: 0.53
Batch: 340; loss: 1.35; acc: 0.55
Batch: 360; loss: 1.57; acc: 0.61
Batch: 380; loss: 1.6; acc: 0.41
Batch: 400; loss: 1.39; acc: 0.64
Batch: 420; loss: 1.48; acc: 0.56
Batch: 440; loss: 1.45; acc: 0.59
Batch: 460; loss: 1.62; acc: 0.53
Batch: 480; loss: 1.5; acc: 0.59
Batch: 500; loss: 1.45; acc: 0.55
Batch: 520; loss: 1.41; acc: 0.61
Batch: 540; loss: 1.32; acc: 0.66
Batch: 560; loss: 1.66; acc: 0.53
Batch: 580; loss: 1.47; acc: 0.58
Batch: 600; loss: 1.46; acc: 0.62
Batch: 620; loss: 1.52; acc: 0.47
Batch: 640; loss: 1.38; acc: 0.61
Batch: 660; loss: 1.69; acc: 0.41
Batch: 680; loss: 1.48; acc: 0.59
Batch: 700; loss: 1.39; acc: 0.69
Batch: 720; loss: 1.52; acc: 0.61
Batch: 740; loss: 1.62; acc: 0.58
Batch: 760; loss: 1.46; acc: 0.56
Batch: 780; loss: 1.52; acc: 0.58
Train Epoch over. train_loss: 1.45; train_accuracy: 0.59 

4.5330190914683044e-05
1.4282944903243333e-05
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 1.16; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.36; acc: 0.64
Batch: 120; loss: 1.4; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.73
Val Epoch over. val_loss: 1.4003128701714194; val_accuracy: 0.6163415605095541 

The current subspace-distance is: 1.4282944903243333e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.4; acc: 0.62
Batch: 20; loss: 1.54; acc: 0.58
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.43; acc: 0.61
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.32; acc: 0.72
Batch: 120; loss: 1.43; acc: 0.58
Batch: 140; loss: 1.53; acc: 0.61
Batch: 160; loss: 1.45; acc: 0.58
Batch: 180; loss: 1.66; acc: 0.47
Batch: 200; loss: 1.37; acc: 0.59
Batch: 220; loss: 1.48; acc: 0.62
Batch: 240; loss: 1.49; acc: 0.58
Batch: 260; loss: 1.57; acc: 0.5
Batch: 280; loss: 1.49; acc: 0.66
Batch: 300; loss: 1.44; acc: 0.58
Batch: 320; loss: 1.52; acc: 0.55
Batch: 340; loss: 1.42; acc: 0.59
Batch: 360; loss: 1.54; acc: 0.56
Batch: 380; loss: 1.47; acc: 0.69
Batch: 400; loss: 1.42; acc: 0.62
Batch: 420; loss: 1.48; acc: 0.58
Batch: 440; loss: 1.42; acc: 0.55
Batch: 460; loss: 1.49; acc: 0.58
Batch: 480; loss: 1.42; acc: 0.56
Batch: 500; loss: 1.4; acc: 0.62
Batch: 520; loss: 1.38; acc: 0.67
Batch: 540; loss: 1.45; acc: 0.56
Batch: 560; loss: 1.52; acc: 0.5
Batch: 580; loss: 1.49; acc: 0.58
Batch: 600; loss: 1.51; acc: 0.61
Batch: 620; loss: 1.48; acc: 0.55
Batch: 640; loss: 1.45; acc: 0.62
Batch: 660; loss: 1.31; acc: 0.66
Batch: 680; loss: 1.49; acc: 0.56
Batch: 700; loss: 1.43; acc: 0.62
Batch: 720; loss: 1.44; acc: 0.56
Batch: 740; loss: 1.49; acc: 0.53
Batch: 760; loss: 1.54; acc: 0.44
Batch: 780; loss: 1.54; acc: 0.61
Train Epoch over. train_loss: 1.45; train_accuracy: 0.59 

4.5422442781273276e-05
1.576492650201544e-05
Batch: 0; loss: 1.62; acc: 0.47
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 1.18; acc: 0.72
Batch: 60; loss: 1.47; acc: 0.53
Batch: 80; loss: 1.3; acc: 0.66
Batch: 100; loss: 1.38; acc: 0.64
Batch: 120; loss: 1.42; acc: 0.61
Batch: 140; loss: 1.31; acc: 0.67
Val Epoch over. val_loss: 1.4134331015264912; val_accuracy: 0.6032046178343949 

The current subspace-distance is: 1.576492650201544e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.61
Batch: 20; loss: 1.37; acc: 0.66
Batch: 40; loss: 1.44; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.59
Batch: 100; loss: 1.54; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.61
Batch: 140; loss: 1.46; acc: 0.55
Batch: 160; loss: 1.27; acc: 0.72
Batch: 180; loss: 1.47; acc: 0.56
Batch: 200; loss: 1.48; acc: 0.62
Batch: 220; loss: 1.56; acc: 0.58
Batch: 240; loss: 1.55; acc: 0.48
Batch: 260; loss: 1.41; acc: 0.66
Batch: 280; loss: 1.38; acc: 0.61
Batch: 300; loss: 1.63; acc: 0.48
Batch: 320; loss: 1.4; acc: 0.56
Batch: 340; loss: 1.58; acc: 0.48
Batch: 360; loss: 1.46; acc: 0.58
Batch: 380; loss: 1.51; acc: 0.56
Batch: 400; loss: 1.52; acc: 0.53
Batch: 420; loss: 1.53; acc: 0.52
Batch: 440; loss: 1.48; acc: 0.58
Batch: 460; loss: 1.47; acc: 0.56
Batch: 480; loss: 1.61; acc: 0.5
Batch: 500; loss: 1.55; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.59
Batch: 540; loss: 1.48; acc: 0.58
Batch: 560; loss: 1.41; acc: 0.59
Batch: 580; loss: 1.37; acc: 0.58
Batch: 600; loss: 1.29; acc: 0.67
Batch: 620; loss: 1.6; acc: 0.47
Batch: 640; loss: 1.42; acc: 0.64
Batch: 660; loss: 1.49; acc: 0.5
Batch: 680; loss: 1.41; acc: 0.7
Batch: 700; loss: 1.39; acc: 0.58
Batch: 720; loss: 1.42; acc: 0.64
Batch: 740; loss: 1.49; acc: 0.58
Batch: 760; loss: 1.22; acc: 0.73
Batch: 780; loss: 1.47; acc: 0.53
Train Epoch over. train_loss: 1.45; train_accuracy: 0.59 

4.591310062096454e-05
1.3804305126541294e-05
Batch: 0; loss: 1.6; acc: 0.48
Batch: 20; loss: 1.32; acc: 0.64
Batch: 40; loss: 1.15; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.27; acc: 0.67
Batch: 100; loss: 1.35; acc: 0.64
Batch: 120; loss: 1.4; acc: 0.61
Batch: 140; loss: 1.28; acc: 0.72
Val Epoch over. val_loss: 1.3907226597427562; val_accuracy: 0.6099721337579618 

The current subspace-distance is: 1.3804305126541294e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.41; acc: 0.67
Batch: 20; loss: 1.57; acc: 0.5
Batch: 40; loss: 1.45; acc: 0.59
Batch: 60; loss: 1.44; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.66
Batch: 100; loss: 1.45; acc: 0.58
Batch: 120; loss: 1.53; acc: 0.55
Batch: 140; loss: 1.45; acc: 0.59
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.53; acc: 0.56
Batch: 200; loss: 1.53; acc: 0.56
Batch: 220; loss: 1.48; acc: 0.47
Batch: 240; loss: 1.5; acc: 0.56
Batch: 260; loss: 1.44; acc: 0.55
Batch: 280; loss: 1.45; acc: 0.59
Batch: 300; loss: 1.48; acc: 0.52
Batch: 320; loss: 1.44; acc: 0.58
Batch: 340; loss: 1.34; acc: 0.62
Batch: 360; loss: 1.5; acc: 0.5
Batch: 380; loss: 1.48; acc: 0.53
Batch: 400; loss: 1.35; acc: 0.75
Batch: 420; loss: 1.51; acc: 0.55
Batch: 440; loss: 1.48; acc: 0.56
Batch: 460; loss: 1.38; acc: 0.69
Batch: 480; loss: 1.48; acc: 0.53
Batch: 500; loss: 1.44; acc: 0.59
Batch: 520; loss: 1.55; acc: 0.52
Batch: 540; loss: 1.33; acc: 0.62
Batch: 560; loss: 1.55; acc: 0.52
Batch: 580; loss: 1.48; acc: 0.52
Batch: 600; loss: 1.43; acc: 0.58
Batch: 620; loss: 1.54; acc: 0.56
Batch: 640; loss: 1.54; acc: 0.53
Batch: 660; loss: 1.38; acc: 0.61
Batch: 680; loss: 1.51; acc: 0.52
Batch: 700; loss: 1.37; acc: 0.58
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.41; acc: 0.67
Batch: 760; loss: 1.27; acc: 0.73
Batch: 780; loss: 1.47; acc: 0.55
Train Epoch over. train_loss: 1.45; train_accuracy: 0.58 

4.661860293708742e-05
1.7058828234439716e-05
Batch: 0; loss: 1.62; acc: 0.48
Batch: 20; loss: 1.35; acc: 0.64
Batch: 40; loss: 1.17; acc: 0.73
Batch: 60; loss: 1.47; acc: 0.53
Batch: 80; loss: 1.31; acc: 0.66
Batch: 100; loss: 1.38; acc: 0.66
Batch: 120; loss: 1.42; acc: 0.59
Batch: 140; loss: 1.3; acc: 0.69
Val Epoch over. val_loss: 1.4098676519029458; val_accuracy: 0.6051950636942676 

The current subspace-distance is: 1.7058828234439716e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.42; acc: 0.58
Batch: 20; loss: 1.56; acc: 0.47
Batch: 40; loss: 1.36; acc: 0.61
Batch: 60; loss: 1.42; acc: 0.64
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.44; acc: 0.62
Batch: 140; loss: 1.43; acc: 0.59
Batch: 160; loss: 1.5; acc: 0.55
Batch: 180; loss: 1.32; acc: 0.69
Batch: 200; loss: 1.36; acc: 0.67
Batch: 220; loss: 1.56; acc: 0.53
Batch: 240; loss: 1.52; acc: 0.52
Batch: 260; loss: 1.53; acc: 0.56
Batch: 280; loss: 1.32; acc: 0.67
Batch: 300; loss: 1.44; acc: 0.56
Batch: 320; loss: 1.44; acc: 0.58
Batch: 340; loss: 1.36; acc: 0.67
Batch: 360; loss: 1.5; acc: 0.53
Batch: 380; loss: 1.37; acc: 0.62
Batch: 400; loss: 1.43; acc: 0.56
Batch: 420; loss: 1.56; acc: 0.5
Batch: 440; loss: 1.44; acc: 0.53
Batch: 460; loss: 1.49; acc: 0.55
Batch: 480; loss: 1.45; acc: 0.59
Batch: 500; loss: 1.36; acc: 0.67
Batch: 520; loss: 1.46; acc: 0.62
Batch: 540; loss: 1.37; acc: 0.58
Batch: 560; loss: 1.46; acc: 0.59
Batch: 580; loss: 1.38; acc: 0.58
Batch: 600; loss: 1.45; acc: 0.61
Batch: 620; loss: 1.42; acc: 0.62
Batch: 640; loss: 1.34; acc: 0.59
Batch: 660; loss: 1.44; acc: 0.56
Batch: 680; loss: 1.5; acc: 0.56
Batch: 700; loss: 1.63; acc: 0.58
Batch: 720; loss: 1.49; acc: 0.58
Batch: 740; loss: 1.48; acc: 0.53
Batch: 760; loss: 1.44; acc: 0.55
Batch: 780; loss: 1.43; acc: 0.66
Train Epoch over. train_loss: 1.45; train_accuracy: 0.58 

4.546675336314365e-05
1.2606124982994515e-05
Batch: 0; loss: 1.61; acc: 0.47
Batch: 20; loss: 1.34; acc: 0.62
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.62
Batch: 140; loss: 1.27; acc: 0.69
Val Epoch over. val_loss: 1.3969705681891957; val_accuracy: 0.6091759554140127 

The current subspace-distance is: 1.2606124982994515e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.32; acc: 0.62
Batch: 20; loss: 1.49; acc: 0.62
Batch: 40; loss: 1.41; acc: 0.55
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.59
Batch: 100; loss: 1.45; acc: 0.58
Batch: 120; loss: 1.74; acc: 0.45
Batch: 140; loss: 1.3; acc: 0.67
Batch: 160; loss: 1.5; acc: 0.53
Batch: 180; loss: 1.46; acc: 0.66
Batch: 200; loss: 1.43; acc: 0.53
Batch: 220; loss: 1.35; acc: 0.59
Batch: 240; loss: 1.41; acc: 0.64
Batch: 260; loss: 1.34; acc: 0.64
Batch: 280; loss: 1.51; acc: 0.52
Batch: 300; loss: 1.42; acc: 0.67
Batch: 320; loss: 1.49; acc: 0.52
Batch: 340; loss: 1.45; acc: 0.61
Batch: 360; loss: 1.44; acc: 0.62
Batch: 380; loss: 1.41; acc: 0.59
Batch: 400; loss: 1.4; acc: 0.62
Batch: 420; loss: 1.57; acc: 0.5
Batch: 440; loss: 1.51; acc: 0.56
Batch: 460; loss: 1.44; acc: 0.59
Batch: 480; loss: 1.49; acc: 0.59
Batch: 500; loss: 1.4; acc: 0.66
Batch: 520; loss: 1.36; acc: 0.62
Batch: 540; loss: 1.27; acc: 0.62
Batch: 560; loss: 1.25; acc: 0.7
Batch: 580; loss: 1.53; acc: 0.59
Batch: 600; loss: 1.25; acc: 0.67
Batch: 620; loss: 1.34; acc: 0.66
Batch: 640; loss: 1.42; acc: 0.67
Batch: 660; loss: 1.42; acc: 0.56
Batch: 680; loss: 1.35; acc: 0.61
Batch: 700; loss: 1.39; acc: 0.61
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.52; acc: 0.61
Batch: 760; loss: 1.42; acc: 0.64
Batch: 780; loss: 1.34; acc: 0.7
Train Epoch over. train_loss: 1.45; train_accuracy: 0.58 

4.6470198867609724e-05
1.5225872630253434e-05
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 1.15; acc: 0.75
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.27; acc: 0.66
Batch: 100; loss: 1.35; acc: 0.67
Batch: 120; loss: 1.39; acc: 0.62
Batch: 140; loss: 1.27; acc: 0.7
Val Epoch over. val_loss: 1.3857818743225876; val_accuracy: 0.6127587579617835 

The current subspace-distance is: 1.5225872630253434e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.57; acc: 0.41
Batch: 20; loss: 1.42; acc: 0.58
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.4; acc: 0.66
Batch: 100; loss: 1.55; acc: 0.55
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 1.59; acc: 0.52
Batch: 160; loss: 1.4; acc: 0.55
Batch: 180; loss: 1.4; acc: 0.61
Batch: 200; loss: 1.27; acc: 0.64
Batch: 220; loss: 1.32; acc: 0.69
Batch: 240; loss: 1.3; acc: 0.62
Batch: 260; loss: 1.37; acc: 0.58
Batch: 280; loss: 1.37; acc: 0.62
Batch: 300; loss: 1.51; acc: 0.53
Batch: 320; loss: 1.34; acc: 0.61
Batch: 340; loss: 1.47; acc: 0.58
Batch: 360; loss: 1.53; acc: 0.59
Batch: 380; loss: 1.29; acc: 0.64
Batch: 400; loss: 1.47; acc: 0.59
Batch: 420; loss: 1.35; acc: 0.55
Batch: 440; loss: 1.54; acc: 0.56
Batch: 460; loss: 1.43; acc: 0.59
Batch: 480; loss: 1.47; acc: 0.55
Batch: 500; loss: 1.51; acc: 0.48
Batch: 520; loss: 1.44; acc: 0.5
Batch: 540; loss: 1.36; acc: 0.69
Batch: 560; loss: 1.46; acc: 0.53
Batch: 580; loss: 1.5; acc: 0.58
Batch: 600; loss: 1.57; acc: 0.52
Batch: 620; loss: 1.33; acc: 0.64
Batch: 640; loss: 1.49; acc: 0.52
Batch: 660; loss: 1.51; acc: 0.5
Batch: 680; loss: 1.37; acc: 0.64
Batch: 700; loss: 1.45; acc: 0.52
Batch: 720; loss: 1.52; acc: 0.55
Batch: 740; loss: 1.41; acc: 0.61
Batch: 760; loss: 1.42; acc: 0.56
Batch: 780; loss: 1.45; acc: 0.56
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.575865750666708e-05
1.184640950668836e-05
Batch: 0; loss: 1.6; acc: 0.5
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 1.16; acc: 0.73
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.28; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.67
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.25; acc: 0.72
Val Epoch over. val_loss: 1.3895860925601546; val_accuracy: 0.6092754777070064 

The current subspace-distance is: 1.184640950668836e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.43; acc: 0.53
Batch: 20; loss: 1.49; acc: 0.56
Batch: 40; loss: 1.43; acc: 0.56
Batch: 60; loss: 1.27; acc: 0.66
Batch: 80; loss: 1.38; acc: 0.66
Batch: 100; loss: 1.48; acc: 0.53
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.54; acc: 0.56
Batch: 160; loss: 1.5; acc: 0.62
Batch: 180; loss: 1.39; acc: 0.61
Batch: 200; loss: 1.42; acc: 0.59
Batch: 220; loss: 1.69; acc: 0.42
Batch: 240; loss: 1.47; acc: 0.55
Batch: 260; loss: 1.51; acc: 0.59
Batch: 280; loss: 1.36; acc: 0.66
Batch: 300; loss: 1.4; acc: 0.58
Batch: 320; loss: 1.53; acc: 0.55
Batch: 340; loss: 1.48; acc: 0.64
Batch: 360; loss: 1.37; acc: 0.67
Batch: 380; loss: 1.44; acc: 0.56
Batch: 400; loss: 1.45; acc: 0.64
Batch: 420; loss: 1.38; acc: 0.61
Batch: 440; loss: 1.37; acc: 0.61
Batch: 460; loss: 1.44; acc: 0.58
Batch: 480; loss: 1.45; acc: 0.52
Batch: 500; loss: 1.57; acc: 0.45
Batch: 520; loss: 1.4; acc: 0.64
Batch: 540; loss: 1.33; acc: 0.69
Batch: 560; loss: 1.35; acc: 0.62
Batch: 580; loss: 1.37; acc: 0.62
Batch: 600; loss: 1.5; acc: 0.59
Batch: 620; loss: 1.36; acc: 0.66
Batch: 640; loss: 1.5; acc: 0.55
Batch: 660; loss: 1.49; acc: 0.52
Batch: 680; loss: 1.48; acc: 0.56
Batch: 700; loss: 1.39; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.58
Batch: 740; loss: 1.41; acc: 0.66
Batch: 760; loss: 1.48; acc: 0.56
Batch: 780; loss: 1.37; acc: 0.62
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.6701326937181875e-05
1.3622336155094672e-05
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.32; acc: 0.62
Batch: 40; loss: 1.15; acc: 0.75
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.35; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.25; acc: 0.72
Val Epoch over. val_loss: 1.3855871668287143; val_accuracy: 0.6072850318471338 

The current subspace-distance is: 1.3622336155094672e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.5
Batch: 20; loss: 1.45; acc: 0.56
Batch: 40; loss: 1.45; acc: 0.58
Batch: 60; loss: 1.38; acc: 0.62
Batch: 80; loss: 1.35; acc: 0.66
Batch: 100; loss: 1.36; acc: 0.66
Batch: 120; loss: 1.35; acc: 0.66
Batch: 140; loss: 1.49; acc: 0.56
Batch: 160; loss: 1.38; acc: 0.56
Batch: 180; loss: 1.28; acc: 0.75
Batch: 200; loss: 1.28; acc: 0.67
Batch: 220; loss: 1.35; acc: 0.58
Batch: 240; loss: 1.42; acc: 0.56
Batch: 260; loss: 1.57; acc: 0.47
Batch: 280; loss: 1.47; acc: 0.55
Batch: 300; loss: 1.37; acc: 0.64
Batch: 320; loss: 1.49; acc: 0.55
Batch: 340; loss: 1.44; acc: 0.56
Batch: 360; loss: 1.62; acc: 0.53
Batch: 380; loss: 1.41; acc: 0.59
Batch: 400; loss: 1.36; acc: 0.61
Batch: 420; loss: 1.38; acc: 0.64
Batch: 440; loss: 1.39; acc: 0.59
Batch: 460; loss: 1.48; acc: 0.61
Batch: 480; loss: 1.44; acc: 0.53
Batch: 500; loss: 1.35; acc: 0.64
Batch: 520; loss: 1.5; acc: 0.55
Batch: 540; loss: 1.45; acc: 0.59
Batch: 560; loss: 1.45; acc: 0.61
Batch: 580; loss: 1.37; acc: 0.69
Batch: 600; loss: 1.48; acc: 0.52
Batch: 620; loss: 1.51; acc: 0.5
Batch: 640; loss: 1.39; acc: 0.62
Batch: 660; loss: 1.47; acc: 0.52
Batch: 680; loss: 1.53; acc: 0.5
Batch: 700; loss: 1.5; acc: 0.55
Batch: 720; loss: 1.46; acc: 0.58
Batch: 740; loss: 1.58; acc: 0.58
Batch: 760; loss: 1.41; acc: 0.62
Batch: 780; loss: 1.37; acc: 0.7
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.7136203647824004e-05
1.5538700608885847e-05
Batch: 0; loss: 1.6; acc: 0.48
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.17; acc: 0.73
Batch: 60; loss: 1.46; acc: 0.58
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.37; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.69
Val Epoch over. val_loss: 1.3962427286585426; val_accuracy: 0.6032046178343949 

The current subspace-distance is: 1.5538700608885847e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.34; acc: 0.64
Batch: 20; loss: 1.47; acc: 0.55
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 1.5; acc: 0.55
Batch: 80; loss: 1.41; acc: 0.66
Batch: 100; loss: 1.42; acc: 0.56
Batch: 120; loss: 1.52; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.53
Batch: 160; loss: 1.32; acc: 0.66
Batch: 180; loss: 1.43; acc: 0.62
Batch: 200; loss: 1.37; acc: 0.64
Batch: 220; loss: 1.38; acc: 0.55
Batch: 240; loss: 1.48; acc: 0.56
Batch: 260; loss: 1.4; acc: 0.61
Batch: 280; loss: 1.65; acc: 0.48
Batch: 300; loss: 1.41; acc: 0.56
Batch: 320; loss: 1.45; acc: 0.59
Batch: 340; loss: 1.54; acc: 0.58
Batch: 360; loss: 1.53; acc: 0.47
Batch: 380; loss: 1.45; acc: 0.56
Batch: 400; loss: 1.56; acc: 0.5
Batch: 420; loss: 1.48; acc: 0.61
Batch: 440; loss: 1.42; acc: 0.67
Batch: 460; loss: 1.34; acc: 0.59
Batch: 480; loss: 1.34; acc: 0.59
Batch: 500; loss: 1.33; acc: 0.67
Batch: 520; loss: 1.49; acc: 0.64
Batch: 540; loss: 1.37; acc: 0.62
Batch: 560; loss: 1.4; acc: 0.62
Batch: 580; loss: 1.39; acc: 0.59
Batch: 600; loss: 1.45; acc: 0.53
Batch: 620; loss: 1.38; acc: 0.66
Batch: 640; loss: 1.42; acc: 0.58
Batch: 660; loss: 1.43; acc: 0.59
Batch: 680; loss: 1.5; acc: 0.61
Batch: 700; loss: 1.54; acc: 0.55
Batch: 720; loss: 1.38; acc: 0.61
Batch: 740; loss: 1.52; acc: 0.47
Batch: 760; loss: 1.4; acc: 0.58
Batch: 780; loss: 1.44; acc: 0.66
Train Epoch over. train_loss: 1.45; train_accuracy: 0.58 

4.789965532836504e-05
1.6775677067926154e-05
Batch: 0; loss: 1.6; acc: 0.45
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.28; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.25; acc: 0.72
Val Epoch over. val_loss: 1.3917797386266624; val_accuracy: 0.6057921974522293 

The current subspace-distance is: 1.6775677067926154e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.41; acc: 0.61
Batch: 20; loss: 1.46; acc: 0.55
Batch: 40; loss: 1.42; acc: 0.58
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.48; acc: 0.55
Batch: 120; loss: 1.25; acc: 0.7
Batch: 140; loss: 1.42; acc: 0.59
Batch: 160; loss: 1.38; acc: 0.59
Batch: 180; loss: 1.47; acc: 0.56
Batch: 200; loss: 1.33; acc: 0.66
Batch: 220; loss: 1.53; acc: 0.59
Batch: 240; loss: 1.46; acc: 0.52
Batch: 260; loss: 1.48; acc: 0.59
Batch: 280; loss: 1.56; acc: 0.5
Batch: 300; loss: 1.39; acc: 0.56
Batch: 320; loss: 1.48; acc: 0.55
Batch: 340; loss: 1.36; acc: 0.59
Batch: 360; loss: 1.52; acc: 0.59
Batch: 380; loss: 1.44; acc: 0.53
Batch: 400; loss: 1.41; acc: 0.56
Batch: 420; loss: 1.37; acc: 0.62
Batch: 440; loss: 1.4; acc: 0.59
Batch: 460; loss: 1.42; acc: 0.64
Batch: 480; loss: 1.4; acc: 0.62
Batch: 500; loss: 1.44; acc: 0.56
Batch: 520; loss: 1.42; acc: 0.59
Batch: 540; loss: 1.52; acc: 0.61
Batch: 560; loss: 1.4; acc: 0.61
Batch: 580; loss: 1.35; acc: 0.61
Batch: 600; loss: 1.55; acc: 0.52
Batch: 620; loss: 1.47; acc: 0.55
Batch: 640; loss: 1.63; acc: 0.47
Batch: 660; loss: 1.38; acc: 0.64
Batch: 680; loss: 1.3; acc: 0.69
Batch: 700; loss: 1.41; acc: 0.56
Batch: 720; loss: 1.47; acc: 0.58
Batch: 740; loss: 1.42; acc: 0.61
Batch: 760; loss: 1.46; acc: 0.55
Batch: 780; loss: 1.39; acc: 0.64
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.742252713185735e-05
1.748763315845281e-05
Batch: 0; loss: 1.6; acc: 0.45
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 1.16; acc: 0.7
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.37; acc: 0.67
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.26; acc: 0.69
Val Epoch over. val_loss: 1.3946668031109366; val_accuracy: 0.6023089171974523 

The current subspace-distance is: 1.748763315845281e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.39; acc: 0.58
Batch: 20; loss: 1.52; acc: 0.53
Batch: 40; loss: 1.48; acc: 0.48
Batch: 60; loss: 1.42; acc: 0.66
Batch: 80; loss: 1.36; acc: 0.55
Batch: 100; loss: 1.48; acc: 0.52
Batch: 120; loss: 1.42; acc: 0.61
Batch: 140; loss: 1.37; acc: 0.64
Batch: 160; loss: 1.41; acc: 0.58
Batch: 180; loss: 1.37; acc: 0.61
Batch: 200; loss: 1.43; acc: 0.56
Batch: 220; loss: 1.51; acc: 0.56
Batch: 240; loss: 1.45; acc: 0.56
Batch: 260; loss: 1.51; acc: 0.52
Batch: 280; loss: 1.67; acc: 0.41
Batch: 300; loss: 1.45; acc: 0.61
Batch: 320; loss: 1.5; acc: 0.53
Batch: 340; loss: 1.47; acc: 0.56
Batch: 360; loss: 1.56; acc: 0.5
Batch: 380; loss: 1.55; acc: 0.53
Batch: 400; loss: 1.39; acc: 0.59
Batch: 420; loss: 1.44; acc: 0.61
Batch: 440; loss: 1.39; acc: 0.67
Batch: 460; loss: 1.55; acc: 0.47
Batch: 480; loss: 1.51; acc: 0.48
Batch: 500; loss: 1.56; acc: 0.52
Batch: 520; loss: 1.61; acc: 0.47
Batch: 540; loss: 1.44; acc: 0.61
Batch: 560; loss: 1.47; acc: 0.55
Batch: 580; loss: 1.43; acc: 0.52
Batch: 600; loss: 1.36; acc: 0.61
Batch: 620; loss: 1.34; acc: 0.56
Batch: 640; loss: 1.38; acc: 0.64
Batch: 660; loss: 1.61; acc: 0.47
Batch: 680; loss: 1.48; acc: 0.58
Batch: 700; loss: 1.43; acc: 0.56
Batch: 720; loss: 1.48; acc: 0.55
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.34; acc: 0.62
Batch: 780; loss: 1.68; acc: 0.52
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.6627785195596516e-05
1.2362679626676254e-05
Batch: 0; loss: 1.6; acc: 0.47
Batch: 20; loss: 1.34; acc: 0.58
Batch: 40; loss: 1.17; acc: 0.7
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.29; acc: 0.62
Batch: 100; loss: 1.37; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.26; acc: 0.69
Val Epoch over. val_loss: 1.3966417191134897; val_accuracy: 0.6018113057324841 

The current subspace-distance is: 1.2362679626676254e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.44; acc: 0.66
Batch: 20; loss: 1.41; acc: 0.59
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.39; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.58
Batch: 100; loss: 1.55; acc: 0.58
Batch: 120; loss: 1.52; acc: 0.48
Batch: 140; loss: 1.35; acc: 0.62
Batch: 160; loss: 1.54; acc: 0.47
Batch: 180; loss: 1.4; acc: 0.62
Batch: 200; loss: 1.41; acc: 0.56
Batch: 220; loss: 1.53; acc: 0.55
Batch: 240; loss: 1.44; acc: 0.61
Batch: 260; loss: 1.41; acc: 0.61
Batch: 280; loss: 1.42; acc: 0.69
Batch: 300; loss: 1.43; acc: 0.64
Batch: 320; loss: 1.51; acc: 0.53
Batch: 340; loss: 1.39; acc: 0.66
Batch: 360; loss: 1.47; acc: 0.59
Batch: 380; loss: 1.43; acc: 0.59
Batch: 400; loss: 1.41; acc: 0.59
Batch: 420; loss: 1.4; acc: 0.62
Batch: 440; loss: 1.45; acc: 0.61
Batch: 460; loss: 1.53; acc: 0.61
Batch: 480; loss: 1.46; acc: 0.52
Batch: 500; loss: 1.42; acc: 0.56
Batch: 520; loss: 1.41; acc: 0.61
Batch: 540; loss: 1.45; acc: 0.56
Batch: 560; loss: 1.6; acc: 0.47
Batch: 580; loss: 1.32; acc: 0.73
Batch: 600; loss: 1.44; acc: 0.58
Batch: 620; loss: 1.43; acc: 0.67
Batch: 640; loss: 1.4; acc: 0.72
Batch: 660; loss: 1.34; acc: 0.62
Batch: 680; loss: 1.58; acc: 0.5
Batch: 700; loss: 1.55; acc: 0.5
Batch: 720; loss: 1.31; acc: 0.62
Batch: 740; loss: 1.48; acc: 0.58
Batch: 760; loss: 1.46; acc: 0.61
Batch: 780; loss: 1.37; acc: 0.58
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.678476761910133e-05
1.433656598237576e-05
Batch: 0; loss: 1.61; acc: 0.47
Batch: 20; loss: 1.35; acc: 0.64
Batch: 40; loss: 1.18; acc: 0.73
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.3; acc: 0.64
Batch: 100; loss: 1.38; acc: 0.66
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 1.26; acc: 0.7
Val Epoch over. val_loss: 1.3992035366167688; val_accuracy: 0.5996218152866242 

The current subspace-distance is: 1.433656598237576e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.44; acc: 0.55
Batch: 20; loss: 1.44; acc: 0.61
Batch: 40; loss: 1.29; acc: 0.59
Batch: 60; loss: 1.39; acc: 0.62
Batch: 80; loss: 1.39; acc: 0.64
Batch: 100; loss: 1.5; acc: 0.59
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 1.47; acc: 0.56
Batch: 160; loss: 1.51; acc: 0.5
Batch: 180; loss: 1.53; acc: 0.58
Batch: 200; loss: 1.34; acc: 0.64
Batch: 220; loss: 1.65; acc: 0.44
Batch: 240; loss: 1.39; acc: 0.58
Batch: 260; loss: 1.35; acc: 0.69
Batch: 280; loss: 1.43; acc: 0.59
Batch: 300; loss: 1.59; acc: 0.53
Batch: 320; loss: 1.47; acc: 0.55
Batch: 340; loss: 1.32; acc: 0.67
Batch: 360; loss: 1.4; acc: 0.58
Batch: 380; loss: 1.38; acc: 0.55
Batch: 400; loss: 1.41; acc: 0.56
Batch: 420; loss: 1.43; acc: 0.62
Batch: 440; loss: 1.42; acc: 0.53
Batch: 460; loss: 1.59; acc: 0.55
Batch: 480; loss: 1.37; acc: 0.64
Batch: 500; loss: 1.55; acc: 0.48
Batch: 520; loss: 1.6; acc: 0.47
Batch: 540; loss: 1.52; acc: 0.55
Batch: 560; loss: 1.51; acc: 0.58
Batch: 580; loss: 1.51; acc: 0.53
Batch: 600; loss: 1.42; acc: 0.66
Batch: 620; loss: 1.37; acc: 0.58
Batch: 640; loss: 1.37; acc: 0.64
Batch: 660; loss: 1.49; acc: 0.53
Batch: 680; loss: 1.34; acc: 0.59
Batch: 700; loss: 1.38; acc: 0.62
Batch: 720; loss: 1.46; acc: 0.56
Batch: 740; loss: 1.46; acc: 0.48
Batch: 760; loss: 1.36; acc: 0.58
Batch: 780; loss: 1.37; acc: 0.64
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.711752262664959e-05
1.5387871826533228e-05
Batch: 0; loss: 1.59; acc: 0.47
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.17; acc: 0.69
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.28; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.24; acc: 0.7
Val Epoch over. val_loss: 1.3898040397911315; val_accuracy: 0.5999203821656051 

The current subspace-distance is: 1.5387871826533228e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.59
Batch: 20; loss: 1.32; acc: 0.66
Batch: 40; loss: 1.41; acc: 0.55
Batch: 60; loss: 1.54; acc: 0.48
Batch: 80; loss: 1.38; acc: 0.59
Batch: 100; loss: 1.3; acc: 0.64
Batch: 120; loss: 1.64; acc: 0.41
Batch: 140; loss: 1.42; acc: 0.66
Batch: 160; loss: 1.61; acc: 0.42
Batch: 180; loss: 1.21; acc: 0.64
Batch: 200; loss: 1.45; acc: 0.56
Batch: 220; loss: 1.31; acc: 0.62
Batch: 240; loss: 1.42; acc: 0.62
Batch: 260; loss: 1.58; acc: 0.56
Batch: 280; loss: 1.36; acc: 0.61
Batch: 300; loss: 1.51; acc: 0.55
Batch: 320; loss: 1.4; acc: 0.7
Batch: 340; loss: 1.46; acc: 0.53
Batch: 360; loss: 1.5; acc: 0.64
Batch: 380; loss: 1.53; acc: 0.53
Batch: 400; loss: 1.62; acc: 0.61
Batch: 420; loss: 1.4; acc: 0.59
Batch: 440; loss: 1.41; acc: 0.55
Batch: 460; loss: 1.51; acc: 0.53
Batch: 480; loss: 1.5; acc: 0.52
Batch: 500; loss: 1.47; acc: 0.59
Batch: 520; loss: 1.29; acc: 0.64
Batch: 540; loss: 1.7; acc: 0.36
Batch: 560; loss: 1.38; acc: 0.62
Batch: 580; loss: 1.42; acc: 0.64
Batch: 600; loss: 1.42; acc: 0.59
Batch: 620; loss: 1.41; acc: 0.59
Batch: 640; loss: 1.36; acc: 0.61
Batch: 660; loss: 1.33; acc: 0.69
Batch: 680; loss: 1.39; acc: 0.61
Batch: 700; loss: 1.35; acc: 0.62
Batch: 720; loss: 1.47; acc: 0.59
Batch: 740; loss: 1.51; acc: 0.61
Batch: 760; loss: 1.54; acc: 0.56
Batch: 780; loss: 1.43; acc: 0.61
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.752014501718804e-05
1.720925502013415e-05
Batch: 0; loss: 1.6; acc: 0.52
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.18; acc: 0.73
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.29; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.64
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 1.25; acc: 0.72
Val Epoch over. val_loss: 1.39884671132276; val_accuracy: 0.6001194267515924 

The current subspace-distance is: 1.720925502013415e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.53
Batch: 20; loss: 1.41; acc: 0.62
Batch: 40; loss: 1.5; acc: 0.56
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.48; acc: 0.55
Batch: 100; loss: 1.55; acc: 0.53
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.56; acc: 0.53
Batch: 160; loss: 1.34; acc: 0.66
Batch: 180; loss: 1.69; acc: 0.5
Batch: 200; loss: 1.46; acc: 0.58
Batch: 220; loss: 1.56; acc: 0.56
Batch: 240; loss: 1.33; acc: 0.61
Batch: 260; loss: 1.39; acc: 0.62
Batch: 280; loss: 1.36; acc: 0.72
Batch: 300; loss: 1.51; acc: 0.48
Batch: 320; loss: 1.4; acc: 0.62
Batch: 340; loss: 1.63; acc: 0.44
Batch: 360; loss: 1.41; acc: 0.53
Batch: 380; loss: 1.5; acc: 0.53
Batch: 400; loss: 1.48; acc: 0.53
Batch: 420; loss: 1.53; acc: 0.48
Batch: 440; loss: 1.48; acc: 0.59
Batch: 460; loss: 1.39; acc: 0.53
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.31; acc: 0.73
Batch: 520; loss: 1.6; acc: 0.47
Batch: 540; loss: 1.37; acc: 0.62
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.37; acc: 0.58
Batch: 600; loss: 1.72; acc: 0.41
Batch: 620; loss: 1.35; acc: 0.59
Batch: 640; loss: 1.52; acc: 0.55
Batch: 660; loss: 1.44; acc: 0.53
Batch: 680; loss: 1.42; acc: 0.66
Batch: 700; loss: 1.38; acc: 0.55
Batch: 720; loss: 1.58; acc: 0.5
Batch: 740; loss: 1.35; acc: 0.69
Batch: 760; loss: 1.34; acc: 0.67
Batch: 780; loss: 1.3; acc: 0.62
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.7531098971376196e-05
1.3050711459072772e-05
Batch: 0; loss: 1.59; acc: 0.5
Batch: 20; loss: 1.32; acc: 0.62
Batch: 40; loss: 1.16; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.28; acc: 0.66
Batch: 100; loss: 1.36; acc: 0.67
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.25; acc: 0.69
Val Epoch over. val_loss: 1.387936497190196; val_accuracy: 0.6044984076433121 

The current subspace-distance is: 1.3050711459072772e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.46; acc: 0.58
Batch: 40; loss: 1.62; acc: 0.41
Batch: 60; loss: 1.15; acc: 0.77
Batch: 80; loss: 1.42; acc: 0.56
Batch: 100; loss: 1.54; acc: 0.52
Batch: 120; loss: 1.32; acc: 0.73
Batch: 140; loss: 1.38; acc: 0.56
Batch: 160; loss: 1.45; acc: 0.56
Batch: 180; loss: 1.47; acc: 0.55
Batch: 200; loss: 1.36; acc: 0.62
Batch: 220; loss: 1.6; acc: 0.45
Batch: 240; loss: 1.24; acc: 0.69
Batch: 260; loss: 1.14; acc: 0.8
Batch: 280; loss: 1.47; acc: 0.53
Batch: 300; loss: 1.5; acc: 0.53
Batch: 320; loss: 1.38; acc: 0.67
Batch: 340; loss: 1.38; acc: 0.58
Batch: 360; loss: 1.4; acc: 0.66
Batch: 380; loss: 1.47; acc: 0.56
Batch: 400; loss: 1.47; acc: 0.58
Batch: 420; loss: 1.52; acc: 0.58
Batch: 440; loss: 1.46; acc: 0.62
Batch: 460; loss: 1.4; acc: 0.64
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.32; acc: 0.59
Batch: 520; loss: 1.52; acc: 0.48
Batch: 540; loss: 1.52; acc: 0.44
Batch: 560; loss: 1.51; acc: 0.62
Batch: 580; loss: 1.53; acc: 0.52
Batch: 600; loss: 1.48; acc: 0.66
Batch: 620; loss: 1.52; acc: 0.48
Batch: 640; loss: 1.38; acc: 0.62
Batch: 660; loss: 1.4; acc: 0.53
Batch: 680; loss: 1.4; acc: 0.62
Batch: 700; loss: 1.39; acc: 0.69
Batch: 720; loss: 1.41; acc: 0.59
Batch: 740; loss: 1.41; acc: 0.58
Batch: 760; loss: 1.41; acc: 0.58
Batch: 780; loss: 1.33; acc: 0.66
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.753201938001439e-05
1.51291560541722e-05
Batch: 0; loss: 1.6; acc: 0.48
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 1.17; acc: 0.69
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.28; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 1.27; acc: 0.7
Val Epoch over. val_loss: 1.4018167014334613; val_accuracy: 0.5982285031847133 

The current subspace-distance is: 1.51291560541722e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.56; acc: 0.52
Batch: 40; loss: 1.51; acc: 0.55
Batch: 60; loss: 1.41; acc: 0.61
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.41; acc: 0.53
Batch: 120; loss: 1.42; acc: 0.61
Batch: 140; loss: 1.37; acc: 0.67
Batch: 160; loss: 1.5; acc: 0.53
Batch: 180; loss: 1.37; acc: 0.64
Batch: 200; loss: 1.53; acc: 0.53
Batch: 220; loss: 1.52; acc: 0.55
Batch: 240; loss: 1.52; acc: 0.55
Batch: 260; loss: 1.42; acc: 0.64
Batch: 280; loss: 1.53; acc: 0.53
Batch: 300; loss: 1.45; acc: 0.58
Batch: 320; loss: 1.53; acc: 0.55
Batch: 340; loss: 1.37; acc: 0.64
Batch: 360; loss: 1.49; acc: 0.59
Batch: 380; loss: 1.54; acc: 0.52
Batch: 400; loss: 1.34; acc: 0.62
Batch: 420; loss: 1.55; acc: 0.53
Batch: 440; loss: 1.36; acc: 0.64
Batch: 460; loss: 1.6; acc: 0.58
Batch: 480; loss: 1.48; acc: 0.59
Batch: 500; loss: 1.48; acc: 0.56
Batch: 520; loss: 1.44; acc: 0.61
Batch: 540; loss: 1.39; acc: 0.64
Batch: 560; loss: 1.39; acc: 0.59
Batch: 580; loss: 1.61; acc: 0.48
Batch: 600; loss: 1.52; acc: 0.52
Batch: 620; loss: 1.47; acc: 0.61
Batch: 640; loss: 1.38; acc: 0.59
Batch: 660; loss: 1.38; acc: 0.56
Batch: 680; loss: 1.31; acc: 0.67
Batch: 700; loss: 1.48; acc: 0.56
Batch: 720; loss: 1.36; acc: 0.66
Batch: 740; loss: 1.48; acc: 0.56
Batch: 760; loss: 1.36; acc: 0.56
Batch: 780; loss: 1.39; acc: 0.58
Train Epoch over. train_loss: 1.44; train_accuracy: 0.58 

4.724477184936404e-05
1.4697633559990209e-05
Batch: 0; loss: 1.61; acc: 0.47
Batch: 20; loss: 1.35; acc: 0.62
Batch: 40; loss: 1.18; acc: 0.7
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.39; acc: 0.66
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 1.26; acc: 0.69
Val Epoch over. val_loss: 1.4036208581013285; val_accuracy: 0.5978304140127388 

The current subspace-distance is: 1.4697633559990209e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_6_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.1089789320611354

The number of parameters is: 267194

The number of individual parameters is:

25
450
25
25
38
43700
38
38
75
131100
75
75
64
86400
64
64
4096
64
640
10
64
64

nonzero elements in E: 26719397
elements in E: 26719400
fraction nonzero: 0.9999998877220296
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.03
Batch: 20; loss: 2.32; acc: 0.14
Batch: 40; loss: 2.23; acc: 0.14
Batch: 60; loss: 2.07; acc: 0.25
Batch: 80; loss: 1.98; acc: 0.38
Batch: 100; loss: 1.99; acc: 0.36
Batch: 120; loss: 2.0; acc: 0.36
Batch: 140; loss: 2.02; acc: 0.3
Batch: 160; loss: 1.95; acc: 0.33
Batch: 180; loss: 1.9; acc: 0.38
Batch: 200; loss: 1.89; acc: 0.41
Batch: 220; loss: 1.9; acc: 0.45
Batch: 240; loss: 1.86; acc: 0.42
Batch: 260; loss: 1.84; acc: 0.53
Batch: 280; loss: 1.9; acc: 0.39
Batch: 300; loss: 1.85; acc: 0.39
Batch: 320; loss: 1.73; acc: 0.59
Batch: 340; loss: 1.82; acc: 0.5
Batch: 360; loss: 1.82; acc: 0.52
Batch: 380; loss: 1.76; acc: 0.48
Batch: 400; loss: 1.82; acc: 0.45
Batch: 420; loss: 1.66; acc: 0.62
Batch: 440; loss: 1.75; acc: 0.55
Batch: 460; loss: 1.7; acc: 0.61
Batch: 480; loss: 1.76; acc: 0.47
Batch: 500; loss: 1.8; acc: 0.45
Batch: 520; loss: 1.7; acc: 0.56
Batch: 540; loss: 1.65; acc: 0.61
Batch: 560; loss: 1.76; acc: 0.56
Batch: 580; loss: 1.65; acc: 0.67
Batch: 600; loss: 1.83; acc: 0.42
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.65; acc: 0.56
Batch: 660; loss: 1.71; acc: 0.59
Batch: 680; loss: 1.75; acc: 0.52
Batch: 700; loss: 1.65; acc: 0.56
Batch: 720; loss: 1.68; acc: 0.53
Batch: 740; loss: 1.7; acc: 0.53
Batch: 760; loss: 1.62; acc: 0.64
Batch: 780; loss: 1.65; acc: 0.66
Train Epoch over. train_loss: 1.84; train_accuracy: 0.45 

5.4462179832626134e-05
4.84275005874224e-05
Batch: 0; loss: 1.68; acc: 0.42
Batch: 20; loss: 1.8; acc: 0.48
Batch: 40; loss: 1.47; acc: 0.73
Batch: 60; loss: 1.68; acc: 0.52
Batch: 80; loss: 1.58; acc: 0.64
Batch: 100; loss: 1.64; acc: 0.58
Batch: 120; loss: 1.79; acc: 0.45
Batch: 140; loss: 1.5; acc: 0.77
Val Epoch over. val_loss: 1.633218976342754; val_accuracy: 0.5876791401273885 

The current subspace-distance is: 4.84275005874224e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.58
Batch: 20; loss: 1.73; acc: 0.5
Batch: 40; loss: 1.71; acc: 0.5
Batch: 60; loss: 1.59; acc: 0.59
Batch: 80; loss: 1.59; acc: 0.66
Batch: 100; loss: 1.63; acc: 0.61
Batch: 120; loss: 1.68; acc: 0.53
Batch: 140; loss: 1.66; acc: 0.56
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.64; acc: 0.58
Batch: 200; loss: 1.48; acc: 0.69
Batch: 220; loss: 1.66; acc: 0.56
Batch: 240; loss: 1.63; acc: 0.59
Batch: 260; loss: 1.62; acc: 0.62
Batch: 280; loss: 1.69; acc: 0.55
Batch: 300; loss: 1.52; acc: 0.66
Batch: 320; loss: 1.5; acc: 0.69
Batch: 340; loss: 1.5; acc: 0.7
Batch: 360; loss: 1.75; acc: 0.45
Batch: 380; loss: 1.55; acc: 0.61
Batch: 400; loss: 1.66; acc: 0.53
Batch: 420; loss: 1.63; acc: 0.62
Batch: 440; loss: 1.75; acc: 0.53
Batch: 460; loss: 1.57; acc: 0.59
Batch: 480; loss: 1.64; acc: 0.62
Batch: 500; loss: 1.58; acc: 0.59
Batch: 520; loss: 1.58; acc: 0.62
Batch: 540; loss: 1.6; acc: 0.59
Batch: 560; loss: 1.59; acc: 0.58
Batch: 580; loss: 1.46; acc: 0.7
Batch: 600; loss: 1.6; acc: 0.5
Batch: 620; loss: 1.73; acc: 0.48
Batch: 640; loss: 1.58; acc: 0.52
Batch: 660; loss: 1.63; acc: 0.58
Batch: 680; loss: 1.56; acc: 0.58
Batch: 700; loss: 1.58; acc: 0.59
Batch: 720; loss: 1.5; acc: 0.7
Batch: 740; loss: 1.53; acc: 0.64
Batch: 760; loss: 1.65; acc: 0.5
Batch: 780; loss: 1.55; acc: 0.64
Train Epoch over. train_loss: 1.61; train_accuracy: 0.58 

6.675771874142811e-05
6.184310041135177e-05
Batch: 0; loss: 1.58; acc: 0.61
Batch: 20; loss: 1.72; acc: 0.55
Batch: 40; loss: 1.36; acc: 0.8
Batch: 60; loss: 1.58; acc: 0.66
Batch: 80; loss: 1.47; acc: 0.67
Batch: 100; loss: 1.58; acc: 0.61
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.35; acc: 0.75
Val Epoch over. val_loss: 1.5355269924090926; val_accuracy: 0.6255971337579618 

The current subspace-distance is: 6.184310041135177e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.67
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.57; acc: 0.61
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.63; acc: 0.53
Batch: 100; loss: 1.69; acc: 0.47
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 1.57; acc: 0.56
Batch: 160; loss: 1.58; acc: 0.55
Batch: 180; loss: 1.61; acc: 0.61
Batch: 200; loss: 1.56; acc: 0.61
Batch: 220; loss: 1.58; acc: 0.55
Batch: 240; loss: 1.44; acc: 0.69
Batch: 260; loss: 1.55; acc: 0.7
Batch: 280; loss: 1.58; acc: 0.58
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.57; acc: 0.52
Batch: 340; loss: 1.48; acc: 0.7
Batch: 360; loss: 1.64; acc: 0.58
Batch: 380; loss: 1.62; acc: 0.56
Batch: 400; loss: 1.53; acc: 0.56
Batch: 420; loss: 1.43; acc: 0.67
Batch: 440; loss: 1.46; acc: 0.67
Batch: 460; loss: 1.53; acc: 0.61
Batch: 480; loss: 1.57; acc: 0.53
Batch: 500; loss: 1.49; acc: 0.66
Batch: 520; loss: 1.53; acc: 0.61
Batch: 540; loss: 1.53; acc: 0.62
Batch: 560; loss: 1.56; acc: 0.56
Batch: 580; loss: 1.54; acc: 0.59
Batch: 600; loss: 1.57; acc: 0.67
Batch: 620; loss: 1.39; acc: 0.72
Batch: 640; loss: 1.62; acc: 0.58
Batch: 660; loss: 1.5; acc: 0.69
Batch: 680; loss: 1.54; acc: 0.61
Batch: 700; loss: 1.66; acc: 0.52
Batch: 720; loss: 1.52; acc: 0.64
Batch: 740; loss: 1.52; acc: 0.48
Batch: 760; loss: 1.41; acc: 0.73
Batch: 780; loss: 1.54; acc: 0.59
Train Epoch over. train_loss: 1.55; train_accuracy: 0.61 

7.579098019050434e-05
7.0696092734579e-05
Batch: 0; loss: 1.49; acc: 0.62
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.29; acc: 0.83
Batch: 60; loss: 1.46; acc: 0.67
Batch: 80; loss: 1.38; acc: 0.72
Batch: 100; loss: 1.48; acc: 0.64
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.23; acc: 0.77
Val Epoch over. val_loss: 1.4693158044936552; val_accuracy: 0.6542595541401274 

The current subspace-distance is: 7.0696092734579e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.51; acc: 0.56
Batch: 20; loss: 1.53; acc: 0.64
Batch: 40; loss: 1.53; acc: 0.55
Batch: 60; loss: 1.52; acc: 0.67
Batch: 80; loss: 1.57; acc: 0.61
Batch: 100; loss: 1.54; acc: 0.56
Batch: 120; loss: 1.5; acc: 0.62
Batch: 140; loss: 1.7; acc: 0.45
Batch: 160; loss: 1.52; acc: 0.59
Batch: 180; loss: 1.46; acc: 0.67
Batch: 200; loss: 1.59; acc: 0.55
Batch: 220; loss: 1.66; acc: 0.47
Batch: 240; loss: 1.41; acc: 0.73
Batch: 260; loss: 1.45; acc: 0.66
Batch: 280; loss: 1.52; acc: 0.69
Batch: 300; loss: 1.5; acc: 0.66
Batch: 320; loss: 1.46; acc: 0.64
Batch: 340; loss: 1.54; acc: 0.62
Batch: 360; loss: 1.48; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.69
Batch: 400; loss: 1.57; acc: 0.64
Batch: 420; loss: 1.43; acc: 0.67
Batch: 440; loss: 1.49; acc: 0.67
Batch: 460; loss: 1.43; acc: 0.64
Batch: 480; loss: 1.38; acc: 0.77
Batch: 500; loss: 1.39; acc: 0.73
Batch: 520; loss: 1.41; acc: 0.73
Batch: 540; loss: 1.56; acc: 0.59
Batch: 560; loss: 1.58; acc: 0.62
Batch: 580; loss: 1.5; acc: 0.64
Batch: 600; loss: 1.49; acc: 0.56
Batch: 620; loss: 1.41; acc: 0.69
Batch: 640; loss: 1.51; acc: 0.59
Batch: 660; loss: 1.61; acc: 0.56
Batch: 680; loss: 1.39; acc: 0.72
Batch: 700; loss: 1.44; acc: 0.62
Batch: 720; loss: 1.51; acc: 0.61
Batch: 740; loss: 1.41; acc: 0.7
Batch: 760; loss: 1.47; acc: 0.61
Batch: 780; loss: 1.46; acc: 0.64
Train Epoch over. train_loss: 1.48; train_accuracy: 0.65 

8.733991126064211e-05
8.138073462760076e-05
Batch: 0; loss: 1.41; acc: 0.72
Batch: 20; loss: 1.66; acc: 0.55
Batch: 40; loss: 1.21; acc: 0.88
Batch: 60; loss: 1.37; acc: 0.7
Batch: 80; loss: 1.33; acc: 0.73
Batch: 100; loss: 1.41; acc: 0.69
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.17; acc: 0.89
Val Epoch over. val_loss: 1.4158394868206825; val_accuracy: 0.6926751592356688 

The current subspace-distance is: 8.138073462760076e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.64
Batch: 20; loss: 1.51; acc: 0.55
Batch: 40; loss: 1.38; acc: 0.73
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.38; acc: 0.72
Batch: 100; loss: 1.34; acc: 0.66
Batch: 120; loss: 1.43; acc: 0.64
Batch: 140; loss: 1.37; acc: 0.72
Batch: 160; loss: 1.48; acc: 0.72
Batch: 180; loss: 1.44; acc: 0.62
Batch: 200; loss: 1.29; acc: 0.78
Batch: 220; loss: 1.48; acc: 0.61
Batch: 240; loss: 1.53; acc: 0.66
Batch: 260; loss: 1.44; acc: 0.62
Batch: 280; loss: 1.39; acc: 0.69
Batch: 300; loss: 1.4; acc: 0.72
Batch: 320; loss: 1.48; acc: 0.61
Batch: 340; loss: 1.49; acc: 0.64
Batch: 360; loss: 1.39; acc: 0.73
Batch: 380; loss: 1.41; acc: 0.77
Batch: 400; loss: 1.44; acc: 0.64
Batch: 420; loss: 1.51; acc: 0.56
Batch: 440; loss: 1.39; acc: 0.66
Batch: 460; loss: 1.39; acc: 0.7
Batch: 480; loss: 1.57; acc: 0.61
Batch: 500; loss: 1.41; acc: 0.72
Batch: 520; loss: 1.4; acc: 0.72
Batch: 540; loss: 1.39; acc: 0.67
Batch: 560; loss: 1.44; acc: 0.66
Batch: 580; loss: 1.37; acc: 0.73
Batch: 600; loss: 1.45; acc: 0.64
Batch: 620; loss: 1.35; acc: 0.69
Batch: 640; loss: 1.37; acc: 0.7
Batch: 660; loss: 1.55; acc: 0.59
Batch: 680; loss: 1.42; acc: 0.66
Batch: 700; loss: 1.35; acc: 0.69
Batch: 720; loss: 1.48; acc: 0.61
Batch: 740; loss: 1.41; acc: 0.66
Batch: 760; loss: 1.33; acc: 0.75
Batch: 780; loss: 1.37; acc: 0.7
Train Epoch over. train_loss: 1.43; train_accuracy: 0.67 

9.477548883296549e-05
8.932001946959645e-05
Batch: 0; loss: 1.36; acc: 0.81
Batch: 20; loss: 1.62; acc: 0.56
Batch: 40; loss: 1.16; acc: 0.88
Batch: 60; loss: 1.3; acc: 0.77
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.4; acc: 0.7
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.89
Val Epoch over. val_loss: 1.37275598611042; val_accuracy: 0.706906847133758 

The current subspace-distance is: 8.932001946959645e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.75
Batch: 40; loss: 1.38; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.62
Batch: 80; loss: 1.29; acc: 0.8
Batch: 100; loss: 1.5; acc: 0.58
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.37; acc: 0.69
Batch: 160; loss: 1.43; acc: 0.61
Batch: 180; loss: 1.3; acc: 0.73
Batch: 200; loss: 1.36; acc: 0.67
Batch: 220; loss: 1.45; acc: 0.59
Batch: 240; loss: 1.4; acc: 0.62
Batch: 260; loss: 1.31; acc: 0.69
Batch: 280; loss: 1.45; acc: 0.62
Batch: 300; loss: 1.32; acc: 0.72
Batch: 320; loss: 1.36; acc: 0.77
Batch: 340; loss: 1.61; acc: 0.53
Batch: 360; loss: 1.4; acc: 0.67
Batch: 380; loss: 1.31; acc: 0.72
Batch: 400; loss: 1.51; acc: 0.61
Batch: 420; loss: 1.37; acc: 0.72
Batch: 440; loss: 1.42; acc: 0.7
Batch: 460; loss: 1.38; acc: 0.69
Batch: 480; loss: 1.27; acc: 0.72
Batch: 500; loss: 1.33; acc: 0.73
Batch: 520; loss: 1.35; acc: 0.7
Batch: 540; loss: 1.27; acc: 0.78
Batch: 560; loss: 1.35; acc: 0.67
Batch: 580; loss: 1.35; acc: 0.67
Batch: 600; loss: 1.34; acc: 0.7
Batch: 620; loss: 1.34; acc: 0.72
Batch: 640; loss: 1.28; acc: 0.72
Batch: 660; loss: 1.41; acc: 0.69
Batch: 680; loss: 1.47; acc: 0.59
Batch: 700; loss: 1.43; acc: 0.64
Batch: 720; loss: 1.43; acc: 0.62
Batch: 740; loss: 1.38; acc: 0.69
Batch: 760; loss: 1.31; acc: 0.7
Batch: 780; loss: 1.3; acc: 0.73
Train Epoch over. train_loss: 1.39; train_accuracy: 0.68 

0.00010113976168213412
9.663485980127007e-05
Batch: 0; loss: 1.31; acc: 0.75
Batch: 20; loss: 1.58; acc: 0.52
Batch: 40; loss: 1.11; acc: 0.88
Batch: 60; loss: 1.25; acc: 0.72
Batch: 80; loss: 1.24; acc: 0.73
Batch: 100; loss: 1.37; acc: 0.66
Batch: 120; loss: 1.51; acc: 0.61
Batch: 140; loss: 1.09; acc: 0.81
Val Epoch over. val_loss: 1.3378573079018077; val_accuracy: 0.6983479299363057 

The current subspace-distance is: 9.663485980127007e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.72
Batch: 20; loss: 1.45; acc: 0.59
Batch: 40; loss: 1.36; acc: 0.64
Batch: 60; loss: 1.4; acc: 0.67
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.4; acc: 0.66
Batch: 120; loss: 1.27; acc: 0.77
Batch: 140; loss: 1.5; acc: 0.61
Batch: 160; loss: 1.35; acc: 0.62
Batch: 180; loss: 1.35; acc: 0.69
Batch: 200; loss: 1.37; acc: 0.67
Batch: 220; loss: 1.42; acc: 0.61
Batch: 240; loss: 1.38; acc: 0.61
Batch: 260; loss: 1.49; acc: 0.59
Batch: 280; loss: 1.35; acc: 0.69
Batch: 300; loss: 1.46; acc: 0.58
Batch: 320; loss: 1.32; acc: 0.72
Batch: 340; loss: 1.34; acc: 0.66
Batch: 360; loss: 1.39; acc: 0.67
Batch: 380; loss: 1.28; acc: 0.69
Batch: 400; loss: 1.26; acc: 0.7
Batch: 420; loss: 1.27; acc: 0.67
Batch: 440; loss: 1.41; acc: 0.66
Batch: 460; loss: 1.37; acc: 0.69
Batch: 480; loss: 1.34; acc: 0.64
Batch: 500; loss: 1.28; acc: 0.7
Batch: 520; loss: 1.3; acc: 0.64
Batch: 540; loss: 1.27; acc: 0.77
Batch: 560; loss: 1.37; acc: 0.61
Batch: 580; loss: 1.45; acc: 0.56
Batch: 600; loss: 1.32; acc: 0.67
Batch: 620; loss: 1.36; acc: 0.66
Batch: 640; loss: 1.37; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.73
Batch: 680; loss: 1.37; acc: 0.67
Batch: 700; loss: 1.48; acc: 0.56
Batch: 720; loss: 1.4; acc: 0.64
Batch: 740; loss: 1.46; acc: 0.55
Batch: 760; loss: 1.26; acc: 0.69
Batch: 780; loss: 1.38; acc: 0.62
Train Epoch over. train_loss: 1.36; train_accuracy: 0.67 

0.00011047690350096673
0.00010470582492416725
Batch: 0; loss: 1.29; acc: 0.77
Batch: 20; loss: 1.56; acc: 0.55
Batch: 40; loss: 1.06; acc: 0.86
Batch: 60; loss: 1.19; acc: 0.73
Batch: 80; loss: 1.22; acc: 0.67
Batch: 100; loss: 1.37; acc: 0.67
Batch: 120; loss: 1.48; acc: 0.61
Batch: 140; loss: 1.06; acc: 0.8
Val Epoch over. val_loss: 1.3097821147578537; val_accuracy: 0.6917794585987261 

The current subspace-distance is: 0.00010470582492416725 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.64
Batch: 20; loss: 1.52; acc: 0.55
Batch: 40; loss: 1.32; acc: 0.66
Batch: 60; loss: 1.37; acc: 0.64
Batch: 80; loss: 1.33; acc: 0.64
Batch: 100; loss: 1.32; acc: 0.67
Batch: 120; loss: 1.38; acc: 0.61
Batch: 140; loss: 1.41; acc: 0.58
Batch: 160; loss: 1.26; acc: 0.7
Batch: 180; loss: 1.26; acc: 0.78
Batch: 200; loss: 1.44; acc: 0.55
Batch: 220; loss: 1.35; acc: 0.61
Batch: 240; loss: 1.36; acc: 0.7
Batch: 260; loss: 1.58; acc: 0.55
Batch: 280; loss: 1.28; acc: 0.78
Batch: 300; loss: 1.34; acc: 0.66
Batch: 320; loss: 1.31; acc: 0.7
Batch: 340; loss: 1.42; acc: 0.64
Batch: 360; loss: 1.27; acc: 0.67
Batch: 380; loss: 1.32; acc: 0.66
Batch: 400; loss: 1.32; acc: 0.73
Batch: 420; loss: 1.4; acc: 0.62
Batch: 440; loss: 1.39; acc: 0.62
Batch: 460; loss: 1.35; acc: 0.67
Batch: 480; loss: 1.33; acc: 0.66
Batch: 500; loss: 1.27; acc: 0.67
Batch: 520; loss: 1.23; acc: 0.77
Batch: 540; loss: 1.4; acc: 0.64
Batch: 560; loss: 1.37; acc: 0.64
Batch: 580; loss: 1.26; acc: 0.8
Batch: 600; loss: 1.41; acc: 0.59
Batch: 620; loss: 1.4; acc: 0.61
Batch: 640; loss: 1.4; acc: 0.67
Batch: 660; loss: 1.41; acc: 0.58
Batch: 680; loss: 1.38; acc: 0.55
Batch: 700; loss: 1.33; acc: 0.62
Batch: 720; loss: 1.37; acc: 0.64
Batch: 740; loss: 1.35; acc: 0.61
Batch: 760; loss: 1.28; acc: 0.67
Batch: 780; loss: 1.3; acc: 0.66
Train Epoch over. train_loss: 1.33; train_accuracy: 0.67 

0.00011971162894042209
0.00011321226338623092
Batch: 0; loss: 1.27; acc: 0.77
Batch: 20; loss: 1.54; acc: 0.5
Batch: 40; loss: 0.99; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.2; acc: 0.69
Batch: 100; loss: 1.35; acc: 0.64
Batch: 120; loss: 1.43; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.75
Val Epoch over. val_loss: 1.265678174936088; val_accuracy: 0.6866042993630573 

The current subspace-distance is: 0.00011321226338623092 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.75
Batch: 20; loss: 1.27; acc: 0.67
Batch: 40; loss: 1.19; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.22; acc: 0.69
Batch: 100; loss: 1.19; acc: 0.78
Batch: 120; loss: 1.23; acc: 0.73
Batch: 140; loss: 1.28; acc: 0.7
Batch: 160; loss: 1.35; acc: 0.67
Batch: 180; loss: 1.37; acc: 0.67
Batch: 200; loss: 1.22; acc: 0.69
Batch: 220; loss: 1.35; acc: 0.59
Batch: 240; loss: 1.39; acc: 0.64
Batch: 260; loss: 1.47; acc: 0.59
Batch: 280; loss: 1.33; acc: 0.73
Batch: 300; loss: 1.51; acc: 0.61
Batch: 320; loss: 1.4; acc: 0.62
Batch: 340; loss: 1.4; acc: 0.59
Batch: 360; loss: 1.16; acc: 0.72
Batch: 380; loss: 1.32; acc: 0.66
Batch: 400; loss: 1.41; acc: 0.59
Batch: 420; loss: 1.25; acc: 0.72
Batch: 440; loss: 1.37; acc: 0.59
Batch: 460; loss: 1.33; acc: 0.69
Batch: 480; loss: 1.29; acc: 0.72
Batch: 500; loss: 1.33; acc: 0.62
Batch: 520; loss: 1.38; acc: 0.64
Batch: 540; loss: 1.35; acc: 0.62
Batch: 560; loss: 1.26; acc: 0.69
Batch: 580; loss: 1.3; acc: 0.69
Batch: 600; loss: 1.32; acc: 0.66
Batch: 620; loss: 1.22; acc: 0.75
Batch: 640; loss: 1.28; acc: 0.72
Batch: 660; loss: 1.51; acc: 0.5
Batch: 680; loss: 1.28; acc: 0.66
Batch: 700; loss: 1.37; acc: 0.61
Batch: 720; loss: 1.4; acc: 0.58
Batch: 740; loss: 1.19; acc: 0.66
Batch: 760; loss: 1.46; acc: 0.59
Batch: 780; loss: 1.22; acc: 0.73
Train Epoch over. train_loss: 1.3; train_accuracy: 0.66 

0.00012854034139309078
0.00012109400267945603
Batch: 0; loss: 1.29; acc: 0.7
Batch: 20; loss: 1.5; acc: 0.48
Batch: 40; loss: 0.93; acc: 0.88
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 1.19; acc: 0.67
Batch: 100; loss: 1.32; acc: 0.69
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 1.03; acc: 0.75
Val Epoch over. val_loss: 1.2364976736390667; val_accuracy: 0.6836186305732485 

The current subspace-distance is: 0.00012109400267945603 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.75
Batch: 20; loss: 1.38; acc: 0.7
Batch: 40; loss: 1.33; acc: 0.67
Batch: 60; loss: 1.25; acc: 0.62
Batch: 80; loss: 1.37; acc: 0.61
Batch: 100; loss: 1.26; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.72
Batch: 140; loss: 1.48; acc: 0.48
Batch: 160; loss: 1.21; acc: 0.69
Batch: 180; loss: 1.22; acc: 0.73
Batch: 200; loss: 1.26; acc: 0.67
Batch: 220; loss: 1.23; acc: 0.67
Batch: 240; loss: 1.22; acc: 0.67
Batch: 260; loss: 1.31; acc: 0.66
Batch: 280; loss: 1.34; acc: 0.62
Batch: 300; loss: 1.14; acc: 0.72
Batch: 320; loss: 1.24; acc: 0.7
Batch: 340; loss: 1.08; acc: 0.8
Batch: 360; loss: 1.29; acc: 0.67
Batch: 380; loss: 1.22; acc: 0.69
Batch: 400; loss: 1.23; acc: 0.78
Batch: 420; loss: 1.39; acc: 0.59
Batch: 440; loss: 1.21; acc: 0.72
Batch: 460; loss: 1.31; acc: 0.59
Batch: 480; loss: 1.19; acc: 0.73
Batch: 500; loss: 1.27; acc: 0.59
Batch: 520; loss: 1.28; acc: 0.69
Batch: 540; loss: 1.16; acc: 0.7
Batch: 560; loss: 1.23; acc: 0.66
Batch: 580; loss: 1.12; acc: 0.75
Batch: 600; loss: 1.35; acc: 0.55
Batch: 620; loss: 1.27; acc: 0.69
Batch: 640; loss: 1.31; acc: 0.66
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.19; acc: 0.7
Batch: 700; loss: 1.27; acc: 0.59
Batch: 720; loss: 1.16; acc: 0.72
Batch: 740; loss: 1.36; acc: 0.61
Batch: 760; loss: 1.23; acc: 0.66
Batch: 780; loss: 1.09; acc: 0.8
Train Epoch over. train_loss: 1.26; train_accuracy: 0.67 

0.00013539886276703328
0.00013017516175750643
Batch: 0; loss: 1.26; acc: 0.66
Batch: 20; loss: 1.45; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.84
Batch: 60; loss: 1.02; acc: 0.81
Batch: 80; loss: 1.15; acc: 0.75
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.38; acc: 0.55
Batch: 140; loss: 0.98; acc: 0.75
Val Epoch over. val_loss: 1.1953498383236538; val_accuracy: 0.6963574840764332 

The current subspace-distance is: 0.00013017516175750643 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 1.16; acc: 0.72
Batch: 60; loss: 1.06; acc: 0.78
Batch: 80; loss: 1.25; acc: 0.61
Batch: 100; loss: 1.3; acc: 0.69
Batch: 120; loss: 1.46; acc: 0.55
Batch: 140; loss: 1.22; acc: 0.67
Batch: 160; loss: 1.19; acc: 0.67
Batch: 180; loss: 1.21; acc: 0.7
Batch: 200; loss: 1.31; acc: 0.69
Batch: 220; loss: 1.2; acc: 0.72
Batch: 240; loss: 1.29; acc: 0.64
Batch: 260; loss: 1.41; acc: 0.59
Batch: 280; loss: 1.21; acc: 0.73
Batch: 300; loss: 1.4; acc: 0.64
Batch: 320; loss: 1.19; acc: 0.72
Batch: 340; loss: 1.01; acc: 0.83
Batch: 360; loss: 1.24; acc: 0.67
Batch: 380; loss: 1.21; acc: 0.62
Batch: 400; loss: 1.23; acc: 0.66
Batch: 420; loss: 1.26; acc: 0.66
Batch: 440; loss: 1.34; acc: 0.59
Batch: 460; loss: 1.13; acc: 0.73
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.43; acc: 0.59
Batch: 520; loss: 1.08; acc: 0.75
Batch: 540; loss: 1.33; acc: 0.66
Batch: 560; loss: 1.15; acc: 0.67
Batch: 580; loss: 1.11; acc: 0.73
Batch: 600; loss: 1.39; acc: 0.58
Batch: 620; loss: 1.22; acc: 0.7
Batch: 640; loss: 1.22; acc: 0.62
Batch: 660; loss: 1.21; acc: 0.67
Batch: 680; loss: 1.21; acc: 0.66
Batch: 700; loss: 1.1; acc: 0.78
Batch: 720; loss: 1.26; acc: 0.69
Batch: 740; loss: 1.08; acc: 0.7
Batch: 760; loss: 1.32; acc: 0.55
Batch: 780; loss: 1.17; acc: 0.77
Train Epoch over. train_loss: 1.23; train_accuracy: 0.67 

0.0001404694194206968
0.00013667730672750622
Batch: 0; loss: 1.25; acc: 0.67
Batch: 20; loss: 1.44; acc: 0.58
Batch: 40; loss: 0.87; acc: 0.86
Batch: 60; loss: 1.01; acc: 0.81
Batch: 80; loss: 1.14; acc: 0.75
Batch: 100; loss: 1.29; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.97; acc: 0.75
Val Epoch over. val_loss: 1.1823760441913727; val_accuracy: 0.6988455414012739 

The current subspace-distance is: 0.00013667730672750622 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.73
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 1.23; acc: 0.62
Batch: 60; loss: 1.29; acc: 0.62
Batch: 80; loss: 1.11; acc: 0.75
Batch: 100; loss: 1.11; acc: 0.77
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 1.33; acc: 0.62
Batch: 180; loss: 1.18; acc: 0.7
Batch: 200; loss: 1.16; acc: 0.67
Batch: 220; loss: 1.03; acc: 0.8
Batch: 240; loss: 1.28; acc: 0.62
Batch: 260; loss: 1.16; acc: 0.66
Batch: 280; loss: 1.19; acc: 0.7
Batch: 300; loss: 1.24; acc: 0.7
Batch: 320; loss: 1.14; acc: 0.67
Batch: 340; loss: 1.13; acc: 0.73
Batch: 360; loss: 1.16; acc: 0.72
Batch: 380; loss: 1.1; acc: 0.78
Batch: 400; loss: 1.36; acc: 0.61
Batch: 420; loss: 1.02; acc: 0.84
Batch: 440; loss: 1.18; acc: 0.69
Batch: 460; loss: 1.17; acc: 0.72
Batch: 480; loss: 1.14; acc: 0.77
Batch: 500; loss: 1.19; acc: 0.7
Batch: 520; loss: 1.24; acc: 0.69
Batch: 540; loss: 1.32; acc: 0.61
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.05; acc: 0.78
Batch: 600; loss: 1.13; acc: 0.72
Batch: 620; loss: 1.16; acc: 0.72
Batch: 640; loss: 1.18; acc: 0.62
Batch: 660; loss: 1.1; acc: 0.7
Batch: 680; loss: 1.23; acc: 0.66
Batch: 700; loss: 1.08; acc: 0.72
Batch: 720; loss: 1.33; acc: 0.58
Batch: 740; loss: 1.11; acc: 0.78
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.34; acc: 0.7
Train Epoch over. train_loss: 1.21; train_accuracy: 0.68 

0.0001454149605706334
0.00013732661318499595
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.4; acc: 0.59
Batch: 40; loss: 0.85; acc: 0.88
Batch: 60; loss: 1.0; acc: 0.84
Batch: 80; loss: 1.13; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.64
Batch: 120; loss: 1.34; acc: 0.53
Batch: 140; loss: 0.95; acc: 0.77
Val Epoch over. val_loss: 1.1617074593616898; val_accuracy: 0.7054140127388535 

The current subspace-distance is: 0.00013732661318499595 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.35; acc: 0.64
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.25; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 1.2; acc: 0.73
Batch: 160; loss: 1.12; acc: 0.77
Batch: 180; loss: 1.09; acc: 0.75
Batch: 200; loss: 1.16; acc: 0.69
Batch: 220; loss: 1.19; acc: 0.62
Batch: 240; loss: 1.13; acc: 0.69
Batch: 260; loss: 1.32; acc: 0.61
Batch: 280; loss: 1.13; acc: 0.75
Batch: 300; loss: 1.14; acc: 0.75
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 1.1; acc: 0.78
Batch: 360; loss: 1.05; acc: 0.72
Batch: 380; loss: 1.19; acc: 0.69
Batch: 400; loss: 1.11; acc: 0.64
Batch: 420; loss: 1.21; acc: 0.7
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.08; acc: 0.72
Batch: 480; loss: 1.23; acc: 0.64
Batch: 500; loss: 1.37; acc: 0.59
Batch: 520; loss: 1.12; acc: 0.77
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.09; acc: 0.75
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.07; acc: 0.77
Batch: 620; loss: 1.26; acc: 0.64
Batch: 640; loss: 1.2; acc: 0.66
Batch: 660; loss: 1.23; acc: 0.62
Batch: 680; loss: 1.07; acc: 0.7
Batch: 700; loss: 1.27; acc: 0.69
Batch: 720; loss: 1.15; acc: 0.67
Batch: 740; loss: 1.22; acc: 0.67
Batch: 760; loss: 1.18; acc: 0.7
Batch: 780; loss: 1.09; acc: 0.75
Train Epoch over. train_loss: 1.19; train_accuracy: 0.69 

0.0001488076086388901
0.00014161824947223067
Batch: 0; loss: 1.18; acc: 0.7
Batch: 20; loss: 1.39; acc: 0.58
Batch: 40; loss: 0.83; acc: 0.88
Batch: 60; loss: 0.98; acc: 0.84
Batch: 80; loss: 1.12; acc: 0.75
Batch: 100; loss: 1.24; acc: 0.66
Batch: 120; loss: 1.32; acc: 0.55
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 1.140905499458313; val_accuracy: 0.7139729299363057 

The current subspace-distance is: 0.00014161824947223067 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.27; acc: 0.67
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.23; acc: 0.55
Batch: 140; loss: 1.28; acc: 0.61
Batch: 160; loss: 1.24; acc: 0.69
Batch: 180; loss: 1.2; acc: 0.66
Batch: 200; loss: 1.23; acc: 0.72
Batch: 220; loss: 1.27; acc: 0.66
Batch: 240; loss: 1.19; acc: 0.69
Batch: 260; loss: 1.07; acc: 0.78
Batch: 280; loss: 1.24; acc: 0.7
Batch: 300; loss: 1.24; acc: 0.61
Batch: 320; loss: 1.06; acc: 0.75
Batch: 340; loss: 1.22; acc: 0.64
Batch: 360; loss: 1.08; acc: 0.7
Batch: 380; loss: 1.04; acc: 0.77
Batch: 400; loss: 1.02; acc: 0.8
Batch: 420; loss: 1.19; acc: 0.72
Batch: 440; loss: 1.34; acc: 0.66
Batch: 460; loss: 1.14; acc: 0.69
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.72
Batch: 520; loss: 1.31; acc: 0.58
Batch: 540; loss: 1.15; acc: 0.69
Batch: 560; loss: 1.2; acc: 0.67
Batch: 580; loss: 1.28; acc: 0.61
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.18; acc: 0.64
Batch: 640; loss: 1.06; acc: 0.8
Batch: 660; loss: 1.18; acc: 0.73
Batch: 680; loss: 1.22; acc: 0.66
Batch: 700; loss: 1.11; acc: 0.7
Batch: 720; loss: 1.16; acc: 0.75
Batch: 740; loss: 1.21; acc: 0.69
Batch: 760; loss: 1.17; acc: 0.7
Batch: 780; loss: 0.95; acc: 0.8
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00015588807582389563
0.00014776892203371972
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.36; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.83
Batch: 80; loss: 1.11; acc: 0.77
Batch: 100; loss: 1.24; acc: 0.64
Batch: 120; loss: 1.29; acc: 0.55
Batch: 140; loss: 0.93; acc: 0.78
Val Epoch over. val_loss: 1.1236141255706737; val_accuracy: 0.7195461783439491 

The current subspace-distance is: 0.00014776892203371972 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.73
Batch: 40; loss: 1.12; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.62
Batch: 80; loss: 1.11; acc: 0.7
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 1.22; acc: 0.61
Batch: 160; loss: 1.15; acc: 0.75
Batch: 180; loss: 1.01; acc: 0.75
Batch: 200; loss: 0.94; acc: 0.84
Batch: 220; loss: 1.1; acc: 0.7
Batch: 240; loss: 1.1; acc: 0.75
Batch: 260; loss: 1.17; acc: 0.69
Batch: 280; loss: 1.03; acc: 0.81
Batch: 300; loss: 1.21; acc: 0.72
Batch: 320; loss: 1.19; acc: 0.72
Batch: 340; loss: 1.1; acc: 0.69
Batch: 360; loss: 1.22; acc: 0.59
Batch: 380; loss: 1.09; acc: 0.72
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 1.12; acc: 0.73
Batch: 440; loss: 1.15; acc: 0.67
Batch: 460; loss: 1.15; acc: 0.7
Batch: 480; loss: 1.23; acc: 0.69
Batch: 500; loss: 1.29; acc: 0.62
Batch: 520; loss: 1.06; acc: 0.69
Batch: 540; loss: 1.3; acc: 0.59
Batch: 560; loss: 1.03; acc: 0.75
Batch: 580; loss: 1.13; acc: 0.8
Batch: 600; loss: 1.32; acc: 0.59
Batch: 620; loss: 1.17; acc: 0.69
Batch: 640; loss: 1.15; acc: 0.75
Batch: 660; loss: 1.3; acc: 0.62
Batch: 680; loss: 1.0; acc: 0.78
Batch: 700; loss: 1.04; acc: 0.73
Batch: 720; loss: 1.3; acc: 0.66
Batch: 740; loss: 1.15; acc: 0.67
Batch: 760; loss: 1.24; acc: 0.7
Batch: 780; loss: 1.2; acc: 0.72
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.00015804139547981322
0.00014980084961280227
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.36; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 1.13; acc: 0.75
Batch: 100; loss: 1.25; acc: 0.61
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 0.92; acc: 0.77
Val Epoch over. val_loss: 1.116611134854092; val_accuracy: 0.7158638535031847 

The current subspace-distance is: 0.00014980084961280227 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.21; acc: 0.7
Batch: 40; loss: 1.16; acc: 0.72
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 1.26; acc: 0.62
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.27; acc: 0.64
Batch: 180; loss: 1.11; acc: 0.72
Batch: 200; loss: 1.08; acc: 0.73
Batch: 220; loss: 1.16; acc: 0.61
Batch: 240; loss: 1.06; acc: 0.72
Batch: 260; loss: 1.12; acc: 0.77
Batch: 280; loss: 1.09; acc: 0.77
Batch: 300; loss: 1.15; acc: 0.64
Batch: 320; loss: 1.26; acc: 0.67
Batch: 340; loss: 1.27; acc: 0.62
Batch: 360; loss: 1.16; acc: 0.77
Batch: 380; loss: 1.15; acc: 0.7
Batch: 400; loss: 1.07; acc: 0.7
Batch: 420; loss: 1.18; acc: 0.67
Batch: 440; loss: 1.12; acc: 0.7
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.23; acc: 0.7
Batch: 500; loss: 1.13; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.15; acc: 0.73
Batch: 560; loss: 1.14; acc: 0.72
Batch: 580; loss: 1.24; acc: 0.66
Batch: 600; loss: 1.08; acc: 0.7
Batch: 620; loss: 1.17; acc: 0.69
Batch: 640; loss: 1.21; acc: 0.69
Batch: 660; loss: 1.15; acc: 0.69
Batch: 680; loss: 1.14; acc: 0.73
Batch: 700; loss: 1.12; acc: 0.7
Batch: 720; loss: 1.18; acc: 0.67
Batch: 740; loss: 1.03; acc: 0.77
Batch: 760; loss: 1.1; acc: 0.75
Batch: 780; loss: 1.11; acc: 0.7
Train Epoch over. train_loss: 1.14; train_accuracy: 0.7 

0.00016211842012126
0.00015729920414742082
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.33; acc: 0.59
Batch: 40; loss: 0.79; acc: 0.89
Batch: 60; loss: 0.93; acc: 0.81
Batch: 80; loss: 1.12; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.25; acc: 0.56
Batch: 140; loss: 0.89; acc: 0.78
Val Epoch over. val_loss: 1.0866088643195524; val_accuracy: 0.7223328025477707 

The current subspace-distance is: 0.00015729920414742082 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 1.18; acc: 0.67
Batch: 60; loss: 1.28; acc: 0.66
Batch: 80; loss: 1.12; acc: 0.66
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 1.11; acc: 0.7
Batch: 180; loss: 1.13; acc: 0.69
Batch: 200; loss: 1.0; acc: 0.75
Batch: 220; loss: 1.03; acc: 0.8
Batch: 240; loss: 1.16; acc: 0.73
Batch: 260; loss: 1.1; acc: 0.77
Batch: 280; loss: 1.17; acc: 0.67
Batch: 300; loss: 1.16; acc: 0.69
Batch: 320; loss: 1.16; acc: 0.62
Batch: 340; loss: 1.19; acc: 0.7
Batch: 360; loss: 1.06; acc: 0.78
Batch: 380; loss: 1.23; acc: 0.56
Batch: 400; loss: 1.17; acc: 0.72
Batch: 420; loss: 1.05; acc: 0.69
Batch: 440; loss: 1.17; acc: 0.69
Batch: 460; loss: 1.19; acc: 0.67
Batch: 480; loss: 0.94; acc: 0.78
Batch: 500; loss: 1.07; acc: 0.69
Batch: 520; loss: 0.95; acc: 0.72
Batch: 540; loss: 1.27; acc: 0.62
Batch: 560; loss: 1.1; acc: 0.62
Batch: 580; loss: 1.01; acc: 0.77
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.23; acc: 0.67
Batch: 640; loss: 1.15; acc: 0.78
Batch: 660; loss: 1.23; acc: 0.64
Batch: 680; loss: 1.18; acc: 0.7
Batch: 700; loss: 1.23; acc: 0.62
Batch: 720; loss: 1.09; acc: 0.66
Batch: 740; loss: 1.08; acc: 0.67
Batch: 760; loss: 1.02; acc: 0.75
Batch: 780; loss: 1.05; acc: 0.7
Train Epoch over. train_loss: 1.13; train_accuracy: 0.7 

0.00016468088142573833
0.00015626155072823167
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 0.94; acc: 0.78
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.21; acc: 0.59
Batch: 120; loss: 1.25; acc: 0.62
Batch: 140; loss: 0.9; acc: 0.8
Val Epoch over. val_loss: 1.0839311552655166; val_accuracy: 0.7249203821656051 

The current subspace-distance is: 0.00015626155072823167 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 1.15; acc: 0.75
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.22; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 1.05; acc: 0.73
Batch: 160; loss: 1.26; acc: 0.66
Batch: 180; loss: 1.15; acc: 0.66
Batch: 200; loss: 1.25; acc: 0.66
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 0.97; acc: 0.81
Batch: 260; loss: 1.09; acc: 0.75
Batch: 280; loss: 0.98; acc: 0.78
Batch: 300; loss: 1.08; acc: 0.69
Batch: 320; loss: 1.07; acc: 0.7
Batch: 340; loss: 0.93; acc: 0.81
Batch: 360; loss: 1.19; acc: 0.64
Batch: 380; loss: 1.07; acc: 0.75
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 0.88; acc: 0.89
Batch: 440; loss: 1.01; acc: 0.75
Batch: 460; loss: 1.11; acc: 0.73
Batch: 480; loss: 1.14; acc: 0.72
Batch: 500; loss: 1.2; acc: 0.62
Batch: 520; loss: 1.03; acc: 0.77
Batch: 540; loss: 1.19; acc: 0.69
Batch: 560; loss: 0.96; acc: 0.75
Batch: 580; loss: 1.1; acc: 0.67
Batch: 600; loss: 1.01; acc: 0.75
Batch: 620; loss: 0.92; acc: 0.8
Batch: 640; loss: 1.13; acc: 0.72
Batch: 660; loss: 1.16; acc: 0.64
Batch: 680; loss: 1.22; acc: 0.72
Batch: 700; loss: 1.05; acc: 0.73
Batch: 720; loss: 1.3; acc: 0.64
Batch: 740; loss: 1.12; acc: 0.69
Batch: 760; loss: 1.07; acc: 0.78
Batch: 780; loss: 1.1; acc: 0.66
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.00016721720749046654
0.0001598482340341434
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.31; acc: 0.58
Batch: 40; loss: 0.76; acc: 0.89
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 1.11; acc: 0.78
Batch: 100; loss: 1.21; acc: 0.61
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.87; acc: 0.8
Val Epoch over. val_loss: 1.0654409129148836; val_accuracy: 0.730593152866242 

The current subspace-distance is: 0.0001598482340341434 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.01; acc: 0.67
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 1.03; acc: 0.75
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.66
Batch: 120; loss: 1.1; acc: 0.72
Batch: 140; loss: 1.18; acc: 0.7
Batch: 160; loss: 1.06; acc: 0.69
Batch: 180; loss: 1.08; acc: 0.77
Batch: 200; loss: 1.18; acc: 0.62
Batch: 220; loss: 1.0; acc: 0.81
Batch: 240; loss: 0.96; acc: 0.77
Batch: 260; loss: 1.06; acc: 0.7
Batch: 280; loss: 1.23; acc: 0.59
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 0.98; acc: 0.75
Batch: 340; loss: 0.99; acc: 0.78
Batch: 360; loss: 1.1; acc: 0.72
Batch: 380; loss: 0.96; acc: 0.77
Batch: 400; loss: 1.05; acc: 0.72
Batch: 420; loss: 1.2; acc: 0.59
Batch: 440; loss: 1.12; acc: 0.69
Batch: 460; loss: 0.99; acc: 0.73
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.16; acc: 0.7
Batch: 520; loss: 1.04; acc: 0.73
Batch: 540; loss: 1.01; acc: 0.75
Batch: 560; loss: 1.12; acc: 0.73
Batch: 580; loss: 1.19; acc: 0.7
Batch: 600; loss: 1.22; acc: 0.64
Batch: 620; loss: 1.2; acc: 0.64
Batch: 640; loss: 1.15; acc: 0.67
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.03; acc: 0.75
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 1.09; acc: 0.7
Batch: 760; loss: 1.22; acc: 0.62
Batch: 780; loss: 1.11; acc: 0.69
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.0001708644995233044
0.0001630442129680887
Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.33; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 0.91; acc: 0.75
Batch: 80; loss: 1.09; acc: 0.78
Batch: 100; loss: 1.21; acc: 0.66
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.83
Val Epoch over. val_loss: 1.05903550507916; val_accuracy: 0.7308917197452229 

The current subspace-distance is: 0.0001630442129680887 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.14; acc: 0.7
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 1.24; acc: 0.72
Batch: 60; loss: 1.09; acc: 0.73
Batch: 80; loss: 0.87; acc: 0.84
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 0.97; acc: 0.84
Batch: 140; loss: 0.96; acc: 0.77
Batch: 160; loss: 1.09; acc: 0.64
Batch: 180; loss: 1.08; acc: 0.73
Batch: 200; loss: 1.14; acc: 0.72
Batch: 220; loss: 1.02; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.66
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 0.91; acc: 0.77
Batch: 300; loss: 1.04; acc: 0.64
Batch: 320; loss: 1.1; acc: 0.73
Batch: 340; loss: 1.15; acc: 0.72
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.66
Batch: 400; loss: 1.02; acc: 0.72
Batch: 420; loss: 1.11; acc: 0.73
Batch: 440; loss: 0.96; acc: 0.81
Batch: 460; loss: 1.13; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.75
Batch: 500; loss: 0.96; acc: 0.75
Batch: 520; loss: 1.15; acc: 0.69
Batch: 540; loss: 1.03; acc: 0.7
Batch: 560; loss: 1.1; acc: 0.73
Batch: 580; loss: 1.17; acc: 0.69
Batch: 600; loss: 1.19; acc: 0.67
Batch: 620; loss: 1.13; acc: 0.7
Batch: 640; loss: 0.89; acc: 0.84
Batch: 660; loss: 1.09; acc: 0.69
Batch: 680; loss: 1.09; acc: 0.73
Batch: 700; loss: 1.18; acc: 0.7
Batch: 720; loss: 1.08; acc: 0.72
Batch: 740; loss: 1.02; acc: 0.75
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 1.29; acc: 0.66
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00017471138562541455
0.00016947554831858724
Batch: 0; loss: 1.04; acc: 0.72
Batch: 20; loss: 1.31; acc: 0.61
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 1.09; acc: 0.77
Batch: 100; loss: 1.18; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.8
Val Epoch over. val_loss: 1.0434551903396656; val_accuracy: 0.7320859872611465 

The current subspace-distance is: 0.00016947554831858724 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 0.96; acc: 0.75
Batch: 120; loss: 1.08; acc: 0.78
Batch: 140; loss: 1.2; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.78
Batch: 180; loss: 1.03; acc: 0.77
Batch: 200; loss: 1.14; acc: 0.64
Batch: 220; loss: 1.16; acc: 0.64
Batch: 240; loss: 0.99; acc: 0.84
Batch: 260; loss: 1.1; acc: 0.67
Batch: 280; loss: 1.07; acc: 0.73
Batch: 300; loss: 1.02; acc: 0.67
Batch: 320; loss: 1.04; acc: 0.73
Batch: 340; loss: 1.06; acc: 0.72
Batch: 360; loss: 1.05; acc: 0.66
Batch: 380; loss: 1.07; acc: 0.67
Batch: 400; loss: 1.06; acc: 0.7
Batch: 420; loss: 1.11; acc: 0.67
Batch: 440; loss: 0.97; acc: 0.77
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.02; acc: 0.75
Batch: 500; loss: 1.0; acc: 0.75
Batch: 520; loss: 1.04; acc: 0.75
Batch: 540; loss: 1.07; acc: 0.72
Batch: 560; loss: 1.1; acc: 0.69
Batch: 580; loss: 1.13; acc: 0.69
Batch: 600; loss: 1.14; acc: 0.72
Batch: 620; loss: 1.02; acc: 0.73
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.2; acc: 0.66
Batch: 680; loss: 1.11; acc: 0.64
Batch: 700; loss: 1.17; acc: 0.67
Batch: 720; loss: 1.03; acc: 0.7
Batch: 740; loss: 1.14; acc: 0.7
Batch: 760; loss: 1.26; acc: 0.64
Batch: 780; loss: 1.0; acc: 0.73
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00017243299225810915
0.00016593960754107684
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 0.91; acc: 0.77
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.21; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 0.86; acc: 0.83
Val Epoch over. val_loss: 1.0485215657835554; val_accuracy: 0.7342754777070064 

The current subspace-distance is: 0.00016593960754107684 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.78
Batch: 20; loss: 1.04; acc: 0.75
Batch: 40; loss: 1.04; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.73
Batch: 80; loss: 1.17; acc: 0.69
Batch: 100; loss: 1.13; acc: 0.61
Batch: 120; loss: 1.12; acc: 0.67
Batch: 140; loss: 1.22; acc: 0.67
Batch: 160; loss: 1.03; acc: 0.67
Batch: 180; loss: 1.16; acc: 0.72
Batch: 200; loss: 1.1; acc: 0.8
Batch: 220; loss: 1.22; acc: 0.73
Batch: 240; loss: 1.11; acc: 0.73
Batch: 260; loss: 1.02; acc: 0.75
Batch: 280; loss: 1.04; acc: 0.67
Batch: 300; loss: 1.04; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.73
Batch: 340; loss: 1.08; acc: 0.69
Batch: 360; loss: 1.03; acc: 0.69
Batch: 380; loss: 1.03; acc: 0.7
Batch: 400; loss: 1.01; acc: 0.83
Batch: 420; loss: 0.91; acc: 0.81
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 1.33; acc: 0.62
Batch: 500; loss: 1.14; acc: 0.64
Batch: 520; loss: 1.16; acc: 0.69
Batch: 540; loss: 1.01; acc: 0.73
Batch: 560; loss: 1.13; acc: 0.77
Batch: 580; loss: 0.98; acc: 0.75
Batch: 600; loss: 1.09; acc: 0.77
Batch: 620; loss: 1.11; acc: 0.69
Batch: 640; loss: 1.34; acc: 0.58
Batch: 660; loss: 1.29; acc: 0.56
Batch: 680; loss: 1.07; acc: 0.78
Batch: 700; loss: 1.12; acc: 0.72
Batch: 720; loss: 1.06; acc: 0.73
Batch: 740; loss: 1.06; acc: 0.7
Batch: 760; loss: 0.95; acc: 0.78
Batch: 780; loss: 1.04; acc: 0.72
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.00017636394477449358
0.000169538936461322
Batch: 0; loss: 1.03; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 1.08; acc: 0.77
Batch: 100; loss: 1.19; acc: 0.62
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 0.85; acc: 0.83
Val Epoch over. val_loss: 1.037532854991354; val_accuracy: 0.7396496815286624 

The current subspace-distance is: 0.000169538936461322 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.06; acc: 0.73
Batch: 20; loss: 1.05; acc: 0.77
Batch: 40; loss: 1.04; acc: 0.7
Batch: 60; loss: 1.07; acc: 0.78
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.64
Batch: 120; loss: 1.12; acc: 0.75
Batch: 140; loss: 1.09; acc: 0.69
Batch: 160; loss: 1.0; acc: 0.75
Batch: 180; loss: 0.98; acc: 0.83
Batch: 200; loss: 1.0; acc: 0.77
Batch: 220; loss: 1.15; acc: 0.64
Batch: 240; loss: 1.15; acc: 0.67
Batch: 260; loss: 0.98; acc: 0.69
Batch: 280; loss: 1.19; acc: 0.66
Batch: 300; loss: 1.08; acc: 0.67
Batch: 320; loss: 1.14; acc: 0.75
Batch: 340; loss: 1.0; acc: 0.73
Batch: 360; loss: 1.06; acc: 0.73
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 1.24; acc: 0.62
Batch: 420; loss: 1.07; acc: 0.77
Batch: 440; loss: 1.17; acc: 0.67
Batch: 460; loss: 0.97; acc: 0.8
Batch: 480; loss: 1.25; acc: 0.59
Batch: 500; loss: 1.07; acc: 0.67
Batch: 520; loss: 1.14; acc: 0.7
Batch: 540; loss: 1.07; acc: 0.67
Batch: 560; loss: 1.0; acc: 0.78
Batch: 580; loss: 0.89; acc: 0.81
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.04; acc: 0.72
Batch: 640; loss: 1.1; acc: 0.73
Batch: 660; loss: 1.08; acc: 0.7
Batch: 680; loss: 1.09; acc: 0.72
Batch: 700; loss: 1.13; acc: 0.69
Batch: 720; loss: 1.06; acc: 0.72
Batch: 740; loss: 1.2; acc: 0.59
Batch: 760; loss: 1.07; acc: 0.69
Batch: 780; loss: 1.12; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.00017531085177324712
0.00016811180103104562
Batch: 0; loss: 1.04; acc: 0.72
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.9; acc: 0.77
Batch: 80; loss: 1.09; acc: 0.77
Batch: 100; loss: 1.18; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 0.86; acc: 0.8
Val Epoch over. val_loss: 1.038560400343245; val_accuracy: 0.7344745222929936 

The current subspace-distance is: 0.00016811180103104562 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.02; acc: 0.75
Batch: 40; loss: 0.94; acc: 0.8
Batch: 60; loss: 1.06; acc: 0.73
Batch: 80; loss: 1.03; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.05; acc: 0.7
Batch: 180; loss: 1.14; acc: 0.72
Batch: 200; loss: 1.03; acc: 0.7
Batch: 220; loss: 1.09; acc: 0.7
Batch: 240; loss: 1.15; acc: 0.66
Batch: 260; loss: 1.1; acc: 0.69
Batch: 280; loss: 1.15; acc: 0.69
Batch: 300; loss: 1.09; acc: 0.7
Batch: 320; loss: 0.97; acc: 0.77
Batch: 340; loss: 0.98; acc: 0.77
Batch: 360; loss: 1.09; acc: 0.69
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.31; acc: 0.56
Batch: 420; loss: 1.12; acc: 0.7
Batch: 440; loss: 0.99; acc: 0.83
Batch: 460; loss: 1.06; acc: 0.77
Batch: 480; loss: 1.09; acc: 0.75
Batch: 500; loss: 1.13; acc: 0.69
Batch: 520; loss: 1.08; acc: 0.75
Batch: 540; loss: 1.17; acc: 0.66
Batch: 560; loss: 1.12; acc: 0.66
Batch: 580; loss: 0.96; acc: 0.75
Batch: 600; loss: 1.27; acc: 0.67
Batch: 620; loss: 1.07; acc: 0.77
Batch: 640; loss: 1.02; acc: 0.75
Batch: 660; loss: 1.16; acc: 0.73
Batch: 680; loss: 1.15; acc: 0.69
Batch: 700; loss: 1.05; acc: 0.7
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.02; acc: 0.75
Batch: 760; loss: 1.06; acc: 0.73
Batch: 780; loss: 0.94; acc: 0.72
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.00017712192493490875
0.00016876084555406123
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 0.91; acc: 0.75
Batch: 80; loss: 1.09; acc: 0.78
Batch: 100; loss: 1.2; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.61
Batch: 140; loss: 0.86; acc: 0.83
Val Epoch over. val_loss: 1.0360867984735282; val_accuracy: 0.7372611464968153 

The current subspace-distance is: 0.00016876084555406123 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.67
Batch: 40; loss: 1.1; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 1.08; acc: 0.69
Batch: 120; loss: 1.26; acc: 0.62
Batch: 140; loss: 1.01; acc: 0.69
Batch: 160; loss: 1.05; acc: 0.73
Batch: 180; loss: 1.06; acc: 0.72
Batch: 200; loss: 0.99; acc: 0.73
Batch: 220; loss: 1.3; acc: 0.75
Batch: 240; loss: 1.15; acc: 0.62
Batch: 260; loss: 1.21; acc: 0.7
Batch: 280; loss: 1.28; acc: 0.64
Batch: 300; loss: 0.93; acc: 0.81
Batch: 320; loss: 0.9; acc: 0.83
Batch: 340; loss: 1.04; acc: 0.72
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.19; acc: 0.7
Batch: 400; loss: 0.93; acc: 0.77
Batch: 420; loss: 0.98; acc: 0.77
Batch: 440; loss: 1.1; acc: 0.7
Batch: 460; loss: 1.0; acc: 0.75
Batch: 480; loss: 1.08; acc: 0.7
Batch: 500; loss: 1.08; acc: 0.73
Batch: 520; loss: 1.17; acc: 0.64
Batch: 540; loss: 1.02; acc: 0.73
Batch: 560; loss: 1.12; acc: 0.7
Batch: 580; loss: 1.17; acc: 0.61
Batch: 600; loss: 1.23; acc: 0.62
Batch: 620; loss: 1.11; acc: 0.69
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.13; acc: 0.73
Batch: 680; loss: 1.03; acc: 0.7
Batch: 700; loss: 1.01; acc: 0.73
Batch: 720; loss: 0.98; acc: 0.77
Batch: 740; loss: 1.14; acc: 0.62
Batch: 760; loss: 1.01; acc: 0.77
Batch: 780; loss: 1.18; acc: 0.66
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.0001788824301911518
0.00017306202789768577
Batch: 0; loss: 1.02; acc: 0.77
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 1.09; acc: 0.78
Batch: 100; loss: 1.17; acc: 0.59
Batch: 120; loss: 1.17; acc: 0.64
Batch: 140; loss: 0.85; acc: 0.83
Val Epoch over. val_loss: 1.0307953589281458; val_accuracy: 0.7359673566878981 

The current subspace-distance is: 0.00017306202789768577 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.0; acc: 0.78
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 0.99; acc: 0.77
Batch: 140; loss: 1.01; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.67
Batch: 180; loss: 0.97; acc: 0.81
Batch: 200; loss: 1.1; acc: 0.67
Batch: 220; loss: 0.92; acc: 0.78
Batch: 240; loss: 1.06; acc: 0.77
Batch: 260; loss: 0.87; acc: 0.8
Batch: 280; loss: 0.97; acc: 0.78
Batch: 300; loss: 1.08; acc: 0.75
Batch: 320; loss: 0.97; acc: 0.78
Batch: 340; loss: 1.04; acc: 0.73
Batch: 360; loss: 0.96; acc: 0.83
Batch: 380; loss: 0.96; acc: 0.77
Batch: 400; loss: 1.23; acc: 0.67
Batch: 420; loss: 1.05; acc: 0.73
Batch: 440; loss: 1.1; acc: 0.67
Batch: 460; loss: 1.22; acc: 0.61
Batch: 480; loss: 1.04; acc: 0.72
Batch: 500; loss: 1.14; acc: 0.75
Batch: 520; loss: 1.07; acc: 0.73
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 0.92; acc: 0.77
Batch: 580; loss: 1.02; acc: 0.77
Batch: 600; loss: 1.02; acc: 0.78
Batch: 620; loss: 1.05; acc: 0.75
Batch: 640; loss: 1.06; acc: 0.72
Batch: 660; loss: 1.16; acc: 0.7
Batch: 680; loss: 1.05; acc: 0.72
Batch: 700; loss: 0.98; acc: 0.78
Batch: 720; loss: 1.09; acc: 0.69
Batch: 740; loss: 1.14; acc: 0.7
Batch: 760; loss: 1.12; acc: 0.69
Batch: 780; loss: 1.13; acc: 0.67
Train Epoch over. train_loss: 1.08; train_accuracy: 0.71 

0.00017764285439625382
0.00016805085761006922
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.3; acc: 0.59
Batch: 40; loss: 0.71; acc: 0.86
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 1.08; acc: 0.78
Batch: 100; loss: 1.18; acc: 0.62
Batch: 120; loss: 1.18; acc: 0.69
Batch: 140; loss: 0.86; acc: 0.83
Val Epoch over. val_loss: 1.0383162301057463; val_accuracy: 0.7368630573248408 

The current subspace-distance is: 0.00016805085761006922 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.75
Batch: 20; loss: 0.94; acc: 0.81
Batch: 40; loss: 1.29; acc: 0.61
Batch: 60; loss: 1.04; acc: 0.72
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.15; acc: 0.67
Batch: 140; loss: 0.96; acc: 0.75
Batch: 160; loss: 1.22; acc: 0.64
Batch: 180; loss: 1.06; acc: 0.72
Batch: 200; loss: 1.26; acc: 0.59
Batch: 220; loss: 1.05; acc: 0.77
Batch: 240; loss: 0.95; acc: 0.72
Batch: 260; loss: 1.14; acc: 0.7
Batch: 280; loss: 1.08; acc: 0.72
Batch: 300; loss: 1.15; acc: 0.7
Batch: 320; loss: 1.21; acc: 0.66
Batch: 340; loss: 1.11; acc: 0.72
Batch: 360; loss: 1.1; acc: 0.7
Batch: 380; loss: 1.0; acc: 0.73
Batch: 400; loss: 1.09; acc: 0.67
Batch: 420; loss: 1.12; acc: 0.7
Batch: 440; loss: 0.93; acc: 0.75
Batch: 460; loss: 0.9; acc: 0.83
Batch: 480; loss: 1.21; acc: 0.69
Batch: 500; loss: 1.02; acc: 0.77
Batch: 520; loss: 1.05; acc: 0.72
Batch: 540; loss: 0.96; acc: 0.69
Batch: 560; loss: 1.06; acc: 0.69
Batch: 580; loss: 1.03; acc: 0.72
Batch: 600; loss: 1.05; acc: 0.77
Batch: 620; loss: 1.08; acc: 0.73
Batch: 640; loss: 1.16; acc: 0.66
Batch: 660; loss: 1.0; acc: 0.72
Batch: 680; loss: 1.37; acc: 0.66
Batch: 700; loss: 0.99; acc: 0.72
Batch: 720; loss: 1.1; acc: 0.7
Batch: 740; loss: 1.02; acc: 0.75
Batch: 760; loss: 0.94; acc: 0.77
Batch: 780; loss: 1.02; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.71 

0.00017625901091378182
0.00017001794185489416
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 0.91; acc: 0.73
Batch: 80; loss: 1.08; acc: 0.8
Batch: 100; loss: 1.17; acc: 0.62
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 0.86; acc: 0.83
Val Epoch over. val_loss: 1.0363120417686025; val_accuracy: 0.7374601910828026 

The current subspace-distance is: 0.00017001794185489416 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 0.94; acc: 0.78
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 1.03; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.09; acc: 0.7
Batch: 160; loss: 1.15; acc: 0.64
Batch: 180; loss: 0.93; acc: 0.84
Batch: 200; loss: 1.27; acc: 0.69
Batch: 220; loss: 1.07; acc: 0.75
Batch: 240; loss: 1.05; acc: 0.73
Batch: 260; loss: 1.04; acc: 0.69
Batch: 280; loss: 1.13; acc: 0.64
Batch: 300; loss: 1.04; acc: 0.77
Batch: 320; loss: 1.17; acc: 0.7
Batch: 340; loss: 1.12; acc: 0.66
Batch: 360; loss: 1.11; acc: 0.72
Batch: 380; loss: 0.99; acc: 0.77
Batch: 400; loss: 1.0; acc: 0.75
Batch: 420; loss: 1.22; acc: 0.62
Batch: 440; loss: 0.96; acc: 0.8
Batch: 460; loss: 1.03; acc: 0.78
Batch: 480; loss: 1.08; acc: 0.72
Batch: 500; loss: 1.1; acc: 0.67
Batch: 520; loss: 1.2; acc: 0.69
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 1.09; acc: 0.69
Batch: 580; loss: 1.05; acc: 0.69
Batch: 600; loss: 0.98; acc: 0.78
Batch: 620; loss: 1.12; acc: 0.73
Batch: 640; loss: 1.06; acc: 0.75
Batch: 660; loss: 1.12; acc: 0.72
Batch: 680; loss: 1.03; acc: 0.72
Batch: 700; loss: 1.01; acc: 0.77
Batch: 720; loss: 0.98; acc: 0.78
Batch: 740; loss: 0.98; acc: 0.75
Batch: 760; loss: 1.08; acc: 0.67
Batch: 780; loss: 1.12; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.71 

0.00017904439300764352
0.00017173110973089933
Batch: 0; loss: 1.01; acc: 0.77
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 0.7; acc: 0.84
Batch: 60; loss: 0.89; acc: 0.78
Batch: 80; loss: 1.08; acc: 0.77
Batch: 100; loss: 1.16; acc: 0.62
Batch: 120; loss: 1.16; acc: 0.62
Batch: 140; loss: 0.85; acc: 0.83
Val Epoch over. val_loss: 1.024669252383481; val_accuracy: 0.7350716560509554 

The current subspace-distance is: 0.00017173110973089933 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.15; acc: 0.69
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 1.26; acc: 0.62
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 1.11; acc: 0.77
Batch: 100; loss: 1.14; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 1.03; acc: 0.73
Batch: 160; loss: 1.04; acc: 0.77
Batch: 180; loss: 1.18; acc: 0.72
Batch: 200; loss: 1.12; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.66
Batch: 240; loss: 1.12; acc: 0.67
Batch: 260; loss: 1.02; acc: 0.72
Batch: 280; loss: 1.05; acc: 0.72
Batch: 300; loss: 1.2; acc: 0.69
Batch: 320; loss: 0.98; acc: 0.72
Batch: 340; loss: 1.1; acc: 0.75
Batch: 360; loss: 1.08; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.62
Batch: 400; loss: 1.02; acc: 0.7
Batch: 420; loss: 0.96; acc: 0.8
Batch: 440; loss: 1.15; acc: 0.67
Batch: 460; loss: 1.04; acc: 0.7
Batch: 480; loss: 1.19; acc: 0.73
Batch: 500; loss: 1.01; acc: 0.78
Batch: 520; loss: 1.12; acc: 0.67
Batch: 540; loss: 1.16; acc: 0.62
Batch: 560; loss: 0.93; acc: 0.77
Batch: 580; loss: 1.24; acc: 0.56
Batch: 600; loss: 1.09; acc: 0.73
Batch: 620; loss: 1.02; acc: 0.7
Batch: 640; loss: 0.98; acc: 0.8
Batch: 660; loss: 1.1; acc: 0.7
Batch: 680; loss: 1.17; acc: 0.73
Batch: 700; loss: 1.11; acc: 0.64
Batch: 720; loss: 1.26; acc: 0.69
Batch: 740; loss: 1.16; acc: 0.64
Batch: 760; loss: 1.04; acc: 0.77
Batch: 780; loss: 1.04; acc: 0.7
Train Epoch over. train_loss: 1.08; train_accuracy: 0.71 

0.00018238864140585065
0.00017558506806381047
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.17; acc: 0.61
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 0.85; acc: 0.84
Val Epoch over. val_loss: 1.0310120290252054; val_accuracy: 0.7351711783439491 

The current subspace-distance is: 0.00017558506806381047 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 1.03; acc: 0.69
Batch: 60; loss: 1.25; acc: 0.7
Batch: 80; loss: 1.28; acc: 0.59
Batch: 100; loss: 1.11; acc: 0.69
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 1.09; acc: 0.69
Batch: 160; loss: 1.09; acc: 0.72
Batch: 180; loss: 1.11; acc: 0.77
Batch: 200; loss: 0.98; acc: 0.7
Batch: 220; loss: 0.92; acc: 0.77
Batch: 240; loss: 1.01; acc: 0.77
Batch: 260; loss: 1.09; acc: 0.7
Batch: 280; loss: 1.15; acc: 0.7
Batch: 300; loss: 1.02; acc: 0.72
Batch: 320; loss: 0.97; acc: 0.75
Batch: 340; loss: 0.93; acc: 0.78
Batch: 360; loss: 1.0; acc: 0.75
Batch: 380; loss: 0.98; acc: 0.78
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 1.09; acc: 0.67
Batch: 440; loss: 1.15; acc: 0.7
Batch: 460; loss: 1.05; acc: 0.72
Batch: 480; loss: 0.97; acc: 0.75
Batch: 500; loss: 1.06; acc: 0.62
Batch: 520; loss: 1.09; acc: 0.72
Batch: 540; loss: 1.1; acc: 0.67
Batch: 560; loss: 1.12; acc: 0.69
Batch: 580; loss: 1.0; acc: 0.75
Batch: 600; loss: 0.95; acc: 0.81
Batch: 620; loss: 1.2; acc: 0.62
Batch: 640; loss: 1.0; acc: 0.72
Batch: 660; loss: 1.19; acc: 0.69
Batch: 680; loss: 0.92; acc: 0.77
Batch: 700; loss: 0.95; acc: 0.77
Batch: 720; loss: 1.08; acc: 0.73
Batch: 740; loss: 1.09; acc: 0.7
Batch: 760; loss: 1.11; acc: 0.72
Batch: 780; loss: 1.01; acc: 0.73
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.0001818749587982893
0.0001733315730234608
Batch: 0; loss: 1.02; acc: 0.77
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 1.07; acc: 0.78
Batch: 100; loss: 1.17; acc: 0.62
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 0.85; acc: 0.83
Val Epoch over. val_loss: 1.029633802593134; val_accuracy: 0.7373606687898089 

The current subspace-distance is: 0.0001733315730234608 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_6_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.1089789320611354

The number of parameters is: 267194

The number of individual parameters is:

25
450
25
25
38
43700
38
38
75
131100
75
75
64
86400
64
64
4096
64
640
10
64
64

nonzero elements in E: 53438795
elements in E: 53438800
fraction nonzero: 0.9999999064350248
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.37; acc: 0.12
Batch: 20; loss: 2.15; acc: 0.27
Batch: 40; loss: 2.09; acc: 0.28
Batch: 60; loss: 2.0; acc: 0.44
Batch: 80; loss: 1.79; acc: 0.53
Batch: 100; loss: 1.69; acc: 0.58
Batch: 120; loss: 1.76; acc: 0.53
Batch: 140; loss: 1.68; acc: 0.62
Batch: 160; loss: 1.62; acc: 0.67
Batch: 180; loss: 1.7; acc: 0.61
Batch: 200; loss: 1.57; acc: 0.66
Batch: 220; loss: 1.66; acc: 0.66
Batch: 240; loss: 1.57; acc: 0.64
Batch: 260; loss: 1.62; acc: 0.59
Batch: 280; loss: 1.55; acc: 0.61
Batch: 300; loss: 1.53; acc: 0.64
Batch: 320; loss: 1.55; acc: 0.67
Batch: 340; loss: 1.59; acc: 0.7
Batch: 360; loss: 1.58; acc: 0.66
Batch: 380; loss: 1.56; acc: 0.69
Batch: 400; loss: 1.48; acc: 0.64
Batch: 420; loss: 1.41; acc: 0.78
Batch: 440; loss: 1.49; acc: 0.72
Batch: 460; loss: 1.51; acc: 0.75
Batch: 480; loss: 1.5; acc: 0.64
Batch: 500; loss: 1.45; acc: 0.69
Batch: 520; loss: 1.49; acc: 0.7
Batch: 540; loss: 1.47; acc: 0.66
Batch: 560; loss: 1.43; acc: 0.72
Batch: 580; loss: 1.46; acc: 0.67
Batch: 600; loss: 1.4; acc: 0.75
Batch: 620; loss: 1.34; acc: 0.8
Batch: 640; loss: 1.44; acc: 0.7
Batch: 660; loss: 1.46; acc: 0.67
Batch: 680; loss: 1.36; acc: 0.72
Batch: 700; loss: 1.44; acc: 0.73
Batch: 720; loss: 1.32; acc: 0.78
Batch: 740; loss: 1.36; acc: 0.73
Batch: 760; loss: 1.46; acc: 0.7
Batch: 780; loss: 1.47; acc: 0.69
Train Epoch over. train_loss: 1.58; train_accuracy: 0.64 

5.310987398843281e-05
4.75169435958378e-05
Batch: 0; loss: 1.39; acc: 0.77
Batch: 20; loss: 1.4; acc: 0.67
Batch: 40; loss: 1.1; acc: 0.88
Batch: 60; loss: 1.33; acc: 0.78
Batch: 80; loss: 1.26; acc: 0.77
Batch: 100; loss: 1.35; acc: 0.8
Batch: 120; loss: 1.45; acc: 0.69
Batch: 140; loss: 1.25; acc: 0.81
Val Epoch over. val_loss: 1.3243172905247682; val_accuracy: 0.7595541401273885 

The current subspace-distance is: 4.75169435958378e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.73
Batch: 20; loss: 1.28; acc: 0.84
Batch: 40; loss: 1.33; acc: 0.78
Batch: 60; loss: 1.38; acc: 0.67
Batch: 80; loss: 1.33; acc: 0.78
Batch: 100; loss: 1.4; acc: 0.7
Batch: 120; loss: 1.38; acc: 0.73
Batch: 140; loss: 1.35; acc: 0.73
Batch: 160; loss: 1.41; acc: 0.67
Batch: 180; loss: 1.33; acc: 0.86
Batch: 200; loss: 1.42; acc: 0.75
Batch: 220; loss: 1.38; acc: 0.67
Batch: 240; loss: 1.25; acc: 0.78
Batch: 260; loss: 1.41; acc: 0.66
Batch: 280; loss: 1.18; acc: 0.83
Batch: 300; loss: 1.28; acc: 0.8
Batch: 320; loss: 1.23; acc: 0.78
Batch: 340; loss: 1.25; acc: 0.8
Batch: 360; loss: 1.29; acc: 0.77
Batch: 380; loss: 1.18; acc: 0.81
Batch: 400; loss: 1.27; acc: 0.73
Batch: 420; loss: 1.41; acc: 0.66
Batch: 440; loss: 1.32; acc: 0.78
Batch: 460; loss: 1.18; acc: 0.84
Batch: 480; loss: 1.25; acc: 0.8
Batch: 500; loss: 1.27; acc: 0.77
Batch: 520; loss: 1.26; acc: 0.83
Batch: 540; loss: 1.31; acc: 0.73
Batch: 560; loss: 1.18; acc: 0.81
Batch: 580; loss: 1.12; acc: 0.78
Batch: 600; loss: 1.25; acc: 0.8
Batch: 620; loss: 1.31; acc: 0.69
Batch: 640; loss: 1.22; acc: 0.75
Batch: 660; loss: 1.28; acc: 0.73
Batch: 680; loss: 1.12; acc: 0.86
Batch: 700; loss: 1.19; acc: 0.75
Batch: 720; loss: 1.17; acc: 0.86
Batch: 740; loss: 1.24; acc: 0.69
Batch: 760; loss: 1.21; acc: 0.78
Batch: 780; loss: 1.16; acc: 0.81
Train Epoch over. train_loss: 1.27; train_accuracy: 0.77 

7.320308941416442e-05
6.808739271946251e-05
Batch: 0; loss: 1.19; acc: 0.73
Batch: 20; loss: 1.22; acc: 0.72
Batch: 40; loss: 0.88; acc: 0.94
Batch: 60; loss: 1.1; acc: 0.81
Batch: 80; loss: 0.98; acc: 0.89
Batch: 100; loss: 1.14; acc: 0.83
Batch: 120; loss: 1.24; acc: 0.72
Batch: 140; loss: 1.02; acc: 0.88
Val Epoch over. val_loss: 1.1225155774195483; val_accuracy: 0.8101114649681529 

The current subspace-distance is: 6.808739271946251e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.67
Batch: 20; loss: 1.22; acc: 0.77
Batch: 40; loss: 1.18; acc: 0.75
Batch: 60; loss: 1.16; acc: 0.75
Batch: 80; loss: 1.01; acc: 0.89
Batch: 100; loss: 1.1; acc: 0.81
Batch: 120; loss: 1.14; acc: 0.8
Batch: 140; loss: 1.06; acc: 0.83
Batch: 160; loss: 1.14; acc: 0.83
Batch: 180; loss: 1.11; acc: 0.78
Batch: 200; loss: 0.98; acc: 0.92
Batch: 220; loss: 1.09; acc: 0.83
Batch: 240; loss: 1.19; acc: 0.78
Batch: 260; loss: 1.08; acc: 0.8
Batch: 280; loss: 1.05; acc: 0.83
Batch: 300; loss: 1.2; acc: 0.75
Batch: 320; loss: 1.1; acc: 0.81
Batch: 340; loss: 1.11; acc: 0.78
Batch: 360; loss: 1.02; acc: 0.83
Batch: 380; loss: 1.16; acc: 0.73
Batch: 400; loss: 1.01; acc: 0.81
Batch: 420; loss: 1.16; acc: 0.75
Batch: 440; loss: 1.15; acc: 0.75
Batch: 460; loss: 1.0; acc: 0.91
Batch: 480; loss: 1.19; acc: 0.77
Batch: 500; loss: 1.12; acc: 0.77
Batch: 520; loss: 1.01; acc: 0.84
Batch: 540; loss: 1.0; acc: 0.8
Batch: 560; loss: 1.13; acc: 0.72
Batch: 580; loss: 1.05; acc: 0.88
Batch: 600; loss: 1.1; acc: 0.83
Batch: 620; loss: 1.01; acc: 0.84
Batch: 640; loss: 1.11; acc: 0.86
Batch: 660; loss: 1.06; acc: 0.8
Batch: 680; loss: 1.2; acc: 0.72
Batch: 700; loss: 1.09; acc: 0.78
Batch: 720; loss: 0.97; acc: 0.86
Batch: 740; loss: 1.02; acc: 0.81
Batch: 760; loss: 1.07; acc: 0.78
Batch: 780; loss: 1.04; acc: 0.81
Train Epoch over. train_loss: 1.11; train_accuracy: 0.8 

8.987103501567617e-05
8.57623090269044e-05
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 1.13; acc: 0.77
Batch: 40; loss: 0.76; acc: 0.94
Batch: 60; loss: 0.97; acc: 0.83
Batch: 80; loss: 0.86; acc: 0.91
Batch: 100; loss: 1.0; acc: 0.86
Batch: 120; loss: 1.14; acc: 0.73
Batch: 140; loss: 0.87; acc: 0.91
Val Epoch over. val_loss: 0.9986280833080317; val_accuracy: 0.8326035031847133 

The current subspace-distance is: 8.57623090269044e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.83
Batch: 20; loss: 1.03; acc: 0.83
Batch: 40; loss: 1.19; acc: 0.66
Batch: 60; loss: 0.97; acc: 0.89
Batch: 80; loss: 0.98; acc: 0.83
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.08; acc: 0.84
Batch: 140; loss: 0.99; acc: 0.86
Batch: 160; loss: 1.1; acc: 0.78
Batch: 180; loss: 1.09; acc: 0.78
Batch: 200; loss: 1.2; acc: 0.67
Batch: 220; loss: 0.96; acc: 0.81
Batch: 240; loss: 0.91; acc: 0.89
Batch: 260; loss: 0.88; acc: 0.95
Batch: 280; loss: 0.98; acc: 0.86
Batch: 300; loss: 0.96; acc: 0.89
Batch: 320; loss: 1.0; acc: 0.88
Batch: 340; loss: 1.04; acc: 0.88
Batch: 360; loss: 1.0; acc: 0.81
Batch: 380; loss: 1.0; acc: 0.83
Batch: 400; loss: 1.07; acc: 0.83
Batch: 420; loss: 0.88; acc: 0.91
Batch: 440; loss: 0.94; acc: 0.91
Batch: 460; loss: 0.95; acc: 0.88
Batch: 480; loss: 1.04; acc: 0.83
Batch: 500; loss: 1.04; acc: 0.78
Batch: 520; loss: 1.0; acc: 0.88
Batch: 540; loss: 0.97; acc: 0.86
Batch: 560; loss: 1.0; acc: 0.81
Batch: 580; loss: 1.0; acc: 0.77
Batch: 600; loss: 1.03; acc: 0.83
Batch: 620; loss: 0.91; acc: 0.88
Batch: 640; loss: 0.86; acc: 0.89
Batch: 660; loss: 0.84; acc: 0.91
Batch: 680; loss: 0.99; acc: 0.86
Batch: 700; loss: 0.91; acc: 0.88
Batch: 720; loss: 0.99; acc: 0.83
Batch: 740; loss: 0.92; acc: 0.84
Batch: 760; loss: 0.91; acc: 0.84
Batch: 780; loss: 0.83; acc: 0.84
Train Epoch over. train_loss: 1.01; train_accuracy: 0.82 

0.00010469391418155283
9.899077122099698e-05
Batch: 0; loss: 0.91; acc: 0.89
Batch: 20; loss: 1.08; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.92
Batch: 60; loss: 0.87; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.92
Batch: 100; loss: 0.91; acc: 0.88
Batch: 120; loss: 1.06; acc: 0.73
Batch: 140; loss: 0.79; acc: 0.92
Val Epoch over. val_loss: 0.9082728350997731; val_accuracy: 0.836484872611465 

The current subspace-distance is: 9.899077122099698e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 1.03; acc: 0.78
Batch: 40; loss: 1.08; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.82; acc: 0.88
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 0.88; acc: 0.86
Batch: 140; loss: 0.87; acc: 0.89
Batch: 160; loss: 0.94; acc: 0.88
Batch: 180; loss: 1.1; acc: 0.81
Batch: 200; loss: 1.01; acc: 0.78
Batch: 220; loss: 0.95; acc: 0.81
Batch: 240; loss: 0.93; acc: 0.86
Batch: 260; loss: 0.92; acc: 0.86
Batch: 280; loss: 1.01; acc: 0.84
Batch: 300; loss: 0.91; acc: 0.78
Batch: 320; loss: 0.81; acc: 0.92
Batch: 340; loss: 0.96; acc: 0.81
Batch: 360; loss: 1.06; acc: 0.77
Batch: 380; loss: 0.91; acc: 0.88
Batch: 400; loss: 0.95; acc: 0.78
Batch: 420; loss: 0.96; acc: 0.84
Batch: 440; loss: 1.03; acc: 0.78
Batch: 460; loss: 0.87; acc: 0.88
Batch: 480; loss: 0.93; acc: 0.84
Batch: 500; loss: 0.93; acc: 0.81
Batch: 520; loss: 0.91; acc: 0.88
Batch: 540; loss: 1.0; acc: 0.81
Batch: 560; loss: 0.9; acc: 0.84
Batch: 580; loss: 0.85; acc: 0.88
Batch: 600; loss: 0.89; acc: 0.83
Batch: 620; loss: 0.8; acc: 0.88
Batch: 640; loss: 0.88; acc: 0.84
Batch: 660; loss: 0.96; acc: 0.78
Batch: 680; loss: 0.84; acc: 0.89
Batch: 700; loss: 0.86; acc: 0.78
Batch: 720; loss: 0.92; acc: 0.84
Batch: 740; loss: 0.86; acc: 0.86
Batch: 760; loss: 0.88; acc: 0.86
Batch: 780; loss: 0.89; acc: 0.78
Train Epoch over. train_loss: 0.94; train_accuracy: 0.82 

0.00011894051567651331
0.00011299286416033283
Batch: 0; loss: 0.86; acc: 0.89
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 0.6; acc: 0.95
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.92
Batch: 100; loss: 0.89; acc: 0.84
Batch: 120; loss: 1.0; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.94
Val Epoch over. val_loss: 0.854432093869349; val_accuracy: 0.8433519108280255 

The current subspace-distance is: 0.00011299286416033283 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.88
Batch: 20; loss: 0.99; acc: 0.77
Batch: 40; loss: 0.81; acc: 0.91
Batch: 60; loss: 0.83; acc: 0.88
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.83
Batch: 140; loss: 0.91; acc: 0.83
Batch: 160; loss: 0.78; acc: 0.89
Batch: 180; loss: 0.87; acc: 0.84
Batch: 200; loss: 0.89; acc: 0.81
Batch: 220; loss: 0.94; acc: 0.77
Batch: 240; loss: 0.89; acc: 0.8
Batch: 260; loss: 0.87; acc: 0.78
Batch: 280; loss: 0.96; acc: 0.81
Batch: 300; loss: 0.84; acc: 0.84
Batch: 320; loss: 0.79; acc: 0.86
Batch: 340; loss: 0.83; acc: 0.84
Batch: 360; loss: 0.87; acc: 0.83
Batch: 380; loss: 0.87; acc: 0.84
Batch: 400; loss: 0.89; acc: 0.83
Batch: 420; loss: 0.95; acc: 0.78
Batch: 440; loss: 1.01; acc: 0.73
Batch: 460; loss: 0.83; acc: 0.81
Batch: 480; loss: 0.9; acc: 0.83
Batch: 500; loss: 0.89; acc: 0.83
Batch: 520; loss: 0.89; acc: 0.81
Batch: 540; loss: 0.84; acc: 0.84
Batch: 560; loss: 0.98; acc: 0.77
Batch: 580; loss: 1.01; acc: 0.77
Batch: 600; loss: 0.75; acc: 0.88
Batch: 620; loss: 0.76; acc: 0.89
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.78; acc: 0.86
Batch: 680; loss: 1.06; acc: 0.77
Batch: 700; loss: 0.97; acc: 0.73
Batch: 720; loss: 0.69; acc: 0.92
Batch: 740; loss: 0.83; acc: 0.89
Batch: 760; loss: 0.89; acc: 0.89
Batch: 780; loss: 0.88; acc: 0.75
Train Epoch over. train_loss: 0.89; train_accuracy: 0.83 

0.00013001727347727865
0.00012364967551548034
Batch: 0; loss: 0.82; acc: 0.86
Batch: 20; loss: 1.07; acc: 0.73
Batch: 40; loss: 0.54; acc: 0.95
Batch: 60; loss: 0.83; acc: 0.81
Batch: 80; loss: 0.67; acc: 0.91
Batch: 100; loss: 0.86; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 0.68; acc: 0.97
Val Epoch over. val_loss: 0.8065907704602381; val_accuracy: 0.8485270700636943 

The current subspace-distance is: 0.00012364967551548034 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.89
Batch: 40; loss: 0.92; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 0.9; acc: 0.86
Batch: 100; loss: 0.84; acc: 0.84
Batch: 120; loss: 0.87; acc: 0.81
Batch: 140; loss: 0.97; acc: 0.77
Batch: 160; loss: 0.78; acc: 0.84
Batch: 180; loss: 0.95; acc: 0.72
Batch: 200; loss: 0.72; acc: 0.88
Batch: 220; loss: 0.84; acc: 0.8
Batch: 240; loss: 0.89; acc: 0.88
Batch: 260; loss: 0.86; acc: 0.84
Batch: 280; loss: 0.86; acc: 0.8
Batch: 300; loss: 0.96; acc: 0.8
Batch: 320; loss: 0.88; acc: 0.78
Batch: 340; loss: 0.73; acc: 0.91
Batch: 360; loss: 0.71; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.88
Batch: 400; loss: 0.93; acc: 0.83
Batch: 420; loss: 0.91; acc: 0.78
Batch: 440; loss: 0.92; acc: 0.73
Batch: 460; loss: 0.82; acc: 0.8
Batch: 480; loss: 0.83; acc: 0.83
Batch: 500; loss: 0.85; acc: 0.84
Batch: 520; loss: 0.8; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.84
Batch: 560; loss: 0.89; acc: 0.86
Batch: 580; loss: 1.04; acc: 0.7
Batch: 600; loss: 0.88; acc: 0.83
Batch: 620; loss: 0.81; acc: 0.88
Batch: 640; loss: 0.83; acc: 0.84
Batch: 660; loss: 0.73; acc: 0.89
Batch: 680; loss: 0.83; acc: 0.83
Batch: 700; loss: 0.76; acc: 0.86
Batch: 720; loss: 0.94; acc: 0.8
Batch: 740; loss: 0.74; acc: 0.84
Batch: 760; loss: 0.87; acc: 0.81
Batch: 780; loss: 0.84; acc: 0.81
Train Epoch over. train_loss: 0.85; train_accuracy: 0.83 

0.00013992081221658736
0.0001328364305663854
Batch: 0; loss: 0.78; acc: 0.88
Batch: 20; loss: 1.05; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.91
Batch: 100; loss: 0.82; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.8
Batch: 140; loss: 0.64; acc: 0.92
Val Epoch over. val_loss: 0.7782518994656338; val_accuracy: 0.8477308917197452 

The current subspace-distance is: 0.0001328364305663854 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.88
Batch: 40; loss: 0.75; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.8
Batch: 80; loss: 0.88; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.88
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 0.78; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.81
Batch: 180; loss: 0.83; acc: 0.84
Batch: 200; loss: 0.86; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.91
Batch: 240; loss: 0.93; acc: 0.73
Batch: 260; loss: 0.92; acc: 0.77
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.91; acc: 0.8
Batch: 320; loss: 0.82; acc: 0.81
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.85; acc: 0.86
Batch: 380; loss: 0.79; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.81
Batch: 420; loss: 0.8; acc: 0.86
Batch: 440; loss: 0.86; acc: 0.86
Batch: 460; loss: 0.95; acc: 0.66
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.78; acc: 0.86
Batch: 520; loss: 0.9; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.86; acc: 0.84
Batch: 580; loss: 0.97; acc: 0.84
Batch: 600; loss: 0.86; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.83
Batch: 640; loss: 0.78; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.84
Batch: 680; loss: 0.84; acc: 0.86
Batch: 700; loss: 0.79; acc: 0.88
Batch: 720; loss: 0.74; acc: 0.89
Batch: 740; loss: 1.02; acc: 0.72
Batch: 760; loss: 0.92; acc: 0.78
Batch: 780; loss: 0.82; acc: 0.84
Train Epoch over. train_loss: 0.82; train_accuracy: 0.83 

0.000147698781802319
0.00014229814405553043
Batch: 0; loss: 0.74; acc: 0.91
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 0.46; acc: 0.95
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.92
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.6; acc: 0.92
Val Epoch over. val_loss: 0.7425788944693887; val_accuracy: 0.8535031847133758 

The current subspace-distance is: 0.00014229814405553043 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.89
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.83; acc: 0.86
Batch: 60; loss: 0.81; acc: 0.86
Batch: 80; loss: 0.81; acc: 0.81
Batch: 100; loss: 0.84; acc: 0.8
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 0.83; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.81
Batch: 180; loss: 0.84; acc: 0.8
Batch: 200; loss: 0.87; acc: 0.78
Batch: 220; loss: 0.71; acc: 0.88
Batch: 240; loss: 0.71; acc: 0.89
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.91; acc: 0.77
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.92; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.91
Batch: 380; loss: 0.81; acc: 0.78
Batch: 400; loss: 0.92; acc: 0.7
Batch: 420; loss: 0.84; acc: 0.8
Batch: 440; loss: 0.74; acc: 0.86
Batch: 460; loss: 0.77; acc: 0.84
Batch: 480; loss: 0.72; acc: 0.88
Batch: 500; loss: 0.88; acc: 0.78
Batch: 520; loss: 0.8; acc: 0.88
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.89
Batch: 580; loss: 0.82; acc: 0.78
Batch: 600; loss: 0.84; acc: 0.86
Batch: 620; loss: 0.74; acc: 0.84
Batch: 640; loss: 0.7; acc: 0.89
Batch: 660; loss: 0.71; acc: 0.86
Batch: 680; loss: 0.82; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.95
Batch: 720; loss: 0.8; acc: 0.83
Batch: 740; loss: 0.8; acc: 0.83
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.84; acc: 0.81
Train Epoch over. train_loss: 0.8; train_accuracy: 0.83 

0.00015588580572512
0.00015083410835359246
Batch: 0; loss: 0.72; acc: 0.89
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.76; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.94
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.94
Val Epoch over. val_loss: 0.7266794169784352; val_accuracy: 0.856687898089172 

The current subspace-distance is: 0.00015083410835359246 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.91
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.9; acc: 0.73
Batch: 100; loss: 0.83; acc: 0.81
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.88
Batch: 200; loss: 0.83; acc: 0.84
Batch: 220; loss: 0.75; acc: 0.86
Batch: 240; loss: 0.94; acc: 0.73
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.94; acc: 0.81
Batch: 300; loss: 0.65; acc: 0.89
Batch: 320; loss: 0.77; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.75; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.88
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.78; acc: 0.86
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.84
Batch: 520; loss: 0.79; acc: 0.83
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.87; acc: 0.78
Batch: 580; loss: 0.96; acc: 0.72
Batch: 600; loss: 0.73; acc: 0.88
Batch: 620; loss: 0.99; acc: 0.77
Batch: 640; loss: 0.96; acc: 0.77
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.79; acc: 0.84
Batch: 700; loss: 0.82; acc: 0.81
Batch: 720; loss: 0.77; acc: 0.83
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.85; acc: 0.83
Batch: 780; loss: 0.84; acc: 0.8
Train Epoch over. train_loss: 0.77; train_accuracy: 0.83 

0.0001630313927307725
0.00015592866111546755
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.95
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.55; acc: 0.94
Val Epoch over. val_loss: 0.7009150216913527; val_accuracy: 0.8542993630573248 

The current subspace-distance is: 0.00015592866111546755 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.91
Batch: 40; loss: 0.95; acc: 0.72
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 0.79; acc: 0.73
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.63; acc: 0.89
Batch: 160; loss: 0.84; acc: 0.8
Batch: 180; loss: 0.84; acc: 0.75
Batch: 200; loss: 0.84; acc: 0.77
Batch: 220; loss: 0.7; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.8; acc: 0.86
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.78; acc: 0.75
Batch: 340; loss: 0.84; acc: 0.81
Batch: 360; loss: 0.82; acc: 0.8
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.78
Batch: 440; loss: 0.89; acc: 0.8
Batch: 460; loss: 0.73; acc: 0.88
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.88
Batch: 520; loss: 0.82; acc: 0.81
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.84
Batch: 580; loss: 0.81; acc: 0.8
Batch: 600; loss: 0.86; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.88
Batch: 640; loss: 0.75; acc: 0.86
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.86
Batch: 700; loss: 0.82; acc: 0.8
Batch: 720; loss: 0.85; acc: 0.78
Batch: 740; loss: 0.88; acc: 0.8
Batch: 760; loss: 0.9; acc: 0.72
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.83 

0.0001658413530094549
0.00015889164933469146
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.94
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.92
Val Epoch over. val_loss: 0.7086936907403788; val_accuracy: 0.8545979299363057 

The current subspace-distance is: 0.00015889164933469146 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.78
Batch: 60; loss: 0.72; acc: 0.91
Batch: 80; loss: 0.72; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.73; acc: 0.91
Batch: 160; loss: 0.92; acc: 0.73
Batch: 180; loss: 0.78; acc: 0.8
Batch: 200; loss: 0.61; acc: 0.89
Batch: 220; loss: 0.78; acc: 0.84
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.68; acc: 0.92
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.9; acc: 0.8
Batch: 320; loss: 0.85; acc: 0.83
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.88; acc: 0.75
Batch: 380; loss: 0.86; acc: 0.8
Batch: 400; loss: 0.69; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.88
Batch: 440; loss: 0.82; acc: 0.78
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.85; acc: 0.8
Batch: 500; loss: 0.62; acc: 0.94
Batch: 520; loss: 0.67; acc: 0.86
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.84; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.88
Batch: 640; loss: 0.78; acc: 0.81
Batch: 660; loss: 0.64; acc: 0.91
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.79; acc: 0.83
Batch: 760; loss: 0.84; acc: 0.8
Batch: 780; loss: 0.77; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.83 

0.00016805235645733774
0.0001611402549315244
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6957849332481433; val_accuracy: 0.8570859872611465 

The current subspace-distance is: 0.0001611402549315244 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 0.78; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.89; acc: 0.77
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.89
Batch: 140; loss: 0.85; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.97
Batch: 220; loss: 1.03; acc: 0.73
Batch: 240; loss: 0.83; acc: 0.84
Batch: 260; loss: 0.76; acc: 0.8
Batch: 280; loss: 0.77; acc: 0.88
Batch: 300; loss: 0.69; acc: 0.91
Batch: 320; loss: 0.7; acc: 0.86
Batch: 340; loss: 0.85; acc: 0.8
Batch: 360; loss: 0.75; acc: 0.86
Batch: 380; loss: 0.79; acc: 0.8
Batch: 400; loss: 0.73; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.94
Batch: 440; loss: 0.67; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.74; acc: 0.83
Batch: 540; loss: 0.6; acc: 0.91
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 0.69; acc: 0.89
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.87; acc: 0.78
Batch: 640; loss: 0.76; acc: 0.75
Batch: 660; loss: 0.71; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.95
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.75; acc: 0.84
Batch: 780; loss: 0.73; acc: 0.83
Train Epoch over. train_loss: 0.75; train_accuracy: 0.83 

0.00016903529467526823
0.00016262968711089343
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.94
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6836288971885754; val_accuracy: 0.8577826433121019 

The current subspace-distance is: 0.00016262968711089343 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.81; acc: 0.8
Batch: 160; loss: 0.56; acc: 0.94
Batch: 180; loss: 0.83; acc: 0.81
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.83
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.72; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.7; acc: 0.84
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 0.65; acc: 0.88
Batch: 360; loss: 0.79; acc: 0.73
Batch: 380; loss: 0.85; acc: 0.78
Batch: 400; loss: 0.82; acc: 0.8
Batch: 420; loss: 0.67; acc: 0.88
Batch: 440; loss: 0.64; acc: 0.88
Batch: 460; loss: 0.7; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.84
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 0.82; acc: 0.81
Batch: 560; loss: 0.68; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.81; acc: 0.81
Batch: 640; loss: 0.82; acc: 0.78
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.85; acc: 0.77
Batch: 700; loss: 0.61; acc: 0.91
Batch: 720; loss: 0.77; acc: 0.81
Batch: 740; loss: 0.81; acc: 0.78
Batch: 760; loss: 0.74; acc: 0.84
Batch: 780; loss: 0.87; acc: 0.73
Train Epoch over. train_loss: 0.74; train_accuracy: 0.84 

0.0001729365612845868
0.00016730473726056516
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.81
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6754174340682425; val_accuracy: 0.8595740445859873 

The current subspace-distance is: 0.00016730473726056516 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.81; acc: 0.78
Batch: 200; loss: 0.57; acc: 0.92
Batch: 220; loss: 0.66; acc: 0.88
Batch: 240; loss: 0.96; acc: 0.75
Batch: 260; loss: 0.67; acc: 0.88
Batch: 280; loss: 0.73; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.65; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.92
Batch: 360; loss: 0.79; acc: 0.78
Batch: 380; loss: 0.7; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.83
Batch: 420; loss: 0.72; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.86
Batch: 460; loss: 0.82; acc: 0.78
Batch: 480; loss: 0.88; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.81
Batch: 520; loss: 0.79; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.89
Batch: 620; loss: 0.75; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 0.93; acc: 0.75
Batch: 720; loss: 0.74; acc: 0.83
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.88
Batch: 780; loss: 0.75; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.0001752436364768073
0.0001681025605648756
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.97; acc: 0.72
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6728835322295025; val_accuracy: 0.8600716560509554 

The current subspace-distance is: 0.0001681025605648756 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 0.79; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.91; acc: 0.78
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.62; acc: 0.91
Batch: 200; loss: 0.76; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.88
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.7; acc: 0.88
Batch: 280; loss: 0.83; acc: 0.81
Batch: 300; loss: 0.73; acc: 0.88
Batch: 320; loss: 0.66; acc: 0.88
Batch: 340; loss: 0.8; acc: 0.8
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.72; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.89
Batch: 440; loss: 0.82; acc: 0.78
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.81
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.89
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.81; acc: 0.8
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.66; acc: 0.89
Batch: 620; loss: 0.8; acc: 0.77
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.91
Batch: 680; loss: 0.76; acc: 0.83
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.91
Batch: 740; loss: 0.72; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.73; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.84 

0.0001765208871802315
0.00017137904069386423
Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6658449309646703; val_accuracy: 0.8614649681528662 

The current subspace-distance is: 0.00017137904069386423 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.93; acc: 0.7
Batch: 100; loss: 0.65; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.79; acc: 0.75
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.89; acc: 0.75
Batch: 240; loss: 0.68; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.73; acc: 0.77
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.88
Batch: 400; loss: 0.67; acc: 0.86
Batch: 420; loss: 0.75; acc: 0.83
Batch: 440; loss: 0.88; acc: 0.77
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.91
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.88
Batch: 540; loss: 0.74; acc: 0.84
Batch: 560; loss: 0.72; acc: 0.88
Batch: 580; loss: 0.86; acc: 0.73
Batch: 600; loss: 0.77; acc: 0.84
Batch: 620; loss: 0.89; acc: 0.78
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.83
Batch: 680; loss: 0.96; acc: 0.67
Batch: 700; loss: 0.73; acc: 0.86
Batch: 720; loss: 0.71; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.81
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.84 

0.0001784345949999988
0.00017142691649496555
Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6667777918706275; val_accuracy: 0.8608678343949044 

The current subspace-distance is: 0.00017142691649496555 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.94
Batch: 40; loss: 0.73; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.91
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.78; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.77; acc: 0.78
Batch: 280; loss: 0.84; acc: 0.8
Batch: 300; loss: 0.61; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.92
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.71; acc: 0.86
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.72; acc: 0.84
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.74; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.84
Batch: 560; loss: 0.92; acc: 0.72
Batch: 580; loss: 0.58; acc: 0.91
Batch: 600; loss: 0.86; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.84
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.68; acc: 0.86
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.88; acc: 0.8
Batch: 760; loss: 0.75; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.84 

0.00017806743562687188
0.00017007686255965382
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.92
Val Epoch over. val_loss: 0.6524101588756416; val_accuracy: 0.8616640127388535 

The current subspace-distance is: 0.00017007686255965382 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.76; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.76; acc: 0.78
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.86
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.78; acc: 0.81
Batch: 320; loss: 0.78; acc: 0.8
Batch: 340; loss: 0.83; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.86
Batch: 380; loss: 0.86; acc: 0.77
Batch: 400; loss: 0.73; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.91
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.91; acc: 0.73
Batch: 500; loss: 0.61; acc: 0.92
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.87; acc: 0.78
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.92
Batch: 720; loss: 0.63; acc: 0.89
Batch: 740; loss: 0.97; acc: 0.69
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.92
Train Epoch over. train_loss: 0.71; train_accuracy: 0.84 

0.00018449017079547048
0.00017455763008911163
Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6560433658824605; val_accuracy: 0.8614649681528662 

The current subspace-distance is: 0.00017455763008911163 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.89
Batch: 160; loss: 0.79; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.88
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.75; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.75
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.73; acc: 0.78
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.79; acc: 0.78
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.66; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.73; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.89
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.88
Batch: 660; loss: 0.78; acc: 0.81
Batch: 680; loss: 0.67; acc: 0.89
Batch: 700; loss: 0.77; acc: 0.73
Batch: 720; loss: 0.68; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.94
Batch: 760; loss: 0.65; acc: 0.91
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.84 

0.00018432088836561888
0.00017599570855963975
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6464114041085456; val_accuracy: 0.8623606687898089 

The current subspace-distance is: 0.00017599570855963975 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.81; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.73; acc: 0.84
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.8; acc: 0.81
Batch: 200; loss: 0.72; acc: 0.81
Batch: 220; loss: 0.81; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.89
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.78; acc: 0.8
Batch: 480; loss: 0.64; acc: 0.92
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.85; acc: 0.72
Batch: 580; loss: 0.74; acc: 0.83
Batch: 600; loss: 0.71; acc: 0.84
Batch: 620; loss: 0.69; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.82; acc: 0.8
Batch: 700; loss: 0.72; acc: 0.77
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.85; acc: 0.73
Batch: 780; loss: 0.84; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00018368763267062604
0.00017730203398969024
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6526445091530017; val_accuracy: 0.8614649681528662 

The current subspace-distance is: 0.00017730203398969024 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.88
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.74; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.88
Batch: 180; loss: 0.73; acc: 0.77
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.95
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.92; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.91
Batch: 420; loss: 0.77; acc: 0.81
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.7; acc: 0.77
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.81
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.73; acc: 0.78
Batch: 700; loss: 0.63; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.67; acc: 0.78
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.76; acc: 0.73
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00018674870079848915
0.0001785622735042125
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.647789283542876; val_accuracy: 0.8606687898089171 

The current subspace-distance is: 0.0001785622735042125 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.91
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.7; acc: 0.8
Batch: 160; loss: 0.73; acc: 0.81
Batch: 180; loss: 0.83; acc: 0.78
Batch: 200; loss: 0.53; acc: 0.92
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.77
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.74; acc: 0.84
Batch: 300; loss: 0.83; acc: 0.78
Batch: 320; loss: 0.86; acc: 0.72
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.94; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.89
Batch: 440; loss: 0.7; acc: 0.83
Batch: 460; loss: 0.77; acc: 0.83
Batch: 480; loss: 0.78; acc: 0.83
Batch: 500; loss: 0.75; acc: 0.84
Batch: 520; loss: 0.91; acc: 0.75
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.73; acc: 0.8
Batch: 720; loss: 0.57; acc: 0.91
Batch: 740; loss: 0.69; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.92
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.0001873846777016297
0.00018019316485151649
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6463220387127748; val_accuracy: 0.861265923566879 

The current subspace-distance is: 0.00018019316485151649 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.86; acc: 0.78
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.71; acc: 0.84
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.89
Batch: 200; loss: 0.78; acc: 0.81
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.86
Batch: 260; loss: 0.75; acc: 0.81
Batch: 280; loss: 0.83; acc: 0.77
Batch: 300; loss: 0.64; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.81; acc: 0.75
Batch: 460; loss: 0.7; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.91
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.91
Batch: 620; loss: 0.64; acc: 0.89
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.88
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.6; acc: 0.89
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.75; acc: 0.78
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.78; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.000186012577614747
0.00017863135144580156
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6432618635475256; val_accuracy: 0.8608678343949044 

The current subspace-distance is: 0.00017863135144580156 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.61; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.81
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.81; acc: 0.81
Batch: 380; loss: 0.68; acc: 0.89
Batch: 400; loss: 0.67; acc: 0.8
Batch: 420; loss: 0.67; acc: 0.86
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.91
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.88; acc: 0.78
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.86
Batch: 640; loss: 0.71; acc: 0.86
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.67; acc: 0.84
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.79; acc: 0.8
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00018972496036440134
0.00018176362209487706
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6430982818269426; val_accuracy: 0.8622611464968153 

The current subspace-distance is: 0.00018176362209487706 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.94
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.62; acc: 0.88
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.75; acc: 0.83
Batch: 280; loss: 0.66; acc: 0.88
Batch: 300; loss: 0.77; acc: 0.83
Batch: 320; loss: 0.7; acc: 0.89
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.82; acc: 0.78
Batch: 380; loss: 0.79; acc: 0.81
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.76; acc: 0.73
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.92
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.67; acc: 0.91
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.89
Batch: 720; loss: 0.72; acc: 0.89
Batch: 740; loss: 0.79; acc: 0.81
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.00018866553728003055
0.0001823836355470121
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6369741067385218; val_accuracy: 0.8613654458598726 

The current subspace-distance is: 0.0001823836355470121 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.8; acc: 0.78
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.98; acc: 0.69
Batch: 160; loss: 0.71; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.79; acc: 0.8
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 0.82; acc: 0.77
Batch: 340; loss: 0.69; acc: 0.88
Batch: 360; loss: 0.74; acc: 0.89
Batch: 380; loss: 0.77; acc: 0.8
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.69; acc: 0.86
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.61; acc: 0.91
Batch: 540; loss: 0.78; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.78; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.68; acc: 0.81
Batch: 640; loss: 0.81; acc: 0.8
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.6; acc: 0.89
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.00018701079534366727
0.000181284049176611
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6448052470471449; val_accuracy: 0.8605692675159236 

The current subspace-distance is: 0.000181284049176611 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.73; acc: 0.77
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.91
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.84; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.82; acc: 0.8
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.84; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.89
Batch: 580; loss: 0.79; acc: 0.78
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.78
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.88
Batch: 700; loss: 0.64; acc: 0.88
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 0.85; acc: 0.73
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.00019016803707927465
0.0001829661923693493
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6338294330675891; val_accuracy: 0.8619625796178344 

The current subspace-distance is: 0.0001829661923693493 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.73; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.58; acc: 0.92
Batch: 320; loss: 0.79; acc: 0.77
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.8; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.77; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.66; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.85; acc: 0.77
Batch: 500; loss: 0.62; acc: 0.89
Batch: 520; loss: 0.8; acc: 0.78
Batch: 540; loss: 0.74; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.82; acc: 0.73
Batch: 700; loss: 0.77; acc: 0.8
Batch: 720; loss: 0.73; acc: 0.86
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.000191161670954898
0.00018448282207828015
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6365556462555174; val_accuracy: 0.8615644904458599 

The current subspace-distance is: 0.00018448282207828015 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.76; acc: 0.86
Batch: 80; loss: 0.78; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.8; acc: 0.8
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.81
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.61; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.81; acc: 0.73
Batch: 280; loss: 0.89; acc: 0.72
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.91
Batch: 380; loss: 0.86; acc: 0.78
Batch: 400; loss: 0.78; acc: 0.81
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.86
Batch: 460; loss: 0.71; acc: 0.84
Batch: 480; loss: 0.65; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.84
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.74; acc: 0.8
Batch: 680; loss: 0.72; acc: 0.78
Batch: 700; loss: 0.79; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.86
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.61; acc: 0.92
Batch: 780; loss: 0.72; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.0001903754164231941
0.00018165248911827803
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6287857654747689; val_accuracy: 0.8596735668789809 

The current subspace-distance is: 0.00018165248911827803 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_6_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.1089789320611354

The number of parameters is: 267194

The number of individual parameters is:

25
450
25
25
38
43700
38
38
75
131100
75
75
64
86400
64
64
4096
64
640
10
64
64

nonzero elements in E: 80158191
elements in E: 80158200
fraction nonzero: 0.9999998877220296
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.46; acc: 0.08
Batch: 20; loss: 2.09; acc: 0.27
Batch: 40; loss: 1.91; acc: 0.41
Batch: 60; loss: 1.86; acc: 0.48
Batch: 80; loss: 1.79; acc: 0.53
Batch: 100; loss: 1.6; acc: 0.64
Batch: 120; loss: 1.63; acc: 0.69
Batch: 140; loss: 1.61; acc: 0.59
Batch: 160; loss: 1.57; acc: 0.72
Batch: 180; loss: 1.56; acc: 0.64
Batch: 200; loss: 1.54; acc: 0.61
Batch: 220; loss: 1.5; acc: 0.72
Batch: 240; loss: 1.51; acc: 0.62
Batch: 260; loss: 1.36; acc: 0.84
Batch: 280; loss: 1.39; acc: 0.78
Batch: 300; loss: 1.36; acc: 0.78
Batch: 320; loss: 1.36; acc: 0.69
Batch: 340; loss: 1.42; acc: 0.7
Batch: 360; loss: 1.45; acc: 0.67
Batch: 380; loss: 1.27; acc: 0.75
Batch: 400; loss: 1.45; acc: 0.66
Batch: 420; loss: 1.2; acc: 0.81
Batch: 440; loss: 1.37; acc: 0.69
Batch: 460; loss: 1.29; acc: 0.73
Batch: 480; loss: 1.4; acc: 0.67
Batch: 500; loss: 1.22; acc: 0.8
Batch: 520; loss: 1.24; acc: 0.77
Batch: 540; loss: 1.39; acc: 0.72
Batch: 560; loss: 1.19; acc: 0.86
Batch: 580; loss: 1.28; acc: 0.81
Batch: 600; loss: 1.27; acc: 0.77
Batch: 620; loss: 1.29; acc: 0.81
Batch: 640; loss: 1.12; acc: 0.84
Batch: 660; loss: 1.25; acc: 0.81
Batch: 680; loss: 1.19; acc: 0.77
Batch: 700; loss: 1.19; acc: 0.78
Batch: 720; loss: 1.06; acc: 0.86
Batch: 740; loss: 1.18; acc: 0.81
Batch: 760; loss: 1.17; acc: 0.81
Batch: 780; loss: 1.24; acc: 0.8
Train Epoch over. train_loss: 1.42; train_accuracy: 0.69 

6.068640868761577e-05
5.586758561548777e-05
Batch: 0; loss: 1.23; acc: 0.73
Batch: 20; loss: 1.34; acc: 0.67
Batch: 40; loss: 0.88; acc: 0.94
Batch: 60; loss: 1.08; acc: 0.83
Batch: 80; loss: 1.0; acc: 0.88
Batch: 100; loss: 1.12; acc: 0.84
Batch: 120; loss: 1.32; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.86
Val Epoch over. val_loss: 1.1248266617203975; val_accuracy: 0.8104100318471338 

The current subspace-distance is: 5.586758561548777e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.75
Batch: 40; loss: 1.19; acc: 0.75
Batch: 60; loss: 1.16; acc: 0.81
Batch: 80; loss: 1.06; acc: 0.86
Batch: 100; loss: 1.08; acc: 0.88
Batch: 120; loss: 1.11; acc: 0.83
Batch: 140; loss: 1.2; acc: 0.75
Batch: 160; loss: 1.13; acc: 0.7
Batch: 180; loss: 1.1; acc: 0.8
Batch: 200; loss: 0.97; acc: 0.89
Batch: 220; loss: 1.01; acc: 0.89
Batch: 240; loss: 1.07; acc: 0.7
Batch: 260; loss: 1.09; acc: 0.84
Batch: 280; loss: 1.2; acc: 0.77
Batch: 300; loss: 1.14; acc: 0.8
Batch: 320; loss: 1.0; acc: 0.86
Batch: 340; loss: 1.06; acc: 0.78
Batch: 360; loss: 1.07; acc: 0.8
Batch: 380; loss: 0.87; acc: 0.92
Batch: 400; loss: 1.03; acc: 0.8
Batch: 420; loss: 0.94; acc: 0.84
Batch: 440; loss: 1.06; acc: 0.84
Batch: 460; loss: 1.03; acc: 0.77
Batch: 480; loss: 0.95; acc: 0.88
Batch: 500; loss: 0.98; acc: 0.86
Batch: 520; loss: 0.98; acc: 0.89
Batch: 540; loss: 1.12; acc: 0.77
Batch: 560; loss: 1.17; acc: 0.7
Batch: 580; loss: 1.13; acc: 0.78
Batch: 600; loss: 0.92; acc: 0.84
Batch: 620; loss: 0.97; acc: 0.83
Batch: 640; loss: 0.95; acc: 0.8
Batch: 660; loss: 0.93; acc: 0.88
Batch: 680; loss: 1.09; acc: 0.8
Batch: 700; loss: 1.09; acc: 0.75
Batch: 720; loss: 0.89; acc: 0.89
Batch: 740; loss: 1.0; acc: 0.83
Batch: 760; loss: 0.92; acc: 0.84
Batch: 780; loss: 1.08; acc: 0.84
Train Epoch over. train_loss: 1.06; train_accuracy: 0.81 

8.540708222426474e-05
8.053660712903365e-05
Batch: 0; loss: 0.98; acc: 0.81
Batch: 20; loss: 1.12; acc: 0.77
Batch: 40; loss: 0.69; acc: 0.95
Batch: 60; loss: 0.93; acc: 0.83
Batch: 80; loss: 0.77; acc: 0.92
Batch: 100; loss: 0.9; acc: 0.91
Batch: 120; loss: 1.13; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.92
Val Epoch over. val_loss: 0.9176347923886244; val_accuracy: 0.847531847133758 

The current subspace-distance is: 8.053660712903365e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.83
Batch: 40; loss: 1.06; acc: 0.75
Batch: 60; loss: 0.94; acc: 0.81
Batch: 80; loss: 0.9; acc: 0.84
Batch: 100; loss: 0.89; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.89
Batch: 140; loss: 1.0; acc: 0.86
Batch: 160; loss: 0.91; acc: 0.83
Batch: 180; loss: 1.07; acc: 0.77
Batch: 200; loss: 0.87; acc: 0.88
Batch: 220; loss: 1.01; acc: 0.75
Batch: 240; loss: 0.91; acc: 0.83
Batch: 260; loss: 0.89; acc: 0.84
Batch: 280; loss: 0.95; acc: 0.84
Batch: 300; loss: 1.05; acc: 0.75
Batch: 320; loss: 0.97; acc: 0.78
Batch: 340; loss: 0.95; acc: 0.81
Batch: 360; loss: 0.93; acc: 0.81
Batch: 380; loss: 0.83; acc: 0.83
Batch: 400; loss: 0.93; acc: 0.75
Batch: 420; loss: 0.84; acc: 0.86
Batch: 440; loss: 0.91; acc: 0.81
Batch: 460; loss: 0.86; acc: 0.84
Batch: 480; loss: 0.95; acc: 0.83
Batch: 500; loss: 0.88; acc: 0.84
Batch: 520; loss: 1.03; acc: 0.78
Batch: 540; loss: 0.73; acc: 0.91
Batch: 560; loss: 0.82; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.92
Batch: 600; loss: 0.82; acc: 0.88
Batch: 620; loss: 0.85; acc: 0.88
Batch: 640; loss: 0.83; acc: 0.94
Batch: 660; loss: 0.87; acc: 0.86
Batch: 680; loss: 0.8; acc: 0.86
Batch: 700; loss: 0.97; acc: 0.8
Batch: 720; loss: 1.01; acc: 0.78
Batch: 740; loss: 0.96; acc: 0.84
Batch: 760; loss: 0.9; acc: 0.81
Batch: 780; loss: 1.1; acc: 0.7
Train Epoch over. train_loss: 0.91; train_accuracy: 0.84 

0.00010208335152128711
9.821036655921489e-05
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.95
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.92
Batch: 100; loss: 0.79; acc: 0.91
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.94
Val Epoch over. val_loss: 0.7854403743318691; val_accuracy: 0.8586783439490446 

The current subspace-distance is: 9.821036655921489e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.94
Batch: 60; loss: 0.85; acc: 0.83
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.84
Batch: 140; loss: 0.84; acc: 0.86
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 0.95; acc: 0.8
Batch: 200; loss: 0.73; acc: 0.91
Batch: 220; loss: 0.83; acc: 0.84
Batch: 240; loss: 0.73; acc: 0.88
Batch: 260; loss: 0.76; acc: 0.89
Batch: 280; loss: 0.75; acc: 0.92
Batch: 300; loss: 0.8; acc: 0.89
Batch: 320; loss: 0.95; acc: 0.77
Batch: 340; loss: 0.93; acc: 0.83
Batch: 360; loss: 0.82; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.95
Batch: 400; loss: 0.8; acc: 0.84
Batch: 420; loss: 0.77; acc: 0.86
Batch: 440; loss: 0.81; acc: 0.84
Batch: 460; loss: 0.8; acc: 0.88
Batch: 480; loss: 0.77; acc: 0.84
Batch: 500; loss: 0.88; acc: 0.77
Batch: 520; loss: 0.75; acc: 0.91
Batch: 540; loss: 0.92; acc: 0.8
Batch: 560; loss: 0.68; acc: 0.89
Batch: 580; loss: 0.8; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.89
Batch: 620; loss: 0.82; acc: 0.84
Batch: 640; loss: 0.72; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.91
Batch: 680; loss: 0.8; acc: 0.81
Batch: 700; loss: 0.88; acc: 0.77
Batch: 720; loss: 0.76; acc: 0.88
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.79; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.88
Train Epoch over. train_loss: 0.8; train_accuracy: 0.85 

0.00011773570440709591
0.00011250760144321248
Batch: 0; loss: 0.71; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.48; acc: 0.95
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.95
Val Epoch over. val_loss: 0.7006773424755995; val_accuracy: 0.8681329617834395 

The current subspace-distance is: 0.00011250760144321248 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 0.77; acc: 0.88
Batch: 80; loss: 0.8; acc: 0.89
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.92
Batch: 140; loss: 0.72; acc: 0.84
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.87; acc: 0.78
Batch: 220; loss: 0.85; acc: 0.81
Batch: 240; loss: 0.81; acc: 0.83
Batch: 260; loss: 0.78; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.74; acc: 0.88
Batch: 320; loss: 0.83; acc: 0.81
Batch: 340; loss: 0.62; acc: 0.91
Batch: 360; loss: 0.91; acc: 0.83
Batch: 380; loss: 0.84; acc: 0.84
Batch: 400; loss: 0.7; acc: 0.86
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.96; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.83
Batch: 480; loss: 0.84; acc: 0.83
Batch: 500; loss: 0.69; acc: 0.91
Batch: 520; loss: 0.95; acc: 0.72
Batch: 540; loss: 0.81; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.94
Batch: 580; loss: 0.68; acc: 0.88
Batch: 600; loss: 0.82; acc: 0.73
Batch: 620; loss: 0.71; acc: 0.92
Batch: 640; loss: 0.69; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.91
Batch: 680; loss: 0.96; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.65; acc: 0.92
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.84; acc: 0.78
Train Epoch over. train_loss: 0.73; train_accuracy: 0.85 

0.00012755875650327653
0.00012292846804484725
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.91
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.6415509573972908; val_accuracy: 0.87609474522293 

The current subspace-distance is: 0.00012292846804484725 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.92
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.95
Batch: 220; loss: 0.8; acc: 0.78
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.92
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.95
Batch: 400; loss: 0.6; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.95
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.92
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.74; acc: 0.81
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.91
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.68; train_accuracy: 0.86 

0.0001409373653586954
0.0001360068708891049
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.6022954083931674; val_accuracy: 0.8771894904458599 

The current subspace-distance is: 0.0001360068708891049 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.94
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.97
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.92
Batch: 260; loss: 0.74; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.89
Batch: 300; loss: 0.7; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.69; acc: 0.84
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.73; acc: 0.84
Batch: 400; loss: 0.76; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.62; acc: 0.89
Batch: 480; loss: 0.73; acc: 0.75
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.89
Batch: 700; loss: 0.67; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.89
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.63; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.92
Train Epoch over. train_loss: 0.63; train_accuracy: 0.87 

0.00015112747496459633
0.00014320685295388103
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.38; acc: 0.97
Val Epoch over. val_loss: 0.5554963202233527; val_accuracy: 0.8839570063694268 

The current subspace-distance is: 0.00014320685295388103 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.92
Batch: 40; loss: 0.81; acc: 0.78
Batch: 60; loss: 0.53; acc: 0.92
Batch: 80; loss: 0.64; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.95
Batch: 160; loss: 0.71; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.95
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.94
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.95
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.98
Batch: 720; loss: 0.62; acc: 0.91
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.66; acc: 0.88
Train Epoch over. train_loss: 0.6; train_accuracy: 0.87 

0.0001580643729539588
0.00015068045468069613
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5312515745876701; val_accuracy: 0.8893312101910829 

The current subspace-distance is: 0.00015068045468069613 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.91
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.88; acc: 0.73
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.92
Batch: 660; loss: 0.74; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.54; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.92
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00016708087059669197
0.00016143567336257547
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.81; acc: 0.77
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5113942423823533; val_accuracy: 0.89171974522293 

The current subspace-distance is: 0.00016143567336257547 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.73; acc: 0.78
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.95
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.66; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.94
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.0001756932761054486
0.00016882222553249449
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.48908581057931205; val_accuracy: 0.8958001592356688 

The current subspace-distance is: 0.00016882222553249449 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.95
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.95
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0001758547150529921
0.00016857402806635946
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.47752539386415177; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 0.00016857402806635946 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.54; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.94
Batch: 480; loss: 0.57; acc: 0.91
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.36; acc: 1.0
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00017711879627313465
0.00016736440011300147
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.47733678692465376; val_accuracy: 0.8968949044585988 

The current subspace-distance is: 0.00016736440011300147 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.83; acc: 0.78
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.95
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.95
Batch: 320; loss: 0.54; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.94
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.92
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.95
Batch: 760; loss: 0.55; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00017834000755101442
0.00016981006774585694
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4734959222708538; val_accuracy: 0.8949044585987261 

The current subspace-distance is: 0.00016981006774585694 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.94
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.95
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.7; acc: 0.78
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.51; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00017941651458386332
0.00017357592878397554
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.46663641322190597; val_accuracy: 0.8988853503184714 

The current subspace-distance is: 0.00017357592878397554 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.77
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.98
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00018100639863405377
0.0001756453129928559
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.45942799851393246; val_accuracy: 0.9010748407643312 

The current subspace-distance is: 0.0001756453129928559 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.95
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.94
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.97
Batch: 600; loss: 0.45; acc: 0.95
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00018652617291081697
0.0001774336415110156
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.45673466336195634; val_accuracy: 0.8999800955414012 

The current subspace-distance is: 0.0001774336415110156 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.95
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.64; acc: 0.81
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.97
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.8
Batch: 440; loss: 0.47; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.53; acc: 0.89
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001878366165328771
0.0001809702953323722
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.4479564728250929; val_accuracy: 0.9018710191082803 

The current subspace-distance is: 0.0001809702953323722 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.97
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.95
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.95
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.00018823776917997748
0.00018189790716860443
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4547137356108161; val_accuracy: 0.8982882165605095 

The current subspace-distance is: 0.00018189790716860443 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.95
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00019032870477531105
0.000182547650183551
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4398137608151527; val_accuracy: 0.9048566878980892 

The current subspace-distance is: 0.000182547650183551 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.97
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.77; acc: 0.81
Batch: 780; loss: 0.69; acc: 0.77
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00019185955170542002
0.00018627787358127534
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4370915228204363; val_accuracy: 0.9041600318471338 

The current subspace-distance is: 0.00018627787358127534 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.69; acc: 0.83
Batch: 680; loss: 0.52; acc: 0.8
Batch: 700; loss: 0.48; acc: 0.95
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.97
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00018976072897203267
0.0001825903309509158
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.44256134359699906; val_accuracy: 0.9044585987261147 

The current subspace-distance is: 0.0001825903309509158 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.97
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00019385229097679257
0.00018705142429098487
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4400653722369747; val_accuracy: 0.904359076433121 

The current subspace-distance is: 0.00018705142429098487 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.95
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.86; acc: 0.77
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.95
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.94
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.95
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00019314285600557923
0.00018551578978076577
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4333950053354737; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 0.00018551578978076577 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.6; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00019466372032184154
0.00018713489407673478
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.43675558069709003; val_accuracy: 0.9052547770700637 

The current subspace-distance is: 0.00018713489407673478 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.95
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00019379258446861058
0.00018545934290159494
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4326668565819977; val_accuracy: 0.9053542993630573 

The current subspace-distance is: 0.00018545934290159494 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.92
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.8
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.62; acc: 0.77
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00019627390429377556
0.0001887056278064847
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4319490344281409; val_accuracy: 0.90625 

The current subspace-distance is: 0.0001887056278064847 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.77
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.6; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00019561027875170112
0.00018871067732106894
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.43155584308751826; val_accuracy: 0.9067476114649682 

The current subspace-distance is: 0.00018871067732106894 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.79; acc: 0.77
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.8
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00019656466611195356
0.0001880096533568576
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4333926110890261; val_accuracy: 0.904359076433121 

The current subspace-distance is: 0.0001880096533568576 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.49; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.81
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.95
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.0001947488635778427
0.00018870166968554258
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.42971849916087596; val_accuracy: 0.9060509554140127 

The current subspace-distance is: 0.00018870166968554258 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.97
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.64; acc: 0.78
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.95
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00019859032181557268
0.0001894382876344025
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4326223305835845; val_accuracy: 0.9058519108280255 

The current subspace-distance is: 0.0001894382876344025 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_6_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.1089789320611354

The number of parameters is: 267194

The number of individual parameters is:

25
450
25
25
38
43700
38
38
75
131100
75
75
64
86400
64
64
4096
64
640
10
64
64

nonzero elements in E: 106877589
elements in E: 106877600
fraction nonzero: 0.9999998970785272
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.41; acc: 0.06
Batch: 20; loss: 2.07; acc: 0.28
Batch: 40; loss: 1.83; acc: 0.58
Batch: 60; loss: 1.78; acc: 0.53
Batch: 80; loss: 1.47; acc: 0.78
Batch: 100; loss: 1.6; acc: 0.62
Batch: 120; loss: 1.5; acc: 0.72
Batch: 140; loss: 1.35; acc: 0.77
Batch: 160; loss: 1.4; acc: 0.77
Batch: 180; loss: 1.3; acc: 0.78
Batch: 200; loss: 1.27; acc: 0.78
Batch: 220; loss: 1.32; acc: 0.72
Batch: 240; loss: 1.2; acc: 0.8
Batch: 260; loss: 1.22; acc: 0.78
Batch: 280; loss: 1.18; acc: 0.84
Batch: 300; loss: 1.2; acc: 0.8
Batch: 320; loss: 1.21; acc: 0.8
Batch: 340; loss: 1.23; acc: 0.75
Batch: 360; loss: 1.07; acc: 0.88
Batch: 380; loss: 1.07; acc: 0.84
Batch: 400; loss: 1.19; acc: 0.73
Batch: 420; loss: 1.1; acc: 0.84
Batch: 440; loss: 1.29; acc: 0.67
Batch: 460; loss: 1.16; acc: 0.73
Batch: 480; loss: 1.13; acc: 0.8
Batch: 500; loss: 1.01; acc: 0.86
Batch: 520; loss: 1.04; acc: 0.84
Batch: 540; loss: 1.23; acc: 0.69
Batch: 560; loss: 0.99; acc: 0.83
Batch: 580; loss: 1.05; acc: 0.83
Batch: 600; loss: 0.97; acc: 0.86
Batch: 620; loss: 1.08; acc: 0.75
Batch: 640; loss: 1.26; acc: 0.7
Batch: 660; loss: 1.04; acc: 0.8
Batch: 680; loss: 1.01; acc: 0.83
Batch: 700; loss: 1.12; acc: 0.75
Batch: 720; loss: 1.08; acc: 0.77
Batch: 740; loss: 0.94; acc: 0.83
Batch: 760; loss: 1.19; acc: 0.75
Batch: 780; loss: 0.97; acc: 0.88
Train Epoch over. train_loss: 1.26; train_accuracy: 0.74 

2.6043178877444007e-05
8.464077836833894e-06
Batch: 0; loss: 0.93; acc: 0.89
Batch: 20; loss: 1.15; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.91; acc: 0.81
Batch: 80; loss: 0.8; acc: 0.86
Batch: 100; loss: 0.94; acc: 0.86
Batch: 120; loss: 1.06; acc: 0.73
Batch: 140; loss: 0.79; acc: 0.94
Val Epoch over. val_loss: 0.9176036290302398; val_accuracy: 0.84265525477707 

The current subspace-distance is: 8.464077836833894e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.84
Batch: 20; loss: 0.91; acc: 0.86
Batch: 40; loss: 0.95; acc: 0.83
Batch: 60; loss: 0.94; acc: 0.77
Batch: 80; loss: 0.91; acc: 0.84
Batch: 100; loss: 0.95; acc: 0.84
Batch: 120; loss: 0.93; acc: 0.8
Batch: 140; loss: 0.92; acc: 0.81
Batch: 160; loss: 1.01; acc: 0.78
Batch: 180; loss: 0.95; acc: 0.78
Batch: 200; loss: 1.03; acc: 0.78
Batch: 220; loss: 0.91; acc: 0.84
Batch: 240; loss: 0.91; acc: 0.83
Batch: 260; loss: 0.98; acc: 0.77
Batch: 280; loss: 0.9; acc: 0.8
Batch: 300; loss: 0.81; acc: 0.88
Batch: 320; loss: 0.85; acc: 0.83
Batch: 340; loss: 0.88; acc: 0.88
Batch: 360; loss: 0.84; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 0.81; acc: 0.86
Batch: 420; loss: 1.08; acc: 0.73
Batch: 440; loss: 0.79; acc: 0.92
Batch: 460; loss: 0.87; acc: 0.81
Batch: 480; loss: 0.8; acc: 0.89
Batch: 500; loss: 0.72; acc: 0.89
Batch: 520; loss: 0.96; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.84
Batch: 560; loss: 0.82; acc: 0.84
Batch: 580; loss: 0.82; acc: 0.86
Batch: 600; loss: 0.92; acc: 0.77
Batch: 620; loss: 0.78; acc: 0.89
Batch: 640; loss: 0.8; acc: 0.83
Batch: 660; loss: 0.77; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.77; acc: 0.84
Batch: 740; loss: 0.87; acc: 0.86
Batch: 760; loss: 0.91; acc: 0.78
Batch: 780; loss: 0.76; acc: 0.84
Train Epoch over. train_loss: 0.87; train_accuracy: 0.83 

3.1582327210344374e-05
1.1924279533559456e-05
Batch: 0; loss: 0.72; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.52; acc: 0.95
Batch: 60; loss: 0.75; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.92
Batch: 100; loss: 0.72; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.98
Val Epoch over. val_loss: 0.7327349172655944; val_accuracy: 0.8717157643312102 

The current subspace-distance is: 1.1924279533559456e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.81
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.84; acc: 0.8
Batch: 120; loss: 0.67; acc: 0.91
Batch: 140; loss: 0.72; acc: 0.89
Batch: 160; loss: 0.82; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.86
Batch: 200; loss: 0.73; acc: 0.89
Batch: 220; loss: 0.76; acc: 0.88
Batch: 240; loss: 0.72; acc: 0.88
Batch: 260; loss: 0.84; acc: 0.83
Batch: 280; loss: 0.75; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.86
Batch: 320; loss: 0.65; acc: 0.91
Batch: 340; loss: 0.8; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.89
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.83; acc: 0.81
Batch: 460; loss: 0.79; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.89
Batch: 500; loss: 0.7; acc: 0.86
Batch: 520; loss: 0.76; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.69; acc: 0.86
Batch: 580; loss: 0.73; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.8; acc: 0.8
Batch: 640; loss: 0.85; acc: 0.81
Batch: 660; loss: 0.72; acc: 0.91
Batch: 680; loss: 0.88; acc: 0.73
Batch: 700; loss: 0.76; acc: 0.77
Batch: 720; loss: 0.62; acc: 0.91
Batch: 740; loss: 0.75; acc: 0.83
Batch: 760; loss: 0.75; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.91
Train Epoch over. train_loss: 0.73; train_accuracy: 0.86 

3.486099376459606e-05
1.3729408237850294e-05
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.94
Batch: 120; loss: 0.79; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.95
Val Epoch over. val_loss: 0.6262249238551802; val_accuracy: 0.8858479299363057 

The current subspace-distance is: 1.3729408237850294e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.97
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.91
Batch: 140; loss: 0.78; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.94
Batch: 280; loss: 0.59; acc: 0.92
Batch: 300; loss: 0.8; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.68; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.92
Batch: 440; loss: 0.69; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.8; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.81; acc: 0.81
Batch: 540; loss: 0.6; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.89
Batch: 620; loss: 0.75; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.92
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.89
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.86 

3.8664777093799785e-05
1.7688636944512837e-05
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.5693641809900855; val_accuracy: 0.8918192675159236 

The current subspace-distance is: 1.7688636944512837e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.56; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.97
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.76; acc: 0.77
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.95
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.41; acc: 0.97
Batch: 380; loss: 0.54; acc: 0.92
Batch: 400; loss: 0.65; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.77
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.92
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.73; acc: 0.77
Batch: 760; loss: 0.62; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.59; train_accuracy: 0.87 

4.164918573223986e-05
1.7484677300672047e-05
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.5097376611202385; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 1.7484677300672047e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.74; acc: 0.77
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.91
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

4.502833689912222e-05
2.013267840084154e-05
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.97
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.46197508133141096; val_accuracy: 0.9054538216560509 

The current subspace-distance is: 2.013267840084154e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.98
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.97
Batch: 580; loss: 0.46; acc: 0.97
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

4.825849100598134e-05
2.156349364668131e-05
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.98
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.42903216771639074; val_accuracy: 0.9101313694267515 

The current subspace-distance is: 2.156349364668131e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.95
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.97
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.8
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.09892197442241e-05
2.3973288989509456e-05
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.98
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.40099312649790647; val_accuracy: 0.913515127388535 

The current subspace-distance is: 2.3973288989509456e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.28; acc: 1.0
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.55; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.98
Batch: 780; loss: 0.54; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.263968341751024e-05
2.300113919773139e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.98
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.3819085538007651; val_accuracy: 0.9164012738853503 

The current subspace-distance is: 2.300113919773139e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.53; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.5513584811706096e-05
2.6775200240081176e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.98
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3674292112611661; val_accuracy: 0.9185907643312102 

The current subspace-distance is: 2.6775200240081176e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.627339851344004e-05
2.529333323764149e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3644718546776255; val_accuracy: 0.9177945859872612 

The current subspace-distance is: 2.529333323764149e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.97
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.95
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.664606942445971e-05
2.5005299903568812e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.98
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3603072933330657; val_accuracy: 0.9196855095541401 

The current subspace-distance is: 2.5005299903568812e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.701362533727661e-05
2.574301470303908e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.98
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.355023132720191; val_accuracy: 0.9201831210191083 

The current subspace-distance is: 2.574301470303908e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.26; acc: 0.97
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.728389442083426e-05
2.6485082344152033e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.349239832466575; val_accuracy: 0.9198845541401274 

The current subspace-distance is: 2.6485082344152033e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.98
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.78
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.8395424275659025e-05
2.691750523808878e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.34897536267140866; val_accuracy: 0.9201831210191083 

The current subspace-distance is: 2.691750523808878e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.21; acc: 1.0
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.9305792092345655e-05
2.6332767447456717e-05
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3470709748613607; val_accuracy: 0.92078025477707 

The current subspace-distance is: 2.6332767447456717e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.55; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.8
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.94
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.27; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.08203154115472e-05
2.9400194762274623e-05
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.34520325178553346; val_accuracy: 0.9217754777070064 

The current subspace-distance is: 2.9400194762274623e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.005085742799565e-05
2.819077781168744e-05
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3380038569307631; val_accuracy: 0.9211783439490446 

The current subspace-distance is: 2.819077781168744e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.98
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.020730143063702e-05
2.7848891477333382e-05
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3330678758063134; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.7848891477333382e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.29; acc: 0.97
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.080716775613837e-05
2.8522019420051947e-05
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3363051232258985; val_accuracy: 0.9211783439490446 

The current subspace-distance is: 2.8522019420051947e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.057395148673095e-05
2.66185961663723e-05
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.33627560051383487; val_accuracy: 0.9210788216560509 

The current subspace-distance is: 2.66185961663723e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.26; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.98
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.27; acc: 0.98
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.175501039251685e-05
2.92719360004412e-05
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.33583680867769156; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 2.92719360004412e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.98
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.78
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.0897098592249677e-05
2.7752255846280605e-05
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.33627029816815807; val_accuracy: 0.9231687898089171 

The current subspace-distance is: 2.7752255846280605e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.73; acc: 0.77
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.129840039648116e-05
2.793946441670414e-05
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3341692660454732; val_accuracy: 0.9222730891719745 

The current subspace-distance is: 2.793946441670414e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.1010869103483856e-05
2.789328391372692e-05
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.33047899366564054; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 2.789328391372692e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.98
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.165247032186016e-05
2.794648207782302e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.33242586928947715; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 2.794648207782302e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.98
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.153119466034696e-05
2.7875332307303324e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3276632599010589; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 2.7875332307303324e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.248041609069332e-05
2.8034299248247407e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3296542087937616; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.8034299248247407e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.97
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.253274477785453e-05
2.9051125238765962e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3250079305878111; val_accuracy: 0.924562101910828 

The current subspace-distance is: 2.9051125238765962e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.173891597427428e-05
2.740258423727937e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.32910137120515676; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 2.740258423727937e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_6_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.1089789320611354

The number of parameters is: 267194

The number of individual parameters is:

25
450
25
25
38
43700
38
38
75
131100
75
75
64
86400
64
64
4096
64
640
10
64
64

nonzero elements in E: 133596989
elements in E: 133597000
fraction nonzero: 0.9999999176628218
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.45; acc: 0.09
Batch: 20; loss: 2.03; acc: 0.39
Batch: 40; loss: 1.74; acc: 0.53
Batch: 60; loss: 1.47; acc: 0.7
Batch: 80; loss: 1.56; acc: 0.61
Batch: 100; loss: 1.44; acc: 0.7
Batch: 120; loss: 1.38; acc: 0.72
Batch: 140; loss: 1.26; acc: 0.78
Batch: 160; loss: 1.27; acc: 0.8
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.72
Batch: 220; loss: 1.15; acc: 0.77
Batch: 240; loss: 1.12; acc: 0.8
Batch: 260; loss: 1.17; acc: 0.81
Batch: 280; loss: 1.13; acc: 0.81
Batch: 300; loss: 1.24; acc: 0.72
Batch: 320; loss: 1.05; acc: 0.83
Batch: 340; loss: 1.01; acc: 0.88
Batch: 360; loss: 1.05; acc: 0.78
Batch: 380; loss: 1.0; acc: 0.88
Batch: 400; loss: 1.07; acc: 0.81
Batch: 420; loss: 0.94; acc: 0.88
Batch: 440; loss: 0.96; acc: 0.86
Batch: 460; loss: 0.97; acc: 0.83
Batch: 480; loss: 1.04; acc: 0.8
Batch: 500; loss: 0.98; acc: 0.8
Batch: 520; loss: 0.98; acc: 0.86
Batch: 540; loss: 1.06; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.88
Batch: 580; loss: 0.98; acc: 0.81
Batch: 600; loss: 0.97; acc: 0.83
Batch: 620; loss: 0.94; acc: 0.78
Batch: 640; loss: 0.89; acc: 0.86
Batch: 660; loss: 0.94; acc: 0.81
Batch: 680; loss: 0.91; acc: 0.86
Batch: 700; loss: 0.83; acc: 0.88
Batch: 720; loss: 0.85; acc: 0.88
Batch: 740; loss: 1.06; acc: 0.73
Batch: 760; loss: 0.76; acc: 0.88
Batch: 780; loss: 0.84; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.77 

2.616389247123152e-05
9.293044968217146e-06
Batch: 0; loss: 0.8; acc: 0.88
Batch: 20; loss: 1.06; acc: 0.73
Batch: 40; loss: 0.58; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.88
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.68; acc: 0.91
Val Epoch over. val_loss: 0.7829813774983594; val_accuracy: 0.8827627388535032 

The current subspace-distance is: 9.293044968217146e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.78
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 0.84; acc: 0.91
Batch: 80; loss: 0.72; acc: 0.91
Batch: 100; loss: 0.82; acc: 0.91
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.88; acc: 0.83
Batch: 160; loss: 0.74; acc: 0.89
Batch: 180; loss: 0.82; acc: 0.86
Batch: 200; loss: 0.76; acc: 0.89
Batch: 220; loss: 0.8; acc: 0.86
Batch: 240; loss: 0.71; acc: 0.91
Batch: 260; loss: 0.7; acc: 0.91
Batch: 280; loss: 0.75; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.92
Batch: 320; loss: 0.81; acc: 0.84
Batch: 340; loss: 0.88; acc: 0.88
Batch: 360; loss: 0.76; acc: 0.88
Batch: 380; loss: 0.78; acc: 0.88
Batch: 400; loss: 0.84; acc: 0.84
Batch: 420; loss: 0.79; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.89
Batch: 480; loss: 0.74; acc: 0.89
Batch: 500; loss: 0.81; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.91
Batch: 540; loss: 0.86; acc: 0.78
Batch: 560; loss: 0.76; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.92
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.69; acc: 0.89
Batch: 640; loss: 0.69; acc: 0.88
Batch: 660; loss: 0.69; acc: 0.88
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.94
Batch: 760; loss: 0.61; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.87 

3.1277395464712754e-05
1.2220514690852724e-05
Batch: 0; loss: 0.59; acc: 0.92
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.94
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.6176968402923293; val_accuracy: 0.9027667197452229 

The current subspace-distance is: 1.2220514690852724e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.95
Batch: 100; loss: 0.7; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.67; acc: 0.91
Batch: 160; loss: 0.63; acc: 0.91
Batch: 180; loss: 0.67; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.74; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.83; acc: 0.84
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.91
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.64; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.94
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.97
Batch: 520; loss: 0.64; acc: 0.86
Batch: 540; loss: 0.75; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.65; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.94
Batch: 660; loss: 0.58; acc: 0.97
Batch: 680; loss: 0.51; acc: 0.92
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.94
Batch: 780; loss: 0.54; acc: 0.94
Train Epoch over. train_loss: 0.61; train_accuracy: 0.89 

3.667864439194091e-05
1.4733385796716902e-05
Batch: 0; loss: 0.45; acc: 0.97
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.97
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Val Epoch over. val_loss: 0.5052215219682948; val_accuracy: 0.9175955414012739 

The current subspace-distance is: 1.4733385796716902e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.94
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.97
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.95
Batch: 180; loss: 0.5; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.97
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.94
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.95
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.33; acc: 1.0
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.95
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.95
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.98
Batch: 580; loss: 0.46; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.53; train_accuracy: 0.9 

4.003885987913236e-05
1.6978869098238647e-05
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.95
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.43943515078277345; val_accuracy: 0.923765923566879 

The current subspace-distance is: 1.6978869098238647e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.47; acc: 0.94
Batch: 260; loss: 0.59; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.94
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.95
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.97
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.91 

4.344791886978783e-05
1.856522067100741e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.22; acc: 1.0
Val Epoch over. val_loss: 0.39579158091241384; val_accuracy: 0.9257563694267515 

The current subspace-distance is: 1.856522067100741e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.95
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.95
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

4.5721935748588294e-05
2.0133928046561778e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.36530108673937; val_accuracy: 0.9295382165605095 

The current subspace-distance is: 2.0133928046561778e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.97
Batch: 180; loss: 0.27; acc: 1.0
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.98
Batch: 420; loss: 0.42; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.92 

4.852125493925996e-05
2.0957502783858217e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3405551862944463; val_accuracy: 0.9339171974522293 

The current subspace-distance is: 2.0957502783858217e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.97
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.22; acc: 0.98
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

5.120374771649949e-05
2.3619028070243075e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.323956557045317; val_accuracy: 0.9374004777070064 

The current subspace-distance is: 2.3619028070243075e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.98
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.97
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.385636541177519e-05
2.372647031734232e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2950321433080989; val_accuracy: 0.9391918789808917 

The current subspace-distance is: 2.372647031734232e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.24; acc: 0.98
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.596336995949969e-05
2.608243448776193e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.2860125282862384; val_accuracy: 0.9402866242038217 

The current subspace-distance is: 2.608243448776193e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.98
Batch: 280; loss: 0.19; acc: 0.98
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.97
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.98
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.98
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.6806540669640526e-05
2.5464316422585398e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2798655039280843; val_accuracy: 0.9419785031847133 

The current subspace-distance is: 2.5464316422585398e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.2; acc: 1.0
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.70962656638585e-05
2.5042385459528305e-05
Batch: 0; loss: 0.2; acc: 1.0
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.275969864409061; val_accuracy: 0.9422770700636943 

The current subspace-distance is: 2.5042385459528305e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.774961391580291e-05
2.5611750970711e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.26921788811873476; val_accuracy: 0.9419785031847133 

The current subspace-distance is: 2.5611750970711e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.98
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.98
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.819511352456175e-05
2.5383687898283824e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.26776480333060976; val_accuracy: 0.9436703821656051 

The current subspace-distance is: 2.5383687898283824e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.8667621487984434e-05
2.6417767003295012e-05
Batch: 0; loss: 0.19; acc: 1.0
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2628009255239918; val_accuracy: 0.9445660828025477 

The current subspace-distance is: 2.6417767003295012e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.2; acc: 0.98
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.925324512645602e-05
2.656028482306283e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.258580559378217; val_accuracy: 0.9432722929936306 

The current subspace-distance is: 2.656028482306283e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.98
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.98
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.98
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.18; acc: 0.98
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.888219311600551e-05
2.5418601580895483e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.25664649842081555; val_accuracy: 0.9440684713375797 

The current subspace-distance is: 2.5418601580895483e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.98
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.002562076901086e-05
2.7006553864339367e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2491220736959178; val_accuracy: 0.944765127388535 

The current subspace-distance is: 2.7006553864339367e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.98
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.121004116721451e-05
2.7199370379094034e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.248298353118122; val_accuracy: 0.9451632165605095 

The current subspace-distance is: 2.7199370379094034e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.127360393293202e-05
2.7757012503570877e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.24299337396955795; val_accuracy: 0.9461584394904459 

The current subspace-distance is: 2.7757012503570877e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.15; acc: 1.0
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.98
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.111170660005882e-05
2.7163590857526287e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2431901157092137; val_accuracy: 0.9465565286624203 

The current subspace-distance is: 2.7163590857526287e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.97
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.168575055198744e-05
2.759970993793104e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.24736006377608913; val_accuracy: 0.9449641719745223 

The current subspace-distance is: 2.759970993793104e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.2; acc: 0.98
Batch: 580; loss: 0.18; acc: 1.0
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.94 

6.252346065593883e-05
2.7917301849811338e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.24009317757597395; val_accuracy: 0.9464570063694268 

The current subspace-distance is: 2.7917301849811338e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.15; acc: 1.0
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.98
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.164132355479524e-05
2.7248370315646753e-05
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.24747589501036202; val_accuracy: 0.9442675159235668 

The current subspace-distance is: 2.7248370315646753e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.98
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.98
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.18; acc: 1.0
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.98
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.248371937545016e-05
2.9750613975920714e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.24066405629466295; val_accuracy: 0.9463574840764332 

The current subspace-distance is: 2.9750613975920714e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.15; acc: 1.0
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.86
Batch: 360; loss: 0.18; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.24152016825974e-05
2.9661408916581422e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.94
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2418071229936211; val_accuracy: 0.945859872611465 

The current subspace-distance is: 2.9661408916581422e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.310748722171411e-05
2.9070743039483204e-05
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.24012628007846273; val_accuracy: 0.9463574840764332 

The current subspace-distance is: 2.9070743039483204e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.98
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.309934542514384e-05
2.9099968742229976e-05
Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2378945264277185; val_accuracy: 0.946656050955414 

The current subspace-distance is: 2.9099968742229976e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.98
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.98
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.97
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.206102989381179e-05
2.7193818823434412e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.23832472095823592; val_accuracy: 0.945859872611465 

The current subspace-distance is: 2.7193818823434412e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.26808832748793e-05
2.771814979496412e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2404287895009776; val_accuracy: 0.9454617834394905 

The current subspace-distance is: 2.771814979496412e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_6_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_6_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
