model : table13slim
N : 5
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 3.4122939498231974

The number of parameters is: 278681

The number of individual parameters is:

28
448
28
28
41
49364
41
41
82
144566
82
82
64
78720
64
64
4096
64
640
10
64
64

nonzero elements in E: 13934049
elements in E: 13934050
fraction nonzero: 0.9999999282333564
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.34; acc: 0.14
Batch: 20; loss: 2.32; acc: 0.14
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.2; acc: 0.2
Batch: 80; loss: 2.05; acc: 0.3
Batch: 100; loss: 2.14; acc: 0.27
Batch: 120; loss: 2.04; acc: 0.28
Batch: 140; loss: 2.18; acc: 0.27
Batch: 160; loss: 2.05; acc: 0.28
Batch: 180; loss: 1.98; acc: 0.36
Batch: 200; loss: 1.93; acc: 0.3
Batch: 220; loss: 2.01; acc: 0.31
Batch: 240; loss: 2.08; acc: 0.28
Batch: 260; loss: 1.91; acc: 0.33
Batch: 280; loss: 1.99; acc: 0.3
Batch: 300; loss: 1.95; acc: 0.38
Batch: 320; loss: 1.97; acc: 0.34
Batch: 340; loss: 1.99; acc: 0.33
Batch: 360; loss: 2.07; acc: 0.2
Batch: 380; loss: 1.96; acc: 0.33
Batch: 400; loss: 1.91; acc: 0.41
Batch: 420; loss: 1.95; acc: 0.36
Batch: 440; loss: 1.95; acc: 0.38
Batch: 460; loss: 1.88; acc: 0.41
Batch: 480; loss: 1.99; acc: 0.31
Batch: 500; loss: 1.93; acc: 0.36
Batch: 520; loss: 1.88; acc: 0.38
Batch: 540; loss: 1.9; acc: 0.39
Batch: 560; loss: 2.05; acc: 0.27
Batch: 580; loss: 1.89; acc: 0.31
Batch: 600; loss: 1.88; acc: 0.39
Batch: 620; loss: 1.85; acc: 0.45
Batch: 640; loss: 1.94; acc: 0.31
Batch: 660; loss: 1.89; acc: 0.44
Batch: 680; loss: 1.91; acc: 0.38
Batch: 700; loss: 1.85; acc: 0.44
Batch: 720; loss: 1.89; acc: 0.44
Batch: 740; loss: 1.85; acc: 0.44
Batch: 760; loss: 1.8; acc: 0.52
Batch: 780; loss: 1.89; acc: 0.42
Train Epoch over. train_loss: 1.99; train_accuracy: 0.33 

2.120738645317033e-05
3.5776124605035875e-06
Batch: 0; loss: 1.99; acc: 0.33
Batch: 20; loss: 2.13; acc: 0.3
Batch: 40; loss: 1.77; acc: 0.53
Batch: 60; loss: 1.89; acc: 0.44
Batch: 80; loss: 1.8; acc: 0.44
Batch: 100; loss: 2.05; acc: 0.33
Batch: 120; loss: 1.95; acc: 0.3
Batch: 140; loss: 1.67; acc: 0.64
Val Epoch over. val_loss: 1.908602599125759; val_accuracy: 0.3948049363057325 

The current subspace-distance is: 3.5776124605035875e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.98; acc: 0.33
Batch: 20; loss: 1.95; acc: 0.41
Batch: 40; loss: 2.05; acc: 0.36
Batch: 60; loss: 1.82; acc: 0.45
Batch: 80; loss: 2.02; acc: 0.34
Batch: 100; loss: 1.82; acc: 0.42
Batch: 120; loss: 1.79; acc: 0.44
Batch: 140; loss: 1.79; acc: 0.48
Batch: 160; loss: 1.83; acc: 0.48
Batch: 180; loss: 1.98; acc: 0.3
Batch: 200; loss: 1.79; acc: 0.52
Batch: 220; loss: 1.9; acc: 0.36
Batch: 240; loss: 1.79; acc: 0.52
Batch: 260; loss: 1.93; acc: 0.42
Batch: 280; loss: 1.89; acc: 0.42
Batch: 300; loss: 1.83; acc: 0.47
Batch: 320; loss: 1.9; acc: 0.45
Batch: 340; loss: 1.88; acc: 0.44
Batch: 360; loss: 1.93; acc: 0.39
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.79; acc: 0.48
Batch: 420; loss: 1.76; acc: 0.47
Batch: 440; loss: 1.89; acc: 0.38
Batch: 460; loss: 1.78; acc: 0.45
Batch: 480; loss: 1.82; acc: 0.41
Batch: 500; loss: 1.79; acc: 0.44
Batch: 520; loss: 1.9; acc: 0.33
Batch: 540; loss: 1.82; acc: 0.45
Batch: 560; loss: 1.85; acc: 0.41
Batch: 580; loss: 1.77; acc: 0.5
Batch: 600; loss: 1.87; acc: 0.42
Batch: 620; loss: 1.85; acc: 0.53
Batch: 640; loss: 1.83; acc: 0.44
Batch: 660; loss: 1.74; acc: 0.48
Batch: 680; loss: 1.78; acc: 0.5
Batch: 700; loss: 1.69; acc: 0.52
Batch: 720; loss: 1.73; acc: 0.52
Batch: 740; loss: 1.85; acc: 0.45
Batch: 760; loss: 1.85; acc: 0.38
Batch: 780; loss: 1.83; acc: 0.5
Train Epoch over. train_loss: 1.85; train_accuracy: 0.43 

2.3787872123648413e-05
5.191811396798585e-06
Batch: 0; loss: 1.86; acc: 0.38
Batch: 20; loss: 1.97; acc: 0.34
Batch: 40; loss: 1.62; acc: 0.66
Batch: 60; loss: 1.78; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.56
Batch: 100; loss: 1.85; acc: 0.48
Batch: 120; loss: 1.88; acc: 0.41
Batch: 140; loss: 1.6; acc: 0.7
Val Epoch over. val_loss: 1.7700407899868715; val_accuracy: 0.5168192675159236 

The current subspace-distance is: 5.191811396798585e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.45
Batch: 20; loss: 1.77; acc: 0.52
Batch: 40; loss: 1.86; acc: 0.36
Batch: 60; loss: 1.81; acc: 0.44
Batch: 80; loss: 1.7; acc: 0.59
Batch: 100; loss: 1.82; acc: 0.47
Batch: 120; loss: 1.74; acc: 0.56
Batch: 140; loss: 1.86; acc: 0.44
Batch: 160; loss: 1.77; acc: 0.53
Batch: 180; loss: 1.68; acc: 0.66
Batch: 200; loss: 1.72; acc: 0.48
Batch: 220; loss: 1.89; acc: 0.41
Batch: 240; loss: 1.82; acc: 0.44
Batch: 260; loss: 1.71; acc: 0.55
Batch: 280; loss: 1.67; acc: 0.59
Batch: 300; loss: 1.84; acc: 0.44
Batch: 320; loss: 1.88; acc: 0.5
Batch: 340; loss: 1.72; acc: 0.61
Batch: 360; loss: 1.77; acc: 0.53
Batch: 380; loss: 1.67; acc: 0.62
Batch: 400; loss: 1.82; acc: 0.41
Batch: 420; loss: 1.72; acc: 0.52
Batch: 440; loss: 1.74; acc: 0.5
Batch: 460; loss: 1.87; acc: 0.38
Batch: 480; loss: 1.76; acc: 0.47
Batch: 500; loss: 1.71; acc: 0.59
Batch: 520; loss: 1.75; acc: 0.52
Batch: 540; loss: 1.71; acc: 0.53
Batch: 560; loss: 1.7; acc: 0.58
Batch: 580; loss: 1.72; acc: 0.58
Batch: 600; loss: 1.63; acc: 0.56
Batch: 620; loss: 1.77; acc: 0.52
Batch: 640; loss: 1.83; acc: 0.41
Batch: 660; loss: 1.72; acc: 0.58
Batch: 680; loss: 1.76; acc: 0.39
Batch: 700; loss: 1.74; acc: 0.52
Batch: 720; loss: 1.74; acc: 0.53
Batch: 740; loss: 1.77; acc: 0.5
Batch: 760; loss: 1.81; acc: 0.47
Batch: 780; loss: 1.67; acc: 0.61
Train Epoch over. train_loss: 1.76; train_accuracy: 0.51 

2.610259434732143e-05
7.594700491608819e-06
Batch: 0; loss: 1.75; acc: 0.45
Batch: 20; loss: 1.83; acc: 0.45
Batch: 40; loss: 1.54; acc: 0.64
Batch: 60; loss: 1.78; acc: 0.56
Batch: 80; loss: 1.59; acc: 0.62
Batch: 100; loss: 1.78; acc: 0.5
Batch: 120; loss: 1.81; acc: 0.45
Batch: 140; loss: 1.6; acc: 0.58
Val Epoch over. val_loss: 1.7153285946815637; val_accuracy: 0.5576234076433121 

The current subspace-distance is: 7.594700491608819e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.82; acc: 0.5
Batch: 20; loss: 1.68; acc: 0.58
Batch: 40; loss: 1.61; acc: 0.64
Batch: 60; loss: 1.57; acc: 0.7
Batch: 80; loss: 1.86; acc: 0.45
Batch: 100; loss: 1.71; acc: 0.48
Batch: 120; loss: 1.78; acc: 0.55
Batch: 140; loss: 1.81; acc: 0.44
Batch: 160; loss: 1.72; acc: 0.56
Batch: 180; loss: 1.77; acc: 0.5
Batch: 200; loss: 1.99; acc: 0.36
Batch: 220; loss: 1.71; acc: 0.56
Batch: 240; loss: 1.67; acc: 0.53
Batch: 260; loss: 1.69; acc: 0.56
Batch: 280; loss: 1.72; acc: 0.47
Batch: 300; loss: 1.73; acc: 0.47
Batch: 320; loss: 1.72; acc: 0.52
Batch: 340; loss: 1.72; acc: 0.5
Batch: 360; loss: 1.65; acc: 0.55
Batch: 380; loss: 1.81; acc: 0.53
Batch: 400; loss: 1.77; acc: 0.52
Batch: 420; loss: 1.68; acc: 0.53
Batch: 440; loss: 1.59; acc: 0.62
Batch: 460; loss: 1.68; acc: 0.58
Batch: 480; loss: 1.72; acc: 0.52
Batch: 500; loss: 1.65; acc: 0.62
Batch: 520; loss: 1.62; acc: 0.61
Batch: 540; loss: 1.79; acc: 0.48
Batch: 560; loss: 1.7; acc: 0.56
Batch: 580; loss: 1.8; acc: 0.44
Batch: 600; loss: 1.67; acc: 0.53
Batch: 620; loss: 1.73; acc: 0.5
Batch: 640; loss: 1.84; acc: 0.5
Batch: 660; loss: 1.69; acc: 0.55
Batch: 680; loss: 1.77; acc: 0.39
Batch: 700; loss: 1.75; acc: 0.52
Batch: 720; loss: 1.73; acc: 0.55
Batch: 740; loss: 1.58; acc: 0.59
Batch: 760; loss: 1.72; acc: 0.52
Batch: 780; loss: 1.73; acc: 0.45
Train Epoch over. train_loss: 1.73; train_accuracy: 0.52 

2.7638798201223835e-05
7.166239811340347e-06
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.74; acc: 0.42
Batch: 40; loss: 1.49; acc: 0.61
Batch: 60; loss: 1.76; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.61
Batch: 100; loss: 1.72; acc: 0.52
Batch: 120; loss: 1.77; acc: 0.45
Batch: 140; loss: 1.57; acc: 0.62
Val Epoch over. val_loss: 1.6804239058950146; val_accuracy: 0.5567277070063694 

The current subspace-distance is: 7.166239811340347e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.59
Batch: 20; loss: 1.61; acc: 0.58
Batch: 40; loss: 1.68; acc: 0.58
Batch: 60; loss: 1.72; acc: 0.55
Batch: 80; loss: 1.75; acc: 0.52
Batch: 100; loss: 1.69; acc: 0.5
Batch: 120; loss: 1.75; acc: 0.5
Batch: 140; loss: 1.63; acc: 0.59
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.64; acc: 0.61
Batch: 200; loss: 1.74; acc: 0.5
Batch: 220; loss: 1.6; acc: 0.62
Batch: 240; loss: 1.75; acc: 0.45
Batch: 260; loss: 1.8; acc: 0.47
Batch: 280; loss: 1.63; acc: 0.56
Batch: 300; loss: 1.71; acc: 0.53
Batch: 320; loss: 1.65; acc: 0.55
Batch: 340; loss: 1.8; acc: 0.45
Batch: 360; loss: 1.75; acc: 0.47
Batch: 380; loss: 1.73; acc: 0.53
Batch: 400; loss: 1.65; acc: 0.64
Batch: 420; loss: 1.64; acc: 0.52
Batch: 440; loss: 1.69; acc: 0.52
Batch: 460; loss: 1.68; acc: 0.53
Batch: 480; loss: 1.74; acc: 0.55
Batch: 500; loss: 1.74; acc: 0.45
Batch: 520; loss: 1.73; acc: 0.5
Batch: 540; loss: 1.68; acc: 0.48
Batch: 560; loss: 1.69; acc: 0.55
Batch: 580; loss: 1.68; acc: 0.53
Batch: 600; loss: 1.59; acc: 0.62
Batch: 620; loss: 1.61; acc: 0.56
Batch: 640; loss: 1.75; acc: 0.48
Batch: 660; loss: 1.6; acc: 0.59
Batch: 680; loss: 1.62; acc: 0.58
Batch: 700; loss: 1.52; acc: 0.59
Batch: 720; loss: 1.58; acc: 0.59
Batch: 740; loss: 1.62; acc: 0.59
Batch: 760; loss: 1.7; acc: 0.48
Batch: 780; loss: 1.65; acc: 0.53
Train Epoch over. train_loss: 1.7; train_accuracy: 0.52 

2.8739412300637923e-05
8.680659448145889e-06
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.42; acc: 0.67
Batch: 60; loss: 1.71; acc: 0.59
Batch: 80; loss: 1.49; acc: 0.66
Batch: 100; loss: 1.66; acc: 0.52
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.54; acc: 0.61
Val Epoch over. val_loss: 1.645207976839345; val_accuracy: 0.5642914012738853 

The current subspace-distance is: 8.680659448145889e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.57; acc: 0.64
Batch: 20; loss: 1.7; acc: 0.55
Batch: 40; loss: 1.61; acc: 0.55
Batch: 60; loss: 1.61; acc: 0.52
Batch: 80; loss: 1.7; acc: 0.5
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.68; acc: 0.66
Batch: 140; loss: 1.64; acc: 0.48
Batch: 160; loss: 1.79; acc: 0.47
Batch: 180; loss: 1.89; acc: 0.36
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.73; acc: 0.44
Batch: 240; loss: 1.73; acc: 0.5
Batch: 260; loss: 1.65; acc: 0.56
Batch: 280; loss: 1.6; acc: 0.53
Batch: 300; loss: 1.65; acc: 0.58
Batch: 320; loss: 1.78; acc: 0.45
Batch: 340; loss: 1.72; acc: 0.52
Batch: 360; loss: 1.69; acc: 0.52
Batch: 380; loss: 1.61; acc: 0.55
Batch: 400; loss: 1.83; acc: 0.36
Batch: 420; loss: 1.8; acc: 0.44
Batch: 440; loss: 1.65; acc: 0.48
Batch: 460; loss: 1.73; acc: 0.56
Batch: 480; loss: 1.6; acc: 0.56
Batch: 500; loss: 1.7; acc: 0.55
Batch: 520; loss: 1.84; acc: 0.41
Batch: 540; loss: 1.62; acc: 0.52
Batch: 560; loss: 1.66; acc: 0.58
Batch: 580; loss: 1.65; acc: 0.56
Batch: 600; loss: 1.74; acc: 0.48
Batch: 620; loss: 1.64; acc: 0.45
Batch: 640; loss: 1.62; acc: 0.56
Batch: 660; loss: 1.64; acc: 0.56
Batch: 680; loss: 1.66; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.58
Batch: 720; loss: 1.77; acc: 0.45
Batch: 740; loss: 1.55; acc: 0.58
Batch: 760; loss: 1.72; acc: 0.48
Batch: 780; loss: 1.51; acc: 0.67
Train Epoch over. train_loss: 1.67; train_accuracy: 0.53 

3.091784310527146e-05
8.887943295121659e-06
Batch: 0; loss: 1.62; acc: 0.48
Batch: 20; loss: 1.71; acc: 0.42
Batch: 40; loss: 1.38; acc: 0.69
Batch: 60; loss: 1.68; acc: 0.55
Batch: 80; loss: 1.43; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.72; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.66
Val Epoch over. val_loss: 1.618919713481976; val_accuracy: 0.5644904458598726 

The current subspace-distance is: 8.887943295121659e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.64; acc: 0.59
Batch: 40; loss: 1.63; acc: 0.59
Batch: 60; loss: 1.58; acc: 0.64
Batch: 80; loss: 1.64; acc: 0.5
Batch: 100; loss: 1.69; acc: 0.59
Batch: 120; loss: 1.67; acc: 0.58
Batch: 140; loss: 1.71; acc: 0.45
Batch: 160; loss: 1.64; acc: 0.56
Batch: 180; loss: 1.51; acc: 0.66
Batch: 200; loss: 1.74; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.56
Batch: 240; loss: 1.56; acc: 0.59
Batch: 260; loss: 1.63; acc: 0.56
Batch: 280; loss: 1.71; acc: 0.47
Batch: 300; loss: 1.7; acc: 0.47
Batch: 320; loss: 1.58; acc: 0.53
Batch: 340; loss: 1.58; acc: 0.56
Batch: 360; loss: 1.69; acc: 0.55
Batch: 380; loss: 1.65; acc: 0.5
Batch: 400; loss: 1.64; acc: 0.53
Batch: 420; loss: 1.53; acc: 0.62
Batch: 440; loss: 1.66; acc: 0.48
Batch: 460; loss: 1.55; acc: 0.59
Batch: 480; loss: 1.71; acc: 0.48
Batch: 500; loss: 1.59; acc: 0.56
Batch: 520; loss: 1.77; acc: 0.42
Batch: 540; loss: 1.56; acc: 0.64
Batch: 560; loss: 1.56; acc: 0.58
Batch: 580; loss: 1.6; acc: 0.62
Batch: 600; loss: 1.58; acc: 0.58
Batch: 620; loss: 1.75; acc: 0.41
Batch: 640; loss: 1.71; acc: 0.5
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.6; acc: 0.55
Batch: 700; loss: 1.7; acc: 0.48
Batch: 720; loss: 1.57; acc: 0.59
Batch: 740; loss: 1.65; acc: 0.55
Batch: 760; loss: 1.64; acc: 0.53
Batch: 780; loss: 1.69; acc: 0.47
Train Epoch over. train_loss: 1.65; train_accuracy: 0.53 

3.1903869967209175e-05
9.498165127297398e-06
Batch: 0; loss: 1.6; acc: 0.5
Batch: 20; loss: 1.71; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.72
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.39; acc: 0.75
Batch: 100; loss: 1.59; acc: 0.58
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.51; acc: 0.62
Val Epoch over. val_loss: 1.6028531774593766; val_accuracy: 0.5655851910828026 

The current subspace-distance is: 9.498165127297398e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.42
Batch: 20; loss: 1.59; acc: 0.58
Batch: 40; loss: 1.71; acc: 0.5
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.56; acc: 0.61
Batch: 100; loss: 1.56; acc: 0.59
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.73; acc: 0.56
Batch: 160; loss: 1.71; acc: 0.5
Batch: 180; loss: 1.63; acc: 0.55
Batch: 200; loss: 1.63; acc: 0.53
Batch: 220; loss: 1.57; acc: 0.59
Batch: 240; loss: 1.59; acc: 0.64
Batch: 260; loss: 1.71; acc: 0.48
Batch: 280; loss: 1.63; acc: 0.59
Batch: 300; loss: 1.61; acc: 0.55
Batch: 320; loss: 1.63; acc: 0.55
Batch: 340; loss: 1.55; acc: 0.61
Batch: 360; loss: 1.46; acc: 0.69
Batch: 380; loss: 1.67; acc: 0.58
Batch: 400; loss: 1.65; acc: 0.61
Batch: 420; loss: 1.54; acc: 0.55
Batch: 440; loss: 1.65; acc: 0.52
Batch: 460; loss: 1.63; acc: 0.56
Batch: 480; loss: 1.75; acc: 0.5
Batch: 500; loss: 1.7; acc: 0.47
Batch: 520; loss: 1.63; acc: 0.59
Batch: 540; loss: 1.67; acc: 0.5
Batch: 560; loss: 1.66; acc: 0.45
Batch: 580; loss: 1.46; acc: 0.64
Batch: 600; loss: 1.49; acc: 0.61
Batch: 620; loss: 1.64; acc: 0.56
Batch: 640; loss: 1.69; acc: 0.56
Batch: 660; loss: 1.66; acc: 0.45
Batch: 680; loss: 1.72; acc: 0.47
Batch: 700; loss: 1.61; acc: 0.56
Batch: 720; loss: 1.6; acc: 0.58
Batch: 740; loss: 1.62; acc: 0.61
Batch: 760; loss: 1.63; acc: 0.45
Batch: 780; loss: 1.52; acc: 0.62
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.278546500951052e-05
9.145118383457884e-06
Batch: 0; loss: 1.61; acc: 0.55
Batch: 20; loss: 1.73; acc: 0.42
Batch: 40; loss: 1.39; acc: 0.72
Batch: 60; loss: 1.67; acc: 0.52
Batch: 80; loss: 1.39; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.73; acc: 0.48
Batch: 140; loss: 1.53; acc: 0.61
Val Epoch over. val_loss: 1.6189123581928813; val_accuracy: 0.5610071656050956 

The current subspace-distance is: 9.145118383457884e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.5
Batch: 20; loss: 1.55; acc: 0.61
Batch: 40; loss: 1.53; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.52; acc: 0.66
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.87; acc: 0.45
Batch: 160; loss: 1.74; acc: 0.48
Batch: 180; loss: 1.51; acc: 0.62
Batch: 200; loss: 1.73; acc: 0.53
Batch: 220; loss: 1.56; acc: 0.61
Batch: 240; loss: 1.62; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.53
Batch: 280; loss: 1.6; acc: 0.59
Batch: 300; loss: 1.67; acc: 0.56
Batch: 320; loss: 1.58; acc: 0.56
Batch: 340; loss: 1.65; acc: 0.5
Batch: 360; loss: 1.62; acc: 0.56
Batch: 380; loss: 1.52; acc: 0.62
Batch: 400; loss: 1.67; acc: 0.45
Batch: 420; loss: 1.57; acc: 0.52
Batch: 440; loss: 1.62; acc: 0.56
Batch: 460; loss: 1.7; acc: 0.55
Batch: 480; loss: 1.58; acc: 0.53
Batch: 500; loss: 1.62; acc: 0.59
Batch: 520; loss: 1.56; acc: 0.58
Batch: 540; loss: 1.6; acc: 0.56
Batch: 560; loss: 1.66; acc: 0.5
Batch: 580; loss: 1.5; acc: 0.64
Batch: 600; loss: 1.62; acc: 0.58
Batch: 620; loss: 1.61; acc: 0.53
Batch: 640; loss: 1.66; acc: 0.55
Batch: 660; loss: 1.58; acc: 0.55
Batch: 680; loss: 1.61; acc: 0.5
Batch: 700; loss: 1.75; acc: 0.5
Batch: 720; loss: 1.55; acc: 0.58
Batch: 740; loss: 1.59; acc: 0.47
Batch: 760; loss: 1.59; acc: 0.53
Batch: 780; loss: 1.63; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.2530471798963845e-05
1.07263676909497e-05
Batch: 0; loss: 1.6; acc: 0.53
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.38; acc: 0.7
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.37; acc: 0.7
Batch: 100; loss: 1.59; acc: 0.58
Batch: 120; loss: 1.71; acc: 0.48
Batch: 140; loss: 1.52; acc: 0.66
Val Epoch over. val_loss: 1.6010748216301014; val_accuracy: 0.5674761146496815 

The current subspace-distance is: 1.07263676909497e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.57; acc: 0.59
Batch: 20; loss: 1.85; acc: 0.41
Batch: 40; loss: 1.57; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.62
Batch: 80; loss: 1.81; acc: 0.44
Batch: 100; loss: 1.57; acc: 0.56
Batch: 120; loss: 1.52; acc: 0.66
Batch: 140; loss: 1.6; acc: 0.61
Batch: 160; loss: 1.63; acc: 0.52
Batch: 180; loss: 1.65; acc: 0.58
Batch: 200; loss: 1.62; acc: 0.44
Batch: 220; loss: 1.62; acc: 0.59
Batch: 240; loss: 1.76; acc: 0.38
Batch: 260; loss: 1.66; acc: 0.53
Batch: 280; loss: 1.56; acc: 0.62
Batch: 300; loss: 1.62; acc: 0.55
Batch: 320; loss: 1.77; acc: 0.44
Batch: 340; loss: 1.66; acc: 0.59
Batch: 360; loss: 1.66; acc: 0.56
Batch: 380; loss: 1.67; acc: 0.47
Batch: 400; loss: 1.66; acc: 0.47
Batch: 420; loss: 1.55; acc: 0.55
Batch: 440; loss: 1.46; acc: 0.66
Batch: 460; loss: 1.84; acc: 0.44
Batch: 480; loss: 1.48; acc: 0.64
Batch: 500; loss: 1.65; acc: 0.55
Batch: 520; loss: 1.65; acc: 0.56
Batch: 540; loss: 1.69; acc: 0.5
Batch: 560; loss: 1.69; acc: 0.52
Batch: 580; loss: 1.67; acc: 0.56
Batch: 600; loss: 1.74; acc: 0.44
Batch: 620; loss: 1.66; acc: 0.47
Batch: 640; loss: 1.76; acc: 0.39
Batch: 660; loss: 1.61; acc: 0.52
Batch: 680; loss: 1.73; acc: 0.47
Batch: 700; loss: 1.6; acc: 0.61
Batch: 720; loss: 1.65; acc: 0.56
Batch: 740; loss: 1.67; acc: 0.48
Batch: 760; loss: 1.71; acc: 0.5
Batch: 780; loss: 1.72; acc: 0.44
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.307828592369333e-05
1.0607468539092224e-05
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.39; acc: 0.7
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.6; acc: 0.58
Batch: 120; loss: 1.71; acc: 0.5
Batch: 140; loss: 1.54; acc: 0.62
Val Epoch over. val_loss: 1.605080177070229; val_accuracy: 0.5659832802547771 

The current subspace-distance is: 1.0607468539092224e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.57; acc: 0.59
Batch: 20; loss: 1.66; acc: 0.52
Batch: 40; loss: 1.64; acc: 0.52
Batch: 60; loss: 1.69; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.7
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.72; acc: 0.44
Batch: 140; loss: 1.74; acc: 0.45
Batch: 160; loss: 1.65; acc: 0.52
Batch: 180; loss: 1.54; acc: 0.61
Batch: 200; loss: 1.58; acc: 0.55
Batch: 220; loss: 1.52; acc: 0.62
Batch: 240; loss: 1.73; acc: 0.52
Batch: 260; loss: 1.65; acc: 0.53
Batch: 280; loss: 1.69; acc: 0.5
Batch: 300; loss: 1.61; acc: 0.52
Batch: 320; loss: 1.65; acc: 0.55
Batch: 340; loss: 1.63; acc: 0.48
Batch: 360; loss: 1.63; acc: 0.55
Batch: 380; loss: 1.54; acc: 0.62
Batch: 400; loss: 1.72; acc: 0.5
Batch: 420; loss: 1.67; acc: 0.48
Batch: 440; loss: 1.73; acc: 0.48
Batch: 460; loss: 1.63; acc: 0.55
Batch: 480; loss: 1.6; acc: 0.53
Batch: 500; loss: 1.62; acc: 0.56
Batch: 520; loss: 1.59; acc: 0.53
Batch: 540; loss: 1.55; acc: 0.58
Batch: 560; loss: 1.7; acc: 0.48
Batch: 580; loss: 1.6; acc: 0.55
Batch: 600; loss: 1.56; acc: 0.59
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.56; acc: 0.55
Batch: 660; loss: 1.62; acc: 0.56
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.6; acc: 0.62
Batch: 720; loss: 1.49; acc: 0.64
Batch: 740; loss: 1.61; acc: 0.56
Batch: 760; loss: 1.65; acc: 0.52
Batch: 780; loss: 1.73; acc: 0.48
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.3238218748010695e-05
1.0453370123286732e-05
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.71; acc: 0.45
Batch: 40; loss: 1.4; acc: 0.72
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.37; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.71; acc: 0.48
Batch: 140; loss: 1.53; acc: 0.59
Val Epoch over. val_loss: 1.6019024051678408; val_accuracy: 0.5677746815286624 

The current subspace-distance is: 1.0453370123286732e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.75; acc: 0.5
Batch: 40; loss: 1.59; acc: 0.5
Batch: 60; loss: 1.74; acc: 0.47
Batch: 80; loss: 1.66; acc: 0.56
Batch: 100; loss: 1.59; acc: 0.53
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.7; acc: 0.55
Batch: 160; loss: 1.59; acc: 0.59
Batch: 180; loss: 1.65; acc: 0.52
Batch: 200; loss: 1.6; acc: 0.55
Batch: 220; loss: 1.75; acc: 0.5
Batch: 240; loss: 1.74; acc: 0.47
Batch: 260; loss: 1.59; acc: 0.48
Batch: 280; loss: 1.58; acc: 0.61
Batch: 300; loss: 1.59; acc: 0.55
Batch: 320; loss: 1.58; acc: 0.55
Batch: 340; loss: 1.76; acc: 0.42
Batch: 360; loss: 1.69; acc: 0.48
Batch: 380; loss: 1.66; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.5
Batch: 420; loss: 1.6; acc: 0.56
Batch: 440; loss: 1.54; acc: 0.67
Batch: 460; loss: 1.66; acc: 0.52
Batch: 480; loss: 1.5; acc: 0.58
Batch: 500; loss: 1.58; acc: 0.55
Batch: 520; loss: 1.64; acc: 0.5
Batch: 540; loss: 1.7; acc: 0.47
Batch: 560; loss: 1.58; acc: 0.59
Batch: 580; loss: 1.66; acc: 0.47
Batch: 600; loss: 1.6; acc: 0.5
Batch: 620; loss: 1.65; acc: 0.56
Batch: 640; loss: 1.65; acc: 0.48
Batch: 660; loss: 1.64; acc: 0.55
Batch: 680; loss: 1.7; acc: 0.53
Batch: 700; loss: 1.56; acc: 0.58
Batch: 720; loss: 1.73; acc: 0.42
Batch: 740; loss: 1.65; acc: 0.53
Batch: 760; loss: 1.57; acc: 0.53
Batch: 780; loss: 1.67; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.3915057429112494e-05
1.1894067938555963e-05
Batch: 0; loss: 1.61; acc: 0.59
Batch: 20; loss: 1.73; acc: 0.44
Batch: 40; loss: 1.4; acc: 0.69
Batch: 60; loss: 1.66; acc: 0.52
Batch: 80; loss: 1.36; acc: 0.69
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.53; acc: 0.62
Val Epoch over. val_loss: 1.6020082144220924; val_accuracy: 0.5621019108280255 

The current subspace-distance is: 1.1894067938555963e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.63; acc: 0.44
Batch: 40; loss: 1.72; acc: 0.45
Batch: 60; loss: 1.73; acc: 0.42
Batch: 80; loss: 1.67; acc: 0.47
Batch: 100; loss: 1.65; acc: 0.53
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.5; acc: 0.56
Batch: 160; loss: 1.65; acc: 0.53
Batch: 180; loss: 1.59; acc: 0.59
Batch: 200; loss: 1.61; acc: 0.59
Batch: 220; loss: 1.77; acc: 0.47
Batch: 240; loss: 1.59; acc: 0.56
Batch: 260; loss: 1.55; acc: 0.56
Batch: 280; loss: 1.6; acc: 0.55
Batch: 300; loss: 1.59; acc: 0.53
Batch: 320; loss: 1.61; acc: 0.56
Batch: 340; loss: 1.55; acc: 0.53
Batch: 360; loss: 1.6; acc: 0.5
Batch: 380; loss: 1.66; acc: 0.53
Batch: 400; loss: 1.59; acc: 0.58
Batch: 420; loss: 1.6; acc: 0.56
Batch: 440; loss: 1.66; acc: 0.55
Batch: 460; loss: 1.67; acc: 0.44
Batch: 480; loss: 1.62; acc: 0.52
Batch: 500; loss: 1.75; acc: 0.45
Batch: 520; loss: 1.62; acc: 0.56
Batch: 540; loss: 1.58; acc: 0.56
Batch: 560; loss: 1.62; acc: 0.64
Batch: 580; loss: 1.7; acc: 0.52
Batch: 600; loss: 1.73; acc: 0.48
Batch: 620; loss: 1.65; acc: 0.55
Batch: 640; loss: 1.59; acc: 0.56
Batch: 660; loss: 1.75; acc: 0.5
Batch: 680; loss: 1.71; acc: 0.41
Batch: 700; loss: 1.63; acc: 0.56
Batch: 720; loss: 1.75; acc: 0.39
Batch: 740; loss: 1.43; acc: 0.69
Batch: 760; loss: 1.59; acc: 0.5
Batch: 780; loss: 1.62; acc: 0.62
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.348530663060956e-05
1.0725404536060523e-05
Batch: 0; loss: 1.61; acc: 0.55
Batch: 20; loss: 1.73; acc: 0.45
Batch: 40; loss: 1.4; acc: 0.7
Batch: 60; loss: 1.66; acc: 0.52
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.71; acc: 0.48
Batch: 140; loss: 1.52; acc: 0.64
Val Epoch over. val_loss: 1.6005182721812254; val_accuracy: 0.5645899681528662 

The current subspace-distance is: 1.0725404536060523e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.44; acc: 0.67
Batch: 20; loss: 1.61; acc: 0.55
Batch: 40; loss: 1.61; acc: 0.52
Batch: 60; loss: 1.79; acc: 0.36
Batch: 80; loss: 1.68; acc: 0.47
Batch: 100; loss: 1.67; acc: 0.5
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.58; acc: 0.55
Batch: 160; loss: 1.47; acc: 0.66
Batch: 180; loss: 1.57; acc: 0.55
Batch: 200; loss: 1.76; acc: 0.41
Batch: 220; loss: 1.72; acc: 0.42
Batch: 240; loss: 1.59; acc: 0.61
Batch: 260; loss: 1.8; acc: 0.47
Batch: 280; loss: 1.62; acc: 0.56
Batch: 300; loss: 1.56; acc: 0.66
Batch: 320; loss: 1.68; acc: 0.52
Batch: 340; loss: 1.64; acc: 0.53
Batch: 360; loss: 1.58; acc: 0.5
Batch: 380; loss: 1.73; acc: 0.47
Batch: 400; loss: 1.52; acc: 0.58
Batch: 420; loss: 1.62; acc: 0.52
Batch: 440; loss: 1.7; acc: 0.42
Batch: 460; loss: 1.87; acc: 0.38
Batch: 480; loss: 1.63; acc: 0.53
Batch: 500; loss: 1.56; acc: 0.59
Batch: 520; loss: 1.68; acc: 0.45
Batch: 540; loss: 1.61; acc: 0.44
Batch: 560; loss: 1.67; acc: 0.52
Batch: 580; loss: 1.58; acc: 0.58
Batch: 600; loss: 1.63; acc: 0.56
Batch: 620; loss: 1.76; acc: 0.47
Batch: 640; loss: 1.68; acc: 0.52
Batch: 660; loss: 1.63; acc: 0.45
Batch: 680; loss: 1.65; acc: 0.52
Batch: 700; loss: 1.62; acc: 0.59
Batch: 720; loss: 1.69; acc: 0.52
Batch: 740; loss: 1.73; acc: 0.52
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.61; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.387090691830963e-05
1.1326746971462853e-05
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.74; acc: 0.42
Batch: 40; loss: 1.4; acc: 0.7
Batch: 60; loss: 1.65; acc: 0.52
Batch: 80; loss: 1.37; acc: 0.66
Batch: 100; loss: 1.6; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.52; acc: 0.62
Val Epoch over. val_loss: 1.598853593419312; val_accuracy: 0.5627985668789809 

The current subspace-distance is: 1.1326746971462853e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.74; acc: 0.45
Batch: 60; loss: 1.6; acc: 0.56
Batch: 80; loss: 1.61; acc: 0.53
Batch: 100; loss: 1.73; acc: 0.44
Batch: 120; loss: 1.61; acc: 0.58
Batch: 140; loss: 1.62; acc: 0.47
Batch: 160; loss: 1.63; acc: 0.48
Batch: 180; loss: 1.64; acc: 0.55
Batch: 200; loss: 1.58; acc: 0.55
Batch: 220; loss: 1.6; acc: 0.62
Batch: 240; loss: 1.63; acc: 0.55
Batch: 260; loss: 1.68; acc: 0.48
Batch: 280; loss: 1.73; acc: 0.48
Batch: 300; loss: 1.65; acc: 0.47
Batch: 320; loss: 1.5; acc: 0.59
Batch: 340; loss: 1.69; acc: 0.48
Batch: 360; loss: 1.55; acc: 0.59
Batch: 380; loss: 1.65; acc: 0.5
Batch: 400; loss: 1.66; acc: 0.53
Batch: 420; loss: 1.58; acc: 0.64
Batch: 440; loss: 1.57; acc: 0.56
Batch: 460; loss: 1.45; acc: 0.58
Batch: 480; loss: 1.74; acc: 0.48
Batch: 500; loss: 1.53; acc: 0.66
Batch: 520; loss: 1.55; acc: 0.64
Batch: 540; loss: 1.75; acc: 0.45
Batch: 560; loss: 1.55; acc: 0.61
Batch: 580; loss: 1.67; acc: 0.55
Batch: 600; loss: 1.71; acc: 0.5
Batch: 620; loss: 1.69; acc: 0.53
Batch: 640; loss: 1.7; acc: 0.52
Batch: 660; loss: 1.56; acc: 0.55
Batch: 680; loss: 1.69; acc: 0.53
Batch: 700; loss: 1.55; acc: 0.56
Batch: 720; loss: 1.62; acc: 0.59
Batch: 740; loss: 1.76; acc: 0.42
Batch: 760; loss: 1.75; acc: 0.45
Batch: 780; loss: 1.7; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.53 

3.378332985448651e-05
1.0549046237429138e-05
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.41; acc: 0.72
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.39; acc: 0.67
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.71; acc: 0.48
Batch: 140; loss: 1.52; acc: 0.66
Val Epoch over. val_loss: 1.6037633806277232; val_accuracy: 0.5683718152866242 

The current subspace-distance is: 1.0549046237429138e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.52; acc: 0.7
Batch: 20; loss: 1.7; acc: 0.47
Batch: 40; loss: 1.69; acc: 0.47
Batch: 60; loss: 1.57; acc: 0.61
Batch: 80; loss: 1.55; acc: 0.61
Batch: 100; loss: 1.58; acc: 0.52
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.7; acc: 0.5
Batch: 160; loss: 1.63; acc: 0.55
Batch: 180; loss: 1.51; acc: 0.58
Batch: 200; loss: 1.59; acc: 0.56
Batch: 220; loss: 1.55; acc: 0.59
Batch: 240; loss: 1.53; acc: 0.61
Batch: 260; loss: 1.69; acc: 0.5
Batch: 280; loss: 1.56; acc: 0.58
Batch: 300; loss: 1.55; acc: 0.61
Batch: 320; loss: 1.64; acc: 0.55
Batch: 340; loss: 1.65; acc: 0.62
Batch: 360; loss: 1.63; acc: 0.53
Batch: 380; loss: 1.68; acc: 0.45
Batch: 400; loss: 1.79; acc: 0.45
Batch: 420; loss: 1.73; acc: 0.44
Batch: 440; loss: 1.71; acc: 0.47
Batch: 460; loss: 1.7; acc: 0.47
Batch: 480; loss: 1.5; acc: 0.66
Batch: 500; loss: 1.52; acc: 0.66
Batch: 520; loss: 1.47; acc: 0.7
Batch: 540; loss: 1.62; acc: 0.53
Batch: 560; loss: 1.61; acc: 0.58
Batch: 580; loss: 1.68; acc: 0.52
Batch: 600; loss: 1.58; acc: 0.53
Batch: 620; loss: 1.57; acc: 0.5
Batch: 640; loss: 1.68; acc: 0.5
Batch: 660; loss: 1.64; acc: 0.5
Batch: 680; loss: 1.7; acc: 0.47
Batch: 700; loss: 1.59; acc: 0.59
Batch: 720; loss: 1.58; acc: 0.55
Batch: 740; loss: 1.72; acc: 0.5
Batch: 760; loss: 1.64; acc: 0.56
Batch: 780; loss: 1.54; acc: 0.62
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.4366694308118895e-05
1.2350332326604985e-05
Batch: 0; loss: 1.62; acc: 0.56
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.4; acc: 0.72
Batch: 60; loss: 1.65; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.69
Batch: 100; loss: 1.6; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.61
Val Epoch over. val_loss: 1.5955710441443571; val_accuracy: 0.5649880573248408 

The current subspace-distance is: 1.2350332326604985e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.44
Batch: 20; loss: 1.73; acc: 0.5
Batch: 40; loss: 1.61; acc: 0.55
Batch: 60; loss: 1.69; acc: 0.48
Batch: 80; loss: 1.79; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.48
Batch: 120; loss: 1.62; acc: 0.55
Batch: 140; loss: 1.54; acc: 0.56
Batch: 160; loss: 1.59; acc: 0.64
Batch: 180; loss: 1.69; acc: 0.52
Batch: 200; loss: 1.54; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.55
Batch: 240; loss: 1.79; acc: 0.42
Batch: 260; loss: 1.49; acc: 0.61
Batch: 280; loss: 1.61; acc: 0.62
Batch: 300; loss: 1.51; acc: 0.62
Batch: 320; loss: 1.74; acc: 0.55
Batch: 340; loss: 1.53; acc: 0.59
Batch: 360; loss: 1.59; acc: 0.59
Batch: 380; loss: 1.78; acc: 0.39
Batch: 400; loss: 1.66; acc: 0.44
Batch: 420; loss: 1.69; acc: 0.48
Batch: 440; loss: 1.7; acc: 0.45
Batch: 460; loss: 1.7; acc: 0.5
Batch: 480; loss: 1.61; acc: 0.56
Batch: 500; loss: 1.62; acc: 0.55
Batch: 520; loss: 1.6; acc: 0.58
Batch: 540; loss: 1.56; acc: 0.56
Batch: 560; loss: 1.7; acc: 0.42
Batch: 580; loss: 1.53; acc: 0.56
Batch: 600; loss: 1.59; acc: 0.53
Batch: 620; loss: 1.64; acc: 0.56
Batch: 640; loss: 1.61; acc: 0.55
Batch: 660; loss: 1.58; acc: 0.53
Batch: 680; loss: 1.59; acc: 0.61
Batch: 700; loss: 1.62; acc: 0.45
Batch: 720; loss: 1.53; acc: 0.7
Batch: 740; loss: 1.65; acc: 0.45
Batch: 760; loss: 1.57; acc: 0.55
Batch: 780; loss: 1.48; acc: 0.64
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.3124222682090476e-05
1.0806807949848007e-05
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.73; acc: 0.42
Batch: 40; loss: 1.4; acc: 0.75
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.37; acc: 0.7
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.51; acc: 0.66
Val Epoch over. val_loss: 1.5947089741943747; val_accuracy: 0.5630971337579618 

The current subspace-distance is: 1.0806807949848007e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.42
Batch: 20; loss: 1.69; acc: 0.56
Batch: 40; loss: 1.62; acc: 0.55
Batch: 60; loss: 1.65; acc: 0.56
Batch: 80; loss: 1.58; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.52
Batch: 120; loss: 1.66; acc: 0.59
Batch: 140; loss: 1.59; acc: 0.59
Batch: 160; loss: 1.7; acc: 0.5
Batch: 180; loss: 1.53; acc: 0.56
Batch: 200; loss: 1.64; acc: 0.56
Batch: 220; loss: 1.64; acc: 0.53
Batch: 240; loss: 1.62; acc: 0.53
Batch: 260; loss: 1.66; acc: 0.44
Batch: 280; loss: 1.88; acc: 0.36
Batch: 300; loss: 1.57; acc: 0.56
Batch: 320; loss: 1.59; acc: 0.58
Batch: 340; loss: 1.69; acc: 0.52
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.6; acc: 0.55
Batch: 400; loss: 1.67; acc: 0.42
Batch: 420; loss: 1.6; acc: 0.55
Batch: 440; loss: 1.62; acc: 0.56
Batch: 460; loss: 1.6; acc: 0.58
Batch: 480; loss: 1.77; acc: 0.47
Batch: 500; loss: 1.76; acc: 0.47
Batch: 520; loss: 1.61; acc: 0.56
Batch: 540; loss: 1.67; acc: 0.53
Batch: 560; loss: 1.7; acc: 0.45
Batch: 580; loss: 1.56; acc: 0.58
Batch: 600; loss: 1.64; acc: 0.59
Batch: 620; loss: 1.57; acc: 0.52
Batch: 640; loss: 1.74; acc: 0.45
Batch: 660; loss: 1.63; acc: 0.53
Batch: 680; loss: 1.72; acc: 0.47
Batch: 700; loss: 1.64; acc: 0.5
Batch: 720; loss: 1.64; acc: 0.56
Batch: 740; loss: 1.7; acc: 0.45
Batch: 760; loss: 1.75; acc: 0.52
Batch: 780; loss: 1.58; acc: 0.59
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.3359432563884184e-05
1.03066386145656e-05
Batch: 0; loss: 1.62; acc: 0.53
Batch: 20; loss: 1.73; acc: 0.42
Batch: 40; loss: 1.41; acc: 0.69
Batch: 60; loss: 1.65; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.69
Batch: 100; loss: 1.61; acc: 0.55
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.5; acc: 0.66
Val Epoch over. val_loss: 1.5947024890571644; val_accuracy: 0.5663813694267515 

The current subspace-distance is: 1.03066386145656e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.55; acc: 0.62
Batch: 40; loss: 1.7; acc: 0.55
Batch: 60; loss: 1.68; acc: 0.52
Batch: 80; loss: 1.8; acc: 0.44
Batch: 100; loss: 1.5; acc: 0.62
Batch: 120; loss: 1.66; acc: 0.55
Batch: 140; loss: 1.68; acc: 0.41
Batch: 160; loss: 1.52; acc: 0.69
Batch: 180; loss: 1.74; acc: 0.41
Batch: 200; loss: 1.71; acc: 0.47
Batch: 220; loss: 1.64; acc: 0.53
Batch: 240; loss: 1.83; acc: 0.44
Batch: 260; loss: 1.61; acc: 0.53
Batch: 280; loss: 1.72; acc: 0.53
Batch: 300; loss: 1.81; acc: 0.42
Batch: 320; loss: 1.62; acc: 0.55
Batch: 340; loss: 1.59; acc: 0.55
Batch: 360; loss: 1.67; acc: 0.42
Batch: 380; loss: 1.52; acc: 0.58
Batch: 400; loss: 1.54; acc: 0.56
Batch: 420; loss: 1.7; acc: 0.48
Batch: 440; loss: 1.53; acc: 0.59
Batch: 460; loss: 1.6; acc: 0.61
Batch: 480; loss: 1.66; acc: 0.47
Batch: 500; loss: 1.57; acc: 0.53
Batch: 520; loss: 1.61; acc: 0.5
Batch: 540; loss: 1.72; acc: 0.39
Batch: 560; loss: 1.54; acc: 0.61
Batch: 580; loss: 1.53; acc: 0.59
Batch: 600; loss: 1.65; acc: 0.48
Batch: 620; loss: 1.58; acc: 0.55
Batch: 640; loss: 1.53; acc: 0.62
Batch: 660; loss: 1.51; acc: 0.64
Batch: 680; loss: 1.62; acc: 0.55
Batch: 700; loss: 1.58; acc: 0.58
Batch: 720; loss: 1.57; acc: 0.59
Batch: 740; loss: 1.65; acc: 0.53
Batch: 760; loss: 1.66; acc: 0.53
Batch: 780; loss: 1.63; acc: 0.53
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.3886382880155e-05
1.1554573575267568e-05
Batch: 0; loss: 1.63; acc: 0.55
Batch: 20; loss: 1.72; acc: 0.45
Batch: 40; loss: 1.42; acc: 0.73
Batch: 60; loss: 1.65; acc: 0.55
Batch: 80; loss: 1.39; acc: 0.69
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.71; acc: 0.5
Batch: 140; loss: 1.5; acc: 0.67
Val Epoch over. val_loss: 1.598160954038049; val_accuracy: 0.5715565286624203 

The current subspace-distance is: 1.1554573575267568e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.52; acc: 0.62
Batch: 20; loss: 1.58; acc: 0.61
Batch: 40; loss: 1.67; acc: 0.45
Batch: 60; loss: 1.62; acc: 0.59
Batch: 80; loss: 1.62; acc: 0.52
Batch: 100; loss: 1.57; acc: 0.59
Batch: 120; loss: 1.64; acc: 0.5
Batch: 140; loss: 1.67; acc: 0.56
Batch: 160; loss: 1.76; acc: 0.47
Batch: 180; loss: 1.56; acc: 0.55
Batch: 200; loss: 1.6; acc: 0.58
Batch: 220; loss: 1.57; acc: 0.55
Batch: 240; loss: 1.66; acc: 0.58
Batch: 260; loss: 1.52; acc: 0.67
Batch: 280; loss: 1.55; acc: 0.62
Batch: 300; loss: 1.52; acc: 0.55
Batch: 320; loss: 1.7; acc: 0.47
Batch: 340; loss: 1.63; acc: 0.56
Batch: 360; loss: 1.64; acc: 0.48
Batch: 380; loss: 1.73; acc: 0.5
Batch: 400; loss: 1.61; acc: 0.53
Batch: 420; loss: 1.64; acc: 0.45
Batch: 440; loss: 1.59; acc: 0.55
Batch: 460; loss: 1.56; acc: 0.61
Batch: 480; loss: 1.65; acc: 0.47
Batch: 500; loss: 1.48; acc: 0.64
Batch: 520; loss: 1.59; acc: 0.56
Batch: 540; loss: 1.52; acc: 0.64
Batch: 560; loss: 1.6; acc: 0.52
Batch: 580; loss: 1.71; acc: 0.48
Batch: 600; loss: 1.54; acc: 0.56
Batch: 620; loss: 1.67; acc: 0.48
Batch: 640; loss: 1.62; acc: 0.55
Batch: 660; loss: 1.59; acc: 0.64
Batch: 680; loss: 1.57; acc: 0.58
Batch: 700; loss: 1.61; acc: 0.56
Batch: 720; loss: 1.5; acc: 0.61
Batch: 740; loss: 1.65; acc: 0.58
Batch: 760; loss: 1.54; acc: 0.61
Batch: 780; loss: 1.68; acc: 0.5
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.4659529774216935e-05
1.2988080015929881e-05
Batch: 0; loss: 1.62; acc: 0.53
Batch: 20; loss: 1.72; acc: 0.44
Batch: 40; loss: 1.4; acc: 0.72
Batch: 60; loss: 1.64; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.7
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.47
Batch: 140; loss: 1.49; acc: 0.67
Val Epoch over. val_loss: 1.5881422059551167; val_accuracy: 0.5685708598726115 

The current subspace-distance is: 1.2988080015929881e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.56
Batch: 20; loss: 1.68; acc: 0.53
Batch: 40; loss: 1.54; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 1.63; acc: 0.45
Batch: 100; loss: 1.66; acc: 0.56
Batch: 120; loss: 1.65; acc: 0.5
Batch: 140; loss: 1.66; acc: 0.48
Batch: 160; loss: 1.61; acc: 0.56
Batch: 180; loss: 1.59; acc: 0.59
Batch: 200; loss: 1.69; acc: 0.5
Batch: 220; loss: 1.68; acc: 0.47
Batch: 240; loss: 1.79; acc: 0.44
Batch: 260; loss: 1.63; acc: 0.53
Batch: 280; loss: 1.64; acc: 0.5
Batch: 300; loss: 1.6; acc: 0.61
Batch: 320; loss: 1.77; acc: 0.36
Batch: 340; loss: 1.68; acc: 0.5
Batch: 360; loss: 1.55; acc: 0.5
Batch: 380; loss: 1.64; acc: 0.53
Batch: 400; loss: 1.64; acc: 0.52
Batch: 420; loss: 1.57; acc: 0.5
Batch: 440; loss: 1.7; acc: 0.45
Batch: 460; loss: 1.66; acc: 0.47
Batch: 480; loss: 1.75; acc: 0.45
Batch: 500; loss: 1.63; acc: 0.56
Batch: 520; loss: 1.62; acc: 0.47
Batch: 540; loss: 1.65; acc: 0.48
Batch: 560; loss: 1.77; acc: 0.41
Batch: 580; loss: 1.61; acc: 0.52
Batch: 600; loss: 1.69; acc: 0.5
Batch: 620; loss: 1.76; acc: 0.44
Batch: 640; loss: 1.57; acc: 0.56
Batch: 660; loss: 1.67; acc: 0.5
Batch: 680; loss: 1.75; acc: 0.44
Batch: 700; loss: 1.65; acc: 0.52
Batch: 720; loss: 1.57; acc: 0.53
Batch: 740; loss: 1.71; acc: 0.39
Batch: 760; loss: 1.68; acc: 0.48
Batch: 780; loss: 1.62; acc: 0.42
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.3961940061999485e-05
1.0791913155117072e-05
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.72; acc: 0.44
Batch: 40; loss: 1.4; acc: 0.72
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.38; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.47
Batch: 140; loss: 1.49; acc: 0.66
Val Epoch over. val_loss: 1.5886640222209274; val_accuracy: 0.5664808917197452 

The current subspace-distance is: 1.0791913155117072e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.77; acc: 0.44
Batch: 20; loss: 1.39; acc: 0.72
Batch: 40; loss: 1.67; acc: 0.47
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.74; acc: 0.47
Batch: 100; loss: 1.7; acc: 0.44
Batch: 120; loss: 1.62; acc: 0.56
Batch: 140; loss: 1.55; acc: 0.64
Batch: 160; loss: 1.74; acc: 0.45
Batch: 180; loss: 1.71; acc: 0.45
Batch: 200; loss: 1.7; acc: 0.42
Batch: 220; loss: 1.71; acc: 0.5
Batch: 240; loss: 1.62; acc: 0.52
Batch: 260; loss: 1.48; acc: 0.62
Batch: 280; loss: 1.59; acc: 0.55
Batch: 300; loss: 1.53; acc: 0.59
Batch: 320; loss: 1.65; acc: 0.52
Batch: 340; loss: 1.58; acc: 0.59
Batch: 360; loss: 1.47; acc: 0.72
Batch: 380; loss: 1.68; acc: 0.53
Batch: 400; loss: 1.66; acc: 0.56
Batch: 420; loss: 1.64; acc: 0.61
Batch: 440; loss: 1.62; acc: 0.53
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.51; acc: 0.67
Batch: 500; loss: 1.69; acc: 0.5
Batch: 520; loss: 1.56; acc: 0.59
Batch: 540; loss: 1.7; acc: 0.42
Batch: 560; loss: 1.72; acc: 0.41
Batch: 580; loss: 1.66; acc: 0.45
Batch: 600; loss: 1.52; acc: 0.59
Batch: 620; loss: 1.52; acc: 0.52
Batch: 640; loss: 1.67; acc: 0.44
Batch: 660; loss: 1.89; acc: 0.34
Batch: 680; loss: 1.64; acc: 0.55
Batch: 700; loss: 1.66; acc: 0.47
Batch: 720; loss: 1.7; acc: 0.52
Batch: 740; loss: 1.55; acc: 0.55
Batch: 760; loss: 1.56; acc: 0.48
Batch: 780; loss: 1.58; acc: 0.56
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.4112748835468665e-05
1.10153860077844e-05
Batch: 0; loss: 1.63; acc: 0.53
Batch: 20; loss: 1.72; acc: 0.44
Batch: 40; loss: 1.42; acc: 0.7
Batch: 60; loss: 1.65; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.69
Batch: 100; loss: 1.62; acc: 0.58
Batch: 120; loss: 1.7; acc: 0.47
Batch: 140; loss: 1.5; acc: 0.64
Val Epoch over. val_loss: 1.5972228126161416; val_accuracy: 0.5679737261146497 

The current subspace-distance is: 1.10153860077844e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.72; acc: 0.5
Batch: 20; loss: 1.54; acc: 0.53
Batch: 40; loss: 1.62; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.5
Batch: 100; loss: 1.63; acc: 0.5
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.72; acc: 0.47
Batch: 160; loss: 1.77; acc: 0.36
Batch: 180; loss: 1.57; acc: 0.58
Batch: 200; loss: 1.64; acc: 0.55
Batch: 220; loss: 1.65; acc: 0.52
Batch: 240; loss: 1.6; acc: 0.55
Batch: 260; loss: 1.51; acc: 0.58
Batch: 280; loss: 1.62; acc: 0.56
Batch: 300; loss: 1.53; acc: 0.58
Batch: 320; loss: 1.7; acc: 0.45
Batch: 340; loss: 1.77; acc: 0.41
Batch: 360; loss: 1.61; acc: 0.5
Batch: 380; loss: 1.59; acc: 0.5
Batch: 400; loss: 1.65; acc: 0.52
Batch: 420; loss: 1.56; acc: 0.58
Batch: 440; loss: 1.69; acc: 0.53
Batch: 460; loss: 1.65; acc: 0.53
Batch: 480; loss: 1.67; acc: 0.47
Batch: 500; loss: 1.62; acc: 0.58
Batch: 520; loss: 1.66; acc: 0.53
Batch: 540; loss: 1.68; acc: 0.47
Batch: 560; loss: 1.64; acc: 0.5
Batch: 580; loss: 1.72; acc: 0.41
Batch: 600; loss: 1.5; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.47
Batch: 640; loss: 1.52; acc: 0.59
Batch: 660; loss: 1.58; acc: 0.58
Batch: 680; loss: 1.72; acc: 0.5
Batch: 700; loss: 1.51; acc: 0.59
Batch: 720; loss: 1.5; acc: 0.61
Batch: 740; loss: 1.74; acc: 0.44
Batch: 760; loss: 1.73; acc: 0.44
Batch: 780; loss: 1.55; acc: 0.59
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.4139891795348376e-05
1.0614681741571985e-05
Batch: 0; loss: 1.63; acc: 0.53
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.41; acc: 0.7
Batch: 60; loss: 1.64; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.62; acc: 0.55
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.49; acc: 0.61
Val Epoch over. val_loss: 1.5939951497278395; val_accuracy: 0.5655851910828026 

The current subspace-distance is: 1.0614681741571985e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.59; acc: 0.59
Batch: 40; loss: 1.76; acc: 0.45
Batch: 60; loss: 1.72; acc: 0.42
Batch: 80; loss: 1.6; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.59; acc: 0.56
Batch: 160; loss: 1.63; acc: 0.55
Batch: 180; loss: 1.67; acc: 0.48
Batch: 200; loss: 1.62; acc: 0.48
Batch: 220; loss: 1.69; acc: 0.42
Batch: 240; loss: 1.8; acc: 0.41
Batch: 260; loss: 1.66; acc: 0.48
Batch: 280; loss: 1.61; acc: 0.55
Batch: 300; loss: 1.51; acc: 0.66
Batch: 320; loss: 1.68; acc: 0.47
Batch: 340; loss: 1.67; acc: 0.41
Batch: 360; loss: 1.62; acc: 0.55
Batch: 380; loss: 1.53; acc: 0.61
Batch: 400; loss: 1.55; acc: 0.52
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.72; acc: 0.44
Batch: 460; loss: 1.68; acc: 0.56
Batch: 480; loss: 1.75; acc: 0.42
Batch: 500; loss: 1.56; acc: 0.62
Batch: 520; loss: 1.55; acc: 0.62
Batch: 540; loss: 1.69; acc: 0.5
Batch: 560; loss: 1.63; acc: 0.59
Batch: 580; loss: 1.65; acc: 0.58
Batch: 600; loss: 1.77; acc: 0.45
Batch: 620; loss: 1.64; acc: 0.5
Batch: 640; loss: 1.59; acc: 0.59
Batch: 660; loss: 1.69; acc: 0.56
Batch: 680; loss: 1.61; acc: 0.58
Batch: 700; loss: 1.7; acc: 0.55
Batch: 720; loss: 1.67; acc: 0.53
Batch: 740; loss: 1.62; acc: 0.5
Batch: 760; loss: 1.65; acc: 0.5
Batch: 780; loss: 1.59; acc: 0.62
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.393376391613856e-05
1.2133646123402286e-05
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.42; acc: 0.72
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.53
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.49; acc: 0.62
Val Epoch over. val_loss: 1.5933173165959158; val_accuracy: 0.5683718152866242 

The current subspace-distance is: 1.2133646123402286e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.61
Batch: 20; loss: 1.54; acc: 0.62
Batch: 40; loss: 1.71; acc: 0.53
Batch: 60; loss: 1.68; acc: 0.47
Batch: 80; loss: 1.67; acc: 0.58
Batch: 100; loss: 1.54; acc: 0.59
Batch: 120; loss: 1.64; acc: 0.62
Batch: 140; loss: 1.74; acc: 0.39
Batch: 160; loss: 1.49; acc: 0.62
Batch: 180; loss: 1.85; acc: 0.42
Batch: 200; loss: 1.68; acc: 0.47
Batch: 220; loss: 1.59; acc: 0.58
Batch: 240; loss: 1.63; acc: 0.56
Batch: 260; loss: 1.56; acc: 0.58
Batch: 280; loss: 1.64; acc: 0.5
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.6; acc: 0.56
Batch: 340; loss: 1.74; acc: 0.44
Batch: 360; loss: 1.7; acc: 0.48
Batch: 380; loss: 1.51; acc: 0.66
Batch: 400; loss: 1.66; acc: 0.56
Batch: 420; loss: 1.62; acc: 0.56
Batch: 440; loss: 1.78; acc: 0.42
Batch: 460; loss: 1.59; acc: 0.64
Batch: 480; loss: 1.61; acc: 0.59
Batch: 500; loss: 1.68; acc: 0.5
Batch: 520; loss: 1.66; acc: 0.52
Batch: 540; loss: 1.58; acc: 0.52
Batch: 560; loss: 1.51; acc: 0.62
Batch: 580; loss: 1.61; acc: 0.52
Batch: 600; loss: 1.59; acc: 0.59
Batch: 620; loss: 1.49; acc: 0.66
Batch: 640; loss: 1.54; acc: 0.59
Batch: 660; loss: 1.76; acc: 0.44
Batch: 680; loss: 1.63; acc: 0.44
Batch: 700; loss: 1.53; acc: 0.56
Batch: 720; loss: 1.76; acc: 0.42
Batch: 740; loss: 1.62; acc: 0.58
Batch: 760; loss: 1.75; acc: 0.47
Batch: 780; loss: 1.51; acc: 0.67
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.33530297211837e-05
1.0140827725990675e-05
Batch: 0; loss: 1.62; acc: 0.5
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.4; acc: 0.72
Batch: 60; loss: 1.63; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.7
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.45
Batch: 140; loss: 1.48; acc: 0.67
Val Epoch over. val_loss: 1.5836308177109737; val_accuracy: 0.5734474522292994 

The current subspace-distance is: 1.0140827725990675e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.52
Batch: 20; loss: 1.54; acc: 0.61
Batch: 40; loss: 1.68; acc: 0.53
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.62; acc: 0.52
Batch: 100; loss: 1.59; acc: 0.55
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.55; acc: 0.55
Batch: 160; loss: 1.51; acc: 0.61
Batch: 180; loss: 1.67; acc: 0.48
Batch: 200; loss: 1.68; acc: 0.39
Batch: 220; loss: 1.63; acc: 0.52
Batch: 240; loss: 1.6; acc: 0.52
Batch: 260; loss: 1.69; acc: 0.52
Batch: 280; loss: 1.65; acc: 0.45
Batch: 300; loss: 1.45; acc: 0.67
Batch: 320; loss: 1.55; acc: 0.7
Batch: 340; loss: 1.55; acc: 0.67
Batch: 360; loss: 1.74; acc: 0.38
Batch: 380; loss: 1.74; acc: 0.47
Batch: 400; loss: 1.58; acc: 0.62
Batch: 420; loss: 1.73; acc: 0.47
Batch: 440; loss: 1.59; acc: 0.56
Batch: 460; loss: 1.53; acc: 0.59
Batch: 480; loss: 1.64; acc: 0.52
Batch: 500; loss: 1.61; acc: 0.56
Batch: 520; loss: 1.73; acc: 0.5
Batch: 540; loss: 1.57; acc: 0.48
Batch: 560; loss: 1.62; acc: 0.53
Batch: 580; loss: 1.61; acc: 0.5
Batch: 600; loss: 1.74; acc: 0.47
Batch: 620; loss: 1.64; acc: 0.56
Batch: 640; loss: 1.65; acc: 0.56
Batch: 660; loss: 1.56; acc: 0.56
Batch: 680; loss: 1.71; acc: 0.52
Batch: 700; loss: 1.64; acc: 0.48
Batch: 720; loss: 1.6; acc: 0.53
Batch: 740; loss: 1.69; acc: 0.41
Batch: 760; loss: 1.48; acc: 0.66
Batch: 780; loss: 1.58; acc: 0.66
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.466461566858925e-05
1.2539737326733302e-05
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.4; acc: 0.7
Batch: 60; loss: 1.63; acc: 0.58
Batch: 80; loss: 1.4; acc: 0.69
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.47
Batch: 140; loss: 1.48; acc: 0.66
Val Epoch over. val_loss: 1.588536524468926; val_accuracy: 0.5683718152866242 

The current subspace-distance is: 1.2539737326733302e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.58
Batch: 20; loss: 1.48; acc: 0.64
Batch: 40; loss: 1.7; acc: 0.41
Batch: 60; loss: 1.69; acc: 0.45
Batch: 80; loss: 1.65; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.58
Batch: 120; loss: 1.56; acc: 0.64
Batch: 140; loss: 1.7; acc: 0.5
Batch: 160; loss: 1.66; acc: 0.5
Batch: 180; loss: 1.57; acc: 0.55
Batch: 200; loss: 1.52; acc: 0.67
Batch: 220; loss: 1.73; acc: 0.52
Batch: 240; loss: 1.59; acc: 0.52
Batch: 260; loss: 1.55; acc: 0.56
Batch: 280; loss: 1.59; acc: 0.61
Batch: 300; loss: 1.62; acc: 0.55
Batch: 320; loss: 1.66; acc: 0.44
Batch: 340; loss: 1.59; acc: 0.56
Batch: 360; loss: 1.55; acc: 0.62
Batch: 380; loss: 1.59; acc: 0.55
Batch: 400; loss: 1.58; acc: 0.52
Batch: 420; loss: 1.64; acc: 0.53
Batch: 440; loss: 1.63; acc: 0.52
Batch: 460; loss: 1.57; acc: 0.52
Batch: 480; loss: 1.58; acc: 0.61
Batch: 500; loss: 1.6; acc: 0.59
Batch: 520; loss: 1.79; acc: 0.36
Batch: 540; loss: 1.64; acc: 0.55
Batch: 560; loss: 1.61; acc: 0.55
Batch: 580; loss: 1.61; acc: 0.61
Batch: 600; loss: 1.58; acc: 0.53
Batch: 620; loss: 1.55; acc: 0.62
Batch: 640; loss: 1.51; acc: 0.64
Batch: 660; loss: 1.64; acc: 0.55
Batch: 680; loss: 1.65; acc: 0.53
Batch: 700; loss: 1.6; acc: 0.62
Batch: 720; loss: 1.59; acc: 0.62
Batch: 740; loss: 1.59; acc: 0.52
Batch: 760; loss: 1.71; acc: 0.47
Batch: 780; loss: 1.52; acc: 0.59
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.35978802468162e-05
1.0632473276928067e-05
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.71; acc: 0.42
Batch: 40; loss: 1.41; acc: 0.7
Batch: 60; loss: 1.64; acc: 0.58
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.48; acc: 0.66
Val Epoch over. val_loss: 1.5917816564535638; val_accuracy: 0.5704617834394905 

The current subspace-distance is: 1.0632473276928067e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.55
Batch: 20; loss: 1.74; acc: 0.55
Batch: 40; loss: 1.59; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.56
Batch: 80; loss: 1.66; acc: 0.55
Batch: 100; loss: 1.75; acc: 0.41
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.44; acc: 0.66
Batch: 160; loss: 1.62; acc: 0.45
Batch: 180; loss: 1.66; acc: 0.56
Batch: 200; loss: 1.55; acc: 0.56
Batch: 220; loss: 1.6; acc: 0.55
Batch: 240; loss: 1.56; acc: 0.5
Batch: 260; loss: 1.56; acc: 0.55
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.7; acc: 0.55
Batch: 320; loss: 1.56; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.58
Batch: 360; loss: 1.66; acc: 0.56
Batch: 380; loss: 1.71; acc: 0.48
Batch: 400; loss: 1.59; acc: 0.53
Batch: 420; loss: 1.57; acc: 0.62
Batch: 440; loss: 1.6; acc: 0.59
Batch: 460; loss: 1.56; acc: 0.61
Batch: 480; loss: 1.59; acc: 0.61
Batch: 500; loss: 1.57; acc: 0.45
Batch: 520; loss: 1.76; acc: 0.5
Batch: 540; loss: 1.63; acc: 0.5
Batch: 560; loss: 1.76; acc: 0.45
Batch: 580; loss: 1.73; acc: 0.47
Batch: 600; loss: 1.69; acc: 0.55
Batch: 620; loss: 1.6; acc: 0.5
Batch: 640; loss: 1.56; acc: 0.61
Batch: 660; loss: 1.58; acc: 0.5
Batch: 680; loss: 1.61; acc: 0.58
Batch: 700; loss: 1.57; acc: 0.56
Batch: 720; loss: 1.66; acc: 0.48
Batch: 740; loss: 1.56; acc: 0.58
Batch: 760; loss: 1.49; acc: 0.66
Batch: 780; loss: 1.69; acc: 0.53
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.4180884540546685e-05
1.1596789590839762e-05
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.42; acc: 0.7
Batch: 60; loss: 1.64; acc: 0.58
Batch: 80; loss: 1.41; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.55
Batch: 120; loss: 1.7; acc: 0.5
Batch: 140; loss: 1.49; acc: 0.62
Val Epoch over. val_loss: 1.596097638652583; val_accuracy: 0.5713574840764332 

The current subspace-distance is: 1.1596789590839762e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.64
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.64; acc: 0.48
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.68; acc: 0.53
Batch: 100; loss: 1.67; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.53; acc: 0.66
Batch: 160; loss: 1.62; acc: 0.55
Batch: 180; loss: 1.54; acc: 0.59
Batch: 200; loss: 1.54; acc: 0.62
Batch: 220; loss: 1.65; acc: 0.47
Batch: 240; loss: 1.73; acc: 0.44
Batch: 260; loss: 1.64; acc: 0.5
Batch: 280; loss: 1.57; acc: 0.58
Batch: 300; loss: 1.62; acc: 0.58
Batch: 320; loss: 1.67; acc: 0.52
Batch: 340; loss: 1.65; acc: 0.53
Batch: 360; loss: 1.6; acc: 0.56
Batch: 380; loss: 1.54; acc: 0.58
Batch: 400; loss: 1.63; acc: 0.48
Batch: 420; loss: 1.59; acc: 0.45
Batch: 440; loss: 1.49; acc: 0.64
Batch: 460; loss: 1.6; acc: 0.59
Batch: 480; loss: 1.69; acc: 0.58
Batch: 500; loss: 1.55; acc: 0.61
Batch: 520; loss: 1.68; acc: 0.55
Batch: 540; loss: 1.62; acc: 0.55
Batch: 560; loss: 1.64; acc: 0.5
Batch: 580; loss: 1.72; acc: 0.52
Batch: 600; loss: 1.61; acc: 0.58
Batch: 620; loss: 1.54; acc: 0.61
Batch: 640; loss: 1.56; acc: 0.59
Batch: 660; loss: 1.68; acc: 0.56
Batch: 680; loss: 1.62; acc: 0.47
Batch: 700; loss: 1.49; acc: 0.61
Batch: 720; loss: 1.59; acc: 0.56
Batch: 740; loss: 1.63; acc: 0.53
Batch: 760; loss: 1.58; acc: 0.59
Batch: 780; loss: 1.56; acc: 0.59
Train Epoch over. train_loss: 1.62; train_accuracy: 0.54 

3.426930197747424e-05
1.1174178325745743e-05
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.71; acc: 0.42
Batch: 40; loss: 1.41; acc: 0.72
Batch: 60; loss: 1.63; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.72
Batch: 100; loss: 1.6; acc: 0.53
Batch: 120; loss: 1.69; acc: 0.47
Batch: 140; loss: 1.47; acc: 0.64
Val Epoch over. val_loss: 1.5832328447111093; val_accuracy: 0.5718550955414012 

The current subspace-distance is: 1.1174178325745743e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.53
Batch: 20; loss: 1.78; acc: 0.44
Batch: 40; loss: 1.76; acc: 0.39
Batch: 60; loss: 1.61; acc: 0.53
Batch: 80; loss: 1.59; acc: 0.59
Batch: 100; loss: 1.5; acc: 0.62
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.68; acc: 0.52
Batch: 160; loss: 1.6; acc: 0.58
Batch: 180; loss: 1.55; acc: 0.61
Batch: 200; loss: 1.56; acc: 0.64
Batch: 220; loss: 1.63; acc: 0.48
Batch: 240; loss: 1.64; acc: 0.56
Batch: 260; loss: 1.54; acc: 0.53
Batch: 280; loss: 1.51; acc: 0.64
Batch: 300; loss: 1.63; acc: 0.45
Batch: 320; loss: 1.75; acc: 0.45
Batch: 340; loss: 1.64; acc: 0.52
Batch: 360; loss: 1.74; acc: 0.39
Batch: 380; loss: 1.71; acc: 0.45
Batch: 400; loss: 1.56; acc: 0.58
Batch: 420; loss: 1.77; acc: 0.5
Batch: 440; loss: 1.7; acc: 0.45
Batch: 460; loss: 1.67; acc: 0.55
Batch: 480; loss: 1.74; acc: 0.44
Batch: 500; loss: 1.56; acc: 0.58
Batch: 520; loss: 1.61; acc: 0.59
Batch: 540; loss: 1.63; acc: 0.62
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.57; acc: 0.5
Batch: 600; loss: 1.55; acc: 0.61
Batch: 620; loss: 1.66; acc: 0.44
Batch: 640; loss: 1.54; acc: 0.59
Batch: 660; loss: 1.57; acc: 0.62
Batch: 680; loss: 1.69; acc: 0.44
Batch: 700; loss: 1.65; acc: 0.48
Batch: 720; loss: 1.52; acc: 0.62
Batch: 740; loss: 1.68; acc: 0.44
Batch: 760; loss: 1.64; acc: 0.52
Batch: 780; loss: 1.71; acc: 0.48
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.442017623456195e-05
1.1825878573290538e-05
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.41; acc: 0.7
Batch: 60; loss: 1.63; acc: 0.58
Batch: 80; loss: 1.4; acc: 0.72
Batch: 100; loss: 1.61; acc: 0.53
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.67
Val Epoch over. val_loss: 1.593591898869557; val_accuracy: 0.5692675159235668 

The current subspace-distance is: 1.1825878573290538e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_5_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.4122939498231974

The number of parameters is: 278681

The number of individual parameters is:

28
448
28
28
41
49364
41
41
82
144566
82
82
64
78720
64
64
4096
64
640
10
64
64

nonzero elements in E: 27868097
elements in E: 27868100
fraction nonzero: 0.9999998923500346
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.23
Batch: 20; loss: 2.16; acc: 0.27
Batch: 40; loss: 2.02; acc: 0.34
Batch: 60; loss: 2.06; acc: 0.28
Batch: 80; loss: 1.97; acc: 0.34
Batch: 100; loss: 1.82; acc: 0.38
Batch: 120; loss: 2.0; acc: 0.23
Batch: 140; loss: 1.86; acc: 0.44
Batch: 160; loss: 1.88; acc: 0.36
Batch: 180; loss: 1.85; acc: 0.41
Batch: 200; loss: 1.82; acc: 0.47
Batch: 220; loss: 1.89; acc: 0.33
Batch: 240; loss: 1.93; acc: 0.34
Batch: 260; loss: 1.81; acc: 0.47
Batch: 280; loss: 1.79; acc: 0.5
Batch: 300; loss: 1.9; acc: 0.28
Batch: 320; loss: 1.79; acc: 0.45
Batch: 340; loss: 1.79; acc: 0.44
Batch: 360; loss: 1.71; acc: 0.45
Batch: 380; loss: 1.81; acc: 0.45
Batch: 400; loss: 1.76; acc: 0.47
Batch: 420; loss: 1.81; acc: 0.47
Batch: 440; loss: 1.74; acc: 0.47
Batch: 460; loss: 1.88; acc: 0.38
Batch: 480; loss: 1.71; acc: 0.47
Batch: 500; loss: 1.84; acc: 0.5
Batch: 520; loss: 1.63; acc: 0.59
Batch: 540; loss: 1.65; acc: 0.61
Batch: 560; loss: 1.8; acc: 0.42
Batch: 580; loss: 1.72; acc: 0.52
Batch: 600; loss: 1.66; acc: 0.58
Batch: 620; loss: 1.67; acc: 0.47
Batch: 640; loss: 1.6; acc: 0.53
Batch: 660; loss: 1.63; acc: 0.55
Batch: 680; loss: 1.51; acc: 0.67
Batch: 700; loss: 1.64; acc: 0.53
Batch: 720; loss: 1.57; acc: 0.64
Batch: 740; loss: 1.68; acc: 0.48
Batch: 760; loss: 1.69; acc: 0.48
Batch: 780; loss: 1.69; acc: 0.52
Train Epoch over. train_loss: 1.79; train_accuracy: 0.45 

5.4964839364401996e-05
5.009809319744818e-05
Batch: 0; loss: 1.6; acc: 0.59
Batch: 20; loss: 1.75; acc: 0.42
Batch: 40; loss: 1.36; acc: 0.73
Batch: 60; loss: 1.51; acc: 0.67
Batch: 80; loss: 1.49; acc: 0.69
Batch: 100; loss: 1.57; acc: 0.64
Batch: 120; loss: 1.62; acc: 0.61
Batch: 140; loss: 1.6; acc: 0.59
Val Epoch over. val_loss: 1.601653780906823; val_accuracy: 0.5856886942675159 

The current subspace-distance is: 5.009809319744818e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.67
Batch: 20; loss: 1.59; acc: 0.56
Batch: 40; loss: 1.63; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.67
Batch: 80; loss: 1.57; acc: 0.61
Batch: 100; loss: 1.63; acc: 0.59
Batch: 120; loss: 1.64; acc: 0.58
Batch: 140; loss: 1.57; acc: 0.64
Batch: 160; loss: 1.53; acc: 0.58
Batch: 180; loss: 1.66; acc: 0.58
Batch: 200; loss: 1.61; acc: 0.53
Batch: 220; loss: 1.6; acc: 0.62
Batch: 240; loss: 1.55; acc: 0.58
Batch: 260; loss: 1.55; acc: 0.66
Batch: 280; loss: 1.52; acc: 0.55
Batch: 300; loss: 1.55; acc: 0.69
Batch: 320; loss: 1.5; acc: 0.67
Batch: 340; loss: 1.69; acc: 0.53
Batch: 360; loss: 1.51; acc: 0.64
Batch: 380; loss: 1.59; acc: 0.66
Batch: 400; loss: 1.56; acc: 0.56
Batch: 420; loss: 1.54; acc: 0.56
Batch: 440; loss: 1.44; acc: 0.75
Batch: 460; loss: 1.54; acc: 0.69
Batch: 480; loss: 1.6; acc: 0.53
Batch: 500; loss: 1.58; acc: 0.55
Batch: 520; loss: 1.48; acc: 0.7
Batch: 540; loss: 1.56; acc: 0.64
Batch: 560; loss: 1.46; acc: 0.67
Batch: 580; loss: 1.67; acc: 0.58
Batch: 600; loss: 1.68; acc: 0.47
Batch: 620; loss: 1.61; acc: 0.52
Batch: 640; loss: 1.56; acc: 0.61
Batch: 660; loss: 1.54; acc: 0.64
Batch: 680; loss: 1.57; acc: 0.52
Batch: 700; loss: 1.5; acc: 0.67
Batch: 720; loss: 1.54; acc: 0.61
Batch: 740; loss: 1.48; acc: 0.66
Batch: 760; loss: 1.49; acc: 0.62
Batch: 780; loss: 1.47; acc: 0.67
Train Epoch over. train_loss: 1.56; train_accuracy: 0.6 

7.048308179946616e-05
6.650644354522228e-05
Batch: 0; loss: 1.51; acc: 0.66
Batch: 20; loss: 1.67; acc: 0.5
Batch: 40; loss: 1.25; acc: 0.75
Batch: 60; loss: 1.43; acc: 0.7
Batch: 80; loss: 1.41; acc: 0.66
Batch: 100; loss: 1.46; acc: 0.7
Batch: 120; loss: 1.49; acc: 0.62
Batch: 140; loss: 1.48; acc: 0.64
Val Epoch over. val_loss: 1.5033581249273507; val_accuracy: 0.6282842356687898 

The current subspace-distance is: 6.650644354522228e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.61
Batch: 20; loss: 1.53; acc: 0.61
Batch: 40; loss: 1.57; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.55
Batch: 80; loss: 1.49; acc: 0.67
Batch: 100; loss: 1.51; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.73
Batch: 160; loss: 1.56; acc: 0.59
Batch: 180; loss: 1.49; acc: 0.69
Batch: 200; loss: 1.51; acc: 0.59
Batch: 220; loss: 1.58; acc: 0.61
Batch: 240; loss: 1.53; acc: 0.62
Batch: 260; loss: 1.54; acc: 0.58
Batch: 280; loss: 1.46; acc: 0.62
Batch: 300; loss: 1.54; acc: 0.53
Batch: 320; loss: 1.51; acc: 0.62
Batch: 340; loss: 1.56; acc: 0.56
Batch: 360; loss: 1.33; acc: 0.77
Batch: 380; loss: 1.51; acc: 0.59
Batch: 400; loss: 1.42; acc: 0.66
Batch: 420; loss: 1.53; acc: 0.59
Batch: 440; loss: 1.54; acc: 0.58
Batch: 460; loss: 1.52; acc: 0.56
Batch: 480; loss: 1.48; acc: 0.62
Batch: 500; loss: 1.5; acc: 0.62
Batch: 520; loss: 1.52; acc: 0.61
Batch: 540; loss: 1.6; acc: 0.5
Batch: 560; loss: 1.59; acc: 0.55
Batch: 580; loss: 1.6; acc: 0.58
Batch: 600; loss: 1.64; acc: 0.56
Batch: 620; loss: 1.47; acc: 0.64
Batch: 640; loss: 1.5; acc: 0.58
Batch: 660; loss: 1.54; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.7
Batch: 700; loss: 1.47; acc: 0.66
Batch: 720; loss: 1.48; acc: 0.69
Batch: 740; loss: 1.57; acc: 0.58
Batch: 760; loss: 1.62; acc: 0.52
Batch: 780; loss: 1.29; acc: 0.84
Train Epoch over. train_loss: 1.51; train_accuracy: 0.62 

8.098877151496708e-05
7.592231850139797e-05
Batch: 0; loss: 1.49; acc: 0.64
Batch: 20; loss: 1.62; acc: 0.55
Batch: 40; loss: 1.22; acc: 0.75
Batch: 60; loss: 1.43; acc: 0.67
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.46; acc: 0.66
Batch: 120; loss: 1.48; acc: 0.61
Batch: 140; loss: 1.41; acc: 0.69
Val Epoch over. val_loss: 1.4780706372230676; val_accuracy: 0.6323646496815286 

The current subspace-distance is: 7.592231850139797e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.35; acc: 0.7
Batch: 40; loss: 1.41; acc: 0.66
Batch: 60; loss: 1.66; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.62
Batch: 100; loss: 1.55; acc: 0.62
Batch: 120; loss: 1.45; acc: 0.64
Batch: 140; loss: 1.54; acc: 0.55
Batch: 160; loss: 1.49; acc: 0.64
Batch: 180; loss: 1.56; acc: 0.58
Batch: 200; loss: 1.49; acc: 0.52
Batch: 220; loss: 1.45; acc: 0.62
Batch: 240; loss: 1.47; acc: 0.64
Batch: 260; loss: 1.62; acc: 0.55
Batch: 280; loss: 1.49; acc: 0.66
Batch: 300; loss: 1.5; acc: 0.61
Batch: 320; loss: 1.4; acc: 0.67
Batch: 340; loss: 1.42; acc: 0.64
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 1.41; acc: 0.69
Batch: 400; loss: 1.48; acc: 0.66
Batch: 420; loss: 1.46; acc: 0.66
Batch: 440; loss: 1.57; acc: 0.55
Batch: 460; loss: 1.55; acc: 0.59
Batch: 480; loss: 1.47; acc: 0.69
Batch: 500; loss: 1.41; acc: 0.7
Batch: 520; loss: 1.33; acc: 0.62
Batch: 540; loss: 1.58; acc: 0.53
Batch: 560; loss: 1.38; acc: 0.69
Batch: 580; loss: 1.37; acc: 0.7
Batch: 600; loss: 1.52; acc: 0.61
Batch: 620; loss: 1.48; acc: 0.66
Batch: 640; loss: 1.51; acc: 0.58
Batch: 660; loss: 1.32; acc: 0.8
Batch: 680; loss: 1.46; acc: 0.64
Batch: 700; loss: 1.41; acc: 0.67
Batch: 720; loss: 1.48; acc: 0.64
Batch: 740; loss: 1.47; acc: 0.59
Batch: 760; loss: 1.49; acc: 0.58
Batch: 780; loss: 1.5; acc: 0.56
Train Epoch over. train_loss: 1.49; train_accuracy: 0.62 

8.937839447753504e-05
8.52983066579327e-05
Batch: 0; loss: 1.48; acc: 0.61
Batch: 20; loss: 1.62; acc: 0.55
Batch: 40; loss: 1.19; acc: 0.75
Batch: 60; loss: 1.42; acc: 0.62
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.47; acc: 0.61
Batch: 120; loss: 1.47; acc: 0.59
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.4576223321781037; val_accuracy: 0.6318670382165605 

The current subspace-distance is: 8.52983066579327e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.55; acc: 0.53
Batch: 20; loss: 1.49; acc: 0.58
Batch: 40; loss: 1.46; acc: 0.64
Batch: 60; loss: 1.38; acc: 0.73
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.52; acc: 0.56
Batch: 120; loss: 1.51; acc: 0.69
Batch: 140; loss: 1.48; acc: 0.64
Batch: 160; loss: 1.41; acc: 0.67
Batch: 180; loss: 1.45; acc: 0.7
Batch: 200; loss: 1.5; acc: 0.64
Batch: 220; loss: 1.51; acc: 0.62
Batch: 240; loss: 1.61; acc: 0.48
Batch: 260; loss: 1.4; acc: 0.66
Batch: 280; loss: 1.48; acc: 0.62
Batch: 300; loss: 1.54; acc: 0.61
Batch: 320; loss: 1.5; acc: 0.62
Batch: 340; loss: 1.43; acc: 0.62
Batch: 360; loss: 1.46; acc: 0.56
Batch: 380; loss: 1.32; acc: 0.69
Batch: 400; loss: 1.5; acc: 0.59
Batch: 420; loss: 1.4; acc: 0.61
Batch: 440; loss: 1.5; acc: 0.64
Batch: 460; loss: 1.47; acc: 0.58
Batch: 480; loss: 1.47; acc: 0.56
Batch: 500; loss: 1.66; acc: 0.52
Batch: 520; loss: 1.49; acc: 0.64
Batch: 540; loss: 1.47; acc: 0.72
Batch: 560; loss: 1.63; acc: 0.56
Batch: 580; loss: 1.5; acc: 0.55
Batch: 600; loss: 1.35; acc: 0.73
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.35; acc: 0.73
Batch: 660; loss: 1.63; acc: 0.5
Batch: 680; loss: 1.53; acc: 0.64
Batch: 700; loss: 1.42; acc: 0.64
Batch: 720; loss: 1.51; acc: 0.61
Batch: 740; loss: 1.42; acc: 0.66
Batch: 760; loss: 1.58; acc: 0.59
Batch: 780; loss: 1.61; acc: 0.58
Train Epoch over. train_loss: 1.47; train_accuracy: 0.62 

9.533816773910075e-05
9.138086170423776e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.61; acc: 0.53
Batch: 40; loss: 1.16; acc: 0.75
Batch: 60; loss: 1.37; acc: 0.64
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.45; acc: 0.66
Batch: 120; loss: 1.45; acc: 0.61
Batch: 140; loss: 1.36; acc: 0.73
Val Epoch over. val_loss: 1.4349046725376395; val_accuracy: 0.6367436305732485 

The current subspace-distance is: 9.138086170423776e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.37; acc: 0.64
Batch: 20; loss: 1.42; acc: 0.61
Batch: 40; loss: 1.42; acc: 0.59
Batch: 60; loss: 1.48; acc: 0.66
Batch: 80; loss: 1.44; acc: 0.67
Batch: 100; loss: 1.56; acc: 0.62
Batch: 120; loss: 1.38; acc: 0.69
Batch: 140; loss: 1.5; acc: 0.66
Batch: 160; loss: 1.56; acc: 0.58
Batch: 180; loss: 1.42; acc: 0.61
Batch: 200; loss: 1.54; acc: 0.62
Batch: 220; loss: 1.45; acc: 0.66
Batch: 240; loss: 1.43; acc: 0.7
Batch: 260; loss: 1.52; acc: 0.62
Batch: 280; loss: 1.48; acc: 0.59
Batch: 300; loss: 1.4; acc: 0.58
Batch: 320; loss: 1.66; acc: 0.45
Batch: 340; loss: 1.57; acc: 0.58
Batch: 360; loss: 1.36; acc: 0.73
Batch: 380; loss: 1.57; acc: 0.58
Batch: 400; loss: 1.47; acc: 0.64
Batch: 420; loss: 1.49; acc: 0.53
Batch: 440; loss: 1.35; acc: 0.73
Batch: 460; loss: 1.44; acc: 0.67
Batch: 480; loss: 1.52; acc: 0.66
Batch: 500; loss: 1.35; acc: 0.69
Batch: 520; loss: 1.41; acc: 0.67
Batch: 540; loss: 1.44; acc: 0.66
Batch: 560; loss: 1.35; acc: 0.7
Batch: 580; loss: 1.37; acc: 0.64
Batch: 600; loss: 1.47; acc: 0.67
Batch: 620; loss: 1.39; acc: 0.69
Batch: 640; loss: 1.6; acc: 0.52
Batch: 660; loss: 1.38; acc: 0.67
Batch: 680; loss: 1.47; acc: 0.66
Batch: 700; loss: 1.48; acc: 0.56
Batch: 720; loss: 1.47; acc: 0.55
Batch: 740; loss: 1.32; acc: 0.67
Batch: 760; loss: 1.59; acc: 0.56
Batch: 780; loss: 1.47; acc: 0.61
Train Epoch over. train_loss: 1.45; train_accuracy: 0.63 

0.00010221808770438656
9.855660027824342e-05
Batch: 0; loss: 1.45; acc: 0.66
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.14; acc: 0.75
Batch: 60; loss: 1.33; acc: 0.62
Batch: 80; loss: 1.34; acc: 0.69
Batch: 100; loss: 1.46; acc: 0.59
Batch: 120; loss: 1.44; acc: 0.66
Batch: 140; loss: 1.36; acc: 0.69
Val Epoch over. val_loss: 1.4118099455620832; val_accuracy: 0.6367436305732485 

The current subspace-distance is: 9.855660027824342e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.69
Batch: 20; loss: 1.45; acc: 0.62
Batch: 40; loss: 1.42; acc: 0.64
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.42; acc: 0.61
Batch: 100; loss: 1.37; acc: 0.61
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 1.44; acc: 0.66
Batch: 160; loss: 1.41; acc: 0.62
Batch: 180; loss: 1.47; acc: 0.69
Batch: 200; loss: 1.32; acc: 0.72
Batch: 220; loss: 1.43; acc: 0.58
Batch: 240; loss: 1.29; acc: 0.72
Batch: 260; loss: 1.22; acc: 0.72
Batch: 280; loss: 1.6; acc: 0.56
Batch: 300; loss: 1.62; acc: 0.48
Batch: 320; loss: 1.49; acc: 0.55
Batch: 340; loss: 1.37; acc: 0.64
Batch: 360; loss: 1.45; acc: 0.67
Batch: 380; loss: 1.3; acc: 0.72
Batch: 400; loss: 1.57; acc: 0.61
Batch: 420; loss: 1.42; acc: 0.69
Batch: 440; loss: 1.48; acc: 0.64
Batch: 460; loss: 1.37; acc: 0.67
Batch: 480; loss: 1.41; acc: 0.61
Batch: 500; loss: 1.3; acc: 0.72
Batch: 520; loss: 1.4; acc: 0.59
Batch: 540; loss: 1.42; acc: 0.52
Batch: 560; loss: 1.39; acc: 0.64
Batch: 580; loss: 1.4; acc: 0.7
Batch: 600; loss: 1.51; acc: 0.55
Batch: 620; loss: 1.68; acc: 0.55
Batch: 640; loss: 1.38; acc: 0.69
Batch: 660; loss: 1.35; acc: 0.66
Batch: 680; loss: 1.42; acc: 0.59
Batch: 700; loss: 1.38; acc: 0.66
Batch: 720; loss: 1.45; acc: 0.56
Batch: 740; loss: 1.6; acc: 0.5
Batch: 760; loss: 1.3; acc: 0.75
Batch: 780; loss: 1.41; acc: 0.61
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

0.00010974606993841007
0.00010407395893707871
Batch: 0; loss: 1.45; acc: 0.62
Batch: 20; loss: 1.61; acc: 0.53
Batch: 40; loss: 1.12; acc: 0.78
Batch: 60; loss: 1.3; acc: 0.69
Batch: 80; loss: 1.32; acc: 0.67
Batch: 100; loss: 1.47; acc: 0.62
Batch: 120; loss: 1.41; acc: 0.62
Batch: 140; loss: 1.39; acc: 0.66
Val Epoch over. val_loss: 1.4058213735082348; val_accuracy: 0.6330613057324841 

The current subspace-distance is: 0.00010407395893707871 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.59
Batch: 20; loss: 1.48; acc: 0.58
Batch: 40; loss: 1.38; acc: 0.75
Batch: 60; loss: 1.41; acc: 0.69
Batch: 80; loss: 1.49; acc: 0.59
Batch: 100; loss: 1.39; acc: 0.62
Batch: 120; loss: 1.33; acc: 0.62
Batch: 140; loss: 1.27; acc: 0.69
Batch: 160; loss: 1.44; acc: 0.62
Batch: 180; loss: 1.41; acc: 0.64
Batch: 200; loss: 1.47; acc: 0.59
Batch: 220; loss: 1.49; acc: 0.59
Batch: 240; loss: 1.45; acc: 0.66
Batch: 260; loss: 1.45; acc: 0.58
Batch: 280; loss: 1.33; acc: 0.66
Batch: 300; loss: 1.52; acc: 0.48
Batch: 320; loss: 1.39; acc: 0.66
Batch: 340; loss: 1.4; acc: 0.69
Batch: 360; loss: 1.35; acc: 0.59
Batch: 380; loss: 1.41; acc: 0.61
Batch: 400; loss: 1.54; acc: 0.56
Batch: 420; loss: 1.42; acc: 0.64
Batch: 440; loss: 1.37; acc: 0.67
Batch: 460; loss: 1.23; acc: 0.77
Batch: 480; loss: 1.35; acc: 0.61
Batch: 500; loss: 1.39; acc: 0.56
Batch: 520; loss: 1.33; acc: 0.72
Batch: 540; loss: 1.5; acc: 0.61
Batch: 560; loss: 1.35; acc: 0.64
Batch: 580; loss: 1.38; acc: 0.66
Batch: 600; loss: 1.32; acc: 0.64
Batch: 620; loss: 1.38; acc: 0.72
Batch: 640; loss: 1.34; acc: 0.67
Batch: 660; loss: 1.36; acc: 0.64
Batch: 680; loss: 1.31; acc: 0.64
Batch: 700; loss: 1.25; acc: 0.66
Batch: 720; loss: 1.45; acc: 0.58
Batch: 740; loss: 1.28; acc: 0.69
Batch: 760; loss: 1.32; acc: 0.64
Batch: 780; loss: 1.49; acc: 0.55
Train Epoch over. train_loss: 1.41; train_accuracy: 0.62 

0.00011583710875129327
0.00011246318172197789
Batch: 0; loss: 1.44; acc: 0.59
Batch: 20; loss: 1.57; acc: 0.53
Batch: 40; loss: 1.1; acc: 0.8
Batch: 60; loss: 1.26; acc: 0.7
Batch: 80; loss: 1.29; acc: 0.67
Batch: 100; loss: 1.43; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.62
Batch: 140; loss: 1.39; acc: 0.64
Val Epoch over. val_loss: 1.3774460257997938; val_accuracy: 0.64171974522293 

The current subspace-distance is: 0.00011246318172197789 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.39; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.25; acc: 0.73
Batch: 100; loss: 1.41; acc: 0.56
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.43; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.56
Batch: 180; loss: 1.3; acc: 0.72
Batch: 200; loss: 1.38; acc: 0.62
Batch: 220; loss: 1.32; acc: 0.69
Batch: 240; loss: 1.42; acc: 0.59
Batch: 260; loss: 1.35; acc: 0.67
Batch: 280; loss: 1.4; acc: 0.59
Batch: 300; loss: 1.38; acc: 0.66
Batch: 320; loss: 1.36; acc: 0.66
Batch: 340; loss: 1.39; acc: 0.61
Batch: 360; loss: 1.29; acc: 0.7
Batch: 380; loss: 1.47; acc: 0.58
Batch: 400; loss: 1.26; acc: 0.67
Batch: 420; loss: 1.33; acc: 0.66
Batch: 440; loss: 1.42; acc: 0.62
Batch: 460; loss: 1.34; acc: 0.62
Batch: 480; loss: 1.44; acc: 0.58
Batch: 500; loss: 1.35; acc: 0.64
Batch: 520; loss: 1.32; acc: 0.69
Batch: 540; loss: 1.5; acc: 0.56
Batch: 560; loss: 1.34; acc: 0.66
Batch: 580; loss: 1.64; acc: 0.42
Batch: 600; loss: 1.52; acc: 0.47
Batch: 620; loss: 1.4; acc: 0.59
Batch: 640; loss: 1.4; acc: 0.61
Batch: 660; loss: 1.39; acc: 0.66
Batch: 680; loss: 1.24; acc: 0.7
Batch: 700; loss: 1.27; acc: 0.64
Batch: 720; loss: 1.49; acc: 0.59
Batch: 740; loss: 1.34; acc: 0.67
Batch: 760; loss: 1.34; acc: 0.66
Batch: 780; loss: 1.47; acc: 0.53
Train Epoch over. train_loss: 1.39; train_accuracy: 0.63 

0.00012601811613421887
0.00012126567889936268
Batch: 0; loss: 1.38; acc: 0.59
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.05; acc: 0.8
Batch: 60; loss: 1.22; acc: 0.75
Batch: 80; loss: 1.23; acc: 0.7
Batch: 100; loss: 1.37; acc: 0.66
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 1.37; acc: 0.66
Val Epoch over. val_loss: 1.3318829012524551; val_accuracy: 0.6491839171974523 

The current subspace-distance is: 0.00012126567889936268 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.55
Batch: 20; loss: 1.41; acc: 0.58
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.32; acc: 0.72
Batch: 80; loss: 1.46; acc: 0.59
Batch: 100; loss: 1.45; acc: 0.56
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 1.53; acc: 0.53
Batch: 160; loss: 1.45; acc: 0.62
Batch: 180; loss: 1.22; acc: 0.78
Batch: 200; loss: 1.31; acc: 0.62
Batch: 220; loss: 1.18; acc: 0.75
Batch: 240; loss: 1.23; acc: 0.72
Batch: 260; loss: 1.4; acc: 0.61
Batch: 280; loss: 1.28; acc: 0.72
Batch: 300; loss: 1.24; acc: 0.73
Batch: 320; loss: 1.26; acc: 0.77
Batch: 340; loss: 1.38; acc: 0.64
Batch: 360; loss: 1.41; acc: 0.59
Batch: 380; loss: 1.4; acc: 0.61
Batch: 400; loss: 1.21; acc: 0.66
Batch: 420; loss: 1.19; acc: 0.78
Batch: 440; loss: 1.44; acc: 0.62
Batch: 460; loss: 1.31; acc: 0.64
Batch: 480; loss: 1.43; acc: 0.61
Batch: 500; loss: 1.42; acc: 0.58
Batch: 520; loss: 1.28; acc: 0.69
Batch: 540; loss: 1.37; acc: 0.59
Batch: 560; loss: 1.43; acc: 0.59
Batch: 580; loss: 1.37; acc: 0.67
Batch: 600; loss: 1.32; acc: 0.64
Batch: 620; loss: 1.27; acc: 0.69
Batch: 640; loss: 1.25; acc: 0.62
Batch: 660; loss: 1.37; acc: 0.62
Batch: 680; loss: 1.3; acc: 0.73
Batch: 700; loss: 1.31; acc: 0.69
Batch: 720; loss: 1.18; acc: 0.72
Batch: 740; loss: 1.38; acc: 0.62
Batch: 760; loss: 1.49; acc: 0.53
Batch: 780; loss: 1.35; acc: 0.67
Train Epoch over. train_loss: 1.35; train_accuracy: 0.64 

0.0001356919383397326
0.00013272256182972342
Batch: 0; loss: 1.32; acc: 0.69
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 1.01; acc: 0.81
Batch: 60; loss: 1.21; acc: 0.72
Batch: 80; loss: 1.18; acc: 0.69
Batch: 100; loss: 1.3; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.2884982513014678; val_accuracy: 0.6682921974522293 

The current subspace-distance is: 0.00013272256182972342 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.72
Batch: 20; loss: 1.44; acc: 0.58
Batch: 40; loss: 1.35; acc: 0.66
Batch: 60; loss: 1.29; acc: 0.67
Batch: 80; loss: 1.32; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.7
Batch: 120; loss: 1.47; acc: 0.59
Batch: 140; loss: 1.22; acc: 0.73
Batch: 160; loss: 1.3; acc: 0.67
Batch: 180; loss: 1.41; acc: 0.59
Batch: 200; loss: 1.46; acc: 0.58
Batch: 220; loss: 1.39; acc: 0.61
Batch: 240; loss: 1.29; acc: 0.7
Batch: 260; loss: 1.47; acc: 0.56
Batch: 280; loss: 1.25; acc: 0.72
Batch: 300; loss: 1.2; acc: 0.77
Batch: 320; loss: 1.43; acc: 0.61
Batch: 340; loss: 1.31; acc: 0.64
Batch: 360; loss: 1.41; acc: 0.62
Batch: 380; loss: 1.26; acc: 0.66
Batch: 400; loss: 1.2; acc: 0.75
Batch: 420; loss: 1.17; acc: 0.67
Batch: 440; loss: 1.47; acc: 0.48
Batch: 460; loss: 1.21; acc: 0.75
Batch: 480; loss: 1.38; acc: 0.55
Batch: 500; loss: 1.28; acc: 0.58
Batch: 520; loss: 1.19; acc: 0.75
Batch: 540; loss: 1.34; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.75
Batch: 600; loss: 1.28; acc: 0.66
Batch: 620; loss: 1.25; acc: 0.69
Batch: 640; loss: 1.46; acc: 0.53
Batch: 660; loss: 1.23; acc: 0.72
Batch: 680; loss: 1.25; acc: 0.66
Batch: 700; loss: 1.34; acc: 0.56
Batch: 720; loss: 1.29; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.58
Batch: 760; loss: 1.56; acc: 0.5
Batch: 780; loss: 1.36; acc: 0.62
Train Epoch over. train_loss: 1.32; train_accuracy: 0.65 

0.00013783351460006088
0.00013495043094735593
Batch: 0; loss: 1.32; acc: 0.67
Batch: 20; loss: 1.42; acc: 0.53
Batch: 40; loss: 1.02; acc: 0.8
Batch: 60; loss: 1.23; acc: 0.72
Batch: 80; loss: 1.18; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 1.33; acc: 0.66
Val Epoch over. val_loss: 1.290313524045762; val_accuracy: 0.664609872611465 

The current subspace-distance is: 0.00013495043094735593 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.61
Batch: 20; loss: 1.25; acc: 0.73
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.21; acc: 0.73
Batch: 80; loss: 1.31; acc: 0.62
Batch: 100; loss: 1.21; acc: 0.67
Batch: 120; loss: 1.29; acc: 0.72
Batch: 140; loss: 1.4; acc: 0.7
Batch: 160; loss: 1.35; acc: 0.69
Batch: 180; loss: 1.35; acc: 0.59
Batch: 200; loss: 1.29; acc: 0.7
Batch: 220; loss: 1.33; acc: 0.66
Batch: 240; loss: 1.36; acc: 0.59
Batch: 260; loss: 1.34; acc: 0.62
Batch: 280; loss: 1.44; acc: 0.59
Batch: 300; loss: 1.27; acc: 0.73
Batch: 320; loss: 1.14; acc: 0.73
Batch: 340; loss: 1.23; acc: 0.72
Batch: 360; loss: 1.18; acc: 0.73
Batch: 380; loss: 1.24; acc: 0.67
Batch: 400; loss: 1.24; acc: 0.73
Batch: 420; loss: 1.18; acc: 0.7
Batch: 440; loss: 1.38; acc: 0.56
Batch: 460; loss: 1.19; acc: 0.78
Batch: 480; loss: 1.29; acc: 0.64
Batch: 500; loss: 1.48; acc: 0.44
Batch: 520; loss: 1.18; acc: 0.75
Batch: 540; loss: 1.36; acc: 0.64
Batch: 560; loss: 1.35; acc: 0.67
Batch: 580; loss: 1.4; acc: 0.59
Batch: 600; loss: 1.35; acc: 0.64
Batch: 620; loss: 1.46; acc: 0.61
Batch: 640; loss: 1.42; acc: 0.58
Batch: 660; loss: 1.3; acc: 0.7
Batch: 680; loss: 1.29; acc: 0.7
Batch: 700; loss: 1.29; acc: 0.61
Batch: 720; loss: 1.26; acc: 0.73
Batch: 740; loss: 1.41; acc: 0.64
Batch: 760; loss: 1.29; acc: 0.7
Batch: 780; loss: 1.31; acc: 0.62
Train Epoch over. train_loss: 1.31; train_accuracy: 0.65 

0.0001435575250070542
0.00013764530012849718
Batch: 0; loss: 1.29; acc: 0.67
Batch: 20; loss: 1.39; acc: 0.5
Batch: 40; loss: 1.0; acc: 0.83
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.15; acc: 0.75
Batch: 100; loss: 1.28; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.7
Val Epoch over. val_loss: 1.2613602265430863; val_accuracy: 0.6806329617834395 

The current subspace-distance is: 0.00013764530012849718 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.08; acc: 0.78
Batch: 40; loss: 1.5; acc: 0.53
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.31; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.69
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 1.39; acc: 0.59
Batch: 160; loss: 1.35; acc: 0.62
Batch: 180; loss: 1.24; acc: 0.72
Batch: 200; loss: 1.28; acc: 0.67
Batch: 220; loss: 1.13; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.69
Batch: 260; loss: 1.29; acc: 0.67
Batch: 280; loss: 1.35; acc: 0.58
Batch: 300; loss: 1.22; acc: 0.69
Batch: 320; loss: 1.26; acc: 0.66
Batch: 340; loss: 1.29; acc: 0.67
Batch: 360; loss: 1.39; acc: 0.62
Batch: 380; loss: 1.29; acc: 0.66
Batch: 400; loss: 1.26; acc: 0.67
Batch: 420; loss: 1.21; acc: 0.73
Batch: 440; loss: 1.38; acc: 0.61
Batch: 460; loss: 1.29; acc: 0.67
Batch: 480; loss: 1.26; acc: 0.62
Batch: 500; loss: 1.18; acc: 0.66
Batch: 520; loss: 1.31; acc: 0.62
Batch: 540; loss: 1.23; acc: 0.69
Batch: 560; loss: 1.23; acc: 0.67
Batch: 580; loss: 1.28; acc: 0.69
Batch: 600; loss: 1.25; acc: 0.66
Batch: 620; loss: 1.29; acc: 0.72
Batch: 640; loss: 1.27; acc: 0.66
Batch: 660; loss: 1.37; acc: 0.62
Batch: 680; loss: 1.38; acc: 0.66
Batch: 700; loss: 1.31; acc: 0.66
Batch: 720; loss: 1.19; acc: 0.73
Batch: 740; loss: 1.34; acc: 0.62
Batch: 760; loss: 1.28; acc: 0.67
Batch: 780; loss: 1.26; acc: 0.69
Train Epoch over. train_loss: 1.29; train_accuracy: 0.66 

0.00014659973385278136
0.00014046461728867143
Batch: 0; loss: 1.27; acc: 0.69
Batch: 20; loss: 1.37; acc: 0.52
Batch: 40; loss: 0.97; acc: 0.84
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.25; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 1.25; acc: 0.77
Val Epoch over. val_loss: 1.2402716951005777; val_accuracy: 0.6851114649681529 

The current subspace-distance is: 0.00014046461728867143 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.73
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.23; acc: 0.67
Batch: 100; loss: 1.27; acc: 0.67
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 1.43; acc: 0.5
Batch: 160; loss: 1.27; acc: 0.69
Batch: 180; loss: 1.18; acc: 0.73
Batch: 200; loss: 1.23; acc: 0.69
Batch: 220; loss: 1.35; acc: 0.73
Batch: 240; loss: 1.46; acc: 0.52
Batch: 260; loss: 1.47; acc: 0.56
Batch: 280; loss: 1.44; acc: 0.52
Batch: 300; loss: 1.36; acc: 0.62
Batch: 320; loss: 1.44; acc: 0.56
Batch: 340; loss: 1.31; acc: 0.66
Batch: 360; loss: 1.17; acc: 0.72
Batch: 380; loss: 1.33; acc: 0.67
Batch: 400; loss: 1.18; acc: 0.73
Batch: 420; loss: 1.24; acc: 0.64
Batch: 440; loss: 1.11; acc: 0.7
Batch: 460; loss: 1.31; acc: 0.59
Batch: 480; loss: 1.5; acc: 0.52
Batch: 500; loss: 1.16; acc: 0.75
Batch: 520; loss: 1.16; acc: 0.77
Batch: 540; loss: 1.33; acc: 0.61
Batch: 560; loss: 1.46; acc: 0.56
Batch: 580; loss: 1.27; acc: 0.72
Batch: 600; loss: 1.3; acc: 0.72
Batch: 620; loss: 1.2; acc: 0.64
Batch: 640; loss: 1.36; acc: 0.62
Batch: 660; loss: 1.22; acc: 0.69
Batch: 680; loss: 1.29; acc: 0.7
Batch: 700; loss: 1.35; acc: 0.66
Batch: 720; loss: 1.36; acc: 0.61
Batch: 740; loss: 1.45; acc: 0.59
Batch: 760; loss: 1.36; acc: 0.61
Batch: 780; loss: 1.34; acc: 0.62
Train Epoch over. train_loss: 1.28; train_accuracy: 0.66 

0.0001482405059505254
0.00014231532986741513
Batch: 0; loss: 1.25; acc: 0.72
Batch: 20; loss: 1.34; acc: 0.55
Batch: 40; loss: 0.97; acc: 0.84
Batch: 60; loss: 1.2; acc: 0.7
Batch: 80; loss: 1.13; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.67
Batch: 140; loss: 1.24; acc: 0.72
Val Epoch over. val_loss: 1.2380487899871389; val_accuracy: 0.6876990445859873 

The current subspace-distance is: 0.00014231532986741513 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 1.26; acc: 0.7
Batch: 60; loss: 1.09; acc: 0.81
Batch: 80; loss: 1.34; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.72
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 1.39; acc: 0.59
Batch: 160; loss: 1.31; acc: 0.59
Batch: 180; loss: 1.25; acc: 0.7
Batch: 200; loss: 1.15; acc: 0.73
Batch: 220; loss: 1.27; acc: 0.69
Batch: 240; loss: 1.29; acc: 0.69
Batch: 260; loss: 1.24; acc: 0.67
Batch: 280; loss: 1.21; acc: 0.77
Batch: 300; loss: 1.37; acc: 0.52
Batch: 320; loss: 1.29; acc: 0.67
Batch: 340; loss: 1.21; acc: 0.75
Batch: 360; loss: 1.21; acc: 0.72
Batch: 380; loss: 1.31; acc: 0.7
Batch: 400; loss: 1.18; acc: 0.75
Batch: 420; loss: 1.22; acc: 0.66
Batch: 440; loss: 1.25; acc: 0.69
Batch: 460; loss: 1.18; acc: 0.73
Batch: 480; loss: 1.32; acc: 0.59
Batch: 500; loss: 1.36; acc: 0.61
Batch: 520; loss: 1.21; acc: 0.73
Batch: 540; loss: 1.29; acc: 0.69
Batch: 560; loss: 1.5; acc: 0.5
Batch: 580; loss: 1.25; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.39; acc: 0.53
Batch: 640; loss: 1.29; acc: 0.61
Batch: 660; loss: 1.09; acc: 0.75
Batch: 680; loss: 1.48; acc: 0.5
Batch: 700; loss: 1.38; acc: 0.64
Batch: 720; loss: 1.25; acc: 0.67
Batch: 740; loss: 1.22; acc: 0.73
Batch: 760; loss: 1.34; acc: 0.61
Batch: 780; loss: 1.36; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.66 

0.00015447632176801562
0.00014747600653208792
Batch: 0; loss: 1.24; acc: 0.67
Batch: 20; loss: 1.33; acc: 0.56
Batch: 40; loss: 0.96; acc: 0.88
Batch: 60; loss: 1.21; acc: 0.7
Batch: 80; loss: 1.12; acc: 0.75
Batch: 100; loss: 1.24; acc: 0.72
Batch: 120; loss: 1.21; acc: 0.67
Batch: 140; loss: 1.22; acc: 0.73
Val Epoch over. val_loss: 1.2233926893039873; val_accuracy: 0.6934713375796179 

The current subspace-distance is: 0.00014747600653208792 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.2; acc: 0.59
Batch: 20; loss: 1.36; acc: 0.59
Batch: 40; loss: 1.26; acc: 0.72
Batch: 60; loss: 1.13; acc: 0.75
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.3; acc: 0.62
Batch: 120; loss: 1.32; acc: 0.64
Batch: 140; loss: 1.17; acc: 0.73
Batch: 160; loss: 1.53; acc: 0.5
Batch: 180; loss: 1.27; acc: 0.67
Batch: 200; loss: 1.38; acc: 0.62
Batch: 220; loss: 1.32; acc: 0.58
Batch: 240; loss: 1.25; acc: 0.7
Batch: 260; loss: 1.26; acc: 0.64
Batch: 280; loss: 1.13; acc: 0.73
Batch: 300; loss: 1.16; acc: 0.77
Batch: 320; loss: 1.21; acc: 0.73
Batch: 340; loss: 1.05; acc: 0.8
Batch: 360; loss: 1.26; acc: 0.69
Batch: 380; loss: 1.33; acc: 0.61
Batch: 400; loss: 1.3; acc: 0.66
Batch: 420; loss: 1.18; acc: 0.72
Batch: 440; loss: 1.32; acc: 0.62
Batch: 460; loss: 1.41; acc: 0.56
Batch: 480; loss: 1.35; acc: 0.53
Batch: 500; loss: 1.51; acc: 0.52
Batch: 520; loss: 1.19; acc: 0.69
Batch: 540; loss: 1.22; acc: 0.73
Batch: 560; loss: 1.26; acc: 0.61
Batch: 580; loss: 1.08; acc: 0.81
Batch: 600; loss: 1.39; acc: 0.59
Batch: 620; loss: 1.38; acc: 0.58
Batch: 640; loss: 1.17; acc: 0.69
Batch: 660; loss: 1.17; acc: 0.72
Batch: 680; loss: 1.08; acc: 0.73
Batch: 700; loss: 1.38; acc: 0.58
Batch: 720; loss: 1.14; acc: 0.7
Batch: 740; loss: 1.33; acc: 0.59
Batch: 760; loss: 1.43; acc: 0.59
Batch: 780; loss: 1.11; acc: 0.75
Train Epoch over. train_loss: 1.26; train_accuracy: 0.66 

0.0001543870457680896
0.00014862898387946188
Batch: 0; loss: 1.21; acc: 0.7
Batch: 20; loss: 1.31; acc: 0.53
Batch: 40; loss: 0.94; acc: 0.83
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.1; acc: 0.77
Batch: 100; loss: 1.21; acc: 0.72
Batch: 120; loss: 1.19; acc: 0.69
Batch: 140; loss: 1.18; acc: 0.77
Val Epoch over. val_loss: 1.2044927147543354; val_accuracy: 0.697452229299363 

The current subspace-distance is: 0.00014862898387946188 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.77
Batch: 40; loss: 1.23; acc: 0.67
Batch: 60; loss: 1.25; acc: 0.69
Batch: 80; loss: 1.27; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.15; acc: 0.7
Batch: 140; loss: 1.31; acc: 0.62
Batch: 160; loss: 1.29; acc: 0.61
Batch: 180; loss: 1.21; acc: 0.72
Batch: 200; loss: 1.25; acc: 0.69
Batch: 220; loss: 1.4; acc: 0.55
Batch: 240; loss: 1.4; acc: 0.56
Batch: 260; loss: 1.33; acc: 0.55
Batch: 280; loss: 1.24; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.72
Batch: 320; loss: 1.33; acc: 0.61
Batch: 340; loss: 1.25; acc: 0.67
Batch: 360; loss: 1.27; acc: 0.58
Batch: 380; loss: 1.01; acc: 0.83
Batch: 400; loss: 1.17; acc: 0.73
Batch: 420; loss: 1.35; acc: 0.61
Batch: 440; loss: 1.36; acc: 0.61
Batch: 460; loss: 1.28; acc: 0.72
Batch: 480; loss: 1.31; acc: 0.64
Batch: 500; loss: 1.33; acc: 0.67
Batch: 520; loss: 1.2; acc: 0.72
Batch: 540; loss: 1.31; acc: 0.59
Batch: 560; loss: 1.29; acc: 0.64
Batch: 580; loss: 1.22; acc: 0.7
Batch: 600; loss: 1.27; acc: 0.66
Batch: 620; loss: 1.1; acc: 0.8
Batch: 640; loss: 1.21; acc: 0.67
Batch: 660; loss: 1.18; acc: 0.7
Batch: 680; loss: 1.29; acc: 0.7
Batch: 700; loss: 1.24; acc: 0.7
Batch: 720; loss: 1.18; acc: 0.77
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.21; acc: 0.69
Batch: 780; loss: 1.12; acc: 0.67
Train Epoch over. train_loss: 1.25; train_accuracy: 0.67 

0.00015790809993632138
0.00015305443957913667
Batch: 0; loss: 1.21; acc: 0.69
Batch: 20; loss: 1.32; acc: 0.53
Batch: 40; loss: 0.95; acc: 0.83
Batch: 60; loss: 1.2; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.23; acc: 0.72
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 1.19; acc: 0.75
Val Epoch over. val_loss: 1.2052314125808181; val_accuracy: 0.6961584394904459 

The current subspace-distance is: 0.00015305443957913667 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.7
Batch: 20; loss: 1.15; acc: 0.66
Batch: 40; loss: 1.22; acc: 0.64
Batch: 60; loss: 1.2; acc: 0.7
Batch: 80; loss: 1.36; acc: 0.56
Batch: 100; loss: 1.3; acc: 0.59
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 1.24; acc: 0.7
Batch: 160; loss: 1.15; acc: 0.69
Batch: 180; loss: 1.16; acc: 0.73
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.66
Batch: 240; loss: 1.27; acc: 0.69
Batch: 260; loss: 1.2; acc: 0.69
Batch: 280; loss: 1.28; acc: 0.64
Batch: 300; loss: 1.36; acc: 0.58
Batch: 320; loss: 1.36; acc: 0.52
Batch: 340; loss: 1.19; acc: 0.64
Batch: 360; loss: 1.14; acc: 0.75
Batch: 380; loss: 1.22; acc: 0.64
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.3; acc: 0.62
Batch: 440; loss: 1.12; acc: 0.69
Batch: 460; loss: 1.18; acc: 0.72
Batch: 480; loss: 1.2; acc: 0.7
Batch: 500; loss: 1.2; acc: 0.69
Batch: 520; loss: 1.02; acc: 0.81
Batch: 540; loss: 1.16; acc: 0.69
Batch: 560; loss: 1.25; acc: 0.7
Batch: 580; loss: 1.28; acc: 0.66
Batch: 600; loss: 1.28; acc: 0.69
Batch: 620; loss: 1.26; acc: 0.62
Batch: 640; loss: 1.37; acc: 0.62
Batch: 660; loss: 1.12; acc: 0.77
Batch: 680; loss: 1.25; acc: 0.72
Batch: 700; loss: 1.29; acc: 0.66
Batch: 720; loss: 1.29; acc: 0.62
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.32; acc: 0.64
Batch: 780; loss: 1.19; acc: 0.73
Train Epoch over. train_loss: 1.25; train_accuracy: 0.67 

0.00015765066200401634
0.00015381832781713456
Batch: 0; loss: 1.19; acc: 0.7
Batch: 20; loss: 1.31; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.84
Batch: 60; loss: 1.21; acc: 0.7
Batch: 80; loss: 1.11; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.7
Batch: 120; loss: 1.21; acc: 0.67
Batch: 140; loss: 1.2; acc: 0.75
Val Epoch over. val_loss: 1.206768944005298; val_accuracy: 0.6930732484076433 

The current subspace-distance is: 0.00015381832781713456 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.38; acc: 0.61
Batch: 40; loss: 1.16; acc: 0.72
Batch: 60; loss: 1.15; acc: 0.75
Batch: 80; loss: 1.27; acc: 0.73
Batch: 100; loss: 1.23; acc: 0.69
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 1.18; acc: 0.73
Batch: 160; loss: 1.27; acc: 0.66
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.43; acc: 0.56
Batch: 220; loss: 1.21; acc: 0.66
Batch: 240; loss: 1.57; acc: 0.5
Batch: 260; loss: 1.35; acc: 0.56
Batch: 280; loss: 1.27; acc: 0.62
Batch: 300; loss: 1.13; acc: 0.72
Batch: 320; loss: 1.18; acc: 0.72
Batch: 340; loss: 1.29; acc: 0.62
Batch: 360; loss: 1.21; acc: 0.73
Batch: 380; loss: 1.18; acc: 0.69
Batch: 400; loss: 1.13; acc: 0.73
Batch: 420; loss: 1.35; acc: 0.64
Batch: 440; loss: 1.11; acc: 0.72
Batch: 460; loss: 1.26; acc: 0.59
Batch: 480; loss: 1.23; acc: 0.66
Batch: 500; loss: 1.19; acc: 0.7
Batch: 520; loss: 1.19; acc: 0.7
Batch: 540; loss: 1.34; acc: 0.53
Batch: 560; loss: 1.31; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.58
Batch: 600; loss: 1.09; acc: 0.75
Batch: 620; loss: 1.25; acc: 0.67
Batch: 640; loss: 1.15; acc: 0.69
Batch: 660; loss: 1.17; acc: 0.77
Batch: 680; loss: 1.15; acc: 0.67
Batch: 700; loss: 0.97; acc: 0.81
Batch: 720; loss: 1.35; acc: 0.62
Batch: 740; loss: 1.1; acc: 0.69
Batch: 760; loss: 1.26; acc: 0.67
Batch: 780; loss: 1.23; acc: 0.66
Train Epoch over. train_loss: 1.24; train_accuracy: 0.67 

0.00016409237287007272
0.00015913316747173667
Batch: 0; loss: 1.18; acc: 0.73
Batch: 20; loss: 1.3; acc: 0.56
Batch: 40; loss: 0.93; acc: 0.84
Batch: 60; loss: 1.2; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.7
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.15; acc: 0.77
Val Epoch over. val_loss: 1.189216306255122; val_accuracy: 0.6964570063694268 

The current subspace-distance is: 0.00015913316747173667 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.61
Batch: 40; loss: 1.34; acc: 0.56
Batch: 60; loss: 1.2; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.66
Batch: 100; loss: 1.18; acc: 0.69
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 1.27; acc: 0.64
Batch: 160; loss: 1.38; acc: 0.59
Batch: 180; loss: 1.35; acc: 0.59
Batch: 200; loss: 1.28; acc: 0.59
Batch: 220; loss: 1.13; acc: 0.75
Batch: 240; loss: 1.34; acc: 0.62
Batch: 260; loss: 1.2; acc: 0.62
Batch: 280; loss: 1.44; acc: 0.52
Batch: 300; loss: 1.22; acc: 0.7
Batch: 320; loss: 1.29; acc: 0.62
Batch: 340; loss: 1.31; acc: 0.69
Batch: 360; loss: 1.13; acc: 0.73
Batch: 380; loss: 1.21; acc: 0.64
Batch: 400; loss: 1.18; acc: 0.7
Batch: 420; loss: 1.36; acc: 0.67
Batch: 440; loss: 1.24; acc: 0.66
Batch: 460; loss: 1.2; acc: 0.64
Batch: 480; loss: 1.24; acc: 0.64
Batch: 500; loss: 1.36; acc: 0.53
Batch: 520; loss: 1.16; acc: 0.69
Batch: 540; loss: 1.39; acc: 0.55
Batch: 560; loss: 1.2; acc: 0.66
Batch: 580; loss: 1.25; acc: 0.64
Batch: 600; loss: 1.4; acc: 0.61
Batch: 620; loss: 1.12; acc: 0.7
Batch: 640; loss: 1.24; acc: 0.64
Batch: 660; loss: 1.24; acc: 0.64
Batch: 680; loss: 1.13; acc: 0.77
Batch: 700; loss: 1.15; acc: 0.78
Batch: 720; loss: 1.1; acc: 0.77
Batch: 740; loss: 1.25; acc: 0.62
Batch: 760; loss: 1.23; acc: 0.66
Batch: 780; loss: 1.22; acc: 0.67
Train Epoch over. train_loss: 1.23; train_accuracy: 0.67 

0.00016914756270125508
0.0001643426512600854
Batch: 0; loss: 1.18; acc: 0.7
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.84
Batch: 60; loss: 1.2; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.75
Val Epoch over. val_loss: 1.1868256296321844; val_accuracy: 0.6925756369426752 

The current subspace-distance is: 0.0001643426512600854 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.36; acc: 0.62
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.18; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.69
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.7
Batch: 140; loss: 1.29; acc: 0.64
Batch: 160; loss: 1.3; acc: 0.66
Batch: 180; loss: 1.15; acc: 0.72
Batch: 200; loss: 1.35; acc: 0.58
Batch: 220; loss: 1.11; acc: 0.72
Batch: 240; loss: 1.45; acc: 0.55
Batch: 260; loss: 1.13; acc: 0.77
Batch: 280; loss: 1.28; acc: 0.62
Batch: 300; loss: 1.1; acc: 0.72
Batch: 320; loss: 1.19; acc: 0.69
Batch: 340; loss: 1.29; acc: 0.59
Batch: 360; loss: 1.23; acc: 0.7
Batch: 380; loss: 1.34; acc: 0.62
Batch: 400; loss: 1.3; acc: 0.58
Batch: 420; loss: 1.24; acc: 0.73
Batch: 440; loss: 1.22; acc: 0.7
Batch: 460; loss: 1.33; acc: 0.56
Batch: 480; loss: 1.23; acc: 0.69
Batch: 500; loss: 1.21; acc: 0.7
Batch: 520; loss: 1.2; acc: 0.69
Batch: 540; loss: 1.43; acc: 0.53
Batch: 560; loss: 1.34; acc: 0.62
Batch: 580; loss: 1.27; acc: 0.64
Batch: 600; loss: 1.29; acc: 0.64
Batch: 620; loss: 1.21; acc: 0.72
Batch: 640; loss: 1.36; acc: 0.61
Batch: 660; loss: 0.97; acc: 0.8
Batch: 680; loss: 1.08; acc: 0.73
Batch: 700; loss: 1.07; acc: 0.84
Batch: 720; loss: 1.18; acc: 0.77
Batch: 740; loss: 1.2; acc: 0.73
Batch: 760; loss: 1.43; acc: 0.52
Batch: 780; loss: 1.13; acc: 0.77
Train Epoch over. train_loss: 1.23; train_accuracy: 0.67 

0.00016733529628254473
0.00016390015662182122
Batch: 0; loss: 1.18; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.83
Batch: 60; loss: 1.2; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.75
Val Epoch over. val_loss: 1.19339280864995; val_accuracy: 0.6886942675159236 

The current subspace-distance is: 0.00016390015662182122 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.25; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.7
Batch: 40; loss: 1.26; acc: 0.62
Batch: 60; loss: 1.23; acc: 0.73
Batch: 80; loss: 1.24; acc: 0.62
Batch: 100; loss: 1.3; acc: 0.64
Batch: 120; loss: 1.33; acc: 0.66
Batch: 140; loss: 1.34; acc: 0.58
Batch: 160; loss: 1.25; acc: 0.62
Batch: 180; loss: 1.25; acc: 0.64
Batch: 200; loss: 1.15; acc: 0.73
Batch: 220; loss: 1.33; acc: 0.59
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.26; acc: 0.58
Batch: 280; loss: 1.0; acc: 0.77
Batch: 300; loss: 1.23; acc: 0.7
Batch: 320; loss: 1.23; acc: 0.64
Batch: 340; loss: 1.23; acc: 0.67
Batch: 360; loss: 1.22; acc: 0.69
Batch: 380; loss: 1.26; acc: 0.66
Batch: 400; loss: 1.19; acc: 0.75
Batch: 420; loss: 1.17; acc: 0.7
Batch: 440; loss: 1.15; acc: 0.72
Batch: 460; loss: 1.11; acc: 0.75
Batch: 480; loss: 1.15; acc: 0.66
Batch: 500; loss: 1.19; acc: 0.8
Batch: 520; loss: 1.19; acc: 0.75
Batch: 540; loss: 1.14; acc: 0.75
Batch: 560; loss: 1.37; acc: 0.53
Batch: 580; loss: 1.09; acc: 0.75
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.75
Batch: 640; loss: 1.27; acc: 0.61
Batch: 660; loss: 1.18; acc: 0.72
Batch: 680; loss: 1.15; acc: 0.69
Batch: 700; loss: 1.1; acc: 0.72
Batch: 720; loss: 1.32; acc: 0.61
Batch: 740; loss: 1.23; acc: 0.64
Batch: 760; loss: 1.12; acc: 0.75
Batch: 780; loss: 1.22; acc: 0.64
Train Epoch over. train_loss: 1.23; train_accuracy: 0.67 

0.00016711681382730603
0.00016304517339449376
Batch: 0; loss: 1.17; acc: 0.72
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 0.92; acc: 0.83
Batch: 60; loss: 1.2; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.21; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.75
Val Epoch over. val_loss: 1.1810490177695159; val_accuracy: 0.6967555732484076 

The current subspace-distance is: 0.00016304517339449376 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.77
Batch: 20; loss: 1.34; acc: 0.58
Batch: 40; loss: 1.16; acc: 0.69
Batch: 60; loss: 1.18; acc: 0.72
Batch: 80; loss: 1.31; acc: 0.64
Batch: 100; loss: 1.26; acc: 0.59
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 1.11; acc: 0.77
Batch: 160; loss: 1.31; acc: 0.64
Batch: 180; loss: 1.33; acc: 0.61
Batch: 200; loss: 1.19; acc: 0.69
Batch: 220; loss: 1.24; acc: 0.58
Batch: 240; loss: 1.29; acc: 0.66
Batch: 260; loss: 1.13; acc: 0.75
Batch: 280; loss: 1.16; acc: 0.73
Batch: 300; loss: 1.19; acc: 0.78
Batch: 320; loss: 1.08; acc: 0.75
Batch: 340; loss: 1.24; acc: 0.7
Batch: 360; loss: 1.26; acc: 0.66
Batch: 380; loss: 1.27; acc: 0.61
Batch: 400; loss: 1.25; acc: 0.64
Batch: 420; loss: 1.14; acc: 0.67
Batch: 440; loss: 1.31; acc: 0.62
Batch: 460; loss: 1.29; acc: 0.58
Batch: 480; loss: 1.21; acc: 0.61
Batch: 500; loss: 1.13; acc: 0.64
Batch: 520; loss: 1.12; acc: 0.73
Batch: 540; loss: 1.19; acc: 0.69
Batch: 560; loss: 1.26; acc: 0.67
Batch: 580; loss: 1.28; acc: 0.64
Batch: 600; loss: 1.07; acc: 0.75
Batch: 620; loss: 1.21; acc: 0.67
Batch: 640; loss: 1.15; acc: 0.8
Batch: 660; loss: 1.23; acc: 0.66
Batch: 680; loss: 1.07; acc: 0.75
Batch: 700; loss: 1.11; acc: 0.66
Batch: 720; loss: 1.19; acc: 0.72
Batch: 740; loss: 1.1; acc: 0.69
Batch: 760; loss: 1.25; acc: 0.66
Batch: 780; loss: 1.19; acc: 0.69
Train Epoch over. train_loss: 1.23; train_accuracy: 0.67 

0.00017105662846006453
0.00016516221512574703
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.8
Batch: 60; loss: 1.19; acc: 0.73
Batch: 80; loss: 1.08; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.21; acc: 0.58
Batch: 140; loss: 1.13; acc: 0.78
Val Epoch over. val_loss: 1.183734915438731; val_accuracy: 0.6906847133757962 

The current subspace-distance is: 0.00016516221512574703 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.17; acc: 0.73
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 1.14; acc: 0.72
Batch: 60; loss: 1.08; acc: 0.75
Batch: 80; loss: 1.42; acc: 0.52
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 1.15; acc: 0.73
Batch: 160; loss: 1.08; acc: 0.77
Batch: 180; loss: 1.46; acc: 0.52
Batch: 200; loss: 1.17; acc: 0.64
Batch: 220; loss: 1.14; acc: 0.72
Batch: 240; loss: 1.25; acc: 0.69
Batch: 260; loss: 1.35; acc: 0.58
Batch: 280; loss: 1.42; acc: 0.55
Batch: 300; loss: 1.05; acc: 0.73
Batch: 320; loss: 1.07; acc: 0.75
Batch: 340; loss: 1.06; acc: 0.77
Batch: 360; loss: 1.12; acc: 0.64
Batch: 380; loss: 1.29; acc: 0.64
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 1.11; acc: 0.73
Batch: 440; loss: 1.34; acc: 0.59
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.23; acc: 0.66
Batch: 520; loss: 1.18; acc: 0.66
Batch: 540; loss: 1.24; acc: 0.66
Batch: 560; loss: 1.13; acc: 0.72
Batch: 580; loss: 1.28; acc: 0.66
Batch: 600; loss: 1.17; acc: 0.72
Batch: 620; loss: 1.5; acc: 0.44
Batch: 640; loss: 1.2; acc: 0.69
Batch: 660; loss: 1.27; acc: 0.66
Batch: 680; loss: 1.29; acc: 0.7
Batch: 700; loss: 1.26; acc: 0.64
Batch: 720; loss: 1.12; acc: 0.73
Batch: 740; loss: 1.25; acc: 0.66
Batch: 760; loss: 1.25; acc: 0.72
Batch: 780; loss: 1.25; acc: 0.64
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.00017052332987077534
0.00016540168144274503
Batch: 0; loss: 1.17; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.84
Batch: 60; loss: 1.21; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.24; acc: 0.64
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 1.16; acc: 0.73
Val Epoch over. val_loss: 1.1985500967426665; val_accuracy: 0.691281847133758 

The current subspace-distance is: 0.00016540168144274503 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.31; acc: 0.61
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 1.28; acc: 0.67
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 1.26; acc: 0.64
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 1.16; acc: 0.69
Batch: 160; loss: 1.15; acc: 0.73
Batch: 180; loss: 1.22; acc: 0.64
Batch: 200; loss: 1.23; acc: 0.75
Batch: 220; loss: 1.43; acc: 0.56
Batch: 240; loss: 1.3; acc: 0.66
Batch: 260; loss: 1.17; acc: 0.73
Batch: 280; loss: 1.06; acc: 0.78
Batch: 300; loss: 1.14; acc: 0.77
Batch: 320; loss: 1.14; acc: 0.72
Batch: 340; loss: 1.15; acc: 0.66
Batch: 360; loss: 1.24; acc: 0.67
Batch: 380; loss: 1.32; acc: 0.64
Batch: 400; loss: 1.29; acc: 0.64
Batch: 420; loss: 1.1; acc: 0.78
Batch: 440; loss: 1.25; acc: 0.67
Batch: 460; loss: 1.19; acc: 0.77
Batch: 480; loss: 1.32; acc: 0.67
Batch: 500; loss: 1.24; acc: 0.64
Batch: 520; loss: 1.22; acc: 0.61
Batch: 540; loss: 1.09; acc: 0.73
Batch: 560; loss: 1.22; acc: 0.64
Batch: 580; loss: 1.17; acc: 0.64
Batch: 600; loss: 1.22; acc: 0.67
Batch: 620; loss: 1.29; acc: 0.64
Batch: 640; loss: 1.21; acc: 0.75
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.1; acc: 0.77
Batch: 700; loss: 1.18; acc: 0.69
Batch: 720; loss: 1.34; acc: 0.66
Batch: 740; loss: 1.23; acc: 0.61
Batch: 760; loss: 1.21; acc: 0.72
Batch: 780; loss: 1.36; acc: 0.58
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.0001720776635920629
0.00016559138020966202
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.84
Batch: 60; loss: 1.2; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.64
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.77
Val Epoch over. val_loss: 1.1831520047916728; val_accuracy: 0.6922770700636943 

The current subspace-distance is: 0.00016559138020966202 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 1.14; acc: 0.69
Batch: 40; loss: 1.23; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.66
Batch: 80; loss: 1.2; acc: 0.67
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 1.19; acc: 0.69
Batch: 160; loss: 1.3; acc: 0.58
Batch: 180; loss: 1.17; acc: 0.72
Batch: 200; loss: 1.18; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.26; acc: 0.59
Batch: 260; loss: 1.22; acc: 0.66
Batch: 280; loss: 1.16; acc: 0.69
Batch: 300; loss: 1.14; acc: 0.73
Batch: 320; loss: 1.38; acc: 0.58
Batch: 340; loss: 1.16; acc: 0.66
Batch: 360; loss: 1.11; acc: 0.77
Batch: 380; loss: 1.19; acc: 0.69
Batch: 400; loss: 1.13; acc: 0.66
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 1.15; acc: 0.72
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 1.11; acc: 0.73
Batch: 500; loss: 1.24; acc: 0.64
Batch: 520; loss: 1.16; acc: 0.64
Batch: 540; loss: 1.16; acc: 0.7
Batch: 560; loss: 1.28; acc: 0.61
Batch: 580; loss: 1.07; acc: 0.78
Batch: 600; loss: 1.32; acc: 0.62
Batch: 620; loss: 1.34; acc: 0.59
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.09; acc: 0.75
Batch: 680; loss: 1.49; acc: 0.58
Batch: 700; loss: 1.04; acc: 0.78
Batch: 720; loss: 1.3; acc: 0.64
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.09; acc: 0.8
Batch: 780; loss: 1.17; acc: 0.7
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.00017141348507720977
0.00016486726235598326
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 0.93; acc: 0.84
Batch: 60; loss: 1.2; acc: 0.7
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.18; acc: 0.75
Val Epoch over. val_loss: 1.1905130144137486; val_accuracy: 0.6905851910828026 

The current subspace-distance is: 0.00016486726235598326 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.51; acc: 0.52
Batch: 20; loss: 1.26; acc: 0.64
Batch: 40; loss: 1.39; acc: 0.64
Batch: 60; loss: 1.33; acc: 0.56
Batch: 80; loss: 1.14; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.1; acc: 0.78
Batch: 140; loss: 1.14; acc: 0.66
Batch: 160; loss: 1.19; acc: 0.67
Batch: 180; loss: 1.0; acc: 0.8
Batch: 200; loss: 1.17; acc: 0.61
Batch: 220; loss: 1.27; acc: 0.66
Batch: 240; loss: 1.1; acc: 0.7
Batch: 260; loss: 1.27; acc: 0.66
Batch: 280; loss: 1.26; acc: 0.7
Batch: 300; loss: 1.1; acc: 0.7
Batch: 320; loss: 1.16; acc: 0.75
Batch: 340; loss: 1.14; acc: 0.72
Batch: 360; loss: 1.14; acc: 0.7
Batch: 380; loss: 1.58; acc: 0.53
Batch: 400; loss: 1.33; acc: 0.64
Batch: 420; loss: 1.0; acc: 0.81
Batch: 440; loss: 1.32; acc: 0.59
Batch: 460; loss: 1.23; acc: 0.69
Batch: 480; loss: 1.13; acc: 0.69
Batch: 500; loss: 1.11; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.73
Batch: 540; loss: 1.0; acc: 0.78
Batch: 560; loss: 1.15; acc: 0.73
Batch: 580; loss: 1.1; acc: 0.66
Batch: 600; loss: 1.27; acc: 0.69
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 1.2; acc: 0.67
Batch: 660; loss: 1.22; acc: 0.62
Batch: 680; loss: 1.34; acc: 0.62
Batch: 700; loss: 1.18; acc: 0.72
Batch: 720; loss: 1.18; acc: 0.64
Batch: 740; loss: 1.21; acc: 0.69
Batch: 760; loss: 1.22; acc: 0.7
Batch: 780; loss: 1.21; acc: 0.69
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.0001717280683806166
0.00016752767260186374
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.59
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 1.2; acc: 0.73
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.21; acc: 0.62
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.78
Val Epoch over. val_loss: 1.1790006388524534; val_accuracy: 0.6900875796178344 

The current subspace-distance is: 0.00016752767260186374 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 1.15; acc: 0.78
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.08; acc: 0.72
Batch: 100; loss: 1.18; acc: 0.69
Batch: 120; loss: 1.21; acc: 0.7
Batch: 140; loss: 1.36; acc: 0.53
Batch: 160; loss: 1.28; acc: 0.62
Batch: 180; loss: 1.09; acc: 0.67
Batch: 200; loss: 1.35; acc: 0.61
Batch: 220; loss: 1.23; acc: 0.66
Batch: 240; loss: 1.07; acc: 0.73
Batch: 260; loss: 1.19; acc: 0.67
Batch: 280; loss: 1.33; acc: 0.55
Batch: 300; loss: 1.07; acc: 0.8
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 1.35; acc: 0.58
Batch: 360; loss: 1.23; acc: 0.66
Batch: 380; loss: 1.28; acc: 0.58
Batch: 400; loss: 1.24; acc: 0.62
Batch: 420; loss: 0.97; acc: 0.8
Batch: 440; loss: 1.24; acc: 0.69
Batch: 460; loss: 1.06; acc: 0.78
Batch: 480; loss: 1.18; acc: 0.7
Batch: 500; loss: 1.2; acc: 0.73
Batch: 520; loss: 1.06; acc: 0.75
Batch: 540; loss: 1.37; acc: 0.56
Batch: 560; loss: 1.08; acc: 0.77
Batch: 580; loss: 1.2; acc: 0.66
Batch: 600; loss: 1.31; acc: 0.66
Batch: 620; loss: 1.25; acc: 0.62
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.24; acc: 0.67
Batch: 680; loss: 1.15; acc: 0.7
Batch: 700; loss: 1.2; acc: 0.64
Batch: 720; loss: 1.3; acc: 0.66
Batch: 740; loss: 1.08; acc: 0.75
Batch: 760; loss: 1.18; acc: 0.69
Batch: 780; loss: 1.22; acc: 0.69
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.00017369913985021412
0.0001668431650614366
Batch: 0; loss: 1.15; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 0.91; acc: 0.81
Batch: 60; loss: 1.19; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.21; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.58
Batch: 140; loss: 1.15; acc: 0.75
Val Epoch over. val_loss: 1.1737564824948645; val_accuracy: 0.6950636942675159 

The current subspace-distance is: 0.0001668431650614366 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.33; acc: 0.55
Batch: 20; loss: 1.25; acc: 0.66
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.14; acc: 0.77
Batch: 80; loss: 1.16; acc: 0.66
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.12; acc: 0.73
Batch: 140; loss: 1.2; acc: 0.64
Batch: 160; loss: 1.31; acc: 0.59
Batch: 180; loss: 1.1; acc: 0.72
Batch: 200; loss: 1.06; acc: 0.77
Batch: 220; loss: 1.3; acc: 0.62
Batch: 240; loss: 1.17; acc: 0.64
Batch: 260; loss: 1.24; acc: 0.64
Batch: 280; loss: 1.02; acc: 0.75
Batch: 300; loss: 1.3; acc: 0.66
Batch: 320; loss: 1.2; acc: 0.69
Batch: 340; loss: 1.23; acc: 0.58
Batch: 360; loss: 1.31; acc: 0.58
Batch: 380; loss: 1.16; acc: 0.7
Batch: 400; loss: 1.15; acc: 0.7
Batch: 420; loss: 1.24; acc: 0.62
Batch: 440; loss: 1.26; acc: 0.61
Batch: 460; loss: 1.31; acc: 0.61
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.24; acc: 0.69
Batch: 520; loss: 1.16; acc: 0.72
Batch: 540; loss: 1.19; acc: 0.7
Batch: 560; loss: 1.15; acc: 0.73
Batch: 580; loss: 1.16; acc: 0.67
Batch: 600; loss: 1.24; acc: 0.61
Batch: 620; loss: 1.17; acc: 0.67
Batch: 640; loss: 1.26; acc: 0.66
Batch: 660; loss: 1.17; acc: 0.7
Batch: 680; loss: 1.08; acc: 0.75
Batch: 700; loss: 1.1; acc: 0.72
Batch: 720; loss: 1.36; acc: 0.64
Batch: 740; loss: 1.22; acc: 0.69
Batch: 760; loss: 1.27; acc: 0.53
Batch: 780; loss: 1.14; acc: 0.7
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.0001716870319796726
0.0001671969221206382
Batch: 0; loss: 1.15; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 0.91; acc: 0.84
Batch: 60; loss: 1.19; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.72
Batch: 100; loss: 1.22; acc: 0.61
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.77
Val Epoch over. val_loss: 1.1750915437746958; val_accuracy: 0.6922770700636943 

The current subspace-distance is: 0.0001671969221206382 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.4; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 1.32; acc: 0.56
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.27; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.09; acc: 0.78
Batch: 140; loss: 1.2; acc: 0.7
Batch: 160; loss: 1.12; acc: 0.72
Batch: 180; loss: 1.25; acc: 0.69
Batch: 200; loss: 1.24; acc: 0.72
Batch: 220; loss: 1.06; acc: 0.81
Batch: 240; loss: 1.12; acc: 0.7
Batch: 260; loss: 1.25; acc: 0.66
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.72
Batch: 320; loss: 1.19; acc: 0.72
Batch: 340; loss: 1.31; acc: 0.62
Batch: 360; loss: 1.22; acc: 0.58
Batch: 380; loss: 1.09; acc: 0.7
Batch: 400; loss: 1.03; acc: 0.78
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 1.17; acc: 0.67
Batch: 460; loss: 1.19; acc: 0.67
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 1.21; acc: 0.61
Batch: 520; loss: 1.14; acc: 0.66
Batch: 540; loss: 1.33; acc: 0.59
Batch: 560; loss: 1.13; acc: 0.77
Batch: 580; loss: 1.34; acc: 0.59
Batch: 600; loss: 1.2; acc: 0.75
Batch: 620; loss: 1.15; acc: 0.67
Batch: 640; loss: 1.15; acc: 0.72
Batch: 660; loss: 1.17; acc: 0.73
Batch: 680; loss: 1.27; acc: 0.61
Batch: 700; loss: 1.29; acc: 0.64
Batch: 720; loss: 1.29; acc: 0.62
Batch: 740; loss: 1.07; acc: 0.75
Batch: 760; loss: 1.31; acc: 0.67
Batch: 780; loss: 1.08; acc: 0.7
Train Epoch over. train_loss: 1.21; train_accuracy: 0.67 

0.00017447280697524548
0.0001687469193711877
Batch: 0; loss: 1.15; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.56
Batch: 40; loss: 0.91; acc: 0.8
Batch: 60; loss: 1.19; acc: 0.73
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.21; acc: 0.59
Batch: 140; loss: 1.13; acc: 0.75
Val Epoch over. val_loss: 1.1721918852465927; val_accuracy: 0.6903861464968153 

The current subspace-distance is: 0.0001687469193711877 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_5_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.4122939498231974

The number of parameters is: 278681

The number of individual parameters is:

28
448
28
28
41
49364
41
41
82
144566
82
82
64
78720
64
64
4096
64
640
10
64
64

nonzero elements in E: 55736196
elements in E: 55736200
fraction nonzero: 0.9999999282333564
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.38; acc: 0.16
Batch: 20; loss: 2.02; acc: 0.33
Batch: 40; loss: 1.95; acc: 0.38
Batch: 60; loss: 1.85; acc: 0.39
Batch: 80; loss: 1.84; acc: 0.36
Batch: 100; loss: 1.78; acc: 0.53
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.6; acc: 0.58
Batch: 160; loss: 1.63; acc: 0.53
Batch: 180; loss: 1.64; acc: 0.48
Batch: 200; loss: 1.62; acc: 0.5
Batch: 220; loss: 1.48; acc: 0.66
Batch: 240; loss: 1.54; acc: 0.61
Batch: 260; loss: 1.49; acc: 0.56
Batch: 280; loss: 1.44; acc: 0.62
Batch: 300; loss: 1.47; acc: 0.64
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.48; acc: 0.62
Batch: 360; loss: 1.41; acc: 0.69
Batch: 380; loss: 1.52; acc: 0.58
Batch: 400; loss: 1.44; acc: 0.64
Batch: 420; loss: 1.42; acc: 0.58
Batch: 440; loss: 1.39; acc: 0.59
Batch: 460; loss: 1.38; acc: 0.7
Batch: 480; loss: 1.45; acc: 0.67
Batch: 500; loss: 1.49; acc: 0.61
Batch: 520; loss: 1.32; acc: 0.7
Batch: 540; loss: 1.34; acc: 0.75
Batch: 560; loss: 1.39; acc: 0.67
Batch: 580; loss: 1.47; acc: 0.59
Batch: 600; loss: 1.35; acc: 0.69
Batch: 620; loss: 1.46; acc: 0.59
Batch: 640; loss: 1.34; acc: 0.7
Batch: 660; loss: 1.31; acc: 0.66
Batch: 680; loss: 1.32; acc: 0.75
Batch: 700; loss: 1.34; acc: 0.7
Batch: 720; loss: 1.39; acc: 0.66
Batch: 740; loss: 1.36; acc: 0.73
Batch: 760; loss: 1.35; acc: 0.67
Batch: 780; loss: 1.27; acc: 0.69
Train Epoch over. train_loss: 1.5; train_accuracy: 0.6 

5.947218232904561e-05
5.522229912457988e-05
Batch: 0; loss: 1.41; acc: 0.66
Batch: 20; loss: 1.37; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.86
Batch: 60; loss: 1.24; acc: 0.78
Batch: 80; loss: 1.19; acc: 0.86
Batch: 100; loss: 1.31; acc: 0.78
Batch: 120; loss: 1.38; acc: 0.67
Batch: 140; loss: 1.21; acc: 0.8
Val Epoch over. val_loss: 1.2846988826800303; val_accuracy: 0.7274084394904459 

The current subspace-distance is: 5.522229912457988e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.72
Batch: 20; loss: 1.35; acc: 0.66
Batch: 40; loss: 1.38; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.67
Batch: 80; loss: 1.28; acc: 0.77
Batch: 100; loss: 1.3; acc: 0.67
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 1.33; acc: 0.72
Batch: 160; loss: 1.33; acc: 0.73
Batch: 180; loss: 1.29; acc: 0.7
Batch: 200; loss: 1.25; acc: 0.72
Batch: 220; loss: 1.28; acc: 0.73
Batch: 240; loss: 1.21; acc: 0.78
Batch: 260; loss: 1.27; acc: 0.77
Batch: 280; loss: 1.11; acc: 0.84
Batch: 300; loss: 1.18; acc: 0.81
Batch: 320; loss: 1.27; acc: 0.69
Batch: 340; loss: 1.32; acc: 0.66
Batch: 360; loss: 1.21; acc: 0.75
Batch: 380; loss: 1.32; acc: 0.64
Batch: 400; loss: 1.25; acc: 0.73
Batch: 420; loss: 1.29; acc: 0.61
Batch: 440; loss: 1.13; acc: 0.78
Batch: 460; loss: 1.15; acc: 0.77
Batch: 480; loss: 1.25; acc: 0.72
Batch: 500; loss: 1.24; acc: 0.72
Batch: 520; loss: 1.26; acc: 0.72
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.2; acc: 0.75
Batch: 580; loss: 1.16; acc: 0.84
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.1; acc: 0.81
Batch: 640; loss: 1.13; acc: 0.8
Batch: 660; loss: 1.17; acc: 0.7
Batch: 680; loss: 1.24; acc: 0.72
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.23; acc: 0.73
Batch: 740; loss: 1.18; acc: 0.8
Batch: 760; loss: 1.25; acc: 0.67
Batch: 780; loss: 1.12; acc: 0.77
Train Epoch over. train_loss: 1.23; train_accuracy: 0.74 

8.340276690432802e-05
7.83231807872653e-05
Batch: 0; loss: 1.2; acc: 0.78
Batch: 20; loss: 1.24; acc: 0.72
Batch: 40; loss: 0.9; acc: 0.92
Batch: 60; loss: 1.08; acc: 0.81
Batch: 80; loss: 0.94; acc: 0.88
Batch: 100; loss: 1.12; acc: 0.88
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.83
Val Epoch over. val_loss: 1.1163207049582415; val_accuracy: 0.7856289808917197 

The current subspace-distance is: 7.83231807872653e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.86
Batch: 40; loss: 1.26; acc: 0.75
Batch: 60; loss: 1.24; acc: 0.75
Batch: 80; loss: 1.19; acc: 0.78
Batch: 100; loss: 1.27; acc: 0.77
Batch: 120; loss: 1.18; acc: 0.75
Batch: 140; loss: 1.06; acc: 0.78
Batch: 160; loss: 1.07; acc: 0.83
Batch: 180; loss: 1.12; acc: 0.83
Batch: 200; loss: 1.09; acc: 0.8
Batch: 220; loss: 1.08; acc: 0.88
Batch: 240; loss: 1.16; acc: 0.75
Batch: 260; loss: 0.96; acc: 0.92
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.05; acc: 0.83
Batch: 320; loss: 1.22; acc: 0.69
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.09; acc: 0.75
Batch: 380; loss: 1.12; acc: 0.72
Batch: 400; loss: 1.09; acc: 0.78
Batch: 420; loss: 1.1; acc: 0.78
Batch: 440; loss: 1.15; acc: 0.77
Batch: 460; loss: 1.12; acc: 0.78
Batch: 480; loss: 1.2; acc: 0.72
Batch: 500; loss: 1.11; acc: 0.73
Batch: 520; loss: 1.08; acc: 0.78
Batch: 540; loss: 0.98; acc: 0.84
Batch: 560; loss: 1.14; acc: 0.78
Batch: 580; loss: 1.07; acc: 0.81
Batch: 600; loss: 1.11; acc: 0.81
Batch: 620; loss: 1.04; acc: 0.8
Batch: 640; loss: 1.13; acc: 0.73
Batch: 660; loss: 1.17; acc: 0.69
Batch: 680; loss: 1.16; acc: 0.78
Batch: 700; loss: 1.18; acc: 0.73
Batch: 720; loss: 1.1; acc: 0.73
Batch: 740; loss: 1.15; acc: 0.78
Batch: 760; loss: 1.03; acc: 0.81
Batch: 780; loss: 1.04; acc: 0.78
Train Epoch over. train_loss: 1.12; train_accuracy: 0.77 

9.820663399295881e-05
9.49898676481098e-05
Batch: 0; loss: 1.09; acc: 0.78
Batch: 20; loss: 1.16; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.91
Batch: 60; loss: 0.98; acc: 0.84
Batch: 80; loss: 0.85; acc: 0.88
Batch: 100; loss: 0.99; acc: 0.89
Batch: 120; loss: 1.13; acc: 0.75
Batch: 140; loss: 1.03; acc: 0.8
Val Epoch over. val_loss: 1.0275335888953725; val_accuracy: 0.7998606687898089 

The current subspace-distance is: 9.49898676481098e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.8
Batch: 20; loss: 1.18; acc: 0.72
Batch: 40; loss: 0.99; acc: 0.8
Batch: 60; loss: 1.11; acc: 0.78
Batch: 80; loss: 1.02; acc: 0.84
Batch: 100; loss: 1.02; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.83
Batch: 140; loss: 1.07; acc: 0.84
Batch: 160; loss: 1.03; acc: 0.81
Batch: 180; loss: 1.1; acc: 0.77
Batch: 200; loss: 1.18; acc: 0.72
Batch: 220; loss: 0.94; acc: 0.91
Batch: 240; loss: 1.12; acc: 0.73
Batch: 260; loss: 1.03; acc: 0.78
Batch: 280; loss: 0.93; acc: 0.86
Batch: 300; loss: 1.06; acc: 0.8
Batch: 320; loss: 1.06; acc: 0.75
Batch: 340; loss: 1.07; acc: 0.8
Batch: 360; loss: 0.98; acc: 0.88
Batch: 380; loss: 0.99; acc: 0.77
Batch: 400; loss: 1.08; acc: 0.78
Batch: 420; loss: 1.12; acc: 0.75
Batch: 440; loss: 0.95; acc: 0.83
Batch: 460; loss: 0.98; acc: 0.84
Batch: 480; loss: 1.09; acc: 0.78
Batch: 500; loss: 0.97; acc: 0.81
Batch: 520; loss: 1.01; acc: 0.81
Batch: 540; loss: 0.93; acc: 0.86
Batch: 560; loss: 1.04; acc: 0.75
Batch: 580; loss: 0.94; acc: 0.83
Batch: 600; loss: 0.95; acc: 0.83
Batch: 620; loss: 0.99; acc: 0.86
Batch: 640; loss: 1.0; acc: 0.8
Batch: 660; loss: 1.06; acc: 0.69
Batch: 680; loss: 1.05; acc: 0.8
Batch: 700; loss: 0.99; acc: 0.81
Batch: 720; loss: 0.96; acc: 0.84
Batch: 740; loss: 1.02; acc: 0.8
Batch: 760; loss: 1.08; acc: 0.75
Batch: 780; loss: 0.98; acc: 0.81
Train Epoch over. train_loss: 1.04; train_accuracy: 0.79 

0.00011423786781961098
0.00011023995466530323
Batch: 0; loss: 1.03; acc: 0.81
Batch: 20; loss: 1.13; acc: 0.78
Batch: 40; loss: 0.7; acc: 0.94
Batch: 60; loss: 0.94; acc: 0.83
Batch: 80; loss: 0.79; acc: 0.89
Batch: 100; loss: 0.89; acc: 0.92
Batch: 120; loss: 1.07; acc: 0.78
Batch: 140; loss: 0.95; acc: 0.81
Val Epoch over. val_loss: 0.9502199273200551; val_accuracy: 0.8248407643312102 

The current subspace-distance is: 0.00011023995466530323 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.94; acc: 0.84
Batch: 40; loss: 0.91; acc: 0.84
Batch: 60; loss: 1.04; acc: 0.8
Batch: 80; loss: 0.98; acc: 0.8
Batch: 100; loss: 0.97; acc: 0.81
Batch: 120; loss: 1.15; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.81
Batch: 160; loss: 0.96; acc: 0.86
Batch: 180; loss: 0.91; acc: 0.88
Batch: 200; loss: 0.79; acc: 0.91
Batch: 220; loss: 0.97; acc: 0.78
Batch: 240; loss: 1.03; acc: 0.78
Batch: 260; loss: 0.99; acc: 0.75
Batch: 280; loss: 0.94; acc: 0.81
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 0.95; acc: 0.81
Batch: 340; loss: 0.88; acc: 0.86
Batch: 360; loss: 1.03; acc: 0.81
Batch: 380; loss: 1.05; acc: 0.77
Batch: 400; loss: 0.95; acc: 0.84
Batch: 420; loss: 0.97; acc: 0.83
Batch: 440; loss: 0.89; acc: 0.84
Batch: 460; loss: 0.91; acc: 0.89
Batch: 480; loss: 1.09; acc: 0.75
Batch: 500; loss: 0.93; acc: 0.89
Batch: 520; loss: 1.05; acc: 0.78
Batch: 540; loss: 0.87; acc: 0.91
Batch: 560; loss: 0.73; acc: 0.94
Batch: 580; loss: 1.05; acc: 0.78
Batch: 600; loss: 1.0; acc: 0.77
Batch: 620; loss: 0.95; acc: 0.8
Batch: 640; loss: 0.78; acc: 0.88
Batch: 660; loss: 0.91; acc: 0.84
Batch: 680; loss: 0.96; acc: 0.77
Batch: 700; loss: 1.0; acc: 0.81
Batch: 720; loss: 0.93; acc: 0.86
Batch: 740; loss: 0.86; acc: 0.86
Batch: 760; loss: 0.88; acc: 0.8
Batch: 780; loss: 0.95; acc: 0.81
Train Epoch over. train_loss: 0.97; train_accuracy: 0.81 

0.00012930535012856126
0.0001269901404157281
Batch: 0; loss: 0.95; acc: 0.81
Batch: 20; loss: 1.06; acc: 0.78
Batch: 40; loss: 0.65; acc: 0.97
Batch: 60; loss: 0.89; acc: 0.84
Batch: 80; loss: 0.72; acc: 0.86
Batch: 100; loss: 0.82; acc: 0.94
Batch: 120; loss: 0.99; acc: 0.8
Batch: 140; loss: 0.85; acc: 0.84
Val Epoch over. val_loss: 0.8807905016431383; val_accuracy: 0.8384753184713376 

The current subspace-distance is: 0.0001269901404157281 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.75
Batch: 20; loss: 0.92; acc: 0.81
Batch: 40; loss: 0.95; acc: 0.81
Batch: 60; loss: 0.95; acc: 0.78
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 0.9; acc: 0.84
Batch: 120; loss: 0.98; acc: 0.83
Batch: 140; loss: 1.07; acc: 0.73
Batch: 160; loss: 0.92; acc: 0.84
Batch: 180; loss: 0.94; acc: 0.83
Batch: 200; loss: 0.89; acc: 0.83
Batch: 220; loss: 0.97; acc: 0.78
Batch: 240; loss: 0.97; acc: 0.81
Batch: 260; loss: 0.89; acc: 0.84
Batch: 280; loss: 0.89; acc: 0.83
Batch: 300; loss: 1.12; acc: 0.66
Batch: 320; loss: 0.96; acc: 0.8
Batch: 340; loss: 0.8; acc: 0.86
Batch: 360; loss: 0.88; acc: 0.8
Batch: 380; loss: 0.84; acc: 0.86
Batch: 400; loss: 0.98; acc: 0.78
Batch: 420; loss: 0.87; acc: 0.83
Batch: 440; loss: 0.89; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.84
Batch: 480; loss: 0.89; acc: 0.75
Batch: 500; loss: 0.92; acc: 0.81
Batch: 520; loss: 0.89; acc: 0.83
Batch: 540; loss: 0.86; acc: 0.86
Batch: 560; loss: 0.86; acc: 0.8
Batch: 580; loss: 0.95; acc: 0.75
Batch: 600; loss: 0.96; acc: 0.75
Batch: 620; loss: 0.74; acc: 0.88
Batch: 640; loss: 0.87; acc: 0.84
Batch: 660; loss: 0.82; acc: 0.92
Batch: 680; loss: 0.97; acc: 0.78
Batch: 700; loss: 1.03; acc: 0.72
Batch: 720; loss: 0.87; acc: 0.77
Batch: 740; loss: 0.86; acc: 0.78
Batch: 760; loss: 0.76; acc: 0.89
Batch: 780; loss: 0.95; acc: 0.77
Train Epoch over. train_loss: 0.9; train_accuracy: 0.82 

0.00014354560698848218
0.0001395162398694083
Batch: 0; loss: 0.84; acc: 0.86
Batch: 20; loss: 1.0; acc: 0.77
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.74; acc: 0.92
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.76; acc: 0.89
Val Epoch over. val_loss: 0.816841960712603; val_accuracy: 0.8460390127388535 

The current subspace-distance is: 0.0001395162398694083 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.84
Batch: 20; loss: 1.01; acc: 0.8
Batch: 40; loss: 0.84; acc: 0.86
Batch: 60; loss: 0.76; acc: 0.91
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.88; acc: 0.81
Batch: 160; loss: 0.75; acc: 0.88
Batch: 180; loss: 0.82; acc: 0.91
Batch: 200; loss: 0.92; acc: 0.77
Batch: 220; loss: 0.77; acc: 0.88
Batch: 240; loss: 0.74; acc: 0.86
Batch: 260; loss: 0.79; acc: 0.91
Batch: 280; loss: 0.81; acc: 0.84
Batch: 300; loss: 0.89; acc: 0.86
Batch: 320; loss: 0.81; acc: 0.88
Batch: 340; loss: 0.86; acc: 0.8
Batch: 360; loss: 0.93; acc: 0.83
Batch: 380; loss: 1.0; acc: 0.73
Batch: 400; loss: 0.81; acc: 0.8
Batch: 420; loss: 0.97; acc: 0.77
Batch: 440; loss: 0.78; acc: 0.89
Batch: 460; loss: 0.85; acc: 0.77
Batch: 480; loss: 0.94; acc: 0.8
Batch: 500; loss: 0.94; acc: 0.81
Batch: 520; loss: 0.93; acc: 0.8
Batch: 540; loss: 0.92; acc: 0.77
Batch: 560; loss: 0.7; acc: 0.88
Batch: 580; loss: 0.95; acc: 0.8
Batch: 600; loss: 0.98; acc: 0.72
Batch: 620; loss: 0.85; acc: 0.75
Batch: 640; loss: 0.96; acc: 0.72
Batch: 660; loss: 0.84; acc: 0.84
Batch: 680; loss: 0.94; acc: 0.78
Batch: 700; loss: 0.96; acc: 0.78
Batch: 720; loss: 0.72; acc: 0.86
Batch: 740; loss: 0.88; acc: 0.83
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.82; acc: 0.81
Train Epoch over. train_loss: 0.85; train_accuracy: 0.82 

0.00015922574675641954
0.0001562456600368023
Batch: 0; loss: 0.77; acc: 0.88
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.91
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.59; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.91
Val Epoch over. val_loss: 0.7640488424878211; val_accuracy: 0.8461385350318471 

The current subspace-distance is: 0.0001562456600368023 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.91; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 0.77; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.89; acc: 0.78
Batch: 160; loss: 0.72; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.92
Batch: 200; loss: 0.94; acc: 0.78
Batch: 220; loss: 0.75; acc: 0.84
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.8; acc: 0.86
Batch: 280; loss: 0.73; acc: 0.84
Batch: 300; loss: 0.7; acc: 0.86
Batch: 320; loss: 0.79; acc: 0.83
Batch: 340; loss: 0.82; acc: 0.78
Batch: 360; loss: 0.78; acc: 0.88
Batch: 380; loss: 0.68; acc: 0.89
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.8; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.88
Batch: 500; loss: 0.75; acc: 0.86
Batch: 520; loss: 0.77; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.83
Batch: 560; loss: 0.69; acc: 0.89
Batch: 580; loss: 1.01; acc: 0.72
Batch: 600; loss: 0.86; acc: 0.81
Batch: 620; loss: 0.79; acc: 0.83
Batch: 640; loss: 0.82; acc: 0.83
Batch: 660; loss: 0.84; acc: 0.77
Batch: 680; loss: 0.87; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.81
Batch: 720; loss: 0.92; acc: 0.83
Batch: 740; loss: 0.77; acc: 0.83
Batch: 760; loss: 0.92; acc: 0.75
Batch: 780; loss: 0.92; acc: 0.78
Train Epoch over. train_loss: 0.8; train_accuracy: 0.83 

0.00017203415336553007
0.00016765408508945256
Batch: 0; loss: 0.71; acc: 0.89
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.95
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.91
Val Epoch over. val_loss: 0.7212133439862805; val_accuracy: 0.8577826433121019 

The current subspace-distance is: 0.00016765408508945256 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.73; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.86
Batch: 100; loss: 0.93; acc: 0.78
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.76; acc: 0.88
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.76; acc: 0.83
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.72; acc: 0.8
Batch: 260; loss: 0.82; acc: 0.75
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.72; acc: 0.88
Batch: 320; loss: 0.71; acc: 0.81
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.78
Batch: 380; loss: 0.78; acc: 0.81
Batch: 400; loss: 0.59; acc: 0.92
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.74; acc: 0.81
Batch: 460; loss: 0.86; acc: 0.8
Batch: 480; loss: 0.82; acc: 0.81
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 0.71; acc: 0.86
Batch: 560; loss: 0.89; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.86; acc: 0.73
Batch: 620; loss: 0.98; acc: 0.77
Batch: 640; loss: 0.84; acc: 0.8
Batch: 660; loss: 0.86; acc: 0.78
Batch: 680; loss: 0.76; acc: 0.81
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 0.64; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.95
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.77; train_accuracy: 0.83 

0.00018606918456498533
0.00017846949049271643
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.92
Val Epoch over. val_loss: 0.6874546296657271; val_accuracy: 0.8564888535031847 

The current subspace-distance is: 0.00017846949049271643 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.89
Batch: 140; loss: 0.92; acc: 0.8
Batch: 160; loss: 0.83; acc: 0.75
Batch: 180; loss: 0.73; acc: 0.86
Batch: 200; loss: 0.76; acc: 0.83
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.85; acc: 0.78
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.83
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.81; acc: 0.81
Batch: 340; loss: 0.84; acc: 0.8
Batch: 360; loss: 0.77; acc: 0.86
Batch: 380; loss: 0.81; acc: 0.8
Batch: 400; loss: 0.76; acc: 0.83
Batch: 420; loss: 0.65; acc: 0.88
Batch: 440; loss: 0.68; acc: 0.91
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 0.75; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.8; acc: 0.8
Batch: 600; loss: 0.77; acc: 0.81
Batch: 620; loss: 0.71; acc: 0.88
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.8; acc: 0.81
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.86; acc: 0.77
Train Epoch over. train_loss: 0.75; train_accuracy: 0.83 

0.0001936043699970469
0.00018688848649617285
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.73
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.92
Val Epoch over. val_loss: 0.6795940919286886; val_accuracy: 0.8519108280254777 

The current subspace-distance is: 0.00018688848649617285 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.96; acc: 0.73
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.84; acc: 0.72
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 0.68; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.68; acc: 0.88
Batch: 160; loss: 0.91; acc: 0.77
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.78; acc: 0.83
Batch: 260; loss: 0.73; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.91
Batch: 300; loss: 0.75; acc: 0.86
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.86
Batch: 360; loss: 0.75; acc: 0.77
Batch: 380; loss: 0.92; acc: 0.77
Batch: 400; loss: 0.69; acc: 0.88
Batch: 420; loss: 0.72; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.92; acc: 0.75
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.92
Batch: 560; loss: 0.69; acc: 0.89
Batch: 580; loss: 0.88; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.88
Batch: 660; loss: 0.71; acc: 0.81
Batch: 680; loss: 0.79; acc: 0.84
Batch: 700; loss: 0.75; acc: 0.8
Batch: 720; loss: 0.55; acc: 0.92
Batch: 740; loss: 0.65; acc: 0.89
Batch: 760; loss: 0.73; acc: 0.86
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.0001996304199565202
0.0001944537361850962
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.73
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6676103820087044; val_accuracy: 0.8548964968152867 

The current subspace-distance is: 0.0001944537361850962 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.82; acc: 0.8
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.91
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.84; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.83; acc: 0.83
Batch: 220; loss: 0.74; acc: 0.77
Batch: 240; loss: 0.72; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.79; acc: 0.75
Batch: 320; loss: 0.73; acc: 0.83
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.73; acc: 0.86
Batch: 440; loss: 0.8; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.84
Batch: 480; loss: 0.74; acc: 0.81
Batch: 500; loss: 0.78; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.89
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.76; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.89
Batch: 620; loss: 0.68; acc: 0.88
Batch: 640; loss: 0.93; acc: 0.72
Batch: 660; loss: 0.71; acc: 0.81
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.7; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.86
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.0001980639499379322
0.000196105771465227
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6615192611126384; val_accuracy: 0.854796974522293 

The current subspace-distance is: 0.000196105771465227 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.91
Batch: 180; loss: 0.76; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.71; acc: 0.83
Batch: 260; loss: 0.88; acc: 0.75
Batch: 280; loss: 0.8; acc: 0.8
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.91
Batch: 360; loss: 0.87; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.88
Batch: 400; loss: 0.64; acc: 0.89
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.87; acc: 0.75
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.8; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.94
Batch: 580; loss: 0.77; acc: 0.78
Batch: 600; loss: 0.77; acc: 0.81
Batch: 620; loss: 0.7; acc: 0.89
Batch: 640; loss: 0.67; acc: 0.92
Batch: 660; loss: 0.7; acc: 0.88
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.67; acc: 0.91
Batch: 720; loss: 0.69; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.94
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.00020171215874142945
0.00019505250384099782
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.91
Val Epoch over. val_loss: 0.656587305911787; val_accuracy: 0.8515127388535032 

The current subspace-distance is: 0.00019505250384099782 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.78; acc: 0.72
Batch: 80; loss: 0.67; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.73; acc: 0.86
Batch: 240; loss: 0.69; acc: 0.89
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.82; acc: 0.77
Batch: 300; loss: 0.64; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.86
Batch: 340; loss: 0.86; acc: 0.8
Batch: 360; loss: 0.77; acc: 0.84
Batch: 380; loss: 0.82; acc: 0.78
Batch: 400; loss: 0.79; acc: 0.78
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.9; acc: 0.7
Batch: 480; loss: 0.74; acc: 0.84
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.86
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.89
Batch: 660; loss: 0.66; acc: 0.88
Batch: 680; loss: 0.75; acc: 0.8
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.93; acc: 0.73
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.58; acc: 0.91
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.0002049956819973886
0.00019879653700627387
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6582079100760685; val_accuracy: 0.8545979299363057 

The current subspace-distance is: 0.00019879653700627387 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.88; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.88
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.8; acc: 0.83
Batch: 200; loss: 0.78; acc: 0.81
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.89
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.92
Batch: 300; loss: 0.79; acc: 0.86
Batch: 320; loss: 0.76; acc: 0.88
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 0.58; acc: 0.91
Batch: 380; loss: 0.56; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.88
Batch: 460; loss: 0.83; acc: 0.73
Batch: 480; loss: 0.66; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.83; acc: 0.75
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.82; acc: 0.88
Batch: 600; loss: 0.63; acc: 0.92
Batch: 620; loss: 0.74; acc: 0.78
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.77
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.65; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020624493481591344
0.0001995219208765775
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6543042188996722; val_accuracy: 0.8500199044585988 

The current subspace-distance is: 0.0001995219208765775 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.6; acc: 0.89
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.69; acc: 0.88
Batch: 80; loss: 0.86; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.81
Batch: 120; loss: 0.6; acc: 0.91
Batch: 140; loss: 0.62; acc: 0.89
Batch: 160; loss: 0.85; acc: 0.78
Batch: 180; loss: 0.65; acc: 0.88
Batch: 200; loss: 0.71; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.75
Batch: 260; loss: 0.62; acc: 0.91
Batch: 280; loss: 0.76; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.58; acc: 0.91
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.79; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.91
Batch: 500; loss: 0.66; acc: 0.83
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.89
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.85; acc: 0.75
Batch: 640; loss: 0.94; acc: 0.78
Batch: 660; loss: 0.67; acc: 0.86
Batch: 680; loss: 0.78; acc: 0.86
Batch: 700; loss: 0.66; acc: 0.89
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.87; acc: 0.75
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.69; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.84 

0.00021173983986955136
0.0002059765683952719
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6430791449394955; val_accuracy: 0.8535031847133758 

The current subspace-distance is: 0.0002059765683952719 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.81; acc: 0.81
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.72; acc: 0.84
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.85; acc: 0.77
Batch: 260; loss: 0.63; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.81; acc: 0.84
Batch: 360; loss: 0.75; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.94
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.78; acc: 0.81
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.65; acc: 0.92
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.76; acc: 0.8
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.76; acc: 0.81
Batch: 680; loss: 0.71; acc: 0.88
Batch: 700; loss: 0.73; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.88
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.74; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00021264368842821568
0.00020611299260053784
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6452998592975033; val_accuracy: 0.852109872611465 

The current subspace-distance is: 0.00020611299260053784 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.81
Batch: 140; loss: 0.67; acc: 0.78
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.64; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.77
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.75; acc: 0.83
Batch: 320; loss: 0.88; acc: 0.73
Batch: 340; loss: 0.61; acc: 0.89
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.88
Batch: 420; loss: 0.84; acc: 0.81
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.87; acc: 0.77
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.79; acc: 0.81
Batch: 600; loss: 0.67; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.91
Batch: 640; loss: 0.7; acc: 0.86
Batch: 660; loss: 0.75; acc: 0.8
Batch: 680; loss: 0.74; acc: 0.89
Batch: 700; loss: 0.81; acc: 0.72
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.0002166629710700363
0.00021064402244519442
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6383780111932451; val_accuracy: 0.8526074840764332 

The current subspace-distance is: 0.00021064402244519442 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.84
Batch: 40; loss: 0.68; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.81; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.7; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.8; acc: 0.77
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.78; acc: 0.78
Batch: 320; loss: 0.67; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.86
Batch: 400; loss: 0.68; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.78
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.92
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.7; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.88
Batch: 640; loss: 0.8; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.74; acc: 0.75
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.86
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.00021807182929478586
0.00021166956867091358
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.49; acc: 0.91
Val Epoch over. val_loss: 0.6312625829581242; val_accuracy: 0.8545979299363057 

The current subspace-distance is: 0.00021166956867091358 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.83; acc: 0.73
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.69; acc: 0.88
Batch: 240; loss: 0.83; acc: 0.75
Batch: 260; loss: 0.73; acc: 0.78
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.68; acc: 0.89
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.55; acc: 0.92
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.7; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.76; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.79; acc: 0.8
Batch: 760; loss: 0.61; acc: 0.89
Batch: 780; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.0002192834363086149
0.00021358764206524938
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.91
Val Epoch over. val_loss: 0.6332322294545022; val_accuracy: 0.852906050955414 

The current subspace-distance is: 0.00021358764206524938 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.89; acc: 0.73
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.75; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.79; acc: 0.77
Batch: 220; loss: 0.63; acc: 0.88
Batch: 240; loss: 0.72; acc: 0.83
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.87; acc: 0.77
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.66; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.77
Batch: 440; loss: 0.73; acc: 0.84
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.6; acc: 0.91
Batch: 520; loss: 0.75; acc: 0.83
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.9; acc: 0.66
Batch: 620; loss: 0.64; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.91
Batch: 660; loss: 0.67; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.91
Batch: 700; loss: 0.76; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.63; acc: 0.88
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.43; acc: 0.94
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.0002206155622843653
0.00021473312517628074
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.6206306718337308; val_accuracy: 0.8597730891719745 

The current subspace-distance is: 0.00021473312517628074 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.92
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.94
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.85; acc: 0.77
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 0.69; acc: 0.78
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.92
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.67; acc: 0.88
Batch: 400; loss: 0.78; acc: 0.78
Batch: 420; loss: 0.79; acc: 0.78
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.79; acc: 0.81
Batch: 500; loss: 0.68; acc: 0.86
Batch: 520; loss: 0.84; acc: 0.78
Batch: 540; loss: 0.74; acc: 0.77
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.83; acc: 0.8
Batch: 660; loss: 0.6; acc: 0.91
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.77; acc: 0.75
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.69; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00022259978868532926
0.00021478427515830845
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.622870419625264; val_accuracy: 0.8544984076433121 

The current subspace-distance is: 0.00021478427515830845 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.88
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.65; acc: 0.8
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.75; acc: 0.83
Batch: 320; loss: 0.65; acc: 0.88
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.84
Batch: 500; loss: 0.79; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.83
Batch: 540; loss: 0.78; acc: 0.75
Batch: 560; loss: 0.73; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.89
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.92
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.55; acc: 0.89
Batch: 720; loss: 0.85; acc: 0.78
Batch: 740; loss: 0.77; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.84
Batch: 780; loss: 0.8; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.0002221297036157921
0.00021692176233045757
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.6273262052778985; val_accuracy: 0.8556926751592356 

The current subspace-distance is: 0.00021692176233045757 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.84; acc: 0.77
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.81; acc: 0.8
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.72; acc: 0.77
Batch: 320; loss: 0.66; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.88
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.59; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.89
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.88; acc: 0.75
Batch: 580; loss: 0.75; acc: 0.77
Batch: 600; loss: 0.51; acc: 0.94
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.59; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.94
Batch: 680; loss: 0.65; acc: 0.91
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00022207169968169183
0.00021864862355869263
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.91
Val Epoch over. val_loss: 0.6224542172851076; val_accuracy: 0.8553941082802548 

The current subspace-distance is: 0.00021864862355869263 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.62; acc: 0.91
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 0.83; acc: 0.75
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.82; acc: 0.83
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.75; acc: 0.78
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.91
Batch: 280; loss: 0.73; acc: 0.78
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.73; acc: 0.81
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.86; acc: 0.78
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.8; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.8
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.71; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.79; acc: 0.83
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.83
Batch: 700; loss: 0.77; acc: 0.8
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00022344444005284458
0.00021824812574777752
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.91
Val Epoch over. val_loss: 0.6162681460000907; val_accuracy: 0.8573845541401274 

The current subspace-distance is: 0.00021824812574777752 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.82; acc: 0.77
Batch: 180; loss: 0.51; acc: 0.94
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.88
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.86
Batch: 340; loss: 0.8; acc: 0.73
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.82; acc: 0.75
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 0.51; acc: 0.92
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.94
Batch: 600; loss: 0.63; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.68; acc: 0.86
Batch: 740; loss: 0.73; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.74; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.0002245619398308918
0.00021802338596899062
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.61806868026211; val_accuracy: 0.8561902866242038 

The current subspace-distance is: 0.00021802338596899062 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.91; acc: 0.77
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.82; acc: 0.8
Batch: 300; loss: 0.7; acc: 0.81
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.88; acc: 0.75
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.81; acc: 0.8
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.76; acc: 0.81
Batch: 660; loss: 0.8; acc: 0.77
Batch: 680; loss: 0.63; acc: 0.89
Batch: 700; loss: 0.71; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.86
Batch: 740; loss: 0.88; acc: 0.75
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00022462420747615397
0.0002196787972934544
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.6204586545373224; val_accuracy: 0.8542993630573248 

The current subspace-distance is: 0.0002196787972934544 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.75; acc: 0.77
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.71; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.89
Batch: 280; loss: 0.74; acc: 0.84
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.86
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.8; acc: 0.7
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.73; acc: 0.86
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 0.71; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.82; acc: 0.8
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.92
Batch: 760; loss: 0.68; acc: 0.81
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00022478991013485938
0.00021906945039518178
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.91
Val Epoch over. val_loss: 0.6178207283566712; val_accuracy: 0.855593152866242 

The current subspace-distance is: 0.00021906945039518178 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.77
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.81; acc: 0.77
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.81; acc: 0.77
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.77; acc: 0.81
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.7; acc: 0.86
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.87; acc: 0.72
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.82; acc: 0.86
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.59; acc: 0.91
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.8; acc: 0.81
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.85; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00022986398835200816
0.00022173770412337035
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.91
Val Epoch over. val_loss: 0.6136153691513523; val_accuracy: 0.8572850318471338 

The current subspace-distance is: 0.00022173770412337035 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.47; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.77; acc: 0.8
Batch: 300; loss: 0.76; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.8; acc: 0.77
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.88
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.78; acc: 0.81
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 0.61; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.92
Batch: 640; loss: 0.77; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.77
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.72; acc: 0.81
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.71; acc: 0.91
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00022524857195094228
0.0002218394511146471
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.6156888852833183; val_accuracy: 0.8586783439490446 

The current subspace-distance is: 0.0002218394511146471 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_5_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.4122939498231974

The number of parameters is: 278681

The number of individual parameters is:

28
448
28
28
41
49364
41
41
82
144566
82
82
64
78720
64
64
4096
64
640
10
64
64

nonzero elements in E: 83604294
elements in E: 83604300
fraction nonzero: 0.9999999282333564
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.2; acc: 0.2
Batch: 20; loss: 2.14; acc: 0.27
Batch: 40; loss: 1.77; acc: 0.47
Batch: 60; loss: 1.85; acc: 0.42
Batch: 80; loss: 1.62; acc: 0.55
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.44; acc: 0.66
Batch: 140; loss: 1.53; acc: 0.62
Batch: 160; loss: 1.56; acc: 0.58
Batch: 180; loss: 1.5; acc: 0.58
Batch: 200; loss: 1.34; acc: 0.73
Batch: 220; loss: 1.45; acc: 0.67
Batch: 240; loss: 1.4; acc: 0.67
Batch: 260; loss: 1.36; acc: 0.72
Batch: 280; loss: 1.38; acc: 0.66
Batch: 300; loss: 1.42; acc: 0.69
Batch: 320; loss: 1.35; acc: 0.69
Batch: 340; loss: 1.35; acc: 0.67
Batch: 360; loss: 1.4; acc: 0.61
Batch: 380; loss: 1.23; acc: 0.81
Batch: 400; loss: 1.16; acc: 0.84
Batch: 420; loss: 1.15; acc: 0.78
Batch: 440; loss: 1.25; acc: 0.78
Batch: 460; loss: 1.34; acc: 0.66
Batch: 480; loss: 1.29; acc: 0.7
Batch: 500; loss: 1.22; acc: 0.75
Batch: 520; loss: 1.24; acc: 0.75
Batch: 540; loss: 1.31; acc: 0.69
Batch: 560; loss: 1.26; acc: 0.77
Batch: 580; loss: 1.12; acc: 0.75
Batch: 600; loss: 1.15; acc: 0.78
Batch: 620; loss: 1.23; acc: 0.77
Batch: 640; loss: 1.15; acc: 0.77
Batch: 660; loss: 1.19; acc: 0.73
Batch: 680; loss: 1.06; acc: 0.8
Batch: 700; loss: 1.24; acc: 0.67
Batch: 720; loss: 1.13; acc: 0.77
Batch: 740; loss: 1.12; acc: 0.81
Batch: 760; loss: 1.1; acc: 0.8
Batch: 780; loss: 1.17; acc: 0.75
Train Epoch over. train_loss: 1.36; train_accuracy: 0.68 

6.346667214529589e-05
5.832100578118116e-05
Batch: 0; loss: 1.1; acc: 0.86
Batch: 20; loss: 1.29; acc: 0.64
Batch: 40; loss: 0.75; acc: 0.89
Batch: 60; loss: 1.06; acc: 0.81
Batch: 80; loss: 0.91; acc: 0.91
Batch: 100; loss: 1.05; acc: 0.84
Batch: 120; loss: 1.27; acc: 0.72
Batch: 140; loss: 0.97; acc: 0.91
Val Epoch over. val_loss: 1.075316654269103; val_accuracy: 0.7924960191082803 

The current subspace-distance is: 5.832100578118116e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.81
Batch: 20; loss: 1.11; acc: 0.8
Batch: 40; loss: 1.18; acc: 0.67
Batch: 60; loss: 1.03; acc: 0.84
Batch: 80; loss: 1.04; acc: 0.84
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.14; acc: 0.75
Batch: 140; loss: 1.03; acc: 0.84
Batch: 160; loss: 0.95; acc: 0.84
Batch: 180; loss: 1.09; acc: 0.78
Batch: 200; loss: 0.93; acc: 0.84
Batch: 220; loss: 0.99; acc: 0.88
Batch: 240; loss: 1.08; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.8
Batch: 280; loss: 1.11; acc: 0.73
Batch: 300; loss: 0.9; acc: 0.88
Batch: 320; loss: 1.02; acc: 0.83
Batch: 340; loss: 0.98; acc: 0.81
Batch: 360; loss: 0.9; acc: 0.84
Batch: 380; loss: 0.91; acc: 0.81
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 1.21; acc: 0.67
Batch: 440; loss: 0.89; acc: 0.84
Batch: 460; loss: 0.9; acc: 0.88
Batch: 480; loss: 0.97; acc: 0.78
Batch: 500; loss: 1.02; acc: 0.78
Batch: 520; loss: 1.03; acc: 0.73
Batch: 540; loss: 1.07; acc: 0.8
Batch: 560; loss: 0.91; acc: 0.86
Batch: 580; loss: 0.94; acc: 0.78
Batch: 600; loss: 0.97; acc: 0.84
Batch: 620; loss: 0.88; acc: 0.88
Batch: 640; loss: 0.96; acc: 0.84
Batch: 660; loss: 0.99; acc: 0.78
Batch: 680; loss: 0.86; acc: 0.88
Batch: 700; loss: 0.96; acc: 0.83
Batch: 720; loss: 0.85; acc: 0.83
Batch: 740; loss: 0.89; acc: 0.78
Batch: 760; loss: 0.83; acc: 0.86
Batch: 780; loss: 0.82; acc: 0.86
Train Epoch over. train_loss: 1.0; train_accuracy: 0.81 

8.911107579478994e-05
8.50661308504641e-05
Batch: 0; loss: 0.87; acc: 0.92
Batch: 20; loss: 1.06; acc: 0.69
Batch: 40; loss: 0.54; acc: 0.97
Batch: 60; loss: 0.83; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.86
Batch: 120; loss: 1.04; acc: 0.77
Batch: 140; loss: 0.71; acc: 0.91
Val Epoch over. val_loss: 0.8511932830142367; val_accuracy: 0.8489251592356688 

The current subspace-distance is: 8.50661308504641e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.86
Batch: 20; loss: 0.96; acc: 0.8
Batch: 40; loss: 0.94; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.84
Batch: 80; loss: 0.98; acc: 0.8
Batch: 100; loss: 0.92; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.91
Batch: 140; loss: 0.81; acc: 0.88
Batch: 160; loss: 0.89; acc: 0.83
Batch: 180; loss: 0.83; acc: 0.88
Batch: 200; loss: 0.79; acc: 0.86
Batch: 220; loss: 0.96; acc: 0.77
Batch: 240; loss: 0.8; acc: 0.84
Batch: 260; loss: 0.96; acc: 0.81
Batch: 280; loss: 0.82; acc: 0.86
Batch: 300; loss: 0.84; acc: 0.88
Batch: 320; loss: 0.8; acc: 0.86
Batch: 340; loss: 0.86; acc: 0.81
Batch: 360; loss: 0.96; acc: 0.78
Batch: 380; loss: 0.83; acc: 0.86
Batch: 400; loss: 0.89; acc: 0.8
Batch: 420; loss: 0.86; acc: 0.84
Batch: 440; loss: 0.96; acc: 0.83
Batch: 460; loss: 0.88; acc: 0.84
Batch: 480; loss: 0.93; acc: 0.8
Batch: 500; loss: 0.9; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.88
Batch: 540; loss: 0.89; acc: 0.81
Batch: 560; loss: 0.8; acc: 0.86
Batch: 580; loss: 0.73; acc: 0.91
Batch: 600; loss: 0.68; acc: 0.91
Batch: 620; loss: 0.82; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.86
Batch: 660; loss: 0.91; acc: 0.8
Batch: 680; loss: 0.82; acc: 0.84
Batch: 700; loss: 0.78; acc: 0.92
Batch: 720; loss: 0.86; acc: 0.86
Batch: 740; loss: 1.0; acc: 0.8
Batch: 760; loss: 0.79; acc: 0.83
Batch: 780; loss: 0.76; acc: 0.92
Train Epoch over. train_loss: 0.86; train_accuracy: 0.84 

0.00010727849439717829
0.00010238824324915186
Batch: 0; loss: 0.76; acc: 0.94
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.45; acc: 0.97
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.91
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.65; acc: 0.92
Val Epoch over. val_loss: 0.7619986237993666; val_accuracy: 0.8705214968152867 

The current subspace-distance is: 0.00010238824324915186 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.88
Batch: 40; loss: 0.81; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.82; acc: 0.86
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.94
Batch: 160; loss: 0.79; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.89
Batch: 200; loss: 0.86; acc: 0.86
Batch: 220; loss: 0.78; acc: 0.84
Batch: 240; loss: 0.73; acc: 0.84
Batch: 260; loss: 0.69; acc: 0.92
Batch: 280; loss: 0.7; acc: 0.86
Batch: 300; loss: 0.79; acc: 0.86
Batch: 320; loss: 0.79; acc: 0.86
Batch: 340; loss: 0.69; acc: 0.88
Batch: 360; loss: 0.8; acc: 0.83
Batch: 380; loss: 0.77; acc: 0.8
Batch: 400; loss: 0.88; acc: 0.84
Batch: 420; loss: 0.86; acc: 0.8
Batch: 440; loss: 0.89; acc: 0.73
Batch: 460; loss: 0.79; acc: 0.86
Batch: 480; loss: 0.91; acc: 0.8
Batch: 500; loss: 0.86; acc: 0.86
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.77; acc: 0.88
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.81; acc: 0.81
Batch: 600; loss: 0.76; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.92
Batch: 660; loss: 0.82; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.83
Batch: 700; loss: 0.81; acc: 0.88
Batch: 720; loss: 0.66; acc: 0.92
Batch: 740; loss: 0.77; acc: 0.84
Batch: 760; loss: 0.76; acc: 0.84
Batch: 780; loss: 0.85; acc: 0.81
Train Epoch over. train_loss: 0.79; train_accuracy: 0.86 

0.00012299478112254292
0.00011825254478026181
Batch: 0; loss: 0.69; acc: 0.92
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.59; acc: 0.95
Val Epoch over. val_loss: 0.6955669066708559; val_accuracy: 0.8807722929936306 

The current subspace-distance is: 0.00011825254478026181 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.92
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 0.68; acc: 0.89
Batch: 80; loss: 0.88; acc: 0.8
Batch: 100; loss: 0.83; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.68; acc: 0.91
Batch: 160; loss: 0.91; acc: 0.73
Batch: 180; loss: 0.68; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.89
Batch: 220; loss: 0.74; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.89
Batch: 260; loss: 0.69; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.86
Batch: 300; loss: 0.74; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.91
Batch: 340; loss: 0.75; acc: 0.91
Batch: 360; loss: 0.84; acc: 0.78
Batch: 380; loss: 0.78; acc: 0.86
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.69; acc: 0.92
Batch: 440; loss: 0.79; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.92
Batch: 480; loss: 0.69; acc: 0.91
Batch: 500; loss: 0.6; acc: 0.92
Batch: 520; loss: 0.81; acc: 0.84
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.83; acc: 0.78
Batch: 580; loss: 0.85; acc: 0.81
Batch: 600; loss: 0.72; acc: 0.89
Batch: 620; loss: 0.66; acc: 0.89
Batch: 640; loss: 0.81; acc: 0.86
Batch: 660; loss: 0.63; acc: 0.91
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.65; acc: 0.88
Batch: 760; loss: 0.71; acc: 0.89
Batch: 780; loss: 0.78; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.87 

0.0001397590385749936
0.0001335147098870948
Batch: 0; loss: 0.63; acc: 0.92
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.6316586960652831; val_accuracy: 0.8893312101910829 

The current subspace-distance is: 0.0001335147098870948 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.91
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.95
Batch: 160; loss: 0.74; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.88
Batch: 200; loss: 0.9; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.91
Batch: 240; loss: 0.56; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.89
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.91
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.89
Batch: 360; loss: 0.85; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.94
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.76; acc: 0.83
Batch: 460; loss: 0.58; acc: 0.95
Batch: 480; loss: 0.64; acc: 0.89
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.69; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.76; acc: 0.78
Batch: 620; loss: 0.54; acc: 0.95
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.71; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.7; acc: 0.84
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.59; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.67; train_accuracy: 0.87 

0.00015260407235473394
0.00014832295710220933
Batch: 0; loss: 0.56; acc: 0.92
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.32; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.5682203684263168; val_accuracy: 0.8932125796178344 

The current subspace-distance is: 0.00014832295710220933 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.94
Batch: 40; loss: 0.54; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.92
Batch: 80; loss: 0.77; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.97
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.92
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.6; acc: 0.95
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.92
Batch: 320; loss: 0.64; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.88
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.73; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.88
Batch: 460; loss: 0.75; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.61; acc: 0.91
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.8
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.67; acc: 0.92
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.73; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.87 

0.00016284042794723064
0.0001586291764397174
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.5477769781070151; val_accuracy: 0.8955015923566879 

The current subspace-distance is: 0.0001586291764397174 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.92
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.95
Batch: 180; loss: 0.57; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.97
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.61; acc: 0.89
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.55; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.6; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.63; acc: 0.92
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.6; acc: 0.91
Batch: 540; loss: 0.75; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.95
Batch: 600; loss: 0.52; acc: 0.92
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.95
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.59; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

0.00017496003420092165
0.00016758195124566555
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.74; acc: 0.75
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.5247721136755245; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.00016758195124566555 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.75
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.57; acc: 0.94
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.89
Batch: 220; loss: 0.55; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.91
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.84; acc: 0.78
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.55; acc: 0.91
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.64; acc: 0.78
Batch: 620; loss: 0.58; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.94
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.46; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

0.00018319646187592298
0.00017740884504746646
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.5029048652026304; val_accuracy: 0.8991839171974523 

The current subspace-distance is: 0.00017740884504746646 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.76; acc: 0.77
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.95
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.61; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.49; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.0001936224871315062
0.00018682327936403453
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.48303135299378896; val_accuracy: 0.9011743630573248 

The current subspace-distance is: 0.00018682327936403453 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.94
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.98
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

0.00019276786770205945
0.0001873502042144537
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.4797828099720038; val_accuracy: 0.9017714968152867 

The current subspace-distance is: 0.0001873502042144537 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.95
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 0.5; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

0.00019777557463385165
0.00019139942014589906
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.4705844659144711; val_accuracy: 0.9036624203821656 

The current subspace-distance is: 0.00019139942014589906 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.47; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.94
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.77
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.94
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.97
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

0.00020190865325275809
0.00019360794976819307
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.46555630074944465; val_accuracy: 0.9028662420382165 

The current subspace-distance is: 0.00019360794976819307 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.71; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.94
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.97
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.95
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.95
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

0.00020182937441859394
0.00019488722318783402
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.46420852336913915; val_accuracy: 0.9032643312101911 

The current subspace-distance is: 0.00019488722318783402 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.57; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.73; acc: 0.77
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00020409506396390498
0.0001966355339391157
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.4510504630910363; val_accuracy: 0.9045581210191083 

The current subspace-distance is: 0.0001966355339391157 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.94
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.94
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.97
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00020676676649600267
0.00020015268819406629
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.4567517551836694; val_accuracy: 0.9041600318471338 

The current subspace-distance is: 0.00020015268819406629 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00020710943499580026
0.0002011754404520616
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.45001428180439457; val_accuracy: 0.9052547770700637 

The current subspace-distance is: 0.0002011754404520616 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.38; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.78
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.91
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.00021022645523771644
0.00020269633387215436
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4418742875954148; val_accuracy: 0.9048566878980892 

The current subspace-distance is: 0.00020269633387215436 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.95
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.98
Batch: 400; loss: 0.41; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.98
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.98
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.00021102734899614006
0.0002055754157481715
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.45112982771958515; val_accuracy: 0.9023686305732485 

The current subspace-distance is: 0.0002055754157481715 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.95
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.73; acc: 0.73
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.95
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.59; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.95
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.68; acc: 0.81
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00021366645523812622
0.00020763714564964175
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4361511298045991; val_accuracy: 0.9087380573248408 

The current subspace-distance is: 0.00020763714564964175 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.64; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.98
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.94
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.0002145198086509481
0.00020528631284832954
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4393866281410691; val_accuracy: 0.9070461783439491 

The current subspace-distance is: 0.00020528631284832954 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.95
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.94
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.94
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00021633719734381884
0.0002075946395052597
Batch: 0; loss: 0.37; acc: 0.98
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4373193605310598; val_accuracy: 0.9068471337579618 

The current subspace-distance is: 0.0002075946395052597 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.97
Batch: 460; loss: 0.43; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.4; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.95
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.94
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.48; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.0002148393396055326
0.0002085837331833318
Batch: 0; loss: 0.36; acc: 1.0
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4357971256705606; val_accuracy: 0.9067476114649682 

The current subspace-distance is: 0.0002085837331833318 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.95
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.52; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.91
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00021655169257428497
0.00020704182679764926
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4314853046919889; val_accuracy: 0.90625 

The current subspace-distance is: 0.00020704182679764926 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.61; acc: 0.83
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.95
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.95
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00021731802553404123
0.00020880041120108217
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.43660560459088366; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 0.00020880041120108217 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.97
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.95
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.94
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.0002163331228075549
0.00020965264411643147
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4326890938600917; val_accuracy: 0.9071457006369427 

The current subspace-distance is: 0.00020965264411643147 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.95
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.8
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.000217587745282799
0.00021165036014281213
Batch: 0; loss: 0.37; acc: 0.98
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.43459081981971764; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 0.00021165036014281213 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.8
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.94
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.97
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.81
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.97
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00021760360687039793
0.00021092411770951003
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.42637351364087145; val_accuracy: 0.9074442675159236 

The current subspace-distance is: 0.00021092411770951003 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.95
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.94
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021956537966616452
0.0002127998450305313
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4344179574281547; val_accuracy: 0.90625 

The current subspace-distance is: 0.0002127998450305313 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.97
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021876307437196374
0.00021294796897564083
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.43042960108085804; val_accuracy: 0.9072452229299363 

The current subspace-distance is: 0.00021294796897564083 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_5_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.4122939498231974

The number of parameters is: 278681

The number of individual parameters is:

28
448
28
28
41
49364
41
41
82
144566
82
82
64
78720
64
64
4096
64
640
10
64
64

nonzero elements in E: 111472390
elements in E: 111472400
fraction nonzero: 0.9999999102916955
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.21; acc: 0.22
Batch: 20; loss: 2.03; acc: 0.22
Batch: 40; loss: 1.78; acc: 0.48
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.61; acc: 0.56
Batch: 100; loss: 1.58; acc: 0.48
Batch: 120; loss: 1.43; acc: 0.66
Batch: 140; loss: 1.37; acc: 0.67
Batch: 160; loss: 1.4; acc: 0.67
Batch: 180; loss: 1.41; acc: 0.66
Batch: 200; loss: 1.28; acc: 0.72
Batch: 220; loss: 1.3; acc: 0.72
Batch: 240; loss: 1.26; acc: 0.72
Batch: 260; loss: 1.18; acc: 0.78
Batch: 280; loss: 1.24; acc: 0.69
Batch: 300; loss: 1.27; acc: 0.67
Batch: 320; loss: 1.12; acc: 0.88
Batch: 340; loss: 1.26; acc: 0.73
Batch: 360; loss: 1.21; acc: 0.73
Batch: 380; loss: 1.12; acc: 0.77
Batch: 400; loss: 1.2; acc: 0.72
Batch: 420; loss: 1.25; acc: 0.72
Batch: 440; loss: 1.11; acc: 0.72
Batch: 460; loss: 1.06; acc: 0.81
Batch: 480; loss: 1.06; acc: 0.83
Batch: 500; loss: 1.18; acc: 0.75
Batch: 520; loss: 0.94; acc: 0.88
Batch: 540; loss: 1.01; acc: 0.8
Batch: 560; loss: 1.03; acc: 0.81
Batch: 580; loss: 0.98; acc: 0.91
Batch: 600; loss: 1.07; acc: 0.78
Batch: 620; loss: 1.01; acc: 0.8
Batch: 640; loss: 0.85; acc: 0.86
Batch: 660; loss: 0.89; acc: 0.83
Batch: 680; loss: 1.02; acc: 0.78
Batch: 700; loss: 0.88; acc: 0.89
Batch: 720; loss: 0.99; acc: 0.89
Batch: 740; loss: 1.0; acc: 0.84
Batch: 760; loss: 1.03; acc: 0.83
Batch: 780; loss: 0.91; acc: 0.86
Train Epoch over. train_loss: 1.21; train_accuracy: 0.74 

2.4972952815005556e-05
8.549656740797218e-06
Batch: 0; loss: 0.97; acc: 0.86
Batch: 20; loss: 1.13; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.94
Batch: 60; loss: 0.9; acc: 0.84
Batch: 80; loss: 0.77; acc: 0.94
Batch: 100; loss: 0.92; acc: 0.83
Batch: 120; loss: 1.04; acc: 0.8
Batch: 140; loss: 0.82; acc: 0.94
Val Epoch over. val_loss: 0.9018413542182582; val_accuracy: 0.8597730891719745 

The current subspace-distance is: 8.549656740797218e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.88
Batch: 40; loss: 0.89; acc: 0.88
Batch: 60; loss: 0.94; acc: 0.84
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 0.9; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.89
Batch: 140; loss: 0.87; acc: 0.92
Batch: 160; loss: 0.96; acc: 0.83
Batch: 180; loss: 0.95; acc: 0.81
Batch: 200; loss: 1.01; acc: 0.8
Batch: 220; loss: 0.84; acc: 0.86
Batch: 240; loss: 0.82; acc: 0.88
Batch: 260; loss: 0.79; acc: 0.91
Batch: 280; loss: 0.84; acc: 0.81
Batch: 300; loss: 0.93; acc: 0.81
Batch: 320; loss: 0.87; acc: 0.83
Batch: 340; loss: 0.86; acc: 0.83
Batch: 360; loss: 0.95; acc: 0.83
Batch: 380; loss: 0.86; acc: 0.81
Batch: 400; loss: 0.85; acc: 0.83
Batch: 420; loss: 0.85; acc: 0.84
Batch: 440; loss: 0.85; acc: 0.86
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 0.87; acc: 0.8
Batch: 500; loss: 0.89; acc: 0.8
Batch: 520; loss: 0.9; acc: 0.78
Batch: 540; loss: 0.82; acc: 0.86
Batch: 560; loss: 0.82; acc: 0.84
Batch: 580; loss: 0.95; acc: 0.84
Batch: 600; loss: 0.84; acc: 0.83
Batch: 620; loss: 0.76; acc: 0.89
Batch: 640; loss: 0.89; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.88
Batch: 680; loss: 0.82; acc: 0.84
Batch: 700; loss: 0.84; acc: 0.86
Batch: 720; loss: 0.75; acc: 0.88
Batch: 740; loss: 0.76; acc: 0.91
Batch: 760; loss: 0.92; acc: 0.78
Batch: 780; loss: 0.68; acc: 0.91
Train Epoch over. train_loss: 0.84; train_accuracy: 0.86 

3.0330795198096894e-05
1.131065255322028e-05
Batch: 0; loss: 0.74; acc: 0.94
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.95
Val Epoch over. val_loss: 0.7073872277311458; val_accuracy: 0.8863455414012739 

The current subspace-distance is: 1.131065255322028e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.88
Batch: 40; loss: 0.75; acc: 0.89
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.87; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.91
Batch: 140; loss: 0.72; acc: 0.92
Batch: 160; loss: 0.65; acc: 0.94
Batch: 180; loss: 0.65; acc: 0.92
Batch: 200; loss: 0.79; acc: 0.88
Batch: 220; loss: 0.77; acc: 0.84
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.84; acc: 0.83
Batch: 280; loss: 0.8; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.91
Batch: 320; loss: 0.74; acc: 0.88
Batch: 340; loss: 0.65; acc: 0.92
Batch: 360; loss: 0.68; acc: 0.88
Batch: 380; loss: 0.79; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.92
Batch: 420; loss: 0.77; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.94
Batch: 480; loss: 0.64; acc: 0.95
Batch: 500; loss: 0.75; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.89
Batch: 560; loss: 0.59; acc: 0.92
Batch: 580; loss: 0.67; acc: 0.86
Batch: 600; loss: 0.66; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.92
Batch: 640; loss: 0.58; acc: 0.89
Batch: 660; loss: 0.8; acc: 0.81
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.64; acc: 0.92
Batch: 720; loss: 0.74; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.91
Batch: 760; loss: 0.73; acc: 0.88
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.88 

3.464548717602156e-05
1.4060211469768547e-05
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.95
Batch: 120; loss: 0.74; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.97
Val Epoch over. val_loss: 0.6030624444317666; val_accuracy: 0.9017714968152867 

The current subspace-distance is: 1.4060211469768547e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.7; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.92
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.56; acc: 0.98
Batch: 220; loss: 0.69; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.95
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.71; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.89
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.95
Batch: 460; loss: 0.72; acc: 0.8
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.83
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.89
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.94
Batch: 780; loss: 0.47; acc: 0.92
Train Epoch over. train_loss: 0.62; train_accuracy: 0.89 

3.720904715009965e-05
1.4849416402284987e-05
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5300978735374038; val_accuracy: 0.9095342356687898 

The current subspace-distance is: 1.4849416402284987e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.94
Batch: 40; loss: 0.6; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.91
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.94
Batch: 180; loss: 0.63; acc: 0.91
Batch: 200; loss: 0.59; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.61; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.92
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.49; acc: 0.94
Batch: 400; loss: 0.6; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.97
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.94
Batch: 520; loss: 0.71; acc: 0.77
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.52; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.97
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.95
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.56; train_accuracy: 0.89 

4.0168484702007845e-05
1.6136778867803514e-05
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.98
Val Epoch over. val_loss: 0.4873671329514995; val_accuracy: 0.912718949044586 

The current subspace-distance is: 1.6136778867803514e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.95
Batch: 220; loss: 0.62; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.94
Batch: 260; loss: 0.51; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.94
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.69; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.9 

4.384930798551068e-05
1.854269066825509e-05
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.24; acc: 1.0
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.44696186454432785; val_accuracy: 0.9170979299363057 

The current subspace-distance is: 1.854269066825509e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.98
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.98
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.95
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.97
Train Epoch over. train_loss: 0.49; train_accuracy: 0.9 

4.5767505071125925e-05
1.928107849380467e-05
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.22; acc: 1.0
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.24; acc: 1.0
Val Epoch over. val_loss: 0.4155746542724075; val_accuracy: 0.9223726114649682 

The current subspace-distance is: 1.928107849380467e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

4.7310630179708824e-05
1.9529312339727767e-05
Batch: 0; loss: 0.37; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.22; acc: 1.0
Val Epoch over. val_loss: 0.397331623135099; val_accuracy: 0.9252587579617835 

The current subspace-distance is: 1.9529312339727767e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.95
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.97
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.66; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.45; train_accuracy: 0.91 

5.006564606446773e-05
2.2052925487514585e-05
Batch: 0; loss: 0.33; acc: 1.0
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.3770432754117212; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 2.2052925487514585e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.97
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

5.166056507732719e-05
2.142768062185496e-05
Batch: 0; loss: 0.33; acc: 1.0
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.37108737752316107; val_accuracy: 0.9282444267515924 

The current subspace-distance is: 2.142768062185496e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.95
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.97
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.97
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.97
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

5.260757461655885e-05
2.224168201792054e-05
Batch: 0; loss: 0.33; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.36555992740734367; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 2.224168201792054e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.97
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.29; acc: 1.0
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.97
Batch: 560; loss: 0.42; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.98
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.97
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

5.303604484652169e-05
2.3270780729944818e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.36019099461045234; val_accuracy: 0.9273487261146497 

The current subspace-distance is: 2.3270780729944818e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.81
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.97
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.97
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

5.357099144021049e-05
2.2976359105086885e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3574902868005121; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.2976359105086885e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.97
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.456499638967216e-05
2.3168720872490667e-05
Batch: 0; loss: 0.33; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.35597944012872734; val_accuracy: 0.927547770700637 

The current subspace-distance is: 2.3168720872490667e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.97
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.98
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.98
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.523337677004747e-05
2.3556405722047202e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3532318801257261; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.3556405722047202e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.98
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.98
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.504251748789102e-05
2.3485905330744572e-05
Batch: 0; loss: 0.33; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.35403022321925803; val_accuracy: 0.925656847133758 

The current subspace-distance is: 2.3485905330744572e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.97
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.68; acc: 0.8
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.58; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.98
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.5256765335798264e-05
2.313304321432952e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3459288151875423; val_accuracy: 0.9269506369426752 

The current subspace-distance is: 2.313304321432952e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.97
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.542865255847573e-05
2.274306461913511e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.34591634286816714; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.274306461913511e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.98
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.98
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.6729088100837544e-05
2.4351658794330433e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3382412721017364; val_accuracy: 0.9288415605095541 

The current subspace-distance is: 2.4351658794330433e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.97
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.6983619288075715e-05
2.4646840756759048e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.33443857084034356; val_accuracy: 0.9294386942675159 

The current subspace-distance is: 2.4646840756759048e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.28; acc: 1.0
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.6876953749451786e-05
2.2636082576354966e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.34007499789356427; val_accuracy: 0.9298367834394905 

The current subspace-distance is: 2.2636082576354966e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.97
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.670166137861088e-05
2.2642905605607666e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.33833028442540747; val_accuracy: 0.9288415605095541 

The current subspace-distance is: 2.2642905605607666e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.81
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.757902908953838e-05
2.5289116820204072e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.33479263211131854; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.5289116820204072e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.97
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.97
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.7630037190392613e-05
2.389997462159954e-05
Batch: 0; loss: 0.31; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3356017920241994; val_accuracy: 0.9286425159235668 

The current subspace-distance is: 2.389997462159954e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.22; acc: 1.0
Batch: 520; loss: 0.38; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.709381366614252e-05
2.4062614102149382e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3377301185184224; val_accuracy: 0.9271496815286624 

The current subspace-distance is: 2.4062614102149382e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.97
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.7679295423440635e-05
2.508326360839419e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.33200257190853166; val_accuracy: 0.9301353503184714 

The current subspace-distance is: 2.508326360839419e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.97
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.97
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.833002069266513e-05
2.5023691705428064e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.33387220608200996; val_accuracy: 0.9286425159235668 

The current subspace-distance is: 2.5023691705428064e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.24; acc: 1.0
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.773219163529575e-05
2.434341877233237e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3336467640415119; val_accuracy: 0.9296377388535032 

The current subspace-distance is: 2.434341877233237e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.26; acc: 1.0
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.98
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.7225355703849345e-05
2.3609502022736706e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.32849419923724643; val_accuracy: 0.9278463375796179 

The current subspace-distance is: 2.3609502022736706e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.97
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.97
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.98
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.98
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.7950139307649806e-05
2.4905248210416175e-05
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3310293988532321; val_accuracy: 0.9288415605095541 

The current subspace-distance is: 2.4905248210416175e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_5_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.4122939498231974

The number of parameters is: 278681

The number of individual parameters is:

28
448
28
28
41
49364
41
41
82
144566
82
82
64
78720
64
64
4096
64
640
10
64
64

nonzero elements in E: 139340489
elements in E: 139340500
fraction nonzero: 0.9999999210566921
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.17
Batch: 20; loss: 1.8; acc: 0.47
Batch: 40; loss: 1.65; acc: 0.53
Batch: 60; loss: 1.51; acc: 0.56
Batch: 80; loss: 1.48; acc: 0.53
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.44; acc: 0.62
Batch: 140; loss: 1.35; acc: 0.81
Batch: 160; loss: 1.33; acc: 0.66
Batch: 180; loss: 1.28; acc: 0.75
Batch: 200; loss: 1.24; acc: 0.78
Batch: 220; loss: 1.28; acc: 0.72
Batch: 240; loss: 1.29; acc: 0.75
Batch: 260; loss: 1.2; acc: 0.78
Batch: 280; loss: 1.37; acc: 0.67
Batch: 300; loss: 1.07; acc: 0.81
Batch: 320; loss: 1.02; acc: 0.86
Batch: 340; loss: 1.09; acc: 0.83
Batch: 360; loss: 1.16; acc: 0.73
Batch: 380; loss: 1.14; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.78
Batch: 420; loss: 1.0; acc: 0.83
Batch: 440; loss: 0.97; acc: 0.81
Batch: 460; loss: 0.88; acc: 0.86
Batch: 480; loss: 1.01; acc: 0.84
Batch: 500; loss: 1.0; acc: 0.83
Batch: 520; loss: 1.01; acc: 0.81
Batch: 540; loss: 0.9; acc: 0.86
Batch: 560; loss: 0.87; acc: 0.89
Batch: 580; loss: 0.89; acc: 0.88
Batch: 600; loss: 0.87; acc: 0.83
Batch: 620; loss: 0.87; acc: 0.89
Batch: 640; loss: 0.71; acc: 0.94
Batch: 660; loss: 0.93; acc: 0.84
Batch: 680; loss: 0.88; acc: 0.81
Batch: 700; loss: 0.98; acc: 0.8
Batch: 720; loss: 0.91; acc: 0.86
Batch: 740; loss: 0.8; acc: 0.83
Batch: 760; loss: 0.82; acc: 0.84
Batch: 780; loss: 0.87; acc: 0.84
Train Epoch over. train_loss: 1.13; train_accuracy: 0.76 

2.451170439599082e-05
8.410959708271548e-06
Batch: 0; loss: 0.77; acc: 0.91
Batch: 20; loss: 1.05; acc: 0.77
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.81; acc: 0.86
Batch: 80; loss: 0.65; acc: 0.92
Batch: 100; loss: 0.79; acc: 0.91
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.94
Val Epoch over. val_loss: 0.796455007070189; val_accuracy: 0.8707205414012739 

The current subspace-distance is: 8.410959708271548e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.92
Batch: 80; loss: 0.82; acc: 0.86
Batch: 100; loss: 0.8; acc: 0.91
Batch: 120; loss: 0.93; acc: 0.7
Batch: 140; loss: 0.71; acc: 0.88
Batch: 160; loss: 0.9; acc: 0.8
Batch: 180; loss: 0.88; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.86
Batch: 220; loss: 0.69; acc: 0.94
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.92
Batch: 280; loss: 0.84; acc: 0.89
Batch: 300; loss: 0.88; acc: 0.86
Batch: 320; loss: 0.79; acc: 0.84
Batch: 340; loss: 0.75; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.91
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.95
Batch: 420; loss: 0.77; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.92
Batch: 460; loss: 0.85; acc: 0.84
Batch: 480; loss: 0.72; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.89
Batch: 520; loss: 0.63; acc: 0.89
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.91
Batch: 580; loss: 0.7; acc: 0.86
Batch: 600; loss: 0.65; acc: 0.89
Batch: 620; loss: 0.64; acc: 0.88
Batch: 640; loss: 0.76; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.94
Batch: 680; loss: 0.69; acc: 0.86
Batch: 700; loss: 0.79; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.8
Batch: 740; loss: 0.59; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.94
Batch: 780; loss: 0.69; acc: 0.86
Train Epoch over. train_loss: 0.73; train_accuracy: 0.87 

2.9689685106859542e-05
1.1465140232758131e-05
Batch: 0; loss: 0.53; acc: 0.97
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.95
Val Epoch over. val_loss: 0.577971902432715; val_accuracy: 0.9023686305732485 

The current subspace-distance is: 1.1465140232758131e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.94
Batch: 160; loss: 0.6; acc: 0.94
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.89
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.95
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.92
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.97
Batch: 400; loss: 0.53; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.94
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.97
Batch: 560; loss: 0.59; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.95
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.97
Batch: 680; loss: 0.64; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.58; train_accuracy: 0.9 

3.447644485277124e-05
1.3568404028774239e-05
Batch: 0; loss: 0.41; acc: 0.98
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.47326918411406743; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 1.3568404028774239e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.47; acc: 0.94
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.94
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.98
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.94
Batch: 400; loss: 0.5; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.94
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.97
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.91 

3.824723171419464e-05
1.5388617612188682e-05
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.40886251989063943; val_accuracy: 0.9274482484076433 

The current subspace-distance is: 1.5388617612188682e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.95
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.97
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.29; acc: 1.0
Batch: 640; loss: 0.36; acc: 0.97
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.94
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.91 

4.0881837776396424e-05
1.710053584247362e-05
Batch: 0; loss: 0.33; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.22; acc: 1.0
Val Epoch over. val_loss: 0.3729299986438387; val_accuracy: 0.9311305732484076 

The current subspace-distance is: 1.710053584247362e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.98
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.97
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.45; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.92 

4.331459786044434e-05
1.746690941217821e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.34672170943894964; val_accuracy: 0.934812898089172 

The current subspace-distance is: 1.746690941217821e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.95
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.98
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.97
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.98
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.97
Batch: 780; loss: 0.31; acc: 0.98
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

4.5463937567546964e-05
1.8577842638478614e-05
Batch: 0; loss: 0.28; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3323775483354641; val_accuracy: 0.934812898089172 

The current subspace-distance is: 1.8577842638478614e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

4.714968235930428e-05
1.9951015929109417e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.30904967200224565; val_accuracy: 0.9383957006369427 

The current subspace-distance is: 1.9951015929109417e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.21; acc: 1.0
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.97
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.93 

4.984956467524171e-05
2.2207155780051835e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2950098613739773; val_accuracy: 0.9396894904458599 

The current subspace-distance is: 2.2207155780051835e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.1055110816378146e-05
2.1917321646469645e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.28692624760661156; val_accuracy: 0.9409832802547771 

The current subspace-distance is: 2.1917321646469645e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.98
Batch: 160; loss: 0.31; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.19; acc: 1.0
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.98
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.242903353064321e-05
2.1342597392504103e-05
Batch: 0; loss: 0.21; acc: 1.0
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.28632606143594547; val_accuracy: 0.9408837579617835 

The current subspace-distance is: 2.1342597392504103e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.21; acc: 1.0
Batch: 520; loss: 0.29; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.97
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.2; acc: 0.98
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.218652222538367e-05
2.165749538107775e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2719544191743917; val_accuracy: 0.9428742038216561 

The current subspace-distance is: 2.165749538107775e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.42; acc: 0.84
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.2729392336914316e-05
2.137259980372619e-05
Batch: 0; loss: 0.2; acc: 1.0
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.27218671356606633; val_accuracy: 0.9433718152866242 

The current subspace-distance is: 2.137259980372619e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.98
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.344405508367345e-05
2.2578749849344604e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.26970986741933095; val_accuracy: 0.9443670382165605 

The current subspace-distance is: 2.2578749849344604e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.468794915941544e-05
2.3956181394169107e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.26835027588590693; val_accuracy: 0.9449641719745223 

The current subspace-distance is: 2.3956181394169107e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.98
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.451425749924965e-05
2.3261529349838383e-05
Batch: 0; loss: 0.19; acc: 1.0
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.26373288652319815; val_accuracy: 0.943172770700637 

The current subspace-distance is: 2.3261529349838383e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.560359204537235e-05
2.4190272597479634e-05
Batch: 0; loss: 0.19; acc: 1.0
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2647468885228892; val_accuracy: 0.943968949044586 

The current subspace-distance is: 2.4190272597479634e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.14; acc: 1.0
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.98
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.4970496421447024e-05
2.3269594748853706e-05
Batch: 0; loss: 0.19; acc: 1.0
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2625296110180533; val_accuracy: 0.9436703821656051 

The current subspace-distance is: 2.3269594748853706e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.647031866828911e-05
2.571489312686026e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25239377900673327; val_accuracy: 0.9451632165605095 

The current subspace-distance is: 2.571489312686026e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.98
Batch: 300; loss: 0.3; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.6525201216572896e-05
2.4275104806292802e-05
Batch: 0; loss: 0.19; acc: 1.0
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.25923962179262927; val_accuracy: 0.943172770700637 

The current subspace-distance is: 2.4275104806292802e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.98
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.16; acc: 1.0
Batch: 300; loss: 0.31; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.98
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.98
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.588716157944873e-05
2.249996759928763e-05
Batch: 0; loss: 0.19; acc: 1.0
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.26051052830591326; val_accuracy: 0.9421775477707006 

The current subspace-distance is: 2.249996759928763e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.97
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.98
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.683445488102734e-05
2.3270378733286634e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2562364812964087; val_accuracy: 0.9450636942675159 

The current subspace-distance is: 2.3270378733286634e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.23; acc: 1.0
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.734957449021749e-05
2.4318993382621557e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2534340416929524; val_accuracy: 0.9438694267515924 

The current subspace-distance is: 2.4318993382621557e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.98
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.6961402151500806e-05
2.4306173145305365e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2551881709865704; val_accuracy: 0.9461584394904459 

The current subspace-distance is: 2.4306173145305365e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.53; acc: 0.8
Batch: 760; loss: 0.27; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.749878255301155e-05
2.5569866920704953e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25293728789895964; val_accuracy: 0.9459593949044586 

The current subspace-distance is: 2.5569866920704953e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.21; acc: 1.0
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.689733006875031e-05
2.3693930415902287e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2534542499454158; val_accuracy: 0.944765127388535 

The current subspace-distance is: 2.3693930415902287e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.98
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.687638622475788e-05
2.3927837901283056e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25532193543614856; val_accuracy: 0.9445660828025477 

The current subspace-distance is: 2.3927837901283056e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.762179716839455e-05
2.4717124688322656e-05
Batch: 0; loss: 0.17; acc: 1.0
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.24913847247126755; val_accuracy: 0.9436703821656051 

The current subspace-distance is: 2.4717124688322656e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.19; acc: 1.0
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.699324901797809e-05
2.4118367946357466e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25275859336374673; val_accuracy: 0.9438694267515924 

The current subspace-distance is: 2.4118367946357466e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.658441659761593e-05
2.410995148238726e-05
Batch: 0; loss: 0.18; acc: 1.0
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.249452722158022; val_accuracy: 0.9449641719745223 

The current subspace-distance is: 2.410995148238726e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_5_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_5_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
