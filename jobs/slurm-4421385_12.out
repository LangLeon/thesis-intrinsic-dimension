model : table13slim
N : 12
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 1.7743928539080627

The number of parameters is: 247624

The number of individual parameters is:

15
270
15
15
22
36960
22
22
43
105952
43
43
64
99072
64
64
4096
64
640
10
64
64

nonzero elements in E: 12381199
elements in E: 12381200
fraction nonzero: 0.9999999192323846
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.38; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.29; acc: 0.11
Batch: 60; loss: 2.25; acc: 0.16
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.24; acc: 0.17
Batch: 120; loss: 2.17; acc: 0.2
Batch: 140; loss: 2.08; acc: 0.2
Batch: 160; loss: 1.98; acc: 0.27
Batch: 180; loss: 1.99; acc: 0.31
Batch: 200; loss: 2.09; acc: 0.27
Batch: 220; loss: 2.09; acc: 0.25
Batch: 240; loss: 2.05; acc: 0.31
Batch: 260; loss: 2.01; acc: 0.31
Batch: 280; loss: 1.97; acc: 0.33
Batch: 300; loss: 2.08; acc: 0.25
Batch: 320; loss: 1.94; acc: 0.47
Batch: 340; loss: 1.89; acc: 0.44
Batch: 360; loss: 1.97; acc: 0.3
Batch: 380; loss: 1.99; acc: 0.34
Batch: 400; loss: 2.0; acc: 0.34
Batch: 420; loss: 1.96; acc: 0.38
Batch: 440; loss: 2.0; acc: 0.39
Batch: 460; loss: 2.01; acc: 0.27
Batch: 480; loss: 2.0; acc: 0.39
Batch: 500; loss: 1.94; acc: 0.38
Batch: 520; loss: 1.91; acc: 0.36
Batch: 540; loss: 1.92; acc: 0.38
Batch: 560; loss: 1.97; acc: 0.33
Batch: 580; loss: 1.94; acc: 0.39
Batch: 600; loss: 1.92; acc: 0.41
Batch: 620; loss: 1.96; acc: 0.33
Batch: 640; loss: 1.88; acc: 0.48
Batch: 660; loss: 1.91; acc: 0.47
Batch: 680; loss: 1.93; acc: 0.38
Batch: 700; loss: 1.87; acc: 0.44
Batch: 720; loss: 1.91; acc: 0.45
Batch: 740; loss: 1.83; acc: 0.42
Batch: 760; loss: 1.9; acc: 0.44
Batch: 780; loss: 1.84; acc: 0.45
Train Epoch over. train_loss: 2.03; train_accuracy: 0.33 

2.2374948457581922e-05
5.371692168409936e-06
Batch: 0; loss: 1.89; acc: 0.34
Batch: 20; loss: 1.97; acc: 0.39
Batch: 40; loss: 1.72; acc: 0.55
Batch: 60; loss: 1.82; acc: 0.53
Batch: 80; loss: 1.72; acc: 0.61
Batch: 100; loss: 1.88; acc: 0.39
Batch: 120; loss: 1.99; acc: 0.36
Batch: 140; loss: 1.83; acc: 0.52
Val Epoch over. val_loss: 1.8653399967084265; val_accuracy: 0.45919585987261147 

The current subspace-distance is: 5.371692168409936e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.89; acc: 0.45
Batch: 20; loss: 1.85; acc: 0.52
Batch: 40; loss: 1.92; acc: 0.47
Batch: 60; loss: 1.82; acc: 0.47
Batch: 80; loss: 1.82; acc: 0.44
Batch: 100; loss: 1.83; acc: 0.5
Batch: 120; loss: 1.88; acc: 0.38
Batch: 140; loss: 1.95; acc: 0.39
Batch: 160; loss: 1.86; acc: 0.45
Batch: 180; loss: 1.86; acc: 0.47
Batch: 200; loss: 1.9; acc: 0.41
Batch: 220; loss: 1.89; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.47
Batch: 260; loss: 1.76; acc: 0.48
Batch: 280; loss: 1.88; acc: 0.44
Batch: 300; loss: 1.78; acc: 0.47
Batch: 320; loss: 1.87; acc: 0.44
Batch: 340; loss: 1.84; acc: 0.42
Batch: 360; loss: 1.82; acc: 0.48
Batch: 380; loss: 1.87; acc: 0.42
Batch: 400; loss: 1.88; acc: 0.45
Batch: 420; loss: 1.87; acc: 0.36
Batch: 440; loss: 1.9; acc: 0.36
Batch: 460; loss: 1.82; acc: 0.45
Batch: 480; loss: 1.81; acc: 0.42
Batch: 500; loss: 1.77; acc: 0.53
Batch: 520; loss: 1.79; acc: 0.45
Batch: 540; loss: 1.88; acc: 0.39
Batch: 560; loss: 1.76; acc: 0.58
Batch: 580; loss: 1.85; acc: 0.45
Batch: 600; loss: 1.76; acc: 0.53
Batch: 620; loss: 1.81; acc: 0.45
Batch: 640; loss: 1.85; acc: 0.41
Batch: 660; loss: 1.79; acc: 0.53
Batch: 680; loss: 1.8; acc: 0.55
Batch: 700; loss: 1.73; acc: 0.5
Batch: 720; loss: 1.72; acc: 0.45
Batch: 740; loss: 1.83; acc: 0.48
Batch: 760; loss: 1.84; acc: 0.36
Batch: 780; loss: 1.87; acc: 0.45
Train Epoch over. train_loss: 1.85; train_accuracy: 0.44 

2.501541530364193e-05
7.669651495234575e-06
Batch: 0; loss: 1.84; acc: 0.36
Batch: 20; loss: 1.91; acc: 0.33
Batch: 40; loss: 1.63; acc: 0.58
Batch: 60; loss: 1.7; acc: 0.66
Batch: 80; loss: 1.63; acc: 0.53
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.89; acc: 0.41
Batch: 140; loss: 1.71; acc: 0.53
Val Epoch over. val_loss: 1.7816366967122266; val_accuracy: 0.47183519108280253 

The current subspace-distance is: 7.669651495234575e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.47
Batch: 60; loss: 1.75; acc: 0.45
Batch: 80; loss: 1.77; acc: 0.55
Batch: 100; loss: 1.91; acc: 0.44
Batch: 120; loss: 1.73; acc: 0.56
Batch: 140; loss: 1.72; acc: 0.53
Batch: 160; loss: 1.74; acc: 0.55
Batch: 180; loss: 1.85; acc: 0.39
Batch: 200; loss: 1.77; acc: 0.45
Batch: 220; loss: 1.8; acc: 0.44
Batch: 240; loss: 1.75; acc: 0.52
Batch: 260; loss: 1.72; acc: 0.52
Batch: 280; loss: 1.58; acc: 0.67
Batch: 300; loss: 1.71; acc: 0.53
Batch: 320; loss: 1.81; acc: 0.42
Batch: 340; loss: 1.73; acc: 0.53
Batch: 360; loss: 1.83; acc: 0.45
Batch: 380; loss: 1.81; acc: 0.41
Batch: 400; loss: 1.72; acc: 0.45
Batch: 420; loss: 1.79; acc: 0.42
Batch: 440; loss: 1.78; acc: 0.48
Batch: 460; loss: 1.81; acc: 0.52
Batch: 480; loss: 1.77; acc: 0.45
Batch: 500; loss: 1.91; acc: 0.41
Batch: 520; loss: 1.78; acc: 0.42
Batch: 540; loss: 1.75; acc: 0.48
Batch: 560; loss: 1.74; acc: 0.48
Batch: 580; loss: 1.69; acc: 0.53
Batch: 600; loss: 1.63; acc: 0.56
Batch: 620; loss: 1.82; acc: 0.45
Batch: 640; loss: 1.87; acc: 0.47
Batch: 660; loss: 1.79; acc: 0.53
Batch: 680; loss: 1.9; acc: 0.38
Batch: 700; loss: 1.81; acc: 0.5
Batch: 720; loss: 1.86; acc: 0.36
Batch: 740; loss: 1.77; acc: 0.42
Batch: 760; loss: 1.83; acc: 0.52
Batch: 780; loss: 1.75; acc: 0.39
Train Epoch over. train_loss: 1.8; train_accuracy: 0.45 

2.7069425414083526e-05
7.680429916945286e-06
Batch: 0; loss: 1.82; acc: 0.45
Batch: 20; loss: 1.9; acc: 0.3
Batch: 40; loss: 1.57; acc: 0.66
Batch: 60; loss: 1.68; acc: 0.59
Batch: 80; loss: 1.61; acc: 0.55
Batch: 100; loss: 1.77; acc: 0.44
Batch: 120; loss: 1.89; acc: 0.39
Batch: 140; loss: 1.64; acc: 0.55
Val Epoch over. val_loss: 1.748616568601815; val_accuracy: 0.4791998407643312 

The current subspace-distance is: 7.680429916945286e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.82; acc: 0.45
Batch: 40; loss: 1.83; acc: 0.39
Batch: 60; loss: 1.79; acc: 0.48
Batch: 80; loss: 1.74; acc: 0.47
Batch: 100; loss: 1.83; acc: 0.44
Batch: 120; loss: 1.86; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.48
Batch: 160; loss: 1.93; acc: 0.31
Batch: 180; loss: 1.78; acc: 0.5
Batch: 200; loss: 1.67; acc: 0.47
Batch: 220; loss: 1.87; acc: 0.41
Batch: 240; loss: 1.96; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.47
Batch: 280; loss: 1.72; acc: 0.5
Batch: 300; loss: 1.74; acc: 0.48
Batch: 320; loss: 1.71; acc: 0.47
Batch: 340; loss: 1.7; acc: 0.52
Batch: 360; loss: 1.76; acc: 0.45
Batch: 380; loss: 1.78; acc: 0.48
Batch: 400; loss: 1.72; acc: 0.47
Batch: 420; loss: 1.74; acc: 0.45
Batch: 440; loss: 1.77; acc: 0.5
Batch: 460; loss: 1.8; acc: 0.48
Batch: 480; loss: 1.74; acc: 0.47
Batch: 500; loss: 1.82; acc: 0.45
Batch: 520; loss: 1.74; acc: 0.55
Batch: 540; loss: 1.76; acc: 0.44
Batch: 560; loss: 1.8; acc: 0.34
Batch: 580; loss: 1.76; acc: 0.42
Batch: 600; loss: 1.77; acc: 0.44
Batch: 620; loss: 1.72; acc: 0.53
Batch: 640; loss: 1.79; acc: 0.52
Batch: 660; loss: 1.75; acc: 0.42
Batch: 680; loss: 1.73; acc: 0.48
Batch: 700; loss: 1.74; acc: 0.45
Batch: 720; loss: 1.74; acc: 0.47
Batch: 740; loss: 1.66; acc: 0.56
Batch: 760; loss: 1.74; acc: 0.38
Batch: 780; loss: 1.69; acc: 0.53
Train Epoch over. train_loss: 1.78; train_accuracy: 0.46 

2.8979784474358894e-05
7.637559065187816e-06
Batch: 0; loss: 1.83; acc: 0.44
Batch: 20; loss: 1.89; acc: 0.3
Batch: 40; loss: 1.55; acc: 0.59
Batch: 60; loss: 1.66; acc: 0.56
Batch: 80; loss: 1.61; acc: 0.5
Batch: 100; loss: 1.77; acc: 0.47
Batch: 120; loss: 1.9; acc: 0.41
Batch: 140; loss: 1.62; acc: 0.56
Val Epoch over. val_loss: 1.736941628395372; val_accuracy: 0.4781050955414013 

The current subspace-distance is: 7.637559065187816e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.71; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.5
Batch: 40; loss: 1.78; acc: 0.47
Batch: 60; loss: 1.93; acc: 0.39
Batch: 80; loss: 1.74; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.61
Batch: 120; loss: 1.79; acc: 0.41
Batch: 140; loss: 1.97; acc: 0.31
Batch: 160; loss: 1.84; acc: 0.44
Batch: 180; loss: 1.74; acc: 0.48
Batch: 200; loss: 1.7; acc: 0.53
Batch: 220; loss: 1.75; acc: 0.5
Batch: 240; loss: 1.73; acc: 0.45
Batch: 260; loss: 1.7; acc: 0.42
Batch: 280; loss: 1.8; acc: 0.36
Batch: 300; loss: 1.75; acc: 0.53
Batch: 320; loss: 1.77; acc: 0.45
Batch: 340; loss: 1.72; acc: 0.53
Batch: 360; loss: 1.63; acc: 0.58
Batch: 380; loss: 1.78; acc: 0.41
Batch: 400; loss: 1.74; acc: 0.48
Batch: 420; loss: 1.77; acc: 0.44
Batch: 440; loss: 1.95; acc: 0.34
Batch: 460; loss: 1.84; acc: 0.38
Batch: 480; loss: 1.7; acc: 0.56
Batch: 500; loss: 1.84; acc: 0.41
Batch: 520; loss: 1.73; acc: 0.48
Batch: 540; loss: 1.73; acc: 0.47
Batch: 560; loss: 1.64; acc: 0.58
Batch: 580; loss: 1.76; acc: 0.52
Batch: 600; loss: 1.81; acc: 0.53
Batch: 620; loss: 1.8; acc: 0.38
Batch: 640; loss: 1.82; acc: 0.39
Batch: 660; loss: 1.74; acc: 0.52
Batch: 680; loss: 1.73; acc: 0.5
Batch: 700; loss: 1.73; acc: 0.44
Batch: 720; loss: 1.69; acc: 0.56
Batch: 740; loss: 1.85; acc: 0.38
Batch: 760; loss: 1.76; acc: 0.47
Batch: 780; loss: 1.9; acc: 0.36
Train Epoch over. train_loss: 1.76; train_accuracy: 0.47 

2.9329648896236904e-05
7.72731436882168e-06
Batch: 0; loss: 1.8; acc: 0.5
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.5; acc: 0.62
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.59; acc: 0.53
Batch: 100; loss: 1.72; acc: 0.47
Batch: 120; loss: 1.86; acc: 0.36
Batch: 140; loss: 1.57; acc: 0.59
Val Epoch over. val_loss: 1.7040598103954534; val_accuracy: 0.5004976114649682 

The current subspace-distance is: 7.72731436882168e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.53
Batch: 20; loss: 1.66; acc: 0.55
Batch: 40; loss: 1.72; acc: 0.48
Batch: 60; loss: 1.71; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.56
Batch: 100; loss: 1.7; acc: 0.47
Batch: 120; loss: 1.74; acc: 0.45
Batch: 140; loss: 1.71; acc: 0.47
Batch: 160; loss: 1.84; acc: 0.39
Batch: 180; loss: 1.83; acc: 0.44
Batch: 200; loss: 1.76; acc: 0.45
Batch: 220; loss: 1.83; acc: 0.45
Batch: 240; loss: 1.7; acc: 0.52
Batch: 260; loss: 1.69; acc: 0.55
Batch: 280; loss: 1.85; acc: 0.41
Batch: 300; loss: 1.68; acc: 0.55
Batch: 320; loss: 1.81; acc: 0.42
Batch: 340; loss: 1.67; acc: 0.55
Batch: 360; loss: 1.64; acc: 0.53
Batch: 380; loss: 1.7; acc: 0.55
Batch: 400; loss: 1.75; acc: 0.48
Batch: 420; loss: 1.77; acc: 0.38
Batch: 440; loss: 1.73; acc: 0.48
Batch: 460; loss: 1.67; acc: 0.53
Batch: 480; loss: 1.7; acc: 0.55
Batch: 500; loss: 1.62; acc: 0.61
Batch: 520; loss: 1.8; acc: 0.41
Batch: 540; loss: 1.7; acc: 0.47
Batch: 560; loss: 1.75; acc: 0.47
Batch: 580; loss: 1.81; acc: 0.44
Batch: 600; loss: 1.61; acc: 0.58
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.75; acc: 0.48
Batch: 660; loss: 1.63; acc: 0.56
Batch: 680; loss: 1.77; acc: 0.47
Batch: 700; loss: 1.66; acc: 0.52
Batch: 720; loss: 1.69; acc: 0.47
Batch: 740; loss: 1.78; acc: 0.47
Batch: 760; loss: 1.71; acc: 0.52
Batch: 780; loss: 1.57; acc: 0.59
Train Epoch over. train_loss: 1.72; train_accuracy: 0.49 

3.1789601052878425e-05
1.043848715198692e-05
Batch: 0; loss: 1.74; acc: 0.45
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.46; acc: 0.62
Batch: 60; loss: 1.59; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.59
Batch: 100; loss: 1.67; acc: 0.56
Batch: 120; loss: 1.83; acc: 0.44
Batch: 140; loss: 1.5; acc: 0.64
Val Epoch over. val_loss: 1.6562545261565287; val_accuracy: 0.5364251592356688 

The current subspace-distance is: 1.043848715198692e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.7; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.44
Batch: 40; loss: 1.67; acc: 0.5
Batch: 60; loss: 1.8; acc: 0.44
Batch: 80; loss: 1.64; acc: 0.53
Batch: 100; loss: 1.76; acc: 0.44
Batch: 120; loss: 1.65; acc: 0.56
Batch: 140; loss: 1.65; acc: 0.53
Batch: 160; loss: 1.67; acc: 0.58
Batch: 180; loss: 1.6; acc: 0.56
Batch: 200; loss: 1.64; acc: 0.56
Batch: 220; loss: 1.57; acc: 0.58
Batch: 240; loss: 1.71; acc: 0.42
Batch: 260; loss: 1.57; acc: 0.61
Batch: 280; loss: 1.65; acc: 0.48
Batch: 300; loss: 1.67; acc: 0.45
Batch: 320; loss: 1.73; acc: 0.47
Batch: 340; loss: 1.73; acc: 0.5
Batch: 360; loss: 1.68; acc: 0.5
Batch: 380; loss: 1.6; acc: 0.53
Batch: 400; loss: 1.65; acc: 0.53
Batch: 420; loss: 1.56; acc: 0.56
Batch: 440; loss: 1.6; acc: 0.56
Batch: 460; loss: 1.71; acc: 0.5
Batch: 480; loss: 1.7; acc: 0.5
Batch: 500; loss: 1.7; acc: 0.52
Batch: 520; loss: 1.79; acc: 0.39
Batch: 540; loss: 1.66; acc: 0.47
Batch: 560; loss: 1.67; acc: 0.52
Batch: 580; loss: 1.66; acc: 0.48
Batch: 600; loss: 1.77; acc: 0.5
Batch: 620; loss: 1.66; acc: 0.45
Batch: 640; loss: 1.71; acc: 0.5
Batch: 660; loss: 1.66; acc: 0.47
Batch: 680; loss: 1.59; acc: 0.56
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.61; acc: 0.61
Batch: 740; loss: 1.65; acc: 0.59
Batch: 760; loss: 1.67; acc: 0.45
Batch: 780; loss: 1.53; acc: 0.61
Train Epoch over. train_loss: 1.68; train_accuracy: 0.51 

3.397427644813433e-05
1.0959646715491544e-05
Batch: 0; loss: 1.7; acc: 0.52
Batch: 20; loss: 1.89; acc: 0.36
Batch: 40; loss: 1.46; acc: 0.59
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.49; acc: 0.64
Batch: 100; loss: 1.64; acc: 0.59
Batch: 120; loss: 1.79; acc: 0.42
Batch: 140; loss: 1.46; acc: 0.61
Val Epoch over. val_loss: 1.629304332338321; val_accuracy: 0.5459792993630573 

The current subspace-distance is: 1.0959646715491544e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.55
Batch: 20; loss: 1.65; acc: 0.52
Batch: 40; loss: 1.6; acc: 0.55
Batch: 60; loss: 1.75; acc: 0.5
Batch: 80; loss: 1.7; acc: 0.42
Batch: 100; loss: 1.67; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.55
Batch: 140; loss: 1.68; acc: 0.5
Batch: 160; loss: 1.66; acc: 0.44
Batch: 180; loss: 1.63; acc: 0.55
Batch: 200; loss: 1.69; acc: 0.47
Batch: 220; loss: 1.79; acc: 0.5
Batch: 240; loss: 1.58; acc: 0.55
Batch: 260; loss: 1.57; acc: 0.55
Batch: 280; loss: 1.6; acc: 0.53
Batch: 300; loss: 1.58; acc: 0.55
Batch: 320; loss: 1.62; acc: 0.53
Batch: 340; loss: 1.65; acc: 0.61
Batch: 360; loss: 1.67; acc: 0.55
Batch: 380; loss: 1.58; acc: 0.56
Batch: 400; loss: 1.64; acc: 0.58
Batch: 420; loss: 1.69; acc: 0.53
Batch: 440; loss: 1.68; acc: 0.53
Batch: 460; loss: 1.65; acc: 0.47
Batch: 480; loss: 1.58; acc: 0.53
Batch: 500; loss: 1.67; acc: 0.5
Batch: 520; loss: 1.71; acc: 0.48
Batch: 540; loss: 1.59; acc: 0.56
Batch: 560; loss: 1.72; acc: 0.53
Batch: 580; loss: 1.67; acc: 0.47
Batch: 600; loss: 1.71; acc: 0.58
Batch: 620; loss: 1.67; acc: 0.47
Batch: 640; loss: 1.71; acc: 0.47
Batch: 660; loss: 1.67; acc: 0.52
Batch: 680; loss: 1.62; acc: 0.58
Batch: 700; loss: 1.71; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.52
Batch: 740; loss: 1.76; acc: 0.45
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.77; acc: 0.5
Train Epoch over. train_loss: 1.65; train_accuracy: 0.52 

3.5764627682510763e-05
1.3366606253839564e-05
Batch: 0; loss: 1.64; acc: 0.5
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.42; acc: 0.58
Batch: 60; loss: 1.5; acc: 0.58
Batch: 80; loss: 1.43; acc: 0.61
Batch: 100; loss: 1.62; acc: 0.56
Batch: 120; loss: 1.74; acc: 0.42
Batch: 140; loss: 1.41; acc: 0.61
Val Epoch over. val_loss: 1.5899220492429793; val_accuracy: 0.5456807324840764 

The current subspace-distance is: 1.3366606253839564e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.48
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.66; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.58
Batch: 80; loss: 1.58; acc: 0.52
Batch: 100; loss: 1.81; acc: 0.39
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.6; acc: 0.53
Batch: 160; loss: 1.6; acc: 0.45
Batch: 180; loss: 1.56; acc: 0.58
Batch: 200; loss: 1.57; acc: 0.52
Batch: 220; loss: 1.65; acc: 0.45
Batch: 240; loss: 1.77; acc: 0.38
Batch: 260; loss: 1.74; acc: 0.42
Batch: 280; loss: 1.58; acc: 0.48
Batch: 300; loss: 1.7; acc: 0.41
Batch: 320; loss: 1.61; acc: 0.45
Batch: 340; loss: 1.63; acc: 0.48
Batch: 360; loss: 1.59; acc: 0.56
Batch: 380; loss: 1.66; acc: 0.55
Batch: 400; loss: 1.6; acc: 0.52
Batch: 420; loss: 1.62; acc: 0.53
Batch: 440; loss: 1.67; acc: 0.48
Batch: 460; loss: 1.7; acc: 0.41
Batch: 480; loss: 1.88; acc: 0.34
Batch: 500; loss: 1.64; acc: 0.53
Batch: 520; loss: 1.65; acc: 0.59
Batch: 540; loss: 1.68; acc: 0.55
Batch: 560; loss: 1.59; acc: 0.53
Batch: 580; loss: 1.57; acc: 0.45
Batch: 600; loss: 1.68; acc: 0.48
Batch: 620; loss: 1.58; acc: 0.55
Batch: 640; loss: 1.69; acc: 0.48
Batch: 660; loss: 1.6; acc: 0.52
Batch: 680; loss: 1.57; acc: 0.56
Batch: 700; loss: 1.48; acc: 0.61
Batch: 720; loss: 1.59; acc: 0.55
Batch: 740; loss: 1.56; acc: 0.55
Batch: 760; loss: 1.56; acc: 0.52
Batch: 780; loss: 1.59; acc: 0.5
Train Epoch over. train_loss: 1.63; train_accuracy: 0.52 

3.8121335819596425e-05
1.46835218401975e-05
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.4; acc: 0.66
Batch: 60; loss: 1.48; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.58
Batch: 120; loss: 1.72; acc: 0.42
Batch: 140; loss: 1.37; acc: 0.66
Val Epoch over. val_loss: 1.5769086552273697; val_accuracy: 0.5456807324840764 

The current subspace-distance is: 1.46835218401975e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.74; acc: 0.48
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.63; acc: 0.53
Batch: 100; loss: 1.66; acc: 0.52
Batch: 120; loss: 1.66; acc: 0.45
Batch: 140; loss: 1.45; acc: 0.64
Batch: 160; loss: 1.63; acc: 0.52
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.49; acc: 0.61
Batch: 220; loss: 1.57; acc: 0.59
Batch: 240; loss: 1.75; acc: 0.44
Batch: 260; loss: 1.63; acc: 0.5
Batch: 280; loss: 1.65; acc: 0.45
Batch: 300; loss: 1.61; acc: 0.55
Batch: 320; loss: 1.67; acc: 0.47
Batch: 340; loss: 1.72; acc: 0.53
Batch: 360; loss: 1.56; acc: 0.52
Batch: 380; loss: 1.58; acc: 0.55
Batch: 400; loss: 1.77; acc: 0.48
Batch: 420; loss: 1.57; acc: 0.52
Batch: 440; loss: 1.48; acc: 0.56
Batch: 460; loss: 1.57; acc: 0.56
Batch: 480; loss: 1.54; acc: 0.53
Batch: 500; loss: 1.47; acc: 0.52
Batch: 520; loss: 1.7; acc: 0.45
Batch: 540; loss: 1.57; acc: 0.48
Batch: 560; loss: 1.56; acc: 0.53
Batch: 580; loss: 1.57; acc: 0.59
Batch: 600; loss: 1.43; acc: 0.67
Batch: 620; loss: 1.65; acc: 0.47
Batch: 640; loss: 1.72; acc: 0.48
Batch: 660; loss: 1.54; acc: 0.53
Batch: 680; loss: 1.72; acc: 0.48
Batch: 700; loss: 1.47; acc: 0.58
Batch: 720; loss: 1.65; acc: 0.47
Batch: 740; loss: 1.63; acc: 0.56
Batch: 760; loss: 1.65; acc: 0.56
Batch: 780; loss: 1.54; acc: 0.55
Train Epoch over. train_loss: 1.62; train_accuracy: 0.51 

3.787482273764908e-05
1.2649545169551857e-05
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.89; acc: 0.39
Batch: 40; loss: 1.4; acc: 0.62
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.4; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.71; acc: 0.38
Batch: 140; loss: 1.36; acc: 0.66
Val Epoch over. val_loss: 1.5723721851968462; val_accuracy: 0.5381170382165605 

The current subspace-distance is: 1.2649545169551857e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.56
Batch: 20; loss: 1.59; acc: 0.55
Batch: 40; loss: 1.59; acc: 0.53
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.7; acc: 0.44
Batch: 100; loss: 1.68; acc: 0.48
Batch: 120; loss: 1.51; acc: 0.62
Batch: 140; loss: 1.55; acc: 0.52
Batch: 160; loss: 1.63; acc: 0.53
Batch: 180; loss: 1.66; acc: 0.48
Batch: 200; loss: 1.69; acc: 0.44
Batch: 220; loss: 1.69; acc: 0.47
Batch: 240; loss: 1.57; acc: 0.56
Batch: 260; loss: 1.74; acc: 0.42
Batch: 280; loss: 1.77; acc: 0.41
Batch: 300; loss: 1.68; acc: 0.53
Batch: 320; loss: 1.57; acc: 0.55
Batch: 340; loss: 1.67; acc: 0.45
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.59; acc: 0.45
Batch: 400; loss: 1.6; acc: 0.48
Batch: 420; loss: 1.63; acc: 0.52
Batch: 440; loss: 1.5; acc: 0.56
Batch: 460; loss: 1.53; acc: 0.56
Batch: 480; loss: 1.64; acc: 0.45
Batch: 500; loss: 1.59; acc: 0.53
Batch: 520; loss: 1.5; acc: 0.61
Batch: 540; loss: 1.7; acc: 0.47
Batch: 560; loss: 1.56; acc: 0.62
Batch: 580; loss: 1.64; acc: 0.47
Batch: 600; loss: 1.53; acc: 0.58
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.55; acc: 0.47
Batch: 660; loss: 1.53; acc: 0.59
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.73; acc: 0.44
Batch: 720; loss: 1.62; acc: 0.47
Batch: 740; loss: 1.64; acc: 0.44
Batch: 760; loss: 1.73; acc: 0.41
Batch: 780; loss: 1.6; acc: 0.55
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

3.949960955651477e-05
1.6124775356729515e-05
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.9; acc: 0.33
Batch: 40; loss: 1.39; acc: 0.61
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.39; acc: 0.58
Batch: 100; loss: 1.61; acc: 0.5
Batch: 120; loss: 1.71; acc: 0.39
Batch: 140; loss: 1.36; acc: 0.62
Val Epoch over. val_loss: 1.5704834058785895; val_accuracy: 0.5352308917197452 

The current subspace-distance is: 1.6124775356729515e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.52; acc: 0.62
Batch: 40; loss: 1.75; acc: 0.38
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.53
Batch: 100; loss: 1.58; acc: 0.48
Batch: 120; loss: 1.8; acc: 0.39
Batch: 140; loss: 1.53; acc: 0.64
Batch: 160; loss: 1.6; acc: 0.42
Batch: 180; loss: 1.5; acc: 0.59
Batch: 200; loss: 1.51; acc: 0.62
Batch: 220; loss: 1.55; acc: 0.58
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.74; acc: 0.44
Batch: 280; loss: 1.63; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.53
Batch: 320; loss: 1.64; acc: 0.56
Batch: 340; loss: 1.64; acc: 0.5
Batch: 360; loss: 1.7; acc: 0.52
Batch: 380; loss: 1.69; acc: 0.41
Batch: 400; loss: 1.6; acc: 0.45
Batch: 420; loss: 1.66; acc: 0.5
Batch: 440; loss: 1.63; acc: 0.47
Batch: 460; loss: 1.69; acc: 0.44
Batch: 480; loss: 1.7; acc: 0.45
Batch: 500; loss: 1.57; acc: 0.52
Batch: 520; loss: 1.62; acc: 0.48
Batch: 540; loss: 1.55; acc: 0.53
Batch: 560; loss: 1.7; acc: 0.42
Batch: 580; loss: 1.7; acc: 0.47
Batch: 600; loss: 1.49; acc: 0.55
Batch: 620; loss: 1.6; acc: 0.52
Batch: 640; loss: 1.8; acc: 0.38
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.68; acc: 0.45
Batch: 700; loss: 1.48; acc: 0.58
Batch: 720; loss: 1.58; acc: 0.52
Batch: 740; loss: 1.63; acc: 0.5
Batch: 760; loss: 1.56; acc: 0.56
Batch: 780; loss: 1.69; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.001036359113641e-05
1.631770828680601e-05
Batch: 0; loss: 1.59; acc: 0.53
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.62
Batch: 60; loss: 1.46; acc: 0.58
Batch: 80; loss: 1.38; acc: 0.61
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.34; acc: 0.67
Val Epoch over. val_loss: 1.560989724602669; val_accuracy: 0.540406050955414 

The current subspace-distance is: 1.631770828680601e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.48; acc: 0.7
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.53; acc: 0.59
Batch: 60; loss: 1.8; acc: 0.38
Batch: 80; loss: 1.64; acc: 0.47
Batch: 100; loss: 1.57; acc: 0.52
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.73; acc: 0.47
Batch: 160; loss: 1.51; acc: 0.55
Batch: 180; loss: 1.64; acc: 0.48
Batch: 200; loss: 1.52; acc: 0.58
Batch: 220; loss: 1.67; acc: 0.45
Batch: 240; loss: 1.72; acc: 0.44
Batch: 260; loss: 1.42; acc: 0.62
Batch: 280; loss: 1.59; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.56
Batch: 320; loss: 1.63; acc: 0.53
Batch: 340; loss: 1.58; acc: 0.48
Batch: 360; loss: 1.51; acc: 0.59
Batch: 380; loss: 1.6; acc: 0.55
Batch: 400; loss: 1.57; acc: 0.56
Batch: 420; loss: 1.74; acc: 0.38
Batch: 440; loss: 1.53; acc: 0.58
Batch: 460; loss: 1.8; acc: 0.44
Batch: 480; loss: 1.63; acc: 0.58
Batch: 500; loss: 1.59; acc: 0.58
Batch: 520; loss: 1.7; acc: 0.48
Batch: 540; loss: 1.62; acc: 0.52
Batch: 560; loss: 1.68; acc: 0.48
Batch: 580; loss: 1.48; acc: 0.58
Batch: 600; loss: 1.59; acc: 0.56
Batch: 620; loss: 1.67; acc: 0.44
Batch: 640; loss: 1.66; acc: 0.5
Batch: 660; loss: 1.53; acc: 0.53
Batch: 680; loss: 1.68; acc: 0.42
Batch: 700; loss: 1.45; acc: 0.61
Batch: 720; loss: 1.54; acc: 0.52
Batch: 740; loss: 1.62; acc: 0.45
Batch: 760; loss: 1.69; acc: 0.42
Batch: 780; loss: 1.51; acc: 0.59
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

3.994209328084253e-05
1.6074436643975787e-05
Batch: 0; loss: 1.61; acc: 0.55
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.4; acc: 0.62
Batch: 60; loss: 1.48; acc: 0.55
Batch: 80; loss: 1.4; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.45
Batch: 120; loss: 1.71; acc: 0.39
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.5825346305871466; val_accuracy: 0.5287619426751592 

The current subspace-distance is: 1.6074436643975787e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.42
Batch: 20; loss: 1.64; acc: 0.45
Batch: 40; loss: 1.64; acc: 0.48
Batch: 60; loss: 1.57; acc: 0.48
Batch: 80; loss: 1.55; acc: 0.45
Batch: 100; loss: 1.79; acc: 0.36
Batch: 120; loss: 1.74; acc: 0.41
Batch: 140; loss: 1.66; acc: 0.44
Batch: 160; loss: 1.73; acc: 0.44
Batch: 180; loss: 1.52; acc: 0.5
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.43; acc: 0.62
Batch: 240; loss: 1.66; acc: 0.47
Batch: 260; loss: 1.51; acc: 0.62
Batch: 280; loss: 1.41; acc: 0.64
Batch: 300; loss: 1.47; acc: 0.59
Batch: 320; loss: 1.7; acc: 0.45
Batch: 340; loss: 1.45; acc: 0.62
Batch: 360; loss: 1.64; acc: 0.45
Batch: 380; loss: 1.65; acc: 0.56
Batch: 400; loss: 1.62; acc: 0.47
Batch: 420; loss: 1.62; acc: 0.47
Batch: 440; loss: 1.58; acc: 0.5
Batch: 460; loss: 1.55; acc: 0.5
Batch: 480; loss: 1.6; acc: 0.48
Batch: 500; loss: 1.68; acc: 0.41
Batch: 520; loss: 1.67; acc: 0.47
Batch: 540; loss: 1.77; acc: 0.48
Batch: 560; loss: 1.67; acc: 0.52
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.61; acc: 0.53
Batch: 620; loss: 1.65; acc: 0.45
Batch: 640; loss: 1.65; acc: 0.39
Batch: 660; loss: 1.58; acc: 0.58
Batch: 680; loss: 1.75; acc: 0.45
Batch: 700; loss: 1.75; acc: 0.5
Batch: 720; loss: 1.53; acc: 0.56
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.63; acc: 0.52
Batch: 780; loss: 1.47; acc: 0.58
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.122940299566835e-05
1.8030757928499952e-05
Batch: 0; loss: 1.6; acc: 0.55
Batch: 20; loss: 1.89; acc: 0.36
Batch: 40; loss: 1.4; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.39; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.47
Batch: 120; loss: 1.7; acc: 0.41
Batch: 140; loss: 1.35; acc: 0.69
Val Epoch over. val_loss: 1.569615373186245; val_accuracy: 0.5366242038216561 

The current subspace-distance is: 1.8030757928499952e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.47
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.56
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.64; acc: 0.45
Batch: 100; loss: 1.68; acc: 0.48
Batch: 120; loss: 1.62; acc: 0.42
Batch: 140; loss: 1.58; acc: 0.56
Batch: 160; loss: 1.45; acc: 0.53
Batch: 180; loss: 1.66; acc: 0.48
Batch: 200; loss: 1.5; acc: 0.53
Batch: 220; loss: 1.59; acc: 0.55
Batch: 240; loss: 1.53; acc: 0.58
Batch: 260; loss: 1.53; acc: 0.62
Batch: 280; loss: 1.71; acc: 0.47
Batch: 300; loss: 1.66; acc: 0.45
Batch: 320; loss: 1.45; acc: 0.62
Batch: 340; loss: 1.8; acc: 0.48
Batch: 360; loss: 1.71; acc: 0.48
Batch: 380; loss: 1.5; acc: 0.59
Batch: 400; loss: 1.64; acc: 0.52
Batch: 420; loss: 1.6; acc: 0.45
Batch: 440; loss: 1.67; acc: 0.45
Batch: 460; loss: 1.53; acc: 0.55
Batch: 480; loss: 1.56; acc: 0.47
Batch: 500; loss: 1.62; acc: 0.59
Batch: 520; loss: 1.55; acc: 0.5
Batch: 540; loss: 1.5; acc: 0.58
Batch: 560; loss: 1.53; acc: 0.53
Batch: 580; loss: 1.57; acc: 0.61
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.64; acc: 0.44
Batch: 640; loss: 1.56; acc: 0.61
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.51; acc: 0.56
Batch: 700; loss: 1.63; acc: 0.42
Batch: 720; loss: 1.54; acc: 0.55
Batch: 740; loss: 1.66; acc: 0.42
Batch: 760; loss: 1.73; acc: 0.5
Batch: 780; loss: 1.83; acc: 0.36
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.008814357803203e-05
1.3396343092608731e-05
Batch: 0; loss: 1.58; acc: 0.55
Batch: 20; loss: 1.87; acc: 0.38
Batch: 40; loss: 1.38; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.39; acc: 0.59
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.68; acc: 0.42
Batch: 140; loss: 1.33; acc: 0.7
Val Epoch over. val_loss: 1.5602928498748; val_accuracy: 0.5408041401273885 

The current subspace-distance is: 1.3396343092608731e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.56
Batch: 20; loss: 1.74; acc: 0.41
Batch: 40; loss: 1.59; acc: 0.53
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.68; acc: 0.41
Batch: 100; loss: 1.66; acc: 0.45
Batch: 120; loss: 1.65; acc: 0.45
Batch: 140; loss: 1.69; acc: 0.39
Batch: 160; loss: 1.6; acc: 0.53
Batch: 180; loss: 1.65; acc: 0.45
Batch: 200; loss: 1.53; acc: 0.52
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.49; acc: 0.64
Batch: 260; loss: 1.53; acc: 0.53
Batch: 280; loss: 1.73; acc: 0.38
Batch: 300; loss: 1.57; acc: 0.52
Batch: 320; loss: 1.68; acc: 0.55
Batch: 340; loss: 1.67; acc: 0.39
Batch: 360; loss: 1.65; acc: 0.5
Batch: 380; loss: 1.62; acc: 0.48
Batch: 400; loss: 1.67; acc: 0.44
Batch: 420; loss: 1.53; acc: 0.62
Batch: 440; loss: 1.53; acc: 0.61
Batch: 460; loss: 1.62; acc: 0.41
Batch: 480; loss: 1.56; acc: 0.62
Batch: 500; loss: 1.48; acc: 0.58
Batch: 520; loss: 1.68; acc: 0.47
Batch: 540; loss: 1.58; acc: 0.52
Batch: 560; loss: 1.54; acc: 0.55
Batch: 580; loss: 1.62; acc: 0.56
Batch: 600; loss: 1.71; acc: 0.41
Batch: 620; loss: 1.68; acc: 0.48
Batch: 640; loss: 1.61; acc: 0.52
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.72; acc: 0.41
Batch: 700; loss: 1.65; acc: 0.48
Batch: 720; loss: 1.58; acc: 0.5
Batch: 740; loss: 1.58; acc: 0.5
Batch: 760; loss: 1.54; acc: 0.56
Batch: 780; loss: 1.46; acc: 0.59
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.0579845517640933e-05
1.3733216292166617e-05
Batch: 0; loss: 1.6; acc: 0.56
Batch: 20; loss: 1.89; acc: 0.36
Batch: 40; loss: 1.39; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.47
Batch: 120; loss: 1.7; acc: 0.39
Batch: 140; loss: 1.33; acc: 0.7
Val Epoch over. val_loss: 1.5662894636202769; val_accuracy: 0.5367237261146497 

The current subspace-distance is: 1.3733216292166617e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.45
Batch: 20; loss: 1.54; acc: 0.5
Batch: 40; loss: 1.46; acc: 0.58
Batch: 60; loss: 1.65; acc: 0.48
Batch: 80; loss: 1.64; acc: 0.52
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.66; acc: 0.45
Batch: 140; loss: 1.63; acc: 0.48
Batch: 160; loss: 1.59; acc: 0.56
Batch: 180; loss: 1.55; acc: 0.55
Batch: 200; loss: 1.63; acc: 0.53
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.7; acc: 0.48
Batch: 260; loss: 1.66; acc: 0.47
Batch: 280; loss: 1.62; acc: 0.55
Batch: 300; loss: 1.62; acc: 0.56
Batch: 320; loss: 1.53; acc: 0.59
Batch: 340; loss: 1.64; acc: 0.45
Batch: 360; loss: 1.65; acc: 0.5
Batch: 380; loss: 1.5; acc: 0.59
Batch: 400; loss: 1.66; acc: 0.45
Batch: 420; loss: 1.63; acc: 0.55
Batch: 440; loss: 1.59; acc: 0.48
Batch: 460; loss: 1.58; acc: 0.5
Batch: 480; loss: 1.6; acc: 0.53
Batch: 500; loss: 1.68; acc: 0.45
Batch: 520; loss: 1.62; acc: 0.48
Batch: 540; loss: 1.53; acc: 0.55
Batch: 560; loss: 1.73; acc: 0.39
Batch: 580; loss: 1.53; acc: 0.61
Batch: 600; loss: 1.73; acc: 0.42
Batch: 620; loss: 1.62; acc: 0.48
Batch: 640; loss: 1.47; acc: 0.61
Batch: 660; loss: 1.68; acc: 0.45
Batch: 680; loss: 1.61; acc: 0.47
Batch: 700; loss: 1.6; acc: 0.5
Batch: 720; loss: 1.58; acc: 0.55
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.74; acc: 0.47
Batch: 780; loss: 1.52; acc: 0.59
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.120243829675019e-05
1.4869694496155716e-05
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.38; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.61
Batch: 80; loss: 1.38; acc: 0.59
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.42
Batch: 140; loss: 1.34; acc: 0.67
Val Epoch over. val_loss: 1.565745772829481; val_accuracy: 0.5299562101910829 

The current subspace-distance is: 1.4869694496155716e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.54; acc: 0.53
Batch: 40; loss: 1.6; acc: 0.55
Batch: 60; loss: 1.59; acc: 0.47
Batch: 80; loss: 1.61; acc: 0.48
Batch: 100; loss: 1.54; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.74; acc: 0.45
Batch: 160; loss: 1.62; acc: 0.47
Batch: 180; loss: 1.71; acc: 0.56
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 1.48; acc: 0.62
Batch: 240; loss: 1.46; acc: 0.59
Batch: 260; loss: 1.53; acc: 0.53
Batch: 280; loss: 1.59; acc: 0.55
Batch: 300; loss: 1.73; acc: 0.52
Batch: 320; loss: 1.67; acc: 0.5
Batch: 340; loss: 1.69; acc: 0.45
Batch: 360; loss: 1.67; acc: 0.42
Batch: 380; loss: 1.66; acc: 0.5
Batch: 400; loss: 1.65; acc: 0.41
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.51; acc: 0.55
Batch: 460; loss: 1.6; acc: 0.47
Batch: 480; loss: 1.59; acc: 0.53
Batch: 500; loss: 1.53; acc: 0.5
Batch: 520; loss: 1.64; acc: 0.5
Batch: 540; loss: 1.63; acc: 0.5
Batch: 560; loss: 1.62; acc: 0.53
Batch: 580; loss: 1.62; acc: 0.52
Batch: 600; loss: 1.53; acc: 0.53
Batch: 620; loss: 1.56; acc: 0.52
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.54; acc: 0.47
Batch: 680; loss: 1.57; acc: 0.53
Batch: 700; loss: 1.54; acc: 0.52
Batch: 720; loss: 1.64; acc: 0.44
Batch: 740; loss: 1.65; acc: 0.48
Batch: 760; loss: 1.6; acc: 0.52
Batch: 780; loss: 1.65; acc: 0.53
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.1546591091901064e-05
1.6138963474077173e-05
Batch: 0; loss: 1.6; acc: 0.59
Batch: 20; loss: 1.89; acc: 0.36
Batch: 40; loss: 1.38; acc: 0.64
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.58
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.42
Batch: 140; loss: 1.33; acc: 0.69
Val Epoch over. val_loss: 1.562281093779643; val_accuracy: 0.5376194267515924 

The current subspace-distance is: 1.6138963474077173e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.69; acc: 0.41
Batch: 60; loss: 1.73; acc: 0.41
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.61; acc: 0.44
Batch: 160; loss: 1.61; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.52
Batch: 200; loss: 1.74; acc: 0.44
Batch: 220; loss: 1.58; acc: 0.52
Batch: 240; loss: 1.5; acc: 0.58
Batch: 260; loss: 1.69; acc: 0.47
Batch: 280; loss: 1.53; acc: 0.55
Batch: 300; loss: 1.66; acc: 0.52
Batch: 320; loss: 1.64; acc: 0.5
Batch: 340; loss: 1.57; acc: 0.48
Batch: 360; loss: 1.56; acc: 0.44
Batch: 380; loss: 1.58; acc: 0.56
Batch: 400; loss: 1.72; acc: 0.5
Batch: 420; loss: 1.7; acc: 0.5
Batch: 440; loss: 1.57; acc: 0.55
Batch: 460; loss: 1.46; acc: 0.61
Batch: 480; loss: 1.56; acc: 0.53
Batch: 500; loss: 1.65; acc: 0.52
Batch: 520; loss: 1.62; acc: 0.47
Batch: 540; loss: 1.64; acc: 0.52
Batch: 560; loss: 1.65; acc: 0.5
Batch: 580; loss: 1.65; acc: 0.44
Batch: 600; loss: 1.71; acc: 0.45
Batch: 620; loss: 1.55; acc: 0.52
Batch: 640; loss: 1.46; acc: 0.55
Batch: 660; loss: 1.62; acc: 0.39
Batch: 680; loss: 1.65; acc: 0.5
Batch: 700; loss: 1.71; acc: 0.47
Batch: 720; loss: 1.64; acc: 0.52
Batch: 740; loss: 1.56; acc: 0.58
Batch: 760; loss: 1.57; acc: 0.48
Batch: 780; loss: 1.55; acc: 0.56
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.123068356420845e-05
1.6689289623172954e-05
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.89; acc: 0.36
Batch: 40; loss: 1.39; acc: 0.64
Batch: 60; loss: 1.48; acc: 0.53
Batch: 80; loss: 1.39; acc: 0.56
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.71; acc: 0.41
Batch: 140; loss: 1.34; acc: 0.7
Val Epoch over. val_loss: 1.5723275416975568; val_accuracy: 0.5284633757961783 

The current subspace-distance is: 1.6689289623172954e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.44
Batch: 20; loss: 1.59; acc: 0.45
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.49; acc: 0.59
Batch: 80; loss: 1.71; acc: 0.5
Batch: 100; loss: 1.48; acc: 0.56
Batch: 120; loss: 1.56; acc: 0.55
Batch: 140; loss: 1.49; acc: 0.62
Batch: 160; loss: 1.58; acc: 0.44
Batch: 180; loss: 1.62; acc: 0.53
Batch: 200; loss: 1.57; acc: 0.48
Batch: 220; loss: 1.71; acc: 0.48
Batch: 240; loss: 1.64; acc: 0.5
Batch: 260; loss: 1.64; acc: 0.55
Batch: 280; loss: 1.64; acc: 0.48
Batch: 300; loss: 1.63; acc: 0.53
Batch: 320; loss: 1.61; acc: 0.53
Batch: 340; loss: 1.64; acc: 0.47
Batch: 360; loss: 1.49; acc: 0.56
Batch: 380; loss: 1.59; acc: 0.52
Batch: 400; loss: 1.72; acc: 0.44
Batch: 420; loss: 1.75; acc: 0.42
Batch: 440; loss: 1.47; acc: 0.5
Batch: 460; loss: 1.66; acc: 0.5
Batch: 480; loss: 1.63; acc: 0.5
Batch: 500; loss: 1.69; acc: 0.38
Batch: 520; loss: 1.4; acc: 0.62
Batch: 540; loss: 1.66; acc: 0.45
Batch: 560; loss: 1.59; acc: 0.48
Batch: 580; loss: 1.7; acc: 0.45
Batch: 600; loss: 1.64; acc: 0.5
Batch: 620; loss: 1.69; acc: 0.44
Batch: 640; loss: 1.57; acc: 0.5
Batch: 660; loss: 1.65; acc: 0.47
Batch: 680; loss: 1.69; acc: 0.44
Batch: 700; loss: 1.6; acc: 0.53
Batch: 720; loss: 1.72; acc: 0.42
Batch: 740; loss: 1.61; acc: 0.52
Batch: 760; loss: 1.63; acc: 0.52
Batch: 780; loss: 1.45; acc: 0.58
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.1297058487543836e-05
1.4744763575436082e-05
Batch: 0; loss: 1.6; acc: 0.59
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.37; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.37; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.32; acc: 0.72
Val Epoch over. val_loss: 1.5550564740114152; val_accuracy: 0.5370222929936306 

The current subspace-distance is: 1.4744763575436082e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.73; acc: 0.36
Batch: 20; loss: 1.53; acc: 0.59
Batch: 40; loss: 1.59; acc: 0.38
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.49; acc: 0.62
Batch: 100; loss: 1.65; acc: 0.44
Batch: 120; loss: 1.48; acc: 0.62
Batch: 140; loss: 1.51; acc: 0.55
Batch: 160; loss: 1.54; acc: 0.55
Batch: 180; loss: 1.72; acc: 0.48
Batch: 200; loss: 1.73; acc: 0.5
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.57; acc: 0.48
Batch: 260; loss: 1.68; acc: 0.41
Batch: 280; loss: 1.48; acc: 0.59
Batch: 300; loss: 1.7; acc: 0.42
Batch: 320; loss: 1.73; acc: 0.42
Batch: 340; loss: 1.58; acc: 0.53
Batch: 360; loss: 1.64; acc: 0.48
Batch: 380; loss: 1.54; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.55
Batch: 420; loss: 1.55; acc: 0.52
Batch: 440; loss: 1.6; acc: 0.53
Batch: 460; loss: 1.61; acc: 0.56
Batch: 480; loss: 1.58; acc: 0.5
Batch: 500; loss: 1.61; acc: 0.53
Batch: 520; loss: 1.46; acc: 0.59
Batch: 540; loss: 1.69; acc: 0.42
Batch: 560; loss: 1.67; acc: 0.47
Batch: 580; loss: 1.47; acc: 0.56
Batch: 600; loss: 1.5; acc: 0.55
Batch: 620; loss: 1.8; acc: 0.38
Batch: 640; loss: 1.74; acc: 0.47
Batch: 660; loss: 1.53; acc: 0.48
Batch: 680; loss: 1.62; acc: 0.45
Batch: 700; loss: 1.79; acc: 0.42
Batch: 720; loss: 1.71; acc: 0.48
Batch: 740; loss: 1.64; acc: 0.55
Batch: 760; loss: 1.56; acc: 0.56
Batch: 780; loss: 1.62; acc: 0.52
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.1825642256299034e-05
1.4600815120502375e-05
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.38; acc: 0.64
Batch: 60; loss: 1.47; acc: 0.53
Batch: 80; loss: 1.39; acc: 0.53
Batch: 100; loss: 1.61; acc: 0.53
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.33; acc: 0.67
Val Epoch over. val_loss: 1.5644125847300148; val_accuracy: 0.5296576433121019 

The current subspace-distance is: 1.4600815120502375e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.48
Batch: 20; loss: 1.59; acc: 0.45
Batch: 40; loss: 1.57; acc: 0.55
Batch: 60; loss: 1.5; acc: 0.67
Batch: 80; loss: 1.31; acc: 0.69
Batch: 100; loss: 1.55; acc: 0.5
Batch: 120; loss: 1.59; acc: 0.45
Batch: 140; loss: 1.68; acc: 0.5
Batch: 160; loss: 1.59; acc: 0.52
Batch: 180; loss: 1.74; acc: 0.42
Batch: 200; loss: 1.76; acc: 0.41
Batch: 220; loss: 1.52; acc: 0.52
Batch: 240; loss: 1.56; acc: 0.5
Batch: 260; loss: 1.85; acc: 0.42
Batch: 280; loss: 1.54; acc: 0.48
Batch: 300; loss: 1.62; acc: 0.5
Batch: 320; loss: 1.54; acc: 0.5
Batch: 340; loss: 1.67; acc: 0.45
Batch: 360; loss: 1.56; acc: 0.47
Batch: 380; loss: 1.71; acc: 0.42
Batch: 400; loss: 1.49; acc: 0.58
Batch: 420; loss: 1.67; acc: 0.47
Batch: 440; loss: 1.66; acc: 0.52
Batch: 460; loss: 1.51; acc: 0.59
Batch: 480; loss: 1.69; acc: 0.45
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.62; acc: 0.58
Batch: 540; loss: 1.72; acc: 0.45
Batch: 560; loss: 1.63; acc: 0.47
Batch: 580; loss: 1.68; acc: 0.41
Batch: 600; loss: 1.56; acc: 0.55
Batch: 620; loss: 1.81; acc: 0.39
Batch: 640; loss: 1.64; acc: 0.52
Batch: 660; loss: 1.56; acc: 0.62
Batch: 680; loss: 1.62; acc: 0.44
Batch: 700; loss: 1.6; acc: 0.47
Batch: 720; loss: 1.47; acc: 0.53
Batch: 740; loss: 1.68; acc: 0.5
Batch: 760; loss: 1.52; acc: 0.53
Batch: 780; loss: 1.48; acc: 0.61
Train Epoch over. train_loss: 1.6; train_accuracy: 0.5 

4.2316292820032686e-05
1.5656696632504463e-05
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.38; acc: 0.64
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.53
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.33; acc: 0.72
Val Epoch over. val_loss: 1.5603680208230475; val_accuracy: 0.5367237261146497 

The current subspace-distance is: 1.5656696632504463e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.64; acc: 0.55
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.76; acc: 0.36
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.56; acc: 0.52
Batch: 120; loss: 1.64; acc: 0.45
Batch: 140; loss: 1.54; acc: 0.56
Batch: 160; loss: 1.6; acc: 0.48
Batch: 180; loss: 1.6; acc: 0.55
Batch: 200; loss: 1.63; acc: 0.52
Batch: 220; loss: 1.43; acc: 0.67
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.52
Batch: 280; loss: 1.53; acc: 0.52
Batch: 300; loss: 1.58; acc: 0.5
Batch: 320; loss: 1.57; acc: 0.52
Batch: 340; loss: 1.56; acc: 0.48
Batch: 360; loss: 1.66; acc: 0.41
Batch: 380; loss: 1.52; acc: 0.58
Batch: 400; loss: 1.6; acc: 0.45
Batch: 420; loss: 1.66; acc: 0.42
Batch: 440; loss: 1.67; acc: 0.48
Batch: 460; loss: 1.54; acc: 0.55
Batch: 480; loss: 1.69; acc: 0.47
Batch: 500; loss: 1.7; acc: 0.52
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.64; acc: 0.5
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.46; acc: 0.64
Batch: 600; loss: 1.55; acc: 0.55
Batch: 620; loss: 1.55; acc: 0.48
Batch: 640; loss: 1.69; acc: 0.53
Batch: 660; loss: 1.56; acc: 0.55
Batch: 680; loss: 1.56; acc: 0.55
Batch: 700; loss: 1.6; acc: 0.52
Batch: 720; loss: 1.63; acc: 0.52
Batch: 740; loss: 1.72; acc: 0.44
Batch: 760; loss: 1.47; acc: 0.55
Batch: 780; loss: 1.59; acc: 0.48
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.1672628867672756e-05
1.664284718572162e-05
Batch: 0; loss: 1.59; acc: 0.55
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.55
Batch: 100; loss: 1.6; acc: 0.52
Batch: 120; loss: 1.68; acc: 0.41
Batch: 140; loss: 1.31; acc: 0.69
Val Epoch over. val_loss: 1.549176179679336; val_accuracy: 0.5391122611464968 

The current subspace-distance is: 1.664284718572162e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.68; acc: 0.47
Batch: 60; loss: 1.41; acc: 0.59
Batch: 80; loss: 1.5; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.68; acc: 0.41
Batch: 160; loss: 1.56; acc: 0.53
Batch: 180; loss: 1.54; acc: 0.55
Batch: 200; loss: 1.62; acc: 0.47
Batch: 220; loss: 1.61; acc: 0.56
Batch: 240; loss: 1.62; acc: 0.5
Batch: 260; loss: 1.64; acc: 0.5
Batch: 280; loss: 1.6; acc: 0.48
Batch: 300; loss: 1.63; acc: 0.5
Batch: 320; loss: 1.64; acc: 0.56
Batch: 340; loss: 1.68; acc: 0.52
Batch: 360; loss: 1.4; acc: 0.62
Batch: 380; loss: 1.51; acc: 0.59
Batch: 400; loss: 1.66; acc: 0.45
Batch: 420; loss: 1.51; acc: 0.59
Batch: 440; loss: 1.71; acc: 0.48
Batch: 460; loss: 1.49; acc: 0.52
Batch: 480; loss: 1.6; acc: 0.52
Batch: 500; loss: 1.52; acc: 0.56
Batch: 520; loss: 1.49; acc: 0.58
Batch: 540; loss: 1.65; acc: 0.45
Batch: 560; loss: 1.66; acc: 0.47
Batch: 580; loss: 1.5; acc: 0.59
Batch: 600; loss: 1.83; acc: 0.33
Batch: 620; loss: 1.63; acc: 0.55
Batch: 640; loss: 1.54; acc: 0.56
Batch: 660; loss: 1.57; acc: 0.53
Batch: 680; loss: 1.74; acc: 0.45
Batch: 700; loss: 1.59; acc: 0.59
Batch: 720; loss: 1.64; acc: 0.5
Batch: 740; loss: 1.6; acc: 0.55
Batch: 760; loss: 1.55; acc: 0.53
Batch: 780; loss: 1.56; acc: 0.52
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.3430183723103255e-05
1.7152206055470742e-05
Batch: 0; loss: 1.58; acc: 0.55
Batch: 20; loss: 1.88; acc: 0.39
Batch: 40; loss: 1.36; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.52
Batch: 120; loss: 1.68; acc: 0.41
Batch: 140; loss: 1.31; acc: 0.69
Val Epoch over. val_loss: 1.5478148908372138; val_accuracy: 0.5386146496815286 

The current subspace-distance is: 1.7152206055470742e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.76; acc: 0.39
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.47
Batch: 100; loss: 1.49; acc: 0.56
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.51; acc: 0.5
Batch: 160; loss: 1.65; acc: 0.55
Batch: 180; loss: 1.6; acc: 0.48
Batch: 200; loss: 1.61; acc: 0.56
Batch: 220; loss: 1.7; acc: 0.41
Batch: 240; loss: 1.56; acc: 0.59
Batch: 260; loss: 1.61; acc: 0.47
Batch: 280; loss: 1.6; acc: 0.5
Batch: 300; loss: 1.51; acc: 0.44
Batch: 320; loss: 1.63; acc: 0.5
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.66; acc: 0.48
Batch: 380; loss: 1.53; acc: 0.53
Batch: 400; loss: 1.61; acc: 0.48
Batch: 420; loss: 1.64; acc: 0.47
Batch: 440; loss: 1.67; acc: 0.39
Batch: 460; loss: 1.7; acc: 0.48
Batch: 480; loss: 1.58; acc: 0.52
Batch: 500; loss: 1.61; acc: 0.53
Batch: 520; loss: 1.72; acc: 0.47
Batch: 540; loss: 1.67; acc: 0.42
Batch: 560; loss: 1.65; acc: 0.39
Batch: 580; loss: 1.54; acc: 0.52
Batch: 600; loss: 1.55; acc: 0.55
Batch: 620; loss: 1.52; acc: 0.52
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.61; acc: 0.52
Batch: 680; loss: 1.54; acc: 0.52
Batch: 700; loss: 1.49; acc: 0.53
Batch: 720; loss: 1.55; acc: 0.52
Batch: 740; loss: 1.45; acc: 0.61
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.68; acc: 0.48
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.17020964960102e-05
1.369498477288289e-05
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.87; acc: 0.36
Batch: 40; loss: 1.37; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.53
Batch: 80; loss: 1.38; acc: 0.58
Batch: 100; loss: 1.6; acc: 0.47
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.32; acc: 0.69
Val Epoch over. val_loss: 1.5538520334632533; val_accuracy: 0.5361265923566879 

The current subspace-distance is: 1.369498477288289e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.55
Batch: 20; loss: 1.47; acc: 0.61
Batch: 40; loss: 1.78; acc: 0.41
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.69; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.39
Batch: 120; loss: 1.46; acc: 0.64
Batch: 140; loss: 1.6; acc: 0.47
Batch: 160; loss: 1.55; acc: 0.52
Batch: 180; loss: 1.5; acc: 0.53
Batch: 200; loss: 1.81; acc: 0.38
Batch: 220; loss: 1.56; acc: 0.56
Batch: 240; loss: 1.59; acc: 0.53
Batch: 260; loss: 1.51; acc: 0.55
Batch: 280; loss: 1.69; acc: 0.36
Batch: 300; loss: 1.59; acc: 0.52
Batch: 320; loss: 1.63; acc: 0.53
Batch: 340; loss: 1.54; acc: 0.53
Batch: 360; loss: 1.82; acc: 0.44
Batch: 380; loss: 1.6; acc: 0.55
Batch: 400; loss: 1.66; acc: 0.44
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.56; acc: 0.52
Batch: 460; loss: 1.62; acc: 0.45
Batch: 480; loss: 1.67; acc: 0.42
Batch: 500; loss: 1.72; acc: 0.38
Batch: 520; loss: 1.66; acc: 0.47
Batch: 540; loss: 1.5; acc: 0.5
Batch: 560; loss: 1.53; acc: 0.56
Batch: 580; loss: 1.55; acc: 0.59
Batch: 600; loss: 1.56; acc: 0.52
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.62; acc: 0.48
Batch: 660; loss: 1.55; acc: 0.59
Batch: 680; loss: 1.68; acc: 0.55
Batch: 700; loss: 1.58; acc: 0.58
Batch: 720; loss: 1.46; acc: 0.55
Batch: 740; loss: 1.78; acc: 0.44
Batch: 760; loss: 1.73; acc: 0.42
Batch: 780; loss: 1.49; acc: 0.52
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.1827617678791285e-05
1.6754564057919197e-05
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.86; acc: 0.34
Batch: 40; loss: 1.37; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.31; acc: 0.7
Val Epoch over. val_loss: 1.556644714561997; val_accuracy: 0.5403065286624203 

The current subspace-distance is: 1.6754564057919197e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.72; acc: 0.47
Batch: 20; loss: 1.65; acc: 0.44
Batch: 40; loss: 1.66; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.66
Batch: 80; loss: 1.61; acc: 0.52
Batch: 100; loss: 1.44; acc: 0.58
Batch: 120; loss: 1.61; acc: 0.52
Batch: 140; loss: 1.45; acc: 0.59
Batch: 160; loss: 1.56; acc: 0.58
Batch: 180; loss: 1.58; acc: 0.52
Batch: 200; loss: 1.46; acc: 0.59
Batch: 220; loss: 1.65; acc: 0.48
Batch: 240; loss: 1.48; acc: 0.56
Batch: 260; loss: 1.54; acc: 0.58
Batch: 280; loss: 1.49; acc: 0.55
Batch: 300; loss: 1.53; acc: 0.47
Batch: 320; loss: 1.68; acc: 0.42
Batch: 340; loss: 1.56; acc: 0.52
Batch: 360; loss: 1.6; acc: 0.55
Batch: 380; loss: 1.62; acc: 0.41
Batch: 400; loss: 1.58; acc: 0.52
Batch: 420; loss: 1.63; acc: 0.45
Batch: 440; loss: 1.64; acc: 0.47
Batch: 460; loss: 1.65; acc: 0.44
Batch: 480; loss: 1.69; acc: 0.41
Batch: 500; loss: 1.63; acc: 0.59
Batch: 520; loss: 1.59; acc: 0.58
Batch: 540; loss: 1.63; acc: 0.5
Batch: 560; loss: 1.65; acc: 0.55
Batch: 580; loss: 1.66; acc: 0.58
Batch: 600; loss: 1.54; acc: 0.58
Batch: 620; loss: 1.68; acc: 0.45
Batch: 640; loss: 1.57; acc: 0.53
Batch: 660; loss: 1.6; acc: 0.52
Batch: 680; loss: 1.58; acc: 0.53
Batch: 700; loss: 1.65; acc: 0.47
Batch: 720; loss: 1.65; acc: 0.47
Batch: 740; loss: 1.59; acc: 0.48
Batch: 760; loss: 1.56; acc: 0.58
Batch: 780; loss: 1.72; acc: 0.42
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.161528340773657e-05
1.2021449947496876e-05
Batch: 0; loss: 1.58; acc: 0.59
Batch: 20; loss: 1.87; acc: 0.38
Batch: 40; loss: 1.37; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.53
Batch: 80; loss: 1.38; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.5
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.31; acc: 0.7
Val Epoch over. val_loss: 1.5503330488873135; val_accuracy: 0.539609872611465 

The current subspace-distance is: 1.2021449947496876e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.53
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.69; acc: 0.48
Batch: 60; loss: 1.59; acc: 0.47
Batch: 80; loss: 1.56; acc: 0.53
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.6; acc: 0.47
Batch: 140; loss: 1.67; acc: 0.42
Batch: 160; loss: 1.57; acc: 0.52
Batch: 180; loss: 1.56; acc: 0.5
Batch: 200; loss: 1.62; acc: 0.53
Batch: 220; loss: 1.59; acc: 0.56
Batch: 240; loss: 1.65; acc: 0.53
Batch: 260; loss: 1.59; acc: 0.48
Batch: 280; loss: 1.69; acc: 0.44
Batch: 300; loss: 1.63; acc: 0.48
Batch: 320; loss: 1.61; acc: 0.52
Batch: 340; loss: 1.48; acc: 0.59
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.62; acc: 0.5
Batch: 400; loss: 1.51; acc: 0.56
Batch: 420; loss: 1.47; acc: 0.53
Batch: 440; loss: 1.62; acc: 0.5
Batch: 460; loss: 1.7; acc: 0.52
Batch: 480; loss: 1.55; acc: 0.5
Batch: 500; loss: 1.57; acc: 0.58
Batch: 520; loss: 1.58; acc: 0.59
Batch: 540; loss: 1.54; acc: 0.56
Batch: 560; loss: 1.62; acc: 0.56
Batch: 580; loss: 1.59; acc: 0.52
Batch: 600; loss: 1.56; acc: 0.44
Batch: 620; loss: 1.53; acc: 0.56
Batch: 640; loss: 1.63; acc: 0.53
Batch: 660; loss: 1.52; acc: 0.52
Batch: 680; loss: 1.65; acc: 0.42
Batch: 700; loss: 1.71; acc: 0.5
Batch: 720; loss: 1.57; acc: 0.56
Batch: 740; loss: 1.55; acc: 0.55
Batch: 760; loss: 1.58; acc: 0.5
Batch: 780; loss: 1.58; acc: 0.55
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.216361412545666e-05
1.565675120218657e-05
Batch: 0; loss: 1.58; acc: 0.61
Batch: 20; loss: 1.88; acc: 0.34
Batch: 40; loss: 1.37; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.38; acc: 0.55
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.32; acc: 0.69
Val Epoch over. val_loss: 1.5519942462823952; val_accuracy: 0.5363256369426752 

The current subspace-distance is: 1.565675120218657e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.56
Batch: 20; loss: 1.57; acc: 0.52
Batch: 40; loss: 1.58; acc: 0.55
Batch: 60; loss: 1.49; acc: 0.53
Batch: 80; loss: 1.68; acc: 0.47
Batch: 100; loss: 1.53; acc: 0.55
Batch: 120; loss: 1.57; acc: 0.48
Batch: 140; loss: 1.54; acc: 0.52
Batch: 160; loss: 1.65; acc: 0.48
Batch: 180; loss: 1.53; acc: 0.56
Batch: 200; loss: 1.51; acc: 0.56
Batch: 220; loss: 1.65; acc: 0.47
Batch: 240; loss: 1.69; acc: 0.48
Batch: 260; loss: 1.73; acc: 0.48
Batch: 280; loss: 1.56; acc: 0.52
Batch: 300; loss: 1.72; acc: 0.44
Batch: 320; loss: 1.67; acc: 0.5
Batch: 340; loss: 1.63; acc: 0.5
Batch: 360; loss: 1.56; acc: 0.58
Batch: 380; loss: 1.5; acc: 0.55
Batch: 400; loss: 1.48; acc: 0.59
Batch: 420; loss: 1.57; acc: 0.53
Batch: 440; loss: 1.57; acc: 0.5
Batch: 460; loss: 1.73; acc: 0.45
Batch: 480; loss: 1.55; acc: 0.58
Batch: 500; loss: 1.75; acc: 0.38
Batch: 520; loss: 1.51; acc: 0.58
Batch: 540; loss: 1.66; acc: 0.48
Batch: 560; loss: 1.63; acc: 0.39
Batch: 580; loss: 1.64; acc: 0.42
Batch: 600; loss: 1.46; acc: 0.56
Batch: 620; loss: 1.8; acc: 0.44
Batch: 640; loss: 1.63; acc: 0.47
Batch: 660; loss: 1.7; acc: 0.45
Batch: 680; loss: 1.54; acc: 0.53
Batch: 700; loss: 1.72; acc: 0.38
Batch: 720; loss: 1.57; acc: 0.55
Batch: 740; loss: 1.69; acc: 0.48
Batch: 760; loss: 1.59; acc: 0.55
Batch: 780; loss: 1.61; acc: 0.53
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.181063923169859e-05
1.5149248611123767e-05
Batch: 0; loss: 1.59; acc: 0.58
Batch: 20; loss: 1.89; acc: 0.36
Batch: 40; loss: 1.36; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.52
Batch: 80; loss: 1.37; acc: 0.55
Batch: 100; loss: 1.6; acc: 0.5
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.32; acc: 0.67
Val Epoch over. val_loss: 1.556862693683357; val_accuracy: 0.5314490445859873 

The current subspace-distance is: 1.5149248611123767e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.58
Batch: 20; loss: 1.5; acc: 0.61
Batch: 40; loss: 1.62; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.56
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.56; acc: 0.61
Batch: 120; loss: 1.53; acc: 0.52
Batch: 140; loss: 1.44; acc: 0.64
Batch: 160; loss: 1.6; acc: 0.59
Batch: 180; loss: 1.55; acc: 0.55
Batch: 200; loss: 1.51; acc: 0.58
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.65; acc: 0.47
Batch: 260; loss: 1.69; acc: 0.52
Batch: 280; loss: 1.63; acc: 0.45
Batch: 300; loss: 1.59; acc: 0.5
Batch: 320; loss: 1.67; acc: 0.48
Batch: 340; loss: 1.65; acc: 0.44
Batch: 360; loss: 1.48; acc: 0.56
Batch: 380; loss: 1.67; acc: 0.47
Batch: 400; loss: 1.57; acc: 0.52
Batch: 420; loss: 1.64; acc: 0.45
Batch: 440; loss: 1.69; acc: 0.45
Batch: 460; loss: 1.64; acc: 0.45
Batch: 480; loss: 1.59; acc: 0.5
Batch: 500; loss: 1.67; acc: 0.48
Batch: 520; loss: 1.59; acc: 0.59
Batch: 540; loss: 1.39; acc: 0.59
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.58; acc: 0.53
Batch: 620; loss: 1.56; acc: 0.58
Batch: 640; loss: 1.72; acc: 0.39
Batch: 660; loss: 1.51; acc: 0.56
Batch: 680; loss: 1.5; acc: 0.52
Batch: 700; loss: 1.54; acc: 0.56
Batch: 720; loss: 1.64; acc: 0.53
Batch: 740; loss: 1.69; acc: 0.5
Batch: 760; loss: 1.48; acc: 0.53
Batch: 780; loss: 1.69; acc: 0.5
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.253070437698625e-05
1.6356816558982246e-05
Batch: 0; loss: 1.58; acc: 0.61
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.35; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.68; acc: 0.41
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.5457289355575659; val_accuracy: 0.5440883757961783 

The current subspace-distance is: 1.6356816558982246e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_12_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.7743928539080627

The number of parameters is: 247624

The number of individual parameters is:

15
270
15
15
22
36960
22
22
43
105952
43
43
64
99072
64
64
4096
64
640
10
64
64

nonzero elements in E: 24762397
elements in E: 24762400
fraction nonzero: 0.9999998788485769
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.46; acc: 0.11
Batch: 20; loss: 2.29; acc: 0.08
Batch: 40; loss: 2.18; acc: 0.16
Batch: 60; loss: 2.16; acc: 0.16
Batch: 80; loss: 2.14; acc: 0.19
Batch: 100; loss: 2.04; acc: 0.19
Batch: 120; loss: 2.03; acc: 0.23
Batch: 140; loss: 2.06; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.28
Batch: 180; loss: 2.02; acc: 0.3
Batch: 200; loss: 1.95; acc: 0.36
Batch: 220; loss: 1.94; acc: 0.39
Batch: 240; loss: 2.0; acc: 0.36
Batch: 260; loss: 1.97; acc: 0.25
Batch: 280; loss: 1.83; acc: 0.5
Batch: 300; loss: 1.86; acc: 0.41
Batch: 320; loss: 1.91; acc: 0.47
Batch: 340; loss: 1.84; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.45
Batch: 380; loss: 1.77; acc: 0.5
Batch: 400; loss: 1.87; acc: 0.38
Batch: 420; loss: 1.89; acc: 0.44
Batch: 440; loss: 1.81; acc: 0.55
Batch: 460; loss: 1.83; acc: 0.44
Batch: 480; loss: 1.73; acc: 0.53
Batch: 500; loss: 1.92; acc: 0.38
Batch: 520; loss: 1.84; acc: 0.41
Batch: 540; loss: 1.83; acc: 0.44
Batch: 560; loss: 1.75; acc: 0.58
Batch: 580; loss: 1.72; acc: 0.55
Batch: 600; loss: 1.83; acc: 0.42
Batch: 620; loss: 1.85; acc: 0.41
Batch: 640; loss: 1.71; acc: 0.52
Batch: 660; loss: 1.82; acc: 0.45
Batch: 680; loss: 1.87; acc: 0.45
Batch: 700; loss: 1.79; acc: 0.53
Batch: 720; loss: 1.79; acc: 0.39
Batch: 740; loss: 1.8; acc: 0.5
Batch: 760; loss: 1.69; acc: 0.55
Batch: 780; loss: 1.7; acc: 0.58
Train Epoch over. train_loss: 1.9; train_accuracy: 0.4 

4.426358646014705e-05
3.84516388294287e-05
Batch: 0; loss: 1.79; acc: 0.47
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.55; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.75
Batch: 80; loss: 1.66; acc: 0.55
Batch: 100; loss: 1.7; acc: 0.61
Batch: 120; loss: 1.91; acc: 0.42
Batch: 140; loss: 1.66; acc: 0.62
Val Epoch over. val_loss: 1.7027238106272022; val_accuracy: 0.5580214968152867 

The current subspace-distance is: 3.84516388294287e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.52
Batch: 20; loss: 1.78; acc: 0.48
Batch: 40; loss: 1.81; acc: 0.47
Batch: 60; loss: 1.68; acc: 0.53
Batch: 80; loss: 1.72; acc: 0.55
Batch: 100; loss: 1.64; acc: 0.61
Batch: 120; loss: 1.72; acc: 0.56
Batch: 140; loss: 1.67; acc: 0.62
Batch: 160; loss: 1.64; acc: 0.56
Batch: 180; loss: 1.56; acc: 0.59
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.74; acc: 0.5
Batch: 240; loss: 1.75; acc: 0.52
Batch: 260; loss: 1.74; acc: 0.55
Batch: 280; loss: 1.72; acc: 0.5
Batch: 300; loss: 1.81; acc: 0.5
Batch: 320; loss: 1.68; acc: 0.53
Batch: 340; loss: 1.72; acc: 0.47
Batch: 360; loss: 1.86; acc: 0.53
Batch: 380; loss: 1.73; acc: 0.47
Batch: 400; loss: 1.63; acc: 0.58
Batch: 420; loss: 1.76; acc: 0.52
Batch: 440; loss: 1.55; acc: 0.66
Batch: 460; loss: 1.68; acc: 0.52
Batch: 480; loss: 1.67; acc: 0.58
Batch: 500; loss: 1.67; acc: 0.53
Batch: 520; loss: 1.6; acc: 0.62
Batch: 540; loss: 1.6; acc: 0.59
Batch: 560; loss: 1.68; acc: 0.55
Batch: 580; loss: 1.73; acc: 0.56
Batch: 600; loss: 1.71; acc: 0.52
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.7; acc: 0.52
Batch: 660; loss: 1.62; acc: 0.56
Batch: 680; loss: 1.56; acc: 0.64
Batch: 700; loss: 1.64; acc: 0.58
Batch: 720; loss: 1.52; acc: 0.59
Batch: 740; loss: 1.56; acc: 0.66
Batch: 760; loss: 1.62; acc: 0.61
Batch: 780; loss: 1.6; acc: 0.53
Train Epoch over. train_loss: 1.66; train_accuracy: 0.56 

5.8405432355357334e-05
5.310013148118742e-05
Batch: 0; loss: 1.7; acc: 0.5
Batch: 20; loss: 1.67; acc: 0.55
Batch: 40; loss: 1.36; acc: 0.67
Batch: 60; loss: 1.47; acc: 0.7
Batch: 80; loss: 1.45; acc: 0.64
Batch: 100; loss: 1.56; acc: 0.64
Batch: 120; loss: 1.72; acc: 0.56
Batch: 140; loss: 1.5; acc: 0.75
Val Epoch over. val_loss: 1.5550832566182324; val_accuracy: 0.6198248407643312 

The current subspace-distance is: 5.310013148118742e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.58
Batch: 20; loss: 1.53; acc: 0.64
Batch: 40; loss: 1.52; acc: 0.55
Batch: 60; loss: 1.5; acc: 0.64
Batch: 80; loss: 1.72; acc: 0.53
Batch: 100; loss: 1.53; acc: 0.64
Batch: 120; loss: 1.64; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.64
Batch: 160; loss: 1.55; acc: 0.62
Batch: 180; loss: 1.6; acc: 0.52
Batch: 200; loss: 1.51; acc: 0.67
Batch: 220; loss: 1.61; acc: 0.56
Batch: 240; loss: 1.49; acc: 0.61
Batch: 260; loss: 1.64; acc: 0.52
Batch: 280; loss: 1.59; acc: 0.52
Batch: 300; loss: 1.6; acc: 0.59
Batch: 320; loss: 1.5; acc: 0.67
Batch: 340; loss: 1.51; acc: 0.69
Batch: 360; loss: 1.55; acc: 0.64
Batch: 380; loss: 1.57; acc: 0.62
Batch: 400; loss: 1.46; acc: 0.66
Batch: 420; loss: 1.53; acc: 0.62
Batch: 440; loss: 1.64; acc: 0.48
Batch: 460; loss: 1.56; acc: 0.62
Batch: 480; loss: 1.62; acc: 0.62
Batch: 500; loss: 1.51; acc: 0.64
Batch: 520; loss: 1.47; acc: 0.66
Batch: 540; loss: 1.6; acc: 0.62
Batch: 560; loss: 1.48; acc: 0.66
Batch: 580; loss: 1.51; acc: 0.62
Batch: 600; loss: 1.51; acc: 0.59
Batch: 620; loss: 1.48; acc: 0.58
Batch: 640; loss: 1.64; acc: 0.64
Batch: 660; loss: 1.48; acc: 0.62
Batch: 680; loss: 1.38; acc: 0.72
Batch: 700; loss: 1.5; acc: 0.56
Batch: 720; loss: 1.41; acc: 0.7
Batch: 740; loss: 1.48; acc: 0.62
Batch: 760; loss: 1.5; acc: 0.62
Batch: 780; loss: 1.52; acc: 0.7
Train Epoch over. train_loss: 1.54; train_accuracy: 0.62 

7.20798343536444e-05
6.527986988658085e-05
Batch: 0; loss: 1.62; acc: 0.62
Batch: 20; loss: 1.52; acc: 0.62
Batch: 40; loss: 1.27; acc: 0.73
Batch: 60; loss: 1.39; acc: 0.7
Batch: 80; loss: 1.39; acc: 0.67
Batch: 100; loss: 1.43; acc: 0.69
Batch: 120; loss: 1.61; acc: 0.56
Batch: 140; loss: 1.38; acc: 0.78
Val Epoch over. val_loss: 1.46439587006903; val_accuracy: 0.6668988853503185 

The current subspace-distance is: 6.527986988658085e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.7
Batch: 20; loss: 1.46; acc: 0.61
Batch: 40; loss: 1.38; acc: 0.67
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.61; acc: 0.62
Batch: 120; loss: 1.48; acc: 0.59
Batch: 140; loss: 1.61; acc: 0.58
Batch: 160; loss: 1.58; acc: 0.61
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.45; acc: 0.64
Batch: 220; loss: 1.43; acc: 0.7
Batch: 240; loss: 1.49; acc: 0.61
Batch: 260; loss: 1.46; acc: 0.59
Batch: 280; loss: 1.45; acc: 0.62
Batch: 300; loss: 1.33; acc: 0.75
Batch: 320; loss: 1.35; acc: 0.69
Batch: 340; loss: 1.48; acc: 0.64
Batch: 360; loss: 1.44; acc: 0.64
Batch: 380; loss: 1.47; acc: 0.66
Batch: 400; loss: 1.38; acc: 0.7
Batch: 420; loss: 1.48; acc: 0.64
Batch: 440; loss: 1.43; acc: 0.67
Batch: 460; loss: 1.38; acc: 0.66
Batch: 480; loss: 1.46; acc: 0.7
Batch: 500; loss: 1.5; acc: 0.61
Batch: 520; loss: 1.44; acc: 0.69
Batch: 540; loss: 1.42; acc: 0.69
Batch: 560; loss: 1.42; acc: 0.67
Batch: 580; loss: 1.48; acc: 0.62
Batch: 600; loss: 1.47; acc: 0.66
Batch: 620; loss: 1.51; acc: 0.64
Batch: 640; loss: 1.41; acc: 0.69
Batch: 660; loss: 1.4; acc: 0.69
Batch: 680; loss: 1.41; acc: 0.67
Batch: 700; loss: 1.52; acc: 0.56
Batch: 720; loss: 1.51; acc: 0.55
Batch: 740; loss: 1.52; acc: 0.52
Batch: 760; loss: 1.44; acc: 0.64
Batch: 780; loss: 1.31; acc: 0.73
Train Epoch over. train_loss: 1.46; train_accuracy: 0.64 

8.081006672000512e-05
7.346143684117123e-05
Batch: 0; loss: 1.52; acc: 0.56
Batch: 20; loss: 1.42; acc: 0.66
Batch: 40; loss: 1.22; acc: 0.78
Batch: 60; loss: 1.29; acc: 0.72
Batch: 80; loss: 1.33; acc: 0.67
Batch: 100; loss: 1.33; acc: 0.77
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.34; acc: 0.75
Val Epoch over. val_loss: 1.393917048053377; val_accuracy: 0.6755573248407644 

The current subspace-distance is: 7.346143684117123e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.72
Batch: 20; loss: 1.4; acc: 0.66
Batch: 40; loss: 1.33; acc: 0.73
Batch: 60; loss: 1.44; acc: 0.67
Batch: 80; loss: 1.27; acc: 0.8
Batch: 100; loss: 1.5; acc: 0.67
Batch: 120; loss: 1.38; acc: 0.66
Batch: 140; loss: 1.52; acc: 0.59
Batch: 160; loss: 1.37; acc: 0.7
Batch: 180; loss: 1.53; acc: 0.62
Batch: 200; loss: 1.4; acc: 0.7
Batch: 220; loss: 1.47; acc: 0.67
Batch: 240; loss: 1.42; acc: 0.64
Batch: 260; loss: 1.47; acc: 0.64
Batch: 280; loss: 1.3; acc: 0.75
Batch: 300; loss: 1.37; acc: 0.7
Batch: 320; loss: 1.47; acc: 0.66
Batch: 340; loss: 1.42; acc: 0.66
Batch: 360; loss: 1.32; acc: 0.72
Batch: 380; loss: 1.5; acc: 0.61
Batch: 400; loss: 1.3; acc: 0.72
Batch: 420; loss: 1.48; acc: 0.59
Batch: 440; loss: 1.38; acc: 0.61
Batch: 460; loss: 1.32; acc: 0.77
Batch: 480; loss: 1.47; acc: 0.67
Batch: 500; loss: 1.32; acc: 0.72
Batch: 520; loss: 1.39; acc: 0.62
Batch: 540; loss: 1.51; acc: 0.66
Batch: 560; loss: 1.44; acc: 0.64
Batch: 580; loss: 1.37; acc: 0.66
Batch: 600; loss: 1.45; acc: 0.61
Batch: 620; loss: 1.52; acc: 0.59
Batch: 640; loss: 1.47; acc: 0.59
Batch: 660; loss: 1.55; acc: 0.58
Batch: 680; loss: 1.35; acc: 0.73
Batch: 700; loss: 1.46; acc: 0.59
Batch: 720; loss: 1.34; acc: 0.73
Batch: 740; loss: 1.45; acc: 0.59
Batch: 760; loss: 1.31; acc: 0.73
Batch: 780; loss: 1.39; acc: 0.66
Train Epoch over. train_loss: 1.41; train_accuracy: 0.65 

9.035746188601479e-05
8.445788262179121e-05
Batch: 0; loss: 1.44; acc: 0.58
Batch: 20; loss: 1.39; acc: 0.61
Batch: 40; loss: 1.22; acc: 0.81
Batch: 60; loss: 1.23; acc: 0.81
Batch: 80; loss: 1.24; acc: 0.73
Batch: 100; loss: 1.3; acc: 0.77
Batch: 120; loss: 1.49; acc: 0.69
Batch: 140; loss: 1.29; acc: 0.73
Val Epoch over. val_loss: 1.3407321591286143; val_accuracy: 0.6838176751592356 

The current subspace-distance is: 8.445788262179121e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.59
Batch: 20; loss: 1.43; acc: 0.62
Batch: 40; loss: 1.31; acc: 0.69
Batch: 60; loss: 1.36; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.67
Batch: 100; loss: 1.43; acc: 0.58
Batch: 120; loss: 1.48; acc: 0.61
Batch: 140; loss: 1.35; acc: 0.64
Batch: 160; loss: 1.48; acc: 0.61
Batch: 180; loss: 1.31; acc: 0.67
Batch: 200; loss: 1.32; acc: 0.64
Batch: 220; loss: 1.43; acc: 0.59
Batch: 240; loss: 1.42; acc: 0.58
Batch: 260; loss: 1.47; acc: 0.62
Batch: 280; loss: 1.45; acc: 0.62
Batch: 300; loss: 1.26; acc: 0.73
Batch: 320; loss: 1.41; acc: 0.61
Batch: 340; loss: 1.25; acc: 0.75
Batch: 360; loss: 1.31; acc: 0.7
Batch: 380; loss: 1.34; acc: 0.67
Batch: 400; loss: 1.38; acc: 0.58
Batch: 420; loss: 1.36; acc: 0.61
Batch: 440; loss: 1.37; acc: 0.61
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.52; acc: 0.56
Batch: 500; loss: 1.32; acc: 0.64
Batch: 520; loss: 1.43; acc: 0.64
Batch: 540; loss: 1.45; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.72
Batch: 580; loss: 1.32; acc: 0.69
Batch: 600; loss: 1.33; acc: 0.62
Batch: 620; loss: 1.29; acc: 0.7
Batch: 640; loss: 1.31; acc: 0.69
Batch: 660; loss: 1.21; acc: 0.72
Batch: 680; loss: 1.38; acc: 0.66
Batch: 700; loss: 1.27; acc: 0.69
Batch: 720; loss: 1.37; acc: 0.67
Batch: 740; loss: 1.28; acc: 0.72
Batch: 760; loss: 1.34; acc: 0.66
Batch: 780; loss: 1.31; acc: 0.64
Train Epoch over. train_loss: 1.34; train_accuracy: 0.67 

0.00010064428352052346
9.502565808361396e-05
Batch: 0; loss: 1.37; acc: 0.59
Batch: 20; loss: 1.31; acc: 0.72
Batch: 40; loss: 1.15; acc: 0.8
Batch: 60; loss: 1.17; acc: 0.77
Batch: 80; loss: 1.13; acc: 0.78
Batch: 100; loss: 1.28; acc: 0.73
Batch: 120; loss: 1.44; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.77
Val Epoch over. val_loss: 1.2603665127116404; val_accuracy: 0.7048168789808917 

The current subspace-distance is: 9.502565808361396e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.67
Batch: 20; loss: 1.45; acc: 0.61
Batch: 40; loss: 1.32; acc: 0.69
Batch: 60; loss: 1.26; acc: 0.67
Batch: 80; loss: 1.35; acc: 0.64
Batch: 100; loss: 1.31; acc: 0.67
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.32; acc: 0.72
Batch: 160; loss: 1.41; acc: 0.66
Batch: 180; loss: 1.24; acc: 0.72
Batch: 200; loss: 1.23; acc: 0.78
Batch: 220; loss: 1.4; acc: 0.64
Batch: 240; loss: 1.36; acc: 0.58
Batch: 260; loss: 1.31; acc: 0.64
Batch: 280; loss: 1.3; acc: 0.59
Batch: 300; loss: 1.27; acc: 0.67
Batch: 320; loss: 1.18; acc: 0.78
Batch: 340; loss: 1.22; acc: 0.7
Batch: 360; loss: 1.26; acc: 0.67
Batch: 380; loss: 1.19; acc: 0.77
Batch: 400; loss: 1.34; acc: 0.69
Batch: 420; loss: 1.14; acc: 0.77
Batch: 440; loss: 1.28; acc: 0.77
Batch: 460; loss: 1.23; acc: 0.66
Batch: 480; loss: 1.32; acc: 0.66
Batch: 500; loss: 1.27; acc: 0.69
Batch: 520; loss: 1.32; acc: 0.66
Batch: 540; loss: 1.18; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.77
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.35; acc: 0.62
Batch: 620; loss: 1.24; acc: 0.67
Batch: 640; loss: 1.2; acc: 0.72
Batch: 660; loss: 1.28; acc: 0.66
Batch: 680; loss: 1.29; acc: 0.7
Batch: 700; loss: 1.31; acc: 0.69
Batch: 720; loss: 1.29; acc: 0.7
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.33; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.68 

0.00011381447257008404
0.00010798336734296754
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.72
Batch: 40; loss: 1.08; acc: 0.81
Batch: 60; loss: 1.14; acc: 0.78
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.73
Batch: 120; loss: 1.4; acc: 0.66
Batch: 140; loss: 1.09; acc: 0.86
Val Epoch over. val_loss: 1.205830188313867; val_accuracy: 0.7153662420382165 

The current subspace-distance is: 0.00010798336734296754 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.72
Batch: 20; loss: 1.23; acc: 0.69
Batch: 40; loss: 1.34; acc: 0.62
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.23; acc: 0.67
Batch: 100; loss: 1.22; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 1.37; acc: 0.53
Batch: 160; loss: 1.12; acc: 0.72
Batch: 180; loss: 1.06; acc: 0.75
Batch: 200; loss: 1.14; acc: 0.72
Batch: 220; loss: 1.2; acc: 0.73
Batch: 240; loss: 1.29; acc: 0.69
Batch: 260; loss: 1.17; acc: 0.66
Batch: 280; loss: 1.18; acc: 0.75
Batch: 300; loss: 1.27; acc: 0.61
Batch: 320; loss: 1.2; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 1.13; acc: 0.69
Batch: 380; loss: 1.24; acc: 0.69
Batch: 400; loss: 1.12; acc: 0.77
Batch: 420; loss: 1.25; acc: 0.7
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.15; acc: 0.72
Batch: 480; loss: 1.4; acc: 0.62
Batch: 500; loss: 1.19; acc: 0.78
Batch: 520; loss: 1.12; acc: 0.7
Batch: 540; loss: 1.25; acc: 0.77
Batch: 560; loss: 1.23; acc: 0.67
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.18; acc: 0.67
Batch: 620; loss: 1.2; acc: 0.67
Batch: 640; loss: 1.14; acc: 0.75
Batch: 660; loss: 1.26; acc: 0.7
Batch: 680; loss: 1.11; acc: 0.72
Batch: 700; loss: 1.23; acc: 0.61
Batch: 720; loss: 1.07; acc: 0.81
Batch: 740; loss: 1.24; acc: 0.64
Batch: 760; loss: 1.19; acc: 0.64
Batch: 780; loss: 1.18; acc: 0.67
Train Epoch over. train_loss: 1.21; train_accuracy: 0.69 

0.00012407262693159282
0.0001177643789560534
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.16; acc: 0.7
Batch: 40; loss: 1.0; acc: 0.81
Batch: 60; loss: 1.09; acc: 0.78
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 1.18; acc: 0.75
Batch: 120; loss: 1.36; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.88
Val Epoch over. val_loss: 1.1365248955738771; val_accuracy: 0.7231289808917197 

The current subspace-distance is: 0.0001177643789560534 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.77
Batch: 20; loss: 1.14; acc: 0.73
Batch: 40; loss: 1.18; acc: 0.67
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.66
Batch: 100; loss: 1.17; acc: 0.59
Batch: 120; loss: 1.33; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.64
Batch: 160; loss: 1.28; acc: 0.69
Batch: 180; loss: 1.15; acc: 0.69
Batch: 200; loss: 1.05; acc: 0.72
Batch: 220; loss: 1.29; acc: 0.53
Batch: 240; loss: 1.17; acc: 0.66
Batch: 260; loss: 1.2; acc: 0.72
Batch: 280; loss: 1.18; acc: 0.69
Batch: 300; loss: 1.23; acc: 0.7
Batch: 320; loss: 1.05; acc: 0.72
Batch: 340; loss: 1.13; acc: 0.69
Batch: 360; loss: 1.12; acc: 0.75
Batch: 380; loss: 1.25; acc: 0.64
Batch: 400; loss: 1.14; acc: 0.75
Batch: 420; loss: 1.17; acc: 0.61
Batch: 440; loss: 1.06; acc: 0.78
Batch: 460; loss: 1.11; acc: 0.73
Batch: 480; loss: 1.18; acc: 0.7
Batch: 500; loss: 1.2; acc: 0.73
Batch: 520; loss: 1.13; acc: 0.75
Batch: 540; loss: 1.17; acc: 0.69
Batch: 560; loss: 1.24; acc: 0.69
Batch: 580; loss: 1.07; acc: 0.77
Batch: 600; loss: 1.12; acc: 0.69
Batch: 620; loss: 1.14; acc: 0.7
Batch: 640; loss: 1.21; acc: 0.62
Batch: 660; loss: 1.1; acc: 0.72
Batch: 680; loss: 1.03; acc: 0.8
Batch: 700; loss: 1.14; acc: 0.66
Batch: 720; loss: 1.12; acc: 0.72
Batch: 740; loss: 1.24; acc: 0.69
Batch: 760; loss: 1.2; acc: 0.67
Batch: 780; loss: 1.3; acc: 0.59
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.00013442889030557126
0.000127924868138507
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 0.94; acc: 0.81
Batch: 60; loss: 1.06; acc: 0.78
Batch: 80; loss: 0.94; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.34; acc: 0.64
Batch: 140; loss: 0.89; acc: 0.89
Val Epoch over. val_loss: 1.0926010635248415; val_accuracy: 0.7293988853503185 

The current subspace-distance is: 0.000127924868138507 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 1.06; acc: 0.73
Batch: 40; loss: 1.12; acc: 0.73
Batch: 60; loss: 1.27; acc: 0.69
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 1.11; acc: 0.75
Batch: 140; loss: 1.14; acc: 0.61
Batch: 160; loss: 1.04; acc: 0.77
Batch: 180; loss: 1.16; acc: 0.66
Batch: 200; loss: 1.18; acc: 0.66
Batch: 220; loss: 1.02; acc: 0.7
Batch: 240; loss: 1.19; acc: 0.66
Batch: 260; loss: 1.2; acc: 0.66
Batch: 280; loss: 1.02; acc: 0.75
Batch: 300; loss: 1.06; acc: 0.78
Batch: 320; loss: 1.22; acc: 0.69
Batch: 340; loss: 1.3; acc: 0.61
Batch: 360; loss: 1.07; acc: 0.77
Batch: 380; loss: 0.96; acc: 0.8
Batch: 400; loss: 1.21; acc: 0.69
Batch: 420; loss: 1.08; acc: 0.69
Batch: 440; loss: 0.94; acc: 0.77
Batch: 460; loss: 1.23; acc: 0.64
Batch: 480; loss: 1.14; acc: 0.64
Batch: 500; loss: 0.95; acc: 0.81
Batch: 520; loss: 1.13; acc: 0.7
Batch: 540; loss: 0.97; acc: 0.8
Batch: 560; loss: 1.12; acc: 0.66
Batch: 580; loss: 1.04; acc: 0.72
Batch: 600; loss: 1.17; acc: 0.7
Batch: 620; loss: 1.09; acc: 0.7
Batch: 640; loss: 1.1; acc: 0.66
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 0.96; acc: 0.77
Batch: 700; loss: 1.16; acc: 0.77
Batch: 720; loss: 1.19; acc: 0.66
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.05; acc: 0.7
Batch: 780; loss: 1.09; acc: 0.73
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.00014617199485655874
0.00013556619524024427
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 1.15; acc: 0.75
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 0.85; acc: 0.91
Val Epoch over. val_loss: 1.056011307011744; val_accuracy: 0.7345740445859873 

The current subspace-distance is: 0.00013556619524024427 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 1.13; acc: 0.69
Batch: 60; loss: 1.12; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 1.19; acc: 0.77
Batch: 120; loss: 1.09; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.73
Batch: 160; loss: 1.24; acc: 0.62
Batch: 180; loss: 1.02; acc: 0.73
Batch: 200; loss: 0.98; acc: 0.75
Batch: 220; loss: 0.92; acc: 0.81
Batch: 240; loss: 1.29; acc: 0.58
Batch: 260; loss: 0.97; acc: 0.77
Batch: 280; loss: 1.05; acc: 0.67
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 1.18; acc: 0.61
Batch: 340; loss: 1.05; acc: 0.72
Batch: 360; loss: 0.95; acc: 0.75
Batch: 380; loss: 1.12; acc: 0.72
Batch: 400; loss: 1.22; acc: 0.64
Batch: 420; loss: 1.04; acc: 0.73
Batch: 440; loss: 1.07; acc: 0.7
Batch: 460; loss: 1.11; acc: 0.72
Batch: 480; loss: 1.21; acc: 0.67
Batch: 500; loss: 1.16; acc: 0.72
Batch: 520; loss: 1.28; acc: 0.59
Batch: 540; loss: 1.09; acc: 0.7
Batch: 560; loss: 1.04; acc: 0.69
Batch: 580; loss: 1.12; acc: 0.72
Batch: 600; loss: 0.98; acc: 0.81
Batch: 620; loss: 1.16; acc: 0.64
Batch: 640; loss: 1.12; acc: 0.67
Batch: 660; loss: 1.15; acc: 0.69
Batch: 680; loss: 1.1; acc: 0.73
Batch: 700; loss: 1.06; acc: 0.72
Batch: 720; loss: 1.15; acc: 0.67
Batch: 740; loss: 1.1; acc: 0.64
Batch: 760; loss: 1.11; acc: 0.73
Batch: 780; loss: 1.08; acc: 0.75
Train Epoch over. train_loss: 1.1; train_accuracy: 0.7 

0.00014791700232308358
0.0001404372596880421
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 1.1; acc: 0.7
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.32; acc: 0.66
Batch: 140; loss: 0.84; acc: 0.91
Val Epoch over. val_loss: 1.0530245455966634; val_accuracy: 0.7356687898089171 

The current subspace-distance is: 0.0001404372596880421 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 1.15; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 1.09; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.92; acc: 0.8
Batch: 160; loss: 1.17; acc: 0.7
Batch: 180; loss: 0.98; acc: 0.77
Batch: 200; loss: 1.16; acc: 0.69
Batch: 220; loss: 1.1; acc: 0.73
Batch: 240; loss: 1.1; acc: 0.67
Batch: 260; loss: 1.09; acc: 0.69
Batch: 280; loss: 1.15; acc: 0.72
Batch: 300; loss: 1.17; acc: 0.67
Batch: 320; loss: 0.91; acc: 0.8
Batch: 340; loss: 1.08; acc: 0.62
Batch: 360; loss: 1.14; acc: 0.69
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.0; acc: 0.75
Batch: 420; loss: 0.99; acc: 0.75
Batch: 440; loss: 1.03; acc: 0.75
Batch: 460; loss: 1.18; acc: 0.7
Batch: 480; loss: 1.08; acc: 0.72
Batch: 500; loss: 1.19; acc: 0.67
Batch: 520; loss: 1.08; acc: 0.73
Batch: 540; loss: 1.11; acc: 0.69
Batch: 560; loss: 0.93; acc: 0.75
Batch: 580; loss: 1.15; acc: 0.62
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 0.92; acc: 0.75
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.02; acc: 0.78
Batch: 680; loss: 1.04; acc: 0.75
Batch: 700; loss: 1.01; acc: 0.75
Batch: 720; loss: 1.25; acc: 0.64
Batch: 740; loss: 1.0; acc: 0.77
Batch: 760; loss: 1.03; acc: 0.78
Batch: 780; loss: 1.37; acc: 0.58
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.00015099435404408723
0.00014197593554854393
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 1.02; acc: 0.73
Batch: 80; loss: 0.88; acc: 0.75
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.88
Val Epoch over. val_loss: 1.0354702704271692; val_accuracy: 0.7333797770700637 

The current subspace-distance is: 0.00014197593554854393 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 0.95; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 1.17; acc: 0.66
Batch: 160; loss: 1.04; acc: 0.7
Batch: 180; loss: 1.03; acc: 0.69
Batch: 200; loss: 1.04; acc: 0.73
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 1.23; acc: 0.58
Batch: 260; loss: 1.14; acc: 0.73
Batch: 280; loss: 1.09; acc: 0.7
Batch: 300; loss: 1.02; acc: 0.75
Batch: 320; loss: 1.08; acc: 0.73
Batch: 340; loss: 1.22; acc: 0.64
Batch: 360; loss: 1.04; acc: 0.69
Batch: 380; loss: 1.22; acc: 0.62
Batch: 400; loss: 1.07; acc: 0.67
Batch: 420; loss: 1.06; acc: 0.78
Batch: 440; loss: 0.99; acc: 0.75
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.11; acc: 0.72
Batch: 500; loss: 1.12; acc: 0.73
Batch: 520; loss: 1.18; acc: 0.69
Batch: 540; loss: 1.17; acc: 0.72
Batch: 560; loss: 0.9; acc: 0.86
Batch: 580; loss: 1.13; acc: 0.69
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 1.05; acc: 0.77
Batch: 640; loss: 1.01; acc: 0.72
Batch: 660; loss: 1.05; acc: 0.69
Batch: 680; loss: 1.15; acc: 0.67
Batch: 700; loss: 0.96; acc: 0.8
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 1.04; acc: 0.7
Batch: 760; loss: 1.03; acc: 0.7
Batch: 780; loss: 1.07; acc: 0.7
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.00015331093163695186
0.00014537294919136912
Batch: 0; loss: 1.0; acc: 0.7
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 1.04; acc: 0.77
Batch: 80; loss: 0.9; acc: 0.73
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 0.84; acc: 0.89
Val Epoch over. val_loss: 1.0440510405097039; val_accuracy: 0.7337778662420382 

The current subspace-distance is: 0.00014537294919136912 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.17; acc: 0.67
Batch: 40; loss: 1.08; acc: 0.72
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.78
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 1.13; acc: 0.64
Batch: 180; loss: 1.0; acc: 0.77
Batch: 200; loss: 1.08; acc: 0.67
Batch: 220; loss: 1.02; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.72
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 1.08; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.59
Batch: 320; loss: 1.03; acc: 0.69
Batch: 340; loss: 0.96; acc: 0.8
Batch: 360; loss: 1.01; acc: 0.75
Batch: 380; loss: 0.97; acc: 0.77
Batch: 400; loss: 1.09; acc: 0.64
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 1.14; acc: 0.69
Batch: 460; loss: 1.03; acc: 0.75
Batch: 480; loss: 1.03; acc: 0.77
Batch: 500; loss: 1.27; acc: 0.62
Batch: 520; loss: 0.99; acc: 0.78
Batch: 540; loss: 0.97; acc: 0.69
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 0.96; acc: 0.7
Batch: 600; loss: 1.11; acc: 0.72
Batch: 620; loss: 1.05; acc: 0.77
Batch: 640; loss: 1.19; acc: 0.66
Batch: 660; loss: 0.97; acc: 0.75
Batch: 680; loss: 1.08; acc: 0.77
Batch: 700; loss: 1.05; acc: 0.66
Batch: 720; loss: 1.15; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.75
Batch: 760; loss: 1.1; acc: 0.66
Batch: 780; loss: 1.0; acc: 0.75
Train Epoch over. train_loss: 1.08; train_accuracy: 0.71 

0.00015591082046739757
0.0001466824469389394
Batch: 0; loss: 0.99; acc: 0.73
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 0.84; acc: 0.8
Batch: 60; loss: 1.03; acc: 0.77
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 1.31; acc: 0.66
Batch: 140; loss: 0.82; acc: 0.86
Val Epoch over. val_loss: 1.0332205136110828; val_accuracy: 0.7327826433121019 

The current subspace-distance is: 0.0001466824469389394 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.62
Batch: 40; loss: 0.94; acc: 0.75
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 0.94; acc: 0.73
Batch: 140; loss: 1.02; acc: 0.72
Batch: 160; loss: 0.97; acc: 0.8
Batch: 180; loss: 0.99; acc: 0.78
Batch: 200; loss: 1.11; acc: 0.67
Batch: 220; loss: 1.12; acc: 0.73
Batch: 240; loss: 1.11; acc: 0.67
Batch: 260; loss: 1.05; acc: 0.7
Batch: 280; loss: 0.99; acc: 0.73
Batch: 300; loss: 1.06; acc: 0.7
Batch: 320; loss: 1.08; acc: 0.72
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.16; acc: 0.62
Batch: 400; loss: 1.06; acc: 0.75
Batch: 420; loss: 1.12; acc: 0.72
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 1.09; acc: 0.75
Batch: 480; loss: 1.14; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 1.14; acc: 0.66
Batch: 540; loss: 1.11; acc: 0.64
Batch: 560; loss: 1.31; acc: 0.62
Batch: 580; loss: 1.03; acc: 0.72
Batch: 600; loss: 0.99; acc: 0.75
Batch: 620; loss: 1.02; acc: 0.69
Batch: 640; loss: 1.04; acc: 0.77
Batch: 660; loss: 1.17; acc: 0.64
Batch: 680; loss: 1.15; acc: 0.58
Batch: 700; loss: 1.08; acc: 0.75
Batch: 720; loss: 0.95; acc: 0.8
Batch: 740; loss: 1.2; acc: 0.64
Batch: 760; loss: 0.98; acc: 0.78
Batch: 780; loss: 0.99; acc: 0.75
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.0001549940643599257
0.00014827257837168872
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 1.02; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 1.14; acc: 0.7
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 0.82; acc: 0.88
Val Epoch over. val_loss: 1.0242309782915056; val_accuracy: 0.7347730891719745 

The current subspace-distance is: 0.00014827257837168872 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 1.02; acc: 0.69
Batch: 60; loss: 1.09; acc: 0.69
Batch: 80; loss: 0.99; acc: 0.72
Batch: 100; loss: 1.11; acc: 0.66
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 1.05; acc: 0.73
Batch: 160; loss: 1.1; acc: 0.66
Batch: 180; loss: 1.08; acc: 0.64
Batch: 200; loss: 0.96; acc: 0.78
Batch: 220; loss: 1.07; acc: 0.7
Batch: 240; loss: 0.91; acc: 0.75
Batch: 260; loss: 1.06; acc: 0.7
Batch: 280; loss: 0.96; acc: 0.75
Batch: 300; loss: 1.2; acc: 0.61
Batch: 320; loss: 1.06; acc: 0.72
Batch: 340; loss: 1.03; acc: 0.72
Batch: 360; loss: 1.05; acc: 0.67
Batch: 380; loss: 1.06; acc: 0.7
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 1.03; acc: 0.73
Batch: 460; loss: 0.9; acc: 0.75
Batch: 480; loss: 1.06; acc: 0.7
Batch: 500; loss: 0.83; acc: 0.83
Batch: 520; loss: 1.08; acc: 0.72
Batch: 540; loss: 1.08; acc: 0.72
Batch: 560; loss: 0.92; acc: 0.73
Batch: 580; loss: 0.89; acc: 0.81
Batch: 600; loss: 1.23; acc: 0.64
Batch: 620; loss: 1.02; acc: 0.77
Batch: 640; loss: 1.04; acc: 0.75
Batch: 660; loss: 0.93; acc: 0.84
Batch: 680; loss: 1.0; acc: 0.77
Batch: 700; loss: 1.06; acc: 0.75
Batch: 720; loss: 1.08; acc: 0.75
Batch: 740; loss: 1.08; acc: 0.66
Batch: 760; loss: 0.98; acc: 0.73
Batch: 780; loss: 0.94; acc: 0.75
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00015763148257974535
0.00014747453678864986
Batch: 0; loss: 0.97; acc: 0.75
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.7
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 0.81; acc: 0.88
Val Epoch over. val_loss: 1.0202986446155864; val_accuracy: 0.7356687898089171 

The current subspace-distance is: 0.00014747453678864986 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.96; acc: 0.8
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.98; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.73
Batch: 100; loss: 0.91; acc: 0.8
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 1.08; acc: 0.73
Batch: 160; loss: 1.14; acc: 0.75
Batch: 180; loss: 1.05; acc: 0.75
Batch: 200; loss: 1.03; acc: 0.73
Batch: 220; loss: 0.97; acc: 0.75
Batch: 240; loss: 1.01; acc: 0.73
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 1.05; acc: 0.78
Batch: 300; loss: 1.04; acc: 0.72
Batch: 320; loss: 1.16; acc: 0.67
Batch: 340; loss: 1.09; acc: 0.69
Batch: 360; loss: 1.13; acc: 0.62
Batch: 380; loss: 0.99; acc: 0.77
Batch: 400; loss: 0.88; acc: 0.78
Batch: 420; loss: 1.17; acc: 0.69
Batch: 440; loss: 1.03; acc: 0.7
Batch: 460; loss: 1.25; acc: 0.66
Batch: 480; loss: 0.97; acc: 0.8
Batch: 500; loss: 1.0; acc: 0.72
Batch: 520; loss: 1.28; acc: 0.66
Batch: 540; loss: 0.97; acc: 0.75
Batch: 560; loss: 1.06; acc: 0.75
Batch: 580; loss: 0.91; acc: 0.83
Batch: 600; loss: 1.13; acc: 0.62
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.1; acc: 0.67
Batch: 660; loss: 0.85; acc: 0.8
Batch: 680; loss: 1.04; acc: 0.72
Batch: 700; loss: 1.13; acc: 0.69
Batch: 720; loss: 0.98; acc: 0.81
Batch: 740; loss: 1.01; acc: 0.67
Batch: 760; loss: 1.03; acc: 0.66
Batch: 780; loss: 1.15; acc: 0.69
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00015837643877603114
0.00015230366261675954
Batch: 0; loss: 0.96; acc: 0.8
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.81; acc: 0.88
Val Epoch over. val_loss: 1.0142669180396255; val_accuracy: 0.734375 

The current subspace-distance is: 0.00015230366261675954 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 1.02; acc: 0.78
Batch: 60; loss: 1.03; acc: 0.77
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 1.13; acc: 0.64
Batch: 160; loss: 0.98; acc: 0.7
Batch: 180; loss: 1.03; acc: 0.72
Batch: 200; loss: 1.05; acc: 0.78
Batch: 220; loss: 1.14; acc: 0.67
Batch: 240; loss: 1.05; acc: 0.7
Batch: 260; loss: 1.1; acc: 0.72
Batch: 280; loss: 1.11; acc: 0.66
Batch: 300; loss: 1.01; acc: 0.73
Batch: 320; loss: 1.19; acc: 0.69
Batch: 340; loss: 1.06; acc: 0.7
Batch: 360; loss: 1.0; acc: 0.7
Batch: 380; loss: 0.88; acc: 0.77
Batch: 400; loss: 1.04; acc: 0.75
Batch: 420; loss: 1.0; acc: 0.73
Batch: 440; loss: 1.06; acc: 0.73
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 0.98; acc: 0.75
Batch: 500; loss: 1.0; acc: 0.7
Batch: 520; loss: 0.99; acc: 0.77
Batch: 540; loss: 1.05; acc: 0.69
Batch: 560; loss: 1.05; acc: 0.67
Batch: 580; loss: 1.32; acc: 0.61
Batch: 600; loss: 1.03; acc: 0.73
Batch: 620; loss: 1.11; acc: 0.73
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.11; acc: 0.73
Batch: 680; loss: 1.07; acc: 0.69
Batch: 700; loss: 1.0; acc: 0.73
Batch: 720; loss: 1.01; acc: 0.73
Batch: 740; loss: 0.88; acc: 0.84
Batch: 760; loss: 1.09; acc: 0.69
Batch: 780; loss: 1.02; acc: 0.69
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.00016234061331488192
0.00015360534598585218
Batch: 0; loss: 0.94; acc: 0.8
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 0.8; acc: 0.81
Val Epoch over. val_loss: 0.9999474051651681; val_accuracy: 0.7393511146496815 

The current subspace-distance is: 0.00015360534598585218 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.04; acc: 0.69
Batch: 40; loss: 1.09; acc: 0.66
Batch: 60; loss: 1.1; acc: 0.73
Batch: 80; loss: 1.22; acc: 0.77
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 1.08; acc: 0.7
Batch: 160; loss: 0.99; acc: 0.77
Batch: 180; loss: 0.99; acc: 0.73
Batch: 200; loss: 0.93; acc: 0.8
Batch: 220; loss: 1.05; acc: 0.72
Batch: 240; loss: 1.11; acc: 0.67
Batch: 260; loss: 1.2; acc: 0.62
Batch: 280; loss: 0.9; acc: 0.77
Batch: 300; loss: 1.01; acc: 0.73
Batch: 320; loss: 0.86; acc: 0.83
Batch: 340; loss: 0.93; acc: 0.72
Batch: 360; loss: 1.11; acc: 0.62
Batch: 380; loss: 1.07; acc: 0.62
Batch: 400; loss: 1.06; acc: 0.72
Batch: 420; loss: 1.19; acc: 0.64
Batch: 440; loss: 1.03; acc: 0.7
Batch: 460; loss: 1.28; acc: 0.58
Batch: 480; loss: 1.15; acc: 0.64
Batch: 500; loss: 0.88; acc: 0.8
Batch: 520; loss: 0.98; acc: 0.75
Batch: 540; loss: 0.99; acc: 0.75
Batch: 560; loss: 0.97; acc: 0.75
Batch: 580; loss: 1.04; acc: 0.77
Batch: 600; loss: 0.98; acc: 0.77
Batch: 620; loss: 1.16; acc: 0.72
Batch: 640; loss: 1.08; acc: 0.72
Batch: 660; loss: 1.0; acc: 0.73
Batch: 680; loss: 1.13; acc: 0.62
Batch: 700; loss: 1.08; acc: 0.72
Batch: 720; loss: 1.01; acc: 0.69
Batch: 740; loss: 0.87; acc: 0.8
Batch: 760; loss: 1.08; acc: 0.66
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.0001673679071245715
0.00016003145719878376
Batch: 0; loss: 0.94; acc: 0.8
Batch: 20; loss: 1.06; acc: 0.7
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.77
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.79; acc: 0.83
Val Epoch over. val_loss: 0.9933658056198411; val_accuracy: 0.7376592356687898 

The current subspace-distance is: 0.00016003145719878376 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.01; acc: 0.78
Batch: 20; loss: 1.24; acc: 0.58
Batch: 40; loss: 1.03; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 0.99; acc: 0.7
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.92; acc: 0.75
Batch: 160; loss: 1.03; acc: 0.67
Batch: 180; loss: 1.26; acc: 0.64
Batch: 200; loss: 1.15; acc: 0.66
Batch: 220; loss: 1.03; acc: 0.73
Batch: 240; loss: 1.09; acc: 0.66
Batch: 260; loss: 0.95; acc: 0.78
Batch: 280; loss: 1.25; acc: 0.59
Batch: 300; loss: 1.06; acc: 0.7
Batch: 320; loss: 1.01; acc: 0.75
Batch: 340; loss: 1.11; acc: 0.72
Batch: 360; loss: 1.28; acc: 0.69
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 0.89; acc: 0.8
Batch: 420; loss: 1.17; acc: 0.59
Batch: 440; loss: 1.18; acc: 0.66
Batch: 460; loss: 1.06; acc: 0.7
Batch: 480; loss: 0.98; acc: 0.78
Batch: 500; loss: 1.03; acc: 0.69
Batch: 520; loss: 1.15; acc: 0.58
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 1.23; acc: 0.62
Batch: 580; loss: 0.98; acc: 0.7
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.99; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.83
Batch: 660; loss: 1.02; acc: 0.72
Batch: 680; loss: 1.06; acc: 0.7
Batch: 700; loss: 1.13; acc: 0.64
Batch: 720; loss: 0.94; acc: 0.75
Batch: 740; loss: 0.87; acc: 0.83
Batch: 760; loss: 0.96; acc: 0.73
Batch: 780; loss: 1.06; acc: 0.7
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

0.00016771028458606452
0.00015855241508688778
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.98; acc: 0.75
Batch: 80; loss: 0.83; acc: 0.78
Batch: 100; loss: 1.14; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.79; acc: 0.83
Val Epoch over. val_loss: 0.9827732994298267; val_accuracy: 0.7455214968152867 

The current subspace-distance is: 0.00015855241508688778 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 1.02; acc: 0.75
Batch: 40; loss: 1.08; acc: 0.61
Batch: 60; loss: 1.25; acc: 0.61
Batch: 80; loss: 1.17; acc: 0.64
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.98; acc: 0.75
Batch: 160; loss: 1.12; acc: 0.66
Batch: 180; loss: 1.07; acc: 0.66
Batch: 200; loss: 1.01; acc: 0.72
Batch: 220; loss: 0.9; acc: 0.83
Batch: 240; loss: 0.95; acc: 0.81
Batch: 260; loss: 1.04; acc: 0.72
Batch: 280; loss: 0.99; acc: 0.77
Batch: 300; loss: 1.07; acc: 0.69
Batch: 320; loss: 1.06; acc: 0.64
Batch: 340; loss: 0.96; acc: 0.75
Batch: 360; loss: 1.06; acc: 0.7
Batch: 380; loss: 0.99; acc: 0.64
Batch: 400; loss: 0.88; acc: 0.81
Batch: 420; loss: 1.03; acc: 0.75
Batch: 440; loss: 0.94; acc: 0.75
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 0.87; acc: 0.8
Batch: 500; loss: 1.05; acc: 0.77
Batch: 520; loss: 0.95; acc: 0.72
Batch: 540; loss: 0.82; acc: 0.83
Batch: 560; loss: 0.94; acc: 0.77
Batch: 580; loss: 1.01; acc: 0.67
Batch: 600; loss: 1.04; acc: 0.72
Batch: 620; loss: 1.02; acc: 0.72
Batch: 640; loss: 0.95; acc: 0.81
Batch: 660; loss: 0.96; acc: 0.75
Batch: 680; loss: 1.21; acc: 0.62
Batch: 700; loss: 1.06; acc: 0.73
Batch: 720; loss: 1.16; acc: 0.67
Batch: 740; loss: 1.07; acc: 0.69
Batch: 760; loss: 0.97; acc: 0.7
Batch: 780; loss: 1.07; acc: 0.73
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

0.00016903228242881596
0.00016035596490837634
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.83; acc: 0.8
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.9774136034546385; val_accuracy: 0.7431329617834395 

The current subspace-distance is: 0.00016035596490837634 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.67
Batch: 80; loss: 0.89; acc: 0.7
Batch: 100; loss: 1.07; acc: 0.66
Batch: 120; loss: 1.19; acc: 0.7
Batch: 140; loss: 1.09; acc: 0.67
Batch: 160; loss: 1.0; acc: 0.7
Batch: 180; loss: 1.2; acc: 0.62
Batch: 200; loss: 0.97; acc: 0.72
Batch: 220; loss: 0.97; acc: 0.75
Batch: 240; loss: 1.04; acc: 0.69
Batch: 260; loss: 1.04; acc: 0.7
Batch: 280; loss: 0.98; acc: 0.72
Batch: 300; loss: 0.97; acc: 0.78
Batch: 320; loss: 0.99; acc: 0.75
Batch: 340; loss: 0.99; acc: 0.77
Batch: 360; loss: 1.0; acc: 0.8
Batch: 380; loss: 1.25; acc: 0.61
Batch: 400; loss: 1.02; acc: 0.69
Batch: 420; loss: 1.12; acc: 0.64
Batch: 440; loss: 0.79; acc: 0.84
Batch: 460; loss: 1.23; acc: 0.59
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 0.92; acc: 0.8
Batch: 520; loss: 1.11; acc: 0.67
Batch: 540; loss: 1.08; acc: 0.66
Batch: 560; loss: 0.98; acc: 0.73
Batch: 580; loss: 0.99; acc: 0.7
Batch: 600; loss: 1.01; acc: 0.75
Batch: 620; loss: 1.07; acc: 0.69
Batch: 640; loss: 1.06; acc: 0.67
Batch: 660; loss: 0.95; acc: 0.78
Batch: 680; loss: 1.03; acc: 0.66
Batch: 700; loss: 1.11; acc: 0.64
Batch: 720; loss: 1.01; acc: 0.73
Batch: 740; loss: 1.05; acc: 0.67
Batch: 760; loss: 0.91; acc: 0.81
Batch: 780; loss: 1.08; acc: 0.72
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

0.0001673733495408669
0.00015974364941939712
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.04; acc: 0.7
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.79; acc: 0.81
Val Epoch over. val_loss: 0.9844267212661209; val_accuracy: 0.7366640127388535 

The current subspace-distance is: 0.00015974364941939712 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 1.22; acc: 0.69
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 0.91; acc: 0.78
Batch: 160; loss: 1.04; acc: 0.64
Batch: 180; loss: 1.02; acc: 0.72
Batch: 200; loss: 0.97; acc: 0.8
Batch: 220; loss: 0.88; acc: 0.8
Batch: 240; loss: 0.95; acc: 0.75
Batch: 260; loss: 1.13; acc: 0.62
Batch: 280; loss: 0.99; acc: 0.7
Batch: 300; loss: 1.2; acc: 0.59
Batch: 320; loss: 1.01; acc: 0.7
Batch: 340; loss: 1.03; acc: 0.77
Batch: 360; loss: 0.86; acc: 0.8
Batch: 380; loss: 1.1; acc: 0.66
Batch: 400; loss: 0.94; acc: 0.75
Batch: 420; loss: 0.99; acc: 0.72
Batch: 440; loss: 1.01; acc: 0.69
Batch: 460; loss: 0.97; acc: 0.73
Batch: 480; loss: 1.04; acc: 0.66
Batch: 500; loss: 1.1; acc: 0.66
Batch: 520; loss: 0.89; acc: 0.77
Batch: 540; loss: 1.07; acc: 0.67
Batch: 560; loss: 1.13; acc: 0.64
Batch: 580; loss: 1.44; acc: 0.48
Batch: 600; loss: 0.95; acc: 0.77
Batch: 620; loss: 1.06; acc: 0.64
Batch: 640; loss: 1.0; acc: 0.72
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 0.96; acc: 0.75
Batch: 700; loss: 1.08; acc: 0.67
Batch: 720; loss: 0.89; acc: 0.77
Batch: 740; loss: 0.98; acc: 0.77
Batch: 760; loss: 1.08; acc: 0.69
Batch: 780; loss: 1.09; acc: 0.7
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

0.00017187627963721752
0.00016231481276918203
Batch: 0; loss: 0.95; acc: 0.75
Batch: 20; loss: 1.05; acc: 0.7
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.79; acc: 0.81
Val Epoch over. val_loss: 0.9935039703253727; val_accuracy: 0.7377587579617835 

The current subspace-distance is: 0.00016231481276918203 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.97; acc: 0.73
Batch: 20; loss: 1.14; acc: 0.67
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 1.04; acc: 0.7
Batch: 160; loss: 1.0; acc: 0.81
Batch: 180; loss: 0.97; acc: 0.7
Batch: 200; loss: 1.13; acc: 0.66
Batch: 220; loss: 0.9; acc: 0.78
Batch: 240; loss: 1.15; acc: 0.69
Batch: 260; loss: 1.22; acc: 0.62
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 0.97; acc: 0.77
Batch: 320; loss: 1.08; acc: 0.67
Batch: 340; loss: 1.16; acc: 0.66
Batch: 360; loss: 0.99; acc: 0.67
Batch: 380; loss: 1.01; acc: 0.67
Batch: 400; loss: 1.01; acc: 0.67
Batch: 420; loss: 0.95; acc: 0.78
Batch: 440; loss: 0.93; acc: 0.73
Batch: 460; loss: 1.03; acc: 0.69
Batch: 480; loss: 1.11; acc: 0.62
Batch: 500; loss: 0.87; acc: 0.8
Batch: 520; loss: 1.05; acc: 0.7
Batch: 540; loss: 1.0; acc: 0.69
Batch: 560; loss: 0.96; acc: 0.75
Batch: 580; loss: 1.05; acc: 0.7
Batch: 600; loss: 0.92; acc: 0.75
Batch: 620; loss: 0.95; acc: 0.73
Batch: 640; loss: 1.09; acc: 0.66
Batch: 660; loss: 0.96; acc: 0.7
Batch: 680; loss: 1.04; acc: 0.69
Batch: 700; loss: 1.1; acc: 0.73
Batch: 720; loss: 1.13; acc: 0.69
Batch: 740; loss: 0.97; acc: 0.73
Batch: 760; loss: 1.17; acc: 0.72
Batch: 780; loss: 0.87; acc: 0.83
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

0.00016967207193374634
0.00016212076297961175
Batch: 0; loss: 0.93; acc: 0.77
Batch: 20; loss: 1.05; acc: 0.69
Batch: 40; loss: 0.75; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.83; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.78; acc: 0.81
Val Epoch over. val_loss: 0.9748167266511614; val_accuracy: 0.738953025477707 

The current subspace-distance is: 0.00016212076297961175 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.78
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 1.0; acc: 0.8
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.89; acc: 0.73
Batch: 120; loss: 1.06; acc: 0.67
Batch: 140; loss: 1.2; acc: 0.59
Batch: 160; loss: 0.87; acc: 0.81
Batch: 180; loss: 0.99; acc: 0.7
Batch: 200; loss: 0.92; acc: 0.77
Batch: 220; loss: 0.89; acc: 0.78
Batch: 240; loss: 1.03; acc: 0.73
Batch: 260; loss: 0.92; acc: 0.67
Batch: 280; loss: 1.03; acc: 0.72
Batch: 300; loss: 1.07; acc: 0.67
Batch: 320; loss: 1.06; acc: 0.62
Batch: 340; loss: 1.08; acc: 0.64
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.11; acc: 0.7
Batch: 400; loss: 1.17; acc: 0.64
Batch: 420; loss: 1.04; acc: 0.75
Batch: 440; loss: 0.96; acc: 0.72
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.02; acc: 0.73
Batch: 500; loss: 1.04; acc: 0.69
Batch: 520; loss: 1.03; acc: 0.73
Batch: 540; loss: 1.13; acc: 0.62
Batch: 560; loss: 1.09; acc: 0.73
Batch: 580; loss: 0.93; acc: 0.81
Batch: 600; loss: 0.95; acc: 0.72
Batch: 620; loss: 0.99; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.72
Batch: 660; loss: 0.9; acc: 0.78
Batch: 680; loss: 1.11; acc: 0.7
Batch: 700; loss: 1.11; acc: 0.66
Batch: 720; loss: 1.02; acc: 0.7
Batch: 740; loss: 0.97; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.73
Batch: 780; loss: 0.9; acc: 0.77
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

0.000169519946211949
0.00015969379455782473
Batch: 0; loss: 0.94; acc: 0.77
Batch: 20; loss: 1.05; acc: 0.7
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.73
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.79; acc: 0.81
Val Epoch over. val_loss: 0.9896839044655964; val_accuracy: 0.7351711783439491 

The current subspace-distance is: 0.00015969379455782473 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.97; acc: 0.73
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 1.03; acc: 0.67
Batch: 60; loss: 0.87; acc: 0.86
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.87; acc: 0.83
Batch: 160; loss: 0.95; acc: 0.75
Batch: 180; loss: 1.11; acc: 0.67
Batch: 200; loss: 1.01; acc: 0.73
Batch: 220; loss: 0.83; acc: 0.83
Batch: 240; loss: 0.91; acc: 0.75
Batch: 260; loss: 1.05; acc: 0.77
Batch: 280; loss: 0.96; acc: 0.75
Batch: 300; loss: 1.14; acc: 0.61
Batch: 320; loss: 1.05; acc: 0.69
Batch: 340; loss: 0.98; acc: 0.72
Batch: 360; loss: 1.16; acc: 0.61
Batch: 380; loss: 1.14; acc: 0.67
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 0.94; acc: 0.78
Batch: 440; loss: 1.02; acc: 0.7
Batch: 460; loss: 0.92; acc: 0.77
Batch: 480; loss: 0.95; acc: 0.77
Batch: 500; loss: 1.0; acc: 0.8
Batch: 520; loss: 0.99; acc: 0.77
Batch: 540; loss: 0.99; acc: 0.73
Batch: 560; loss: 0.97; acc: 0.7
Batch: 580; loss: 1.03; acc: 0.67
Batch: 600; loss: 1.22; acc: 0.72
Batch: 620; loss: 0.84; acc: 0.83
Batch: 640; loss: 0.92; acc: 0.73
Batch: 660; loss: 0.94; acc: 0.75
Batch: 680; loss: 0.99; acc: 0.69
Batch: 700; loss: 0.87; acc: 0.81
Batch: 720; loss: 1.02; acc: 0.72
Batch: 740; loss: 0.99; acc: 0.75
Batch: 760; loss: 1.13; acc: 0.62
Batch: 780; loss: 1.03; acc: 0.7
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

0.00016975420294329524
0.00016353357932530344
Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 0.83; acc: 0.78
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.8; acc: 0.81
Val Epoch over. val_loss: 0.9852162519837641; val_accuracy: 0.7355692675159236 

The current subspace-distance is: 0.00016353357932530344 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.01; acc: 0.69
Batch: 20; loss: 1.0; acc: 0.75
Batch: 40; loss: 1.29; acc: 0.61
Batch: 60; loss: 1.05; acc: 0.77
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 0.96; acc: 0.72
Batch: 120; loss: 1.1; acc: 0.69
Batch: 140; loss: 0.99; acc: 0.72
Batch: 160; loss: 1.12; acc: 0.66
Batch: 180; loss: 1.03; acc: 0.72
Batch: 200; loss: 0.98; acc: 0.75
Batch: 220; loss: 1.08; acc: 0.69
Batch: 240; loss: 1.03; acc: 0.7
Batch: 260; loss: 1.0; acc: 0.7
Batch: 280; loss: 1.09; acc: 0.66
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 0.92; acc: 0.77
Batch: 340; loss: 1.21; acc: 0.62
Batch: 360; loss: 0.96; acc: 0.7
Batch: 380; loss: 1.02; acc: 0.67
Batch: 400; loss: 1.1; acc: 0.64
Batch: 420; loss: 1.01; acc: 0.78
Batch: 440; loss: 1.08; acc: 0.61
Batch: 460; loss: 0.85; acc: 0.78
Batch: 480; loss: 1.12; acc: 0.7
Batch: 500; loss: 0.97; acc: 0.73
Batch: 520; loss: 1.02; acc: 0.7
Batch: 540; loss: 0.98; acc: 0.72
Batch: 560; loss: 0.97; acc: 0.77
Batch: 580; loss: 0.96; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.7
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.11; acc: 0.58
Batch: 680; loss: 1.22; acc: 0.62
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 1.06; acc: 0.67
Batch: 740; loss: 0.98; acc: 0.73
Batch: 760; loss: 0.93; acc: 0.8
Batch: 780; loss: 1.06; acc: 0.67
Train Epoch over. train_loss: 1.02; train_accuracy: 0.71 

0.0001742975873639807
0.00016776791017036885
Batch: 0; loss: 0.94; acc: 0.75
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 0.83; acc: 0.78
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 0.79; acc: 0.81
Val Epoch over. val_loss: 0.980721350688084; val_accuracy: 0.738156847133758 

The current subspace-distance is: 0.00016776791017036885 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.0; acc: 0.8
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.59
Batch: 80; loss: 1.09; acc: 0.64
Batch: 100; loss: 1.22; acc: 0.61
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 0.93; acc: 0.72
Batch: 160; loss: 0.94; acc: 0.78
Batch: 180; loss: 1.15; acc: 0.62
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 1.09; acc: 0.72
Batch: 240; loss: 1.03; acc: 0.73
Batch: 260; loss: 0.87; acc: 0.78
Batch: 280; loss: 1.02; acc: 0.67
Batch: 300; loss: 0.91; acc: 0.73
Batch: 320; loss: 0.94; acc: 0.8
Batch: 340; loss: 1.0; acc: 0.7
Batch: 360; loss: 1.13; acc: 0.69
Batch: 380; loss: 1.07; acc: 0.72
Batch: 400; loss: 0.86; acc: 0.81
Batch: 420; loss: 1.19; acc: 0.67
Batch: 440; loss: 1.05; acc: 0.66
Batch: 460; loss: 1.07; acc: 0.77
Batch: 480; loss: 1.11; acc: 0.64
Batch: 500; loss: 1.13; acc: 0.61
Batch: 520; loss: 1.07; acc: 0.7
Batch: 540; loss: 0.95; acc: 0.72
Batch: 560; loss: 0.94; acc: 0.77
Batch: 580; loss: 1.02; acc: 0.78
Batch: 600; loss: 0.9; acc: 0.75
Batch: 620; loss: 0.95; acc: 0.77
Batch: 640; loss: 0.98; acc: 0.75
Batch: 660; loss: 1.01; acc: 0.75
Batch: 680; loss: 1.11; acc: 0.69
Batch: 700; loss: 1.06; acc: 0.72
Batch: 720; loss: 1.1; acc: 0.72
Batch: 740; loss: 1.01; acc: 0.75
Batch: 760; loss: 0.95; acc: 0.78
Batch: 780; loss: 0.93; acc: 0.73
Train Epoch over. train_loss: 1.02; train_accuracy: 0.71 

0.00017242584726773202
0.0001629051985219121
Batch: 0; loss: 0.93; acc: 0.77
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 0.81; acc: 0.81
Batch: 100; loss: 1.13; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 0.78; acc: 0.8
Val Epoch over. val_loss: 0.9721431849868434; val_accuracy: 0.7391520700636943 

The current subspace-distance is: 0.0001629051985219121 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.08; acc: 0.72
Batch: 40; loss: 1.12; acc: 0.66
Batch: 60; loss: 1.16; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.0; acc: 0.7
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 1.07; acc: 0.62
Batch: 160; loss: 0.97; acc: 0.75
Batch: 180; loss: 1.04; acc: 0.72
Batch: 200; loss: 1.11; acc: 0.67
Batch: 220; loss: 0.87; acc: 0.72
Batch: 240; loss: 1.07; acc: 0.72
Batch: 260; loss: 1.06; acc: 0.73
Batch: 280; loss: 1.11; acc: 0.64
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 0.91; acc: 0.75
Batch: 340; loss: 0.99; acc: 0.75
Batch: 360; loss: 1.24; acc: 0.67
Batch: 380; loss: 0.95; acc: 0.73
Batch: 400; loss: 1.27; acc: 0.56
Batch: 420; loss: 1.03; acc: 0.7
Batch: 440; loss: 1.1; acc: 0.67
Batch: 460; loss: 1.06; acc: 0.75
Batch: 480; loss: 1.04; acc: 0.67
Batch: 500; loss: 1.06; acc: 0.67
Batch: 520; loss: 0.9; acc: 0.78
Batch: 540; loss: 1.07; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.64
Batch: 580; loss: 1.24; acc: 0.56
Batch: 600; loss: 1.13; acc: 0.64
Batch: 620; loss: 1.03; acc: 0.73
Batch: 640; loss: 1.0; acc: 0.73
Batch: 660; loss: 0.96; acc: 0.81
Batch: 680; loss: 0.91; acc: 0.77
Batch: 700; loss: 0.95; acc: 0.75
Batch: 720; loss: 1.0; acc: 0.75
Batch: 740; loss: 0.99; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.69
Batch: 780; loss: 1.04; acc: 0.69
Train Epoch over. train_loss: 1.02; train_accuracy: 0.71 

0.00017131181084550917
0.0001635581866139546
Batch: 0; loss: 0.93; acc: 0.77
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 0.82; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 0.79; acc: 0.81
Val Epoch over. val_loss: 0.9769254049677758; val_accuracy: 0.7386544585987261 

The current subspace-distance is: 0.0001635581866139546 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.69
Batch: 40; loss: 0.91; acc: 0.75
Batch: 60; loss: 1.08; acc: 0.67
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 0.98; acc: 0.73
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.98; acc: 0.77
Batch: 160; loss: 0.96; acc: 0.73
Batch: 180; loss: 1.0; acc: 0.73
Batch: 200; loss: 0.92; acc: 0.78
Batch: 220; loss: 1.0; acc: 0.73
Batch: 240; loss: 1.17; acc: 0.67
Batch: 260; loss: 1.22; acc: 0.62
Batch: 280; loss: 1.17; acc: 0.64
Batch: 300; loss: 1.06; acc: 0.62
Batch: 320; loss: 1.19; acc: 0.62
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 0.87; acc: 0.81
Batch: 380; loss: 1.01; acc: 0.75
Batch: 400; loss: 0.98; acc: 0.72
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 0.92; acc: 0.77
Batch: 460; loss: 0.97; acc: 0.75
Batch: 480; loss: 0.97; acc: 0.69
Batch: 500; loss: 0.99; acc: 0.72
Batch: 520; loss: 0.92; acc: 0.73
Batch: 540; loss: 1.01; acc: 0.69
Batch: 560; loss: 1.01; acc: 0.75
Batch: 580; loss: 1.3; acc: 0.56
Batch: 600; loss: 0.97; acc: 0.66
Batch: 620; loss: 1.14; acc: 0.69
Batch: 640; loss: 0.99; acc: 0.72
Batch: 660; loss: 1.0; acc: 0.7
Batch: 680; loss: 0.98; acc: 0.73
Batch: 700; loss: 1.17; acc: 0.64
Batch: 720; loss: 0.93; acc: 0.72
Batch: 740; loss: 1.02; acc: 0.73
Batch: 760; loss: 1.05; acc: 0.7
Batch: 780; loss: 0.97; acc: 0.7
Train Epoch over. train_loss: 1.02; train_accuracy: 0.71 

0.00017353302973788232
0.0001642133720451966
Batch: 0; loss: 0.95; acc: 0.77
Batch: 20; loss: 1.05; acc: 0.7
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 0.84; acc: 0.81
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 0.8; acc: 0.81
Val Epoch over. val_loss: 0.9873472102888071; val_accuracy: 0.7361664012738853 

The current subspace-distance is: 0.0001642133720451966 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_12_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.7743928539080627

The number of parameters is: 247624

The number of individual parameters is:

15
270
15
15
22
36960
22
22
43
105952
43
43
64
99072
64
64
4096
64
640
10
64
64

nonzero elements in E: 49524796
elements in E: 49524800
fraction nonzero: 0.9999999192323846
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.12
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.01; acc: 0.3
Batch: 60; loss: 1.89; acc: 0.45
Batch: 80; loss: 2.0; acc: 0.33
Batch: 100; loss: 1.82; acc: 0.55
Batch: 120; loss: 1.84; acc: 0.48
Batch: 140; loss: 1.83; acc: 0.48
Batch: 160; loss: 1.73; acc: 0.56
Batch: 180; loss: 1.71; acc: 0.62
Batch: 200; loss: 1.63; acc: 0.64
Batch: 220; loss: 1.7; acc: 0.56
Batch: 240; loss: 1.54; acc: 0.69
Batch: 260; loss: 1.69; acc: 0.59
Batch: 280; loss: 1.67; acc: 0.61
Batch: 300; loss: 1.48; acc: 0.75
Batch: 320; loss: 1.49; acc: 0.7
Batch: 340; loss: 1.61; acc: 0.64
Batch: 360; loss: 1.47; acc: 0.67
Batch: 380; loss: 1.62; acc: 0.62
Batch: 400; loss: 1.46; acc: 0.67
Batch: 420; loss: 1.42; acc: 0.7
Batch: 440; loss: 1.49; acc: 0.7
Batch: 460; loss: 1.54; acc: 0.62
Batch: 480; loss: 1.49; acc: 0.67
Batch: 500; loss: 1.36; acc: 0.78
Batch: 520; loss: 1.48; acc: 0.69
Batch: 540; loss: 1.4; acc: 0.7
Batch: 560; loss: 1.35; acc: 0.73
Batch: 580; loss: 1.49; acc: 0.77
Batch: 600; loss: 1.49; acc: 0.7
Batch: 620; loss: 1.45; acc: 0.67
Batch: 640; loss: 1.25; acc: 0.8
Batch: 660; loss: 1.51; acc: 0.62
Batch: 680; loss: 1.34; acc: 0.75
Batch: 700; loss: 1.35; acc: 0.66
Batch: 720; loss: 1.37; acc: 0.83
Batch: 740; loss: 1.32; acc: 0.7
Batch: 760; loss: 1.36; acc: 0.67
Batch: 780; loss: 1.29; acc: 0.7
Train Epoch over. train_loss: 1.58; train_accuracy: 0.62 

5.657100700773299e-05
5.200149098527618e-05
Batch: 0; loss: 1.41; acc: 0.64
Batch: 20; loss: 1.46; acc: 0.67
Batch: 40; loss: 1.04; acc: 0.88
Batch: 60; loss: 1.25; acc: 0.75
Batch: 80; loss: 1.16; acc: 0.84
Batch: 100; loss: 1.26; acc: 0.78
Batch: 120; loss: 1.44; acc: 0.67
Batch: 140; loss: 1.13; acc: 0.84
Val Epoch over. val_loss: 1.2857906947470015; val_accuracy: 0.7628383757961783 

The current subspace-distance is: 5.200149098527618e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.73
Batch: 20; loss: 1.42; acc: 0.62
Batch: 40; loss: 1.41; acc: 0.67
Batch: 60; loss: 1.42; acc: 0.66
Batch: 80; loss: 1.36; acc: 0.75
Batch: 100; loss: 1.27; acc: 0.77
Batch: 120; loss: 1.29; acc: 0.75
Batch: 140; loss: 1.31; acc: 0.72
Batch: 160; loss: 1.19; acc: 0.77
Batch: 180; loss: 1.39; acc: 0.67
Batch: 200; loss: 1.22; acc: 0.77
Batch: 220; loss: 1.28; acc: 0.66
Batch: 240; loss: 1.25; acc: 0.7
Batch: 260; loss: 1.36; acc: 0.72
Batch: 280; loss: 1.21; acc: 0.78
Batch: 300; loss: 1.13; acc: 0.84
Batch: 320; loss: 1.22; acc: 0.77
Batch: 340; loss: 1.23; acc: 0.73
Batch: 360; loss: 1.11; acc: 0.86
Batch: 380; loss: 1.26; acc: 0.81
Batch: 400; loss: 1.24; acc: 0.72
Batch: 420; loss: 1.11; acc: 0.83
Batch: 440; loss: 1.23; acc: 0.73
Batch: 460; loss: 1.14; acc: 0.77
Batch: 480; loss: 1.22; acc: 0.77
Batch: 500; loss: 1.11; acc: 0.78
Batch: 520; loss: 1.15; acc: 0.75
Batch: 540; loss: 1.19; acc: 0.75
Batch: 560; loss: 1.16; acc: 0.81
Batch: 580; loss: 1.17; acc: 0.78
Batch: 600; loss: 1.19; acc: 0.78
Batch: 620; loss: 1.14; acc: 0.83
Batch: 640; loss: 1.19; acc: 0.78
Batch: 660; loss: 1.1; acc: 0.77
Batch: 680; loss: 1.21; acc: 0.75
Batch: 700; loss: 1.04; acc: 0.88
Batch: 720; loss: 1.18; acc: 0.81
Batch: 740; loss: 1.02; acc: 0.84
Batch: 760; loss: 1.16; acc: 0.81
Batch: 780; loss: 1.19; acc: 0.75
Train Epoch over. train_loss: 1.23; train_accuracy: 0.76 

8.0302961578127e-05
7.489750714739785e-05
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.69
Batch: 40; loss: 0.82; acc: 0.95
Batch: 60; loss: 1.09; acc: 0.73
Batch: 80; loss: 0.95; acc: 0.86
Batch: 100; loss: 1.08; acc: 0.81
Batch: 120; loss: 1.28; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.89
Val Epoch over. val_loss: 1.0911621170438779; val_accuracy: 0.801453025477707 

The current subspace-distance is: 7.489750714739785e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.86
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 1.1; acc: 0.77
Batch: 60; loss: 1.08; acc: 0.81
Batch: 80; loss: 1.09; acc: 0.83
Batch: 100; loss: 1.18; acc: 0.8
Batch: 120; loss: 1.05; acc: 0.83
Batch: 140; loss: 1.21; acc: 0.73
Batch: 160; loss: 1.14; acc: 0.72
Batch: 180; loss: 1.13; acc: 0.73
Batch: 200; loss: 1.02; acc: 0.86
Batch: 220; loss: 1.18; acc: 0.77
Batch: 240; loss: 1.01; acc: 0.86
Batch: 260; loss: 1.03; acc: 0.88
Batch: 280; loss: 1.04; acc: 0.81
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 0.97; acc: 0.88
Batch: 340; loss: 1.03; acc: 0.83
Batch: 360; loss: 1.16; acc: 0.75
Batch: 380; loss: 1.24; acc: 0.67
Batch: 400; loss: 1.0; acc: 0.77
Batch: 420; loss: 1.04; acc: 0.78
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.08; acc: 0.78
Batch: 480; loss: 1.01; acc: 0.88
Batch: 500; loss: 1.14; acc: 0.78
Batch: 520; loss: 1.15; acc: 0.66
Batch: 540; loss: 1.15; acc: 0.78
Batch: 560; loss: 0.97; acc: 0.81
Batch: 580; loss: 1.17; acc: 0.73
Batch: 600; loss: 1.06; acc: 0.78
Batch: 620; loss: 1.0; acc: 0.81
Batch: 640; loss: 1.25; acc: 0.7
Batch: 660; loss: 1.09; acc: 0.77
Batch: 680; loss: 1.12; acc: 0.8
Batch: 700; loss: 0.95; acc: 0.84
Batch: 720; loss: 1.05; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.78
Batch: 760; loss: 1.14; acc: 0.7
Batch: 780; loss: 1.26; acc: 0.7
Train Epoch over. train_loss: 1.09; train_accuracy: 0.78 

9.675296314526349e-05
9.143172792391852e-05
Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 0.72; acc: 0.94
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 0.93; acc: 0.86
Batch: 120; loss: 1.24; acc: 0.72
Batch: 140; loss: 0.86; acc: 0.84
Val Epoch over. val_loss: 0.9814346945209868; val_accuracy: 0.8107085987261147 

The current subspace-distance is: 9.143172792391852e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.83
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.1; acc: 0.75
Batch: 140; loss: 0.93; acc: 0.86
Batch: 160; loss: 0.95; acc: 0.8
Batch: 180; loss: 0.92; acc: 0.84
Batch: 200; loss: 1.06; acc: 0.8
Batch: 220; loss: 0.98; acc: 0.86
Batch: 240; loss: 1.07; acc: 0.78
Batch: 260; loss: 1.03; acc: 0.77
Batch: 280; loss: 0.9; acc: 0.81
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.02; acc: 0.8
Batch: 340; loss: 1.09; acc: 0.67
Batch: 360; loss: 1.07; acc: 0.78
Batch: 380; loss: 0.89; acc: 0.88
Batch: 400; loss: 0.96; acc: 0.78
Batch: 420; loss: 0.99; acc: 0.75
Batch: 440; loss: 1.02; acc: 0.78
Batch: 460; loss: 0.99; acc: 0.81
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.02; acc: 0.81
Batch: 520; loss: 0.89; acc: 0.86
Batch: 540; loss: 1.03; acc: 0.73
Batch: 560; loss: 0.96; acc: 0.75
Batch: 580; loss: 1.1; acc: 0.72
Batch: 600; loss: 0.93; acc: 0.84
Batch: 620; loss: 0.92; acc: 0.81
Batch: 640; loss: 0.97; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.86
Batch: 680; loss: 0.86; acc: 0.84
Batch: 700; loss: 0.98; acc: 0.77
Batch: 720; loss: 0.9; acc: 0.81
Batch: 740; loss: 0.92; acc: 0.83
Batch: 760; loss: 0.89; acc: 0.86
Batch: 780; loss: 0.89; acc: 0.77
Train Epoch over. train_loss: 0.99; train_accuracy: 0.79 

0.00011288533278275281
0.00010698629193939269
Batch: 0; loss: 1.01; acc: 0.66
Batch: 20; loss: 1.04; acc: 0.73
Batch: 40; loss: 0.64; acc: 0.95
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.73; acc: 0.88
Batch: 100; loss: 0.87; acc: 0.86
Batch: 120; loss: 1.2; acc: 0.72
Batch: 140; loss: 0.74; acc: 0.89
Val Epoch over. val_loss: 0.8827690102492168; val_accuracy: 0.8215565286624203 

The current subspace-distance is: 0.00010698629193939269 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.8
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.98; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.73
Batch: 100; loss: 0.8; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.83
Batch: 140; loss: 0.84; acc: 0.78
Batch: 160; loss: 1.03; acc: 0.73
Batch: 180; loss: 1.04; acc: 0.72
Batch: 200; loss: 0.84; acc: 0.86
Batch: 220; loss: 0.89; acc: 0.84
Batch: 240; loss: 1.07; acc: 0.72
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 1.1; acc: 0.64
Batch: 300; loss: 1.01; acc: 0.72
Batch: 320; loss: 0.81; acc: 0.83
Batch: 340; loss: 0.99; acc: 0.77
Batch: 360; loss: 0.9; acc: 0.83
Batch: 380; loss: 0.89; acc: 0.84
Batch: 400; loss: 0.84; acc: 0.84
Batch: 420; loss: 1.0; acc: 0.77
Batch: 440; loss: 0.87; acc: 0.78
Batch: 460; loss: 0.88; acc: 0.8
Batch: 480; loss: 0.95; acc: 0.81
Batch: 500; loss: 1.06; acc: 0.7
Batch: 520; loss: 0.8; acc: 0.86
Batch: 540; loss: 0.96; acc: 0.8
Batch: 560; loss: 0.93; acc: 0.77
Batch: 580; loss: 0.82; acc: 0.84
Batch: 600; loss: 0.85; acc: 0.83
Batch: 620; loss: 0.94; acc: 0.84
Batch: 640; loss: 0.86; acc: 0.83
Batch: 660; loss: 1.03; acc: 0.75
Batch: 680; loss: 0.94; acc: 0.77
Batch: 700; loss: 0.79; acc: 0.84
Batch: 720; loss: 0.94; acc: 0.73
Batch: 740; loss: 0.78; acc: 0.89
Batch: 760; loss: 0.85; acc: 0.83
Batch: 780; loss: 0.96; acc: 0.73
Train Epoch over. train_loss: 0.9; train_accuracy: 0.8 

0.0001242825819645077
0.00011889207235071808
Batch: 0; loss: 0.96; acc: 0.7
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.57; acc: 0.95
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.64; acc: 0.89
Batch: 100; loss: 0.83; acc: 0.83
Batch: 120; loss: 1.13; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.89
Val Epoch over. val_loss: 0.8112525397045597; val_accuracy: 0.8290207006369427 

The current subspace-distance is: 0.00011889207235071808 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.8
Batch: 20; loss: 0.91; acc: 0.86
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.85; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.8
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.83; acc: 0.84
Batch: 160; loss: 0.93; acc: 0.77
Batch: 180; loss: 0.81; acc: 0.84
Batch: 200; loss: 0.81; acc: 0.81
Batch: 220; loss: 0.93; acc: 0.7
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.8
Batch: 280; loss: 0.95; acc: 0.75
Batch: 300; loss: 0.81; acc: 0.83
Batch: 320; loss: 0.87; acc: 0.8
Batch: 340; loss: 0.91; acc: 0.72
Batch: 360; loss: 0.69; acc: 0.86
Batch: 380; loss: 0.88; acc: 0.78
Batch: 400; loss: 0.79; acc: 0.84
Batch: 420; loss: 0.72; acc: 0.84
Batch: 440; loss: 0.7; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.89
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.78; acc: 0.86
Batch: 520; loss: 0.95; acc: 0.75
Batch: 540; loss: 0.85; acc: 0.78
Batch: 560; loss: 1.06; acc: 0.7
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.89; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.86
Batch: 660; loss: 0.89; acc: 0.84
Batch: 680; loss: 0.81; acc: 0.81
Batch: 700; loss: 0.65; acc: 0.89
Batch: 720; loss: 0.68; acc: 0.88
Batch: 740; loss: 0.75; acc: 0.91
Batch: 760; loss: 0.76; acc: 0.83
Batch: 780; loss: 0.79; acc: 0.8
Train Epoch over. train_loss: 0.84; train_accuracy: 0.81 

0.00013325564214028418
0.0001292523229494691
Batch: 0; loss: 0.9; acc: 0.73
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.94
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 0.6; acc: 0.91
Val Epoch over. val_loss: 0.7664986537520293; val_accuracy: 0.8315087579617835 

The current subspace-distance is: 0.0001292523229494691 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.75
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.91; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.88
Batch: 160; loss: 0.68; acc: 0.89
Batch: 180; loss: 0.79; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.89
Batch: 220; loss: 0.92; acc: 0.81
Batch: 240; loss: 0.83; acc: 0.78
Batch: 260; loss: 0.92; acc: 0.78
Batch: 280; loss: 0.92; acc: 0.73
Batch: 300; loss: 0.98; acc: 0.73
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.91; acc: 0.75
Batch: 360; loss: 0.72; acc: 0.89
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.89
Batch: 420; loss: 0.73; acc: 0.86
Batch: 440; loss: 0.84; acc: 0.77
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.84; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.8; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.89
Batch: 560; loss: 0.76; acc: 0.89
Batch: 580; loss: 0.68; acc: 0.88
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.89
Batch: 660; loss: 0.67; acc: 0.88
Batch: 680; loss: 0.67; acc: 0.89
Batch: 700; loss: 0.94; acc: 0.72
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.83; acc: 0.77
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.8; train_accuracy: 0.81 

0.00014116897364147007
0.00013547492562793195
Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.95
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 1.0; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.91
Val Epoch over. val_loss: 0.7267380892091496; val_accuracy: 0.8402667197452229 

The current subspace-distance is: 0.00013547492562793195 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 0.94; acc: 0.73
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.86; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.83
Batch: 260; loss: 0.85; acc: 0.78
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.99; acc: 0.75
Batch: 340; loss: 0.92; acc: 0.73
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.91; acc: 0.86
Batch: 400; loss: 0.81; acc: 0.8
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.95; acc: 0.75
Batch: 480; loss: 0.76; acc: 0.78
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.84; acc: 0.8
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.89
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.77; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.79; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.79; acc: 0.78
Batch: 780; loss: 0.9; acc: 0.81
Train Epoch over. train_loss: 0.77; train_accuracy: 0.82 

0.00014916752115823328
0.00014342366193886846
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.95
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 1.0; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.86
Val Epoch over. val_loss: 0.7078632047981214; val_accuracy: 0.8385748407643312 

The current subspace-distance is: 0.00014342366193886846 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.78
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.61; acc: 0.91
Batch: 100; loss: 0.82; acc: 0.83
Batch: 120; loss: 0.83; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.89
Batch: 160; loss: 0.78; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.88; acc: 0.73
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.84; acc: 0.78
Batch: 280; loss: 0.79; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.87; acc: 0.73
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.88
Batch: 400; loss: 0.65; acc: 0.88
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.85; acc: 0.78
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.73; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.76; acc: 0.81
Batch: 540; loss: 0.86; acc: 0.69
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 0.96; acc: 0.75
Batch: 600; loss: 0.83; acc: 0.8
Batch: 620; loss: 0.88; acc: 0.77
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.84; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.89
Batch: 740; loss: 0.75; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.88
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00015486856864299625
0.00014743770589120686
Batch: 0; loss: 0.79; acc: 0.8
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.48; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.89
Val Epoch over. val_loss: 0.6903863211346281; val_accuracy: 0.8436504777070064 

The current subspace-distance is: 0.00014743770589120686 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.76; acc: 0.86
Batch: 160; loss: 0.69; acc: 0.88
Batch: 180; loss: 0.9; acc: 0.78
Batch: 200; loss: 0.92; acc: 0.69
Batch: 220; loss: 0.81; acc: 0.78
Batch: 240; loss: 0.88; acc: 0.8
Batch: 260; loss: 0.99; acc: 0.7
Batch: 280; loss: 0.68; acc: 0.88
Batch: 300; loss: 0.84; acc: 0.83
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 0.88; acc: 0.73
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.7; acc: 0.89
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.89
Batch: 480; loss: 0.67; acc: 0.89
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.86; acc: 0.75
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.81; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.72; acc: 0.84
Batch: 700; loss: 0.82; acc: 0.75
Batch: 720; loss: 0.79; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.78
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.00016155729827005416
0.0001566837017890066
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.6652062516303578; val_accuracy: 0.8479299363057324 

The current subspace-distance is: 0.0001566837017890066 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.91; acc: 0.78
Batch: 60; loss: 0.57; acc: 0.91
Batch: 80; loss: 0.85; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.77; acc: 0.73
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.75; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.55; acc: 0.92
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.78; acc: 0.75
Batch: 340; loss: 0.69; acc: 0.84
Batch: 360; loss: 0.81; acc: 0.78
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.92; acc: 0.7
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.74; acc: 0.86
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.88
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.76; acc: 0.86
Batch: 580; loss: 0.79; acc: 0.8
Batch: 600; loss: 0.81; acc: 0.78
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.88; acc: 0.8
Batch: 660; loss: 0.85; acc: 0.72
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.77; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.73; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00016298300761263818
0.00015491664817091078
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.6613208812892817; val_accuracy: 0.8487261146496815 

The current subspace-distance is: 0.00015491664817091078 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.81; acc: 0.75
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.79; acc: 0.8
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 1.0; acc: 0.7
Batch: 240; loss: 0.68; acc: 0.86
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.89; acc: 0.75
Batch: 320; loss: 0.66; acc: 0.86
Batch: 340; loss: 0.97; acc: 0.73
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.72; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.89
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.81; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.79; acc: 0.81
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.85; acc: 0.72
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.89; acc: 0.78
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.6; acc: 0.89
Batch: 780; loss: 0.73; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.0001689316559350118
0.00016207157750613987
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.6516421725795527; val_accuracy: 0.8492237261146497 

The current subspace-distance is: 0.00016207157750613987 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.78; acc: 0.78
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.91; acc: 0.73
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.84; acc: 0.78
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.76; acc: 0.78
Batch: 580; loss: 0.74; acc: 0.86
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.84
Batch: 640; loss: 0.74; acc: 0.83
Batch: 660; loss: 0.69; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.0001687311741989106
0.00016353829414583743
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.91
Val Epoch over. val_loss: 0.6512261908145467; val_accuracy: 0.8515127388535032 

The current subspace-distance is: 0.00016353829414583743 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.72; acc: 0.8
Batch: 100; loss: 0.85; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.83; acc: 0.77
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.73; acc: 0.77
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.78
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.79; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.91; acc: 0.78
Batch: 540; loss: 0.75; acc: 0.77
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.56; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.77; acc: 0.81
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.81
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00016987805429380387
0.00016352359671145678
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.89
Val Epoch over. val_loss: 0.6426908015065892; val_accuracy: 0.8517117834394905 

The current subspace-distance is: 0.00016352359671145678 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.8; acc: 0.81
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.75; acc: 0.78
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.76; acc: 0.75
Batch: 380; loss: 0.5; acc: 0.97
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.77
Batch: 500; loss: 0.69; acc: 0.84
Batch: 520; loss: 0.73; acc: 0.86
Batch: 540; loss: 0.83; acc: 0.78
Batch: 560; loss: 0.75; acc: 0.78
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.76; acc: 0.84
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.62; acc: 0.88
Batch: 680; loss: 0.69; acc: 0.84
Batch: 700; loss: 0.79; acc: 0.77
Batch: 720; loss: 0.86; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.86
Batch: 760; loss: 0.8; acc: 0.78
Batch: 780; loss: 0.8; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00017133682558778673
0.00016198850062210113
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.49; acc: 0.89
Val Epoch over. val_loss: 0.6411562916959167; val_accuracy: 0.8503184713375797 

The current subspace-distance is: 0.00016198850062210113 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.79; acc: 0.73
Batch: 260; loss: 0.79; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.74; acc: 0.88
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.86; acc: 0.7
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.72; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.75
Batch: 500; loss: 0.63; acc: 0.78
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.67; acc: 0.84
Batch: 600; loss: 0.75; acc: 0.78
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.82; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00017281415057368577
0.0001670525671215728
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.6353860548727072; val_accuracy: 0.8517117834394905 

The current subspace-distance is: 0.0001670525671215728 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.8; acc: 0.77
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.85; acc: 0.77
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.76; acc: 0.83
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.73; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.84; acc: 0.83
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.73
Batch: 640; loss: 0.58; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.91
Batch: 680; loss: 0.89; acc: 0.7
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.76; acc: 0.81
Batch: 740; loss: 0.68; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.75
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00017587126058060676
0.00016895693261176348
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.91
Val Epoch over. val_loss: 0.6317361564772903; val_accuracy: 0.8520103503184714 

The current subspace-distance is: 0.00016895693261176348 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.91
Batch: 200; loss: 0.81; acc: 0.81
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.91
Batch: 280; loss: 0.87; acc: 0.73
Batch: 300; loss: 0.83; acc: 0.73
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.7; acc: 0.78
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.89
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.78; acc: 0.81
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.72; acc: 0.75
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.89
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.7; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.0001765022607287392
0.00017010411829687655
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.89
Val Epoch over. val_loss: 0.6237577072754028; val_accuracy: 0.850218949044586 

The current subspace-distance is: 0.00017010411829687655 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.77; acc: 0.73
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.83; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.8
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.76; acc: 0.75
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.92
Batch: 420; loss: 0.69; acc: 0.86
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.71; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.88
Batch: 520; loss: 0.87; acc: 0.75
Batch: 540; loss: 0.79; acc: 0.83
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.88
Batch: 640; loss: 0.78; acc: 0.75
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00017753730935510248
0.00016886669618543237
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6174208787596149; val_accuracy: 0.8507165605095541 

The current subspace-distance is: 0.00016886669618543237 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.8
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.87; acc: 0.73
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.71; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.84
Batch: 380; loss: 0.76; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 0.73; acc: 0.81
Batch: 560; loss: 0.76; acc: 0.77
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.58; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.75; acc: 0.77
Batch: 660; loss: 0.71; acc: 0.78
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.0001815735304262489
0.0001722821471048519
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.616079289252591; val_accuracy: 0.8541003184713376 

The current subspace-distance is: 0.0001722821471048519 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.81; acc: 0.8
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.69
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.74; acc: 0.8
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.85; acc: 0.75
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.88
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.75; acc: 0.8
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.00018377805827185512
0.00017535316874273121
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6193416931067303; val_accuracy: 0.853702229299363 

The current subspace-distance is: 0.00017535316874273121 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.92
Batch: 160; loss: 0.81; acc: 0.78
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.78; acc: 0.7
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.73; acc: 0.8
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.91
Batch: 460; loss: 0.83; acc: 0.75
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.54; acc: 0.91
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.8; acc: 0.75
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.59; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.86
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.78; acc: 0.78
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.0001840736367739737
0.00017562134598847479
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6081249146324814; val_accuracy: 0.8561902866242038 

The current subspace-distance is: 0.00017562134598847479 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.6; acc: 0.86
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.92
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.8; acc: 0.78
Batch: 400; loss: 0.67; acc: 0.88
Batch: 420; loss: 0.8; acc: 0.78
Batch: 440; loss: 0.55; acc: 0.92
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.7; acc: 0.78
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.6; acc: 0.91
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.76; acc: 0.86
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.77; acc: 0.77
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.0001855142618296668
0.00017777588800527155
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6151457007523555; val_accuracy: 0.8546974522292994 

The current subspace-distance is: 0.00017777588800527155 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.77; acc: 0.75
Batch: 180; loss: 0.88; acc: 0.8
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.77; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.8
Batch: 260; loss: 0.69; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.71; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.78
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.78; acc: 0.78
Batch: 380; loss: 0.51; acc: 0.94
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.72; acc: 0.75
Batch: 480; loss: 0.72; acc: 0.83
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.86
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00018541414465289563
0.00017861872038338333
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.91
Val Epoch over. val_loss: 0.6109763281360553; val_accuracy: 0.8557921974522293 

The current subspace-distance is: 0.00017861872038338333 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.78; acc: 0.78
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.79; acc: 0.73
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.62; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.68; acc: 0.8
Batch: 620; loss: 0.62; acc: 0.88
Batch: 640; loss: 0.71; acc: 0.8
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.77; acc: 0.72
Batch: 720; loss: 0.63; acc: 0.91
Batch: 740; loss: 0.73; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.68; acc: 0.8
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00018280657241120934
0.00017574804951436818
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.91
Val Epoch over. val_loss: 0.6017879653888144; val_accuracy: 0.8564888535031847 

The current subspace-distance is: 0.00017574804951436818 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.62; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.63; acc: 0.84
Batch: 340; loss: 0.78; acc: 0.73
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.88
Batch: 640; loss: 0.83; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00018522041500546038
0.00017672876128926873
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.91
Val Epoch over. val_loss: 0.6053749360855977; val_accuracy: 0.8546974522292994 

The current subspace-distance is: 0.00017672876128926873 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.78; acc: 0.77
Batch: 180; loss: 0.8; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.89
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.73; acc: 0.75
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.69; acc: 0.84
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.74; acc: 0.81
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.76; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.73; acc: 0.8
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00018182607891503721
0.0001739893777994439
Batch: 0; loss: 0.68; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.5997938462503397; val_accuracy: 0.8567874203821656 

The current subspace-distance is: 0.0001739893777994439 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.78; acc: 0.8
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.68; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.85; acc: 0.73
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.61; acc: 0.91
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.68; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.64; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.71; acc: 0.8
Batch: 760; loss: 0.71; acc: 0.8
Batch: 780; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00018486269982531667
0.0001764531625667587
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6031782258847717; val_accuracy: 0.8571855095541401 

The current subspace-distance is: 0.0001764531625667587 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.71; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.62; acc: 0.88
Batch: 240; loss: 0.71; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.73; acc: 0.88
Batch: 400; loss: 0.51; acc: 0.94
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.86; acc: 0.7
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.82; acc: 0.81
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.92
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00018738466314971447
0.00018252356676384807
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6019137871872847; val_accuracy: 0.8579816878980892 

The current subspace-distance is: 0.00018252356676384807 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.61; acc: 0.83
Batch: 200; loss: 0.58; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.72; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.79; acc: 0.81
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.91
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.76; acc: 0.75
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00018749217269942164
0.00018077065760735422
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.6012530285082046; val_accuracy: 0.8564888535031847 

The current subspace-distance is: 0.00018077065760735422 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_12_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.7743928539080627

The number of parameters is: 247624

The number of individual parameters is:

15
270
15
15
22
36960
22
22
43
105952
43
43
64
99072
64
64
4096
64
640
10
64
64

nonzero elements in E: 74287194
elements in E: 74287200
fraction nonzero: 0.9999999192323846
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.4; acc: 0.08
Batch: 20; loss: 2.15; acc: 0.33
Batch: 40; loss: 1.98; acc: 0.38
Batch: 60; loss: 1.93; acc: 0.45
Batch: 80; loss: 1.8; acc: 0.44
Batch: 100; loss: 1.66; acc: 0.61
Batch: 120; loss: 1.63; acc: 0.64
Batch: 140; loss: 1.62; acc: 0.59
Batch: 160; loss: 1.54; acc: 0.69
Batch: 180; loss: 1.54; acc: 0.7
Batch: 200; loss: 1.46; acc: 0.77
Batch: 220; loss: 1.48; acc: 0.75
Batch: 240; loss: 1.56; acc: 0.7
Batch: 260; loss: 1.4; acc: 0.8
Batch: 280; loss: 1.52; acc: 0.67
Batch: 300; loss: 1.44; acc: 0.7
Batch: 320; loss: 1.28; acc: 0.78
Batch: 340; loss: 1.44; acc: 0.72
Batch: 360; loss: 1.44; acc: 0.73
Batch: 380; loss: 1.32; acc: 0.75
Batch: 400; loss: 1.35; acc: 0.78
Batch: 420; loss: 1.3; acc: 0.77
Batch: 440; loss: 1.25; acc: 0.83
Batch: 460; loss: 1.33; acc: 0.8
Batch: 480; loss: 1.23; acc: 0.81
Batch: 500; loss: 1.16; acc: 0.92
Batch: 520; loss: 1.24; acc: 0.78
Batch: 540; loss: 1.23; acc: 0.81
Batch: 560; loss: 1.27; acc: 0.84
Batch: 580; loss: 1.24; acc: 0.78
Batch: 600; loss: 1.3; acc: 0.73
Batch: 620; loss: 1.15; acc: 0.81
Batch: 640; loss: 1.27; acc: 0.75
Batch: 660; loss: 1.2; acc: 0.84
Batch: 680; loss: 1.15; acc: 0.84
Batch: 700; loss: 1.24; acc: 0.75
Batch: 720; loss: 1.19; acc: 0.77
Batch: 740; loss: 1.12; acc: 0.86
Batch: 760; loss: 1.22; acc: 0.75
Batch: 780; loss: 1.12; acc: 0.78
Train Epoch over. train_loss: 1.44; train_accuracy: 0.7 

5.937513924436644e-05
5.400952068157494e-05
Batch: 0; loss: 1.19; acc: 0.78
Batch: 20; loss: 1.26; acc: 0.69
Batch: 40; loss: 0.9; acc: 0.89
Batch: 60; loss: 1.1; acc: 0.75
Batch: 80; loss: 0.95; acc: 0.86
Batch: 100; loss: 1.08; acc: 0.91
Batch: 120; loss: 1.21; acc: 0.73
Batch: 140; loss: 0.89; acc: 0.88
Val Epoch over. val_loss: 1.0865221600623647; val_accuracy: 0.8289211783439491 

The current subspace-distance is: 5.400952068157494e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.83
Batch: 20; loss: 1.15; acc: 0.84
Batch: 40; loss: 1.02; acc: 0.88
Batch: 60; loss: 1.05; acc: 0.81
Batch: 80; loss: 1.03; acc: 0.88
Batch: 100; loss: 1.04; acc: 0.89
Batch: 120; loss: 1.02; acc: 0.92
Batch: 140; loss: 1.11; acc: 0.75
Batch: 160; loss: 0.97; acc: 0.88
Batch: 180; loss: 1.15; acc: 0.73
Batch: 200; loss: 1.04; acc: 0.84
Batch: 220; loss: 1.13; acc: 0.75
Batch: 240; loss: 0.88; acc: 0.91
Batch: 260; loss: 1.02; acc: 0.84
Batch: 280; loss: 1.0; acc: 0.83
Batch: 300; loss: 1.02; acc: 0.8
Batch: 320; loss: 1.04; acc: 0.8
Batch: 340; loss: 0.99; acc: 0.84
Batch: 360; loss: 1.04; acc: 0.84
Batch: 380; loss: 0.94; acc: 0.86
Batch: 400; loss: 0.97; acc: 0.86
Batch: 420; loss: 1.05; acc: 0.78
Batch: 440; loss: 0.92; acc: 0.86
Batch: 460; loss: 0.9; acc: 0.88
Batch: 480; loss: 1.04; acc: 0.72
Batch: 500; loss: 0.93; acc: 0.83
Batch: 520; loss: 0.89; acc: 0.84
Batch: 540; loss: 0.93; acc: 0.86
Batch: 560; loss: 0.91; acc: 0.89
Batch: 580; loss: 0.89; acc: 0.86
Batch: 600; loss: 1.02; acc: 0.72
Batch: 620; loss: 0.95; acc: 0.8
Batch: 640; loss: 1.06; acc: 0.78
Batch: 660; loss: 0.98; acc: 0.8
Batch: 680; loss: 0.98; acc: 0.81
Batch: 700; loss: 0.91; acc: 0.83
Batch: 720; loss: 0.86; acc: 0.89
Batch: 740; loss: 0.99; acc: 0.78
Batch: 760; loss: 1.04; acc: 0.77
Batch: 780; loss: 0.94; acc: 0.83
Train Epoch over. train_loss: 1.02; train_accuracy: 0.82 

8.217243885155767e-05
7.69999751355499e-05
Batch: 0; loss: 0.9; acc: 0.84
Batch: 20; loss: 1.07; acc: 0.73
Batch: 40; loss: 0.66; acc: 0.89
Batch: 60; loss: 0.94; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.92
Batch: 100; loss: 0.85; acc: 0.95
Batch: 120; loss: 1.0; acc: 0.78
Batch: 140; loss: 0.74; acc: 0.91
Val Epoch over. val_loss: 0.8674042129972178; val_accuracy: 0.8538017515923567 

The current subspace-distance is: 7.69999751355499e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.83
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 0.83; acc: 0.81
Batch: 120; loss: 0.99; acc: 0.77
Batch: 140; loss: 0.95; acc: 0.8
Batch: 160; loss: 0.88; acc: 0.84
Batch: 180; loss: 0.84; acc: 0.86
Batch: 200; loss: 0.95; acc: 0.72
Batch: 220; loss: 0.71; acc: 0.95
Batch: 240; loss: 0.91; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.88
Batch: 280; loss: 0.83; acc: 0.86
Batch: 300; loss: 0.86; acc: 0.83
Batch: 320; loss: 0.93; acc: 0.83
Batch: 340; loss: 0.88; acc: 0.86
Batch: 360; loss: 0.84; acc: 0.83
Batch: 380; loss: 0.8; acc: 0.91
Batch: 400; loss: 0.92; acc: 0.81
Batch: 420; loss: 0.81; acc: 0.86
Batch: 440; loss: 0.8; acc: 0.84
Batch: 460; loss: 0.79; acc: 0.88
Batch: 480; loss: 0.92; acc: 0.81
Batch: 500; loss: 0.84; acc: 0.83
Batch: 520; loss: 0.83; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.89
Batch: 560; loss: 0.83; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.89
Batch: 600; loss: 0.8; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.89
Batch: 640; loss: 0.77; acc: 0.89
Batch: 660; loss: 0.79; acc: 0.86
Batch: 680; loss: 0.82; acc: 0.84
Batch: 700; loss: 0.82; acc: 0.81
Batch: 720; loss: 0.68; acc: 0.92
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.78; acc: 0.84
Batch: 780; loss: 0.88; acc: 0.77
Train Epoch over. train_loss: 0.85; train_accuracy: 0.84 

0.00010014267172664404
9.498488361714408e-05
Batch: 0; loss: 0.75; acc: 0.86
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.77; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.94
Batch: 100; loss: 0.72; acc: 0.94
Batch: 120; loss: 0.88; acc: 0.81
Batch: 140; loss: 0.66; acc: 0.89
Val Epoch over. val_loss: 0.7372262914469287; val_accuracy: 0.8665406050955414 

The current subspace-distance is: 9.498488361714408e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.92
Batch: 20; loss: 0.78; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.91
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 0.81; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.83
Batch: 140; loss: 0.78; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.95
Batch: 180; loss: 1.05; acc: 0.73
Batch: 200; loss: 0.76; acc: 0.81
Batch: 220; loss: 0.68; acc: 0.92
Batch: 240; loss: 0.82; acc: 0.84
Batch: 260; loss: 0.81; acc: 0.8
Batch: 280; loss: 0.77; acc: 0.86
Batch: 300; loss: 0.78; acc: 0.86
Batch: 320; loss: 0.77; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.86
Batch: 360; loss: 0.74; acc: 0.86
Batch: 380; loss: 0.76; acc: 0.89
Batch: 400; loss: 0.68; acc: 0.88
Batch: 420; loss: 0.9; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.84; acc: 0.81
Batch: 480; loss: 0.62; acc: 0.91
Batch: 500; loss: 0.67; acc: 0.89
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.62; acc: 0.89
Batch: 560; loss: 0.99; acc: 0.78
Batch: 580; loss: 0.75; acc: 0.86
Batch: 600; loss: 0.71; acc: 0.88
Batch: 620; loss: 0.67; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.91
Batch: 660; loss: 0.72; acc: 0.89
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.94
Batch: 740; loss: 0.72; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.76; acc: 0.84
Train Epoch over. train_loss: 0.75; train_accuracy: 0.85 

0.0001147836956079118
0.00010915377788478509
Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.97
Batch: 100; loss: 0.63; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.88
Val Epoch over. val_loss: 0.6472297568989408; val_accuracy: 0.8788813694267515 

The current subspace-distance is: 0.00010915377788478509 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.8; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 0.62; acc: 0.94
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.91
Batch: 300; loss: 0.67; acc: 0.88
Batch: 320; loss: 0.79; acc: 0.88
Batch: 340; loss: 0.76; acc: 0.81
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 0.63; acc: 0.91
Batch: 400; loss: 0.81; acc: 0.83
Batch: 420; loss: 0.69; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.91
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.75; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.95
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.65; acc: 0.91
Train Epoch over. train_loss: 0.69; train_accuracy: 0.86 

0.0001251633366337046
0.00011855911725433543
Batch: 0; loss: 0.6; acc: 0.92
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.97
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.88
Val Epoch over. val_loss: 0.5995161296076076; val_accuracy: 0.8827627388535032 

The current subspace-distance is: 0.00011855911725433543 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.95
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.67; acc: 0.86
Batch: 260; loss: 0.59; acc: 0.91
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.61; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.89
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.78; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.92
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.8; acc: 0.77
Batch: 620; loss: 0.65; acc: 0.86
Batch: 640; loss: 0.68; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.92
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.86 

0.00013524785754270852
0.0001285477919736877
Batch: 0; loss: 0.55; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.97
Batch: 100; loss: 0.55; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.89
Val Epoch over. val_loss: 0.5578690691358724; val_accuracy: 0.8840565286624203 

The current subspace-distance is: 0.0001285477919736877 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.91
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.58; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.61; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.84
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.92
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.61; train_accuracy: 0.87 

0.00014640494191553444
0.00013945752289146185
Batch: 0; loss: 0.53; acc: 0.94
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.97
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.5421603401755072; val_accuracy: 0.8892316878980892 

The current subspace-distance is: 0.00013945752289146185 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.66; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.44; acc: 0.95
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.94
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.62; acc: 0.88
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.77; acc: 0.77
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.58; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.95
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.00015446783800143749
0.00014700338942930102
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.95
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.91
Val Epoch over. val_loss: 0.5044802374141232; val_accuracy: 0.895203025477707 

The current subspace-distance is: 0.00014700338942930102 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.0001618840469745919
0.000154772205860354
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.92
Val Epoch over. val_loss: 0.4879218021965331; val_accuracy: 0.9000796178343949 

The current subspace-distance is: 0.000154772205860354 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.94
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.7; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00017090323672164232
0.0001628557511139661
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.92
Val Epoch over. val_loss: 0.44933514382429185; val_accuracy: 0.9040605095541401 

The current subspace-distance is: 0.0001628557511139661 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.62; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.92
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.0001724507164908573
0.00016385498747695237
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.45073730085685754; val_accuracy: 0.9041600318471338 

The current subspace-distance is: 0.00016385498747695237 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.94
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.95
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.95
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.95
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.97
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.0001761504972819239
0.0001690054195933044
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.44927244932408544; val_accuracy: 0.9039609872611465 

The current subspace-distance is: 0.0001690054195933044 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.98
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001770642847986892
0.0001704225578578189
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.44147245718795025; val_accuracy: 0.9050557324840764 

The current subspace-distance is: 0.0001704225578578189 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.78
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.95
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001809785608202219
0.00017210538499057293
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.4389339133053069; val_accuracy: 0.90515525477707 

The current subspace-distance is: 0.00017210538499057293 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.65; acc: 0.75
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0001820391626097262
0.00017342870705761015
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.4352841245330823; val_accuracy: 0.90625 

The current subspace-distance is: 0.00017342870705761015 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.92
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.0001826934894779697
0.00017513205239083618
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.4276126435228214; val_accuracy: 0.9070461783439491 

The current subspace-distance is: 0.00017513205239083618 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.4; acc: 0.95
Batch: 260; loss: 0.53; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.73; acc: 0.81
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.95
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00018430431373417377
0.0001752504031173885
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.42281623260610424; val_accuracy: 0.9085390127388535 

The current subspace-distance is: 0.0001752504031173885 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.97
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.57; acc: 0.81
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00018611832638271153
0.0001777551369741559
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4164512743046329; val_accuracy: 0.9072452229299363 

The current subspace-distance is: 0.0001777551369741559 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.97
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.95
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.68; acc: 0.8
Batch: 440; loss: 0.35; acc: 0.97
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00018894251843448728
0.0001811149704735726
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.42073860669591623; val_accuracy: 0.9077428343949044 

The current subspace-distance is: 0.0001811149704735726 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.95
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.27; acc: 0.98
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.98
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.61; acc: 0.78
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.95
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0001918999623740092
0.00018438216648064554
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4096578185915188; val_accuracy: 0.9087380573248408 

The current subspace-distance is: 0.00018438216648064554 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.8
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.97
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.000192915991647169
0.00018553399422671646
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.40909060237893635; val_accuracy: 0.9098328025477707 

The current subspace-distance is: 0.00018553399422671646 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.98
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00019323798187542707
0.00018739204097073525
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.4146290855233077; val_accuracy: 0.9085390127388535 

The current subspace-distance is: 0.00018739204097073525 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.97
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0001923974195960909
0.00018260223441757262
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.404059245328235; val_accuracy: 0.9113256369426752 

The current subspace-distance is: 0.00018260223441757262 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00019336277910042554
0.00018390864715911448
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.26; acc: 1.0
Val Epoch over. val_loss: 0.4105220829984944; val_accuracy: 0.9071457006369427 

The current subspace-distance is: 0.00018390864715911448 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0001927555276779458
0.0001842528727138415
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.41309723609192356; val_accuracy: 0.908140923566879 

The current subspace-distance is: 0.0001842528727138415 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.52; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.43; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.95
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00019353411335032433
0.00018534332048147917
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.98
Val Epoch over. val_loss: 0.40731419860177737; val_accuracy: 0.9090366242038217 

The current subspace-distance is: 0.00018534332048147917 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.98
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.78
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.35; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00019618560327216983
0.00018790550529956818
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4029338093130452; val_accuracy: 0.9083399681528662 

The current subspace-distance is: 0.00018790550529956818 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.97
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.33; acc: 1.0
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.97
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00019737656111828983
0.00018803936836775392
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4064852378930256; val_accuracy: 0.910031847133758 

The current subspace-distance is: 0.00018803936836775392 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00019630354654509574
0.000188603182323277
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4034562851213346; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.000188603182323277 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.43; acc: 0.83
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.62; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00019574060570448637
0.00018760476086754352
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4005958421784601; val_accuracy: 0.908937101910828 

The current subspace-distance is: 0.00018760476086754352 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_12_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.7743928539080627

The number of parameters is: 247624

The number of individual parameters is:

15
270
15
15
22
36960
22
22
43
105952
43
43
64
99072
64
64
4096
64
640
10
64
64

nonzero elements in E: 99049588
elements in E: 99049600
fraction nonzero: 0.9999998788485769
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.16
Batch: 20; loss: 1.94; acc: 0.39
Batch: 40; loss: 1.87; acc: 0.48
Batch: 60; loss: 1.67; acc: 0.62
Batch: 80; loss: 1.58; acc: 0.66
Batch: 100; loss: 1.5; acc: 0.7
Batch: 120; loss: 1.46; acc: 0.72
Batch: 140; loss: 1.35; acc: 0.8
Batch: 160; loss: 1.52; acc: 0.67
Batch: 180; loss: 1.38; acc: 0.73
Batch: 200; loss: 1.23; acc: 0.83
Batch: 220; loss: 1.3; acc: 0.8
Batch: 240; loss: 1.36; acc: 0.73
Batch: 260; loss: 1.28; acc: 0.78
Batch: 280; loss: 1.23; acc: 0.8
Batch: 300; loss: 1.2; acc: 0.8
Batch: 320; loss: 1.21; acc: 0.78
Batch: 340; loss: 1.15; acc: 0.78
Batch: 360; loss: 1.19; acc: 0.77
Batch: 380; loss: 1.13; acc: 0.81
Batch: 400; loss: 1.16; acc: 0.73
Batch: 420; loss: 1.09; acc: 0.84
Batch: 440; loss: 1.0; acc: 0.86
Batch: 460; loss: 1.14; acc: 0.8
Batch: 480; loss: 1.1; acc: 0.81
Batch: 500; loss: 0.97; acc: 0.89
Batch: 520; loss: 0.92; acc: 0.89
Batch: 540; loss: 1.04; acc: 0.8
Batch: 560; loss: 1.18; acc: 0.73
Batch: 580; loss: 0.9; acc: 0.91
Batch: 600; loss: 1.1; acc: 0.81
Batch: 620; loss: 0.98; acc: 0.81
Batch: 640; loss: 0.96; acc: 0.81
Batch: 660; loss: 0.84; acc: 0.92
Batch: 680; loss: 0.88; acc: 0.91
Batch: 700; loss: 0.91; acc: 0.84
Batch: 720; loss: 0.89; acc: 0.89
Batch: 740; loss: 0.95; acc: 0.86
Batch: 760; loss: 0.84; acc: 0.94
Batch: 780; loss: 0.98; acc: 0.83
Train Epoch over. train_loss: 1.21; train_accuracy: 0.77 

2.502853749319911e-05
7.946148798509967e-06
Batch: 0; loss: 0.92; acc: 0.88
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 0.61; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.92
Batch: 100; loss: 0.83; acc: 0.91
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.7; acc: 0.94
Val Epoch over. val_loss: 0.840416679716414; val_accuracy: 0.8609673566878981 

The current subspace-distance is: 7.946148798509967e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.83
Batch: 20; loss: 0.74; acc: 0.92
Batch: 40; loss: 0.86; acc: 0.86
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 0.95; acc: 0.78
Batch: 100; loss: 0.82; acc: 0.91
Batch: 120; loss: 0.94; acc: 0.81
Batch: 140; loss: 0.84; acc: 0.84
Batch: 160; loss: 0.94; acc: 0.8
Batch: 180; loss: 0.89; acc: 0.8
Batch: 200; loss: 0.99; acc: 0.81
Batch: 220; loss: 0.94; acc: 0.81
Batch: 240; loss: 0.77; acc: 0.88
Batch: 260; loss: 0.98; acc: 0.8
Batch: 280; loss: 0.79; acc: 0.91
Batch: 300; loss: 0.81; acc: 0.89
Batch: 320; loss: 0.89; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.95
Batch: 360; loss: 0.99; acc: 0.81
Batch: 380; loss: 0.85; acc: 0.84
Batch: 400; loss: 0.71; acc: 0.91
Batch: 420; loss: 0.76; acc: 0.89
Batch: 440; loss: 0.89; acc: 0.8
Batch: 460; loss: 0.88; acc: 0.8
Batch: 480; loss: 0.79; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.89
Batch: 520; loss: 0.69; acc: 0.92
Batch: 540; loss: 0.68; acc: 0.88
Batch: 560; loss: 0.68; acc: 0.92
Batch: 580; loss: 0.72; acc: 0.84
Batch: 600; loss: 0.85; acc: 0.81
Batch: 620; loss: 0.94; acc: 0.78
Batch: 640; loss: 0.8; acc: 0.86
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.74; acc: 0.88
Batch: 700; loss: 0.88; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.88
Batch: 740; loss: 0.77; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.98
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.81; train_accuracy: 0.86 

3.018612551386468e-05
1.0728575034590904e-05
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.98
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.94
Val Epoch over. val_loss: 0.6820014717092939; val_accuracy: 0.8777866242038217 

The current subspace-distance is: 1.0728575034590904e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.84
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.81
Batch: 80; loss: 0.8; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.81; acc: 0.86
Batch: 140; loss: 0.87; acc: 0.81
Batch: 160; loss: 0.74; acc: 0.84
Batch: 180; loss: 0.77; acc: 0.83
Batch: 200; loss: 0.74; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.94
Batch: 240; loss: 0.74; acc: 0.86
Batch: 260; loss: 0.85; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.84
Batch: 300; loss: 0.73; acc: 0.84
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.67; acc: 0.92
Batch: 360; loss: 0.67; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.92
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.61; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.92
Batch: 480; loss: 0.71; acc: 0.89
Batch: 500; loss: 0.74; acc: 0.86
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.61; acc: 0.91
Batch: 560; loss: 0.84; acc: 0.77
Batch: 580; loss: 0.6; acc: 0.91
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.73; acc: 0.81
Batch: 680; loss: 0.74; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.8
Batch: 720; loss: 0.64; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.72; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.7; train_accuracy: 0.87 

3.52199531334918e-05
1.4568880033039022e-05
Batch: 0; loss: 0.72; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.98
Batch: 100; loss: 0.59; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.5832588258822253; val_accuracy: 0.8955015923566879 

The current subspace-distance is: 1.4568880033039022e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.67; acc: 0.86
Batch: 160; loss: 0.71; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.94
Batch: 240; loss: 0.58; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.88
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.73; acc: 0.84
Batch: 360; loss: 0.59; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.95
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.73; acc: 0.78
Batch: 500; loss: 0.51; acc: 0.91
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.69; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.97
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.91
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.61; train_accuracy: 0.88 

3.849309723591432e-05
1.5530158634646796e-05
Batch: 0; loss: 0.63; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.34; acc: 1.0
Val Epoch over. val_loss: 0.5160093607416578; val_accuracy: 0.9044585987261147 

The current subspace-distance is: 1.5530158634646796e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.92
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.91
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.39; acc: 0.97
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.95
Batch: 280; loss: 0.55; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.94
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.92
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.95
Batch: 620; loss: 0.69; acc: 0.77
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.94
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.89 

4.191072366666049e-05
1.7618012861930765e-05
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.29; acc: 1.0
Val Epoch over. val_loss: 0.464375849077656; val_accuracy: 0.910828025477707 

The current subspace-distance is: 1.7618012861930765e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.97
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.77
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.95
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.9 

4.488886406761594e-05
1.8705284674069844e-05
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.25; acc: 1.0
Val Epoch over. val_loss: 0.4308001026036633; val_accuracy: 0.9147093949044586 

The current subspace-distance is: 1.8705284674069844e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.75; acc: 0.77
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.98
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.95
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.77
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

4.825172072742134e-05
2.101537938870024e-05
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.21; acc: 1.0
Val Epoch over. val_loss: 0.3990263393160644; val_accuracy: 0.9176950636942676 

The current subspace-distance is: 2.101537938870024e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.95
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.97
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.97
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.026214785175398e-05
2.0834208044107072e-05
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.3757988837114565; val_accuracy: 0.92078025477707 

The current subspace-distance is: 2.0834208044107072e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.81
Batch: 300; loss: 0.55; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.95
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

5.3315179684432223e-05
2.4572280381107703e-05
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 1.0
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.3520516259180512; val_accuracy: 0.9228702229299363 

The current subspace-distance is: 2.4572280381107703e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.39919565198943e-05
2.224580748588778e-05
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.17; acc: 1.0
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.3483837490818303; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 2.224580748588778e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.94
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.98
Batch: 620; loss: 0.4; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.65; acc: 0.8
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.59699947189074e-05
2.2767006157664582e-05
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 1.0
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3387674614787102; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 2.2767006157664582e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.81
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.97
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.62383393116761e-05
2.4009557819226757e-05
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.17; acc: 1.0
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.33802786127776857; val_accuracy: 0.9261544585987261 

The current subspace-distance is: 2.4009557819226757e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.98
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.683379640686326e-05
2.306019450770691e-05
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3312504786974306; val_accuracy: 0.9272492038216561 

The current subspace-distance is: 2.306019450770691e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.733117359341122e-05
2.354321622988209e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.15; acc: 1.0
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.326994607497932; val_accuracy: 0.9268511146496815 

The current subspace-distance is: 2.354321622988209e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.98
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.7704251958057284e-05
2.3827065888326615e-05
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.14; acc: 1.0
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.32122378990908335; val_accuracy: 0.9279458598726115 

The current subspace-distance is: 2.3827065888326615e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.98
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.97
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.98
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.7686349464347586e-05
2.383794890192803e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3206981153814656; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.383794890192803e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.98
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.98
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.932986459811218e-05
2.4445824237773195e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.14; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3140556392775979; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.4445824237773195e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.98
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.927242818870582e-05
2.4861348720151e-05
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.14; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.31736387820190687; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.4861348720151e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.98
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.98
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.9569705626927316e-05
2.4314569600392133e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.13; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.30978360112495484; val_accuracy: 0.930234872611465 

The current subspace-distance is: 2.4314569600392133e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.98
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.0309601394692436e-05
2.6150379198952578e-05
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.30742818933383675; val_accuracy: 0.9301353503184714 

The current subspace-distance is: 2.6150379198952578e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.97
Batch: 580; loss: 0.44; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.9937097830697894e-05
2.460850191710051e-05
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.31354747665156224; val_accuracy: 0.9274482484076433 

The current subspace-distance is: 2.460850191710051e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.23; acc: 1.0
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.065679917810485e-05
2.633929398143664e-05
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.13; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.30333777948929247; val_accuracy: 0.9304339171974523 

The current subspace-distance is: 2.633929398143664e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.09182970947586e-05
2.5854818886728026e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.31128943037645074; val_accuracy: 0.9287420382165605 

The current subspace-distance is: 2.5854818886728026e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.98
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.085943823563866e-05
2.524328556319233e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.30874551519466814; val_accuracy: 0.9296377388535032 

The current subspace-distance is: 2.524328556319233e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.98
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.088707596063614e-05
2.69641986960778e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3138563046884385; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 2.69641986960778e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.98
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.078482329030521e-05
2.3844251700211316e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3058629198248979; val_accuracy: 0.9290406050955414 

The current subspace-distance is: 2.3844251700211316e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.98
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.101197868702002e-05
2.4858944016159512e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.13; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3035247165021623; val_accuracy: 0.9303343949044586 

The current subspace-distance is: 2.4858944016159512e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.22; acc: 0.98
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.149538239696994e-05
2.474424945830833e-05
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.13; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3039428751654686; val_accuracy: 0.9286425159235668 

The current subspace-distance is: 2.474424945830833e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.133093847893178e-05
2.5446401195949875e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3037397356549646; val_accuracy: 0.9288415605095541 

The current subspace-distance is: 2.5446401195949875e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.186156679177657e-05
2.715525624807924e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3024110062296983; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.715525624807924e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_12_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.7743928539080627

The number of parameters is: 247624

The number of individual parameters is:

15
270
15
15
22
36960
22
22
43
105952
43
43
64
99072
64
64
4096
64
640
10
64
64

nonzero elements in E: 123811988
elements in E: 123812000
fraction nonzero: 0.9999999030788616
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.09
Batch: 20; loss: 2.0; acc: 0.28
Batch: 40; loss: 1.66; acc: 0.66
Batch: 60; loss: 1.72; acc: 0.55
Batch: 80; loss: 1.59; acc: 0.61
Batch: 100; loss: 1.5; acc: 0.73
Batch: 120; loss: 1.43; acc: 0.69
Batch: 140; loss: 1.34; acc: 0.8
Batch: 160; loss: 1.33; acc: 0.73
Batch: 180; loss: 1.38; acc: 0.72
Batch: 200; loss: 1.17; acc: 0.86
Batch: 220; loss: 1.15; acc: 0.92
Batch: 240; loss: 1.18; acc: 0.86
Batch: 260; loss: 1.12; acc: 0.86
Batch: 280; loss: 1.14; acc: 0.84
Batch: 300; loss: 1.08; acc: 0.86
Batch: 320; loss: 1.09; acc: 0.83
Batch: 340; loss: 1.04; acc: 0.84
Batch: 360; loss: 1.06; acc: 0.84
Batch: 380; loss: 1.16; acc: 0.77
Batch: 400; loss: 1.07; acc: 0.83
Batch: 420; loss: 0.98; acc: 0.89
Batch: 440; loss: 0.89; acc: 0.95
Batch: 460; loss: 1.11; acc: 0.81
Batch: 480; loss: 0.97; acc: 0.88
Batch: 500; loss: 0.99; acc: 0.78
Batch: 520; loss: 1.03; acc: 0.77
Batch: 540; loss: 0.9; acc: 0.91
Batch: 560; loss: 0.97; acc: 0.83
Batch: 580; loss: 1.03; acc: 0.83
Batch: 600; loss: 1.01; acc: 0.81
Batch: 620; loss: 0.89; acc: 0.84
Batch: 640; loss: 0.87; acc: 0.86
Batch: 660; loss: 1.01; acc: 0.81
Batch: 680; loss: 0.74; acc: 0.91
Batch: 700; loss: 0.92; acc: 0.84
Batch: 720; loss: 0.84; acc: 0.89
Batch: 740; loss: 0.92; acc: 0.86
Batch: 760; loss: 0.97; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.84
Train Epoch over. train_loss: 1.15; train_accuracy: 0.79 

2.4893750378396362e-05
9.00614759302698e-06
Batch: 0; loss: 0.8; acc: 0.94
Batch: 20; loss: 0.97; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.98
Batch: 60; loss: 0.78; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.94
Batch: 100; loss: 0.78; acc: 0.92
Batch: 120; loss: 0.95; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.92
Val Epoch over. val_loss: 0.7950165081935324; val_accuracy: 0.8857484076433121 

The current subspace-distance is: 9.00614759302698e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.83
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 0.85; acc: 0.89
Batch: 60; loss: 0.87; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.84
Batch: 100; loss: 0.84; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.81
Batch: 140; loss: 0.8; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.88
Batch: 180; loss: 0.72; acc: 0.89
Batch: 200; loss: 0.82; acc: 0.81
Batch: 220; loss: 0.79; acc: 0.88
Batch: 240; loss: 0.82; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.88
Batch: 280; loss: 0.74; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.89
Batch: 320; loss: 0.75; acc: 0.89
Batch: 340; loss: 0.75; acc: 0.89
Batch: 360; loss: 0.72; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.88
Batch: 400; loss: 0.75; acc: 0.89
Batch: 420; loss: 0.79; acc: 0.89
Batch: 440; loss: 0.71; acc: 0.89
Batch: 460; loss: 0.77; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.88
Batch: 500; loss: 0.73; acc: 0.89
Batch: 520; loss: 0.78; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.91
Batch: 580; loss: 0.63; acc: 0.91
Batch: 600; loss: 0.61; acc: 0.91
Batch: 620; loss: 0.66; acc: 0.91
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.98
Batch: 680; loss: 0.73; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.94
Batch: 720; loss: 0.62; acc: 0.91
Batch: 740; loss: 0.64; acc: 0.92
Batch: 760; loss: 0.74; acc: 0.88
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.88 

3.123537317151204e-05
1.2687052731052972e-05
Batch: 0; loss: 0.65; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.86
Batch: 40; loss: 0.36; acc: 1.0
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.95
Batch: 100; loss: 0.59; acc: 0.95
Batch: 120; loss: 0.72; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.97
Val Epoch over. val_loss: 0.6001532913013629; val_accuracy: 0.9028662420382165 

The current subspace-distance is: 1.2687052731052972e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.86
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.91
Batch: 80; loss: 0.72; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.95
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.91
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.91
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.77; acc: 0.84
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.91
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.97
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.95
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.79; acc: 0.77
Batch: 680; loss: 0.49; acc: 0.95
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.95
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.91
Train Epoch over. train_loss: 0.61; train_accuracy: 0.89 

3.481411113170907e-05
1.4605960132030305e-05
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.97
Val Epoch over. val_loss: 0.5070536265707319; val_accuracy: 0.9145103503184714 

The current subspace-distance is: 1.4605960132030305e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.69; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.97
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.95
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.95
Batch: 560; loss: 0.42; acc: 0.95
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.44; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.95
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.9 

3.962816845159978e-05
1.6524225429748185e-05
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.4372319222255877; val_accuracy: 0.9211783439490446 

The current subspace-distance is: 1.6524225429748185e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.95
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.97
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.95
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.97
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.91 

4.371511749923229e-05
1.9751374566112645e-05
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.98
Val Epoch over. val_loss: 0.388388211844833; val_accuracy: 0.9273487261146497 

The current subspace-distance is: 1.9751374566112645e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.95
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.97
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

4.642414205591194e-05
2.0700954337371513e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.36419631085198395; val_accuracy: 0.9297372611464968 

The current subspace-distance is: 2.0700954337371513e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.98
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.97
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.98
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.92 

4.847345553571358e-05
2.2010543034411967e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3282201699200709; val_accuracy: 0.9339171974522293 

The current subspace-distance is: 2.2010543034411967e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.98
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.97
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.188005889067426e-05
2.2997452106210403e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.31738502508515765; val_accuracy: 0.9324243630573248 

The current subspace-distance is: 2.2997452106210403e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.98
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.98
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.98
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.98
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.2926287025911734e-05
2.2028003513696603e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.29651966615087666; val_accuracy: 0.9362062101910829 

The current subspace-distance is: 2.2028003513696603e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.3; acc: 1.0
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.98
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.5624092055950314e-05
2.492892053851392e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.2843635615649497; val_accuracy: 0.9361066878980892 

The current subspace-distance is: 2.492892053851392e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.6556924391770735e-05
2.5053624995052814e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.2777678909100545; val_accuracy: 0.9380971337579618 

The current subspace-distance is: 2.5053624995052814e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.712144775316119e-05
2.5112121875281446e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.27529761532119884; val_accuracy: 0.9394904458598726 

The current subspace-distance is: 2.5112121875281446e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.97
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.859503289684653e-05
2.6470048396731727e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.2723578471381953; val_accuracy: 0.9387937898089171 

The current subspace-distance is: 2.6470048396731727e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.17; acc: 0.98
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.98
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.783648884971626e-05
2.504163967387285e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.27015572226351237; val_accuracy: 0.9389928343949044 

The current subspace-distance is: 2.504163967387285e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.98
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.912301639909856e-05
2.8786351322196424e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.2692598756046812; val_accuracy: 0.9397890127388535 

The current subspace-distance is: 2.8786351322196424e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.9990608860971406e-05
2.8830339942942373e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.26369485164144235; val_accuracy: 0.9403861464968153 

The current subspace-distance is: 2.8830339942942373e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.984049494145438e-05
2.6956240617437288e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.25916949906356773; val_accuracy: 0.9414808917197452 

The current subspace-distance is: 2.6956240617437288e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.131410191301256e-05
3.0059763957979158e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.25909615066021113; val_accuracy: 0.9415804140127388 

The current subspace-distance is: 3.0059763957979158e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.22; acc: 1.0
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.98
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.121044134488329e-05
2.8765263778041117e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2567196760302896; val_accuracy: 0.9410828025477707 

The current subspace-distance is: 2.8765263778041117e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.98
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.15; acc: 1.0
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.073865733924322e-05
2.8801945518353023e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.24971198784128115; val_accuracy: 0.9425756369426752 

The current subspace-distance is: 2.8801945518353023e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.17; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.15; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.176406895974651e-05
2.9486547646229155e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.25192358202425535; val_accuracy: 0.9414808917197452 

The current subspace-distance is: 2.9486547646229155e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.98
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.98
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.0653226682916284e-05
2.6807907488546334e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.25035828000800625; val_accuracy: 0.9429737261146497 

The current subspace-distance is: 2.6807907488546334e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.111781840445474e-05
2.7189797037863173e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.24956899814924616; val_accuracy: 0.9427746815286624 

The current subspace-distance is: 2.7189797037863173e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.98
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.13; acc: 1.0
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.175602902658284e-05
2.79703253909247e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2519999599191034; val_accuracy: 0.9442675159235668 

The current subspace-distance is: 2.79703253909247e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.11; acc: 1.0
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.98
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.150786794023588e-05
2.7881764253834262e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2491828860465888; val_accuracy: 0.9429737261146497 

The current subspace-distance is: 2.7881764253834262e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.166155071696267e-05
2.795452019199729e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.25103554874658585; val_accuracy: 0.9409832802547771 

The current subspace-distance is: 2.795452019199729e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.19; acc: 1.0
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.98
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.97
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.98
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.29377318546176e-05
2.9624732633237727e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.250824806369414; val_accuracy: 0.9419785031847133 

The current subspace-distance is: 2.9624732633237727e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.98
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.204160308698192e-05
2.8182788810227066e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2507938173641065; val_accuracy: 0.9425756369426752 

The current subspace-distance is: 2.8182788810227066e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.98
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.227781705092639e-05
2.8911363187944517e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.24951720195021598; val_accuracy: 0.9429737261146497 

The current subspace-distance is: 2.8911363187944517e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.18; acc: 1.0
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.245850818231702e-05
2.8520986234070733e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2443173941533277; val_accuracy: 0.9429737261146497 

The current subspace-distance is: 2.8520986234070733e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_12_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_12_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
