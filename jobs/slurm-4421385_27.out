model : table13slim
N : 11
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:52

Channel scaling factor: 1.21

The number of parameters is: 262995

The number of individual parameters is:

10
180
10
10
15
32700
15
15
30
98100
30
30
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 13149748
elements in E: 13149750
fraction nonzero: 0.9999998479058537
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.31; acc: 0.17
Batch: 20; loss: 2.28; acc: 0.11
Batch: 40; loss: 2.24; acc: 0.14
Batch: 60; loss: 2.24; acc: 0.16
Batch: 80; loss: 2.21; acc: 0.12
Batch: 100; loss: 2.23; acc: 0.17
Batch: 120; loss: 2.14; acc: 0.16
Batch: 140; loss: 2.11; acc: 0.23
Batch: 160; loss: 2.17; acc: 0.23
Batch: 180; loss: 2.15; acc: 0.27
Batch: 200; loss: 2.23; acc: 0.19
Batch: 220; loss: 2.04; acc: 0.27
Batch: 240; loss: 2.12; acc: 0.25
Batch: 260; loss: 2.11; acc: 0.34
Batch: 280; loss: 2.13; acc: 0.19
Batch: 300; loss: 2.06; acc: 0.28
Batch: 320; loss: 2.02; acc: 0.22
Batch: 340; loss: 2.03; acc: 0.28
Batch: 360; loss: 1.96; acc: 0.36
Batch: 380; loss: 1.98; acc: 0.38
Batch: 400; loss: 1.94; acc: 0.39
Batch: 420; loss: 2.01; acc: 0.34
Batch: 440; loss: 1.94; acc: 0.48
Batch: 460; loss: 1.99; acc: 0.44
Batch: 480; loss: 2.02; acc: 0.31
Batch: 500; loss: 2.01; acc: 0.31
Batch: 520; loss: 1.88; acc: 0.34
Batch: 540; loss: 2.01; acc: 0.41
Batch: 560; loss: 1.99; acc: 0.44
Batch: 580; loss: 1.94; acc: 0.44
Batch: 600; loss: 1.97; acc: 0.41
Batch: 620; loss: 2.02; acc: 0.31
Batch: 640; loss: 2.02; acc: 0.28
Batch: 660; loss: 1.97; acc: 0.36
Batch: 680; loss: 1.99; acc: 0.34
Batch: 700; loss: 1.94; acc: 0.39
Batch: 720; loss: 1.98; acc: 0.38
Batch: 740; loss: 1.99; acc: 0.28
Batch: 760; loss: 2.0; acc: 0.31
Batch: 780; loss: 1.89; acc: 0.41
Train Epoch over. train_loss: 2.06; train_accuracy: 0.29 

2.2456479200627655e-05
4.369785983726615e-06
Batch: 0; loss: 1.92; acc: 0.44
Batch: 20; loss: 1.92; acc: 0.36
Batch: 40; loss: 1.77; acc: 0.53
Batch: 60; loss: 1.95; acc: 0.31
Batch: 80; loss: 1.89; acc: 0.42
Batch: 100; loss: 1.91; acc: 0.36
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 1.83; acc: 0.48
Val Epoch over. val_loss: 1.9312271183463419; val_accuracy: 0.3830613057324841 

The current subspace-distance is: 4.369785983726615e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.01; acc: 0.3
Batch: 20; loss: 1.91; acc: 0.42
Batch: 40; loss: 1.96; acc: 0.45
Batch: 60; loss: 1.9; acc: 0.41
Batch: 80; loss: 1.98; acc: 0.34
Batch: 100; loss: 1.92; acc: 0.45
Batch: 120; loss: 1.95; acc: 0.34
Batch: 140; loss: 1.96; acc: 0.39
Batch: 160; loss: 2.1; acc: 0.22
Batch: 180; loss: 1.93; acc: 0.34
Batch: 200; loss: 1.8; acc: 0.53
Batch: 220; loss: 1.94; acc: 0.38
Batch: 240; loss: 1.89; acc: 0.33
Batch: 260; loss: 1.85; acc: 0.48
Batch: 280; loss: 1.81; acc: 0.42
Batch: 300; loss: 1.92; acc: 0.31
Batch: 320; loss: 1.9; acc: 0.41
Batch: 340; loss: 1.84; acc: 0.5
Batch: 360; loss: 1.83; acc: 0.42
Batch: 380; loss: 1.94; acc: 0.33
Batch: 400; loss: 1.86; acc: 0.47
Batch: 420; loss: 1.88; acc: 0.39
Batch: 440; loss: 1.8; acc: 0.47
Batch: 460; loss: 1.83; acc: 0.5
Batch: 480; loss: 1.89; acc: 0.42
Batch: 500; loss: 1.93; acc: 0.41
Batch: 520; loss: 1.8; acc: 0.53
Batch: 540; loss: 1.86; acc: 0.38
Batch: 560; loss: 1.91; acc: 0.42
Batch: 580; loss: 1.76; acc: 0.48
Batch: 600; loss: 1.91; acc: 0.34
Batch: 620; loss: 1.85; acc: 0.48
Batch: 640; loss: 1.89; acc: 0.36
Batch: 660; loss: 1.74; acc: 0.56
Batch: 680; loss: 1.75; acc: 0.45
Batch: 700; loss: 1.98; acc: 0.39
Batch: 720; loss: 1.83; acc: 0.45
Batch: 740; loss: 1.85; acc: 0.5
Batch: 760; loss: 1.74; acc: 0.45
Batch: 780; loss: 1.82; acc: 0.42
Train Epoch over. train_loss: 1.89; train_accuracy: 0.42 

2.564741407695692e-05
6.865839168312959e-06
Batch: 0; loss: 1.7; acc: 0.53
Batch: 20; loss: 1.82; acc: 0.5
Batch: 40; loss: 1.59; acc: 0.59
Batch: 60; loss: 1.83; acc: 0.47
Batch: 80; loss: 1.72; acc: 0.55
Batch: 100; loss: 1.79; acc: 0.5
Batch: 120; loss: 1.79; acc: 0.47
Batch: 140; loss: 1.62; acc: 0.58
Val Epoch over. val_loss: 1.7867664637838958; val_accuracy: 0.4759156050955414 

The current subspace-distance is: 6.865839168312959e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.56
Batch: 20; loss: 1.79; acc: 0.45
Batch: 40; loss: 1.81; acc: 0.45
Batch: 60; loss: 1.71; acc: 0.56
Batch: 80; loss: 1.79; acc: 0.5
Batch: 100; loss: 1.72; acc: 0.44
Batch: 120; loss: 1.74; acc: 0.47
Batch: 140; loss: 1.91; acc: 0.48
Batch: 160; loss: 1.77; acc: 0.53
Batch: 180; loss: 1.69; acc: 0.55
Batch: 200; loss: 1.79; acc: 0.44
Batch: 220; loss: 1.82; acc: 0.48
Batch: 240; loss: 1.81; acc: 0.48
Batch: 260; loss: 1.7; acc: 0.5
Batch: 280; loss: 1.71; acc: 0.47
Batch: 300; loss: 1.75; acc: 0.5
Batch: 320; loss: 1.67; acc: 0.58
Batch: 340; loss: 1.76; acc: 0.5
Batch: 360; loss: 1.7; acc: 0.56
Batch: 380; loss: 1.71; acc: 0.48
Batch: 400; loss: 1.91; acc: 0.31
Batch: 420; loss: 1.77; acc: 0.48
Batch: 440; loss: 1.68; acc: 0.45
Batch: 460; loss: 1.65; acc: 0.56
Batch: 480; loss: 1.83; acc: 0.45
Batch: 500; loss: 1.85; acc: 0.41
Batch: 520; loss: 1.73; acc: 0.47
Batch: 540; loss: 1.8; acc: 0.47
Batch: 560; loss: 1.65; acc: 0.5
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.71; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.53
Batch: 640; loss: 1.74; acc: 0.55
Batch: 660; loss: 1.71; acc: 0.5
Batch: 680; loss: 1.7; acc: 0.59
Batch: 700; loss: 1.82; acc: 0.42
Batch: 720; loss: 1.78; acc: 0.44
Batch: 740; loss: 1.73; acc: 0.53
Batch: 760; loss: 1.71; acc: 0.58
Batch: 780; loss: 1.77; acc: 0.55
Train Epoch over. train_loss: 1.75; train_accuracy: 0.5 

2.870016214728821e-05
8.381517545785755e-06
Batch: 0; loss: 1.62; acc: 0.59
Batch: 20; loss: 1.8; acc: 0.44
Batch: 40; loss: 1.45; acc: 0.7
Batch: 60; loss: 1.75; acc: 0.55
Batch: 80; loss: 1.59; acc: 0.62
Batch: 100; loss: 1.65; acc: 0.62
Batch: 120; loss: 1.71; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.67
Val Epoch over. val_loss: 1.697933409624039; val_accuracy: 0.533140923566879 

The current subspace-distance is: 8.381517545785755e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.53
Batch: 20; loss: 1.7; acc: 0.58
Batch: 40; loss: 1.8; acc: 0.45
Batch: 60; loss: 1.67; acc: 0.53
Batch: 80; loss: 1.8; acc: 0.52
Batch: 100; loss: 1.54; acc: 0.66
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.75; acc: 0.44
Batch: 160; loss: 1.7; acc: 0.53
Batch: 180; loss: 1.69; acc: 0.47
Batch: 200; loss: 1.71; acc: 0.5
Batch: 220; loss: 1.66; acc: 0.52
Batch: 240; loss: 1.69; acc: 0.5
Batch: 260; loss: 1.72; acc: 0.5
Batch: 280; loss: 1.57; acc: 0.58
Batch: 300; loss: 1.77; acc: 0.53
Batch: 320; loss: 1.66; acc: 0.56
Batch: 340; loss: 1.72; acc: 0.48
Batch: 360; loss: 1.64; acc: 0.55
Batch: 380; loss: 1.8; acc: 0.45
Batch: 400; loss: 1.56; acc: 0.66
Batch: 420; loss: 1.57; acc: 0.62
Batch: 440; loss: 1.73; acc: 0.5
Batch: 460; loss: 1.64; acc: 0.58
Batch: 480; loss: 1.63; acc: 0.58
Batch: 500; loss: 1.7; acc: 0.5
Batch: 520; loss: 1.61; acc: 0.48
Batch: 540; loss: 1.65; acc: 0.53
Batch: 560; loss: 1.75; acc: 0.5
Batch: 580; loss: 1.65; acc: 0.62
Batch: 600; loss: 1.69; acc: 0.55
Batch: 620; loss: 1.74; acc: 0.44
Batch: 640; loss: 1.72; acc: 0.5
Batch: 660; loss: 1.67; acc: 0.47
Batch: 680; loss: 1.77; acc: 0.39
Batch: 700; loss: 1.74; acc: 0.45
Batch: 720; loss: 1.59; acc: 0.55
Batch: 740; loss: 1.81; acc: 0.42
Batch: 760; loss: 1.55; acc: 0.59
Batch: 780; loss: 1.79; acc: 0.42
Train Epoch over. train_loss: 1.7; train_accuracy: 0.52 

3.0000455808476545e-05
8.311911187774967e-06
Batch: 0; loss: 1.61; acc: 0.59
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.39; acc: 0.72
Batch: 60; loss: 1.7; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.62
Batch: 100; loss: 1.58; acc: 0.72
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.52; acc: 0.62
Val Epoch over. val_loss: 1.663386780744905; val_accuracy: 0.5378184713375797 

The current subspace-distance is: 8.311911187774967e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.61
Batch: 20; loss: 1.7; acc: 0.56
Batch: 40; loss: 1.75; acc: 0.5
Batch: 60; loss: 1.75; acc: 0.41
Batch: 80; loss: 1.65; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.55
Batch: 120; loss: 1.65; acc: 0.64
Batch: 140; loss: 1.68; acc: 0.45
Batch: 160; loss: 1.71; acc: 0.47
Batch: 180; loss: 1.73; acc: 0.39
Batch: 200; loss: 1.64; acc: 0.53
Batch: 220; loss: 1.7; acc: 0.5
Batch: 240; loss: 1.65; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.62
Batch: 280; loss: 1.56; acc: 0.67
Batch: 300; loss: 1.76; acc: 0.45
Batch: 320; loss: 1.73; acc: 0.52
Batch: 340; loss: 1.67; acc: 0.53
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.61; acc: 0.55
Batch: 400; loss: 1.65; acc: 0.58
Batch: 420; loss: 1.71; acc: 0.45
Batch: 440; loss: 1.76; acc: 0.47
Batch: 460; loss: 1.75; acc: 0.44
Batch: 480; loss: 1.62; acc: 0.55
Batch: 500; loss: 1.77; acc: 0.45
Batch: 520; loss: 1.72; acc: 0.47
Batch: 540; loss: 1.7; acc: 0.5
Batch: 560; loss: 1.63; acc: 0.55
Batch: 580; loss: 1.66; acc: 0.48
Batch: 600; loss: 1.55; acc: 0.62
Batch: 620; loss: 1.58; acc: 0.56
Batch: 640; loss: 1.7; acc: 0.48
Batch: 660; loss: 1.65; acc: 0.52
Batch: 680; loss: 1.74; acc: 0.55
Batch: 700; loss: 1.58; acc: 0.5
Batch: 720; loss: 1.71; acc: 0.45
Batch: 740; loss: 1.81; acc: 0.39
Batch: 760; loss: 1.76; acc: 0.38
Batch: 780; loss: 1.75; acc: 0.45
Train Epoch over. train_loss: 1.68; train_accuracy: 0.52 

3.1495190341956913e-05
1.150425032392377e-05
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.75; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.73
Batch: 60; loss: 1.69; acc: 0.56
Batch: 80; loss: 1.49; acc: 0.69
Batch: 100; loss: 1.55; acc: 0.69
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.51; acc: 0.55
Val Epoch over. val_loss: 1.6430891402967416; val_accuracy: 0.5334394904458599 

The current subspace-distance is: 1.150425032392377e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.71; acc: 0.45
Batch: 40; loss: 1.66; acc: 0.58
Batch: 60; loss: 1.69; acc: 0.48
Batch: 80; loss: 1.76; acc: 0.44
Batch: 100; loss: 1.75; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.75; acc: 0.48
Batch: 160; loss: 1.6; acc: 0.59
Batch: 180; loss: 1.59; acc: 0.59
Batch: 200; loss: 1.74; acc: 0.38
Batch: 220; loss: 1.61; acc: 0.5
Batch: 240; loss: 1.64; acc: 0.53
Batch: 260; loss: 1.65; acc: 0.53
Batch: 280; loss: 1.58; acc: 0.58
Batch: 300; loss: 1.78; acc: 0.38
Batch: 320; loss: 1.77; acc: 0.5
Batch: 340; loss: 1.65; acc: 0.48
Batch: 360; loss: 1.72; acc: 0.48
Batch: 380; loss: 1.63; acc: 0.55
Batch: 400; loss: 1.58; acc: 0.53
Batch: 420; loss: 1.54; acc: 0.61
Batch: 440; loss: 1.72; acc: 0.44
Batch: 460; loss: 1.62; acc: 0.55
Batch: 480; loss: 1.7; acc: 0.48
Batch: 500; loss: 1.56; acc: 0.61
Batch: 520; loss: 1.68; acc: 0.52
Batch: 540; loss: 1.62; acc: 0.47
Batch: 560; loss: 1.6; acc: 0.55
Batch: 580; loss: 1.6; acc: 0.55
Batch: 600; loss: 1.67; acc: 0.44
Batch: 620; loss: 1.55; acc: 0.55
Batch: 640; loss: 1.73; acc: 0.45
Batch: 660; loss: 1.66; acc: 0.5
Batch: 680; loss: 1.65; acc: 0.5
Batch: 700; loss: 1.51; acc: 0.64
Batch: 720; loss: 1.55; acc: 0.59
Batch: 740; loss: 1.69; acc: 0.48
Batch: 760; loss: 1.69; acc: 0.45
Batch: 780; loss: 1.66; acc: 0.56
Train Epoch over. train_loss: 1.66; train_accuracy: 0.52 

3.237830969737843e-05
1.1189256838406436e-05
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.72; acc: 0.44
Batch: 40; loss: 1.37; acc: 0.7
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.47; acc: 0.67
Batch: 100; loss: 1.53; acc: 0.72
Batch: 120; loss: 1.61; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.58
Val Epoch over. val_loss: 1.6214479306700882; val_accuracy: 0.5342356687898089 

The current subspace-distance is: 1.1189256838406436e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.55
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.77; acc: 0.45
Batch: 60; loss: 1.67; acc: 0.45
Batch: 80; loss: 1.58; acc: 0.55
Batch: 100; loss: 1.77; acc: 0.42
Batch: 120; loss: 1.73; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.45
Batch: 160; loss: 1.65; acc: 0.55
Batch: 180; loss: 1.69; acc: 0.44
Batch: 200; loss: 1.61; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.47
Batch: 240; loss: 1.61; acc: 0.5
Batch: 260; loss: 1.63; acc: 0.5
Batch: 280; loss: 1.65; acc: 0.5
Batch: 300; loss: 1.65; acc: 0.5
Batch: 320; loss: 1.55; acc: 0.58
Batch: 340; loss: 1.63; acc: 0.52
Batch: 360; loss: 1.67; acc: 0.53
Batch: 380; loss: 1.6; acc: 0.61
Batch: 400; loss: 1.71; acc: 0.47
Batch: 420; loss: 1.63; acc: 0.45
Batch: 440; loss: 1.62; acc: 0.61
Batch: 460; loss: 1.85; acc: 0.42
Batch: 480; loss: 1.73; acc: 0.38
Batch: 500; loss: 1.61; acc: 0.55
Batch: 520; loss: 1.83; acc: 0.3
Batch: 540; loss: 1.7; acc: 0.44
Batch: 560; loss: 1.62; acc: 0.56
Batch: 580; loss: 1.52; acc: 0.61
Batch: 600; loss: 1.62; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.48
Batch: 640; loss: 1.84; acc: 0.38
Batch: 660; loss: 1.79; acc: 0.41
Batch: 680; loss: 1.54; acc: 0.61
Batch: 700; loss: 1.62; acc: 0.52
Batch: 720; loss: 1.64; acc: 0.56
Batch: 740; loss: 1.71; acc: 0.45
Batch: 760; loss: 1.67; acc: 0.5
Batch: 780; loss: 1.7; acc: 0.47
Train Epoch over. train_loss: 1.64; train_accuracy: 0.52 

3.39783109666314e-05
1.2854603482992388e-05
Batch: 0; loss: 1.56; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.42
Batch: 40; loss: 1.35; acc: 0.64
Batch: 60; loss: 1.63; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.5; acc: 0.64
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.53; acc: 0.52
Val Epoch over. val_loss: 1.5930022031638273; val_accuracy: 0.5291600318471338 

The current subspace-distance is: 1.2854603482992388e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.36
Batch: 20; loss: 1.56; acc: 0.5
Batch: 40; loss: 1.52; acc: 0.59
Batch: 60; loss: 1.74; acc: 0.47
Batch: 80; loss: 1.66; acc: 0.52
Batch: 100; loss: 1.71; acc: 0.42
Batch: 120; loss: 1.6; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.45
Batch: 160; loss: 1.53; acc: 0.53
Batch: 180; loss: 1.59; acc: 0.56
Batch: 200; loss: 1.61; acc: 0.59
Batch: 220; loss: 1.49; acc: 0.58
Batch: 240; loss: 1.52; acc: 0.58
Batch: 260; loss: 1.7; acc: 0.42
Batch: 280; loss: 1.64; acc: 0.41
Batch: 300; loss: 1.62; acc: 0.59
Batch: 320; loss: 1.55; acc: 0.56
Batch: 340; loss: 1.72; acc: 0.47
Batch: 360; loss: 1.66; acc: 0.5
Batch: 380; loss: 1.55; acc: 0.55
Batch: 400; loss: 1.67; acc: 0.52
Batch: 420; loss: 1.54; acc: 0.55
Batch: 440; loss: 1.74; acc: 0.42
Batch: 460; loss: 1.63; acc: 0.48
Batch: 480; loss: 1.6; acc: 0.47
Batch: 500; loss: 1.6; acc: 0.52
Batch: 520; loss: 1.72; acc: 0.44
Batch: 540; loss: 1.55; acc: 0.58
Batch: 560; loss: 1.73; acc: 0.52
Batch: 580; loss: 1.58; acc: 0.52
Batch: 600; loss: 1.64; acc: 0.42
Batch: 620; loss: 1.62; acc: 0.47
Batch: 640; loss: 1.55; acc: 0.53
Batch: 660; loss: 1.46; acc: 0.61
Batch: 680; loss: 1.73; acc: 0.47
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.49; acc: 0.61
Batch: 740; loss: 1.63; acc: 0.48
Batch: 760; loss: 1.57; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.53
Train Epoch over. train_loss: 1.62; train_accuracy: 0.51 

3.489384107524529e-05
1.0289395504514687e-05
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.36; acc: 0.66
Batch: 60; loss: 1.61; acc: 0.56
Batch: 80; loss: 1.43; acc: 0.61
Batch: 100; loss: 1.51; acc: 0.66
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.53; acc: 0.55
Val Epoch over. val_loss: 1.5811762817346366; val_accuracy: 0.535031847133758 

The current subspace-distance is: 1.0289395504514687e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.55; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.48
Batch: 40; loss: 1.6; acc: 0.53
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.6; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.54; acc: 0.64
Batch: 160; loss: 1.61; acc: 0.5
Batch: 180; loss: 1.71; acc: 0.44
Batch: 200; loss: 1.64; acc: 0.53
Batch: 220; loss: 1.69; acc: 0.44
Batch: 240; loss: 1.64; acc: 0.55
Batch: 260; loss: 1.63; acc: 0.48
Batch: 280; loss: 1.43; acc: 0.69
Batch: 300; loss: 1.61; acc: 0.55
Batch: 320; loss: 1.61; acc: 0.5
Batch: 340; loss: 1.59; acc: 0.53
Batch: 360; loss: 1.49; acc: 0.53
Batch: 380; loss: 1.57; acc: 0.47
Batch: 400; loss: 1.55; acc: 0.56
Batch: 420; loss: 1.53; acc: 0.48
Batch: 440; loss: 1.65; acc: 0.44
Batch: 460; loss: 1.47; acc: 0.61
Batch: 480; loss: 1.54; acc: 0.58
Batch: 500; loss: 1.45; acc: 0.62
Batch: 520; loss: 1.55; acc: 0.5
Batch: 540; loss: 1.58; acc: 0.58
Batch: 560; loss: 1.57; acc: 0.47
Batch: 580; loss: 1.66; acc: 0.44
Batch: 600; loss: 1.56; acc: 0.56
Batch: 620; loss: 1.52; acc: 0.56
Batch: 640; loss: 1.67; acc: 0.45
Batch: 660; loss: 1.65; acc: 0.48
Batch: 680; loss: 1.67; acc: 0.5
Batch: 700; loss: 1.42; acc: 0.64
Batch: 720; loss: 1.63; acc: 0.59
Batch: 740; loss: 1.59; acc: 0.53
Batch: 760; loss: 1.74; acc: 0.41
Batch: 780; loss: 1.64; acc: 0.5
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

3.692521568154916e-05
1.1855069715238642e-05
Batch: 0; loss: 1.55; acc: 0.58
Batch: 20; loss: 1.64; acc: 0.45
Batch: 40; loss: 1.31; acc: 0.64
Batch: 60; loss: 1.56; acc: 0.59
Batch: 80; loss: 1.39; acc: 0.59
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.56; acc: 0.55
Batch: 140; loss: 1.51; acc: 0.53
Val Epoch over. val_loss: 1.549607932187949; val_accuracy: 0.5431926751592356 

The current subspace-distance is: 1.1855069715238642e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.56
Batch: 20; loss: 1.64; acc: 0.5
Batch: 40; loss: 1.6; acc: 0.45
Batch: 60; loss: 1.58; acc: 0.47
Batch: 80; loss: 1.5; acc: 0.62
Batch: 100; loss: 1.67; acc: 0.45
Batch: 120; loss: 1.6; acc: 0.61
Batch: 140; loss: 1.6; acc: 0.55
Batch: 160; loss: 1.61; acc: 0.48
Batch: 180; loss: 1.79; acc: 0.36
Batch: 200; loss: 1.6; acc: 0.53
Batch: 220; loss: 1.58; acc: 0.59
Batch: 240; loss: 1.45; acc: 0.61
Batch: 260; loss: 1.7; acc: 0.5
Batch: 280; loss: 1.67; acc: 0.52
Batch: 300; loss: 1.58; acc: 0.53
Batch: 320; loss: 1.51; acc: 0.56
Batch: 340; loss: 1.5; acc: 0.59
Batch: 360; loss: 1.69; acc: 0.47
Batch: 380; loss: 1.55; acc: 0.53
Batch: 400; loss: 1.52; acc: 0.58
Batch: 420; loss: 1.65; acc: 0.44
Batch: 440; loss: 1.69; acc: 0.41
Batch: 460; loss: 1.63; acc: 0.55
Batch: 480; loss: 1.5; acc: 0.58
Batch: 500; loss: 1.48; acc: 0.52
Batch: 520; loss: 1.61; acc: 0.5
Batch: 540; loss: 1.58; acc: 0.55
Batch: 560; loss: 1.65; acc: 0.52
Batch: 580; loss: 1.69; acc: 0.44
Batch: 600; loss: 1.47; acc: 0.59
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.69; acc: 0.41
Batch: 660; loss: 1.51; acc: 0.53
Batch: 680; loss: 1.53; acc: 0.56
Batch: 700; loss: 1.58; acc: 0.5
Batch: 720; loss: 1.53; acc: 0.55
Batch: 740; loss: 1.61; acc: 0.55
Batch: 760; loss: 1.51; acc: 0.52
Batch: 780; loss: 1.45; acc: 0.59
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

3.776379526243545e-05
1.3030984518991318e-05
Batch: 0; loss: 1.54; acc: 0.56
Batch: 20; loss: 1.63; acc: 0.45
Batch: 40; loss: 1.3; acc: 0.67
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 1.36; acc: 0.59
Batch: 100; loss: 1.45; acc: 0.62
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.49; acc: 0.48
Val Epoch over. val_loss: 1.5343786872875917; val_accuracy: 0.5484673566878981 

The current subspace-distance is: 1.3030984518991318e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.62
Batch: 20; loss: 1.69; acc: 0.5
Batch: 40; loss: 1.65; acc: 0.5
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.44
Batch: 100; loss: 1.65; acc: 0.56
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.45; acc: 0.58
Batch: 160; loss: 1.57; acc: 0.52
Batch: 180; loss: 1.47; acc: 0.66
Batch: 200; loss: 1.64; acc: 0.48
Batch: 220; loss: 1.61; acc: 0.52
Batch: 240; loss: 1.58; acc: 0.52
Batch: 260; loss: 1.55; acc: 0.5
Batch: 280; loss: 1.53; acc: 0.62
Batch: 300; loss: 1.74; acc: 0.5
Batch: 320; loss: 1.65; acc: 0.44
Batch: 340; loss: 1.58; acc: 0.53
Batch: 360; loss: 1.69; acc: 0.48
Batch: 380; loss: 1.58; acc: 0.52
Batch: 400; loss: 1.56; acc: 0.45
Batch: 420; loss: 1.51; acc: 0.61
Batch: 440; loss: 1.58; acc: 0.52
Batch: 460; loss: 1.5; acc: 0.64
Batch: 480; loss: 1.66; acc: 0.48
Batch: 500; loss: 1.57; acc: 0.5
Batch: 520; loss: 1.57; acc: 0.48
Batch: 540; loss: 1.7; acc: 0.47
Batch: 560; loss: 1.56; acc: 0.56
Batch: 580; loss: 1.39; acc: 0.64
Batch: 600; loss: 1.68; acc: 0.41
Batch: 620; loss: 1.6; acc: 0.44
Batch: 640; loss: 1.46; acc: 0.58
Batch: 660; loss: 1.74; acc: 0.42
Batch: 680; loss: 1.48; acc: 0.61
Batch: 700; loss: 1.64; acc: 0.41
Batch: 720; loss: 1.78; acc: 0.48
Batch: 740; loss: 1.7; acc: 0.38
Batch: 760; loss: 1.7; acc: 0.44
Batch: 780; loss: 1.64; acc: 0.47
Train Epoch over. train_loss: 1.58; train_accuracy: 0.52 

3.966623262385838e-05
1.487721692683408e-05
Batch: 0; loss: 1.55; acc: 0.56
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.52; acc: 0.64
Batch: 80; loss: 1.35; acc: 0.61
Batch: 100; loss: 1.44; acc: 0.58
Batch: 120; loss: 1.54; acc: 0.56
Batch: 140; loss: 1.5; acc: 0.5
Val Epoch over. val_loss: 1.5268895664032858; val_accuracy: 0.5555334394904459 

The current subspace-distance is: 1.487721692683408e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.61; acc: 0.47
Batch: 60; loss: 1.63; acc: 0.41
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.54; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.5
Batch: 140; loss: 1.59; acc: 0.58
Batch: 160; loss: 1.41; acc: 0.72
Batch: 180; loss: 1.55; acc: 0.58
Batch: 200; loss: 1.68; acc: 0.42
Batch: 220; loss: 1.64; acc: 0.45
Batch: 240; loss: 1.53; acc: 0.58
Batch: 260; loss: 1.55; acc: 0.53
Batch: 280; loss: 1.51; acc: 0.56
Batch: 300; loss: 1.6; acc: 0.52
Batch: 320; loss: 1.55; acc: 0.55
Batch: 340; loss: 1.62; acc: 0.53
Batch: 360; loss: 1.51; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.59
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.41; acc: 0.61
Batch: 440; loss: 1.58; acc: 0.5
Batch: 460; loss: 1.71; acc: 0.45
Batch: 480; loss: 1.54; acc: 0.56
Batch: 500; loss: 1.55; acc: 0.58
Batch: 520; loss: 1.49; acc: 0.55
Batch: 540; loss: 1.68; acc: 0.48
Batch: 560; loss: 1.59; acc: 0.48
Batch: 580; loss: 1.49; acc: 0.58
Batch: 600; loss: 1.37; acc: 0.61
Batch: 620; loss: 1.45; acc: 0.66
Batch: 640; loss: 1.54; acc: 0.55
Batch: 660; loss: 1.41; acc: 0.55
Batch: 680; loss: 1.52; acc: 0.59
Batch: 700; loss: 1.7; acc: 0.5
Batch: 720; loss: 1.44; acc: 0.59
Batch: 740; loss: 1.66; acc: 0.48
Batch: 760; loss: 1.63; acc: 0.53
Batch: 780; loss: 1.54; acc: 0.52
Train Epoch over. train_loss: 1.57; train_accuracy: 0.53 

4.002798596047796e-05
1.6302972653647885e-05
Batch: 0; loss: 1.55; acc: 0.59
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.29; acc: 0.69
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.35; acc: 0.61
Batch: 100; loss: 1.44; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.56
Batch: 140; loss: 1.49; acc: 0.5
Val Epoch over. val_loss: 1.5289120696912146; val_accuracy: 0.555234872611465 

The current subspace-distance is: 1.6302972653647885e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.43; acc: 0.61
Batch: 20; loss: 1.53; acc: 0.61
Batch: 40; loss: 1.62; acc: 0.45
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.6; acc: 0.56
Batch: 120; loss: 1.34; acc: 0.69
Batch: 140; loss: 1.57; acc: 0.5
Batch: 160; loss: 1.67; acc: 0.5
Batch: 180; loss: 1.65; acc: 0.47
Batch: 200; loss: 1.63; acc: 0.5
Batch: 220; loss: 1.57; acc: 0.59
Batch: 240; loss: 1.58; acc: 0.52
Batch: 260; loss: 1.54; acc: 0.58
Batch: 280; loss: 1.7; acc: 0.42
Batch: 300; loss: 1.66; acc: 0.48
Batch: 320; loss: 1.58; acc: 0.52
Batch: 340; loss: 1.51; acc: 0.62
Batch: 360; loss: 1.65; acc: 0.47
Batch: 380; loss: 1.4; acc: 0.64
Batch: 400; loss: 1.51; acc: 0.58
Batch: 420; loss: 1.67; acc: 0.48
Batch: 440; loss: 1.62; acc: 0.53
Batch: 460; loss: 1.54; acc: 0.5
Batch: 480; loss: 1.42; acc: 0.59
Batch: 500; loss: 1.56; acc: 0.55
Batch: 520; loss: 1.7; acc: 0.44
Batch: 540; loss: 1.49; acc: 0.59
Batch: 560; loss: 1.6; acc: 0.48
Batch: 580; loss: 1.58; acc: 0.5
Batch: 600; loss: 1.41; acc: 0.59
Batch: 620; loss: 1.53; acc: 0.53
Batch: 640; loss: 1.68; acc: 0.47
Batch: 660; loss: 1.55; acc: 0.55
Batch: 680; loss: 1.6; acc: 0.5
Batch: 700; loss: 1.65; acc: 0.5
Batch: 720; loss: 1.73; acc: 0.38
Batch: 740; loss: 1.6; acc: 0.47
Batch: 760; loss: 1.53; acc: 0.62
Batch: 780; loss: 1.48; acc: 0.56
Train Epoch over. train_loss: 1.57; train_accuracy: 0.53 

3.945037315133959e-05
1.4800378266954795e-05
Batch: 0; loss: 1.53; acc: 0.59
Batch: 20; loss: 1.62; acc: 0.5
Batch: 40; loss: 1.28; acc: 0.7
Batch: 60; loss: 1.5; acc: 0.56
Batch: 80; loss: 1.34; acc: 0.59
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.48; acc: 0.5
Val Epoch over. val_loss: 1.5191851915067929; val_accuracy: 0.5543391719745223 

The current subspace-distance is: 1.4800378266954795e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.5
Batch: 20; loss: 1.51; acc: 0.52
Batch: 40; loss: 1.62; acc: 0.56
Batch: 60; loss: 1.49; acc: 0.7
Batch: 80; loss: 1.62; acc: 0.55
Batch: 100; loss: 1.67; acc: 0.47
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.54; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.56
Batch: 180; loss: 1.4; acc: 0.67
Batch: 200; loss: 1.66; acc: 0.44
Batch: 220; loss: 1.59; acc: 0.52
Batch: 240; loss: 1.53; acc: 0.55
Batch: 260; loss: 1.72; acc: 0.45
Batch: 280; loss: 1.49; acc: 0.61
Batch: 300; loss: 1.51; acc: 0.59
Batch: 320; loss: 1.56; acc: 0.53
Batch: 340; loss: 1.5; acc: 0.61
Batch: 360; loss: 1.6; acc: 0.52
Batch: 380; loss: 1.46; acc: 0.61
Batch: 400; loss: 1.59; acc: 0.47
Batch: 420; loss: 1.61; acc: 0.53
Batch: 440; loss: 1.58; acc: 0.58
Batch: 460; loss: 1.5; acc: 0.56
Batch: 480; loss: 1.56; acc: 0.58
Batch: 500; loss: 1.65; acc: 0.45
Batch: 520; loss: 1.48; acc: 0.56
Batch: 540; loss: 1.7; acc: 0.48
Batch: 560; loss: 1.69; acc: 0.47
Batch: 580; loss: 1.5; acc: 0.61
Batch: 600; loss: 1.51; acc: 0.52
Batch: 620; loss: 1.58; acc: 0.48
Batch: 640; loss: 1.54; acc: 0.55
Batch: 660; loss: 1.41; acc: 0.59
Batch: 680; loss: 1.5; acc: 0.61
Batch: 700; loss: 1.49; acc: 0.59
Batch: 720; loss: 1.54; acc: 0.61
Batch: 740; loss: 1.47; acc: 0.56
Batch: 760; loss: 1.6; acc: 0.58
Batch: 780; loss: 1.59; acc: 0.5
Train Epoch over. train_loss: 1.56; train_accuracy: 0.53 

3.994622966274619e-05
1.2880891517852433e-05
Batch: 0; loss: 1.54; acc: 0.58
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.27; acc: 0.69
Batch: 60; loss: 1.5; acc: 0.59
Batch: 80; loss: 1.33; acc: 0.64
Batch: 100; loss: 1.43; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.52
Val Epoch over. val_loss: 1.5151286785769615; val_accuracy: 0.5618033439490446 

The current subspace-distance is: 1.2880891517852433e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.43; acc: 0.59
Batch: 40; loss: 1.71; acc: 0.47
Batch: 60; loss: 1.42; acc: 0.59
Batch: 80; loss: 1.56; acc: 0.52
Batch: 100; loss: 1.56; acc: 0.5
Batch: 120; loss: 1.6; acc: 0.53
Batch: 140; loss: 1.42; acc: 0.61
Batch: 160; loss: 1.57; acc: 0.52
Batch: 180; loss: 1.52; acc: 0.48
Batch: 200; loss: 1.53; acc: 0.48
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.78; acc: 0.45
Batch: 280; loss: 1.55; acc: 0.55
Batch: 300; loss: 1.55; acc: 0.61
Batch: 320; loss: 1.55; acc: 0.52
Batch: 340; loss: 1.59; acc: 0.48
Batch: 360; loss: 1.53; acc: 0.52
Batch: 380; loss: 1.71; acc: 0.47
Batch: 400; loss: 1.54; acc: 0.47
Batch: 420; loss: 1.55; acc: 0.5
Batch: 440; loss: 1.62; acc: 0.5
Batch: 460; loss: 1.6; acc: 0.5
Batch: 480; loss: 1.63; acc: 0.53
Batch: 500; loss: 1.36; acc: 0.69
Batch: 520; loss: 1.6; acc: 0.52
Batch: 540; loss: 1.6; acc: 0.44
Batch: 560; loss: 1.6; acc: 0.5
Batch: 580; loss: 1.63; acc: 0.48
Batch: 600; loss: 1.54; acc: 0.55
Batch: 620; loss: 1.58; acc: 0.53
Batch: 640; loss: 1.57; acc: 0.55
Batch: 660; loss: 1.63; acc: 0.53
Batch: 680; loss: 1.46; acc: 0.62
Batch: 700; loss: 1.59; acc: 0.53
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.5
Batch: 760; loss: 1.53; acc: 0.48
Batch: 780; loss: 1.56; acc: 0.58
Train Epoch over. train_loss: 1.56; train_accuracy: 0.53 

4.085379987373017e-05
1.4711642506881617e-05
Batch: 0; loss: 1.53; acc: 0.58
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.27; acc: 0.69
Batch: 60; loss: 1.49; acc: 0.58
Batch: 80; loss: 1.32; acc: 0.64
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.47; acc: 0.48
Val Epoch over. val_loss: 1.5090500063197627; val_accuracy: 0.5586186305732485 

The current subspace-distance is: 1.4711642506881617e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.4; acc: 0.61
Batch: 20; loss: 1.5; acc: 0.59
Batch: 40; loss: 1.43; acc: 0.59
Batch: 60; loss: 1.48; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.61
Batch: 100; loss: 1.52; acc: 0.62
Batch: 120; loss: 1.65; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.56
Batch: 160; loss: 1.36; acc: 0.66
Batch: 180; loss: 1.49; acc: 0.56
Batch: 200; loss: 1.58; acc: 0.47
Batch: 220; loss: 1.45; acc: 0.61
Batch: 240; loss: 1.52; acc: 0.5
Batch: 260; loss: 1.79; acc: 0.38
Batch: 280; loss: 1.6; acc: 0.53
Batch: 300; loss: 1.55; acc: 0.56
Batch: 320; loss: 1.49; acc: 0.66
Batch: 340; loss: 1.69; acc: 0.44
Batch: 360; loss: 1.61; acc: 0.45
Batch: 380; loss: 1.5; acc: 0.52
Batch: 400; loss: 1.66; acc: 0.44
Batch: 420; loss: 1.67; acc: 0.5
Batch: 440; loss: 1.62; acc: 0.56
Batch: 460; loss: 1.49; acc: 0.59
Batch: 480; loss: 1.59; acc: 0.45
Batch: 500; loss: 1.45; acc: 0.62
Batch: 520; loss: 1.65; acc: 0.47
Batch: 540; loss: 1.58; acc: 0.5
Batch: 560; loss: 1.6; acc: 0.45
Batch: 580; loss: 1.62; acc: 0.45
Batch: 600; loss: 1.46; acc: 0.66
Batch: 620; loss: 1.59; acc: 0.5
Batch: 640; loss: 1.53; acc: 0.52
Batch: 660; loss: 1.48; acc: 0.56
Batch: 680; loss: 1.5; acc: 0.55
Batch: 700; loss: 1.54; acc: 0.59
Batch: 720; loss: 1.38; acc: 0.62
Batch: 740; loss: 1.43; acc: 0.62
Batch: 760; loss: 1.54; acc: 0.59
Batch: 780; loss: 1.67; acc: 0.53
Train Epoch over. train_loss: 1.55; train_accuracy: 0.53 

4.120870653423481e-05
1.439709376427345e-05
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.25; acc: 0.7
Batch: 60; loss: 1.48; acc: 0.59
Batch: 80; loss: 1.31; acc: 0.66
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.53; acc: 0.55
Batch: 140; loss: 1.46; acc: 0.5
Val Epoch over. val_loss: 1.504785830807534; val_accuracy: 0.5673765923566879 

The current subspace-distance is: 1.439709376427345e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.58
Batch: 40; loss: 1.5; acc: 0.56
Batch: 60; loss: 1.61; acc: 0.48
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.7; acc: 0.52
Batch: 160; loss: 1.5; acc: 0.61
Batch: 180; loss: 1.56; acc: 0.53
Batch: 200; loss: 1.42; acc: 0.64
Batch: 220; loss: 1.55; acc: 0.5
Batch: 240; loss: 1.65; acc: 0.52
Batch: 260; loss: 1.58; acc: 0.59
Batch: 280; loss: 1.55; acc: 0.48
Batch: 300; loss: 1.44; acc: 0.62
Batch: 320; loss: 1.57; acc: 0.52
Batch: 340; loss: 1.51; acc: 0.55
Batch: 360; loss: 1.41; acc: 0.58
Batch: 380; loss: 1.44; acc: 0.53
Batch: 400; loss: 1.55; acc: 0.48
Batch: 420; loss: 1.6; acc: 0.44
Batch: 440; loss: 1.56; acc: 0.52
Batch: 460; loss: 1.71; acc: 0.48
Batch: 480; loss: 1.57; acc: 0.53
Batch: 500; loss: 1.45; acc: 0.56
Batch: 520; loss: 1.61; acc: 0.5
Batch: 540; loss: 1.55; acc: 0.55
Batch: 560; loss: 1.58; acc: 0.47
Batch: 580; loss: 1.57; acc: 0.61
Batch: 600; loss: 1.56; acc: 0.59
Batch: 620; loss: 1.56; acc: 0.48
Batch: 640; loss: 1.61; acc: 0.47
Batch: 660; loss: 1.68; acc: 0.47
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.42; acc: 0.61
Batch: 720; loss: 1.65; acc: 0.59
Batch: 740; loss: 1.58; acc: 0.55
Batch: 760; loss: 1.58; acc: 0.55
Batch: 780; loss: 1.48; acc: 0.53
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.124047336517833e-05
1.3595236850960646e-05
Batch: 0; loss: 1.54; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.26; acc: 0.73
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.31; acc: 0.64
Batch: 100; loss: 1.41; acc: 0.59
Batch: 120; loss: 1.52; acc: 0.56
Batch: 140; loss: 1.45; acc: 0.52
Val Epoch over. val_loss: 1.4999294728989814; val_accuracy: 0.5684713375796179 

The current subspace-distance is: 1.3595236850960646e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.47
Batch: 20; loss: 1.49; acc: 0.56
Batch: 40; loss: 1.6; acc: 0.48
Batch: 60; loss: 1.5; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.5
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.36; acc: 0.67
Batch: 140; loss: 1.6; acc: 0.56
Batch: 160; loss: 1.47; acc: 0.64
Batch: 180; loss: 1.44; acc: 0.59
Batch: 200; loss: 1.58; acc: 0.48
Batch: 220; loss: 1.48; acc: 0.52
Batch: 240; loss: 1.59; acc: 0.45
Batch: 260; loss: 1.63; acc: 0.47
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.6; acc: 0.47
Batch: 320; loss: 1.54; acc: 0.5
Batch: 340; loss: 1.6; acc: 0.52
Batch: 360; loss: 1.52; acc: 0.5
Batch: 380; loss: 1.65; acc: 0.45
Batch: 400; loss: 1.48; acc: 0.55
Batch: 420; loss: 1.5; acc: 0.56
Batch: 440; loss: 1.43; acc: 0.62
Batch: 460; loss: 1.61; acc: 0.5
Batch: 480; loss: 1.54; acc: 0.53
Batch: 500; loss: 1.55; acc: 0.53
Batch: 520; loss: 1.54; acc: 0.55
Batch: 540; loss: 1.6; acc: 0.52
Batch: 560; loss: 1.57; acc: 0.47
Batch: 580; loss: 1.57; acc: 0.52
Batch: 600; loss: 1.68; acc: 0.39
Batch: 620; loss: 1.45; acc: 0.58
Batch: 640; loss: 1.59; acc: 0.5
Batch: 660; loss: 1.57; acc: 0.52
Batch: 680; loss: 1.61; acc: 0.52
Batch: 700; loss: 1.53; acc: 0.53
Batch: 720; loss: 1.66; acc: 0.47
Batch: 740; loss: 1.57; acc: 0.48
Batch: 760; loss: 1.58; acc: 0.53
Batch: 780; loss: 1.45; acc: 0.53
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.319236904848367e-05
1.8681326764635742e-05
Batch: 0; loss: 1.54; acc: 0.55
Batch: 20; loss: 1.59; acc: 0.53
Batch: 40; loss: 1.24; acc: 0.7
Batch: 60; loss: 1.47; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.69
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.56
Val Epoch over. val_loss: 1.494790258680939; val_accuracy: 0.5732484076433121 

The current subspace-distance is: 1.8681326764635742e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.54; acc: 0.58
Batch: 20; loss: 1.43; acc: 0.53
Batch: 40; loss: 1.49; acc: 0.52
Batch: 60; loss: 1.6; acc: 0.47
Batch: 80; loss: 1.6; acc: 0.55
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.56; acc: 0.5
Batch: 160; loss: 1.49; acc: 0.61
Batch: 180; loss: 1.68; acc: 0.41
Batch: 200; loss: 1.49; acc: 0.61
Batch: 220; loss: 1.47; acc: 0.62
Batch: 240; loss: 1.55; acc: 0.55
Batch: 260; loss: 1.55; acc: 0.55
Batch: 280; loss: 1.53; acc: 0.44
Batch: 300; loss: 1.4; acc: 0.55
Batch: 320; loss: 1.51; acc: 0.53
Batch: 340; loss: 1.55; acc: 0.56
Batch: 360; loss: 1.41; acc: 0.58
Batch: 380; loss: 1.54; acc: 0.53
Batch: 400; loss: 1.65; acc: 0.47
Batch: 420; loss: 1.68; acc: 0.47
Batch: 440; loss: 1.43; acc: 0.59
Batch: 460; loss: 1.55; acc: 0.5
Batch: 480; loss: 1.4; acc: 0.62
Batch: 500; loss: 1.6; acc: 0.53
Batch: 520; loss: 1.58; acc: 0.58
Batch: 540; loss: 1.43; acc: 0.56
Batch: 560; loss: 1.58; acc: 0.5
Batch: 580; loss: 1.61; acc: 0.52
Batch: 600; loss: 1.51; acc: 0.59
Batch: 620; loss: 1.59; acc: 0.48
Batch: 640; loss: 1.54; acc: 0.59
Batch: 660; loss: 1.59; acc: 0.5
Batch: 680; loss: 1.49; acc: 0.59
Batch: 700; loss: 1.44; acc: 0.58
Batch: 720; loss: 1.52; acc: 0.5
Batch: 740; loss: 1.41; acc: 0.58
Batch: 760; loss: 1.49; acc: 0.62
Batch: 780; loss: 1.46; acc: 0.59
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.3462096073199064e-05
1.6690586562617682e-05
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.24; acc: 0.69
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.4; acc: 0.62
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.53
Val Epoch over. val_loss: 1.492469076897688; val_accuracy: 0.5725517515923567 

The current subspace-distance is: 1.6690586562617682e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.45
Batch: 20; loss: 1.51; acc: 0.59
Batch: 40; loss: 1.55; acc: 0.47
Batch: 60; loss: 1.52; acc: 0.5
Batch: 80; loss: 1.64; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.56
Batch: 120; loss: 1.39; acc: 0.59
Batch: 140; loss: 1.63; acc: 0.52
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.49; acc: 0.58
Batch: 200; loss: 1.58; acc: 0.53
Batch: 220; loss: 1.49; acc: 0.55
Batch: 240; loss: 1.54; acc: 0.56
Batch: 260; loss: 1.62; acc: 0.48
Batch: 280; loss: 1.58; acc: 0.5
Batch: 300; loss: 1.38; acc: 0.67
Batch: 320; loss: 1.61; acc: 0.47
Batch: 340; loss: 1.66; acc: 0.42
Batch: 360; loss: 1.58; acc: 0.5
Batch: 380; loss: 1.46; acc: 0.53
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.39; acc: 0.58
Batch: 440; loss: 1.62; acc: 0.52
Batch: 460; loss: 1.61; acc: 0.44
Batch: 480; loss: 1.68; acc: 0.38
Batch: 500; loss: 1.57; acc: 0.53
Batch: 520; loss: 1.48; acc: 0.62
Batch: 540; loss: 1.6; acc: 0.47
Batch: 560; loss: 1.5; acc: 0.52
Batch: 580; loss: 1.5; acc: 0.55
Batch: 600; loss: 1.67; acc: 0.39
Batch: 620; loss: 1.52; acc: 0.55
Batch: 640; loss: 1.5; acc: 0.55
Batch: 660; loss: 1.57; acc: 0.52
Batch: 680; loss: 1.51; acc: 0.56
Batch: 700; loss: 1.6; acc: 0.48
Batch: 720; loss: 1.6; acc: 0.45
Batch: 740; loss: 1.44; acc: 0.61
Batch: 760; loss: 1.54; acc: 0.53
Batch: 780; loss: 1.53; acc: 0.55
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.319847721490078e-05
1.540522134746425e-05
Batch: 0; loss: 1.53; acc: 0.58
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.24; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.29; acc: 0.67
Batch: 100; loss: 1.4; acc: 0.59
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.43; acc: 0.58
Val Epoch over. val_loss: 1.4851547715010915; val_accuracy: 0.578125 

The current subspace-distance is: 1.540522134746425e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.61
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.55; acc: 0.47
Batch: 60; loss: 1.64; acc: 0.47
Batch: 80; loss: 1.37; acc: 0.61
Batch: 100; loss: 1.57; acc: 0.47
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.52
Batch: 160; loss: 1.59; acc: 0.55
Batch: 180; loss: 1.56; acc: 0.59
Batch: 200; loss: 1.46; acc: 0.52
Batch: 220; loss: 1.46; acc: 0.59
Batch: 240; loss: 1.51; acc: 0.53
Batch: 260; loss: 1.56; acc: 0.55
Batch: 280; loss: 1.51; acc: 0.58
Batch: 300; loss: 1.43; acc: 0.59
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.57; acc: 0.52
Batch: 360; loss: 1.56; acc: 0.52
Batch: 380; loss: 1.51; acc: 0.5
Batch: 400; loss: 1.47; acc: 0.58
Batch: 420; loss: 1.53; acc: 0.47
Batch: 440; loss: 1.7; acc: 0.47
Batch: 460; loss: 1.53; acc: 0.58
Batch: 480; loss: 1.43; acc: 0.59
Batch: 500; loss: 1.64; acc: 0.48
Batch: 520; loss: 1.48; acc: 0.56
Batch: 540; loss: 1.53; acc: 0.56
Batch: 560; loss: 1.58; acc: 0.52
Batch: 580; loss: 1.38; acc: 0.61
Batch: 600; loss: 1.63; acc: 0.5
Batch: 620; loss: 1.46; acc: 0.53
Batch: 640; loss: 1.64; acc: 0.52
Batch: 660; loss: 1.6; acc: 0.45
Batch: 680; loss: 1.42; acc: 0.62
Batch: 700; loss: 1.59; acc: 0.56
Batch: 720; loss: 1.55; acc: 0.58
Batch: 740; loss: 1.75; acc: 0.42
Batch: 760; loss: 1.58; acc: 0.5
Batch: 780; loss: 1.38; acc: 0.66
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.500063732848503e-05
1.8508404536987655e-05
Batch: 0; loss: 1.53; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.29; acc: 0.67
Batch: 100; loss: 1.4; acc: 0.64
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.43; acc: 0.56
Val Epoch over. val_loss: 1.4822495515179481; val_accuracy: 0.5795183121019108 

The current subspace-distance is: 1.8508404536987655e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.5
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.46; acc: 0.59
Batch: 60; loss: 1.57; acc: 0.48
Batch: 80; loss: 1.38; acc: 0.66
Batch: 100; loss: 1.39; acc: 0.66
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.41; acc: 0.58
Batch: 160; loss: 1.47; acc: 0.56
Batch: 180; loss: 1.63; acc: 0.44
Batch: 200; loss: 1.51; acc: 0.45
Batch: 220; loss: 1.56; acc: 0.53
Batch: 240; loss: 1.47; acc: 0.61
Batch: 260; loss: 1.72; acc: 0.42
Batch: 280; loss: 1.35; acc: 0.67
Batch: 300; loss: 1.45; acc: 0.58
Batch: 320; loss: 1.64; acc: 0.48
Batch: 340; loss: 1.63; acc: 0.5
Batch: 360; loss: 1.56; acc: 0.56
Batch: 380; loss: 1.39; acc: 0.59
Batch: 400; loss: 1.54; acc: 0.55
Batch: 420; loss: 1.57; acc: 0.48
Batch: 440; loss: 1.6; acc: 0.5
Batch: 460; loss: 1.49; acc: 0.59
Batch: 480; loss: 1.4; acc: 0.64
Batch: 500; loss: 1.56; acc: 0.59
Batch: 520; loss: 1.44; acc: 0.64
Batch: 540; loss: 1.44; acc: 0.52
Batch: 560; loss: 1.48; acc: 0.55
Batch: 580; loss: 1.5; acc: 0.61
Batch: 600; loss: 1.54; acc: 0.58
Batch: 620; loss: 1.46; acc: 0.61
Batch: 640; loss: 1.43; acc: 0.56
Batch: 660; loss: 1.47; acc: 0.64
Batch: 680; loss: 1.49; acc: 0.52
Batch: 700; loss: 1.44; acc: 0.61
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.44; acc: 0.62
Batch: 760; loss: 1.44; acc: 0.59
Batch: 780; loss: 1.55; acc: 0.5
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.407394953886978e-05
1.6858361050253734e-05
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.45; acc: 0.58
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.4; acc: 0.62
Batch: 120; loss: 1.5; acc: 0.56
Batch: 140; loss: 1.43; acc: 0.53
Val Epoch over. val_loss: 1.479419967171493; val_accuracy: 0.5722531847133758 

The current subspace-distance is: 1.6858361050253734e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.55
Batch: 20; loss: 1.51; acc: 0.56
Batch: 40; loss: 1.42; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.58
Batch: 100; loss: 1.5; acc: 0.56
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 1.33; acc: 0.72
Batch: 160; loss: 1.52; acc: 0.53
Batch: 180; loss: 1.32; acc: 0.72
Batch: 200; loss: 1.57; acc: 0.56
Batch: 220; loss: 1.57; acc: 0.52
Batch: 240; loss: 1.54; acc: 0.58
Batch: 260; loss: 1.48; acc: 0.53
Batch: 280; loss: 1.63; acc: 0.56
Batch: 300; loss: 1.62; acc: 0.5
Batch: 320; loss: 1.58; acc: 0.48
Batch: 340; loss: 1.51; acc: 0.56
Batch: 360; loss: 1.48; acc: 0.48
Batch: 380; loss: 1.5; acc: 0.55
Batch: 400; loss: 1.52; acc: 0.53
Batch: 420; loss: 1.63; acc: 0.44
Batch: 440; loss: 1.72; acc: 0.41
Batch: 460; loss: 1.57; acc: 0.53
Batch: 480; loss: 1.52; acc: 0.48
Batch: 500; loss: 1.49; acc: 0.55
Batch: 520; loss: 1.64; acc: 0.58
Batch: 540; loss: 1.51; acc: 0.56
Batch: 560; loss: 1.53; acc: 0.5
Batch: 580; loss: 1.58; acc: 0.52
Batch: 600; loss: 1.47; acc: 0.59
Batch: 620; loss: 1.46; acc: 0.61
Batch: 640; loss: 1.59; acc: 0.5
Batch: 660; loss: 1.49; acc: 0.52
Batch: 680; loss: 1.59; acc: 0.58
Batch: 700; loss: 1.64; acc: 0.52
Batch: 720; loss: 1.33; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.58
Batch: 760; loss: 1.68; acc: 0.45
Batch: 780; loss: 1.58; acc: 0.45
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.379597885417752e-05
1.5411576896440238e-05
Batch: 0; loss: 1.53; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.22; acc: 0.72
Batch: 60; loss: 1.45; acc: 0.58
Batch: 80; loss: 1.28; acc: 0.66
Batch: 100; loss: 1.41; acc: 0.59
Batch: 120; loss: 1.51; acc: 0.56
Batch: 140; loss: 1.42; acc: 0.53
Val Epoch over. val_loss: 1.4798041369504988; val_accuracy: 0.5742436305732485 

The current subspace-distance is: 1.5411576896440238e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.48
Batch: 20; loss: 1.57; acc: 0.52
Batch: 40; loss: 1.53; acc: 0.56
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.63; acc: 0.44
Batch: 100; loss: 1.45; acc: 0.62
Batch: 120; loss: 1.43; acc: 0.62
Batch: 140; loss: 1.38; acc: 0.62
Batch: 160; loss: 1.58; acc: 0.53
Batch: 180; loss: 1.72; acc: 0.5
Batch: 200; loss: 1.39; acc: 0.64
Batch: 220; loss: 1.37; acc: 0.59
Batch: 240; loss: 1.38; acc: 0.62
Batch: 260; loss: 1.52; acc: 0.55
Batch: 280; loss: 1.51; acc: 0.62
Batch: 300; loss: 1.46; acc: 0.67
Batch: 320; loss: 1.49; acc: 0.56
Batch: 340; loss: 1.58; acc: 0.5
Batch: 360; loss: 1.5; acc: 0.55
Batch: 380; loss: 1.6; acc: 0.55
Batch: 400; loss: 1.55; acc: 0.56
Batch: 420; loss: 1.71; acc: 0.53
Batch: 440; loss: 1.47; acc: 0.55
Batch: 460; loss: 1.51; acc: 0.5
Batch: 480; loss: 1.53; acc: 0.55
Batch: 500; loss: 1.43; acc: 0.62
Batch: 520; loss: 1.6; acc: 0.52
Batch: 540; loss: 1.45; acc: 0.55
Batch: 560; loss: 1.62; acc: 0.52
Batch: 580; loss: 1.53; acc: 0.47
Batch: 600; loss: 1.44; acc: 0.56
Batch: 620; loss: 1.59; acc: 0.53
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.53; acc: 0.56
Batch: 680; loss: 1.44; acc: 0.59
Batch: 700; loss: 1.4; acc: 0.61
Batch: 720; loss: 1.65; acc: 0.41
Batch: 740; loss: 1.57; acc: 0.5
Batch: 760; loss: 1.6; acc: 0.53
Batch: 780; loss: 1.32; acc: 0.72
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.453417568583973e-05
1.5818764950381592e-05
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.2; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.28; acc: 0.66
Batch: 100; loss: 1.39; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.53
Val Epoch over. val_loss: 1.470039552184427; val_accuracy: 0.5746417197452229 

The current subspace-distance is: 1.5818764950381592e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.55
Batch: 20; loss: 1.62; acc: 0.5
Batch: 40; loss: 1.52; acc: 0.53
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.49; acc: 0.62
Batch: 100; loss: 1.54; acc: 0.55
Batch: 120; loss: 1.51; acc: 0.61
Batch: 140; loss: 1.62; acc: 0.53
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.39; acc: 0.66
Batch: 200; loss: 1.44; acc: 0.62
Batch: 220; loss: 1.6; acc: 0.47
Batch: 240; loss: 1.67; acc: 0.5
Batch: 260; loss: 1.48; acc: 0.59
Batch: 280; loss: 1.6; acc: 0.5
Batch: 300; loss: 1.56; acc: 0.52
Batch: 320; loss: 1.5; acc: 0.58
Batch: 340; loss: 1.42; acc: 0.59
Batch: 360; loss: 1.45; acc: 0.61
Batch: 380; loss: 1.34; acc: 0.66
Batch: 400; loss: 1.53; acc: 0.55
Batch: 420; loss: 1.42; acc: 0.59
Batch: 440; loss: 1.52; acc: 0.47
Batch: 460; loss: 1.47; acc: 0.56
Batch: 480; loss: 1.59; acc: 0.5
Batch: 500; loss: 1.52; acc: 0.56
Batch: 520; loss: 1.65; acc: 0.45
Batch: 540; loss: 1.62; acc: 0.5
Batch: 560; loss: 1.57; acc: 0.55
Batch: 580; loss: 1.56; acc: 0.44
Batch: 600; loss: 1.48; acc: 0.55
Batch: 620; loss: 1.49; acc: 0.58
Batch: 640; loss: 1.43; acc: 0.62
Batch: 660; loss: 1.3; acc: 0.64
Batch: 680; loss: 1.43; acc: 0.58
Batch: 700; loss: 1.47; acc: 0.66
Batch: 720; loss: 1.73; acc: 0.47
Batch: 740; loss: 1.49; acc: 0.58
Batch: 760; loss: 1.44; acc: 0.58
Batch: 780; loss: 1.62; acc: 0.5
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.486334364628419e-05
1.5930792869767174e-05
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.43; acc: 0.59
Batch: 80; loss: 1.27; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.62
Batch: 120; loss: 1.49; acc: 0.56
Batch: 140; loss: 1.42; acc: 0.53
Val Epoch over. val_loss: 1.4709607142551688; val_accuracy: 0.5730493630573248 

The current subspace-distance is: 1.5930792869767174e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.72; acc: 0.42
Batch: 20; loss: 1.54; acc: 0.53
Batch: 40; loss: 1.5; acc: 0.48
Batch: 60; loss: 1.53; acc: 0.52
Batch: 80; loss: 1.53; acc: 0.53
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.42; acc: 0.59
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.47; acc: 0.59
Batch: 200; loss: 1.53; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.5
Batch: 240; loss: 1.38; acc: 0.59
Batch: 260; loss: 1.53; acc: 0.62
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.46; acc: 0.62
Batch: 320; loss: 1.49; acc: 0.5
Batch: 340; loss: 1.48; acc: 0.53
Batch: 360; loss: 1.38; acc: 0.55
Batch: 380; loss: 1.66; acc: 0.5
Batch: 400; loss: 1.47; acc: 0.53
Batch: 420; loss: 1.54; acc: 0.5
Batch: 440; loss: 1.44; acc: 0.64
Batch: 460; loss: 1.42; acc: 0.55
Batch: 480; loss: 1.34; acc: 0.67
Batch: 500; loss: 1.51; acc: 0.56
Batch: 520; loss: 1.4; acc: 0.64
Batch: 540; loss: 1.38; acc: 0.62
Batch: 560; loss: 1.52; acc: 0.59
Batch: 580; loss: 1.47; acc: 0.56
Batch: 600; loss: 1.59; acc: 0.48
Batch: 620; loss: 1.58; acc: 0.5
Batch: 640; loss: 1.57; acc: 0.39
Batch: 660; loss: 1.56; acc: 0.52
Batch: 680; loss: 1.62; acc: 0.53
Batch: 700; loss: 1.45; acc: 0.61
Batch: 720; loss: 1.55; acc: 0.53
Batch: 740; loss: 1.5; acc: 0.55
Batch: 760; loss: 1.5; acc: 0.52
Batch: 780; loss: 1.62; acc: 0.45
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.524325413512997e-05
1.770406379364431e-05
Batch: 0; loss: 1.52; acc: 0.58
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.22; acc: 0.69
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.39; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.56
Batch: 140; loss: 1.42; acc: 0.53
Val Epoch over. val_loss: 1.4760314620983828; val_accuracy: 0.5729498407643312 

The current subspace-distance is: 1.770406379364431e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.52; acc: 0.53
Batch: 40; loss: 1.65; acc: 0.42
Batch: 60; loss: 1.54; acc: 0.59
Batch: 80; loss: 1.49; acc: 0.52
Batch: 100; loss: 1.45; acc: 0.58
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.46; acc: 0.53
Batch: 160; loss: 1.61; acc: 0.5
Batch: 180; loss: 1.58; acc: 0.47
Batch: 200; loss: 1.47; acc: 0.61
Batch: 220; loss: 1.56; acc: 0.58
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.55; acc: 0.53
Batch: 280; loss: 1.47; acc: 0.59
Batch: 300; loss: 1.46; acc: 0.53
Batch: 320; loss: 1.46; acc: 0.56
Batch: 340; loss: 1.46; acc: 0.61
Batch: 360; loss: 1.5; acc: 0.58
Batch: 380; loss: 1.52; acc: 0.5
Batch: 400; loss: 1.64; acc: 0.53
Batch: 420; loss: 1.46; acc: 0.56
Batch: 440; loss: 1.55; acc: 0.59
Batch: 460; loss: 1.6; acc: 0.52
Batch: 480; loss: 1.4; acc: 0.61
Batch: 500; loss: 1.66; acc: 0.45
Batch: 520; loss: 1.67; acc: 0.52
Batch: 540; loss: 1.53; acc: 0.48
Batch: 560; loss: 1.59; acc: 0.48
Batch: 580; loss: 1.52; acc: 0.48
Batch: 600; loss: 1.52; acc: 0.55
Batch: 620; loss: 1.63; acc: 0.47
Batch: 640; loss: 1.66; acc: 0.48
Batch: 660; loss: 1.54; acc: 0.56
Batch: 680; loss: 1.56; acc: 0.5
Batch: 700; loss: 1.47; acc: 0.61
Batch: 720; loss: 1.43; acc: 0.58
Batch: 740; loss: 1.59; acc: 0.42
Batch: 760; loss: 1.44; acc: 0.61
Batch: 780; loss: 1.42; acc: 0.61
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.476911999518052e-05
1.614586108189542e-05
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.6; acc: 0.53
Batch: 40; loss: 1.22; acc: 0.69
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.39; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.53
Val Epoch over. val_loss: 1.4735927376777502; val_accuracy: 0.5741441082802548 

The current subspace-distance is: 1.614586108189542e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.43; acc: 0.59
Batch: 20; loss: 1.42; acc: 0.64
Batch: 40; loss: 1.58; acc: 0.45
Batch: 60; loss: 1.56; acc: 0.45
Batch: 80; loss: 1.49; acc: 0.52
Batch: 100; loss: 1.61; acc: 0.55
Batch: 120; loss: 1.63; acc: 0.59
Batch: 140; loss: 1.45; acc: 0.58
Batch: 160; loss: 1.54; acc: 0.47
Batch: 180; loss: 1.56; acc: 0.52
Batch: 200; loss: 1.46; acc: 0.59
Batch: 220; loss: 1.5; acc: 0.55
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 1.43; acc: 0.55
Batch: 280; loss: 1.39; acc: 0.59
Batch: 300; loss: 1.47; acc: 0.58
Batch: 320; loss: 1.48; acc: 0.52
Batch: 340; loss: 1.53; acc: 0.53
Batch: 360; loss: 1.53; acc: 0.55
Batch: 380; loss: 1.5; acc: 0.59
Batch: 400; loss: 1.48; acc: 0.56
Batch: 420; loss: 1.52; acc: 0.47
Batch: 440; loss: 1.53; acc: 0.55
Batch: 460; loss: 1.44; acc: 0.64
Batch: 480; loss: 1.44; acc: 0.61
Batch: 500; loss: 1.42; acc: 0.61
Batch: 520; loss: 1.58; acc: 0.5
Batch: 540; loss: 1.5; acc: 0.55
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.49; acc: 0.58
Batch: 600; loss: 1.6; acc: 0.5
Batch: 620; loss: 1.56; acc: 0.61
Batch: 640; loss: 1.5; acc: 0.56
Batch: 660; loss: 1.51; acc: 0.5
Batch: 680; loss: 1.59; acc: 0.48
Batch: 700; loss: 1.67; acc: 0.41
Batch: 720; loss: 1.53; acc: 0.52
Batch: 740; loss: 1.56; acc: 0.5
Batch: 760; loss: 1.62; acc: 0.48
Batch: 780; loss: 1.53; acc: 0.48
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.500951399677433e-05
1.6712470824131742e-05
Batch: 0; loss: 1.52; acc: 0.58
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.22; acc: 0.72
Batch: 60; loss: 1.45; acc: 0.58
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.39; acc: 0.62
Batch: 120; loss: 1.5; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.56
Val Epoch over. val_loss: 1.4759379215301223; val_accuracy: 0.5767316878980892 

The current subspace-distance is: 1.6712470824131742e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.46; acc: 0.62
Batch: 20; loss: 1.51; acc: 0.55
Batch: 40; loss: 1.55; acc: 0.52
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.52; acc: 0.52
Batch: 100; loss: 1.86; acc: 0.39
Batch: 120; loss: 1.5; acc: 0.48
Batch: 140; loss: 1.74; acc: 0.39
Batch: 160; loss: 1.42; acc: 0.64
Batch: 180; loss: 1.51; acc: 0.56
Batch: 200; loss: 1.53; acc: 0.59
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.54; acc: 0.5
Batch: 260; loss: 1.55; acc: 0.53
Batch: 280; loss: 1.43; acc: 0.53
Batch: 300; loss: 1.49; acc: 0.55
Batch: 320; loss: 1.45; acc: 0.55
Batch: 340; loss: 1.51; acc: 0.56
Batch: 360; loss: 1.6; acc: 0.48
Batch: 380; loss: 1.5; acc: 0.53
Batch: 400; loss: 1.45; acc: 0.53
Batch: 420; loss: 1.5; acc: 0.59
Batch: 440; loss: 1.57; acc: 0.44
Batch: 460; loss: 1.39; acc: 0.59
Batch: 480; loss: 1.58; acc: 0.47
Batch: 500; loss: 1.41; acc: 0.59
Batch: 520; loss: 1.48; acc: 0.59
Batch: 540; loss: 1.41; acc: 0.55
Batch: 560; loss: 1.6; acc: 0.47
Batch: 580; loss: 1.49; acc: 0.55
Batch: 600; loss: 1.58; acc: 0.47
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.62; acc: 0.41
Batch: 660; loss: 1.32; acc: 0.59
Batch: 680; loss: 1.58; acc: 0.55
Batch: 700; loss: 1.6; acc: 0.5
Batch: 720; loss: 1.53; acc: 0.52
Batch: 740; loss: 1.7; acc: 0.44
Batch: 760; loss: 1.37; acc: 0.72
Batch: 780; loss: 1.72; acc: 0.39
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.6373465011129156e-05
1.8879600247601047e-05
Batch: 0; loss: 1.52; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.44; acc: 0.59
Batch: 80; loss: 1.27; acc: 0.67
Batch: 100; loss: 1.4; acc: 0.59
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.58
Val Epoch over. val_loss: 1.4719253892351867; val_accuracy: 0.5744426751592356 

The current subspace-distance is: 1.8879600247601047e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.46; acc: 0.5
Batch: 20; loss: 1.48; acc: 0.52
Batch: 40; loss: 1.58; acc: 0.52
Batch: 60; loss: 1.63; acc: 0.48
Batch: 80; loss: 1.57; acc: 0.52
Batch: 100; loss: 1.44; acc: 0.61
Batch: 120; loss: 1.58; acc: 0.5
Batch: 140; loss: 1.51; acc: 0.56
Batch: 160; loss: 1.51; acc: 0.64
Batch: 180; loss: 1.53; acc: 0.59
Batch: 200; loss: 1.52; acc: 0.53
Batch: 220; loss: 1.53; acc: 0.55
Batch: 240; loss: 1.55; acc: 0.55
Batch: 260; loss: 1.48; acc: 0.56
Batch: 280; loss: 1.47; acc: 0.59
Batch: 300; loss: 1.57; acc: 0.53
Batch: 320; loss: 1.65; acc: 0.55
Batch: 340; loss: 1.35; acc: 0.62
Batch: 360; loss: 1.51; acc: 0.45
Batch: 380; loss: 1.64; acc: 0.48
Batch: 400; loss: 1.65; acc: 0.45
Batch: 420; loss: 1.49; acc: 0.59
Batch: 440; loss: 1.54; acc: 0.5
Batch: 460; loss: 1.47; acc: 0.61
Batch: 480; loss: 1.57; acc: 0.5
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.38; acc: 0.61
Batch: 540; loss: 1.51; acc: 0.56
Batch: 560; loss: 1.54; acc: 0.52
Batch: 580; loss: 1.62; acc: 0.53
Batch: 600; loss: 1.51; acc: 0.58
Batch: 620; loss: 1.6; acc: 0.42
Batch: 640; loss: 1.57; acc: 0.56
Batch: 660; loss: 1.45; acc: 0.64
Batch: 680; loss: 1.35; acc: 0.62
Batch: 700; loss: 1.44; acc: 0.59
Batch: 720; loss: 1.54; acc: 0.47
Batch: 740; loss: 1.47; acc: 0.61
Batch: 760; loss: 1.37; acc: 0.66
Batch: 780; loss: 1.49; acc: 0.55
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.561719833873212e-05
1.8527658539824188e-05
Batch: 0; loss: 1.52; acc: 0.56
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 1.2; acc: 0.73
Batch: 60; loss: 1.43; acc: 0.58
Batch: 80; loss: 1.27; acc: 0.66
Batch: 100; loss: 1.39; acc: 0.64
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.56
Val Epoch over. val_loss: 1.4664188395639894; val_accuracy: 0.5740445859872612 

The current subspace-distance is: 1.8527658539824188e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_11_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.21

The number of parameters is: 262995

The number of individual parameters is:

10
180
10
10
15
32700
15
15
30
98100
30
30
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 26299497
elements in E: 26299500
fraction nonzero: 0.9999998859293903
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.19
Batch: 20; loss: 2.2; acc: 0.12
Batch: 40; loss: 2.17; acc: 0.19
Batch: 60; loss: 2.11; acc: 0.17
Batch: 80; loss: 2.14; acc: 0.22
Batch: 100; loss: 2.07; acc: 0.31
Batch: 120; loss: 1.93; acc: 0.34
Batch: 140; loss: 1.98; acc: 0.31
Batch: 160; loss: 1.94; acc: 0.42
Batch: 180; loss: 1.96; acc: 0.41
Batch: 200; loss: 1.88; acc: 0.44
Batch: 220; loss: 1.91; acc: 0.33
Batch: 240; loss: 1.93; acc: 0.41
Batch: 260; loss: 1.82; acc: 0.59
Batch: 280; loss: 1.84; acc: 0.45
Batch: 300; loss: 1.93; acc: 0.39
Batch: 320; loss: 1.75; acc: 0.55
Batch: 340; loss: 1.82; acc: 0.45
Batch: 360; loss: 1.83; acc: 0.56
Batch: 380; loss: 2.0; acc: 0.38
Batch: 400; loss: 1.86; acc: 0.44
Batch: 420; loss: 1.9; acc: 0.41
Batch: 440; loss: 1.84; acc: 0.39
Batch: 460; loss: 1.86; acc: 0.39
Batch: 480; loss: 1.83; acc: 0.39
Batch: 500; loss: 1.83; acc: 0.42
Batch: 520; loss: 1.79; acc: 0.47
Batch: 540; loss: 1.85; acc: 0.36
Batch: 560; loss: 1.74; acc: 0.5
Batch: 580; loss: 1.9; acc: 0.42
Batch: 600; loss: 1.74; acc: 0.52
Batch: 620; loss: 1.74; acc: 0.55
Batch: 640; loss: 1.68; acc: 0.58
Batch: 660; loss: 1.66; acc: 0.64
Batch: 680; loss: 1.76; acc: 0.55
Batch: 700; loss: 1.74; acc: 0.52
Batch: 720; loss: 1.78; acc: 0.45
Batch: 740; loss: 1.93; acc: 0.3
Batch: 760; loss: 1.68; acc: 0.56
Batch: 780; loss: 1.77; acc: 0.55
Train Epoch over. train_loss: 1.88; train_accuracy: 0.42 

4.532176535576582e-05
3.927321813534945e-05
Batch: 0; loss: 1.64; acc: 0.59
Batch: 20; loss: 1.78; acc: 0.5
Batch: 40; loss: 1.49; acc: 0.7
Batch: 60; loss: 1.61; acc: 0.61
Batch: 80; loss: 1.7; acc: 0.58
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.8; acc: 0.48
Batch: 140; loss: 1.64; acc: 0.55
Val Epoch over. val_loss: 1.7061145616944429; val_accuracy: 0.5447850318471338 

The current subspace-distance is: 3.927321813534945e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.53
Batch: 20; loss: 1.76; acc: 0.55
Batch: 40; loss: 1.78; acc: 0.42
Batch: 60; loss: 1.66; acc: 0.58
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.64; acc: 0.55
Batch: 120; loss: 1.72; acc: 0.52
Batch: 140; loss: 1.79; acc: 0.39
Batch: 160; loss: 1.64; acc: 0.55
Batch: 180; loss: 1.6; acc: 0.56
Batch: 200; loss: 1.82; acc: 0.42
Batch: 220; loss: 1.66; acc: 0.55
Batch: 240; loss: 1.69; acc: 0.48
Batch: 260; loss: 1.61; acc: 0.59
Batch: 280; loss: 1.63; acc: 0.58
Batch: 300; loss: 1.66; acc: 0.53
Batch: 320; loss: 1.62; acc: 0.66
Batch: 340; loss: 1.61; acc: 0.62
Batch: 360; loss: 1.5; acc: 0.72
Batch: 380; loss: 1.65; acc: 0.62
Batch: 400; loss: 1.61; acc: 0.56
Batch: 420; loss: 1.71; acc: 0.5
Batch: 440; loss: 1.69; acc: 0.52
Batch: 460; loss: 1.63; acc: 0.67
Batch: 480; loss: 1.65; acc: 0.55
Batch: 500; loss: 1.59; acc: 0.66
Batch: 520; loss: 1.55; acc: 0.58
Batch: 540; loss: 1.72; acc: 0.53
Batch: 560; loss: 1.63; acc: 0.56
Batch: 580; loss: 1.51; acc: 0.61
Batch: 600; loss: 1.46; acc: 0.64
Batch: 620; loss: 1.72; acc: 0.47
Batch: 640; loss: 1.64; acc: 0.5
Batch: 660; loss: 1.53; acc: 0.59
Batch: 680; loss: 1.44; acc: 0.67
Batch: 700; loss: 1.73; acc: 0.58
Batch: 720; loss: 1.6; acc: 0.59
Batch: 740; loss: 1.59; acc: 0.55
Batch: 760; loss: 1.53; acc: 0.64
Batch: 780; loss: 1.5; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.56 

6.480020238086581e-05
5.863586193299852e-05
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.26; acc: 0.8
Batch: 60; loss: 1.4; acc: 0.7
Batch: 80; loss: 1.5; acc: 0.61
Batch: 100; loss: 1.52; acc: 0.73
Batch: 120; loss: 1.66; acc: 0.59
Batch: 140; loss: 1.47; acc: 0.62
Val Epoch over. val_loss: 1.538995478563248; val_accuracy: 0.6279856687898089 

The current subspace-distance is: 5.863586193299852e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.48
Batch: 20; loss: 1.56; acc: 0.61
Batch: 40; loss: 1.5; acc: 0.64
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.62; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.58
Batch: 120; loss: 1.54; acc: 0.64
Batch: 140; loss: 1.43; acc: 0.72
Batch: 160; loss: 1.53; acc: 0.58
Batch: 180; loss: 1.57; acc: 0.55
Batch: 200; loss: 1.62; acc: 0.55
Batch: 220; loss: 1.55; acc: 0.56
Batch: 240; loss: 1.52; acc: 0.67
Batch: 260; loss: 1.44; acc: 0.64
Batch: 280; loss: 1.45; acc: 0.66
Batch: 300; loss: 1.58; acc: 0.53
Batch: 320; loss: 1.48; acc: 0.61
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.49; acc: 0.61
Batch: 380; loss: 1.46; acc: 0.7
Batch: 400; loss: 1.52; acc: 0.59
Batch: 420; loss: 1.47; acc: 0.64
Batch: 440; loss: 1.58; acc: 0.61
Batch: 460; loss: 1.58; acc: 0.59
Batch: 480; loss: 1.61; acc: 0.56
Batch: 500; loss: 1.6; acc: 0.55
Batch: 520; loss: 1.49; acc: 0.64
Batch: 540; loss: 1.57; acc: 0.59
Batch: 560; loss: 1.52; acc: 0.58
Batch: 580; loss: 1.47; acc: 0.62
Batch: 600; loss: 1.46; acc: 0.64
Batch: 620; loss: 1.48; acc: 0.58
Batch: 640; loss: 1.64; acc: 0.55
Batch: 660; loss: 1.51; acc: 0.61
Batch: 680; loss: 1.32; acc: 0.73
Batch: 700; loss: 1.41; acc: 0.72
Batch: 720; loss: 1.43; acc: 0.66
Batch: 740; loss: 1.47; acc: 0.62
Batch: 760; loss: 1.41; acc: 0.64
Batch: 780; loss: 1.47; acc: 0.69
Train Epoch over. train_loss: 1.5; train_accuracy: 0.63 

7.886497769504786e-05
7.55832516006194e-05
Batch: 0; loss: 1.34; acc: 0.75
Batch: 20; loss: 1.59; acc: 0.56
Batch: 40; loss: 1.11; acc: 0.88
Batch: 60; loss: 1.3; acc: 0.8
Batch: 80; loss: 1.36; acc: 0.7
Batch: 100; loss: 1.42; acc: 0.77
Batch: 120; loss: 1.57; acc: 0.62
Batch: 140; loss: 1.31; acc: 0.78
Val Epoch over. val_loss: 1.4153772038259325; val_accuracy: 0.6858081210191083 

The current subspace-distance is: 7.55832516006194e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.73
Batch: 20; loss: 1.44; acc: 0.66
Batch: 40; loss: 1.44; acc: 0.66
Batch: 60; loss: 1.42; acc: 0.64
Batch: 80; loss: 1.41; acc: 0.67
Batch: 100; loss: 1.46; acc: 0.64
Batch: 120; loss: 1.46; acc: 0.64
Batch: 140; loss: 1.39; acc: 0.67
Batch: 160; loss: 1.48; acc: 0.59
Batch: 180; loss: 1.45; acc: 0.67
Batch: 200; loss: 1.44; acc: 0.67
Batch: 220; loss: 1.37; acc: 0.67
Batch: 240; loss: 1.5; acc: 0.64
Batch: 260; loss: 1.56; acc: 0.66
Batch: 280; loss: 1.49; acc: 0.61
Batch: 300; loss: 1.38; acc: 0.67
Batch: 320; loss: 1.5; acc: 0.56
Batch: 340; loss: 1.33; acc: 0.77
Batch: 360; loss: 1.57; acc: 0.59
Batch: 380; loss: 1.54; acc: 0.66
Batch: 400; loss: 1.36; acc: 0.77
Batch: 420; loss: 1.53; acc: 0.58
Batch: 440; loss: 1.48; acc: 0.64
Batch: 460; loss: 1.49; acc: 0.62
Batch: 480; loss: 1.43; acc: 0.64
Batch: 500; loss: 1.48; acc: 0.59
Batch: 520; loss: 1.52; acc: 0.56
Batch: 540; loss: 1.38; acc: 0.7
Batch: 560; loss: 1.54; acc: 0.61
Batch: 580; loss: 1.42; acc: 0.67
Batch: 600; loss: 1.41; acc: 0.72
Batch: 620; loss: 1.42; acc: 0.56
Batch: 640; loss: 1.44; acc: 0.67
Batch: 660; loss: 1.42; acc: 0.72
Batch: 680; loss: 1.39; acc: 0.73
Batch: 700; loss: 1.33; acc: 0.66
Batch: 720; loss: 1.38; acc: 0.73
Batch: 740; loss: 1.43; acc: 0.7
Batch: 760; loss: 1.45; acc: 0.67
Batch: 780; loss: 1.43; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.67 

9.213999146595597e-05
8.71375304996036e-05
Batch: 0; loss: 1.32; acc: 0.73
Batch: 20; loss: 1.52; acc: 0.59
Batch: 40; loss: 1.06; acc: 0.84
Batch: 60; loss: 1.29; acc: 0.81
Batch: 80; loss: 1.27; acc: 0.81
Batch: 100; loss: 1.35; acc: 0.72
Batch: 120; loss: 1.53; acc: 0.62
Batch: 140; loss: 1.25; acc: 0.89
Val Epoch over. val_loss: 1.364796856406388; val_accuracy: 0.7106886942675159 

The current subspace-distance is: 8.71375304996036e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.37; acc: 0.67
Batch: 20; loss: 1.39; acc: 0.67
Batch: 40; loss: 1.45; acc: 0.64
Batch: 60; loss: 1.5; acc: 0.66
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.45; acc: 0.64
Batch: 120; loss: 1.38; acc: 0.78
Batch: 140; loss: 1.3; acc: 0.75
Batch: 160; loss: 1.39; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.77
Batch: 200; loss: 1.43; acc: 0.66
Batch: 220; loss: 1.3; acc: 0.75
Batch: 240; loss: 1.44; acc: 0.59
Batch: 260; loss: 1.49; acc: 0.55
Batch: 280; loss: 1.34; acc: 0.78
Batch: 300; loss: 1.36; acc: 0.72
Batch: 320; loss: 1.49; acc: 0.61
Batch: 340; loss: 1.42; acc: 0.67
Batch: 360; loss: 1.41; acc: 0.69
Batch: 380; loss: 1.49; acc: 0.61
Batch: 400; loss: 1.25; acc: 0.72
Batch: 420; loss: 1.37; acc: 0.66
Batch: 440; loss: 1.31; acc: 0.75
Batch: 460; loss: 1.49; acc: 0.64
Batch: 480; loss: 1.46; acc: 0.64
Batch: 500; loss: 1.16; acc: 0.83
Batch: 520; loss: 1.31; acc: 0.7
Batch: 540; loss: 1.4; acc: 0.72
Batch: 560; loss: 1.44; acc: 0.64
Batch: 580; loss: 1.42; acc: 0.58
Batch: 600; loss: 1.47; acc: 0.66
Batch: 620; loss: 1.37; acc: 0.75
Batch: 640; loss: 1.33; acc: 0.7
Batch: 660; loss: 1.4; acc: 0.7
Batch: 680; loss: 1.4; acc: 0.73
Batch: 700; loss: 1.42; acc: 0.66
Batch: 720; loss: 1.49; acc: 0.58
Batch: 740; loss: 1.37; acc: 0.64
Batch: 760; loss: 1.41; acc: 0.7
Batch: 780; loss: 1.37; acc: 0.69
Train Epoch over. train_loss: 1.39; train_accuracy: 0.68 

0.00010186670260736719
9.77538584265858e-05
Batch: 0; loss: 1.29; acc: 0.75
Batch: 20; loss: 1.47; acc: 0.66
Batch: 40; loss: 1.01; acc: 0.83
Batch: 60; loss: 1.22; acc: 0.8
Batch: 80; loss: 1.2; acc: 0.88
Batch: 100; loss: 1.28; acc: 0.75
Batch: 120; loss: 1.47; acc: 0.64
Batch: 140; loss: 1.18; acc: 0.92
Val Epoch over. val_loss: 1.3179028588495436; val_accuracy: 0.7172571656050956 

The current subspace-distance is: 9.77538584265858e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.72
Batch: 20; loss: 1.46; acc: 0.58
Batch: 40; loss: 1.3; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.72
Batch: 80; loss: 1.32; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.77
Batch: 120; loss: 1.31; acc: 0.72
Batch: 140; loss: 1.44; acc: 0.59
Batch: 160; loss: 1.42; acc: 0.62
Batch: 180; loss: 1.4; acc: 0.66
Batch: 200; loss: 1.19; acc: 0.83
Batch: 220; loss: 1.32; acc: 0.66
Batch: 240; loss: 1.47; acc: 0.66
Batch: 260; loss: 1.31; acc: 0.77
Batch: 280; loss: 1.44; acc: 0.55
Batch: 300; loss: 1.52; acc: 0.66
Batch: 320; loss: 1.35; acc: 0.73
Batch: 340; loss: 1.33; acc: 0.67
Batch: 360; loss: 1.27; acc: 0.7
Batch: 380; loss: 1.21; acc: 0.72
Batch: 400; loss: 1.33; acc: 0.72
Batch: 420; loss: 1.33; acc: 0.8
Batch: 440; loss: 1.43; acc: 0.62
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.35; acc: 0.62
Batch: 500; loss: 1.38; acc: 0.61
Batch: 520; loss: 1.35; acc: 0.64
Batch: 540; loss: 1.48; acc: 0.62
Batch: 560; loss: 1.44; acc: 0.64
Batch: 580; loss: 1.33; acc: 0.67
Batch: 600; loss: 1.44; acc: 0.61
Batch: 620; loss: 1.3; acc: 0.78
Batch: 640; loss: 1.38; acc: 0.66
Batch: 660; loss: 1.3; acc: 0.75
Batch: 680; loss: 1.42; acc: 0.66
Batch: 700; loss: 1.27; acc: 0.69
Batch: 720; loss: 1.49; acc: 0.56
Batch: 740; loss: 1.39; acc: 0.64
Batch: 760; loss: 1.3; acc: 0.73
Batch: 780; loss: 1.25; acc: 0.78
Train Epoch over. train_loss: 1.35; train_accuracy: 0.68 

0.00011186105257365853
0.00010801531607285142
Batch: 0; loss: 1.25; acc: 0.73
Batch: 20; loss: 1.43; acc: 0.75
Batch: 40; loss: 0.98; acc: 0.81
Batch: 60; loss: 1.18; acc: 0.81
Batch: 80; loss: 1.17; acc: 0.81
Batch: 100; loss: 1.21; acc: 0.83
Batch: 120; loss: 1.43; acc: 0.64
Batch: 140; loss: 1.16; acc: 0.89
Val Epoch over. val_loss: 1.288196704949543; val_accuracy: 0.722531847133758 

The current subspace-distance is: 0.00010801531607285142 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.66
Batch: 20; loss: 1.36; acc: 0.7
Batch: 40; loss: 1.39; acc: 0.61
Batch: 60; loss: 1.26; acc: 0.75
Batch: 80; loss: 1.35; acc: 0.62
Batch: 100; loss: 1.27; acc: 0.72
Batch: 120; loss: 1.29; acc: 0.7
Batch: 140; loss: 1.43; acc: 0.55
Batch: 160; loss: 1.28; acc: 0.75
Batch: 180; loss: 1.36; acc: 0.7
Batch: 200; loss: 1.46; acc: 0.62
Batch: 220; loss: 1.22; acc: 0.77
Batch: 240; loss: 1.37; acc: 0.67
Batch: 260; loss: 1.22; acc: 0.69
Batch: 280; loss: 1.43; acc: 0.69
Batch: 300; loss: 1.21; acc: 0.78
Batch: 320; loss: 1.35; acc: 0.73
Batch: 340; loss: 1.32; acc: 0.67
Batch: 360; loss: 1.29; acc: 0.69
Batch: 380; loss: 1.36; acc: 0.73
Batch: 400; loss: 1.28; acc: 0.69
Batch: 420; loss: 1.44; acc: 0.61
Batch: 440; loss: 1.28; acc: 0.64
Batch: 460; loss: 1.23; acc: 0.78
Batch: 480; loss: 1.28; acc: 0.73
Batch: 500; loss: 1.29; acc: 0.73
Batch: 520; loss: 1.26; acc: 0.69
Batch: 540; loss: 1.31; acc: 0.69
Batch: 560; loss: 1.42; acc: 0.58
Batch: 580; loss: 1.39; acc: 0.62
Batch: 600; loss: 1.22; acc: 0.78
Batch: 620; loss: 1.34; acc: 0.67
Batch: 640; loss: 1.23; acc: 0.78
Batch: 660; loss: 1.41; acc: 0.64
Batch: 680; loss: 1.14; acc: 0.78
Batch: 700; loss: 1.35; acc: 0.64
Batch: 720; loss: 1.31; acc: 0.72
Batch: 740; loss: 1.38; acc: 0.62
Batch: 760; loss: 1.19; acc: 0.69
Batch: 780; loss: 1.33; acc: 0.69
Train Epoch over. train_loss: 1.33; train_accuracy: 0.68 

0.0001229259796673432
0.000115752533019986
Batch: 0; loss: 1.22; acc: 0.72
Batch: 20; loss: 1.38; acc: 0.7
Batch: 40; loss: 0.94; acc: 0.86
Batch: 60; loss: 1.14; acc: 0.78
Batch: 80; loss: 1.14; acc: 0.8
Batch: 100; loss: 1.16; acc: 0.81
Batch: 120; loss: 1.38; acc: 0.69
Batch: 140; loss: 1.13; acc: 0.88
Val Epoch over. val_loss: 1.2608609491852438; val_accuracy: 0.7204418789808917 

The current subspace-distance is: 0.000115752533019986 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.77
Batch: 20; loss: 1.27; acc: 0.66
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.36; acc: 0.67
Batch: 80; loss: 1.2; acc: 0.78
Batch: 100; loss: 1.22; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.73
Batch: 140; loss: 1.24; acc: 0.78
Batch: 160; loss: 1.42; acc: 0.58
Batch: 180; loss: 1.22; acc: 0.72
Batch: 200; loss: 1.2; acc: 0.66
Batch: 220; loss: 1.26; acc: 0.72
Batch: 240; loss: 1.25; acc: 0.73
Batch: 260; loss: 1.18; acc: 0.78
Batch: 280; loss: 1.45; acc: 0.56
Batch: 300; loss: 1.22; acc: 0.7
Batch: 320; loss: 1.1; acc: 0.86
Batch: 340; loss: 1.4; acc: 0.62
Batch: 360; loss: 1.16; acc: 0.8
Batch: 380; loss: 1.24; acc: 0.78
Batch: 400; loss: 1.31; acc: 0.72
Batch: 420; loss: 1.23; acc: 0.7
Batch: 440; loss: 1.32; acc: 0.7
Batch: 460; loss: 1.24; acc: 0.66
Batch: 480; loss: 1.22; acc: 0.67
Batch: 500; loss: 1.3; acc: 0.73
Batch: 520; loss: 1.29; acc: 0.72
Batch: 540; loss: 1.37; acc: 0.66
Batch: 560; loss: 1.22; acc: 0.77
Batch: 580; loss: 1.29; acc: 0.67
Batch: 600; loss: 1.24; acc: 0.7
Batch: 620; loss: 1.29; acc: 0.7
Batch: 640; loss: 1.29; acc: 0.67
Batch: 660; loss: 1.29; acc: 0.69
Batch: 680; loss: 1.35; acc: 0.72
Batch: 700; loss: 1.21; acc: 0.67
Batch: 720; loss: 1.4; acc: 0.66
Batch: 740; loss: 1.25; acc: 0.73
Batch: 760; loss: 1.1; acc: 0.8
Batch: 780; loss: 1.37; acc: 0.7
Train Epoch over. train_loss: 1.29; train_accuracy: 0.69 

0.0001330568629782647
0.00012777603114955127
Batch: 0; loss: 1.2; acc: 0.75
Batch: 20; loss: 1.32; acc: 0.75
Batch: 40; loss: 0.91; acc: 0.88
Batch: 60; loss: 1.11; acc: 0.75
Batch: 80; loss: 1.11; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.84
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 1.07; acc: 0.86
Val Epoch over. val_loss: 1.2132733976765044; val_accuracy: 0.731687898089172 

The current subspace-distance is: 0.00012777603114955127 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.69
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 1.27; acc: 0.75
Batch: 60; loss: 1.25; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.7
Batch: 140; loss: 1.31; acc: 0.59
Batch: 160; loss: 1.3; acc: 0.75
Batch: 180; loss: 1.28; acc: 0.66
Batch: 200; loss: 1.15; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.7
Batch: 240; loss: 1.23; acc: 0.73
Batch: 260; loss: 1.22; acc: 0.69
Batch: 280; loss: 1.3; acc: 0.69
Batch: 300; loss: 1.09; acc: 0.77
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.36; acc: 0.62
Batch: 360; loss: 1.28; acc: 0.73
Batch: 380; loss: 1.2; acc: 0.8
Batch: 400; loss: 1.22; acc: 0.67
Batch: 420; loss: 1.2; acc: 0.73
Batch: 440; loss: 1.24; acc: 0.7
Batch: 460; loss: 1.24; acc: 0.73
Batch: 480; loss: 1.12; acc: 0.77
Batch: 500; loss: 1.38; acc: 0.64
Batch: 520; loss: 1.14; acc: 0.8
Batch: 540; loss: 1.23; acc: 0.77
Batch: 560; loss: 1.22; acc: 0.67
Batch: 580; loss: 1.2; acc: 0.72
Batch: 600; loss: 1.21; acc: 0.72
Batch: 620; loss: 1.22; acc: 0.73
Batch: 640; loss: 1.17; acc: 0.77
Batch: 660; loss: 1.33; acc: 0.69
Batch: 680; loss: 1.22; acc: 0.7
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.16; acc: 0.7
Batch: 740; loss: 1.24; acc: 0.67
Batch: 760; loss: 1.21; acc: 0.7
Batch: 780; loss: 1.21; acc: 0.72
Train Epoch over. train_loss: 1.25; train_accuracy: 0.69 

0.0001439594489056617
0.0001381343463435769
Batch: 0; loss: 1.18; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.73
Batch: 40; loss: 0.87; acc: 0.86
Batch: 60; loss: 1.08; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.81
Batch: 100; loss: 1.04; acc: 0.83
Batch: 120; loss: 1.3; acc: 0.56
Batch: 140; loss: 1.0; acc: 0.86
Val Epoch over. val_loss: 1.166440628136799; val_accuracy: 0.7352707006369427 

The current subspace-distance is: 0.0001381343463435769 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.72
Batch: 20; loss: 1.18; acc: 0.73
Batch: 40; loss: 1.25; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.69
Batch: 80; loss: 1.29; acc: 0.67
Batch: 100; loss: 1.17; acc: 0.72
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.38; acc: 0.61
Batch: 160; loss: 1.22; acc: 0.72
Batch: 180; loss: 1.16; acc: 0.67
Batch: 200; loss: 1.32; acc: 0.7
Batch: 220; loss: 1.37; acc: 0.48
Batch: 240; loss: 1.24; acc: 0.69
Batch: 260; loss: 1.14; acc: 0.72
Batch: 280; loss: 1.18; acc: 0.72
Batch: 300; loss: 1.12; acc: 0.75
Batch: 320; loss: 1.18; acc: 0.69
Batch: 340; loss: 1.24; acc: 0.7
Batch: 360; loss: 1.27; acc: 0.67
Batch: 380; loss: 1.14; acc: 0.69
Batch: 400; loss: 1.21; acc: 0.7
Batch: 420; loss: 1.19; acc: 0.72
Batch: 440; loss: 1.32; acc: 0.67
Batch: 460; loss: 1.21; acc: 0.7
Batch: 480; loss: 1.35; acc: 0.64
Batch: 500; loss: 1.2; acc: 0.67
Batch: 520; loss: 1.32; acc: 0.72
Batch: 540; loss: 1.37; acc: 0.61
Batch: 560; loss: 1.43; acc: 0.59
Batch: 580; loss: 1.21; acc: 0.72
Batch: 600; loss: 1.19; acc: 0.59
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.25; acc: 0.75
Batch: 660; loss: 1.07; acc: 0.73
Batch: 680; loss: 1.16; acc: 0.7
Batch: 700; loss: 1.33; acc: 0.66
Batch: 720; loss: 1.15; acc: 0.67
Batch: 740; loss: 1.22; acc: 0.69
Batch: 760; loss: 1.46; acc: 0.53
Batch: 780; loss: 1.22; acc: 0.72
Train Epoch over. train_loss: 1.21; train_accuracy: 0.7 

0.00015229408745653927
0.00014728169480804354
Batch: 0; loss: 1.17; acc: 0.77
Batch: 20; loss: 1.22; acc: 0.73
Batch: 40; loss: 0.86; acc: 0.86
Batch: 60; loss: 1.05; acc: 0.75
Batch: 80; loss: 1.03; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.81
Batch: 120; loss: 1.25; acc: 0.59
Batch: 140; loss: 0.94; acc: 0.86
Val Epoch over. val_loss: 1.1381218516902558; val_accuracy: 0.7402468152866242 

The current subspace-distance is: 0.00014728169480804354 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.32; acc: 0.64
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 1.17; acc: 0.67
Batch: 60; loss: 1.35; acc: 0.62
Batch: 80; loss: 1.25; acc: 0.62
Batch: 100; loss: 1.41; acc: 0.58
Batch: 120; loss: 1.06; acc: 0.73
Batch: 140; loss: 1.3; acc: 0.62
Batch: 160; loss: 1.12; acc: 0.7
Batch: 180; loss: 1.19; acc: 0.69
Batch: 200; loss: 1.18; acc: 0.66
Batch: 220; loss: 1.27; acc: 0.64
Batch: 240; loss: 1.16; acc: 0.66
Batch: 260; loss: 1.24; acc: 0.7
Batch: 280; loss: 1.35; acc: 0.64
Batch: 300; loss: 1.24; acc: 0.66
Batch: 320; loss: 0.97; acc: 0.86
Batch: 340; loss: 1.13; acc: 0.69
Batch: 360; loss: 1.19; acc: 0.64
Batch: 380; loss: 1.14; acc: 0.69
Batch: 400; loss: 0.99; acc: 0.78
Batch: 420; loss: 1.21; acc: 0.69
Batch: 440; loss: 1.17; acc: 0.69
Batch: 460; loss: 1.22; acc: 0.72
Batch: 480; loss: 1.04; acc: 0.75
Batch: 500; loss: 1.25; acc: 0.67
Batch: 520; loss: 1.26; acc: 0.67
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.35; acc: 0.58
Batch: 580; loss: 1.19; acc: 0.66
Batch: 600; loss: 1.18; acc: 0.73
Batch: 620; loss: 1.21; acc: 0.72
Batch: 640; loss: 1.23; acc: 0.64
Batch: 660; loss: 1.3; acc: 0.67
Batch: 680; loss: 1.13; acc: 0.7
Batch: 700; loss: 1.06; acc: 0.77
Batch: 720; loss: 1.25; acc: 0.64
Batch: 740; loss: 0.95; acc: 0.86
Batch: 760; loss: 1.17; acc: 0.7
Batch: 780; loss: 1.26; acc: 0.64
Train Epoch over. train_loss: 1.19; train_accuracy: 0.7 

0.00015500221343245357
0.00014975400699768215
Batch: 0; loss: 1.16; acc: 0.77
Batch: 20; loss: 1.22; acc: 0.73
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 1.06; acc: 0.77
Batch: 80; loss: 1.03; acc: 0.8
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.26; acc: 0.55
Batch: 140; loss: 0.94; acc: 0.86
Val Epoch over. val_loss: 1.1298587781608485; val_accuracy: 0.7325835987261147 

The current subspace-distance is: 0.00014975400699768215 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.2; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.24; acc: 0.66
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 1.33; acc: 0.67
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.23; acc: 0.7
Batch: 200; loss: 1.14; acc: 0.73
Batch: 220; loss: 1.25; acc: 0.69
Batch: 240; loss: 1.08; acc: 0.75
Batch: 260; loss: 1.21; acc: 0.61
Batch: 280; loss: 1.17; acc: 0.67
Batch: 300; loss: 1.12; acc: 0.78
Batch: 320; loss: 1.03; acc: 0.77
Batch: 340; loss: 1.22; acc: 0.75
Batch: 360; loss: 1.06; acc: 0.75
Batch: 380; loss: 1.12; acc: 0.72
Batch: 400; loss: 1.03; acc: 0.77
Batch: 420; loss: 1.26; acc: 0.66
Batch: 440; loss: 1.03; acc: 0.77
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.23; acc: 0.66
Batch: 500; loss: 1.09; acc: 0.75
Batch: 520; loss: 1.12; acc: 0.75
Batch: 540; loss: 1.29; acc: 0.67
Batch: 560; loss: 1.2; acc: 0.73
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.25; acc: 0.62
Batch: 620; loss: 1.18; acc: 0.67
Batch: 640; loss: 1.11; acc: 0.7
Batch: 660; loss: 1.35; acc: 0.62
Batch: 680; loss: 1.31; acc: 0.59
Batch: 700; loss: 1.24; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.7
Batch: 740; loss: 1.14; acc: 0.8
Batch: 760; loss: 0.97; acc: 0.78
Batch: 780; loss: 1.28; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.7 

0.0001620348048163578
0.00015543408517260104
Batch: 0; loss: 1.15; acc: 0.77
Batch: 20; loss: 1.2; acc: 0.72
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 1.01; acc: 0.8
Batch: 100; loss: 0.98; acc: 0.8
Batch: 120; loss: 1.23; acc: 0.59
Batch: 140; loss: 0.92; acc: 0.86
Val Epoch over. val_loss: 1.1188698809617643; val_accuracy: 0.7382563694267515 

The current subspace-distance is: 0.00015543408517260104 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.3; acc: 0.58
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 1.04; acc: 0.75
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 1.27; acc: 0.67
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 1.06; acc: 0.7
Batch: 160; loss: 1.24; acc: 0.64
Batch: 180; loss: 1.16; acc: 0.67
Batch: 200; loss: 1.24; acc: 0.66
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 1.09; acc: 0.78
Batch: 260; loss: 1.33; acc: 0.67
Batch: 280; loss: 1.11; acc: 0.73
Batch: 300; loss: 1.2; acc: 0.73
Batch: 320; loss: 0.96; acc: 0.78
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.14; acc: 0.72
Batch: 380; loss: 1.28; acc: 0.72
Batch: 400; loss: 1.22; acc: 0.72
Batch: 420; loss: 1.29; acc: 0.66
Batch: 440; loss: 1.16; acc: 0.67
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.19; acc: 0.72
Batch: 500; loss: 1.13; acc: 0.75
Batch: 520; loss: 1.26; acc: 0.59
Batch: 540; loss: 1.33; acc: 0.64
Batch: 560; loss: 0.92; acc: 0.84
Batch: 580; loss: 1.16; acc: 0.62
Batch: 600; loss: 1.1; acc: 0.72
Batch: 620; loss: 1.18; acc: 0.7
Batch: 640; loss: 1.1; acc: 0.77
Batch: 660; loss: 1.13; acc: 0.77
Batch: 680; loss: 1.17; acc: 0.66
Batch: 700; loss: 1.18; acc: 0.78
Batch: 720; loss: 1.27; acc: 0.64
Batch: 740; loss: 1.09; acc: 0.81
Batch: 760; loss: 1.06; acc: 0.8
Batch: 780; loss: 1.18; acc: 0.72
Train Epoch over. train_loss: 1.17; train_accuracy: 0.7 

0.00016470788978040218
0.00015558127779513597
Batch: 0; loss: 1.14; acc: 0.75
Batch: 20; loss: 1.2; acc: 0.73
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 1.03; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.81
Batch: 120; loss: 1.22; acc: 0.59
Batch: 140; loss: 0.91; acc: 0.86
Val Epoch over. val_loss: 1.1085384552645836; val_accuracy: 0.738953025477707 

The current subspace-distance is: 0.00015558127779513597 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 0.89; acc: 0.88
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 1.18; acc: 0.7
Batch: 140; loss: 1.14; acc: 0.72
Batch: 160; loss: 1.22; acc: 0.59
Batch: 180; loss: 1.22; acc: 0.64
Batch: 200; loss: 1.23; acc: 0.7
Batch: 220; loss: 1.09; acc: 0.72
Batch: 240; loss: 1.07; acc: 0.77
Batch: 260; loss: 1.15; acc: 0.73
Batch: 280; loss: 1.06; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.67
Batch: 320; loss: 1.27; acc: 0.7
Batch: 340; loss: 1.09; acc: 0.73
Batch: 360; loss: 0.97; acc: 0.83
Batch: 380; loss: 1.11; acc: 0.75
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.01; acc: 0.8
Batch: 440; loss: 1.28; acc: 0.72
Batch: 460; loss: 1.21; acc: 0.64
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 1.13; acc: 0.66
Batch: 520; loss: 1.18; acc: 0.72
Batch: 540; loss: 1.14; acc: 0.72
Batch: 560; loss: 1.19; acc: 0.66
Batch: 580; loss: 1.25; acc: 0.64
Batch: 600; loss: 1.02; acc: 0.83
Batch: 620; loss: 1.26; acc: 0.67
Batch: 640; loss: 1.19; acc: 0.75
Batch: 660; loss: 1.32; acc: 0.66
Batch: 680; loss: 1.14; acc: 0.72
Batch: 700; loss: 1.16; acc: 0.77
Batch: 720; loss: 1.11; acc: 0.7
Batch: 740; loss: 1.02; acc: 0.77
Batch: 760; loss: 1.1; acc: 0.72
Batch: 780; loss: 1.2; acc: 0.67
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.00016673798381816596
0.00016032086568884552
Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 1.19; acc: 0.7
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 1.02; acc: 0.78
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 0.89; acc: 0.89
Val Epoch over. val_loss: 1.0973514618387648; val_accuracy: 0.7379578025477707 

The current subspace-distance is: 0.00016032086568884552 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.99; acc: 0.78
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 1.28; acc: 0.7
Batch: 60; loss: 1.17; acc: 0.75
Batch: 80; loss: 1.25; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.61
Batch: 140; loss: 1.13; acc: 0.69
Batch: 160; loss: 1.09; acc: 0.69
Batch: 180; loss: 1.09; acc: 0.73
Batch: 200; loss: 1.31; acc: 0.69
Batch: 220; loss: 1.24; acc: 0.7
Batch: 240; loss: 1.15; acc: 0.75
Batch: 260; loss: 1.29; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.78
Batch: 300; loss: 1.23; acc: 0.7
Batch: 320; loss: 1.01; acc: 0.8
Batch: 340; loss: 1.13; acc: 0.66
Batch: 360; loss: 1.04; acc: 0.78
Batch: 380; loss: 1.32; acc: 0.56
Batch: 400; loss: 1.11; acc: 0.7
Batch: 420; loss: 1.15; acc: 0.75
Batch: 440; loss: 1.11; acc: 0.7
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.15; acc: 0.69
Batch: 500; loss: 1.15; acc: 0.69
Batch: 520; loss: 0.99; acc: 0.77
Batch: 540; loss: 1.14; acc: 0.66
Batch: 560; loss: 1.15; acc: 0.69
Batch: 580; loss: 1.18; acc: 0.73
Batch: 600; loss: 1.28; acc: 0.66
Batch: 620; loss: 1.18; acc: 0.66
Batch: 640; loss: 1.17; acc: 0.66
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 1.27; acc: 0.62
Batch: 700; loss: 1.11; acc: 0.77
Batch: 720; loss: 1.13; acc: 0.75
Batch: 740; loss: 1.29; acc: 0.66
Batch: 760; loss: 1.14; acc: 0.7
Batch: 780; loss: 1.03; acc: 0.78
Train Epoch over. train_loss: 1.15; train_accuracy: 0.7 

0.00016740855062380433
0.00016060154302977026
Batch: 0; loss: 1.13; acc: 0.73
Batch: 20; loss: 1.16; acc: 0.72
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 1.01; acc: 0.78
Batch: 80; loss: 0.98; acc: 0.77
Batch: 100; loss: 0.94; acc: 0.81
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 0.87; acc: 0.88
Val Epoch over. val_loss: 1.0857638838184867; val_accuracy: 0.7375597133757962 

The current subspace-distance is: 0.00016060154302977026 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.69
Batch: 20; loss: 1.21; acc: 0.72
Batch: 40; loss: 1.29; acc: 0.62
Batch: 60; loss: 1.09; acc: 0.77
Batch: 80; loss: 1.22; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.64
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.21; acc: 0.67
Batch: 160; loss: 1.26; acc: 0.61
Batch: 180; loss: 1.21; acc: 0.64
Batch: 200; loss: 1.19; acc: 0.66
Batch: 220; loss: 1.22; acc: 0.67
Batch: 240; loss: 1.22; acc: 0.69
Batch: 260; loss: 1.1; acc: 0.7
Batch: 280; loss: 1.08; acc: 0.73
Batch: 300; loss: 1.18; acc: 0.73
Batch: 320; loss: 1.06; acc: 0.77
Batch: 340; loss: 1.11; acc: 0.75
Batch: 360; loss: 1.28; acc: 0.66
Batch: 380; loss: 1.16; acc: 0.66
Batch: 400; loss: 1.12; acc: 0.69
Batch: 420; loss: 1.06; acc: 0.75
Batch: 440; loss: 1.02; acc: 0.77
Batch: 460; loss: 1.07; acc: 0.72
Batch: 480; loss: 1.05; acc: 0.77
Batch: 500; loss: 1.14; acc: 0.73
Batch: 520; loss: 1.22; acc: 0.67
Batch: 540; loss: 1.11; acc: 0.72
Batch: 560; loss: 1.0; acc: 0.8
Batch: 580; loss: 1.14; acc: 0.69
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 1.06; acc: 0.75
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.13; acc: 0.67
Batch: 680; loss: 1.06; acc: 0.73
Batch: 700; loss: 1.15; acc: 0.64
Batch: 720; loss: 1.04; acc: 0.78
Batch: 740; loss: 1.11; acc: 0.67
Batch: 760; loss: 1.06; acc: 0.75
Batch: 780; loss: 1.12; acc: 0.78
Train Epoch over. train_loss: 1.14; train_accuracy: 0.7 

0.0001702053123153746
0.00016448801034130156
Batch: 0; loss: 1.12; acc: 0.77
Batch: 20; loss: 1.16; acc: 0.73
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 0.98; acc: 0.78
Batch: 100; loss: 0.93; acc: 0.81
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 0.87; acc: 0.88
Val Epoch over. val_loss: 1.0793295540627401; val_accuracy: 0.7390525477707006 

The current subspace-distance is: 0.00016448801034130156 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.06; acc: 0.73
Batch: 20; loss: 1.26; acc: 0.69
Batch: 40; loss: 1.3; acc: 0.62
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.77
Batch: 160; loss: 1.14; acc: 0.73
Batch: 180; loss: 1.22; acc: 0.66
Batch: 200; loss: 1.09; acc: 0.77
Batch: 220; loss: 1.19; acc: 0.7
Batch: 240; loss: 1.17; acc: 0.67
Batch: 260; loss: 1.08; acc: 0.73
Batch: 280; loss: 1.19; acc: 0.7
Batch: 300; loss: 1.11; acc: 0.7
Batch: 320; loss: 1.23; acc: 0.62
Batch: 340; loss: 1.26; acc: 0.66
Batch: 360; loss: 0.99; acc: 0.8
Batch: 380; loss: 1.03; acc: 0.81
Batch: 400; loss: 0.99; acc: 0.73
Batch: 420; loss: 1.0; acc: 0.77
Batch: 440; loss: 0.94; acc: 0.81
Batch: 460; loss: 1.22; acc: 0.64
Batch: 480; loss: 1.13; acc: 0.78
Batch: 500; loss: 1.11; acc: 0.73
Batch: 520; loss: 1.19; acc: 0.67
Batch: 540; loss: 1.07; acc: 0.75
Batch: 560; loss: 0.96; acc: 0.8
Batch: 580; loss: 1.28; acc: 0.66
Batch: 600; loss: 1.04; acc: 0.78
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.29; acc: 0.59
Batch: 660; loss: 1.06; acc: 0.75
Batch: 680; loss: 1.06; acc: 0.73
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 1.26; acc: 0.64
Batch: 740; loss: 0.95; acc: 0.8
Batch: 760; loss: 1.06; acc: 0.72
Batch: 780; loss: 1.1; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.7 

0.00017309981922153383
0.0001672164216870442
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.14; acc: 0.7
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 0.95; acc: 0.78
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.89
Val Epoch over. val_loss: 1.0601447740937495; val_accuracy: 0.7425358280254777 

The current subspace-distance is: 0.0001672164216870442 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.34; acc: 0.59
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 1.13; acc: 0.75
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.07; acc: 0.72
Batch: 140; loss: 1.18; acc: 0.66
Batch: 160; loss: 1.33; acc: 0.66
Batch: 180; loss: 1.15; acc: 0.69
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 1.28; acc: 0.66
Batch: 240; loss: 1.17; acc: 0.67
Batch: 260; loss: 1.34; acc: 0.61
Batch: 280; loss: 1.11; acc: 0.7
Batch: 300; loss: 1.04; acc: 0.72
Batch: 320; loss: 1.4; acc: 0.62
Batch: 340; loss: 1.28; acc: 0.67
Batch: 360; loss: 0.98; acc: 0.83
Batch: 380; loss: 1.02; acc: 0.78
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.27; acc: 0.69
Batch: 440; loss: 1.12; acc: 0.73
Batch: 460; loss: 1.15; acc: 0.66
Batch: 480; loss: 1.25; acc: 0.58
Batch: 500; loss: 1.16; acc: 0.77
Batch: 520; loss: 1.15; acc: 0.73
Batch: 540; loss: 1.24; acc: 0.69
Batch: 560; loss: 1.04; acc: 0.77
Batch: 580; loss: 1.05; acc: 0.78
Batch: 600; loss: 1.22; acc: 0.61
Batch: 620; loss: 1.13; acc: 0.69
Batch: 640; loss: 1.21; acc: 0.67
Batch: 660; loss: 1.18; acc: 0.66
Batch: 680; loss: 1.04; acc: 0.78
Batch: 700; loss: 1.23; acc: 0.7
Batch: 720; loss: 1.04; acc: 0.78
Batch: 740; loss: 1.16; acc: 0.67
Batch: 760; loss: 0.96; acc: 0.7
Batch: 780; loss: 1.02; acc: 0.77
Train Epoch over. train_loss: 1.13; train_accuracy: 0.7 

0.00017645632033236325
0.000169137871125713
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.96; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 0.83; acc: 0.88
Val Epoch over. val_loss: 1.0577972557893984; val_accuracy: 0.7421377388535032 

The current subspace-distance is: 0.000169137871125713 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.04; acc: 0.72
Batch: 40; loss: 1.07; acc: 0.78
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.25; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.64
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 1.18; acc: 0.64
Batch: 160; loss: 1.15; acc: 0.7
Batch: 180; loss: 1.14; acc: 0.7
Batch: 200; loss: 1.17; acc: 0.62
Batch: 220; loss: 1.11; acc: 0.73
Batch: 240; loss: 1.03; acc: 0.73
Batch: 260; loss: 1.12; acc: 0.67
Batch: 280; loss: 1.13; acc: 0.72
Batch: 300; loss: 1.23; acc: 0.69
Batch: 320; loss: 0.88; acc: 0.8
Batch: 340; loss: 1.04; acc: 0.7
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.23; acc: 0.61
Batch: 400; loss: 1.16; acc: 0.7
Batch: 420; loss: 1.1; acc: 0.72
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.75
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.07; acc: 0.77
Batch: 540; loss: 1.12; acc: 0.67
Batch: 560; loss: 1.26; acc: 0.59
Batch: 580; loss: 0.96; acc: 0.77
Batch: 600; loss: 1.0; acc: 0.75
Batch: 620; loss: 1.2; acc: 0.73
Batch: 640; loss: 1.1; acc: 0.69
Batch: 660; loss: 1.2; acc: 0.64
Batch: 680; loss: 1.16; acc: 0.69
Batch: 700; loss: 1.06; acc: 0.72
Batch: 720; loss: 1.17; acc: 0.69
Batch: 740; loss: 1.04; acc: 0.73
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.00018003889999818057
0.000173296793946065
Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 0.96; acc: 0.78
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 1.18; acc: 0.62
Batch: 140; loss: 0.81; acc: 0.89
Val Epoch over. val_loss: 1.0590106738600762; val_accuracy: 0.7377587579617835 

The current subspace-distance is: 0.000173296793946065 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 1.11; acc: 0.7
Batch: 60; loss: 1.29; acc: 0.59
Batch: 80; loss: 1.18; acc: 0.67
Batch: 100; loss: 1.24; acc: 0.69
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 1.03; acc: 0.77
Batch: 160; loss: 1.19; acc: 0.59
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 1.11; acc: 0.64
Batch: 220; loss: 1.09; acc: 0.75
Batch: 240; loss: 1.02; acc: 0.77
Batch: 260; loss: 1.07; acc: 0.78
Batch: 280; loss: 1.14; acc: 0.69
Batch: 300; loss: 0.91; acc: 0.73
Batch: 320; loss: 1.11; acc: 0.67
Batch: 340; loss: 1.29; acc: 0.64
Batch: 360; loss: 1.06; acc: 0.73
Batch: 380; loss: 1.09; acc: 0.72
Batch: 400; loss: 1.15; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.72
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 1.01; acc: 0.72
Batch: 480; loss: 1.22; acc: 0.64
Batch: 500; loss: 1.3; acc: 0.69
Batch: 520; loss: 1.11; acc: 0.67
Batch: 540; loss: 1.01; acc: 0.75
Batch: 560; loss: 0.93; acc: 0.77
Batch: 580; loss: 1.07; acc: 0.77
Batch: 600; loss: 1.04; acc: 0.67
Batch: 620; loss: 1.04; acc: 0.77
Batch: 640; loss: 1.1; acc: 0.66
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.09; acc: 0.69
Batch: 700; loss: 0.98; acc: 0.77
Batch: 720; loss: 1.15; acc: 0.67
Batch: 740; loss: 1.09; acc: 0.7
Batch: 760; loss: 1.15; acc: 0.73
Batch: 780; loss: 1.23; acc: 0.66
Train Epoch over. train_loss: 1.12; train_accuracy: 0.71 

0.0001821778860175982
0.00017621961887925863
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 0.8; acc: 0.89
Val Epoch over. val_loss: 1.040459790427214; val_accuracy: 0.7437300955414012 

The current subspace-distance is: 0.00017621961887925863 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 1.17; acc: 0.67
Batch: 60; loss: 0.99; acc: 0.8
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.95; acc: 0.78
Batch: 160; loss: 1.0; acc: 0.75
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 1.07; acc: 0.72
Batch: 220; loss: 1.18; acc: 0.7
Batch: 240; loss: 1.16; acc: 0.66
Batch: 260; loss: 1.28; acc: 0.58
Batch: 280; loss: 1.21; acc: 0.69
Batch: 300; loss: 1.11; acc: 0.72
Batch: 320; loss: 1.29; acc: 0.59
Batch: 340; loss: 1.08; acc: 0.73
Batch: 360; loss: 1.06; acc: 0.67
Batch: 380; loss: 1.39; acc: 0.58
Batch: 400; loss: 1.16; acc: 0.72
Batch: 420; loss: 1.03; acc: 0.72
Batch: 440; loss: 1.05; acc: 0.75
Batch: 460; loss: 1.33; acc: 0.58
Batch: 480; loss: 1.13; acc: 0.69
Batch: 500; loss: 1.02; acc: 0.77
Batch: 520; loss: 1.33; acc: 0.58
Batch: 540; loss: 0.89; acc: 0.75
Batch: 560; loss: 1.21; acc: 0.66
Batch: 580; loss: 1.15; acc: 0.73
Batch: 600; loss: 1.19; acc: 0.69
Batch: 620; loss: 0.95; acc: 0.78
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.08; acc: 0.73
Batch: 680; loss: 1.16; acc: 0.66
Batch: 700; loss: 0.93; acc: 0.78
Batch: 720; loss: 1.05; acc: 0.67
Batch: 740; loss: 1.21; acc: 0.69
Batch: 760; loss: 1.04; acc: 0.7
Batch: 780; loss: 1.01; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00018666687537916005
0.00017785411910153925
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 1.16; acc: 0.64
Batch: 140; loss: 0.8; acc: 0.88
Val Epoch over. val_loss: 1.0413872846372567; val_accuracy: 0.7424363057324841 

The current subspace-distance is: 0.00017785411910153925 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.98; acc: 0.81
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.62
Batch: 100; loss: 1.19; acc: 0.67
Batch: 120; loss: 1.12; acc: 0.77
Batch: 140; loss: 0.96; acc: 0.77
Batch: 160; loss: 1.11; acc: 0.66
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 1.37; acc: 0.61
Batch: 220; loss: 1.01; acc: 0.72
Batch: 240; loss: 1.05; acc: 0.73
Batch: 260; loss: 1.01; acc: 0.78
Batch: 280; loss: 1.02; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.72
Batch: 320; loss: 1.15; acc: 0.72
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 1.03; acc: 0.72
Batch: 380; loss: 0.99; acc: 0.78
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.04; acc: 0.8
Batch: 440; loss: 0.99; acc: 0.78
Batch: 460; loss: 1.17; acc: 0.7
Batch: 480; loss: 1.36; acc: 0.59
Batch: 500; loss: 0.95; acc: 0.77
Batch: 520; loss: 1.04; acc: 0.72
Batch: 540; loss: 0.94; acc: 0.75
Batch: 560; loss: 1.01; acc: 0.77
Batch: 580; loss: 1.15; acc: 0.66
Batch: 600; loss: 0.95; acc: 0.78
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.14; acc: 0.73
Batch: 660; loss: 1.02; acc: 0.75
Batch: 680; loss: 1.05; acc: 0.75
Batch: 700; loss: 1.05; acc: 0.73
Batch: 720; loss: 1.05; acc: 0.73
Batch: 740; loss: 1.28; acc: 0.62
Batch: 760; loss: 1.19; acc: 0.7
Batch: 780; loss: 0.97; acc: 0.73
Train Epoch over. train_loss: 1.11; train_accuracy: 0.71 

0.00018739856022875756
0.00017849162395577878
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 0.8; acc: 0.88
Val Epoch over. val_loss: 1.0424571025903058; val_accuracy: 0.743531050955414 

The current subspace-distance is: 0.00017849162395577878 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.15; acc: 0.69
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 0.91; acc: 0.84
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 0.95; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.92; acc: 0.81
Batch: 160; loss: 1.09; acc: 0.64
Batch: 180; loss: 1.13; acc: 0.7
Batch: 200; loss: 1.14; acc: 0.7
Batch: 220; loss: 1.12; acc: 0.7
Batch: 240; loss: 1.19; acc: 0.67
Batch: 260; loss: 1.14; acc: 0.7
Batch: 280; loss: 1.06; acc: 0.77
Batch: 300; loss: 1.03; acc: 0.73
Batch: 320; loss: 1.13; acc: 0.67
Batch: 340; loss: 1.09; acc: 0.67
Batch: 360; loss: 1.33; acc: 0.61
Batch: 380; loss: 1.1; acc: 0.67
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 1.19; acc: 0.72
Batch: 440; loss: 1.14; acc: 0.7
Batch: 460; loss: 1.12; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.72
Batch: 520; loss: 1.13; acc: 0.67
Batch: 540; loss: 1.04; acc: 0.72
Batch: 560; loss: 1.03; acc: 0.75
Batch: 580; loss: 1.16; acc: 0.73
Batch: 600; loss: 1.02; acc: 0.73
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 0.98; acc: 0.77
Batch: 660; loss: 1.17; acc: 0.75
Batch: 680; loss: 1.11; acc: 0.67
Batch: 700; loss: 1.12; acc: 0.69
Batch: 720; loss: 1.15; acc: 0.69
Batch: 740; loss: 1.09; acc: 0.7
Batch: 760; loss: 1.12; acc: 0.72
Batch: 780; loss: 1.01; acc: 0.75
Train Epoch over. train_loss: 1.11; train_accuracy: 0.71 

0.00018674966122489423
0.0001787211949704215
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 1.16; acc: 0.62
Batch: 140; loss: 0.79; acc: 0.89
Val Epoch over. val_loss: 1.0394433831713001; val_accuracy: 0.7430334394904459 

The current subspace-distance is: 0.0001787211949704215 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.3; acc: 0.67
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 1.21; acc: 0.67
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.03; acc: 0.77
Batch: 140; loss: 1.28; acc: 0.64
Batch: 160; loss: 1.02; acc: 0.81
Batch: 180; loss: 1.24; acc: 0.67
Batch: 200; loss: 1.09; acc: 0.7
Batch: 220; loss: 1.19; acc: 0.67
Batch: 240; loss: 1.04; acc: 0.72
Batch: 260; loss: 1.03; acc: 0.78
Batch: 280; loss: 1.22; acc: 0.59
Batch: 300; loss: 1.18; acc: 0.66
Batch: 320; loss: 1.0; acc: 0.75
Batch: 340; loss: 0.99; acc: 0.77
Batch: 360; loss: 1.2; acc: 0.64
Batch: 380; loss: 1.06; acc: 0.73
Batch: 400; loss: 1.13; acc: 0.7
Batch: 420; loss: 1.0; acc: 0.8
Batch: 440; loss: 0.97; acc: 0.78
Batch: 460; loss: 1.03; acc: 0.67
Batch: 480; loss: 0.93; acc: 0.75
Batch: 500; loss: 1.16; acc: 0.67
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.11; acc: 0.7
Batch: 560; loss: 1.13; acc: 0.73
Batch: 580; loss: 1.33; acc: 0.64
Batch: 600; loss: 1.08; acc: 0.75
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.03; acc: 0.75
Batch: 660; loss: 1.17; acc: 0.66
Batch: 680; loss: 1.17; acc: 0.66
Batch: 700; loss: 1.11; acc: 0.72
Batch: 720; loss: 1.11; acc: 0.69
Batch: 740; loss: 1.07; acc: 0.77
Batch: 760; loss: 1.07; acc: 0.69
Batch: 780; loss: 0.92; acc: 0.8
Train Epoch over. train_loss: 1.11; train_accuracy: 0.71 

0.0001860683405539021
0.00017820655193645507
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 1.15; acc: 0.64
Batch: 140; loss: 0.8; acc: 0.89
Val Epoch over. val_loss: 1.042405911691629; val_accuracy: 0.7409434713375797 

The current subspace-distance is: 0.00017820655193645507 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.69
Batch: 20; loss: 1.14; acc: 0.73
Batch: 40; loss: 1.12; acc: 0.61
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 1.16; acc: 0.62
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 1.03; acc: 0.73
Batch: 160; loss: 1.16; acc: 0.7
Batch: 180; loss: 1.17; acc: 0.64
Batch: 200; loss: 1.2; acc: 0.69
Batch: 220; loss: 1.21; acc: 0.64
Batch: 240; loss: 1.0; acc: 0.84
Batch: 260; loss: 1.12; acc: 0.72
Batch: 280; loss: 1.12; acc: 0.62
Batch: 300; loss: 1.25; acc: 0.59
Batch: 320; loss: 0.94; acc: 0.83
Batch: 340; loss: 0.98; acc: 0.75
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.1; acc: 0.73
Batch: 420; loss: 1.13; acc: 0.64
Batch: 440; loss: 0.98; acc: 0.75
Batch: 460; loss: 0.94; acc: 0.8
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 1.2; acc: 0.72
Batch: 520; loss: 0.95; acc: 0.81
Batch: 540; loss: 1.07; acc: 0.72
Batch: 560; loss: 1.08; acc: 0.67
Batch: 580; loss: 1.18; acc: 0.67
Batch: 600; loss: 1.14; acc: 0.72
Batch: 620; loss: 1.03; acc: 0.72
Batch: 640; loss: 0.92; acc: 0.72
Batch: 660; loss: 1.08; acc: 0.69
Batch: 680; loss: 0.98; acc: 0.78
Batch: 700; loss: 1.16; acc: 0.67
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 0.94; acc: 0.78
Batch: 760; loss: 1.23; acc: 0.64
Batch: 780; loss: 1.14; acc: 0.73
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00018745558918453753
0.00017775209562387317
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.91; acc: 0.8
Batch: 120; loss: 1.17; acc: 0.66
Batch: 140; loss: 0.8; acc: 0.89
Val Epoch over. val_loss: 1.0406689617284544; val_accuracy: 0.7431329617834395 

The current subspace-distance is: 0.00017775209562387317 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.32; acc: 0.62
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.05; acc: 0.66
Batch: 120; loss: 1.03; acc: 0.77
Batch: 140; loss: 1.22; acc: 0.7
Batch: 160; loss: 1.03; acc: 0.73
Batch: 180; loss: 1.19; acc: 0.62
Batch: 200; loss: 1.13; acc: 0.77
Batch: 220; loss: 1.26; acc: 0.58
Batch: 240; loss: 1.24; acc: 0.64
Batch: 260; loss: 1.01; acc: 0.73
Batch: 280; loss: 1.14; acc: 0.67
Batch: 300; loss: 0.98; acc: 0.81
Batch: 320; loss: 1.19; acc: 0.69
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.39; acc: 0.61
Batch: 380; loss: 1.03; acc: 0.69
Batch: 400; loss: 1.07; acc: 0.7
Batch: 420; loss: 1.14; acc: 0.67
Batch: 440; loss: 1.04; acc: 0.75
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 1.21; acc: 0.62
Batch: 500; loss: 1.08; acc: 0.75
Batch: 520; loss: 1.08; acc: 0.72
Batch: 540; loss: 1.18; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 0.87; acc: 0.81
Batch: 600; loss: 1.14; acc: 0.67
Batch: 620; loss: 1.13; acc: 0.66
Batch: 640; loss: 1.04; acc: 0.73
Batch: 660; loss: 1.12; acc: 0.67
Batch: 680; loss: 0.98; acc: 0.75
Batch: 700; loss: 1.02; acc: 0.72
Batch: 720; loss: 1.23; acc: 0.64
Batch: 740; loss: 0.92; acc: 0.78
Batch: 760; loss: 1.24; acc: 0.66
Batch: 780; loss: 1.0; acc: 0.75
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.0001842585188569501
0.00017549055337440223
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 0.96; acc: 0.8
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 1.16; acc: 0.62
Batch: 140; loss: 0.79; acc: 0.89
Val Epoch over. val_loss: 1.039950613383275; val_accuracy: 0.7436305732484076 

The current subspace-distance is: 0.00017549055337440223 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.97; acc: 0.73
Batch: 20; loss: 0.89; acc: 0.86
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.17; acc: 0.66
Batch: 80; loss: 1.06; acc: 0.78
Batch: 100; loss: 1.37; acc: 0.58
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 0.99; acc: 0.77
Batch: 160; loss: 1.1; acc: 0.7
Batch: 180; loss: 1.2; acc: 0.7
Batch: 200; loss: 1.22; acc: 0.7
Batch: 220; loss: 0.89; acc: 0.8
Batch: 240; loss: 1.1; acc: 0.69
Batch: 260; loss: 1.13; acc: 0.64
Batch: 280; loss: 1.17; acc: 0.62
Batch: 300; loss: 1.08; acc: 0.77
Batch: 320; loss: 1.0; acc: 0.8
Batch: 340; loss: 1.16; acc: 0.72
Batch: 360; loss: 0.98; acc: 0.8
Batch: 380; loss: 1.11; acc: 0.75
Batch: 400; loss: 1.05; acc: 0.72
Batch: 420; loss: 1.06; acc: 0.73
Batch: 440; loss: 1.25; acc: 0.58
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 1.11; acc: 0.7
Batch: 500; loss: 1.13; acc: 0.67
Batch: 520; loss: 1.04; acc: 0.75
Batch: 540; loss: 1.02; acc: 0.72
Batch: 560; loss: 1.08; acc: 0.7
Batch: 580; loss: 1.1; acc: 0.69
Batch: 600; loss: 1.11; acc: 0.67
Batch: 620; loss: 1.15; acc: 0.69
Batch: 640; loss: 1.12; acc: 0.73
Batch: 660; loss: 1.21; acc: 0.67
Batch: 680; loss: 1.07; acc: 0.73
Batch: 700; loss: 1.1; acc: 0.72
Batch: 720; loss: 0.99; acc: 0.7
Batch: 740; loss: 1.25; acc: 0.69
Batch: 760; loss: 1.18; acc: 0.64
Batch: 780; loss: 1.02; acc: 0.77
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00018952814571093768
0.00018233541049994528
Batch: 0; loss: 1.09; acc: 0.75
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.95; acc: 0.78
Batch: 100; loss: 0.91; acc: 0.8
Batch: 120; loss: 1.16; acc: 0.62
Batch: 140; loss: 0.79; acc: 0.89
Val Epoch over. val_loss: 1.0461211682884557; val_accuracy: 0.7374601910828026 

The current subspace-distance is: 0.00018233541049994528 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.29; acc: 0.66
Batch: 60; loss: 1.1; acc: 0.75
Batch: 80; loss: 1.18; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 1.14; acc: 0.67
Batch: 160; loss: 1.05; acc: 0.72
Batch: 180; loss: 1.09; acc: 0.69
Batch: 200; loss: 1.25; acc: 0.66
Batch: 220; loss: 1.17; acc: 0.67
Batch: 240; loss: 0.95; acc: 0.81
Batch: 260; loss: 1.12; acc: 0.77
Batch: 280; loss: 1.17; acc: 0.66
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.26; acc: 0.62
Batch: 340; loss: 1.08; acc: 0.75
Batch: 360; loss: 1.18; acc: 0.62
Batch: 380; loss: 1.14; acc: 0.7
Batch: 400; loss: 1.08; acc: 0.69
Batch: 420; loss: 1.0; acc: 0.7
Batch: 440; loss: 1.15; acc: 0.66
Batch: 460; loss: 1.2; acc: 0.64
Batch: 480; loss: 1.18; acc: 0.69
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 1.05; acc: 0.69
Batch: 540; loss: 0.95; acc: 0.83
Batch: 560; loss: 1.06; acc: 0.78
Batch: 580; loss: 1.07; acc: 0.75
Batch: 600; loss: 1.06; acc: 0.73
Batch: 620; loss: 1.12; acc: 0.66
Batch: 640; loss: 1.06; acc: 0.7
Batch: 660; loss: 1.08; acc: 0.67
Batch: 680; loss: 1.13; acc: 0.67
Batch: 700; loss: 1.02; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 0.86; acc: 0.84
Batch: 760; loss: 1.28; acc: 0.62
Batch: 780; loss: 1.28; acc: 0.62
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00018607596575748175
0.0001826117222663015
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.94; acc: 0.75
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.83
Batch: 120; loss: 1.15; acc: 0.66
Batch: 140; loss: 0.78; acc: 0.88
Val Epoch over. val_loss: 1.0224784532929683; val_accuracy: 0.7445262738853503 

The current subspace-distance is: 0.0001826117222663015 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.7
Batch: 40; loss: 1.06; acc: 0.66
Batch: 60; loss: 0.98; acc: 0.83
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 1.28; acc: 0.59
Batch: 160; loss: 1.1; acc: 0.69
Batch: 180; loss: 1.2; acc: 0.66
Batch: 200; loss: 0.97; acc: 0.78
Batch: 220; loss: 1.0; acc: 0.77
Batch: 240; loss: 1.2; acc: 0.58
Batch: 260; loss: 1.05; acc: 0.73
Batch: 280; loss: 1.01; acc: 0.72
Batch: 300; loss: 1.09; acc: 0.67
Batch: 320; loss: 1.07; acc: 0.7
Batch: 340; loss: 0.86; acc: 0.77
Batch: 360; loss: 1.07; acc: 0.77
Batch: 380; loss: 1.2; acc: 0.66
Batch: 400; loss: 1.07; acc: 0.72
Batch: 420; loss: 1.21; acc: 0.7
Batch: 440; loss: 1.13; acc: 0.67
Batch: 460; loss: 1.22; acc: 0.62
Batch: 480; loss: 1.09; acc: 0.7
Batch: 500; loss: 0.99; acc: 0.8
Batch: 520; loss: 1.19; acc: 0.72
Batch: 540; loss: 1.01; acc: 0.73
Batch: 560; loss: 1.01; acc: 0.75
Batch: 580; loss: 1.18; acc: 0.69
Batch: 600; loss: 1.06; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 0.94; acc: 0.73
Batch: 660; loss: 1.18; acc: 0.66
Batch: 680; loss: 0.93; acc: 0.8
Batch: 700; loss: 1.04; acc: 0.72
Batch: 720; loss: 1.0; acc: 0.72
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 1.04; acc: 0.66
Batch: 780; loss: 1.1; acc: 0.75
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00018535793060436845
0.0001802294427761808
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.75
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.91; acc: 0.78
Batch: 120; loss: 1.16; acc: 0.61
Batch: 140; loss: 0.78; acc: 0.89
Val Epoch over. val_loss: 1.032076262744369; val_accuracy: 0.740843949044586 

The current subspace-distance is: 0.0001802294427761808 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.25; acc: 0.64
Batch: 20; loss: 1.18; acc: 0.73
Batch: 40; loss: 1.28; acc: 0.62
Batch: 60; loss: 0.96; acc: 0.8
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.14; acc: 0.69
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 1.41; acc: 0.58
Batch: 160; loss: 1.29; acc: 0.61
Batch: 180; loss: 1.15; acc: 0.64
Batch: 200; loss: 1.07; acc: 0.69
Batch: 220; loss: 0.96; acc: 0.75
Batch: 240; loss: 1.12; acc: 0.7
Batch: 260; loss: 1.17; acc: 0.69
Batch: 280; loss: 1.14; acc: 0.66
Batch: 300; loss: 1.07; acc: 0.72
Batch: 320; loss: 1.17; acc: 0.73
Batch: 340; loss: 1.05; acc: 0.75
Batch: 360; loss: 1.02; acc: 0.7
Batch: 380; loss: 1.21; acc: 0.66
Batch: 400; loss: 0.87; acc: 0.83
Batch: 420; loss: 1.05; acc: 0.78
Batch: 440; loss: 1.07; acc: 0.69
Batch: 460; loss: 1.0; acc: 0.75
Batch: 480; loss: 1.1; acc: 0.73
Batch: 500; loss: 1.05; acc: 0.72
Batch: 520; loss: 1.15; acc: 0.75
Batch: 540; loss: 1.09; acc: 0.72
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 1.04; acc: 0.75
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.02; acc: 0.81
Batch: 640; loss: 0.95; acc: 0.8
Batch: 660; loss: 0.93; acc: 0.81
Batch: 680; loss: 1.18; acc: 0.7
Batch: 700; loss: 0.91; acc: 0.75
Batch: 720; loss: 1.14; acc: 0.69
Batch: 740; loss: 1.0; acc: 0.78
Batch: 760; loss: 1.07; acc: 0.73
Batch: 780; loss: 1.04; acc: 0.73
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00019017545855604112
0.00018299443763680756
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 0.94; acc: 0.77
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 0.76; acc: 0.88
Val Epoch over. val_loss: 1.0162427869572002; val_accuracy: 0.7407444267515924 

The current subspace-distance is: 0.00018299443763680756 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_11_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.21

The number of parameters is: 262995

The number of individual parameters is:

10
180
10
10
15
32700
15
15
30
98100
30
30
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 52598995
elements in E: 52599000
fraction nonzero: 0.9999999049411585
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.14
Batch: 20; loss: 2.15; acc: 0.17
Batch: 40; loss: 2.14; acc: 0.19
Batch: 60; loss: 2.01; acc: 0.31
Batch: 80; loss: 1.96; acc: 0.34
Batch: 100; loss: 2.0; acc: 0.36
Batch: 120; loss: 1.78; acc: 0.59
Batch: 140; loss: 1.83; acc: 0.48
Batch: 160; loss: 1.78; acc: 0.5
Batch: 180; loss: 1.73; acc: 0.56
Batch: 200; loss: 1.78; acc: 0.52
Batch: 220; loss: 1.74; acc: 0.61
Batch: 240; loss: 1.76; acc: 0.48
Batch: 260; loss: 1.69; acc: 0.66
Batch: 280; loss: 1.66; acc: 0.66
Batch: 300; loss: 1.7; acc: 0.53
Batch: 320; loss: 1.65; acc: 0.55
Batch: 340; loss: 1.6; acc: 0.66
Batch: 360; loss: 1.68; acc: 0.58
Batch: 380; loss: 1.58; acc: 0.69
Batch: 400; loss: 1.57; acc: 0.61
Batch: 420; loss: 1.65; acc: 0.55
Batch: 440; loss: 1.6; acc: 0.62
Batch: 460; loss: 1.59; acc: 0.58
Batch: 480; loss: 1.49; acc: 0.69
Batch: 500; loss: 1.48; acc: 0.67
Batch: 520; loss: 1.49; acc: 0.62
Batch: 540; loss: 1.56; acc: 0.61
Batch: 560; loss: 1.54; acc: 0.64
Batch: 580; loss: 1.53; acc: 0.64
Batch: 600; loss: 1.45; acc: 0.8
Batch: 620; loss: 1.43; acc: 0.72
Batch: 640; loss: 1.52; acc: 0.67
Batch: 660; loss: 1.52; acc: 0.61
Batch: 680; loss: 1.52; acc: 0.62
Batch: 700; loss: 1.4; acc: 0.67
Batch: 720; loss: 1.37; acc: 0.75
Batch: 740; loss: 1.54; acc: 0.64
Batch: 760; loss: 1.42; acc: 0.72
Batch: 780; loss: 1.48; acc: 0.55
Train Epoch over. train_loss: 1.66; train_accuracy: 0.57 

5.435288403532468e-05
4.958585122949444e-05
Batch: 0; loss: 1.41; acc: 0.66
Batch: 20; loss: 1.56; acc: 0.55
Batch: 40; loss: 1.15; acc: 0.88
Batch: 60; loss: 1.38; acc: 0.67
Batch: 80; loss: 1.2; acc: 0.84
Batch: 100; loss: 1.37; acc: 0.78
Batch: 120; loss: 1.46; acc: 0.67
Batch: 140; loss: 1.24; acc: 0.8
Val Epoch over. val_loss: 1.3852863972354088; val_accuracy: 0.7156648089171974 

The current subspace-distance is: 4.958585122949444e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.67
Batch: 20; loss: 1.51; acc: 0.58
Batch: 40; loss: 1.44; acc: 0.66
Batch: 60; loss: 1.36; acc: 0.72
Batch: 80; loss: 1.36; acc: 0.7
Batch: 100; loss: 1.42; acc: 0.67
Batch: 120; loss: 1.39; acc: 0.72
Batch: 140; loss: 1.4; acc: 0.7
Batch: 160; loss: 1.29; acc: 0.84
Batch: 180; loss: 1.35; acc: 0.77
Batch: 200; loss: 1.32; acc: 0.77
Batch: 220; loss: 1.24; acc: 0.8
Batch: 240; loss: 1.29; acc: 0.72
Batch: 260; loss: 1.33; acc: 0.73
Batch: 280; loss: 1.36; acc: 0.73
Batch: 300; loss: 1.3; acc: 0.73
Batch: 320; loss: 1.47; acc: 0.73
Batch: 340; loss: 1.37; acc: 0.67
Batch: 360; loss: 1.37; acc: 0.64
Batch: 380; loss: 1.34; acc: 0.66
Batch: 400; loss: 1.24; acc: 0.77
Batch: 420; loss: 1.32; acc: 0.72
Batch: 440; loss: 1.27; acc: 0.72
Batch: 460; loss: 1.22; acc: 0.8
Batch: 480; loss: 1.31; acc: 0.7
Batch: 500; loss: 1.29; acc: 0.7
Batch: 520; loss: 1.29; acc: 0.72
Batch: 540; loss: 1.34; acc: 0.66
Batch: 560; loss: 1.23; acc: 0.73
Batch: 580; loss: 1.35; acc: 0.64
Batch: 600; loss: 1.23; acc: 0.69
Batch: 620; loss: 1.21; acc: 0.8
Batch: 640; loss: 1.23; acc: 0.78
Batch: 660; loss: 1.44; acc: 0.67
Batch: 680; loss: 1.21; acc: 0.78
Batch: 700; loss: 1.14; acc: 0.77
Batch: 720; loss: 1.24; acc: 0.75
Batch: 740; loss: 1.13; acc: 0.77
Batch: 760; loss: 1.14; acc: 0.8
Batch: 780; loss: 1.32; acc: 0.72
Train Epoch over. train_loss: 1.32; train_accuracy: 0.72 

7.621158147230744e-05
7.130917947506532e-05
Batch: 0; loss: 1.29; acc: 0.72
Batch: 20; loss: 1.35; acc: 0.64
Batch: 40; loss: 0.98; acc: 0.88
Batch: 60; loss: 1.19; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.86
Batch: 100; loss: 1.16; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.81
Val Epoch over. val_loss: 1.2048557484225861; val_accuracy: 0.7566679936305732 

The current subspace-distance is: 7.130917947506532e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.72
Batch: 40; loss: 1.22; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.67
Batch: 80; loss: 1.29; acc: 0.77
Batch: 100; loss: 1.15; acc: 0.81
Batch: 120; loss: 1.2; acc: 0.77
Batch: 140; loss: 1.24; acc: 0.72
Batch: 160; loss: 1.22; acc: 0.75
Batch: 180; loss: 1.22; acc: 0.69
Batch: 200; loss: 1.21; acc: 0.69
Batch: 220; loss: 1.1; acc: 0.8
Batch: 240; loss: 1.27; acc: 0.72
Batch: 260; loss: 1.16; acc: 0.73
Batch: 280; loss: 1.08; acc: 0.78
Batch: 300; loss: 1.16; acc: 0.77
Batch: 320; loss: 1.25; acc: 0.72
Batch: 340; loss: 1.22; acc: 0.72
Batch: 360; loss: 1.43; acc: 0.61
Batch: 380; loss: 1.24; acc: 0.69
Batch: 400; loss: 1.23; acc: 0.72
Batch: 420; loss: 1.16; acc: 0.81
Batch: 440; loss: 1.21; acc: 0.73
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.12; acc: 0.72
Batch: 500; loss: 1.17; acc: 0.72
Batch: 520; loss: 1.07; acc: 0.78
Batch: 540; loss: 1.2; acc: 0.73
Batch: 560; loss: 1.15; acc: 0.75
Batch: 580; loss: 1.17; acc: 0.75
Batch: 600; loss: 1.31; acc: 0.64
Batch: 620; loss: 1.24; acc: 0.75
Batch: 640; loss: 1.09; acc: 0.77
Batch: 660; loss: 1.1; acc: 0.8
Batch: 680; loss: 1.15; acc: 0.75
Batch: 700; loss: 1.07; acc: 0.81
Batch: 720; loss: 1.21; acc: 0.7
Batch: 740; loss: 1.1; acc: 0.77
Batch: 760; loss: 1.09; acc: 0.78
Batch: 780; loss: 1.23; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.75 

9.468059579376131e-05
9.013188537210226e-05
Batch: 0; loss: 1.22; acc: 0.69
Batch: 20; loss: 1.25; acc: 0.64
Batch: 40; loss: 0.82; acc: 0.92
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.84
Batch: 100; loss: 1.02; acc: 0.88
Batch: 120; loss: 1.17; acc: 0.7
Batch: 140; loss: 0.94; acc: 0.86
Val Epoch over. val_loss: 1.07773171412717; val_accuracy: 0.7787619426751592 

The current subspace-distance is: 9.013188537210226e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.17; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.75
Batch: 80; loss: 1.37; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 1.08; acc: 0.75
Batch: 160; loss: 1.23; acc: 0.67
Batch: 180; loss: 1.04; acc: 0.81
Batch: 200; loss: 1.27; acc: 0.69
Batch: 220; loss: 1.19; acc: 0.7
Batch: 240; loss: 1.11; acc: 0.78
Batch: 260; loss: 1.02; acc: 0.77
Batch: 280; loss: 1.17; acc: 0.64
Batch: 300; loss: 1.02; acc: 0.8
Batch: 320; loss: 1.12; acc: 0.7
Batch: 340; loss: 1.07; acc: 0.75
Batch: 360; loss: 1.15; acc: 0.77
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 0.98; acc: 0.83
Batch: 420; loss: 0.98; acc: 0.78
Batch: 440; loss: 1.15; acc: 0.75
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 0.99; acc: 0.81
Batch: 500; loss: 1.01; acc: 0.84
Batch: 520; loss: 1.21; acc: 0.67
Batch: 540; loss: 1.04; acc: 0.78
Batch: 560; loss: 0.94; acc: 0.83
Batch: 580; loss: 0.98; acc: 0.77
Batch: 600; loss: 0.96; acc: 0.8
Batch: 620; loss: 1.05; acc: 0.73
Batch: 640; loss: 1.1; acc: 0.72
Batch: 660; loss: 1.05; acc: 0.77
Batch: 680; loss: 1.05; acc: 0.77
Batch: 700; loss: 0.95; acc: 0.88
Batch: 720; loss: 1.13; acc: 0.69
Batch: 740; loss: 1.03; acc: 0.75
Batch: 760; loss: 0.91; acc: 0.86
Batch: 780; loss: 0.8; acc: 0.92
Train Epoch over. train_loss: 1.08; train_accuracy: 0.76 

0.00011288398673059419
0.00010755739640444517
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 0.71; acc: 0.94
Batch: 60; loss: 0.95; acc: 0.78
Batch: 80; loss: 0.85; acc: 0.84
Batch: 100; loss: 0.96; acc: 0.88
Batch: 120; loss: 1.09; acc: 0.75
Batch: 140; loss: 0.85; acc: 0.91
Val Epoch over. val_loss: 0.993687988466518; val_accuracy: 0.7907046178343949 

The current subspace-distance is: 0.00010755739640444517 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 0.84; acc: 0.86
Batch: 40; loss: 0.97; acc: 0.84
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 0.98; acc: 0.81
Batch: 100; loss: 0.93; acc: 0.81
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 0.93; acc: 0.83
Batch: 160; loss: 1.06; acc: 0.77
Batch: 180; loss: 1.03; acc: 0.73
Batch: 200; loss: 1.02; acc: 0.78
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.83; acc: 0.89
Batch: 260; loss: 1.06; acc: 0.77
Batch: 280; loss: 0.92; acc: 0.81
Batch: 300; loss: 1.03; acc: 0.72
Batch: 320; loss: 0.97; acc: 0.81
Batch: 340; loss: 0.91; acc: 0.81
Batch: 360; loss: 1.14; acc: 0.72
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 0.89; acc: 0.84
Batch: 420; loss: 1.07; acc: 0.77
Batch: 440; loss: 1.11; acc: 0.7
Batch: 460; loss: 1.02; acc: 0.81
Batch: 480; loss: 0.96; acc: 0.81
Batch: 500; loss: 1.08; acc: 0.77
Batch: 520; loss: 0.99; acc: 0.77
Batch: 540; loss: 0.91; acc: 0.78
Batch: 560; loss: 0.92; acc: 0.81
Batch: 580; loss: 1.02; acc: 0.73
Batch: 600; loss: 1.04; acc: 0.75
Batch: 620; loss: 1.06; acc: 0.69
Batch: 640; loss: 0.85; acc: 0.84
Batch: 660; loss: 0.94; acc: 0.78
Batch: 680; loss: 1.01; acc: 0.8
Batch: 700; loss: 0.86; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.78
Batch: 740; loss: 0.92; acc: 0.78
Batch: 760; loss: 0.91; acc: 0.83
Batch: 780; loss: 0.8; acc: 0.86
Train Epoch over. train_loss: 0.99; train_accuracy: 0.78 

0.00012843686272390187
0.0001206778033520095
Batch: 0; loss: 1.09; acc: 0.69
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.64; acc: 0.89
Batch: 60; loss: 0.89; acc: 0.8
Batch: 80; loss: 0.77; acc: 0.88
Batch: 100; loss: 0.91; acc: 0.89
Batch: 120; loss: 1.04; acc: 0.77
Batch: 140; loss: 0.75; acc: 0.92
Val Epoch over. val_loss: 0.9094842729295135; val_accuracy: 0.8129976114649682 

The current subspace-distance is: 0.0001206778033520095 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.89
Batch: 20; loss: 0.88; acc: 0.81
Batch: 40; loss: 0.94; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 0.96; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.88
Batch: 140; loss: 0.94; acc: 0.81
Batch: 160; loss: 0.91; acc: 0.81
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 0.85; acc: 0.8
Batch: 220; loss: 0.87; acc: 0.77
Batch: 240; loss: 1.05; acc: 0.72
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.9; acc: 0.8
Batch: 300; loss: 0.89; acc: 0.81
Batch: 320; loss: 0.73; acc: 0.89
Batch: 340; loss: 1.04; acc: 0.77
Batch: 360; loss: 0.93; acc: 0.8
Batch: 380; loss: 0.78; acc: 0.83
Batch: 400; loss: 0.94; acc: 0.78
Batch: 420; loss: 0.85; acc: 0.86
Batch: 440; loss: 0.89; acc: 0.81
Batch: 460; loss: 0.98; acc: 0.77
Batch: 480; loss: 0.84; acc: 0.83
Batch: 500; loss: 0.93; acc: 0.8
Batch: 520; loss: 0.9; acc: 0.77
Batch: 540; loss: 0.88; acc: 0.88
Batch: 560; loss: 0.88; acc: 0.83
Batch: 580; loss: 0.94; acc: 0.8
Batch: 600; loss: 0.93; acc: 0.81
Batch: 620; loss: 1.0; acc: 0.67
Batch: 640; loss: 0.86; acc: 0.83
Batch: 660; loss: 0.89; acc: 0.81
Batch: 680; loss: 1.04; acc: 0.72
Batch: 700; loss: 0.93; acc: 0.77
Batch: 720; loss: 0.82; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.86
Batch: 760; loss: 0.86; acc: 0.83
Batch: 780; loss: 0.95; acc: 0.75
Train Epoch over. train_loss: 0.92; train_accuracy: 0.79 

0.0001421998458681628
0.00013744979514740407
Batch: 0; loss: 1.03; acc: 0.73
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.83
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.92
Val Epoch over. val_loss: 0.8439740318401604; val_accuracy: 0.8201632165605095 

The current subspace-distance is: 0.00013744979514740407 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 0.95; acc: 0.78
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 0.91; acc: 0.81
Batch: 80; loss: 0.85; acc: 0.88
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.84
Batch: 140; loss: 0.93; acc: 0.77
Batch: 160; loss: 0.84; acc: 0.81
Batch: 180; loss: 0.83; acc: 0.83
Batch: 200; loss: 0.92; acc: 0.73
Batch: 220; loss: 0.85; acc: 0.8
Batch: 240; loss: 0.88; acc: 0.88
Batch: 260; loss: 0.78; acc: 0.86
Batch: 280; loss: 0.89; acc: 0.78
Batch: 300; loss: 0.97; acc: 0.83
Batch: 320; loss: 0.93; acc: 0.75
Batch: 340; loss: 0.99; acc: 0.73
Batch: 360; loss: 0.92; acc: 0.73
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.78; acc: 0.88
Batch: 420; loss: 0.95; acc: 0.77
Batch: 440; loss: 0.83; acc: 0.78
Batch: 460; loss: 0.96; acc: 0.78
Batch: 480; loss: 0.81; acc: 0.8
Batch: 500; loss: 0.91; acc: 0.8
Batch: 520; loss: 0.84; acc: 0.81
Batch: 540; loss: 0.87; acc: 0.8
Batch: 560; loss: 0.88; acc: 0.78
Batch: 580; loss: 0.86; acc: 0.8
Batch: 600; loss: 0.93; acc: 0.78
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 1.01; acc: 0.72
Batch: 660; loss: 0.82; acc: 0.88
Batch: 680; loss: 0.81; acc: 0.83
Batch: 700; loss: 0.9; acc: 0.8
Batch: 720; loss: 0.91; acc: 0.81
Batch: 740; loss: 0.87; acc: 0.8
Batch: 760; loss: 0.89; acc: 0.8
Batch: 780; loss: 0.79; acc: 0.84
Train Epoch over. train_loss: 0.87; train_accuracy: 0.8 

0.00015358782547991723
0.00014623980678152293
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.66
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.92
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 0.66; acc: 0.91
Val Epoch over. val_loss: 0.7986775247534369; val_accuracy: 0.8233479299363057 

The current subspace-distance is: 0.00014623980678152293 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.88
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 0.74; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.92
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.84
Batch: 140; loss: 0.88; acc: 0.81
Batch: 160; loss: 0.82; acc: 0.86
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 0.82; acc: 0.8
Batch: 220; loss: 0.89; acc: 0.81
Batch: 240; loss: 0.76; acc: 0.83
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.9; acc: 0.75
Batch: 300; loss: 0.75; acc: 0.86
Batch: 320; loss: 0.86; acc: 0.69
Batch: 340; loss: 0.9; acc: 0.73
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.82; acc: 0.81
Batch: 400; loss: 1.07; acc: 0.62
Batch: 420; loss: 0.74; acc: 0.88
Batch: 440; loss: 0.78; acc: 0.84
Batch: 460; loss: 0.84; acc: 0.77
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.9; acc: 0.78
Batch: 520; loss: 0.79; acc: 0.84
Batch: 540; loss: 0.82; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.74; acc: 0.84
Batch: 620; loss: 1.02; acc: 0.72
Batch: 640; loss: 0.85; acc: 0.78
Batch: 660; loss: 0.89; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.89
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.84; acc: 0.81
Batch: 740; loss: 0.84; acc: 0.75
Batch: 760; loss: 0.8; acc: 0.8
Batch: 780; loss: 0.79; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

0.00016499453340657055
0.00015880740829743445
Batch: 0; loss: 0.95; acc: 0.73
Batch: 20; loss: 0.96; acc: 0.64
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.63; acc: 0.91
Val Epoch over. val_loss: 0.7680206270354568; val_accuracy: 0.8279259554140127 

The current subspace-distance is: 0.00015880740829743445 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.86
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.84
Batch: 140; loss: 0.9; acc: 0.69
Batch: 160; loss: 0.78; acc: 0.84
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.83
Batch: 220; loss: 0.84; acc: 0.8
Batch: 240; loss: 0.76; acc: 0.91
Batch: 260; loss: 0.77; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.86
Batch: 300; loss: 0.93; acc: 0.73
Batch: 320; loss: 0.96; acc: 0.75
Batch: 340; loss: 0.86; acc: 0.77
Batch: 360; loss: 0.81; acc: 0.75
Batch: 380; loss: 0.96; acc: 0.77
Batch: 400; loss: 0.73; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.88
Batch: 440; loss: 0.94; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.7
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.88
Batch: 520; loss: 0.74; acc: 0.86
Batch: 540; loss: 0.87; acc: 0.84
Batch: 560; loss: 0.71; acc: 0.86
Batch: 580; loss: 0.8; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.75
Batch: 620; loss: 0.84; acc: 0.75
Batch: 640; loss: 0.89; acc: 0.7
Batch: 660; loss: 0.82; acc: 0.8
Batch: 680; loss: 0.78; acc: 0.86
Batch: 700; loss: 1.02; acc: 0.73
Batch: 720; loss: 0.9; acc: 0.8
Batch: 740; loss: 0.81; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.77
Batch: 780; loss: 0.82; acc: 0.78
Train Epoch over. train_loss: 0.8; train_accuracy: 0.81 

0.00017154624219983816
0.00016373205289710313
Batch: 0; loss: 0.91; acc: 0.77
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.59; acc: 0.91
Val Epoch over. val_loss: 0.7319299926044075; val_accuracy: 0.8336982484076433 

The current subspace-distance is: 0.00016373205289710313 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.91
Batch: 20; loss: 0.84; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.73; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 0.78; acc: 0.8
Batch: 280; loss: 0.78; acc: 0.83
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.88; acc: 0.77
Batch: 360; loss: 0.85; acc: 0.78
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.8
Batch: 420; loss: 0.78; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.86
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.83; acc: 0.8
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.68; acc: 0.88
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.82; acc: 0.78
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.8
Batch: 640; loss: 0.82; acc: 0.77
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.78; acc: 0.73
Batch: 720; loss: 0.88; acc: 0.69
Batch: 740; loss: 0.8; acc: 0.8
Batch: 760; loss: 0.8; acc: 0.83
Batch: 780; loss: 0.82; acc: 0.75
Train Epoch over. train_loss: 0.78; train_accuracy: 0.81 

0.00017894945631269366
0.00017317796300631016
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.57; acc: 0.92
Val Epoch over. val_loss: 0.716717390877426; val_accuracy: 0.8312101910828026 

The current subspace-distance is: 0.00017317796300631016 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.73
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.82; acc: 0.72
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.83; acc: 0.78
Batch: 160; loss: 0.65; acc: 0.86
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.77; acc: 0.77
Batch: 240; loss: 0.67; acc: 0.86
Batch: 260; loss: 0.9; acc: 0.77
Batch: 280; loss: 0.69; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 0.84; acc: 0.81
Batch: 360; loss: 0.86; acc: 0.72
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.83
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.83; acc: 0.77
Batch: 460; loss: 0.86; acc: 0.8
Batch: 480; loss: 0.71; acc: 0.86
Batch: 500; loss: 0.78; acc: 0.78
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.63; acc: 0.91
Batch: 580; loss: 0.74; acc: 0.83
Batch: 600; loss: 0.86; acc: 0.73
Batch: 620; loss: 0.83; acc: 0.8
Batch: 640; loss: 0.71; acc: 0.86
Batch: 660; loss: 0.76; acc: 0.78
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.85; acc: 0.78
Batch: 740; loss: 0.79; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00018240833014715463
0.00017485686112195253
Batch: 0; loss: 0.87; acc: 0.75
Batch: 20; loss: 0.91; acc: 0.69
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.69
Batch: 140; loss: 0.56; acc: 0.92
Val Epoch over. val_loss: 0.7066117743397974; val_accuracy: 0.8346934713375797 

The current subspace-distance is: 0.00017485686112195253 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 0.79; acc: 0.78
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.77
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.74; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.89
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.83; acc: 0.77
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.88; acc: 0.78
Batch: 260; loss: 0.73; acc: 0.89
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.77; acc: 0.73
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.79; acc: 0.84
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.81; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.82; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 0.83; acc: 0.8
Batch: 580; loss: 0.81; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.91
Batch: 640; loss: 0.91; acc: 0.67
Batch: 660; loss: 0.84; acc: 0.8
Batch: 680; loss: 0.78; acc: 0.75
Batch: 700; loss: 0.91; acc: 0.75
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.79; acc: 0.73
Batch: 760; loss: 0.73; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00018261230434291065
0.00017583112639840692
Batch: 0; loss: 0.88; acc: 0.75
Batch: 20; loss: 0.94; acc: 0.67
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 0.56; acc: 0.94
Val Epoch over. val_loss: 0.7086553664723779; val_accuracy: 0.8311106687898089 

The current subspace-distance is: 0.00017583112639840692 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.92
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.88
Batch: 80; loss: 0.85; acc: 0.77
Batch: 100; loss: 0.8; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.89
Batch: 140; loss: 0.76; acc: 0.84
Batch: 160; loss: 0.88; acc: 0.75
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.73; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.79; acc: 0.75
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.67; acc: 0.86
Batch: 420; loss: 0.87; acc: 0.75
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.71; acc: 0.86
Batch: 480; loss: 0.66; acc: 0.86
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 0.86; acc: 0.77
Batch: 640; loss: 0.85; acc: 0.81
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.87; acc: 0.73
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00018533901311457157
0.0001787096116458997
Batch: 0; loss: 0.88; acc: 0.73
Batch: 20; loss: 0.94; acc: 0.66
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.54; acc: 0.94
Val Epoch over. val_loss: 0.7002605457974088; val_accuracy: 0.830812101910828 

The current subspace-distance is: 0.0001787096116458997 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.75
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.77
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.76; acc: 0.83
Batch: 200; loss: 0.96; acc: 0.7
Batch: 220; loss: 0.77; acc: 0.83
Batch: 240; loss: 0.8; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.86
Batch: 360; loss: 0.8; acc: 0.77
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.73; acc: 0.84
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.86; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.91; acc: 0.72
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.81
Batch: 620; loss: 0.71; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.8; acc: 0.8
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.81; acc: 0.84
Batch: 760; loss: 0.79; acc: 0.81
Batch: 780; loss: 0.77; acc: 0.83
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.0001887894031824544
0.00018024229211732745
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.92; acc: 0.69
Batch: 140; loss: 0.55; acc: 0.92
Val Epoch over. val_loss: 0.6978173231243328; val_accuracy: 0.8314092356687898 

The current subspace-distance is: 0.00018024229211732745 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.65; acc: 0.91
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.65; acc: 0.89
Batch: 340; loss: 0.82; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.8; acc: 0.75
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.82; acc: 0.78
Batch: 440; loss: 0.77; acc: 0.83
Batch: 460; loss: 0.84; acc: 0.75
Batch: 480; loss: 0.67; acc: 0.84
Batch: 500; loss: 0.67; acc: 0.88
Batch: 520; loss: 0.74; acc: 0.83
Batch: 540; loss: 0.74; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.82; acc: 0.78
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.9; acc: 0.73
Batch: 640; loss: 0.74; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.76; acc: 0.84
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.75; acc: 0.77
Batch: 740; loss: 0.76; acc: 0.86
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.74; train_accuracy: 0.81 

0.00018905778415501118
0.0001801176549633965
Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 0.92; acc: 0.67
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.69
Batch: 140; loss: 0.54; acc: 0.94
Val Epoch over. val_loss: 0.6902726979771997; val_accuracy: 0.8336982484076433 

The current subspace-distance is: 0.0001801176549633965 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.85; acc: 0.75
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.77; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.89
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.65; acc: 0.91
Batch: 300; loss: 0.81; acc: 0.73
Batch: 320; loss: 0.85; acc: 0.72
Batch: 340; loss: 0.84; acc: 0.78
Batch: 360; loss: 0.89; acc: 0.78
Batch: 380; loss: 0.73; acc: 0.8
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.67; acc: 0.88
Batch: 460; loss: 0.77; acc: 0.77
Batch: 480; loss: 0.73; acc: 0.83
Batch: 500; loss: 0.84; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.68; acc: 0.86
Batch: 560; loss: 0.93; acc: 0.77
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.69; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 0.58; acc: 0.91
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.75; acc: 0.77
Batch: 760; loss: 0.58; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.74; train_accuracy: 0.81 

0.00019211843027733266
0.00018392085621599108
Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.9; acc: 0.67
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.52; acc: 0.94
Val Epoch over. val_loss: 0.680213835588686; val_accuracy: 0.8392714968152867 

The current subspace-distance is: 0.00018392085621599108 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.81; acc: 0.78
Batch: 160; loss: 0.88; acc: 0.72
Batch: 180; loss: 0.9; acc: 0.7
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.8; acc: 0.78
Batch: 300; loss: 0.86; acc: 0.72
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.84; acc: 0.75
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.79; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.91
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.7; acc: 0.75
Batch: 500; loss: 0.67; acc: 0.89
Batch: 520; loss: 0.76; acc: 0.86
Batch: 540; loss: 0.78; acc: 0.77
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.74; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.88
Batch: 680; loss: 0.73; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.82; acc: 0.77
Batch: 740; loss: 0.68; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.81 

0.00019565415277611464
0.00018708404968492687
Batch: 0; loss: 0.86; acc: 0.72
Batch: 20; loss: 0.92; acc: 0.66
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.67
Batch: 140; loss: 0.53; acc: 0.95
Val Epoch over. val_loss: 0.6881407220272502; val_accuracy: 0.8324044585987261 

The current subspace-distance is: 0.00018708404968492687 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.84; acc: 0.81
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.65; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.84; acc: 0.8
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.85; acc: 0.72
Batch: 260; loss: 0.79; acc: 0.84
Batch: 280; loss: 0.78; acc: 0.81
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.91; acc: 0.75
Batch: 400; loss: 0.79; acc: 0.73
Batch: 420; loss: 0.7; acc: 0.78
Batch: 440; loss: 0.89; acc: 0.78
Batch: 460; loss: 0.83; acc: 0.8
Batch: 480; loss: 0.77; acc: 0.81
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.83; acc: 0.7
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.72; acc: 0.8
Batch: 580; loss: 0.86; acc: 0.77
Batch: 600; loss: 0.66; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.8; acc: 0.77
Batch: 660; loss: 0.65; acc: 0.84
Batch: 680; loss: 0.94; acc: 0.75
Batch: 700; loss: 0.81; acc: 0.73
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 1.03; acc: 0.69
Batch: 780; loss: 0.79; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.81 

0.00019552145386114717
0.00018813904898706824
Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.66
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.72
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.6720030985440418; val_accuracy: 0.8338972929936306 

The current subspace-distance is: 0.00018813904898706824 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 0.57; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.91
Batch: 100; loss: 0.85; acc: 0.75
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.57; acc: 0.91
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.72; acc: 0.81
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.83; acc: 0.8
Batch: 280; loss: 0.81; acc: 0.78
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.84; acc: 0.75
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.86; acc: 0.77
Batch: 380; loss: 0.75; acc: 0.75
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.75; acc: 0.81
Batch: 440; loss: 0.59; acc: 0.89
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.68; acc: 0.78
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.92; acc: 0.72
Batch: 540; loss: 0.73; acc: 0.78
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.79; acc: 0.8
Batch: 640; loss: 0.79; acc: 0.78
Batch: 660; loss: 0.62; acc: 0.88
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.69; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.89
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 0.72; train_accuracy: 0.81 

0.00019594287732616067
0.00018884857126977295
Batch: 0; loss: 0.85; acc: 0.69
Batch: 20; loss: 0.92; acc: 0.66
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.6721798673177221; val_accuracy: 0.8340963375796179 

The current subspace-distance is: 0.00018884857126977295 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.99; acc: 0.7
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.61; acc: 0.92
Batch: 140; loss: 0.76; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.89
Batch: 180; loss: 0.79; acc: 0.73
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.82; acc: 0.78
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.74; acc: 0.8
Batch: 280; loss: 0.8; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.77
Batch: 320; loss: 0.78; acc: 0.81
Batch: 340; loss: 0.86; acc: 0.75
Batch: 360; loss: 0.79; acc: 0.81
Batch: 380; loss: 0.73; acc: 0.84
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 1.03; acc: 0.69
Batch: 440; loss: 0.71; acc: 0.81
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.83
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.85; acc: 0.8
Batch: 620; loss: 0.77; acc: 0.84
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.78
Batch: 680; loss: 0.75; acc: 0.83
Batch: 700; loss: 0.86; acc: 0.81
Batch: 720; loss: 0.84; acc: 0.8
Batch: 740; loss: 0.84; acc: 0.78
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.84; acc: 0.84
Train Epoch over. train_loss: 0.72; train_accuracy: 0.81 

0.0001992589677684009
0.0001899708149721846
Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.92; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.52; acc: 0.94
Val Epoch over. val_loss: 0.6706103756549252; val_accuracy: 0.8348925159235668 

The current subspace-distance is: 0.0001899708149721846 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.79; acc: 0.8
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.61; acc: 0.91
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.75
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.85; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.96; acc: 0.72
Batch: 280; loss: 0.86; acc: 0.8
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.74; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.67; acc: 0.88
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.79; acc: 0.77
Batch: 460; loss: 0.59; acc: 0.89
Batch: 480; loss: 0.66; acc: 0.8
Batch: 500; loss: 0.92; acc: 0.75
Batch: 520; loss: 0.77; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.74; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.89; acc: 0.75
Batch: 660; loss: 0.72; acc: 0.81
Batch: 680; loss: 0.76; acc: 0.83
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.77; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.81 

0.0001998699881369248
0.00019362001330591738
Batch: 0; loss: 0.84; acc: 0.69
Batch: 20; loss: 0.91; acc: 0.64
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6628321133981085; val_accuracy: 0.8357882165605095 

The current subspace-distance is: 0.00019362001330591738 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.64; acc: 0.89
Batch: 160; loss: 0.74; acc: 0.72
Batch: 180; loss: 0.72; acc: 0.77
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.79; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.75; acc: 0.78
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.58; acc: 0.89
Batch: 440; loss: 0.67; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.7; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.88; acc: 0.75
Batch: 540; loss: 0.76; acc: 0.8
Batch: 560; loss: 0.87; acc: 0.75
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.94
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.79; acc: 0.73
Batch: 700; loss: 0.84; acc: 0.72
Batch: 720; loss: 0.54; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.81 

0.00020001751545350999
0.00019334565149620175
Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.9; acc: 0.66
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6576814725520505; val_accuracy: 0.837281050955414 

The current subspace-distance is: 0.00019334565149620175 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.83; acc: 0.81
Batch: 200; loss: 0.8; acc: 0.78
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.76; acc: 0.88
Batch: 320; loss: 0.74; acc: 0.78
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.88; acc: 0.73
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.54; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.7; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.88; acc: 0.77
Batch: 520; loss: 0.81; acc: 0.78
Batch: 540; loss: 0.7; acc: 0.8
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.79; acc: 0.8
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.9; acc: 0.73
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.94; acc: 0.77
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.82; acc: 0.75
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.66; acc: 0.88
Train Epoch over. train_loss: 0.71; train_accuracy: 0.81 

0.00020134834630880505
0.00019321423314977437
Batch: 0; loss: 0.82; acc: 0.7
Batch: 20; loss: 0.9; acc: 0.66
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6632693605438159; val_accuracy: 0.8365843949044586 

The current subspace-distance is: 0.00019321423314977437 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.71; acc: 0.84
Batch: 240; loss: 0.95; acc: 0.73
Batch: 260; loss: 0.78; acc: 0.8
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.87; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.84
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.76; acc: 0.78
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.8
Batch: 460; loss: 0.51; acc: 0.92
Batch: 480; loss: 0.66; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.75; acc: 0.7
Batch: 540; loss: 0.63; acc: 0.91
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.8; acc: 0.81
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 1.09; acc: 0.69
Batch: 680; loss: 0.76; acc: 0.78
Batch: 700; loss: 0.71; acc: 0.8
Batch: 720; loss: 0.77; acc: 0.8
Batch: 740; loss: 0.86; acc: 0.78
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.81 

0.00020480435341596603
0.0001974587212316692
Batch: 0; loss: 0.84; acc: 0.69
Batch: 20; loss: 0.91; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.6626393400180112; val_accuracy: 0.8354896496815286 

The current subspace-distance is: 0.0001974587212316692 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.74; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.64; acc: 0.86
Batch: 240; loss: 0.78; acc: 0.8
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.82; acc: 0.75
Batch: 300; loss: 0.67; acc: 0.8
Batch: 320; loss: 0.9; acc: 0.77
Batch: 340; loss: 0.73; acc: 0.81
Batch: 360; loss: 0.83; acc: 0.77
Batch: 380; loss: 0.81; acc: 0.83
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.89; acc: 0.8
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.77; acc: 0.75
Batch: 580; loss: 0.59; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.76; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.76; acc: 0.8
Batch: 700; loss: 0.79; acc: 0.8
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.79; acc: 0.77
Batch: 780; loss: 0.6; acc: 0.92
Train Epoch over. train_loss: 0.71; train_accuracy: 0.81 

0.00020208017667755485
0.00019528910343069583
Batch: 0; loss: 0.83; acc: 0.69
Batch: 20; loss: 0.91; acc: 0.64
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6619719498476405; val_accuracy: 0.8325039808917197 

The current subspace-distance is: 0.00019528910343069583 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.92
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.62; acc: 0.91
Batch: 80; loss: 0.87; acc: 0.7
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.67; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.71; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.87; acc: 0.73
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.86; acc: 0.73
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.81; acc: 0.78
Batch: 360; loss: 0.53; acc: 0.94
Batch: 380; loss: 0.69; acc: 0.78
Batch: 400; loss: 0.84; acc: 0.77
Batch: 420; loss: 0.75; acc: 0.81
Batch: 440; loss: 0.9; acc: 0.75
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.88
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.94; acc: 0.73
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.9; acc: 0.73
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.92
Batch: 700; loss: 0.74; acc: 0.81
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.79; acc: 0.77
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.0002020629617618397
0.00019482523202896118
Batch: 0; loss: 0.83; acc: 0.72
Batch: 20; loss: 0.91; acc: 0.64
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.6604051398243874; val_accuracy: 0.8338972929936306 

The current subspace-distance is: 0.00019482523202896118 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.78; acc: 0.75
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.83; acc: 0.78
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.91
Batch: 380; loss: 0.8; acc: 0.83
Batch: 400; loss: 0.9; acc: 0.77
Batch: 420; loss: 0.79; acc: 0.8
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 0.77; acc: 0.78
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.63; acc: 0.88
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.83; acc: 0.78
Batch: 580; loss: 0.76; acc: 0.78
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.72; acc: 0.8
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.75
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.79; acc: 0.77
Batch: 720; loss: 0.8; acc: 0.77
Batch: 740; loss: 0.77; acc: 0.77
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.0002041473489953205
0.00019563062232919037
Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.9; acc: 0.67
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.72
Batch: 140; loss: 0.49; acc: 0.95
Val Epoch over. val_loss: 0.6619055711539688; val_accuracy: 0.8371815286624203 

The current subspace-distance is: 0.00019563062232919037 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.7
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.76; acc: 0.72
Batch: 160; loss: 0.85; acc: 0.73
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.7; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.71; acc: 0.78
Batch: 320; loss: 0.74; acc: 0.83
Batch: 340; loss: 0.65; acc: 0.88
Batch: 360; loss: 0.76; acc: 0.78
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.81; acc: 0.81
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.82; acc: 0.78
Batch: 520; loss: 0.58; acc: 0.91
Batch: 540; loss: 0.73; acc: 0.75
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.84; acc: 0.81
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.71; acc: 0.84
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.79; acc: 0.8
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.77; acc: 0.83
Batch: 760; loss: 0.95; acc: 0.64
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00020352315914351493
0.0001943612442119047
Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.66
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.6604849244378934; val_accuracy: 0.8362858280254777 

The current subspace-distance is: 0.0001943612442119047 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.88
Batch: 80; loss: 0.83; acc: 0.72
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.81; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.83; acc: 0.7
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.74; acc: 0.78
Batch: 340; loss: 0.72; acc: 0.8
Batch: 360; loss: 0.69; acc: 0.86
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.81; acc: 0.78
Batch: 420; loss: 0.63; acc: 0.92
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.65; acc: 0.8
Batch: 500; loss: 0.91; acc: 0.78
Batch: 520; loss: 0.72; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.94
Batch: 620; loss: 0.85; acc: 0.78
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.67; acc: 0.88
Batch: 700; loss: 0.72; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.88; acc: 0.75
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00020404781389515847
0.00019529354176484048
Batch: 0; loss: 0.82; acc: 0.7
Batch: 20; loss: 0.9; acc: 0.66
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6549982551936131; val_accuracy: 0.8397691082802548 

The current subspace-distance is: 0.00019529354176484048 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.75
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.74; acc: 0.78
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.79; acc: 0.8
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.75; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.73; acc: 0.81
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.75
Batch: 560; loss: 0.67; acc: 0.84
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.76; acc: 0.77
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.77
Batch: 720; loss: 0.72; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.77; acc: 0.81
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.00020382102229632437
0.00019846030045300722
Batch: 0; loss: 0.82; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.64
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6596311900266416; val_accuracy: 0.8366839171974523 

The current subspace-distance is: 0.00019846030045300722 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_11_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.21

The number of parameters is: 262995

The number of individual parameters is:

10
180
10
10
15
32700
15
15
30
98100
30
30
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 78898491
elements in E: 78898500
fraction nonzero: 0.9999998859293903
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.44; acc: 0.09
Batch: 20; loss: 2.06; acc: 0.28
Batch: 40; loss: 1.93; acc: 0.39
Batch: 60; loss: 1.98; acc: 0.34
Batch: 80; loss: 1.79; acc: 0.47
Batch: 100; loss: 1.64; acc: 0.66
Batch: 120; loss: 1.61; acc: 0.66
Batch: 140; loss: 1.64; acc: 0.56
Batch: 160; loss: 1.66; acc: 0.62
Batch: 180; loss: 1.68; acc: 0.56
Batch: 200; loss: 1.59; acc: 0.69
Batch: 220; loss: 1.47; acc: 0.7
Batch: 240; loss: 1.61; acc: 0.62
Batch: 260; loss: 1.51; acc: 0.64
Batch: 280; loss: 1.45; acc: 0.67
Batch: 300; loss: 1.46; acc: 0.67
Batch: 320; loss: 1.42; acc: 0.72
Batch: 340; loss: 1.42; acc: 0.62
Batch: 360; loss: 1.36; acc: 0.75
Batch: 380; loss: 1.38; acc: 0.7
Batch: 400; loss: 1.39; acc: 0.72
Batch: 420; loss: 1.39; acc: 0.69
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.21; acc: 0.83
Batch: 480; loss: 1.33; acc: 0.7
Batch: 500; loss: 1.24; acc: 0.75
Batch: 520; loss: 1.2; acc: 0.78
Batch: 540; loss: 1.23; acc: 0.75
Batch: 560; loss: 1.21; acc: 0.8
Batch: 580; loss: 1.29; acc: 0.7
Batch: 600; loss: 1.34; acc: 0.73
Batch: 620; loss: 1.35; acc: 0.72
Batch: 640; loss: 1.15; acc: 0.86
Batch: 660; loss: 1.23; acc: 0.73
Batch: 680; loss: 1.31; acc: 0.8
Batch: 700; loss: 1.22; acc: 0.81
Batch: 720; loss: 1.23; acc: 0.83
Batch: 740; loss: 1.11; acc: 0.83
Batch: 760; loss: 1.19; acc: 0.75
Batch: 780; loss: 1.24; acc: 0.7
Train Epoch over. train_loss: 1.44; train_accuracy: 0.68 

5.962884097243659e-05
5.4583215387538075e-05
Batch: 0; loss: 1.23; acc: 0.72
Batch: 20; loss: 1.27; acc: 0.66
Batch: 40; loss: 0.9; acc: 0.86
Batch: 60; loss: 1.11; acc: 0.77
Batch: 80; loss: 1.0; acc: 0.94
Batch: 100; loss: 1.16; acc: 0.8
Batch: 120; loss: 1.23; acc: 0.75
Batch: 140; loss: 0.97; acc: 0.92
Val Epoch over. val_loss: 1.145001371195362; val_accuracy: 0.7913017515923567 

The current subspace-distance is: 5.4583215387538075e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.8
Batch: 20; loss: 1.26; acc: 0.8
Batch: 40; loss: 1.09; acc: 0.83
Batch: 60; loss: 1.32; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.8
Batch: 100; loss: 1.05; acc: 0.86
Batch: 120; loss: 1.11; acc: 0.78
Batch: 140; loss: 1.15; acc: 0.83
Batch: 160; loss: 1.17; acc: 0.83
Batch: 180; loss: 1.16; acc: 0.73
Batch: 200; loss: 1.27; acc: 0.77
Batch: 220; loss: 1.22; acc: 0.75
Batch: 240; loss: 1.21; acc: 0.78
Batch: 260; loss: 0.98; acc: 0.88
Batch: 280; loss: 1.1; acc: 0.81
Batch: 300; loss: 1.09; acc: 0.8
Batch: 320; loss: 1.01; acc: 0.83
Batch: 340; loss: 1.13; acc: 0.8
Batch: 360; loss: 1.23; acc: 0.69
Batch: 380; loss: 1.19; acc: 0.75
Batch: 400; loss: 1.06; acc: 0.81
Batch: 420; loss: 1.05; acc: 0.78
Batch: 440; loss: 1.03; acc: 0.86
Batch: 460; loss: 1.2; acc: 0.72
Batch: 480; loss: 1.05; acc: 0.83
Batch: 500; loss: 1.04; acc: 0.83
Batch: 520; loss: 1.05; acc: 0.77
Batch: 540; loss: 1.17; acc: 0.83
Batch: 560; loss: 1.12; acc: 0.77
Batch: 580; loss: 0.91; acc: 0.89
Batch: 600; loss: 1.05; acc: 0.7
Batch: 620; loss: 0.92; acc: 0.88
Batch: 640; loss: 0.99; acc: 0.83
Batch: 660; loss: 1.09; acc: 0.73
Batch: 680; loss: 0.9; acc: 0.84
Batch: 700; loss: 1.05; acc: 0.81
Batch: 720; loss: 1.07; acc: 0.77
Batch: 740; loss: 0.96; acc: 0.88
Batch: 760; loss: 1.19; acc: 0.69
Batch: 780; loss: 1.03; acc: 0.81
Train Epoch over. train_loss: 1.11; train_accuracy: 0.78 

8.126404281938449e-05
7.607138104503974e-05
Batch: 0; loss: 1.08; acc: 0.77
Batch: 20; loss: 1.11; acc: 0.77
Batch: 40; loss: 0.76; acc: 0.94
Batch: 60; loss: 0.93; acc: 0.78
Batch: 80; loss: 0.86; acc: 0.94
Batch: 100; loss: 0.95; acc: 0.84
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.83; acc: 0.92
Val Epoch over. val_loss: 0.9929615710950961; val_accuracy: 0.816281847133758 

The current subspace-distance is: 7.607138104503974e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 1.14; acc: 0.81
Batch: 60; loss: 1.02; acc: 0.8
Batch: 80; loss: 0.96; acc: 0.81
Batch: 100; loss: 0.96; acc: 0.8
Batch: 120; loss: 1.03; acc: 0.84
Batch: 140; loss: 1.06; acc: 0.75
Batch: 160; loss: 0.81; acc: 0.92
Batch: 180; loss: 0.9; acc: 0.88
Batch: 200; loss: 1.01; acc: 0.83
Batch: 220; loss: 1.0; acc: 0.83
Batch: 240; loss: 1.03; acc: 0.78
Batch: 260; loss: 0.92; acc: 0.8
Batch: 280; loss: 0.94; acc: 0.81
Batch: 300; loss: 0.95; acc: 0.78
Batch: 320; loss: 0.99; acc: 0.75
Batch: 340; loss: 0.93; acc: 0.89
Batch: 360; loss: 0.93; acc: 0.88
Batch: 380; loss: 1.02; acc: 0.77
Batch: 400; loss: 1.08; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.83
Batch: 440; loss: 0.82; acc: 0.91
Batch: 460; loss: 0.92; acc: 0.83
Batch: 480; loss: 1.0; acc: 0.84
Batch: 500; loss: 0.94; acc: 0.78
Batch: 520; loss: 1.1; acc: 0.7
Batch: 540; loss: 1.09; acc: 0.8
Batch: 560; loss: 0.95; acc: 0.8
Batch: 580; loss: 0.99; acc: 0.72
Batch: 600; loss: 0.97; acc: 0.78
Batch: 620; loss: 0.96; acc: 0.88
Batch: 640; loss: 1.01; acc: 0.78
Batch: 660; loss: 0.98; acc: 0.8
Batch: 680; loss: 0.96; acc: 0.77
Batch: 700; loss: 0.97; acc: 0.8
Batch: 720; loss: 0.99; acc: 0.78
Batch: 740; loss: 1.06; acc: 0.75
Batch: 760; loss: 0.91; acc: 0.83
Batch: 780; loss: 0.97; acc: 0.8
Train Epoch over. train_loss: 0.98; train_accuracy: 0.81 

9.864845196716487e-05
9.401728311786428e-05
Batch: 0; loss: 0.99; acc: 0.81
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.95
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.95
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.92
Val Epoch over. val_loss: 0.8810028870394275; val_accuracy: 0.8396695859872612 

The current subspace-distance is: 9.401728311786428e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.81
Batch: 40; loss: 1.01; acc: 0.78
Batch: 60; loss: 1.02; acc: 0.8
Batch: 80; loss: 0.95; acc: 0.83
Batch: 100; loss: 0.87; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.81
Batch: 140; loss: 0.95; acc: 0.75
Batch: 160; loss: 0.89; acc: 0.8
Batch: 180; loss: 0.85; acc: 0.91
Batch: 200; loss: 0.89; acc: 0.83
Batch: 220; loss: 0.94; acc: 0.84
Batch: 240; loss: 0.91; acc: 0.84
Batch: 260; loss: 0.92; acc: 0.81
Batch: 280; loss: 0.89; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.94
Batch: 320; loss: 0.9; acc: 0.84
Batch: 340; loss: 1.02; acc: 0.75
Batch: 360; loss: 0.91; acc: 0.78
Batch: 380; loss: 0.98; acc: 0.73
Batch: 400; loss: 0.78; acc: 0.88
Batch: 420; loss: 1.07; acc: 0.75
Batch: 440; loss: 0.91; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.8
Batch: 480; loss: 0.91; acc: 0.81
Batch: 500; loss: 0.82; acc: 0.88
Batch: 520; loss: 0.81; acc: 0.78
Batch: 540; loss: 0.78; acc: 0.89
Batch: 560; loss: 0.84; acc: 0.86
Batch: 580; loss: 0.9; acc: 0.81
Batch: 600; loss: 0.88; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.86
Batch: 640; loss: 0.79; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.86
Batch: 680; loss: 0.81; acc: 0.84
Batch: 700; loss: 0.9; acc: 0.8
Batch: 720; loss: 0.86; acc: 0.8
Batch: 740; loss: 0.92; acc: 0.78
Batch: 760; loss: 0.81; acc: 0.81
Batch: 780; loss: 0.7; acc: 0.89
Train Epoch over. train_loss: 0.88; train_accuracy: 0.83 

0.00011701437324518338
0.00011100377014372498
Batch: 0; loss: 0.87; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.53; acc: 0.97
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.98
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.83
Batch: 140; loss: 0.63; acc: 0.89
Val Epoch over. val_loss: 0.779142095025178; val_accuracy: 0.8572850318471338 

The current subspace-distance is: 0.00011100377014372498 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.89; acc: 0.8
Batch: 60; loss: 0.85; acc: 0.8
Batch: 80; loss: 0.84; acc: 0.84
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 0.75; acc: 0.88
Batch: 140; loss: 0.76; acc: 0.88
Batch: 160; loss: 0.91; acc: 0.78
Batch: 180; loss: 0.82; acc: 0.83
Batch: 200; loss: 0.8; acc: 0.84
Batch: 220; loss: 0.88; acc: 0.8
Batch: 240; loss: 1.04; acc: 0.73
Batch: 260; loss: 0.79; acc: 0.86
Batch: 280; loss: 0.95; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.86
Batch: 320; loss: 0.8; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.91
Batch: 380; loss: 0.82; acc: 0.86
Batch: 400; loss: 0.76; acc: 0.86
Batch: 420; loss: 0.77; acc: 0.89
Batch: 440; loss: 0.9; acc: 0.8
Batch: 460; loss: 0.82; acc: 0.89
Batch: 480; loss: 0.77; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.88
Batch: 540; loss: 0.82; acc: 0.81
Batch: 560; loss: 0.82; acc: 0.89
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.69; acc: 0.89
Batch: 620; loss: 0.8; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.89; acc: 0.81
Batch: 680; loss: 0.72; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.72; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.89
Batch: 760; loss: 0.68; acc: 0.89
Batch: 780; loss: 0.79; acc: 0.84
Train Epoch over. train_loss: 0.79; train_accuracy: 0.85 

0.0001327380450675264
0.00012690373114310205
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.681761186024186; val_accuracy: 0.8744028662420382 

The current subspace-distance is: 0.00012690373114310205 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.86
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.83; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.86
Batch: 160; loss: 0.85; acc: 0.78
Batch: 180; loss: 0.62; acc: 0.94
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.79; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.92
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.74; acc: 0.84
Batch: 320; loss: 0.7; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.63; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.92
Batch: 420; loss: 0.79; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.92
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.64; acc: 0.92
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.92
Batch: 600; loss: 0.77; acc: 0.83
Batch: 620; loss: 0.78; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.91
Batch: 660; loss: 0.6; acc: 0.91
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.95
Batch: 740; loss: 0.63; acc: 0.89
Batch: 760; loss: 0.86; acc: 0.83
Batch: 780; loss: 0.64; acc: 0.92
Train Epoch over. train_loss: 0.7; train_accuracy: 0.86 

0.00014751798880752176
0.0001406228548148647
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.97
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.94
Val Epoch over. val_loss: 0.6135036971918337; val_accuracy: 0.884952229299363 

The current subspace-distance is: 0.0001406228548148647 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.92
Batch: 40; loss: 0.7; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.91
Batch: 140; loss: 0.61; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.92
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.57; acc: 0.91
Batch: 240; loss: 0.77; acc: 0.8
Batch: 260; loss: 0.76; acc: 0.8
Batch: 280; loss: 0.55; acc: 0.92
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.58; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.77; acc: 0.78
Batch: 440; loss: 0.53; acc: 0.94
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.74; acc: 0.84
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.94
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.83
Batch: 700; loss: 0.47; acc: 0.94
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.92
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.87 

0.00016158832295332104
0.0001543409889563918
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.98
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.5667540615151643; val_accuracy: 0.8876393312101911 

The current subspace-distance is: 0.0001543409889563918 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.92
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.95
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.94
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.94
Batch: 300; loss: 0.55; acc: 0.94
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.78; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.95
Batch: 480; loss: 0.66; acc: 0.83
Batch: 500; loss: 0.45; acc: 0.97
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.95
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.88
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.87 

0.00017182166629936546
0.00016270637570414692
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.98
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.526527591192039; val_accuracy: 0.8901273885350318 

The current subspace-distance is: 0.00016270637570414692 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.89
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.91
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 0.49; acc: 0.94
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.94
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.0001818819873733446
0.00017542450223118067
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.5016345125474747; val_accuracy: 0.8903264331210191 

The current subspace-distance is: 0.00017542450223118067 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.45; acc: 0.95
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.97
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.76; acc: 0.73
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.92
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00018812008784152567
0.00018122569599654526
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.478856917209686; val_accuracy: 0.8922173566878981 

The current subspace-distance is: 0.00018122569599654526 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.64; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.95
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00019126453844364733
0.00018217874458059669
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.47414562466797555; val_accuracy: 0.894406847133758 

The current subspace-distance is: 0.00018217874458059669 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.94
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.47; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.94
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00019316529505886137
0.0001836773008108139
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.47007096307292867; val_accuracy: 0.8934116242038217 

The current subspace-distance is: 0.0001836773008108139 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.56; acc: 0.89
Batch: 180; loss: 0.64; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.98
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.95
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.95
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019543945381883532
0.00018902616284321994
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.29; acc: 1.0
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4628933824741157; val_accuracy: 0.8942078025477707 

The current subspace-distance is: 0.00018902616284321994 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.98
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.63; acc: 0.8
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.64; acc: 0.78
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.0001998261723201722
0.00019180212984792888
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.28; acc: 1.0
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.4563197777339607; val_accuracy: 0.894406847133758 

The current subspace-distance is: 0.00019180212984792888 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.94
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.95
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.94
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.000199613074073568
0.0001949804718606174
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.98
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4575586377815076; val_accuracy: 0.8940087579617835 

The current subspace-distance is: 0.0001949804718606174 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.63; acc: 0.8
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.91
Batch: 640; loss: 0.75; acc: 0.8
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.77
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0002061284612864256
0.00019796831475105137
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.26; acc: 1.0
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.4538675413769522; val_accuracy: 0.8950039808917197 

The current subspace-distance is: 0.00019796831475105137 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.95
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.8
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.97
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.62; acc: 0.78
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00020517196389846504
0.0001981633249670267
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.98
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4391827741815786; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.0001981633249670267 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.95
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0002101981663145125
0.00020089605823159218
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.98
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.4412733900129415; val_accuracy: 0.8958996815286624 

The current subspace-distance is: 0.00020089605823159218 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021185842342674732
0.00020377396140247583
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.25; acc: 1.0
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4331433132385752; val_accuracy: 0.8983877388535032 

The current subspace-distance is: 0.00020377396140247583 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.8
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.64; acc: 0.81
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00020919986127410084
0.0002015598292928189
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.98
Val Epoch over. val_loss: 0.42882019755946604; val_accuracy: 0.8990843949044586 

The current subspace-distance is: 0.0002015598292928189 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.63; acc: 0.8
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.94
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021136790746822953
0.0002033575001405552
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.24; acc: 1.0
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4292786929068292; val_accuracy: 0.8981886942675159 

The current subspace-distance is: 0.0002033575001405552 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.98
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.6; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.8
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.78
Batch: 400; loss: 0.62; acc: 0.78
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.77
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.95
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021299129002727568
0.0002031784679275006
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 1.0
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4269378089411244; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 0.0002031784679275006 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.63; acc: 0.83
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00021299876971170306
0.00020498051890172064
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 1.0
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.42305904437022607; val_accuracy: 0.8999800955414012 

The current subspace-distance is: 0.00020498051890172064 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.8
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.55; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.75
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.0002135091635864228
0.00020443268294911832
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4255088502244585; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 0.00020443268294911832 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.97
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00021411235502455384
0.0002054135111393407
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.24; acc: 1.0
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.42678612328259047; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 0.0002054135111393407 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.97
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00021226097305770963
0.00020397946354933083
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.23; acc: 1.0
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.41817696943024923; val_accuracy: 0.9015724522292994 

The current subspace-distance is: 0.00020397946354933083 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00021512925741262734
0.00020861515076830983
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 1.0
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.4138616437365295; val_accuracy: 0.901671974522293 

The current subspace-distance is: 0.00020861515076830983 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.7; acc: 0.78
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.98
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00021737815404776484
0.0002068148460239172
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 1.0
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.41906291730464645; val_accuracy: 0.9009753184713376 

The current subspace-distance is: 0.0002068148460239172 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.98
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00021559777087531984
0.00020752249110955745
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.23; acc: 1.0
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4195806903254454; val_accuracy: 0.9017714968152867 

The current subspace-distance is: 0.00020752249110955745 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00022053690918255597
0.0002112018846673891
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.23; acc: 1.0
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.41720071624798377; val_accuracy: 0.9017714968152867 

The current subspace-distance is: 0.0002112018846673891 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_11_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.21

The number of parameters is: 262995

The number of individual parameters is:

10
180
10
10
15
32700
15
15
30
98100
30
30
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 105197989
elements in E: 105198000
fraction nonzero: 0.9999998954352745
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.14
Batch: 20; loss: 1.97; acc: 0.36
Batch: 40; loss: 1.77; acc: 0.64
Batch: 60; loss: 1.71; acc: 0.62
Batch: 80; loss: 1.66; acc: 0.61
Batch: 100; loss: 1.51; acc: 0.69
Batch: 120; loss: 1.54; acc: 0.59
Batch: 140; loss: 1.46; acc: 0.7
Batch: 160; loss: 1.41; acc: 0.73
Batch: 180; loss: 1.39; acc: 0.77
Batch: 200; loss: 1.36; acc: 0.75
Batch: 220; loss: 1.35; acc: 0.73
Batch: 240; loss: 1.23; acc: 0.77
Batch: 260; loss: 1.15; acc: 0.86
Batch: 280; loss: 1.32; acc: 0.77
Batch: 300; loss: 1.2; acc: 0.77
Batch: 320; loss: 1.22; acc: 0.84
Batch: 340; loss: 1.23; acc: 0.77
Batch: 360; loss: 1.13; acc: 0.86
Batch: 380; loss: 1.32; acc: 0.72
Batch: 400; loss: 1.2; acc: 0.78
Batch: 420; loss: 1.26; acc: 0.73
Batch: 440; loss: 1.05; acc: 0.86
Batch: 460; loss: 1.08; acc: 0.83
Batch: 480; loss: 1.02; acc: 0.89
Batch: 500; loss: 1.04; acc: 0.86
Batch: 520; loss: 1.03; acc: 0.84
Batch: 540; loss: 1.07; acc: 0.77
Batch: 560; loss: 1.13; acc: 0.77
Batch: 580; loss: 1.05; acc: 0.84
Batch: 600; loss: 0.93; acc: 0.86
Batch: 620; loss: 1.0; acc: 0.88
Batch: 640; loss: 1.0; acc: 0.83
Batch: 660; loss: 1.1; acc: 0.75
Batch: 680; loss: 0.94; acc: 0.89
Batch: 700; loss: 1.03; acc: 0.83
Batch: 720; loss: 1.01; acc: 0.81
Batch: 740; loss: 0.9; acc: 0.91
Batch: 760; loss: 0.98; acc: 0.81
Batch: 780; loss: 0.99; acc: 0.8
Train Epoch over. train_loss: 1.24; train_accuracy: 0.75 

2.5489191102678888e-05
9.057643183041364e-06
Batch: 0; loss: 0.96; acc: 0.86
Batch: 20; loss: 1.1; acc: 0.8
Batch: 40; loss: 0.68; acc: 0.95
Batch: 60; loss: 0.88; acc: 0.84
Batch: 80; loss: 0.8; acc: 0.91
Batch: 100; loss: 0.93; acc: 0.88
Batch: 120; loss: 1.04; acc: 0.83
Batch: 140; loss: 0.8; acc: 0.89
Val Epoch over. val_loss: 0.917913722764155; val_accuracy: 0.8628582802547771 

The current subspace-distance is: 9.057643183041364e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 0.88; acc: 0.89
Batch: 40; loss: 0.94; acc: 0.84
Batch: 60; loss: 0.99; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 0.97; acc: 0.83
Batch: 120; loss: 1.12; acc: 0.78
Batch: 140; loss: 0.89; acc: 0.84
Batch: 160; loss: 0.8; acc: 0.89
Batch: 180; loss: 0.87; acc: 0.91
Batch: 200; loss: 0.94; acc: 0.83
Batch: 220; loss: 0.81; acc: 0.89
Batch: 240; loss: 0.84; acc: 0.91
Batch: 260; loss: 0.89; acc: 0.88
Batch: 280; loss: 0.81; acc: 0.88
Batch: 300; loss: 0.86; acc: 0.89
Batch: 320; loss: 0.9; acc: 0.75
Batch: 340; loss: 0.81; acc: 0.86
Batch: 360; loss: 0.87; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.89
Batch: 400; loss: 0.89; acc: 0.81
Batch: 420; loss: 0.86; acc: 0.88
Batch: 440; loss: 0.77; acc: 0.91
Batch: 460; loss: 0.87; acc: 0.83
Batch: 480; loss: 0.83; acc: 0.86
Batch: 500; loss: 0.74; acc: 0.91
Batch: 520; loss: 0.76; acc: 0.88
Batch: 540; loss: 0.87; acc: 0.84
Batch: 560; loss: 0.86; acc: 0.89
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.92; acc: 0.8
Batch: 620; loss: 0.76; acc: 0.92
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.83; acc: 0.81
Batch: 680; loss: 0.89; acc: 0.81
Batch: 700; loss: 0.79; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.88
Batch: 740; loss: 0.72; acc: 0.88
Batch: 760; loss: 0.7; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.88
Train Epoch over. train_loss: 0.86; train_accuracy: 0.85 

3.233992902096361e-05
1.3094014320813585e-05
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 0.7110491499399684; val_accuracy: 0.8763933121019108 

The current subspace-distance is: 1.3094014320813585e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.83
Batch: 40; loss: 0.68; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.94
Batch: 80; loss: 0.88; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.86
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.83
Batch: 200; loss: 0.87; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.91
Batch: 240; loss: 0.64; acc: 0.89
Batch: 260; loss: 0.83; acc: 0.75
Batch: 280; loss: 0.74; acc: 0.88
Batch: 300; loss: 0.75; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.91
Batch: 340; loss: 0.75; acc: 0.88
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.89
Batch: 400; loss: 0.66; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.92
Batch: 440; loss: 0.79; acc: 0.83
Batch: 460; loss: 0.84; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.81; acc: 0.78
Batch: 520; loss: 0.76; acc: 0.81
Batch: 540; loss: 0.77; acc: 0.86
Batch: 560; loss: 0.68; acc: 0.89
Batch: 580; loss: 0.69; acc: 0.86
Batch: 600; loss: 0.72; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.91
Batch: 640; loss: 0.72; acc: 0.86
Batch: 660; loss: 0.74; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.7; acc: 0.86
Batch: 740; loss: 0.68; acc: 0.91
Batch: 760; loss: 0.77; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.92
Train Epoch over. train_loss: 0.72; train_accuracy: 0.86 

3.696127168950625e-05
1.5565768990200013e-05
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.617215321701803; val_accuracy: 0.8893312101910829 

The current subspace-distance is: 1.5565768990200013e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.88
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.58; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.94
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.91
Batch: 200; loss: 0.56; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.92
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.67; acc: 0.88
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.94
Batch: 440; loss: 0.51; acc: 0.94
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.95
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.92
Batch: 540; loss: 0.79; acc: 0.8
Batch: 560; loss: 0.54; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.92
Batch: 620; loss: 0.66; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.94
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.97
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.63; train_accuracy: 0.87 

4.0684255509404466e-05
1.6866108126123436e-05
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.95
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.97
Val Epoch over. val_loss: 0.5484898832573253; val_accuracy: 0.8967953821656051 

The current subspace-distance is: 1.6866108126123436e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.54; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.66; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.83
Batch: 260; loss: 0.57; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.92
Batch: 400; loss: 0.68; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

4.4374824938131496e-05
1.991910903598182e-05
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.97
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.4909874403932292; val_accuracy: 0.9007762738853503 

The current subspace-distance is: 1.991910903598182e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.97
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.94
Batch: 260; loss: 0.68; acc: 0.8
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.94
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.95
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.43; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.92
Batch: 600; loss: 0.6; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.42; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

4.8100686399266124e-05
2.1009591364418156e-05
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4625299108825671; val_accuracy: 0.9031648089171974 

The current subspace-distance is: 2.1009591364418156e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.97
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.27; acc: 1.0
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.29; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.97
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.98
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

5.0721359002636746e-05
2.3601720386068337e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.4293899006524663; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 2.3601720386068337e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.6; acc: 0.8
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.230993338045664e-05
2.38327011174988e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.410503307630302; val_accuracy: 0.9088375796178344 

The current subspace-distance is: 2.38327011174988e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.78
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.78
Batch: 440; loss: 0.39; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.4541796998819336e-05
2.2783551685279235e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.38176715696693225; val_accuracy: 0.9112261146496815 

The current subspace-distance is: 2.2783551685279235e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.97
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.797950871055946e-05
2.5996851036325097e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3599390610577954; val_accuracy: 0.9155055732484076 

The current subspace-distance is: 2.5996851036325097e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.856282950844616e-05
2.6424530005897395e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.35832462939107496; val_accuracy: 0.9157046178343949 

The current subspace-distance is: 2.6424530005897395e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.929313556407578e-05
2.7772273824666627e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3515074860518146; val_accuracy: 0.9164012738853503 

The current subspace-distance is: 2.7772273824666627e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.98
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.97
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.74; acc: 0.73
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.97
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.994318053126335e-05
2.5769651983864605e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.351063545722111; val_accuracy: 0.9166003184713376 

The current subspace-distance is: 2.5769651983864605e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.97
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.0714930441463366e-05
2.833373764588032e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3473447339170298; val_accuracy: 0.9179936305732485 

The current subspace-distance is: 2.833373764588032e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.98
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.98
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.0901991673745215e-05
2.7281515940558165e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.34108362987542606; val_accuracy: 0.9159036624203821 

The current subspace-distance is: 2.7281515940558165e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.54; acc: 0.81
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.154759466880932e-05
2.868247429432813e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3378141168386314; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 2.868247429432813e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.220545037649572e-05
2.9268625439726748e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3291488958012526; val_accuracy: 0.919187898089172 

The current subspace-distance is: 2.9268625439726748e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.2; acc: 1.0
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.98
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.289468728937209e-05
2.860783934011124e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3310831316337464; val_accuracy: 0.9195859872611465 

The current subspace-distance is: 2.860783934011124e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.304366252152249e-05
2.9209721105871722e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3247812193385355; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 2.9209721105871722e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.98
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.378233229042962e-05
2.866745853680186e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3190151148825694; val_accuracy: 0.9212778662420382 

The current subspace-distance is: 2.866745853680186e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.366938032442704e-05
2.942021455965005e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3186440908700038; val_accuracy: 0.9215764331210191 

The current subspace-distance is: 2.942021455965005e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.98
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.38802521280013e-05
2.9867946068407036e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.321585278412339; val_accuracy: 0.921875 

The current subspace-distance is: 2.9867946068407036e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.18; acc: 1.0
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.98
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.98
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.482715252786875e-05
2.8921740522491746e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3213367564663006; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 2.8921740522491746e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.98
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.83
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.422820297302678e-05
3.0920873541617766e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.3161034308801031; val_accuracy: 0.92296974522293 

The current subspace-distance is: 3.0920873541617766e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.97
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.453249807236716e-05
2.9294826163095422e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.31678388775533933; val_accuracy: 0.92296974522293 

The current subspace-distance is: 2.9294826163095422e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.97
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.62209713482298e-05
3.0418506867135875e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.31556543570225404; val_accuracy: 0.9228702229299363 

The current subspace-distance is: 3.0418506867135875e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.61; acc: 0.78
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.97
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.98
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.464325997512788e-05
3.0358840376720764e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.31420519482937587; val_accuracy: 0.923765923566879 

The current subspace-distance is: 3.0358840376720764e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.97
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.495036359410733e-05
2.9164990337449126e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.311142137950393; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.9164990337449126e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.559577741427347e-05
3.1926549127092585e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.3111775024871158; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 3.1926549127092585e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.8
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.98
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.97
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.451004446716979e-05
2.9023180104559287e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.30947696517227563; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 2.9023180104559287e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_11_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.21

The number of parameters is: 262995

The number of individual parameters is:

10
180
10
10
15
32700
15
15
30
98100
30
30
64
126720
64
64
4096
64
640
10
64
64

nonzero elements in E: 131497489
elements in E: 131497500
fraction nonzero: 0.9999999163482195
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.05; acc: 0.33
Batch: 40; loss: 1.73; acc: 0.53
Batch: 60; loss: 1.67; acc: 0.62
Batch: 80; loss: 1.55; acc: 0.73
Batch: 100; loss: 1.47; acc: 0.7
Batch: 120; loss: 1.47; acc: 0.75
Batch: 140; loss: 1.31; acc: 0.81
Batch: 160; loss: 1.37; acc: 0.69
Batch: 180; loss: 1.23; acc: 0.88
Batch: 200; loss: 1.21; acc: 0.8
Batch: 220; loss: 1.27; acc: 0.77
Batch: 240; loss: 1.17; acc: 0.84
Batch: 260; loss: 1.12; acc: 0.83
Batch: 280; loss: 1.25; acc: 0.78
Batch: 300; loss: 1.14; acc: 0.84
Batch: 320; loss: 1.28; acc: 0.69
Batch: 340; loss: 1.18; acc: 0.78
Batch: 360; loss: 1.2; acc: 0.78
Batch: 380; loss: 0.98; acc: 0.89
Batch: 400; loss: 1.07; acc: 0.78
Batch: 420; loss: 1.22; acc: 0.7
Batch: 440; loss: 1.14; acc: 0.75
Batch: 460; loss: 0.93; acc: 0.92
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.97; acc: 0.86
Batch: 520; loss: 1.16; acc: 0.73
Batch: 540; loss: 0.92; acc: 0.91
Batch: 560; loss: 1.02; acc: 0.8
Batch: 580; loss: 1.04; acc: 0.8
Batch: 600; loss: 0.99; acc: 0.84
Batch: 620; loss: 1.04; acc: 0.84
Batch: 640; loss: 0.92; acc: 0.84
Batch: 660; loss: 1.03; acc: 0.77
Batch: 680; loss: 0.88; acc: 0.88
Batch: 700; loss: 0.89; acc: 0.86
Batch: 720; loss: 0.95; acc: 0.86
Batch: 740; loss: 0.83; acc: 0.88
Batch: 760; loss: 0.85; acc: 0.88
Batch: 780; loss: 0.85; acc: 0.91
Train Epoch over. train_loss: 1.18; train_accuracy: 0.78 

2.5991801521740854e-05
9.012665032059886e-06
Batch: 0; loss: 0.88; acc: 0.89
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.95
Batch: 60; loss: 0.87; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.97
Batch: 100; loss: 0.85; acc: 0.91
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 0.67; acc: 0.97
Val Epoch over. val_loss: 0.8347383661634603; val_accuracy: 0.8696257961783439 

The current subspace-distance is: 9.012665032059886e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.86
Batch: 40; loss: 1.0; acc: 0.72
Batch: 60; loss: 0.9; acc: 0.84
Batch: 80; loss: 0.84; acc: 0.84
Batch: 100; loss: 0.91; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.92
Batch: 140; loss: 0.86; acc: 0.77
Batch: 160; loss: 0.8; acc: 0.91
Batch: 180; loss: 0.75; acc: 0.89
Batch: 200; loss: 0.84; acc: 0.86
Batch: 220; loss: 0.89; acc: 0.81
Batch: 240; loss: 1.0; acc: 0.86
Batch: 260; loss: 0.83; acc: 0.86
Batch: 280; loss: 0.74; acc: 0.88
Batch: 300; loss: 0.76; acc: 0.89
Batch: 320; loss: 0.77; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.88
Batch: 360; loss: 0.77; acc: 0.81
Batch: 380; loss: 0.83; acc: 0.91
Batch: 400; loss: 0.77; acc: 0.86
Batch: 420; loss: 0.81; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.84
Batch: 460; loss: 0.77; acc: 0.86
Batch: 480; loss: 0.68; acc: 0.86
Batch: 500; loss: 0.7; acc: 0.94
Batch: 520; loss: 0.74; acc: 0.89
Batch: 540; loss: 0.79; acc: 0.86
Batch: 560; loss: 0.82; acc: 0.86
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.84; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.94
Batch: 680; loss: 0.7; acc: 0.91
Batch: 700; loss: 0.71; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.95
Batch: 760; loss: 0.72; acc: 0.84
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.78; train_accuracy: 0.87 

3.128227035631426e-05
1.1883717888849787e-05
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.98
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.98
Val Epoch over. val_loss: 0.626220160798662; val_accuracy: 0.8988853503184714 

The current subspace-distance is: 1.1883717888849787e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.62; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.95
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.98
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.67; acc: 0.89
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.83
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.65; acc: 0.91
Batch: 260; loss: 0.82; acc: 0.83
Batch: 280; loss: 0.8; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.94
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.94
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.67; acc: 0.89
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.54; acc: 0.94
Batch: 500; loss: 0.54; acc: 0.92
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.92
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.59; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.91
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.94
Batch: 720; loss: 0.72; acc: 0.8
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.68; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.63; train_accuracy: 0.89 

3.6235989682609215e-05
1.5652833099011332e-05
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.33; acc: 1.0
Val Epoch over. val_loss: 0.5170771789019275; val_accuracy: 0.9126194267515924 

The current subspace-distance is: 1.5652833099011332e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.59; acc: 0.91
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.65; acc: 0.94
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.92
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.95
Batch: 440; loss: 0.56; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.94
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.94
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.95
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.54; train_accuracy: 0.9 

4.0890816308092326e-05
1.828171116358135e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.28; acc: 1.0
Val Epoch over. val_loss: 0.4590660420003211; val_accuracy: 0.9193869426751592 

The current subspace-distance is: 1.828171116358135e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.95
Batch: 440; loss: 0.52; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.92
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

4.357512079877779e-05
2.0154942831140943e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.24; acc: 1.0
Val Epoch over. val_loss: 0.4147169751346491; val_accuracy: 0.9205812101910829 

The current subspace-distance is: 2.0154942831140943e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.95
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.95
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.91 

4.616567093762569e-05
2.0071331164217554e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.21; acc: 1.0
Val Epoch over. val_loss: 0.37904767502265374; val_accuracy: 0.9252587579617835 

The current subspace-distance is: 2.0071331164217554e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.97
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.88838049932383e-05
2.229518759122584e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.3612126795349607; val_accuracy: 0.9271496815286624 

The current subspace-distance is: 2.229518759122584e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.97
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.159618376637809e-05
2.4014405425987206e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.3422662246568947; val_accuracy: 0.9294386942675159 

The current subspace-distance is: 2.4014405425987206e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.98
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.98
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.97
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.98
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

5.3699168347520754e-05
2.5063547582249157e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.32625708406328396; val_accuracy: 0.9335191082802548 

The current subspace-distance is: 2.5063547582249157e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.97
Batch: 240; loss: 0.3; acc: 1.0
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.485929796122946e-05
2.532721555326134e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3135077346851871; val_accuracy: 0.9354100318471338 

The current subspace-distance is: 2.532721555326134e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.97
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.610959487967193e-05
2.5840115995379165e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3113492715415681; val_accuracy: 0.9353105095541401 

The current subspace-distance is: 2.5840115995379165e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.98
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.98
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.6519485951866955e-05
2.5205448764609173e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.30088081604735867; val_accuracy: 0.9381966560509554 

The current subspace-distance is: 2.5205448764609173e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.69550902582705e-05
2.396847412455827e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.2984494037783829; val_accuracy: 0.9372014331210191 

The current subspace-distance is: 2.396847412455827e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.98
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.98
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.759046689490788e-05
2.6065574274980463e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.29551125502890085; val_accuracy: 0.9379976114649682 

The current subspace-distance is: 2.6065574274980463e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.98
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.98
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.783073356724344e-05
2.63103920588037e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2891534306346231; val_accuracy: 0.9387937898089171 

The current subspace-distance is: 2.63103920588037e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.86
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.8401001297170296e-05
2.6679941584006883e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2867352375464075; val_accuracy: 0.9391918789808917 

The current subspace-distance is: 2.6679941584006883e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.19; acc: 1.0
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.98
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.975908788968809e-05
2.7645466616377234e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.28681424359796914; val_accuracy: 0.9391918789808917 

The current subspace-distance is: 2.7645466616377234e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.33; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.9161786339245737e-05
2.5783585442695767e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2807229266614671; val_accuracy: 0.9389928343949044 

The current subspace-distance is: 2.5783585442695767e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.95
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.062990360078402e-05
2.734026566031389e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2767809190947539; val_accuracy: 0.9410828025477707 

The current subspace-distance is: 2.734026566031389e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.0342310462147e-05
2.851416684279684e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2761740565869459; val_accuracy: 0.9389928343949044 

The current subspace-distance is: 2.851416684279684e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.81
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.107976514613256e-05
2.6994195650331676e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.27171007287540255; val_accuracy: 0.9383957006369427 

The current subspace-distance is: 2.6994195650331676e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.98
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.125260551925749e-05
2.8791338991140947e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.272290798414285; val_accuracy: 0.940187101910828 

The current subspace-distance is: 2.8791338991140947e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.98
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.0141719586681575e-05
2.637537181726657e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2735948375171157; val_accuracy: 0.9416799363057324 

The current subspace-distance is: 2.637537181726657e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.17; acc: 1.0
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.2; acc: 0.98
Batch: 400; loss: 0.2; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.125929940026253e-05
2.7266711185802706e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2741734114517072; val_accuracy: 0.9384952229299363 

The current subspace-distance is: 2.7266711185802706e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.98
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.20152786723338e-05
2.7984475309494883e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.27287444970600166; val_accuracy: 0.9413813694267515 

The current subspace-distance is: 2.7984475309494883e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.97
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.98
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.113625568104908e-05
2.7438583856564946e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.27172859819831363; val_accuracy: 0.9416799363057324 

The current subspace-distance is: 2.7438583856564946e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.98
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.98
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.180619675433263e-05
2.778747511911206e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2680932693420702; val_accuracy: 0.9415804140127388 

The current subspace-distance is: 2.778747511911206e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.19; acc: 1.0
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.98
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.98
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.12568182987161e-05
2.6714733394328505e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2698684803144947; val_accuracy: 0.9414808917197452 

The current subspace-distance is: 2.6714733394328505e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.98
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.23; acc: 1.0
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.198124174261466e-05
2.9043585527688265e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2698442534466458; val_accuracy: 0.9410828025477707 

The current subspace-distance is: 2.9043585527688265e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.98
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.98
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.98
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.216758629307151e-05
2.7995552954962477e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.26886481113114935; val_accuracy: 0.9417794585987261 

The current subspace-distance is: 2.7995552954962477e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_11_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:52/N_11_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
