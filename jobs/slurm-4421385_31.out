model : table13slim
N : 15
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 16:23:19

Channel scaling factor: 1.03

The number of parameters is: 284685

The number of individual parameters is:

9
162
9
9
13
35802
13
13
25
99450
25
25
64
144000
64
64
4096
64
640
10
64
64

nonzero elements in E: 14234248
elements in E: 14234250
fraction nonzero: 0.9999998594938265
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.54; acc: 0.05
Batch: 20; loss: 2.44; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.17
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.25; acc: 0.12
Batch: 100; loss: 2.26; acc: 0.11
Batch: 120; loss: 2.21; acc: 0.19
Batch: 140; loss: 2.12; acc: 0.27
Batch: 160; loss: 2.13; acc: 0.2
Batch: 180; loss: 2.05; acc: 0.27
Batch: 200; loss: 2.1; acc: 0.22
Batch: 220; loss: 2.12; acc: 0.31
Batch: 240; loss: 2.05; acc: 0.31
Batch: 260; loss: 2.02; acc: 0.3
Batch: 280; loss: 2.0; acc: 0.31
Batch: 300; loss: 2.01; acc: 0.2
Batch: 320; loss: 1.97; acc: 0.38
Batch: 340; loss: 2.08; acc: 0.33
Batch: 360; loss: 2.0; acc: 0.33
Batch: 380; loss: 1.94; acc: 0.38
Batch: 400; loss: 1.93; acc: 0.39
Batch: 420; loss: 1.96; acc: 0.38
Batch: 440; loss: 2.0; acc: 0.38
Batch: 460; loss: 1.95; acc: 0.33
Batch: 480; loss: 1.93; acc: 0.42
Batch: 500; loss: 1.97; acc: 0.33
Batch: 520; loss: 1.92; acc: 0.42
Batch: 540; loss: 1.91; acc: 0.39
Batch: 560; loss: 1.94; acc: 0.31
Batch: 580; loss: 2.01; acc: 0.33
Batch: 600; loss: 1.97; acc: 0.39
Batch: 620; loss: 1.89; acc: 0.47
Batch: 640; loss: 1.99; acc: 0.36
Batch: 660; loss: 1.85; acc: 0.47
Batch: 680; loss: 1.99; acc: 0.39
Batch: 700; loss: 1.94; acc: 0.38
Batch: 720; loss: 1.91; acc: 0.41
Batch: 740; loss: 1.93; acc: 0.44
Batch: 760; loss: 1.93; acc: 0.41
Batch: 780; loss: 1.86; acc: 0.53
Train Epoch over. train_loss: 2.03; train_accuracy: 0.32 

2.394644616288133e-05
5.850796696904581e-06
Batch: 0; loss: 2.07; acc: 0.27
Batch: 20; loss: 1.94; acc: 0.39
Batch: 40; loss: 1.68; acc: 0.64
Batch: 60; loss: 1.87; acc: 0.42
Batch: 80; loss: 1.82; acc: 0.48
Batch: 100; loss: 1.9; acc: 0.42
Batch: 120; loss: 1.98; acc: 0.41
Batch: 140; loss: 1.8; acc: 0.62
Val Epoch over. val_loss: 1.8980007475348795; val_accuracy: 0.4341162420382166 

The current subspace-distance is: 5.850796696904581e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.5
Batch: 20; loss: 2.02; acc: 0.42
Batch: 40; loss: 1.91; acc: 0.42
Batch: 60; loss: 1.85; acc: 0.53
Batch: 80; loss: 1.87; acc: 0.42
Batch: 100; loss: 1.94; acc: 0.36
Batch: 120; loss: 1.83; acc: 0.47
Batch: 140; loss: 1.84; acc: 0.45
Batch: 160; loss: 1.95; acc: 0.39
Batch: 180; loss: 1.85; acc: 0.5
Batch: 200; loss: 1.84; acc: 0.41
Batch: 220; loss: 1.85; acc: 0.33
Batch: 240; loss: 1.85; acc: 0.48
Batch: 260; loss: 1.81; acc: 0.48
Batch: 280; loss: 1.77; acc: 0.52
Batch: 300; loss: 1.91; acc: 0.41
Batch: 320; loss: 1.83; acc: 0.48
Batch: 340; loss: 1.97; acc: 0.36
Batch: 360; loss: 1.79; acc: 0.48
Batch: 380; loss: 1.92; acc: 0.39
Batch: 400; loss: 1.84; acc: 0.42
Batch: 420; loss: 2.04; acc: 0.36
Batch: 440; loss: 1.83; acc: 0.5
Batch: 460; loss: 1.97; acc: 0.38
Batch: 480; loss: 1.93; acc: 0.38
Batch: 500; loss: 1.85; acc: 0.42
Batch: 520; loss: 1.76; acc: 0.48
Batch: 540; loss: 1.96; acc: 0.38
Batch: 560; loss: 1.87; acc: 0.47
Batch: 580; loss: 1.78; acc: 0.45
Batch: 600; loss: 1.83; acc: 0.48
Batch: 620; loss: 1.89; acc: 0.31
Batch: 640; loss: 1.79; acc: 0.48
Batch: 660; loss: 1.86; acc: 0.44
Batch: 680; loss: 1.98; acc: 0.34
Batch: 700; loss: 1.84; acc: 0.42
Batch: 720; loss: 1.84; acc: 0.47
Batch: 740; loss: 1.97; acc: 0.33
Batch: 760; loss: 1.84; acc: 0.42
Batch: 780; loss: 1.87; acc: 0.41
Train Epoch over. train_loss: 1.86; train_accuracy: 0.45 

2.6616202376317233e-05
6.997988293733215e-06
Batch: 0; loss: 1.9; acc: 0.42
Batch: 20; loss: 1.87; acc: 0.44
Batch: 40; loss: 1.54; acc: 0.72
Batch: 60; loss: 1.84; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.56
Batch: 100; loss: 1.78; acc: 0.5
Batch: 120; loss: 1.94; acc: 0.34
Batch: 140; loss: 1.74; acc: 0.61
Val Epoch over. val_loss: 1.7933727829319657; val_accuracy: 0.4912420382165605 

The current subspace-distance is: 6.997988293733215e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.84; acc: 0.39
Batch: 20; loss: 1.8; acc: 0.53
Batch: 40; loss: 1.78; acc: 0.53
Batch: 60; loss: 1.83; acc: 0.38
Batch: 80; loss: 1.78; acc: 0.45
Batch: 100; loss: 1.74; acc: 0.53
Batch: 120; loss: 1.85; acc: 0.36
Batch: 140; loss: 1.91; acc: 0.38
Batch: 160; loss: 1.76; acc: 0.53
Batch: 180; loss: 1.65; acc: 0.58
Batch: 200; loss: 1.68; acc: 0.47
Batch: 220; loss: 1.77; acc: 0.52
Batch: 240; loss: 1.91; acc: 0.36
Batch: 260; loss: 1.81; acc: 0.48
Batch: 280; loss: 1.77; acc: 0.48
Batch: 300; loss: 1.87; acc: 0.44
Batch: 320; loss: 1.81; acc: 0.48
Batch: 340; loss: 1.65; acc: 0.58
Batch: 360; loss: 1.61; acc: 0.64
Batch: 380; loss: 1.77; acc: 0.45
Batch: 400; loss: 1.79; acc: 0.55
Batch: 420; loss: 1.86; acc: 0.44
Batch: 440; loss: 1.81; acc: 0.38
Batch: 460; loss: 1.72; acc: 0.58
Batch: 480; loss: 1.92; acc: 0.34
Batch: 500; loss: 1.68; acc: 0.64
Batch: 520; loss: 1.87; acc: 0.39
Batch: 540; loss: 1.69; acc: 0.52
Batch: 560; loss: 1.71; acc: 0.59
Batch: 580; loss: 1.83; acc: 0.5
Batch: 600; loss: 1.67; acc: 0.61
Batch: 620; loss: 1.89; acc: 0.38
Batch: 640; loss: 1.62; acc: 0.61
Batch: 660; loss: 1.65; acc: 0.48
Batch: 680; loss: 1.82; acc: 0.47
Batch: 700; loss: 1.81; acc: 0.52
Batch: 720; loss: 1.7; acc: 0.61
Batch: 740; loss: 1.86; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.52
Batch: 780; loss: 1.67; acc: 0.56
Train Epoch over. train_loss: 1.79; train_accuracy: 0.47 

2.992687586811371e-05
9.953262633644044e-06
Batch: 0; loss: 1.83; acc: 0.41
Batch: 20; loss: 1.83; acc: 0.39
Batch: 40; loss: 1.47; acc: 0.69
Batch: 60; loss: 1.83; acc: 0.44
Batch: 80; loss: 1.67; acc: 0.58
Batch: 100; loss: 1.75; acc: 0.47
Batch: 120; loss: 1.93; acc: 0.38
Batch: 140; loss: 1.68; acc: 0.62
Val Epoch over. val_loss: 1.7479671300596493; val_accuracy: 0.4974124203821656 

The current subspace-distance is: 9.953262633644044e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.48
Batch: 20; loss: 1.78; acc: 0.5
Batch: 40; loss: 1.81; acc: 0.5
Batch: 60; loss: 1.77; acc: 0.53
Batch: 80; loss: 1.83; acc: 0.45
Batch: 100; loss: 1.85; acc: 0.39
Batch: 120; loss: 1.96; acc: 0.31
Batch: 140; loss: 1.79; acc: 0.5
Batch: 160; loss: 1.71; acc: 0.39
Batch: 180; loss: 1.67; acc: 0.47
Batch: 200; loss: 1.78; acc: 0.44
Batch: 220; loss: 1.72; acc: 0.52
Batch: 240; loss: 1.77; acc: 0.55
Batch: 260; loss: 1.8; acc: 0.45
Batch: 280; loss: 1.79; acc: 0.45
Batch: 300; loss: 1.73; acc: 0.48
Batch: 320; loss: 1.89; acc: 0.31
Batch: 340; loss: 1.77; acc: 0.48
Batch: 360; loss: 1.76; acc: 0.47
Batch: 380; loss: 1.7; acc: 0.52
Batch: 400; loss: 1.88; acc: 0.39
Batch: 420; loss: 1.75; acc: 0.5
Batch: 440; loss: 1.76; acc: 0.47
Batch: 460; loss: 1.74; acc: 0.56
Batch: 480; loss: 1.7; acc: 0.53
Batch: 500; loss: 1.61; acc: 0.64
Batch: 520; loss: 1.66; acc: 0.52
Batch: 540; loss: 1.82; acc: 0.47
Batch: 560; loss: 1.75; acc: 0.53
Batch: 580; loss: 1.81; acc: 0.44
Batch: 600; loss: 1.71; acc: 0.5
Batch: 620; loss: 1.76; acc: 0.47
Batch: 640; loss: 1.83; acc: 0.45
Batch: 660; loss: 1.87; acc: 0.45
Batch: 680; loss: 1.74; acc: 0.52
Batch: 700; loss: 1.77; acc: 0.53
Batch: 720; loss: 1.69; acc: 0.52
Batch: 740; loss: 1.71; acc: 0.48
Batch: 760; loss: 1.86; acc: 0.36
Batch: 780; loss: 1.81; acc: 0.48
Train Epoch over. train_loss: 1.76; train_accuracy: 0.47 

3.0512870580423623e-05
8.759857337281574e-06
Batch: 0; loss: 1.78; acc: 0.47
Batch: 20; loss: 1.84; acc: 0.38
Batch: 40; loss: 1.44; acc: 0.69
Batch: 60; loss: 1.8; acc: 0.42
Batch: 80; loss: 1.67; acc: 0.48
Batch: 100; loss: 1.71; acc: 0.48
Batch: 120; loss: 1.91; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Val Epoch over. val_loss: 1.7223846358098802; val_accuracy: 0.50109474522293 

The current subspace-distance is: 8.759857337281574e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.56
Batch: 20; loss: 1.68; acc: 0.55
Batch: 40; loss: 1.76; acc: 0.44
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.76; acc: 0.52
Batch: 100; loss: 1.76; acc: 0.45
Batch: 120; loss: 1.68; acc: 0.52
Batch: 140; loss: 1.7; acc: 0.5
Batch: 160; loss: 1.73; acc: 0.5
Batch: 180; loss: 1.6; acc: 0.66
Batch: 200; loss: 1.72; acc: 0.47
Batch: 220; loss: 1.75; acc: 0.5
Batch: 240; loss: 1.66; acc: 0.59
Batch: 260; loss: 1.78; acc: 0.44
Batch: 280; loss: 1.77; acc: 0.42
Batch: 300; loss: 1.74; acc: 0.5
Batch: 320; loss: 1.63; acc: 0.5
Batch: 340; loss: 1.75; acc: 0.45
Batch: 360; loss: 1.61; acc: 0.52
Batch: 380; loss: 1.68; acc: 0.56
Batch: 400; loss: 1.71; acc: 0.48
Batch: 420; loss: 1.68; acc: 0.53
Batch: 440; loss: 1.66; acc: 0.52
Batch: 460; loss: 1.67; acc: 0.55
Batch: 480; loss: 1.73; acc: 0.52
Batch: 500; loss: 1.74; acc: 0.44
Batch: 520; loss: 1.76; acc: 0.42
Batch: 540; loss: 1.82; acc: 0.53
Batch: 560; loss: 1.67; acc: 0.53
Batch: 580; loss: 1.6; acc: 0.58
Batch: 600; loss: 1.81; acc: 0.41
Batch: 620; loss: 1.71; acc: 0.55
Batch: 640; loss: 1.77; acc: 0.48
Batch: 660; loss: 1.71; acc: 0.5
Batch: 680; loss: 1.81; acc: 0.53
Batch: 700; loss: 1.79; acc: 0.44
Batch: 720; loss: 1.76; acc: 0.44
Batch: 740; loss: 1.78; acc: 0.45
Batch: 760; loss: 1.78; acc: 0.45
Batch: 780; loss: 1.72; acc: 0.53
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.246983033022843e-05
9.606841558706947e-06
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.82; acc: 0.38
Batch: 40; loss: 1.42; acc: 0.69
Batch: 60; loss: 1.79; acc: 0.42
Batch: 80; loss: 1.65; acc: 0.55
Batch: 100; loss: 1.67; acc: 0.59
Batch: 120; loss: 1.9; acc: 0.39
Batch: 140; loss: 1.7; acc: 0.56
Val Epoch over. val_loss: 1.7010497409067336; val_accuracy: 0.5025875796178344 

The current subspace-distance is: 9.606841558706947e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.82; acc: 0.42
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.73; acc: 0.53
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.83; acc: 0.36
Batch: 100; loss: 1.78; acc: 0.44
Batch: 120; loss: 1.74; acc: 0.62
Batch: 140; loss: 1.88; acc: 0.44
Batch: 160; loss: 1.69; acc: 0.52
Batch: 180; loss: 1.68; acc: 0.45
Batch: 200; loss: 1.73; acc: 0.52
Batch: 220; loss: 1.72; acc: 0.45
Batch: 240; loss: 1.78; acc: 0.42
Batch: 260; loss: 1.7; acc: 0.48
Batch: 280; loss: 1.84; acc: 0.41
Batch: 300; loss: 1.67; acc: 0.5
Batch: 320; loss: 1.7; acc: 0.52
Batch: 340; loss: 1.74; acc: 0.5
Batch: 360; loss: 1.86; acc: 0.44
Batch: 380; loss: 1.76; acc: 0.47
Batch: 400; loss: 1.82; acc: 0.38
Batch: 420; loss: 1.79; acc: 0.41
Batch: 440; loss: 1.71; acc: 0.47
Batch: 460; loss: 1.8; acc: 0.45
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.64; acc: 0.55
Batch: 520; loss: 1.76; acc: 0.48
Batch: 540; loss: 1.76; acc: 0.52
Batch: 560; loss: 1.77; acc: 0.44
Batch: 580; loss: 1.8; acc: 0.42
Batch: 600; loss: 1.68; acc: 0.45
Batch: 620; loss: 1.88; acc: 0.34
Batch: 640; loss: 1.68; acc: 0.5
Batch: 660; loss: 1.75; acc: 0.47
Batch: 680; loss: 1.76; acc: 0.47
Batch: 700; loss: 1.63; acc: 0.55
Batch: 720; loss: 1.8; acc: 0.44
Batch: 740; loss: 1.8; acc: 0.44
Batch: 760; loss: 1.76; acc: 0.47
Batch: 780; loss: 1.72; acc: 0.52
Train Epoch over. train_loss: 1.73; train_accuracy: 0.48 

3.494277552817948e-05
1.1533245924510993e-05
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.83; acc: 0.36
Batch: 40; loss: 1.43; acc: 0.66
Batch: 60; loss: 1.77; acc: 0.44
Batch: 80; loss: 1.67; acc: 0.53
Batch: 100; loss: 1.65; acc: 0.58
Batch: 120; loss: 1.89; acc: 0.39
Batch: 140; loss: 1.72; acc: 0.55
Val Epoch over. val_loss: 1.6973491952677442; val_accuracy: 0.5039808917197452 

The current subspace-distance is: 1.1533245924510993e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.56
Batch: 40; loss: 1.71; acc: 0.42
Batch: 60; loss: 1.86; acc: 0.44
Batch: 80; loss: 1.48; acc: 0.66
Batch: 100; loss: 1.79; acc: 0.38
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.74; acc: 0.52
Batch: 160; loss: 1.79; acc: 0.41
Batch: 180; loss: 1.67; acc: 0.5
Batch: 200; loss: 1.7; acc: 0.5
Batch: 220; loss: 1.73; acc: 0.42
Batch: 240; loss: 1.71; acc: 0.44
Batch: 260; loss: 1.62; acc: 0.55
Batch: 280; loss: 1.98; acc: 0.39
Batch: 300; loss: 1.75; acc: 0.5
Batch: 320; loss: 1.52; acc: 0.64
Batch: 340; loss: 1.72; acc: 0.44
Batch: 360; loss: 1.72; acc: 0.42
Batch: 380; loss: 1.78; acc: 0.48
Batch: 400; loss: 1.75; acc: 0.45
Batch: 420; loss: 1.73; acc: 0.5
Batch: 440; loss: 1.66; acc: 0.52
Batch: 460; loss: 1.63; acc: 0.48
Batch: 480; loss: 1.77; acc: 0.44
Batch: 500; loss: 1.65; acc: 0.53
Batch: 520; loss: 1.7; acc: 0.5
Batch: 540; loss: 1.6; acc: 0.64
Batch: 560; loss: 1.52; acc: 0.61
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.73; acc: 0.48
Batch: 620; loss: 1.65; acc: 0.61
Batch: 640; loss: 1.69; acc: 0.52
Batch: 660; loss: 1.76; acc: 0.44
Batch: 680; loss: 1.65; acc: 0.5
Batch: 700; loss: 1.78; acc: 0.39
Batch: 720; loss: 1.66; acc: 0.55
Batch: 740; loss: 1.85; acc: 0.39
Batch: 760; loss: 1.77; acc: 0.42
Batch: 780; loss: 1.69; acc: 0.48
Train Epoch over. train_loss: 1.72; train_accuracy: 0.48 

3.507685323711485e-05
9.868555935099721e-06
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.82; acc: 0.38
Batch: 40; loss: 1.41; acc: 0.61
Batch: 60; loss: 1.75; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.55
Batch: 100; loss: 1.63; acc: 0.58
Batch: 120; loss: 1.89; acc: 0.39
Batch: 140; loss: 1.71; acc: 0.55
Val Epoch over. val_loss: 1.6837748205585845; val_accuracy: 0.5021894904458599 

The current subspace-distance is: 9.868555935099721e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.52
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.7; acc: 0.44
Batch: 60; loss: 1.75; acc: 0.47
Batch: 80; loss: 1.74; acc: 0.44
Batch: 100; loss: 1.81; acc: 0.47
Batch: 120; loss: 1.85; acc: 0.44
Batch: 140; loss: 1.72; acc: 0.5
Batch: 160; loss: 1.7; acc: 0.5
Batch: 180; loss: 1.67; acc: 0.5
Batch: 200; loss: 1.69; acc: 0.5
Batch: 220; loss: 1.8; acc: 0.39
Batch: 240; loss: 1.86; acc: 0.41
Batch: 260; loss: 1.75; acc: 0.47
Batch: 280; loss: 1.66; acc: 0.5
Batch: 300; loss: 1.71; acc: 0.44
Batch: 320; loss: 1.78; acc: 0.42
Batch: 340; loss: 1.55; acc: 0.59
Batch: 360; loss: 1.79; acc: 0.44
Batch: 380; loss: 1.67; acc: 0.5
Batch: 400; loss: 1.69; acc: 0.47
Batch: 420; loss: 1.79; acc: 0.52
Batch: 440; loss: 1.59; acc: 0.5
Batch: 460; loss: 1.78; acc: 0.39
Batch: 480; loss: 1.65; acc: 0.53
Batch: 500; loss: 1.77; acc: 0.44
Batch: 520; loss: 1.8; acc: 0.48
Batch: 540; loss: 1.64; acc: 0.52
Batch: 560; loss: 1.73; acc: 0.5
Batch: 580; loss: 1.78; acc: 0.44
Batch: 600; loss: 1.77; acc: 0.34
Batch: 620; loss: 1.82; acc: 0.45
Batch: 640; loss: 1.63; acc: 0.48
Batch: 660; loss: 1.81; acc: 0.44
Batch: 680; loss: 1.76; acc: 0.45
Batch: 700; loss: 1.59; acc: 0.55
Batch: 720; loss: 1.65; acc: 0.61
Batch: 740; loss: 1.82; acc: 0.48
Batch: 760; loss: 1.72; acc: 0.42
Batch: 780; loss: 1.81; acc: 0.38
Train Epoch over. train_loss: 1.71; train_accuracy: 0.48 

3.6070847272640094e-05
1.1595183423196431e-05
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.81; acc: 0.38
Batch: 40; loss: 1.39; acc: 0.64
Batch: 60; loss: 1.72; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.53
Batch: 100; loss: 1.62; acc: 0.59
Batch: 120; loss: 1.9; acc: 0.42
Batch: 140; loss: 1.7; acc: 0.53
Val Epoch over. val_loss: 1.6710652074996073; val_accuracy: 0.5066679936305732 

The current subspace-distance is: 1.1595183423196431e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.86; acc: 0.36
Batch: 20; loss: 1.79; acc: 0.52
Batch: 40; loss: 1.82; acc: 0.39
Batch: 60; loss: 1.77; acc: 0.36
Batch: 80; loss: 1.77; acc: 0.45
Batch: 100; loss: 1.58; acc: 0.59
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.8; acc: 0.42
Batch: 160; loss: 1.8; acc: 0.36
Batch: 180; loss: 1.85; acc: 0.48
Batch: 200; loss: 1.71; acc: 0.56
Batch: 220; loss: 1.7; acc: 0.53
Batch: 240; loss: 1.71; acc: 0.45
Batch: 260; loss: 1.74; acc: 0.44
Batch: 280; loss: 1.79; acc: 0.33
Batch: 300; loss: 1.67; acc: 0.47
Batch: 320; loss: 1.71; acc: 0.48
Batch: 340; loss: 1.66; acc: 0.5
Batch: 360; loss: 1.7; acc: 0.5
Batch: 380; loss: 1.72; acc: 0.36
Batch: 400; loss: 1.62; acc: 0.52
Batch: 420; loss: 1.69; acc: 0.52
Batch: 440; loss: 1.73; acc: 0.48
Batch: 460; loss: 1.87; acc: 0.39
Batch: 480; loss: 1.59; acc: 0.56
Batch: 500; loss: 1.73; acc: 0.45
Batch: 520; loss: 1.65; acc: 0.53
Batch: 540; loss: 1.83; acc: 0.44
Batch: 560; loss: 1.76; acc: 0.41
Batch: 580; loss: 1.7; acc: 0.55
Batch: 600; loss: 1.71; acc: 0.44
Batch: 620; loss: 1.61; acc: 0.5
Batch: 640; loss: 1.63; acc: 0.5
Batch: 660; loss: 1.65; acc: 0.48
Batch: 680; loss: 1.63; acc: 0.55
Batch: 700; loss: 1.64; acc: 0.5
Batch: 720; loss: 1.63; acc: 0.61
Batch: 740; loss: 1.61; acc: 0.58
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.76; acc: 0.44
Train Epoch over. train_loss: 1.7; train_accuracy: 0.48 

3.7349644117057323e-05
1.3826832400809508e-05
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.38; acc: 0.69
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.53
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.9; acc: 0.41
Batch: 140; loss: 1.68; acc: 0.52
Val Epoch over. val_loss: 1.661220069903477; val_accuracy: 0.5098527070063694 

The current subspace-distance is: 1.3826832400809508e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.7; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.44
Batch: 60; loss: 1.75; acc: 0.44
Batch: 80; loss: 1.68; acc: 0.5
Batch: 100; loss: 1.69; acc: 0.5
Batch: 120; loss: 1.68; acc: 0.47
Batch: 140; loss: 1.67; acc: 0.47
Batch: 160; loss: 1.71; acc: 0.47
Batch: 180; loss: 1.75; acc: 0.45
Batch: 200; loss: 1.74; acc: 0.45
Batch: 220; loss: 1.68; acc: 0.56
Batch: 240; loss: 1.67; acc: 0.52
Batch: 260; loss: 1.59; acc: 0.53
Batch: 280; loss: 1.59; acc: 0.56
Batch: 300; loss: 1.67; acc: 0.48
Batch: 320; loss: 1.66; acc: 0.5
Batch: 340; loss: 1.67; acc: 0.47
Batch: 360; loss: 1.64; acc: 0.52
Batch: 380; loss: 1.79; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.53
Batch: 420; loss: 1.75; acc: 0.47
Batch: 440; loss: 1.65; acc: 0.44
Batch: 460; loss: 1.75; acc: 0.42
Batch: 480; loss: 1.75; acc: 0.44
Batch: 500; loss: 1.59; acc: 0.56
Batch: 520; loss: 1.76; acc: 0.42
Batch: 540; loss: 1.7; acc: 0.52
Batch: 560; loss: 1.61; acc: 0.52
Batch: 580; loss: 1.7; acc: 0.47
Batch: 600; loss: 1.63; acc: 0.53
Batch: 620; loss: 1.77; acc: 0.53
Batch: 640; loss: 1.61; acc: 0.55
Batch: 660; loss: 1.63; acc: 0.55
Batch: 680; loss: 1.69; acc: 0.45
Batch: 700; loss: 1.75; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.5
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.57; acc: 0.5
Batch: 780; loss: 1.65; acc: 0.45
Train Epoch over. train_loss: 1.68; train_accuracy: 0.49 

3.811253554886207e-05
1.2153104762546718e-05
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.41
Batch: 40; loss: 1.35; acc: 0.72
Batch: 60; loss: 1.66; acc: 0.56
Batch: 80; loss: 1.66; acc: 0.52
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.91; acc: 0.41
Batch: 140; loss: 1.64; acc: 0.52
Val Epoch over. val_loss: 1.6414855171920388; val_accuracy: 0.5105493630573248 

The current subspace-distance is: 1.2153104762546718e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.44
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.64; acc: 0.55
Batch: 60; loss: 1.59; acc: 0.56
Batch: 80; loss: 1.52; acc: 0.56
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.47; acc: 0.62
Batch: 140; loss: 1.78; acc: 0.42
Batch: 160; loss: 1.66; acc: 0.48
Batch: 180; loss: 1.7; acc: 0.47
Batch: 200; loss: 1.71; acc: 0.42
Batch: 220; loss: 1.59; acc: 0.59
Batch: 240; loss: 1.73; acc: 0.41
Batch: 260; loss: 1.82; acc: 0.41
Batch: 280; loss: 1.59; acc: 0.53
Batch: 300; loss: 1.67; acc: 0.53
Batch: 320; loss: 1.63; acc: 0.56
Batch: 340; loss: 1.79; acc: 0.42
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 1.75; acc: 0.44
Batch: 400; loss: 1.76; acc: 0.42
Batch: 420; loss: 1.71; acc: 0.53
Batch: 440; loss: 1.64; acc: 0.52
Batch: 460; loss: 1.8; acc: 0.39
Batch: 480; loss: 1.87; acc: 0.34
Batch: 500; loss: 1.69; acc: 0.45
Batch: 520; loss: 1.62; acc: 0.47
Batch: 540; loss: 1.73; acc: 0.41
Batch: 560; loss: 1.63; acc: 0.5
Batch: 580; loss: 1.67; acc: 0.52
Batch: 600; loss: 1.72; acc: 0.36
Batch: 620; loss: 1.72; acc: 0.5
Batch: 640; loss: 1.81; acc: 0.31
Batch: 660; loss: 1.63; acc: 0.52
Batch: 680; loss: 1.74; acc: 0.44
Batch: 700; loss: 1.64; acc: 0.48
Batch: 720; loss: 1.7; acc: 0.45
Batch: 740; loss: 1.67; acc: 0.52
Batch: 760; loss: 1.68; acc: 0.53
Batch: 780; loss: 1.56; acc: 0.53
Train Epoch over. train_loss: 1.67; train_accuracy: 0.49 

3.85265848308336e-05
1.371426787954988e-05
Batch: 0; loss: 1.63; acc: 0.56
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.36; acc: 0.7
Batch: 60; loss: 1.67; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.52
Batch: 100; loss: 1.61; acc: 0.55
Batch: 120; loss: 1.91; acc: 0.39
Batch: 140; loss: 1.64; acc: 0.56
Val Epoch over. val_loss: 1.6404026655634498; val_accuracy: 0.5163216560509554 

The current subspace-distance is: 1.371426787954988e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.78; acc: 0.44
Batch: 40; loss: 1.58; acc: 0.52
Batch: 60; loss: 1.67; acc: 0.56
Batch: 80; loss: 1.67; acc: 0.5
Batch: 100; loss: 1.54; acc: 0.62
Batch: 120; loss: 1.69; acc: 0.53
Batch: 140; loss: 1.7; acc: 0.55
Batch: 160; loss: 1.75; acc: 0.45
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.78; acc: 0.5
Batch: 220; loss: 1.59; acc: 0.56
Batch: 240; loss: 1.59; acc: 0.47
Batch: 260; loss: 1.72; acc: 0.5
Batch: 280; loss: 1.73; acc: 0.53
Batch: 300; loss: 1.63; acc: 0.52
Batch: 320; loss: 1.57; acc: 0.56
Batch: 340; loss: 1.62; acc: 0.48
Batch: 360; loss: 1.61; acc: 0.5
Batch: 380; loss: 1.64; acc: 0.53
Batch: 400; loss: 1.66; acc: 0.5
Batch: 420; loss: 1.69; acc: 0.5
Batch: 440; loss: 1.73; acc: 0.5
Batch: 460; loss: 1.71; acc: 0.47
Batch: 480; loss: 1.65; acc: 0.52
Batch: 500; loss: 1.59; acc: 0.52
Batch: 520; loss: 1.74; acc: 0.42
Batch: 540; loss: 1.69; acc: 0.42
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.79; acc: 0.47
Batch: 600; loss: 1.74; acc: 0.39
Batch: 620; loss: 1.88; acc: 0.39
Batch: 640; loss: 1.65; acc: 0.64
Batch: 660; loss: 1.7; acc: 0.44
Batch: 680; loss: 1.58; acc: 0.44
Batch: 700; loss: 1.6; acc: 0.56
Batch: 720; loss: 1.63; acc: 0.53
Batch: 740; loss: 1.64; acc: 0.56
Batch: 760; loss: 1.73; acc: 0.48
Batch: 780; loss: 1.74; acc: 0.42
Train Epoch over. train_loss: 1.67; train_accuracy: 0.49 

3.852026566164568e-05
1.3455116459226701e-05
Batch: 0; loss: 1.62; acc: 0.56
Batch: 20; loss: 1.75; acc: 0.44
Batch: 40; loss: 1.35; acc: 0.69
Batch: 60; loss: 1.65; acc: 0.55
Batch: 80; loss: 1.65; acc: 0.5
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.91; acc: 0.38
Batch: 140; loss: 1.61; acc: 0.56
Val Epoch over. val_loss: 1.630812906915215; val_accuracy: 0.5136345541401274 

The current subspace-distance is: 1.3455116459226701e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.55
Batch: 20; loss: 1.72; acc: 0.41
Batch: 40; loss: 1.72; acc: 0.45
Batch: 60; loss: 1.58; acc: 0.56
Batch: 80; loss: 1.67; acc: 0.5
Batch: 100; loss: 1.56; acc: 0.59
Batch: 120; loss: 1.71; acc: 0.5
Batch: 140; loss: 1.58; acc: 0.48
Batch: 160; loss: 1.75; acc: 0.45
Batch: 180; loss: 1.67; acc: 0.53
Batch: 200; loss: 1.65; acc: 0.48
Batch: 220; loss: 1.59; acc: 0.52
Batch: 240; loss: 1.54; acc: 0.59
Batch: 260; loss: 1.69; acc: 0.44
Batch: 280; loss: 1.7; acc: 0.38
Batch: 300; loss: 1.59; acc: 0.56
Batch: 320; loss: 1.63; acc: 0.53
Batch: 340; loss: 1.63; acc: 0.56
Batch: 360; loss: 1.62; acc: 0.59
Batch: 380; loss: 1.73; acc: 0.45
Batch: 400; loss: 1.58; acc: 0.53
Batch: 420; loss: 1.78; acc: 0.44
Batch: 440; loss: 1.63; acc: 0.56
Batch: 460; loss: 1.61; acc: 0.45
Batch: 480; loss: 1.5; acc: 0.55
Batch: 500; loss: 1.55; acc: 0.62
Batch: 520; loss: 1.55; acc: 0.52
Batch: 540; loss: 1.79; acc: 0.45
Batch: 560; loss: 1.62; acc: 0.59
Batch: 580; loss: 1.74; acc: 0.47
Batch: 600; loss: 1.71; acc: 0.5
Batch: 620; loss: 1.74; acc: 0.45
Batch: 640; loss: 1.63; acc: 0.58
Batch: 660; loss: 1.68; acc: 0.47
Batch: 680; loss: 1.68; acc: 0.52
Batch: 700; loss: 1.78; acc: 0.44
Batch: 720; loss: 1.59; acc: 0.45
Batch: 740; loss: 1.77; acc: 0.47
Batch: 760; loss: 1.69; acc: 0.5
Batch: 780; loss: 1.61; acc: 0.56
Train Epoch over. train_loss: 1.66; train_accuracy: 0.5 

3.9489819755544886e-05
1.2319731467869133e-05
Batch: 0; loss: 1.62; acc: 0.53
Batch: 20; loss: 1.73; acc: 0.45
Batch: 40; loss: 1.34; acc: 0.7
Batch: 60; loss: 1.64; acc: 0.53
Batch: 80; loss: 1.65; acc: 0.5
Batch: 100; loss: 1.58; acc: 0.59
Batch: 120; loss: 1.91; acc: 0.36
Batch: 140; loss: 1.59; acc: 0.55
Val Epoch over. val_loss: 1.621383196988683; val_accuracy: 0.5173168789808917 

The current subspace-distance is: 1.2319731467869133e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.55; acc: 0.59
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.66; acc: 0.53
Batch: 60; loss: 1.63; acc: 0.41
Batch: 80; loss: 1.82; acc: 0.41
Batch: 100; loss: 1.68; acc: 0.52
Batch: 120; loss: 1.63; acc: 0.56
Batch: 140; loss: 1.56; acc: 0.58
Batch: 160; loss: 1.54; acc: 0.56
Batch: 180; loss: 1.74; acc: 0.47
Batch: 200; loss: 1.61; acc: 0.44
Batch: 220; loss: 1.73; acc: 0.48
Batch: 240; loss: 1.64; acc: 0.53
Batch: 260; loss: 1.68; acc: 0.48
Batch: 280; loss: 1.62; acc: 0.47
Batch: 300; loss: 1.72; acc: 0.53
Batch: 320; loss: 1.55; acc: 0.59
Batch: 340; loss: 1.67; acc: 0.41
Batch: 360; loss: 1.73; acc: 0.44
Batch: 380; loss: 1.57; acc: 0.55
Batch: 400; loss: 1.76; acc: 0.38
Batch: 420; loss: 1.51; acc: 0.59
Batch: 440; loss: 1.57; acc: 0.47
Batch: 460; loss: 1.67; acc: 0.58
Batch: 480; loss: 1.67; acc: 0.47
Batch: 500; loss: 1.61; acc: 0.5
Batch: 520; loss: 1.65; acc: 0.48
Batch: 540; loss: 1.69; acc: 0.42
Batch: 560; loss: 1.6; acc: 0.5
Batch: 580; loss: 1.73; acc: 0.5
Batch: 600; loss: 1.49; acc: 0.61
Batch: 620; loss: 1.65; acc: 0.52
Batch: 640; loss: 1.63; acc: 0.47
Batch: 660; loss: 1.58; acc: 0.56
Batch: 680; loss: 1.6; acc: 0.48
Batch: 700; loss: 1.49; acc: 0.59
Batch: 720; loss: 1.83; acc: 0.39
Batch: 740; loss: 1.6; acc: 0.48
Batch: 760; loss: 1.56; acc: 0.52
Batch: 780; loss: 1.57; acc: 0.55
Train Epoch over. train_loss: 1.65; train_accuracy: 0.5 

3.998594911536202e-05
1.2964213965460658e-05
Batch: 0; loss: 1.59; acc: 0.58
Batch: 20; loss: 1.73; acc: 0.45
Batch: 40; loss: 1.32; acc: 0.7
Batch: 60; loss: 1.63; acc: 0.52
Batch: 80; loss: 1.62; acc: 0.52
Batch: 100; loss: 1.56; acc: 0.61
Batch: 120; loss: 1.9; acc: 0.39
Batch: 140; loss: 1.58; acc: 0.53
Val Epoch over. val_loss: 1.6084860403826282; val_accuracy: 0.5215963375796179 

The current subspace-distance is: 1.2964213965460658e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.55
Batch: 20; loss: 1.8; acc: 0.47
Batch: 40; loss: 1.76; acc: 0.41
Batch: 60; loss: 1.65; acc: 0.56
Batch: 80; loss: 1.68; acc: 0.48
Batch: 100; loss: 1.55; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.68; acc: 0.47
Batch: 160; loss: 1.7; acc: 0.47
Batch: 180; loss: 1.79; acc: 0.44
Batch: 200; loss: 1.67; acc: 0.55
Batch: 220; loss: 1.68; acc: 0.45
Batch: 240; loss: 1.69; acc: 0.52
Batch: 260; loss: 1.63; acc: 0.53
Batch: 280; loss: 1.57; acc: 0.55
Batch: 300; loss: 1.61; acc: 0.45
Batch: 320; loss: 1.55; acc: 0.52
Batch: 340; loss: 1.68; acc: 0.5
Batch: 360; loss: 1.68; acc: 0.44
Batch: 380; loss: 1.68; acc: 0.45
Batch: 400; loss: 1.63; acc: 0.5
Batch: 420; loss: 1.65; acc: 0.53
Batch: 440; loss: 1.68; acc: 0.5
Batch: 460; loss: 1.65; acc: 0.48
Batch: 480; loss: 1.58; acc: 0.5
Batch: 500; loss: 1.61; acc: 0.55
Batch: 520; loss: 1.78; acc: 0.34
Batch: 540; loss: 1.65; acc: 0.47
Batch: 560; loss: 1.72; acc: 0.47
Batch: 580; loss: 1.76; acc: 0.42
Batch: 600; loss: 1.51; acc: 0.58
Batch: 620; loss: 1.58; acc: 0.52
Batch: 640; loss: 1.74; acc: 0.5
Batch: 660; loss: 1.72; acc: 0.39
Batch: 680; loss: 1.74; acc: 0.41
Batch: 700; loss: 1.57; acc: 0.5
Batch: 720; loss: 1.61; acc: 0.58
Batch: 740; loss: 1.66; acc: 0.52
Batch: 760; loss: 1.62; acc: 0.53
Batch: 780; loss: 1.76; acc: 0.41
Train Epoch over. train_loss: 1.65; train_accuracy: 0.5 

4.0427246858598664e-05
1.4831336557108443e-05
Batch: 0; loss: 1.6; acc: 0.52
Batch: 20; loss: 1.72; acc: 0.45
Batch: 40; loss: 1.34; acc: 0.7
Batch: 60; loss: 1.64; acc: 0.53
Batch: 80; loss: 1.63; acc: 0.52
Batch: 100; loss: 1.56; acc: 0.61
Batch: 120; loss: 1.9; acc: 0.41
Batch: 140; loss: 1.57; acc: 0.55
Val Epoch over. val_loss: 1.6141118616055532; val_accuracy: 0.5267714968152867 

The current subspace-distance is: 1.4831336557108443e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.52; acc: 0.61
Batch: 40; loss: 1.74; acc: 0.48
Batch: 60; loss: 1.59; acc: 0.58
Batch: 80; loss: 1.72; acc: 0.52
Batch: 100; loss: 1.65; acc: 0.42
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.68; acc: 0.56
Batch: 180; loss: 1.66; acc: 0.5
Batch: 200; loss: 1.77; acc: 0.44
Batch: 220; loss: 1.65; acc: 0.52
Batch: 240; loss: 1.63; acc: 0.47
Batch: 260; loss: 1.74; acc: 0.41
Batch: 280; loss: 1.68; acc: 0.5
Batch: 300; loss: 1.49; acc: 0.64
Batch: 320; loss: 1.51; acc: 0.55
Batch: 340; loss: 1.65; acc: 0.52
Batch: 360; loss: 1.57; acc: 0.55
Batch: 380; loss: 1.75; acc: 0.45
Batch: 400; loss: 1.55; acc: 0.56
Batch: 420; loss: 1.64; acc: 0.55
Batch: 440; loss: 1.56; acc: 0.55
Batch: 460; loss: 1.69; acc: 0.5
Batch: 480; loss: 1.62; acc: 0.61
Batch: 500; loss: 1.61; acc: 0.58
Batch: 520; loss: 1.71; acc: 0.45
Batch: 540; loss: 1.58; acc: 0.64
Batch: 560; loss: 1.72; acc: 0.44
Batch: 580; loss: 1.7; acc: 0.47
Batch: 600; loss: 1.59; acc: 0.53
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.71; acc: 0.5
Batch: 660; loss: 1.63; acc: 0.44
Batch: 680; loss: 1.74; acc: 0.45
Batch: 700; loss: 1.6; acc: 0.55
Batch: 720; loss: 1.54; acc: 0.55
Batch: 740; loss: 1.73; acc: 0.52
Batch: 760; loss: 1.59; acc: 0.59
Batch: 780; loss: 1.56; acc: 0.62
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

4.0274349885294214e-05
1.4472850125457626e-05
Batch: 0; loss: 1.58; acc: 0.5
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.32; acc: 0.69
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.6; acc: 0.53
Batch: 100; loss: 1.53; acc: 0.61
Batch: 120; loss: 1.89; acc: 0.38
Batch: 140; loss: 1.55; acc: 0.56
Val Epoch over. val_loss: 1.5979125302308683; val_accuracy: 0.5299562101910829 

The current subspace-distance is: 1.4472850125457626e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.45
Batch: 40; loss: 1.52; acc: 0.61
Batch: 60; loss: 1.67; acc: 0.52
Batch: 80; loss: 1.63; acc: 0.53
Batch: 100; loss: 1.66; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.53
Batch: 140; loss: 1.59; acc: 0.48
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.62; acc: 0.55
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.6; acc: 0.5
Batch: 240; loss: 1.69; acc: 0.42
Batch: 260; loss: 1.57; acc: 0.58
Batch: 280; loss: 1.61; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.52
Batch: 320; loss: 1.67; acc: 0.52
Batch: 340; loss: 1.79; acc: 0.38
Batch: 360; loss: 1.57; acc: 0.59
Batch: 380; loss: 1.57; acc: 0.55
Batch: 400; loss: 1.7; acc: 0.56
Batch: 420; loss: 1.59; acc: 0.53
Batch: 440; loss: 1.57; acc: 0.59
Batch: 460; loss: 1.65; acc: 0.52
Batch: 480; loss: 1.57; acc: 0.61
Batch: 500; loss: 1.69; acc: 0.45
Batch: 520; loss: 1.54; acc: 0.58
Batch: 540; loss: 1.59; acc: 0.55
Batch: 560; loss: 1.53; acc: 0.56
Batch: 580; loss: 1.59; acc: 0.58
Batch: 600; loss: 1.5; acc: 0.66
Batch: 620; loss: 1.68; acc: 0.55
Batch: 640; loss: 1.66; acc: 0.56
Batch: 660; loss: 1.69; acc: 0.47
Batch: 680; loss: 1.68; acc: 0.5
Batch: 700; loss: 1.61; acc: 0.48
Batch: 720; loss: 1.54; acc: 0.55
Batch: 740; loss: 1.69; acc: 0.53
Batch: 760; loss: 1.68; acc: 0.53
Batch: 780; loss: 1.72; acc: 0.47
Train Epoch over. train_loss: 1.63; train_accuracy: 0.51 

4.19532552768942e-05
1.5736692148493603e-05
Batch: 0; loss: 1.57; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.45
Batch: 40; loss: 1.31; acc: 0.72
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.56
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.87; acc: 0.42
Batch: 140; loss: 1.53; acc: 0.59
Val Epoch over. val_loss: 1.5863846289883754; val_accuracy: 0.5406050955414012 

The current subspace-distance is: 1.5736692148493603e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.58; acc: 0.47
Batch: 40; loss: 1.52; acc: 0.59
Batch: 60; loss: 1.69; acc: 0.44
Batch: 80; loss: 1.61; acc: 0.44
Batch: 100; loss: 1.79; acc: 0.45
Batch: 120; loss: 1.63; acc: 0.47
Batch: 140; loss: 1.55; acc: 0.56
Batch: 160; loss: 1.52; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.53
Batch: 200; loss: 1.61; acc: 0.55
Batch: 220; loss: 1.52; acc: 0.58
Batch: 240; loss: 1.71; acc: 0.44
Batch: 260; loss: 1.56; acc: 0.55
Batch: 280; loss: 1.77; acc: 0.41
Batch: 300; loss: 1.54; acc: 0.61
Batch: 320; loss: 1.59; acc: 0.53
Batch: 340; loss: 1.61; acc: 0.55
Batch: 360; loss: 1.49; acc: 0.52
Batch: 380; loss: 1.55; acc: 0.55
Batch: 400; loss: 1.53; acc: 0.56
Batch: 420; loss: 1.59; acc: 0.53
Batch: 440; loss: 1.51; acc: 0.58
Batch: 460; loss: 1.52; acc: 0.52
Batch: 480; loss: 1.53; acc: 0.59
Batch: 500; loss: 1.61; acc: 0.53
Batch: 520; loss: 1.7; acc: 0.47
Batch: 540; loss: 1.57; acc: 0.52
Batch: 560; loss: 1.59; acc: 0.48
Batch: 580; loss: 1.64; acc: 0.53
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.63; acc: 0.48
Batch: 640; loss: 1.66; acc: 0.42
Batch: 660; loss: 1.66; acc: 0.48
Batch: 680; loss: 1.59; acc: 0.45
Batch: 700; loss: 1.52; acc: 0.58
Batch: 720; loss: 1.6; acc: 0.61
Batch: 740; loss: 1.71; acc: 0.48
Batch: 760; loss: 1.66; acc: 0.53
Batch: 780; loss: 1.6; acc: 0.5
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.097510827705264e-05
1.2925203918712214e-05
Batch: 0; loss: 1.54; acc: 0.53
Batch: 20; loss: 1.68; acc: 0.45
Batch: 40; loss: 1.29; acc: 0.7
Batch: 60; loss: 1.6; acc: 0.5
Batch: 80; loss: 1.56; acc: 0.56
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.85; acc: 0.39
Batch: 140; loss: 1.51; acc: 0.61
Val Epoch over. val_loss: 1.5674221546027312; val_accuracy: 0.5505573248407644 

The current subspace-distance is: 1.2925203918712214e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.5
Batch: 20; loss: 1.56; acc: 0.52
Batch: 40; loss: 1.56; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.53
Batch: 80; loss: 1.69; acc: 0.47
Batch: 100; loss: 1.77; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.53
Batch: 140; loss: 1.73; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.55
Batch: 180; loss: 1.43; acc: 0.64
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.7; acc: 0.44
Batch: 240; loss: 1.53; acc: 0.59
Batch: 260; loss: 1.53; acc: 0.5
Batch: 280; loss: 1.46; acc: 0.62
Batch: 300; loss: 1.6; acc: 0.53
Batch: 320; loss: 1.53; acc: 0.55
Batch: 340; loss: 1.63; acc: 0.48
Batch: 360; loss: 1.63; acc: 0.5
Batch: 380; loss: 1.57; acc: 0.52
Batch: 400; loss: 1.78; acc: 0.41
Batch: 420; loss: 1.58; acc: 0.52
Batch: 440; loss: 1.7; acc: 0.47
Batch: 460; loss: 1.61; acc: 0.58
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.77; acc: 0.41
Batch: 520; loss: 1.65; acc: 0.48
Batch: 540; loss: 1.65; acc: 0.48
Batch: 560; loss: 1.5; acc: 0.66
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.55; acc: 0.55
Batch: 620; loss: 1.61; acc: 0.53
Batch: 640; loss: 1.63; acc: 0.5
Batch: 660; loss: 1.62; acc: 0.58
Batch: 680; loss: 1.67; acc: 0.48
Batch: 700; loss: 1.39; acc: 0.72
Batch: 720; loss: 1.6; acc: 0.64
Batch: 740; loss: 1.69; acc: 0.48
Batch: 760; loss: 1.62; acc: 0.53
Batch: 780; loss: 1.73; acc: 0.44
Train Epoch over. train_loss: 1.61; train_accuracy: 0.53 

4.1988070734078065e-05
1.363367755402578e-05
Batch: 0; loss: 1.54; acc: 0.53
Batch: 20; loss: 1.67; acc: 0.45
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 1.61; acc: 0.5
Batch: 80; loss: 1.56; acc: 0.58
Batch: 100; loss: 1.48; acc: 0.59
Batch: 120; loss: 1.85; acc: 0.41
Batch: 140; loss: 1.48; acc: 0.66
Val Epoch over. val_loss: 1.5657397189717384; val_accuracy: 0.5534434713375797 

The current subspace-distance is: 1.363367755402578e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.48
Batch: 20; loss: 1.56; acc: 0.52
Batch: 40; loss: 1.66; acc: 0.52
Batch: 60; loss: 1.66; acc: 0.58
Batch: 80; loss: 1.6; acc: 0.58
Batch: 100; loss: 1.69; acc: 0.44
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.72; acc: 0.52
Batch: 160; loss: 1.7; acc: 0.44
Batch: 180; loss: 1.67; acc: 0.47
Batch: 200; loss: 1.48; acc: 0.58
Batch: 220; loss: 1.76; acc: 0.45
Batch: 240; loss: 1.57; acc: 0.56
Batch: 260; loss: 1.63; acc: 0.53
Batch: 280; loss: 1.59; acc: 0.55
Batch: 300; loss: 1.6; acc: 0.55
Batch: 320; loss: 1.58; acc: 0.55
Batch: 340; loss: 1.61; acc: 0.58
Batch: 360; loss: 1.51; acc: 0.59
Batch: 380; loss: 1.66; acc: 0.44
Batch: 400; loss: 1.69; acc: 0.45
Batch: 420; loss: 1.46; acc: 0.62
Batch: 440; loss: 1.72; acc: 0.38
Batch: 460; loss: 1.62; acc: 0.53
Batch: 480; loss: 1.45; acc: 0.66
Batch: 500; loss: 1.52; acc: 0.59
Batch: 520; loss: 1.57; acc: 0.56
Batch: 540; loss: 1.66; acc: 0.52
Batch: 560; loss: 1.66; acc: 0.56
Batch: 580; loss: 1.52; acc: 0.61
Batch: 600; loss: 1.65; acc: 0.44
Batch: 620; loss: 1.53; acc: 0.58
Batch: 640; loss: 1.55; acc: 0.56
Batch: 660; loss: 1.54; acc: 0.58
Batch: 680; loss: 1.53; acc: 0.58
Batch: 700; loss: 1.51; acc: 0.64
Batch: 720; loss: 1.55; acc: 0.56
Batch: 740; loss: 1.58; acc: 0.58
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.68; acc: 0.47
Train Epoch over. train_loss: 1.6; train_accuracy: 0.53 

4.2921241401927546e-05
1.4944040231057443e-05
Batch: 0; loss: 1.52; acc: 0.52
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.26; acc: 0.75
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.46; acc: 0.62
Batch: 120; loss: 1.84; acc: 0.39
Batch: 140; loss: 1.47; acc: 0.62
Val Epoch over. val_loss: 1.5508840759848332; val_accuracy: 0.5603105095541401 

The current subspace-distance is: 1.4944040231057443e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.57; acc: 0.62
Batch: 60; loss: 1.54; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.58
Batch: 100; loss: 1.51; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.56; acc: 0.48
Batch: 160; loss: 1.62; acc: 0.5
Batch: 180; loss: 1.64; acc: 0.52
Batch: 200; loss: 1.46; acc: 0.61
Batch: 220; loss: 1.57; acc: 0.52
Batch: 240; loss: 1.58; acc: 0.61
Batch: 260; loss: 1.46; acc: 0.61
Batch: 280; loss: 1.62; acc: 0.5
Batch: 300; loss: 1.56; acc: 0.67
Batch: 320; loss: 1.47; acc: 0.62
Batch: 340; loss: 1.72; acc: 0.38
Batch: 360; loss: 1.57; acc: 0.55
Batch: 380; loss: 1.64; acc: 0.48
Batch: 400; loss: 1.57; acc: 0.59
Batch: 420; loss: 1.55; acc: 0.56
Batch: 440; loss: 1.66; acc: 0.56
Batch: 460; loss: 1.48; acc: 0.61
Batch: 480; loss: 1.5; acc: 0.64
Batch: 500; loss: 1.53; acc: 0.56
Batch: 520; loss: 1.5; acc: 0.64
Batch: 540; loss: 1.62; acc: 0.55
Batch: 560; loss: 1.56; acc: 0.53
Batch: 580; loss: 1.47; acc: 0.58
Batch: 600; loss: 1.69; acc: 0.45
Batch: 620; loss: 1.43; acc: 0.59
Batch: 640; loss: 1.5; acc: 0.66
Batch: 660; loss: 1.71; acc: 0.5
Batch: 680; loss: 1.56; acc: 0.52
Batch: 700; loss: 1.54; acc: 0.53
Batch: 720; loss: 1.57; acc: 0.53
Batch: 740; loss: 1.52; acc: 0.62
Batch: 760; loss: 1.51; acc: 0.61
Batch: 780; loss: 1.56; acc: 0.59
Train Epoch over. train_loss: 1.6; train_accuracy: 0.53 

4.298782005207613e-05
1.5512552636209875e-05
Batch: 0; loss: 1.52; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.42
Batch: 40; loss: 1.25; acc: 0.78
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.56; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.64
Batch: 120; loss: 1.83; acc: 0.41
Batch: 140; loss: 1.47; acc: 0.62
Val Epoch over. val_loss: 1.5474576061698282; val_accuracy: 0.5621019108280255 

The current subspace-distance is: 1.5512552636209875e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.58; acc: 0.44
Batch: 40; loss: 1.62; acc: 0.56
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.61; acc: 0.5
Batch: 100; loss: 1.54; acc: 0.56
Batch: 120; loss: 1.36; acc: 0.69
Batch: 140; loss: 1.6; acc: 0.52
Batch: 160; loss: 1.54; acc: 0.59
Batch: 180; loss: 1.55; acc: 0.53
Batch: 200; loss: 1.63; acc: 0.5
Batch: 220; loss: 1.64; acc: 0.59
Batch: 240; loss: 1.55; acc: 0.61
Batch: 260; loss: 1.59; acc: 0.56
Batch: 280; loss: 1.62; acc: 0.48
Batch: 300; loss: 1.49; acc: 0.53
Batch: 320; loss: 1.61; acc: 0.59
Batch: 340; loss: 1.61; acc: 0.44
Batch: 360; loss: 1.71; acc: 0.44
Batch: 380; loss: 1.65; acc: 0.5
Batch: 400; loss: 1.6; acc: 0.56
Batch: 420; loss: 1.51; acc: 0.56
Batch: 440; loss: 1.64; acc: 0.53
Batch: 460; loss: 1.72; acc: 0.47
Batch: 480; loss: 1.66; acc: 0.48
Batch: 500; loss: 1.57; acc: 0.55
Batch: 520; loss: 1.52; acc: 0.52
Batch: 540; loss: 1.69; acc: 0.45
Batch: 560; loss: 1.67; acc: 0.45
Batch: 580; loss: 1.52; acc: 0.64
Batch: 600; loss: 1.52; acc: 0.53
Batch: 620; loss: 1.69; acc: 0.47
Batch: 640; loss: 1.59; acc: 0.53
Batch: 660; loss: 1.6; acc: 0.56
Batch: 680; loss: 1.72; acc: 0.5
Batch: 700; loss: 1.61; acc: 0.61
Batch: 720; loss: 1.67; acc: 0.41
Batch: 740; loss: 1.51; acc: 0.52
Batch: 760; loss: 1.69; acc: 0.52
Batch: 780; loss: 1.69; acc: 0.44
Train Epoch over. train_loss: 1.59; train_accuracy: 0.53 

4.501486546359956e-05
1.9335448087076657e-05
Batch: 0; loss: 1.52; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.42
Batch: 40; loss: 1.25; acc: 0.75
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.46; acc: 0.61
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.46; acc: 0.64
Val Epoch over. val_loss: 1.5468317779006473; val_accuracy: 0.5611066878980892 

The current subspace-distance is: 1.9335448087076657e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.64; acc: 0.48
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.6; acc: 0.52
Batch: 100; loss: 1.61; acc: 0.58
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.6; acc: 0.55
Batch: 160; loss: 1.65; acc: 0.39
Batch: 180; loss: 1.64; acc: 0.5
Batch: 200; loss: 1.67; acc: 0.5
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.53; acc: 0.64
Batch: 260; loss: 1.49; acc: 0.61
Batch: 280; loss: 1.53; acc: 0.55
Batch: 300; loss: 1.56; acc: 0.56
Batch: 320; loss: 1.44; acc: 0.67
Batch: 340; loss: 1.61; acc: 0.5
Batch: 360; loss: 1.66; acc: 0.53
Batch: 380; loss: 1.53; acc: 0.47
Batch: 400; loss: 1.44; acc: 0.66
Batch: 420; loss: 1.66; acc: 0.47
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.56; acc: 0.55
Batch: 480; loss: 1.6; acc: 0.55
Batch: 500; loss: 1.51; acc: 0.58
Batch: 520; loss: 1.39; acc: 0.61
Batch: 540; loss: 1.86; acc: 0.33
Batch: 560; loss: 1.68; acc: 0.47
Batch: 580; loss: 1.66; acc: 0.5
Batch: 600; loss: 1.64; acc: 0.55
Batch: 620; loss: 1.55; acc: 0.53
Batch: 640; loss: 1.68; acc: 0.47
Batch: 660; loss: 1.74; acc: 0.36
Batch: 680; loss: 1.64; acc: 0.53
Batch: 700; loss: 1.48; acc: 0.64
Batch: 720; loss: 1.62; acc: 0.48
Batch: 740; loss: 1.6; acc: 0.55
Batch: 760; loss: 1.66; acc: 0.5
Batch: 780; loss: 1.55; acc: 0.55
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

4.398565579322167e-05
1.5751986211398616e-05
Batch: 0; loss: 1.53; acc: 0.53
Batch: 20; loss: 1.66; acc: 0.45
Batch: 40; loss: 1.26; acc: 0.78
Batch: 60; loss: 1.6; acc: 0.52
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.46; acc: 0.64
Batch: 120; loss: 1.84; acc: 0.39
Batch: 140; loss: 1.47; acc: 0.64
Val Epoch over. val_loss: 1.5511282545745753; val_accuracy: 0.5625995222929936 

The current subspace-distance is: 1.5751986211398616e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.51; acc: 0.59
Batch: 20; loss: 1.62; acc: 0.55
Batch: 40; loss: 1.6; acc: 0.56
Batch: 60; loss: 1.51; acc: 0.56
Batch: 80; loss: 1.74; acc: 0.5
Batch: 100; loss: 1.57; acc: 0.62
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.57; acc: 0.55
Batch: 160; loss: 1.62; acc: 0.52
Batch: 180; loss: 1.57; acc: 0.58
Batch: 200; loss: 1.66; acc: 0.47
Batch: 220; loss: 1.48; acc: 0.64
Batch: 240; loss: 1.69; acc: 0.53
Batch: 260; loss: 1.66; acc: 0.53
Batch: 280; loss: 1.6; acc: 0.58
Batch: 300; loss: 1.49; acc: 0.59
Batch: 320; loss: 1.51; acc: 0.55
Batch: 340; loss: 1.69; acc: 0.39
Batch: 360; loss: 1.49; acc: 0.5
Batch: 380; loss: 1.54; acc: 0.55
Batch: 400; loss: 1.59; acc: 0.59
Batch: 420; loss: 1.57; acc: 0.5
Batch: 440; loss: 1.52; acc: 0.59
Batch: 460; loss: 1.73; acc: 0.47
Batch: 480; loss: 1.55; acc: 0.58
Batch: 500; loss: 1.57; acc: 0.47
Batch: 520; loss: 1.63; acc: 0.55
Batch: 540; loss: 1.51; acc: 0.53
Batch: 560; loss: 1.57; acc: 0.48
Batch: 580; loss: 1.62; acc: 0.53
Batch: 600; loss: 1.68; acc: 0.52
Batch: 620; loss: 1.54; acc: 0.55
Batch: 640; loss: 1.46; acc: 0.61
Batch: 660; loss: 1.55; acc: 0.52
Batch: 680; loss: 1.56; acc: 0.59
Batch: 700; loss: 1.7; acc: 0.5
Batch: 720; loss: 1.55; acc: 0.56
Batch: 740; loss: 1.56; acc: 0.52
Batch: 760; loss: 1.65; acc: 0.45
Batch: 780; loss: 1.68; acc: 0.52
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

4.2992480302928016e-05
1.4626062693423592e-05
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.25; acc: 0.78
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.56; acc: 0.58
Batch: 100; loss: 1.44; acc: 0.66
Batch: 120; loss: 1.84; acc: 0.39
Batch: 140; loss: 1.46; acc: 0.66
Val Epoch over. val_loss: 1.5463799731746601; val_accuracy: 0.5646894904458599 

The current subspace-distance is: 1.4626062693423592e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.68; acc: 0.47
Batch: 40; loss: 1.54; acc: 0.59
Batch: 60; loss: 1.75; acc: 0.47
Batch: 80; loss: 1.54; acc: 0.48
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.8; acc: 0.36
Batch: 140; loss: 1.77; acc: 0.36
Batch: 160; loss: 1.59; acc: 0.58
Batch: 180; loss: 1.54; acc: 0.59
Batch: 200; loss: 1.54; acc: 0.59
Batch: 220; loss: 1.61; acc: 0.48
Batch: 240; loss: 1.6; acc: 0.55
Batch: 260; loss: 1.51; acc: 0.55
Batch: 280; loss: 1.65; acc: 0.44
Batch: 300; loss: 1.57; acc: 0.53
Batch: 320; loss: 1.62; acc: 0.47
Batch: 340; loss: 1.72; acc: 0.5
Batch: 360; loss: 1.53; acc: 0.55
Batch: 380; loss: 1.49; acc: 0.64
Batch: 400; loss: 1.6; acc: 0.52
Batch: 420; loss: 1.49; acc: 0.58
Batch: 440; loss: 1.6; acc: 0.55
Batch: 460; loss: 1.56; acc: 0.58
Batch: 480; loss: 1.51; acc: 0.64
Batch: 500; loss: 1.61; acc: 0.44
Batch: 520; loss: 1.5; acc: 0.62
Batch: 540; loss: 1.61; acc: 0.55
Batch: 560; loss: 1.45; acc: 0.67
Batch: 580; loss: 1.59; acc: 0.5
Batch: 600; loss: 1.38; acc: 0.64
Batch: 620; loss: 1.53; acc: 0.61
Batch: 640; loss: 1.55; acc: 0.56
Batch: 660; loss: 1.54; acc: 0.58
Batch: 680; loss: 1.78; acc: 0.42
Batch: 700; loss: 1.61; acc: 0.45
Batch: 720; loss: 1.7; acc: 0.45
Batch: 740; loss: 1.56; acc: 0.53
Batch: 760; loss: 1.52; acc: 0.59
Batch: 780; loss: 1.59; acc: 0.48
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

4.418626849655993e-05
1.5266399714164436e-05
Batch: 0; loss: 1.51; acc: 0.53
Batch: 20; loss: 1.65; acc: 0.42
Batch: 40; loss: 1.25; acc: 0.8
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.44; acc: 0.64
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.45; acc: 0.66
Val Epoch over. val_loss: 1.5410608539156094; val_accuracy: 0.5654856687898089 

The current subspace-distance is: 1.5266399714164436e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.56; acc: 0.59
Batch: 40; loss: 1.41; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.58
Batch: 80; loss: 1.61; acc: 0.48
Batch: 100; loss: 1.5; acc: 0.67
Batch: 120; loss: 1.55; acc: 0.58
Batch: 140; loss: 1.58; acc: 0.5
Batch: 160; loss: 1.68; acc: 0.45
Batch: 180; loss: 1.53; acc: 0.58
Batch: 200; loss: 1.69; acc: 0.41
Batch: 220; loss: 1.51; acc: 0.56
Batch: 240; loss: 1.56; acc: 0.58
Batch: 260; loss: 1.49; acc: 0.58
Batch: 280; loss: 1.71; acc: 0.42
Batch: 300; loss: 1.58; acc: 0.52
Batch: 320; loss: 1.54; acc: 0.52
Batch: 340; loss: 1.69; acc: 0.48
Batch: 360; loss: 1.5; acc: 0.55
Batch: 380; loss: 1.52; acc: 0.56
Batch: 400; loss: 1.8; acc: 0.41
Batch: 420; loss: 1.67; acc: 0.47
Batch: 440; loss: 1.57; acc: 0.53
Batch: 460; loss: 1.62; acc: 0.45
Batch: 480; loss: 1.45; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.61
Batch: 520; loss: 1.71; acc: 0.47
Batch: 540; loss: 1.59; acc: 0.52
Batch: 560; loss: 1.59; acc: 0.52
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.48; acc: 0.59
Batch: 620; loss: 1.59; acc: 0.55
Batch: 640; loss: 1.67; acc: 0.48
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.56; acc: 0.5
Batch: 700; loss: 1.58; acc: 0.55
Batch: 720; loss: 1.59; acc: 0.52
Batch: 740; loss: 1.54; acc: 0.53
Batch: 760; loss: 1.65; acc: 0.45
Batch: 780; loss: 1.53; acc: 0.59
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

4.3520860344870016e-05
1.495626838732278e-05
Batch: 0; loss: 1.51; acc: 0.55
Batch: 20; loss: 1.65; acc: 0.45
Batch: 40; loss: 1.25; acc: 0.78
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.59
Batch: 100; loss: 1.45; acc: 0.64
Batch: 120; loss: 1.84; acc: 0.39
Batch: 140; loss: 1.45; acc: 0.66
Val Epoch over. val_loss: 1.5425276437382789; val_accuracy: 0.5668789808917197 

The current subspace-distance is: 1.495626838732278e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.53
Batch: 40; loss: 1.58; acc: 0.58
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.61; acc: 0.39
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.76; acc: 0.45
Batch: 140; loss: 1.57; acc: 0.52
Batch: 160; loss: 1.55; acc: 0.53
Batch: 180; loss: 1.6; acc: 0.53
Batch: 200; loss: 1.54; acc: 0.53
Batch: 220; loss: 1.6; acc: 0.55
Batch: 240; loss: 1.67; acc: 0.45
Batch: 260; loss: 1.73; acc: 0.47
Batch: 280; loss: 1.66; acc: 0.5
Batch: 300; loss: 1.54; acc: 0.55
Batch: 320; loss: 1.68; acc: 0.56
Batch: 340; loss: 1.64; acc: 0.47
Batch: 360; loss: 1.51; acc: 0.64
Batch: 380; loss: 1.57; acc: 0.55
Batch: 400; loss: 1.56; acc: 0.58
Batch: 420; loss: 1.68; acc: 0.5
Batch: 440; loss: 1.73; acc: 0.44
Batch: 460; loss: 1.55; acc: 0.5
Batch: 480; loss: 1.5; acc: 0.69
Batch: 500; loss: 1.63; acc: 0.5
Batch: 520; loss: 1.54; acc: 0.59
Batch: 540; loss: 1.77; acc: 0.42
Batch: 560; loss: 1.41; acc: 0.66
Batch: 580; loss: 1.57; acc: 0.61
Batch: 600; loss: 1.5; acc: 0.59
Batch: 620; loss: 1.69; acc: 0.44
Batch: 640; loss: 1.51; acc: 0.61
Batch: 660; loss: 1.58; acc: 0.52
Batch: 680; loss: 1.71; acc: 0.48
Batch: 700; loss: 1.46; acc: 0.58
Batch: 720; loss: 1.63; acc: 0.5
Batch: 740; loss: 1.65; acc: 0.53
Batch: 760; loss: 1.62; acc: 0.5
Batch: 780; loss: 1.55; acc: 0.53
Train Epoch over. train_loss: 1.58; train_accuracy: 0.54 

4.3706659198505804e-05
1.4790844033996109e-05
Batch: 0; loss: 1.51; acc: 0.55
Batch: 20; loss: 1.64; acc: 0.45
Batch: 40; loss: 1.23; acc: 0.81
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.64
Batch: 120; loss: 1.82; acc: 0.39
Batch: 140; loss: 1.43; acc: 0.64
Val Epoch over. val_loss: 1.5313260896950012; val_accuracy: 0.5712579617834395 

The current subspace-distance is: 1.4790844033996109e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.56
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.66; acc: 0.55
Batch: 60; loss: 1.53; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.47
Batch: 100; loss: 1.53; acc: 0.53
Batch: 120; loss: 1.51; acc: 0.62
Batch: 140; loss: 1.54; acc: 0.59
Batch: 160; loss: 1.62; acc: 0.53
Batch: 180; loss: 1.61; acc: 0.53
Batch: 200; loss: 1.56; acc: 0.52
Batch: 220; loss: 1.57; acc: 0.61
Batch: 240; loss: 1.57; acc: 0.47
Batch: 260; loss: 1.5; acc: 0.56
Batch: 280; loss: 1.45; acc: 0.61
Batch: 300; loss: 1.51; acc: 0.62
Batch: 320; loss: 1.62; acc: 0.5
Batch: 340; loss: 1.53; acc: 0.55
Batch: 360; loss: 1.7; acc: 0.56
Batch: 380; loss: 1.4; acc: 0.67
Batch: 400; loss: 1.6; acc: 0.53
Batch: 420; loss: 1.55; acc: 0.61
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.55; acc: 0.59
Batch: 480; loss: 1.56; acc: 0.48
Batch: 500; loss: 1.62; acc: 0.62
Batch: 520; loss: 1.53; acc: 0.56
Batch: 540; loss: 1.64; acc: 0.61
Batch: 560; loss: 1.7; acc: 0.47
Batch: 580; loss: 1.58; acc: 0.53
Batch: 600; loss: 1.65; acc: 0.48
Batch: 620; loss: 1.54; acc: 0.58
Batch: 640; loss: 1.6; acc: 0.56
Batch: 660; loss: 1.42; acc: 0.62
Batch: 680; loss: 1.61; acc: 0.5
Batch: 700; loss: 1.52; acc: 0.53
Batch: 720; loss: 1.67; acc: 0.5
Batch: 740; loss: 1.51; acc: 0.58
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.57; acc: 0.47
Train Epoch over. train_loss: 1.58; train_accuracy: 0.54 

4.4334487029118463e-05
1.6088892152765766e-05
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.64; acc: 0.44
Batch: 40; loss: 1.24; acc: 0.81
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.43; acc: 0.62
Batch: 120; loss: 1.82; acc: 0.41
Batch: 140; loss: 1.44; acc: 0.67
Val Epoch over. val_loss: 1.5345524268545163; val_accuracy: 0.5703622611464968 

The current subspace-distance is: 1.6088892152765766e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.44
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 1.54; acc: 0.5
Batch: 60; loss: 1.62; acc: 0.56
Batch: 80; loss: 1.67; acc: 0.48
Batch: 100; loss: 1.64; acc: 0.55
Batch: 120; loss: 1.63; acc: 0.53
Batch: 140; loss: 1.61; acc: 0.52
Batch: 160; loss: 1.54; acc: 0.53
Batch: 180; loss: 1.65; acc: 0.45
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.62; acc: 0.5
Batch: 240; loss: 1.53; acc: 0.62
Batch: 260; loss: 1.54; acc: 0.5
Batch: 280; loss: 1.59; acc: 0.52
Batch: 300; loss: 1.51; acc: 0.56
Batch: 320; loss: 1.59; acc: 0.52
Batch: 340; loss: 1.57; acc: 0.5
Batch: 360; loss: 1.63; acc: 0.58
Batch: 380; loss: 1.54; acc: 0.5
Batch: 400; loss: 1.58; acc: 0.52
Batch: 420; loss: 1.53; acc: 0.56
Batch: 440; loss: 1.63; acc: 0.52
Batch: 460; loss: 1.51; acc: 0.53
Batch: 480; loss: 1.53; acc: 0.61
Batch: 500; loss: 1.58; acc: 0.56
Batch: 520; loss: 1.59; acc: 0.48
Batch: 540; loss: 1.57; acc: 0.58
Batch: 560; loss: 1.54; acc: 0.52
Batch: 580; loss: 1.64; acc: 0.53
Batch: 600; loss: 1.53; acc: 0.5
Batch: 620; loss: 1.54; acc: 0.5
Batch: 640; loss: 1.48; acc: 0.61
Batch: 660; loss: 1.41; acc: 0.7
Batch: 680; loss: 1.52; acc: 0.58
Batch: 700; loss: 1.56; acc: 0.47
Batch: 720; loss: 1.75; acc: 0.36
Batch: 740; loss: 1.54; acc: 0.52
Batch: 760; loss: 1.53; acc: 0.61
Batch: 780; loss: 1.63; acc: 0.5
Train Epoch over. train_loss: 1.58; train_accuracy: 0.54 

4.3729985918616876e-05
1.3357238458411302e-05
Batch: 0; loss: 1.49; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.47
Batch: 40; loss: 1.23; acc: 0.83
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.53; acc: 0.58
Batch: 100; loss: 1.43; acc: 0.59
Batch: 120; loss: 1.81; acc: 0.42
Batch: 140; loss: 1.44; acc: 0.66
Val Epoch over. val_loss: 1.5316117485617375; val_accuracy: 0.5696656050955414 

The current subspace-distance is: 1.3357238458411302e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.5
Batch: 20; loss: 1.63; acc: 0.44
Batch: 40; loss: 1.6; acc: 0.58
Batch: 60; loss: 1.71; acc: 0.44
Batch: 80; loss: 1.58; acc: 0.45
Batch: 100; loss: 1.58; acc: 0.59
Batch: 120; loss: 1.51; acc: 0.61
Batch: 140; loss: 1.57; acc: 0.52
Batch: 160; loss: 1.69; acc: 0.59
Batch: 180; loss: 1.46; acc: 0.55
Batch: 200; loss: 1.72; acc: 0.44
Batch: 220; loss: 1.72; acc: 0.45
Batch: 240; loss: 1.64; acc: 0.41
Batch: 260; loss: 1.69; acc: 0.5
Batch: 280; loss: 1.63; acc: 0.48
Batch: 300; loss: 1.79; acc: 0.44
Batch: 320; loss: 1.67; acc: 0.45
Batch: 340; loss: 1.37; acc: 0.7
Batch: 360; loss: 1.58; acc: 0.44
Batch: 380; loss: 1.53; acc: 0.55
Batch: 400; loss: 1.66; acc: 0.5
Batch: 420; loss: 1.58; acc: 0.56
Batch: 440; loss: 1.56; acc: 0.48
Batch: 460; loss: 1.6; acc: 0.59
Batch: 480; loss: 1.5; acc: 0.59
Batch: 500; loss: 1.43; acc: 0.66
Batch: 520; loss: 1.51; acc: 0.62
Batch: 540; loss: 1.57; acc: 0.52
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.6; acc: 0.53
Batch: 600; loss: 1.71; acc: 0.48
Batch: 620; loss: 1.73; acc: 0.5
Batch: 640; loss: 1.62; acc: 0.55
Batch: 660; loss: 1.7; acc: 0.5
Batch: 680; loss: 1.56; acc: 0.5
Batch: 700; loss: 1.53; acc: 0.53
Batch: 720; loss: 1.5; acc: 0.56
Batch: 740; loss: 1.54; acc: 0.58
Batch: 760; loss: 1.63; acc: 0.58
Batch: 780; loss: 1.56; acc: 0.55
Train Epoch over. train_loss: 1.58; train_accuracy: 0.54 

4.500630166148767e-05
1.766160858096555e-05
Batch: 0; loss: 1.5; acc: 0.53
Batch: 20; loss: 1.64; acc: 0.44
Batch: 40; loss: 1.22; acc: 0.83
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.54; acc: 0.58
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.81; acc: 0.42
Batch: 140; loss: 1.44; acc: 0.66
Val Epoch over. val_loss: 1.5236829253518658; val_accuracy: 0.5719546178343949 

The current subspace-distance is: 1.766160858096555e-05 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_15_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.03

The number of parameters is: 284685

The number of individual parameters is:

9
162
9
9
13
35802
13
13
25
99450
25
25
64
144000
64
64
4096
64
640
10
64
64

nonzero elements in E: 28468497
elements in E: 28468500
fraction nonzero: 0.9999998946203699
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.51; acc: 0.05
Batch: 20; loss: 2.4; acc: 0.14
Batch: 40; loss: 2.28; acc: 0.17
Batch: 60; loss: 2.03; acc: 0.33
Batch: 80; loss: 1.96; acc: 0.47
Batch: 100; loss: 2.03; acc: 0.38
Batch: 120; loss: 1.97; acc: 0.36
Batch: 140; loss: 1.97; acc: 0.38
Batch: 160; loss: 1.91; acc: 0.45
Batch: 180; loss: 2.01; acc: 0.25
Batch: 200; loss: 1.87; acc: 0.48
Batch: 220; loss: 1.85; acc: 0.45
Batch: 240; loss: 1.84; acc: 0.39
Batch: 260; loss: 1.72; acc: 0.53
Batch: 280; loss: 1.88; acc: 0.44
Batch: 300; loss: 1.84; acc: 0.52
Batch: 320; loss: 1.69; acc: 0.58
Batch: 340; loss: 1.72; acc: 0.53
Batch: 360; loss: 1.79; acc: 0.47
Batch: 380; loss: 1.85; acc: 0.42
Batch: 400; loss: 1.8; acc: 0.48
Batch: 420; loss: 1.73; acc: 0.56
Batch: 440; loss: 1.68; acc: 0.61
Batch: 460; loss: 1.71; acc: 0.55
Batch: 480; loss: 1.69; acc: 0.56
Batch: 500; loss: 1.82; acc: 0.52
Batch: 520; loss: 1.67; acc: 0.62
Batch: 540; loss: 1.78; acc: 0.44
Batch: 560; loss: 1.78; acc: 0.5
Batch: 580; loss: 1.73; acc: 0.52
Batch: 600; loss: 1.66; acc: 0.58
Batch: 620; loss: 1.68; acc: 0.59
Batch: 640; loss: 1.67; acc: 0.61
Batch: 660; loss: 1.55; acc: 0.66
Batch: 680; loss: 1.65; acc: 0.5
Batch: 700; loss: 1.67; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.45
Batch: 740; loss: 1.76; acc: 0.55
Batch: 760; loss: 1.68; acc: 0.5
Batch: 780; loss: 1.6; acc: 0.64
Train Epoch over. train_loss: 1.82; train_accuracy: 0.48 

5.87577378610149e-05
5.3996103815734386e-05
Batch: 0; loss: 1.77; acc: 0.47
Batch: 20; loss: 1.67; acc: 0.53
Batch: 40; loss: 1.47; acc: 0.66
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.64
Batch: 100; loss: 1.73; acc: 0.55
Batch: 120; loss: 1.72; acc: 0.55
Batch: 140; loss: 1.41; acc: 0.75
Val Epoch over. val_loss: 1.6308234861701916; val_accuracy: 0.6024084394904459 

The current subspace-distance is: 5.3996103815734386e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.52
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.65; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.67
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.67; acc: 0.56
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.66; acc: 0.55
Batch: 160; loss: 1.67; acc: 0.58
Batch: 180; loss: 1.59; acc: 0.61
Batch: 200; loss: 1.61; acc: 0.59
Batch: 220; loss: 1.74; acc: 0.52
Batch: 240; loss: 1.56; acc: 0.66
Batch: 260; loss: 1.48; acc: 0.72
Batch: 280; loss: 1.6; acc: 0.58
Batch: 300; loss: 1.6; acc: 0.61
Batch: 320; loss: 1.56; acc: 0.61
Batch: 340; loss: 1.68; acc: 0.56
Batch: 360; loss: 1.48; acc: 0.67
Batch: 380; loss: 1.6; acc: 0.62
Batch: 400; loss: 1.62; acc: 0.52
Batch: 420; loss: 1.58; acc: 0.58
Batch: 440; loss: 1.69; acc: 0.5
Batch: 460; loss: 1.69; acc: 0.53
Batch: 480; loss: 1.54; acc: 0.62
Batch: 500; loss: 1.55; acc: 0.62
Batch: 520; loss: 1.57; acc: 0.58
Batch: 540; loss: 1.72; acc: 0.45
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.63; acc: 0.52
Batch: 600; loss: 1.61; acc: 0.53
Batch: 620; loss: 1.49; acc: 0.75
Batch: 640; loss: 1.43; acc: 0.73
Batch: 660; loss: 1.55; acc: 0.59
Batch: 680; loss: 1.51; acc: 0.66
Batch: 700; loss: 1.49; acc: 0.66
Batch: 720; loss: 1.59; acc: 0.66
Batch: 740; loss: 1.54; acc: 0.64
Batch: 760; loss: 1.53; acc: 0.59
Batch: 780; loss: 1.53; acc: 0.61
Train Epoch over. train_loss: 1.59; train_accuracy: 0.59 

7.878101314418018e-05
7.38463131710887e-05
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.57; acc: 0.61
Batch: 40; loss: 1.3; acc: 0.75
Batch: 60; loss: 1.54; acc: 0.58
Batch: 80; loss: 1.31; acc: 0.77
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.29; acc: 0.73
Val Epoch over. val_loss: 1.5077424019005647; val_accuracy: 0.6283837579617835 

The current subspace-distance is: 7.38463131710887e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.55; acc: 0.61
Batch: 20; loss: 1.41; acc: 0.75
Batch: 40; loss: 1.49; acc: 0.69
Batch: 60; loss: 1.73; acc: 0.44
Batch: 80; loss: 1.47; acc: 0.62
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.46; acc: 0.64
Batch: 160; loss: 1.45; acc: 0.62
Batch: 180; loss: 1.53; acc: 0.61
Batch: 200; loss: 1.57; acc: 0.52
Batch: 220; loss: 1.71; acc: 0.44
Batch: 240; loss: 1.48; acc: 0.59
Batch: 260; loss: 1.4; acc: 0.67
Batch: 280; loss: 1.45; acc: 0.64
Batch: 300; loss: 1.57; acc: 0.61
Batch: 320; loss: 1.52; acc: 0.67
Batch: 340; loss: 1.55; acc: 0.58
Batch: 360; loss: 1.65; acc: 0.5
Batch: 380; loss: 1.47; acc: 0.66
Batch: 400; loss: 1.45; acc: 0.64
Batch: 420; loss: 1.4; acc: 0.7
Batch: 440; loss: 1.46; acc: 0.7
Batch: 460; loss: 1.43; acc: 0.62
Batch: 480; loss: 1.49; acc: 0.66
Batch: 500; loss: 1.62; acc: 0.5
Batch: 520; loss: 1.61; acc: 0.48
Batch: 540; loss: 1.38; acc: 0.72
Batch: 560; loss: 1.47; acc: 0.66
Batch: 580; loss: 1.5; acc: 0.59
Batch: 600; loss: 1.63; acc: 0.47
Batch: 620; loss: 1.56; acc: 0.59
Batch: 640; loss: 1.46; acc: 0.64
Batch: 660; loss: 1.43; acc: 0.64
Batch: 680; loss: 1.5; acc: 0.61
Batch: 700; loss: 1.48; acc: 0.62
Batch: 720; loss: 1.42; acc: 0.67
Batch: 740; loss: 1.55; acc: 0.66
Batch: 760; loss: 1.49; acc: 0.56
Batch: 780; loss: 1.38; acc: 0.61
Train Epoch over. train_loss: 1.5; train_accuracy: 0.61 

9.463162132306024e-05
8.869248995324597e-05
Batch: 0; loss: 1.58; acc: 0.59
Batch: 20; loss: 1.54; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.77
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.21; acc: 0.81
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.25; acc: 0.7
Val Epoch over. val_loss: 1.4485326853527385; val_accuracy: 0.6377388535031847 

The current subspace-distance is: 8.869248995324597e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.69
Batch: 20; loss: 1.33; acc: 0.67
Batch: 40; loss: 1.41; acc: 0.67
Batch: 60; loss: 1.44; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.45; acc: 0.66
Batch: 120; loss: 1.46; acc: 0.66
Batch: 140; loss: 1.47; acc: 0.61
Batch: 160; loss: 1.56; acc: 0.56
Batch: 180; loss: 1.42; acc: 0.7
Batch: 200; loss: 1.56; acc: 0.58
Batch: 220; loss: 1.33; acc: 0.72
Batch: 240; loss: 1.5; acc: 0.59
Batch: 260; loss: 1.52; acc: 0.58
Batch: 280; loss: 1.39; acc: 0.64
Batch: 300; loss: 1.47; acc: 0.55
Batch: 320; loss: 1.38; acc: 0.64
Batch: 340; loss: 1.45; acc: 0.58
Batch: 360; loss: 1.47; acc: 0.67
Batch: 380; loss: 1.39; acc: 0.61
Batch: 400; loss: 1.5; acc: 0.58
Batch: 420; loss: 1.54; acc: 0.55
Batch: 440; loss: 1.45; acc: 0.58
Batch: 460; loss: 1.45; acc: 0.64
Batch: 480; loss: 1.36; acc: 0.67
Batch: 500; loss: 1.57; acc: 0.55
Batch: 520; loss: 1.38; acc: 0.66
Batch: 540; loss: 1.38; acc: 0.62
Batch: 560; loss: 1.57; acc: 0.58
Batch: 580; loss: 1.58; acc: 0.48
Batch: 600; loss: 1.46; acc: 0.69
Batch: 620; loss: 1.43; acc: 0.69
Batch: 640; loss: 1.49; acc: 0.56
Batch: 660; loss: 1.41; acc: 0.66
Batch: 680; loss: 1.39; acc: 0.62
Batch: 700; loss: 1.26; acc: 0.72
Batch: 720; loss: 1.44; acc: 0.66
Batch: 740; loss: 1.58; acc: 0.5
Batch: 760; loss: 1.35; acc: 0.73
Batch: 780; loss: 1.53; acc: 0.56
Train Epoch over. train_loss: 1.44; train_accuracy: 0.62 

0.0001051701110554859
0.00010010456026066095
Batch: 0; loss: 1.52; acc: 0.59
Batch: 20; loss: 1.5; acc: 0.61
Batch: 40; loss: 1.18; acc: 0.75
Batch: 60; loss: 1.38; acc: 0.61
Batch: 80; loss: 1.14; acc: 0.81
Batch: 100; loss: 1.42; acc: 0.64
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.2; acc: 0.72
Val Epoch over. val_loss: 1.3910850696502977; val_accuracy: 0.6506767515923567 

The current subspace-distance is: 0.00010010456026066095 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.62
Batch: 20; loss: 1.41; acc: 0.62
Batch: 40; loss: 1.42; acc: 0.66
Batch: 60; loss: 1.33; acc: 0.66
Batch: 80; loss: 1.51; acc: 0.53
Batch: 100; loss: 1.32; acc: 0.7
Batch: 120; loss: 1.37; acc: 0.64
Batch: 140; loss: 1.36; acc: 0.66
Batch: 160; loss: 1.41; acc: 0.64
Batch: 180; loss: 1.5; acc: 0.42
Batch: 200; loss: 1.4; acc: 0.61
Batch: 220; loss: 1.39; acc: 0.62
Batch: 240; loss: 1.48; acc: 0.53
Batch: 260; loss: 1.41; acc: 0.66
Batch: 280; loss: 1.38; acc: 0.61
Batch: 300; loss: 1.35; acc: 0.67
Batch: 320; loss: 1.39; acc: 0.59
Batch: 340; loss: 1.36; acc: 0.73
Batch: 360; loss: 1.39; acc: 0.66
Batch: 380; loss: 1.46; acc: 0.52
Batch: 400; loss: 1.29; acc: 0.69
Batch: 420; loss: 1.37; acc: 0.64
Batch: 440; loss: 1.5; acc: 0.48
Batch: 460; loss: 1.36; acc: 0.66
Batch: 480; loss: 1.38; acc: 0.61
Batch: 500; loss: 1.54; acc: 0.56
Batch: 520; loss: 1.37; acc: 0.62
Batch: 540; loss: 1.41; acc: 0.59
Batch: 560; loss: 1.42; acc: 0.61
Batch: 580; loss: 1.54; acc: 0.52
Batch: 600; loss: 1.57; acc: 0.52
Batch: 620; loss: 1.46; acc: 0.61
Batch: 640; loss: 1.45; acc: 0.61
Batch: 660; loss: 1.35; acc: 0.69
Batch: 680; loss: 1.45; acc: 0.61
Batch: 700; loss: 1.55; acc: 0.58
Batch: 720; loss: 1.43; acc: 0.59
Batch: 740; loss: 1.43; acc: 0.62
Batch: 760; loss: 1.39; acc: 0.7
Batch: 780; loss: 1.39; acc: 0.59
Train Epoch over. train_loss: 1.41; train_accuracy: 0.62 

0.00011237819853704423
0.00010649723117239773
Batch: 0; loss: 1.47; acc: 0.56
Batch: 20; loss: 1.48; acc: 0.62
Batch: 40; loss: 1.14; acc: 0.75
Batch: 60; loss: 1.31; acc: 0.67
Batch: 80; loss: 1.09; acc: 0.8
Batch: 100; loss: 1.37; acc: 0.67
Batch: 120; loss: 1.56; acc: 0.53
Batch: 140; loss: 1.16; acc: 0.72
Val Epoch over. val_loss: 1.3482350543805748; val_accuracy: 0.6546576433121019 

The current subspace-distance is: 0.00010649723117239773 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.44; acc: 0.58
Batch: 40; loss: 1.48; acc: 0.56
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.42; acc: 0.66
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.64
Batch: 160; loss: 1.38; acc: 0.62
Batch: 180; loss: 1.48; acc: 0.55
Batch: 200; loss: 1.41; acc: 0.64
Batch: 220; loss: 1.37; acc: 0.59
Batch: 240; loss: 1.38; acc: 0.66
Batch: 260; loss: 1.36; acc: 0.66
Batch: 280; loss: 1.26; acc: 0.66
Batch: 300; loss: 1.35; acc: 0.64
Batch: 320; loss: 1.33; acc: 0.64
Batch: 340; loss: 1.51; acc: 0.5
Batch: 360; loss: 1.29; acc: 0.75
Batch: 380; loss: 1.32; acc: 0.66
Batch: 400; loss: 1.43; acc: 0.62
Batch: 420; loss: 1.42; acc: 0.59
Batch: 440; loss: 1.27; acc: 0.75
Batch: 460; loss: 1.32; acc: 0.67
Batch: 480; loss: 1.38; acc: 0.69
Batch: 500; loss: 1.32; acc: 0.59
Batch: 520; loss: 1.25; acc: 0.72
Batch: 540; loss: 1.46; acc: 0.55
Batch: 560; loss: 1.44; acc: 0.59
Batch: 580; loss: 1.27; acc: 0.69
Batch: 600; loss: 1.53; acc: 0.56
Batch: 620; loss: 1.43; acc: 0.53
Batch: 640; loss: 1.3; acc: 0.64
Batch: 660; loss: 1.4; acc: 0.62
Batch: 680; loss: 1.38; acc: 0.59
Batch: 700; loss: 1.4; acc: 0.64
Batch: 720; loss: 1.3; acc: 0.67
Batch: 740; loss: 1.43; acc: 0.64
Batch: 760; loss: 1.29; acc: 0.66
Batch: 780; loss: 1.46; acc: 0.62
Train Epoch over. train_loss: 1.38; train_accuracy: 0.62 

0.00012083822366548702
0.00011412894673412666
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.49; acc: 0.59
Batch: 40; loss: 1.12; acc: 0.77
Batch: 60; loss: 1.29; acc: 0.69
Batch: 80; loss: 1.07; acc: 0.8
Batch: 100; loss: 1.32; acc: 0.73
Batch: 120; loss: 1.53; acc: 0.52
Batch: 140; loss: 1.13; acc: 0.72
Val Epoch over. val_loss: 1.325474664663813; val_accuracy: 0.6468949044585988 

The current subspace-distance is: 0.00011412894673412666 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.52
Batch: 20; loss: 1.47; acc: 0.56
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.32; acc: 0.62
Batch: 80; loss: 1.33; acc: 0.66
Batch: 100; loss: 1.45; acc: 0.59
Batch: 120; loss: 1.33; acc: 0.66
Batch: 140; loss: 1.31; acc: 0.66
Batch: 160; loss: 1.34; acc: 0.66
Batch: 180; loss: 1.3; acc: 0.67
Batch: 200; loss: 1.32; acc: 0.7
Batch: 220; loss: 1.21; acc: 0.73
Batch: 240; loss: 1.29; acc: 0.72
Batch: 260; loss: 1.24; acc: 0.66
Batch: 280; loss: 1.4; acc: 0.58
Batch: 300; loss: 1.47; acc: 0.52
Batch: 320; loss: 1.58; acc: 0.42
Batch: 340; loss: 1.41; acc: 0.59
Batch: 360; loss: 1.23; acc: 0.72
Batch: 380; loss: 1.4; acc: 0.58
Batch: 400; loss: 1.37; acc: 0.62
Batch: 420; loss: 1.17; acc: 0.72
Batch: 440; loss: 1.28; acc: 0.7
Batch: 460; loss: 1.35; acc: 0.67
Batch: 480; loss: 1.22; acc: 0.72
Batch: 500; loss: 1.33; acc: 0.64
Batch: 520; loss: 1.33; acc: 0.61
Batch: 540; loss: 1.44; acc: 0.59
Batch: 560; loss: 1.39; acc: 0.52
Batch: 580; loss: 1.22; acc: 0.67
Batch: 600; loss: 1.41; acc: 0.59
Batch: 620; loss: 1.25; acc: 0.67
Batch: 640; loss: 1.4; acc: 0.66
Batch: 660; loss: 1.46; acc: 0.55
Batch: 680; loss: 1.38; acc: 0.66
Batch: 700; loss: 1.35; acc: 0.64
Batch: 720; loss: 1.4; acc: 0.59
Batch: 740; loss: 1.26; acc: 0.7
Batch: 760; loss: 1.25; acc: 0.66
Batch: 780; loss: 1.38; acc: 0.59
Train Epoch over. train_loss: 1.36; train_accuracy: 0.63 

0.00012647901894524693
0.00012100870662834495
Batch: 0; loss: 1.43; acc: 0.58
Batch: 20; loss: 1.48; acc: 0.58
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.27; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.78
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.47; acc: 0.56
Batch: 140; loss: 1.11; acc: 0.72
Val Epoch over. val_loss: 1.30133190048728; val_accuracy: 0.6508757961783439 

The current subspace-distance is: 0.00012100870662834495 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.7
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 1.44; acc: 0.56
Batch: 60; loss: 1.41; acc: 0.61
Batch: 80; loss: 1.32; acc: 0.66
Batch: 100; loss: 1.34; acc: 0.61
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.64
Batch: 160; loss: 1.38; acc: 0.53
Batch: 180; loss: 1.29; acc: 0.69
Batch: 200; loss: 1.66; acc: 0.39
Batch: 220; loss: 1.3; acc: 0.61
Batch: 240; loss: 1.33; acc: 0.66
Batch: 260; loss: 1.35; acc: 0.61
Batch: 280; loss: 1.38; acc: 0.56
Batch: 300; loss: 1.24; acc: 0.72
Batch: 320; loss: 1.3; acc: 0.69
Batch: 340; loss: 1.28; acc: 0.56
Batch: 360; loss: 1.44; acc: 0.58
Batch: 380; loss: 1.19; acc: 0.7
Batch: 400; loss: 1.32; acc: 0.66
Batch: 420; loss: 1.61; acc: 0.48
Batch: 440; loss: 1.35; acc: 0.61
Batch: 460; loss: 1.48; acc: 0.56
Batch: 480; loss: 1.3; acc: 0.59
Batch: 500; loss: 1.4; acc: 0.61
Batch: 520; loss: 1.41; acc: 0.61
Batch: 540; loss: 1.3; acc: 0.67
Batch: 560; loss: 1.38; acc: 0.61
Batch: 580; loss: 1.33; acc: 0.64
Batch: 600; loss: 1.27; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.72
Batch: 640; loss: 1.28; acc: 0.58
Batch: 660; loss: 1.29; acc: 0.66
Batch: 680; loss: 1.2; acc: 0.66
Batch: 700; loss: 1.25; acc: 0.7
Batch: 720; loss: 1.2; acc: 0.7
Batch: 740; loss: 1.16; acc: 0.7
Batch: 760; loss: 1.39; acc: 0.59
Batch: 780; loss: 1.37; acc: 0.64
Train Epoch over. train_loss: 1.34; train_accuracy: 0.63 

0.00013506603136193007
0.00012993918790016323
Batch: 0; loss: 1.4; acc: 0.58
Batch: 20; loss: 1.45; acc: 0.58
Batch: 40; loss: 1.06; acc: 0.72
Batch: 60; loss: 1.28; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.67
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.1; acc: 0.73
Val Epoch over. val_loss: 1.2862165763879279; val_accuracy: 0.6453025477707006 

The current subspace-distance is: 0.00012993918790016323 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.36; acc: 0.58
Batch: 40; loss: 1.35; acc: 0.59
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.32; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.64
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 1.42; acc: 0.59
Batch: 160; loss: 1.16; acc: 0.73
Batch: 180; loss: 1.37; acc: 0.69
Batch: 200; loss: 1.39; acc: 0.61
Batch: 220; loss: 1.22; acc: 0.77
Batch: 240; loss: 1.33; acc: 0.59
Batch: 260; loss: 1.29; acc: 0.62
Batch: 280; loss: 1.4; acc: 0.53
Batch: 300; loss: 1.32; acc: 0.61
Batch: 320; loss: 1.33; acc: 0.64
Batch: 340; loss: 1.45; acc: 0.52
Batch: 360; loss: 1.32; acc: 0.59
Batch: 380; loss: 1.33; acc: 0.67
Batch: 400; loss: 1.22; acc: 0.7
Batch: 420; loss: 1.31; acc: 0.55
Batch: 440; loss: 1.35; acc: 0.69
Batch: 460; loss: 1.28; acc: 0.61
Batch: 480; loss: 1.26; acc: 0.66
Batch: 500; loss: 1.27; acc: 0.62
Batch: 520; loss: 1.14; acc: 0.7
Batch: 540; loss: 1.37; acc: 0.62
Batch: 560; loss: 1.43; acc: 0.53
Batch: 580; loss: 1.35; acc: 0.58
Batch: 600; loss: 1.46; acc: 0.56
Batch: 620; loss: 1.21; acc: 0.64
Batch: 640; loss: 1.19; acc: 0.7
Batch: 660; loss: 1.39; acc: 0.59
Batch: 680; loss: 1.28; acc: 0.66
Batch: 700; loss: 1.21; acc: 0.66
Batch: 720; loss: 1.37; acc: 0.61
Batch: 740; loss: 1.42; acc: 0.62
Batch: 760; loss: 1.27; acc: 0.66
Batch: 780; loss: 1.31; acc: 0.64
Train Epoch over. train_loss: 1.32; train_accuracy: 0.63 

0.00014209923392627388
0.00013703036529477686
Batch: 0; loss: 1.38; acc: 0.59
Batch: 20; loss: 1.42; acc: 0.61
Batch: 40; loss: 1.03; acc: 0.75
Batch: 60; loss: 1.25; acc: 0.67
Batch: 80; loss: 1.03; acc: 0.72
Batch: 100; loss: 1.24; acc: 0.67
Batch: 120; loss: 1.42; acc: 0.55
Batch: 140; loss: 1.08; acc: 0.69
Val Epoch over. val_loss: 1.2635607149950259; val_accuracy: 0.6537619426751592 

The current subspace-distance is: 0.00013703036529477686 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.34; acc: 0.58
Batch: 80; loss: 1.4; acc: 0.58
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 1.22; acc: 0.67
Batch: 140; loss: 1.31; acc: 0.66
Batch: 160; loss: 1.41; acc: 0.55
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.36; acc: 0.62
Batch: 220; loss: 1.25; acc: 0.75
Batch: 240; loss: 1.37; acc: 0.58
Batch: 260; loss: 1.31; acc: 0.62
Batch: 280; loss: 1.33; acc: 0.59
Batch: 300; loss: 1.34; acc: 0.59
Batch: 320; loss: 1.28; acc: 0.62
Batch: 340; loss: 1.22; acc: 0.67
Batch: 360; loss: 1.24; acc: 0.67
Batch: 380; loss: 1.44; acc: 0.59
Batch: 400; loss: 1.15; acc: 0.73
Batch: 420; loss: 1.26; acc: 0.7
Batch: 440; loss: 1.31; acc: 0.58
Batch: 460; loss: 1.22; acc: 0.67
Batch: 480; loss: 1.22; acc: 0.66
Batch: 500; loss: 1.37; acc: 0.56
Batch: 520; loss: 1.35; acc: 0.59
Batch: 540; loss: 1.23; acc: 0.7
Batch: 560; loss: 1.21; acc: 0.69
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.41; acc: 0.53
Batch: 620; loss: 1.09; acc: 0.77
Batch: 640; loss: 1.19; acc: 0.67
Batch: 660; loss: 1.33; acc: 0.56
Batch: 680; loss: 1.23; acc: 0.66
Batch: 700; loss: 1.32; acc: 0.67
Batch: 720; loss: 1.23; acc: 0.67
Batch: 740; loss: 1.42; acc: 0.55
Batch: 760; loss: 1.39; acc: 0.56
Batch: 780; loss: 1.46; acc: 0.45
Train Epoch over. train_loss: 1.31; train_accuracy: 0.63 

0.00014812116569373757
0.00014359642227645963
Batch: 0; loss: 1.37; acc: 0.58
Batch: 20; loss: 1.4; acc: 0.59
Batch: 40; loss: 1.01; acc: 0.75
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.7
Batch: 120; loss: 1.41; acc: 0.56
Batch: 140; loss: 1.08; acc: 0.69
Val Epoch over. val_loss: 1.2645471593376938; val_accuracy: 0.6548566878980892 

The current subspace-distance is: 0.00014359642227645963 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 1.51; acc: 0.48
Batch: 60; loss: 1.3; acc: 0.59
Batch: 80; loss: 1.36; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.66
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.37; acc: 0.61
Batch: 160; loss: 1.49; acc: 0.55
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.28; acc: 0.59
Batch: 220; loss: 1.25; acc: 0.7
Batch: 240; loss: 1.4; acc: 0.62
Batch: 260; loss: 1.44; acc: 0.53
Batch: 280; loss: 1.28; acc: 0.67
Batch: 300; loss: 1.23; acc: 0.67
Batch: 320; loss: 1.28; acc: 0.64
Batch: 340; loss: 1.33; acc: 0.59
Batch: 360; loss: 1.17; acc: 0.73
Batch: 380; loss: 1.27; acc: 0.62
Batch: 400; loss: 1.33; acc: 0.56
Batch: 420; loss: 1.31; acc: 0.61
Batch: 440; loss: 1.42; acc: 0.52
Batch: 460; loss: 1.26; acc: 0.66
Batch: 480; loss: 1.33; acc: 0.64
Batch: 500; loss: 1.29; acc: 0.61
Batch: 520; loss: 1.34; acc: 0.58
Batch: 540; loss: 1.23; acc: 0.69
Batch: 560; loss: 1.31; acc: 0.61
Batch: 580; loss: 1.19; acc: 0.72
Batch: 600; loss: 1.34; acc: 0.62
Batch: 620; loss: 1.25; acc: 0.7
Batch: 640; loss: 1.33; acc: 0.59
Batch: 660; loss: 1.37; acc: 0.56
Batch: 680; loss: 1.52; acc: 0.52
Batch: 700; loss: 1.22; acc: 0.66
Batch: 720; loss: 1.28; acc: 0.66
Batch: 740; loss: 1.36; acc: 0.56
Batch: 760; loss: 1.38; acc: 0.59
Batch: 780; loss: 1.28; acc: 0.62
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.0001517266791779548
0.00014730628754477948
Batch: 0; loss: 1.36; acc: 0.58
Batch: 20; loss: 1.39; acc: 0.56
Batch: 40; loss: 1.0; acc: 0.77
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.22; acc: 0.7
Batch: 120; loss: 1.41; acc: 0.55
Batch: 140; loss: 1.07; acc: 0.69
Val Epoch over. val_loss: 1.2534857035442522; val_accuracy: 0.6466958598726115 

The current subspace-distance is: 0.00014730628754477948 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.42; acc: 0.5
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 1.26; acc: 0.66
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.37; acc: 0.52
Batch: 100; loss: 1.34; acc: 0.61
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 1.41; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.66
Batch: 200; loss: 1.16; acc: 0.66
Batch: 220; loss: 1.35; acc: 0.58
Batch: 240; loss: 1.18; acc: 0.7
Batch: 260; loss: 1.28; acc: 0.61
Batch: 280; loss: 1.22; acc: 0.69
Batch: 300; loss: 1.11; acc: 0.81
Batch: 320; loss: 1.1; acc: 0.8
Batch: 340; loss: 1.11; acc: 0.73
Batch: 360; loss: 1.19; acc: 0.75
Batch: 380; loss: 1.28; acc: 0.64
Batch: 400; loss: 1.39; acc: 0.66
Batch: 420; loss: 1.25; acc: 0.7
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.2; acc: 0.75
Batch: 480; loss: 1.17; acc: 0.7
Batch: 500; loss: 1.22; acc: 0.61
Batch: 520; loss: 1.05; acc: 0.78
Batch: 540; loss: 1.29; acc: 0.64
Batch: 560; loss: 1.26; acc: 0.62
Batch: 580; loss: 1.2; acc: 0.72
Batch: 600; loss: 1.32; acc: 0.58
Batch: 620; loss: 1.31; acc: 0.55
Batch: 640; loss: 1.38; acc: 0.58
Batch: 660; loss: 1.45; acc: 0.58
Batch: 680; loss: 1.23; acc: 0.61
Batch: 700; loss: 1.24; acc: 0.66
Batch: 720; loss: 1.37; acc: 0.59
Batch: 740; loss: 1.32; acc: 0.58
Batch: 760; loss: 1.2; acc: 0.62
Batch: 780; loss: 1.51; acc: 0.48
Train Epoch over. train_loss: 1.29; train_accuracy: 0.63 

0.00015300814993679523
0.00014744348300155252
Batch: 0; loss: 1.35; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.59
Batch: 40; loss: 0.99; acc: 0.8
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.53
Batch: 140; loss: 1.07; acc: 0.69
Val Epoch over. val_loss: 1.2472537049821988; val_accuracy: 0.65515525477707 

The current subspace-distance is: 0.00014744348300155252 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.36; acc: 0.55
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 1.26; acc: 0.69
Batch: 60; loss: 1.21; acc: 0.75
Batch: 80; loss: 1.34; acc: 0.61
Batch: 100; loss: 1.27; acc: 0.62
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.72
Batch: 160; loss: 1.26; acc: 0.66
Batch: 180; loss: 1.14; acc: 0.75
Batch: 200; loss: 1.35; acc: 0.59
Batch: 220; loss: 1.25; acc: 0.64
Batch: 240; loss: 1.22; acc: 0.69
Batch: 260; loss: 1.29; acc: 0.66
Batch: 280; loss: 1.28; acc: 0.64
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 1.47; acc: 0.55
Batch: 340; loss: 1.26; acc: 0.67
Batch: 360; loss: 1.17; acc: 0.69
Batch: 380; loss: 1.23; acc: 0.61
Batch: 400; loss: 1.3; acc: 0.58
Batch: 420; loss: 1.36; acc: 0.55
Batch: 440; loss: 1.36; acc: 0.62
Batch: 460; loss: 1.44; acc: 0.58
Batch: 480; loss: 1.33; acc: 0.61
Batch: 500; loss: 1.3; acc: 0.67
Batch: 520; loss: 1.37; acc: 0.62
Batch: 540; loss: 1.31; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.7
Batch: 580; loss: 1.23; acc: 0.64
Batch: 600; loss: 1.29; acc: 0.59
Batch: 620; loss: 1.37; acc: 0.53
Batch: 640; loss: 1.27; acc: 0.66
Batch: 660; loss: 1.29; acc: 0.64
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.29; acc: 0.59
Batch: 720; loss: 1.33; acc: 0.59
Batch: 740; loss: 1.38; acc: 0.64
Batch: 760; loss: 1.09; acc: 0.69
Batch: 780; loss: 1.33; acc: 0.66
Train Epoch over. train_loss: 1.29; train_accuracy: 0.63 

0.0001561543031129986
0.00015082786558195949
Batch: 0; loss: 1.34; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.41; acc: 0.53
Batch: 140; loss: 1.06; acc: 0.69
Val Epoch over. val_loss: 1.2418607864410254; val_accuracy: 0.6541600318471338 

The current subspace-distance is: 0.00015082786558195949 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.36; acc: 0.53
Batch: 20; loss: 1.47; acc: 0.53
Batch: 40; loss: 1.36; acc: 0.67
Batch: 60; loss: 1.27; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.66
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 1.22; acc: 0.59
Batch: 160; loss: 1.35; acc: 0.56
Batch: 180; loss: 1.29; acc: 0.69
Batch: 200; loss: 1.21; acc: 0.7
Batch: 220; loss: 1.24; acc: 0.64
Batch: 240; loss: 1.23; acc: 0.61
Batch: 260; loss: 1.26; acc: 0.61
Batch: 280; loss: 1.26; acc: 0.59
Batch: 300; loss: 1.18; acc: 0.7
Batch: 320; loss: 1.49; acc: 0.52
Batch: 340; loss: 1.19; acc: 0.7
Batch: 360; loss: 1.19; acc: 0.72
Batch: 380; loss: 1.4; acc: 0.55
Batch: 400; loss: 1.27; acc: 0.64
Batch: 420; loss: 1.39; acc: 0.64
Batch: 440; loss: 1.36; acc: 0.62
Batch: 460; loss: 1.23; acc: 0.66
Batch: 480; loss: 1.4; acc: 0.59
Batch: 500; loss: 1.38; acc: 0.58
Batch: 520; loss: 1.14; acc: 0.75
Batch: 540; loss: 1.36; acc: 0.55
Batch: 560; loss: 1.31; acc: 0.61
Batch: 580; loss: 1.39; acc: 0.62
Batch: 600; loss: 1.33; acc: 0.55
Batch: 620; loss: 1.23; acc: 0.64
Batch: 640; loss: 1.26; acc: 0.58
Batch: 660; loss: 1.49; acc: 0.52
Batch: 680; loss: 1.44; acc: 0.58
Batch: 700; loss: 1.26; acc: 0.64
Batch: 720; loss: 1.31; acc: 0.67
Batch: 740; loss: 1.4; acc: 0.53
Batch: 760; loss: 1.33; acc: 0.64
Batch: 780; loss: 1.43; acc: 0.5
Train Epoch over. train_loss: 1.29; train_accuracy: 0.64 

0.000155271336552687
0.0001509350840933621
Batch: 0; loss: 1.33; acc: 0.59
Batch: 20; loss: 1.36; acc: 0.58
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 1.18; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.05; acc: 0.7
Val Epoch over. val_loss: 1.2320891474462619; val_accuracy: 0.6586385350318471 

The current subspace-distance is: 0.0001509350840933621 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.38; acc: 0.59
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.19; acc: 0.67
Batch: 120; loss: 1.07; acc: 0.73
Batch: 140; loss: 1.18; acc: 0.69
Batch: 160; loss: 1.3; acc: 0.66
Batch: 180; loss: 1.26; acc: 0.64
Batch: 200; loss: 1.25; acc: 0.7
Batch: 220; loss: 1.37; acc: 0.59
Batch: 240; loss: 1.31; acc: 0.61
Batch: 260; loss: 1.29; acc: 0.55
Batch: 280; loss: 1.17; acc: 0.72
Batch: 300; loss: 1.39; acc: 0.61
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 1.29; acc: 0.61
Batch: 360; loss: 1.18; acc: 0.7
Batch: 380; loss: 1.28; acc: 0.58
Batch: 400; loss: 1.35; acc: 0.59
Batch: 420; loss: 1.26; acc: 0.72
Batch: 440; loss: 1.25; acc: 0.69
Batch: 460; loss: 1.26; acc: 0.61
Batch: 480; loss: 1.49; acc: 0.55
Batch: 500; loss: 1.43; acc: 0.59
Batch: 520; loss: 1.39; acc: 0.62
Batch: 540; loss: 1.26; acc: 0.64
Batch: 560; loss: 1.3; acc: 0.67
Batch: 580; loss: 1.41; acc: 0.59
Batch: 600; loss: 1.38; acc: 0.58
Batch: 620; loss: 1.28; acc: 0.61
Batch: 640; loss: 1.19; acc: 0.62
Batch: 660; loss: 1.39; acc: 0.59
Batch: 680; loss: 1.48; acc: 0.58
Batch: 700; loss: 1.5; acc: 0.52
Batch: 720; loss: 1.14; acc: 0.67
Batch: 740; loss: 1.19; acc: 0.72
Batch: 760; loss: 1.39; acc: 0.59
Batch: 780; loss: 1.22; acc: 0.61
Train Epoch over. train_loss: 1.28; train_accuracy: 0.64 

0.00015674246242269874
0.00015117002476472408
Batch: 0; loss: 1.33; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.59
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.2; acc: 0.69
Batch: 120; loss: 1.41; acc: 0.56
Batch: 140; loss: 1.07; acc: 0.69
Val Epoch over. val_loss: 1.2407865118069255; val_accuracy: 0.65515525477707 

The current subspace-distance is: 0.00015117002476472408 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.72
Batch: 40; loss: 1.29; acc: 0.58
Batch: 60; loss: 1.27; acc: 0.62
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.33; acc: 0.55
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 1.3; acc: 0.66
Batch: 160; loss: 1.33; acc: 0.62
Batch: 180; loss: 1.33; acc: 0.59
Batch: 200; loss: 1.25; acc: 0.62
Batch: 220; loss: 1.27; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.69
Batch: 260; loss: 1.25; acc: 0.64
Batch: 280; loss: 1.39; acc: 0.58
Batch: 300; loss: 1.36; acc: 0.62
Batch: 320; loss: 1.25; acc: 0.69
Batch: 340; loss: 1.3; acc: 0.67
Batch: 360; loss: 1.32; acc: 0.59
Batch: 380; loss: 1.37; acc: 0.58
Batch: 400; loss: 1.12; acc: 0.78
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.38; acc: 0.61
Batch: 460; loss: 1.26; acc: 0.69
Batch: 480; loss: 1.21; acc: 0.69
Batch: 500; loss: 1.29; acc: 0.64
Batch: 520; loss: 1.27; acc: 0.62
Batch: 540; loss: 1.44; acc: 0.55
Batch: 560; loss: 1.42; acc: 0.56
Batch: 580; loss: 1.42; acc: 0.61
Batch: 600; loss: 1.3; acc: 0.61
Batch: 620; loss: 1.19; acc: 0.69
Batch: 640; loss: 1.33; acc: 0.55
Batch: 660; loss: 1.32; acc: 0.59
Batch: 680; loss: 1.24; acc: 0.62
Batch: 700; loss: 1.27; acc: 0.66
Batch: 720; loss: 1.3; acc: 0.64
Batch: 740; loss: 1.15; acc: 0.7
Batch: 760; loss: 1.3; acc: 0.61
Batch: 780; loss: 1.34; acc: 0.66
Train Epoch over. train_loss: 1.28; train_accuracy: 0.64 

0.00016057773609645665
0.000155365007231012
Batch: 0; loss: 1.34; acc: 0.58
Batch: 20; loss: 1.36; acc: 0.58
Batch: 40; loss: 0.99; acc: 0.8
Batch: 60; loss: 1.22; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 1.06; acc: 0.7
Val Epoch over. val_loss: 1.2436015962795088; val_accuracy: 0.6603304140127388 

The current subspace-distance is: 0.000155365007231012 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.72
Batch: 40; loss: 1.31; acc: 0.67
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.29; acc: 0.69
Batch: 120; loss: 1.12; acc: 0.72
Batch: 140; loss: 1.29; acc: 0.58
Batch: 160; loss: 1.22; acc: 0.66
Batch: 180; loss: 1.29; acc: 0.56
Batch: 200; loss: 1.31; acc: 0.62
Batch: 220; loss: 1.26; acc: 0.62
Batch: 240; loss: 1.29; acc: 0.61
Batch: 260; loss: 1.35; acc: 0.62
Batch: 280; loss: 1.34; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.73
Batch: 320; loss: 1.19; acc: 0.72
Batch: 340; loss: 1.48; acc: 0.58
Batch: 360; loss: 1.23; acc: 0.66
Batch: 380; loss: 1.3; acc: 0.56
Batch: 400; loss: 1.31; acc: 0.56
Batch: 420; loss: 1.29; acc: 0.61
Batch: 440; loss: 1.17; acc: 0.73
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.38; acc: 0.56
Batch: 500; loss: 1.26; acc: 0.67
Batch: 520; loss: 1.23; acc: 0.64
Batch: 540; loss: 1.25; acc: 0.64
Batch: 560; loss: 1.34; acc: 0.56
Batch: 580; loss: 1.45; acc: 0.56
Batch: 600; loss: 1.3; acc: 0.69
Batch: 620; loss: 1.26; acc: 0.69
Batch: 640; loss: 1.13; acc: 0.69
Batch: 660; loss: 1.39; acc: 0.56
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.22; acc: 0.67
Batch: 720; loss: 1.25; acc: 0.67
Batch: 740; loss: 1.32; acc: 0.56
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 1.24; acc: 0.64
Train Epoch over. train_loss: 1.27; train_accuracy: 0.64 

0.00016137630154844373
0.00015350041212514043
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.34; acc: 0.58
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.04; acc: 0.7
Val Epoch over. val_loss: 1.2249483549670808; val_accuracy: 0.6632165605095541 

The current subspace-distance is: 0.00015350041212514043 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.69
Batch: 40; loss: 1.11; acc: 0.73
Batch: 60; loss: 1.27; acc: 0.66
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 1.29; acc: 0.66
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.28; acc: 0.64
Batch: 180; loss: 1.16; acc: 0.73
Batch: 200; loss: 1.39; acc: 0.52
Batch: 220; loss: 1.24; acc: 0.66
Batch: 240; loss: 1.17; acc: 0.73
Batch: 260; loss: 1.21; acc: 0.72
Batch: 280; loss: 1.24; acc: 0.66
Batch: 300; loss: 1.35; acc: 0.62
Batch: 320; loss: 1.31; acc: 0.59
Batch: 340; loss: 1.26; acc: 0.67
Batch: 360; loss: 1.29; acc: 0.73
Batch: 380; loss: 1.23; acc: 0.67
Batch: 400; loss: 1.23; acc: 0.69
Batch: 420; loss: 1.31; acc: 0.61
Batch: 440; loss: 1.3; acc: 0.59
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.4; acc: 0.56
Batch: 500; loss: 1.45; acc: 0.56
Batch: 520; loss: 1.11; acc: 0.75
Batch: 540; loss: 1.15; acc: 0.77
Batch: 560; loss: 1.32; acc: 0.58
Batch: 580; loss: 1.38; acc: 0.59
Batch: 600; loss: 1.15; acc: 0.75
Batch: 620; loss: 1.17; acc: 0.62
Batch: 640; loss: 1.26; acc: 0.58
Batch: 660; loss: 1.26; acc: 0.61
Batch: 680; loss: 1.32; acc: 0.66
Batch: 700; loss: 1.14; acc: 0.72
Batch: 720; loss: 1.18; acc: 0.69
Batch: 740; loss: 1.41; acc: 0.61
Batch: 760; loss: 1.22; acc: 0.66
Batch: 780; loss: 1.41; acc: 0.55
Train Epoch over. train_loss: 1.27; train_accuracy: 0.64 

0.0001670421042945236
0.00016129606228787452
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.34; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.81
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.04; acc: 0.7
Val Epoch over. val_loss: 1.2152458664717947; val_accuracy: 0.6630175159235668 

The current subspace-distance is: 0.00016129606228787452 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.7
Batch: 20; loss: 1.3; acc: 0.69
Batch: 40; loss: 1.17; acc: 0.73
Batch: 60; loss: 1.39; acc: 0.56
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 1.34; acc: 0.58
Batch: 140; loss: 1.3; acc: 0.61
Batch: 160; loss: 1.38; acc: 0.59
Batch: 180; loss: 1.36; acc: 0.56
Batch: 200; loss: 1.19; acc: 0.67
Batch: 220; loss: 1.31; acc: 0.7
Batch: 240; loss: 1.14; acc: 0.73
Batch: 260; loss: 1.19; acc: 0.67
Batch: 280; loss: 1.23; acc: 0.64
Batch: 300; loss: 1.18; acc: 0.7
Batch: 320; loss: 1.27; acc: 0.56
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.31; acc: 0.61
Batch: 380; loss: 1.29; acc: 0.61
Batch: 400; loss: 1.29; acc: 0.62
Batch: 420; loss: 1.07; acc: 0.69
Batch: 440; loss: 1.5; acc: 0.45
Batch: 460; loss: 1.22; acc: 0.67
Batch: 480; loss: 1.17; acc: 0.72
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 1.31; acc: 0.59
Batch: 540; loss: 1.3; acc: 0.59
Batch: 560; loss: 1.3; acc: 0.64
Batch: 580; loss: 1.17; acc: 0.66
Batch: 600; loss: 1.24; acc: 0.66
Batch: 620; loss: 1.23; acc: 0.73
Batch: 640; loss: 1.22; acc: 0.62
Batch: 660; loss: 1.24; acc: 0.7
Batch: 680; loss: 1.14; acc: 0.75
Batch: 700; loss: 1.27; acc: 0.62
Batch: 720; loss: 1.3; acc: 0.69
Batch: 740; loss: 1.22; acc: 0.72
Batch: 760; loss: 1.26; acc: 0.58
Batch: 780; loss: 1.27; acc: 0.56
Train Epoch over. train_loss: 1.26; train_accuracy: 0.64 

0.00016632652841508389
0.00016120390500873327
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.33; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.81
Batch: 60; loss: 1.18; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.75
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.03; acc: 0.7
Val Epoch over. val_loss: 1.218402473789871; val_accuracy: 0.666202229299363 

The current subspace-distance is: 0.00016120390500873327 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.02; acc: 0.77
Batch: 20; loss: 1.24; acc: 0.69
Batch: 40; loss: 1.28; acc: 0.66
Batch: 60; loss: 1.29; acc: 0.69
Batch: 80; loss: 1.14; acc: 0.73
Batch: 100; loss: 1.41; acc: 0.53
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.62
Batch: 160; loss: 1.17; acc: 0.7
Batch: 180; loss: 1.19; acc: 0.72
Batch: 200; loss: 1.17; acc: 0.7
Batch: 220; loss: 1.11; acc: 0.72
Batch: 240; loss: 1.19; acc: 0.59
Batch: 260; loss: 1.36; acc: 0.62
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.35; acc: 0.55
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.3; acc: 0.64
Batch: 360; loss: 1.35; acc: 0.64
Batch: 380; loss: 1.16; acc: 0.72
Batch: 400; loss: 1.07; acc: 0.67
Batch: 420; loss: 1.31; acc: 0.59
Batch: 440; loss: 1.36; acc: 0.61
Batch: 460; loss: 1.2; acc: 0.64
Batch: 480; loss: 1.22; acc: 0.69
Batch: 500; loss: 1.23; acc: 0.64
Batch: 520; loss: 1.39; acc: 0.56
Batch: 540; loss: 1.39; acc: 0.61
Batch: 560; loss: 1.22; acc: 0.72
Batch: 580; loss: 1.27; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.69
Batch: 620; loss: 1.28; acc: 0.66
Batch: 640; loss: 1.27; acc: 0.64
Batch: 660; loss: 1.11; acc: 0.78
Batch: 680; loss: 1.3; acc: 0.64
Batch: 700; loss: 1.14; acc: 0.66
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.15; acc: 0.73
Batch: 780; loss: 1.19; acc: 0.69
Train Epoch over. train_loss: 1.26; train_accuracy: 0.65 

0.00016952602891251445
0.0001641058042878285
Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 1.34; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.83
Batch: 60; loss: 1.18; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.7
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.05; acc: 0.69
Val Epoch over. val_loss: 1.2208193282412876; val_accuracy: 0.6673964968152867 

The current subspace-distance is: 0.0001641058042878285 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.26; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 1.29; acc: 0.56
Batch: 60; loss: 1.34; acc: 0.61
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 1.41; acc: 0.47
Batch: 160; loss: 1.42; acc: 0.56
Batch: 180; loss: 1.23; acc: 0.72
Batch: 200; loss: 1.28; acc: 0.61
Batch: 220; loss: 1.16; acc: 0.7
Batch: 240; loss: 1.32; acc: 0.61
Batch: 260; loss: 1.32; acc: 0.59
Batch: 280; loss: 1.2; acc: 0.72
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 1.14; acc: 0.73
Batch: 340; loss: 1.21; acc: 0.69
Batch: 360; loss: 1.04; acc: 0.78
Batch: 380; loss: 1.46; acc: 0.53
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 1.32; acc: 0.61
Batch: 440; loss: 1.36; acc: 0.58
Batch: 460; loss: 1.22; acc: 0.72
Batch: 480; loss: 1.13; acc: 0.67
Batch: 500; loss: 1.38; acc: 0.59
Batch: 520; loss: 1.19; acc: 0.72
Batch: 540; loss: 1.47; acc: 0.48
Batch: 560; loss: 1.2; acc: 0.67
Batch: 580; loss: 1.27; acc: 0.62
Batch: 600; loss: 1.18; acc: 0.67
Batch: 620; loss: 1.27; acc: 0.67
Batch: 640; loss: 1.12; acc: 0.69
Batch: 660; loss: 1.2; acc: 0.75
Batch: 680; loss: 1.44; acc: 0.56
Batch: 700; loss: 1.17; acc: 0.69
Batch: 720; loss: 1.42; acc: 0.48
Batch: 740; loss: 1.23; acc: 0.62
Batch: 760; loss: 1.31; acc: 0.55
Batch: 780; loss: 1.34; acc: 0.64
Train Epoch over. train_loss: 1.26; train_accuracy: 0.65 

0.00016909746045712382
0.0001623899588594213
Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 1.33; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.81
Batch: 60; loss: 1.18; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.03; acc: 0.7
Val Epoch over. val_loss: 1.2121436546562583; val_accuracy: 0.671875 

The current subspace-distance is: 0.0001623899588594213 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 1.23; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.31; acc: 0.64
Batch: 100; loss: 0.94; acc: 0.81
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 1.3; acc: 0.62
Batch: 160; loss: 1.3; acc: 0.56
Batch: 180; loss: 1.36; acc: 0.56
Batch: 200; loss: 1.3; acc: 0.62
Batch: 220; loss: 1.37; acc: 0.56
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.4; acc: 0.62
Batch: 280; loss: 1.29; acc: 0.64
Batch: 300; loss: 1.19; acc: 0.66
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.14; acc: 0.69
Batch: 360; loss: 1.25; acc: 0.59
Batch: 380; loss: 1.46; acc: 0.55
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.32; acc: 0.56
Batch: 440; loss: 1.22; acc: 0.69
Batch: 460; loss: 1.28; acc: 0.67
Batch: 480; loss: 1.27; acc: 0.69
Batch: 500; loss: 1.22; acc: 0.72
Batch: 520; loss: 1.3; acc: 0.59
Batch: 540; loss: 1.08; acc: 0.78
Batch: 560; loss: 1.31; acc: 0.62
Batch: 580; loss: 1.35; acc: 0.61
Batch: 600; loss: 1.36; acc: 0.58
Batch: 620; loss: 1.19; acc: 0.66
Batch: 640; loss: 1.19; acc: 0.67
Batch: 660; loss: 1.23; acc: 0.72
Batch: 680; loss: 1.23; acc: 0.73
Batch: 700; loss: 1.3; acc: 0.59
Batch: 720; loss: 1.47; acc: 0.47
Batch: 740; loss: 1.19; acc: 0.66
Batch: 760; loss: 1.24; acc: 0.69
Batch: 780; loss: 1.34; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.000168536338605918
0.00016304153541568667
Batch: 0; loss: 1.3; acc: 0.58
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 0.95; acc: 0.83
Batch: 60; loss: 1.18; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.7
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.04; acc: 0.67
Val Epoch over. val_loss: 1.2167353937580327; val_accuracy: 0.6689888535031847 

The current subspace-distance is: 0.00016304153541568667 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 1.38; acc: 0.56
Batch: 60; loss: 1.28; acc: 0.67
Batch: 80; loss: 1.28; acc: 0.61
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.62
Batch: 140; loss: 1.23; acc: 0.66
Batch: 160; loss: 1.31; acc: 0.67
Batch: 180; loss: 1.28; acc: 0.62
Batch: 200; loss: 1.26; acc: 0.62
Batch: 220; loss: 1.19; acc: 0.67
Batch: 240; loss: 1.22; acc: 0.7
Batch: 260; loss: 1.24; acc: 0.72
Batch: 280; loss: 1.25; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.7
Batch: 320; loss: 1.52; acc: 0.56
Batch: 340; loss: 1.15; acc: 0.67
Batch: 360; loss: 1.18; acc: 0.72
Batch: 380; loss: 1.39; acc: 0.58
Batch: 400; loss: 1.21; acc: 0.72
Batch: 420; loss: 1.26; acc: 0.62
Batch: 440; loss: 1.39; acc: 0.61
Batch: 460; loss: 1.15; acc: 0.77
Batch: 480; loss: 1.15; acc: 0.75
Batch: 500; loss: 1.41; acc: 0.56
Batch: 520; loss: 1.2; acc: 0.7
Batch: 540; loss: 1.28; acc: 0.61
Batch: 560; loss: 1.38; acc: 0.56
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.67
Batch: 620; loss: 1.13; acc: 0.7
Batch: 640; loss: 1.26; acc: 0.7
Batch: 660; loss: 1.14; acc: 0.72
Batch: 680; loss: 1.15; acc: 0.69
Batch: 700; loss: 1.3; acc: 0.56
Batch: 720; loss: 1.32; acc: 0.59
Batch: 740; loss: 1.46; acc: 0.56
Batch: 760; loss: 1.28; acc: 0.61
Batch: 780; loss: 1.34; acc: 0.59
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.00016995058103930205
0.00016254509682767093
Batch: 0; loss: 1.29; acc: 0.61
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 0.95; acc: 0.83
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.04; acc: 0.69
Val Epoch over. val_loss: 1.2105393117400491; val_accuracy: 0.6717754777070064 

The current subspace-distance is: 0.00016254509682767093 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.29; acc: 0.61
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.03; acc: 0.73
Batch: 80; loss: 1.27; acc: 0.69
Batch: 100; loss: 1.27; acc: 0.61
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.24; acc: 0.66
Batch: 160; loss: 1.32; acc: 0.55
Batch: 180; loss: 1.25; acc: 0.7
Batch: 200; loss: 1.47; acc: 0.56
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 1.18; acc: 0.7
Batch: 260; loss: 1.41; acc: 0.64
Batch: 280; loss: 1.21; acc: 0.67
Batch: 300; loss: 1.21; acc: 0.61
Batch: 320; loss: 1.13; acc: 0.69
Batch: 340; loss: 1.47; acc: 0.59
Batch: 360; loss: 1.33; acc: 0.64
Batch: 380; loss: 1.39; acc: 0.62
Batch: 400; loss: 1.12; acc: 0.72
Batch: 420; loss: 1.17; acc: 0.67
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.34; acc: 0.62
Batch: 480; loss: 1.2; acc: 0.67
Batch: 500; loss: 1.29; acc: 0.61
Batch: 520; loss: 1.16; acc: 0.69
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.19; acc: 0.69
Batch: 580; loss: 1.18; acc: 0.67
Batch: 600; loss: 1.32; acc: 0.67
Batch: 620; loss: 1.12; acc: 0.7
Batch: 640; loss: 1.32; acc: 0.56
Batch: 660; loss: 1.03; acc: 0.86
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.2; acc: 0.69
Batch: 720; loss: 1.13; acc: 0.69
Batch: 740; loss: 1.3; acc: 0.55
Batch: 760; loss: 1.28; acc: 0.61
Batch: 780; loss: 1.36; acc: 0.61
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.00017067565931938589
0.00016371988749597222
Batch: 0; loss: 1.28; acc: 0.61
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 0.94; acc: 0.83
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.14; acc: 0.7
Batch: 120; loss: 1.39; acc: 0.58
Batch: 140; loss: 1.03; acc: 0.69
Val Epoch over. val_loss: 1.2019443508166416; val_accuracy: 0.6720740445859873 

The current subspace-distance is: 0.00016371988749597222 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 1.21; acc: 0.67
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.19; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 1.13; acc: 0.73
Batch: 160; loss: 1.16; acc: 0.64
Batch: 180; loss: 1.15; acc: 0.69
Batch: 200; loss: 1.39; acc: 0.61
Batch: 220; loss: 1.2; acc: 0.67
Batch: 240; loss: 1.23; acc: 0.62
Batch: 260; loss: 1.16; acc: 0.64
Batch: 280; loss: 1.27; acc: 0.62
Batch: 300; loss: 1.16; acc: 0.72
Batch: 320; loss: 1.12; acc: 0.69
Batch: 340; loss: 1.2; acc: 0.66
Batch: 360; loss: 1.24; acc: 0.66
Batch: 380; loss: 1.26; acc: 0.64
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 1.47; acc: 0.53
Batch: 440; loss: 1.37; acc: 0.61
Batch: 460; loss: 1.14; acc: 0.77
Batch: 480; loss: 1.32; acc: 0.52
Batch: 500; loss: 1.29; acc: 0.69
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 1.3; acc: 0.62
Batch: 560; loss: 1.34; acc: 0.59
Batch: 580; loss: 1.23; acc: 0.67
Batch: 600; loss: 1.23; acc: 0.62
Batch: 620; loss: 1.32; acc: 0.66
Batch: 640; loss: 1.33; acc: 0.59
Batch: 660; loss: 1.19; acc: 0.72
Batch: 680; loss: 1.31; acc: 0.59
Batch: 700; loss: 1.29; acc: 0.64
Batch: 720; loss: 1.26; acc: 0.72
Batch: 740; loss: 1.31; acc: 0.58
Batch: 760; loss: 1.13; acc: 0.77
Batch: 780; loss: 1.08; acc: 0.8
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.00017016915080603212
0.00016424735076725483
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 0.94; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.72
Val Epoch over. val_loss: 1.1898759018843341; val_accuracy: 0.6819267515923567 

The current subspace-distance is: 0.00016424735076725483 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 1.44; acc: 0.52
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.32; acc: 0.66
Batch: 100; loss: 1.25; acc: 0.66
Batch: 120; loss: 1.32; acc: 0.58
Batch: 140; loss: 1.24; acc: 0.67
Batch: 160; loss: 1.23; acc: 0.67
Batch: 180; loss: 1.25; acc: 0.67
Batch: 200; loss: 1.17; acc: 0.64
Batch: 220; loss: 1.19; acc: 0.64
Batch: 240; loss: 1.12; acc: 0.67
Batch: 260; loss: 1.19; acc: 0.73
Batch: 280; loss: 1.24; acc: 0.69
Batch: 300; loss: 1.22; acc: 0.7
Batch: 320; loss: 1.38; acc: 0.62
Batch: 340; loss: 1.15; acc: 0.72
Batch: 360; loss: 1.2; acc: 0.67
Batch: 380; loss: 1.11; acc: 0.7
Batch: 400; loss: 1.37; acc: 0.58
Batch: 420; loss: 1.34; acc: 0.59
Batch: 440; loss: 1.43; acc: 0.53
Batch: 460; loss: 1.29; acc: 0.59
Batch: 480; loss: 1.3; acc: 0.61
Batch: 500; loss: 1.5; acc: 0.53
Batch: 520; loss: 1.1; acc: 0.75
Batch: 540; loss: 1.18; acc: 0.69
Batch: 560; loss: 1.29; acc: 0.64
Batch: 580; loss: 1.28; acc: 0.66
Batch: 600; loss: 1.29; acc: 0.66
Batch: 620; loss: 1.44; acc: 0.48
Batch: 640; loss: 1.11; acc: 0.73
Batch: 660; loss: 1.03; acc: 0.73
Batch: 680; loss: 1.13; acc: 0.7
Batch: 700; loss: 1.36; acc: 0.62
Batch: 720; loss: 1.29; acc: 0.67
Batch: 740; loss: 1.22; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.72
Batch: 780; loss: 1.19; acc: 0.69
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.00017323243082500994
0.00016656203661113977
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.31; acc: 0.61
Batch: 40; loss: 0.94; acc: 0.81
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 1.02; acc: 0.7
Val Epoch over. val_loss: 1.196954521783598; val_accuracy: 0.6790406050955414 

The current subspace-distance is: 0.00016656203661113977 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.61
Batch: 40; loss: 1.31; acc: 0.55
Batch: 60; loss: 1.2; acc: 0.73
Batch: 80; loss: 1.09; acc: 0.69
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 1.3; acc: 0.64
Batch: 160; loss: 1.37; acc: 0.58
Batch: 180; loss: 1.36; acc: 0.67
Batch: 200; loss: 1.25; acc: 0.64
Batch: 220; loss: 1.22; acc: 0.7
Batch: 240; loss: 1.16; acc: 0.75
Batch: 260; loss: 1.36; acc: 0.66
Batch: 280; loss: 1.09; acc: 0.77
Batch: 300; loss: 1.3; acc: 0.64
Batch: 320; loss: 1.14; acc: 0.7
Batch: 340; loss: 1.23; acc: 0.66
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.21; acc: 0.61
Batch: 420; loss: 1.23; acc: 0.69
Batch: 440; loss: 1.28; acc: 0.62
Batch: 460; loss: 1.15; acc: 0.67
Batch: 480; loss: 1.32; acc: 0.55
Batch: 500; loss: 1.19; acc: 0.69
Batch: 520; loss: 1.17; acc: 0.72
Batch: 540; loss: 1.25; acc: 0.61
Batch: 560; loss: 1.17; acc: 0.67
Batch: 580; loss: 1.16; acc: 0.67
Batch: 600; loss: 1.2; acc: 0.7
Batch: 620; loss: 1.25; acc: 0.64
Batch: 640; loss: 1.27; acc: 0.62
Batch: 660; loss: 1.21; acc: 0.69
Batch: 680; loss: 1.15; acc: 0.72
Batch: 700; loss: 1.26; acc: 0.69
Batch: 720; loss: 1.29; acc: 0.58
Batch: 740; loss: 1.2; acc: 0.67
Batch: 760; loss: 1.27; acc: 0.62
Batch: 780; loss: 1.17; acc: 0.75
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.00017176430264953524
0.0001660393609199673
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 0.95; acc: 0.83
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.39; acc: 0.59
Batch: 140; loss: 1.02; acc: 0.7
Val Epoch over. val_loss: 1.20041212078872; val_accuracy: 0.6752587579617835 

The current subspace-distance is: 0.0001660393609199673 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.29; acc: 0.64
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 1.27; acc: 0.67
Batch: 60; loss: 1.32; acc: 0.62
Batch: 80; loss: 1.26; acc: 0.72
Batch: 100; loss: 1.33; acc: 0.59
Batch: 120; loss: 1.27; acc: 0.59
Batch: 140; loss: 1.24; acc: 0.61
Batch: 160; loss: 1.27; acc: 0.62
Batch: 180; loss: 1.32; acc: 0.64
Batch: 200; loss: 1.33; acc: 0.61
Batch: 220; loss: 1.36; acc: 0.58
Batch: 240; loss: 1.23; acc: 0.67
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 1.03; acc: 0.78
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.27; acc: 0.7
Batch: 380; loss: 1.17; acc: 0.69
Batch: 400; loss: 1.05; acc: 0.73
Batch: 420; loss: 1.22; acc: 0.75
Batch: 440; loss: 1.31; acc: 0.62
Batch: 460; loss: 1.37; acc: 0.64
Batch: 480; loss: 1.27; acc: 0.66
Batch: 500; loss: 1.16; acc: 0.67
Batch: 520; loss: 1.32; acc: 0.59
Batch: 540; loss: 1.23; acc: 0.64
Batch: 560; loss: 1.37; acc: 0.58
Batch: 580; loss: 1.24; acc: 0.64
Batch: 600; loss: 1.47; acc: 0.61
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 1.21; acc: 0.62
Batch: 660; loss: 1.17; acc: 0.69
Batch: 680; loss: 1.19; acc: 0.69
Batch: 700; loss: 1.35; acc: 0.67
Batch: 720; loss: 1.26; acc: 0.67
Batch: 740; loss: 1.38; acc: 0.52
Batch: 760; loss: 1.13; acc: 0.73
Batch: 780; loss: 1.29; acc: 0.67
Train Epoch over. train_loss: 1.24; train_accuracy: 0.65 

0.00017383504018653184
0.00016789877554401755
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 0.95; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.39; acc: 0.59
Batch: 140; loss: 1.02; acc: 0.7
Val Epoch over. val_loss: 1.1998329789015898; val_accuracy: 0.6694864649681529 

The current subspace-distance is: 0.00016789877554401755 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 1.27; acc: 0.69
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.16; acc: 0.73
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.19; acc: 0.61
Batch: 140; loss: 1.23; acc: 0.69
Batch: 160; loss: 1.34; acc: 0.64
Batch: 180; loss: 1.22; acc: 0.64
Batch: 200; loss: 1.24; acc: 0.64
Batch: 220; loss: 1.29; acc: 0.64
Batch: 240; loss: 1.2; acc: 0.67
Batch: 260; loss: 1.21; acc: 0.75
Batch: 280; loss: 1.11; acc: 0.7
Batch: 300; loss: 1.36; acc: 0.59
Batch: 320; loss: 1.22; acc: 0.61
Batch: 340; loss: 1.04; acc: 0.8
Batch: 360; loss: 1.12; acc: 0.7
Batch: 380; loss: 1.28; acc: 0.69
Batch: 400; loss: 1.28; acc: 0.66
Batch: 420; loss: 1.2; acc: 0.67
Batch: 440; loss: 1.47; acc: 0.55
Batch: 460; loss: 1.2; acc: 0.72
Batch: 480; loss: 1.36; acc: 0.64
Batch: 500; loss: 1.28; acc: 0.62
Batch: 520; loss: 1.42; acc: 0.55
Batch: 540; loss: 1.28; acc: 0.64
Batch: 560; loss: 1.2; acc: 0.67
Batch: 580; loss: 1.21; acc: 0.69
Batch: 600; loss: 1.24; acc: 0.66
Batch: 620; loss: 1.28; acc: 0.66
Batch: 640; loss: 1.37; acc: 0.67
Batch: 660; loss: 1.27; acc: 0.69
Batch: 680; loss: 1.32; acc: 0.59
Batch: 700; loss: 1.33; acc: 0.58
Batch: 720; loss: 1.1; acc: 0.7
Batch: 740; loss: 1.2; acc: 0.67
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.15; acc: 0.67
Train Epoch over. train_loss: 1.24; train_accuracy: 0.65 

0.00017498912347946316
0.00016743145533837378
Batch: 0; loss: 1.28; acc: 0.66
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 0.95; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 1.39; acc: 0.61
Batch: 140; loss: 1.02; acc: 0.69
Val Epoch over. val_loss: 1.199515433827783; val_accuracy: 0.6787420382165605 

The current subspace-distance is: 0.00016743145533837378 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 1.38; acc: 0.58
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 1.15; acc: 0.73
Batch: 100; loss: 1.28; acc: 0.59
Batch: 120; loss: 1.34; acc: 0.59
Batch: 140; loss: 1.2; acc: 0.69
Batch: 160; loss: 1.13; acc: 0.66
Batch: 180; loss: 1.16; acc: 0.7
Batch: 200; loss: 1.25; acc: 0.67
Batch: 220; loss: 1.41; acc: 0.55
Batch: 240; loss: 1.31; acc: 0.58
Batch: 260; loss: 1.12; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.7
Batch: 300; loss: 1.26; acc: 0.58
Batch: 320; loss: 1.18; acc: 0.73
Batch: 340; loss: 1.13; acc: 0.73
Batch: 360; loss: 1.15; acc: 0.69
Batch: 380; loss: 1.19; acc: 0.67
Batch: 400; loss: 1.24; acc: 0.64
Batch: 420; loss: 1.1; acc: 0.69
Batch: 440; loss: 1.25; acc: 0.64
Batch: 460; loss: 1.11; acc: 0.77
Batch: 480; loss: 1.31; acc: 0.62
Batch: 500; loss: 1.21; acc: 0.69
Batch: 520; loss: 1.26; acc: 0.67
Batch: 540; loss: 1.26; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.72
Batch: 580; loss: 1.27; acc: 0.62
Batch: 600; loss: 1.27; acc: 0.64
Batch: 620; loss: 1.18; acc: 0.59
Batch: 640; loss: 1.39; acc: 0.61
Batch: 660; loss: 1.31; acc: 0.66
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.23; acc: 0.72
Batch: 720; loss: 1.17; acc: 0.67
Batch: 740; loss: 1.24; acc: 0.64
Batch: 760; loss: 1.17; acc: 0.7
Batch: 780; loss: 1.29; acc: 0.58
Train Epoch over. train_loss: 1.24; train_accuracy: 0.65 

0.0001736755802994594
0.00016894307918846607
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 1.02; acc: 0.69
Val Epoch over. val_loss: 1.1957404446450008; val_accuracy: 0.6778463375796179 

The current subspace-distance is: 0.00016894307918846607 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_15_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.03

The number of parameters is: 284685

The number of individual parameters is:

9
162
9
9
13
35802
13
13
25
99450
25
25
64
144000
64
64
4096
64
640
10
64
64

nonzero elements in E: 56936995
elements in E: 56937000
fraction nonzero: 0.9999999121836416
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.48; acc: 0.03
Batch: 20; loss: 2.23; acc: 0.22
Batch: 40; loss: 2.12; acc: 0.3
Batch: 60; loss: 1.91; acc: 0.41
Batch: 80; loss: 1.93; acc: 0.44
Batch: 100; loss: 1.82; acc: 0.5
Batch: 120; loss: 1.76; acc: 0.5
Batch: 140; loss: 1.77; acc: 0.48
Batch: 160; loss: 1.84; acc: 0.44
Batch: 180; loss: 1.63; acc: 0.58
Batch: 200; loss: 1.62; acc: 0.66
Batch: 220; loss: 1.64; acc: 0.55
Batch: 240; loss: 1.67; acc: 0.58
Batch: 260; loss: 1.63; acc: 0.56
Batch: 280; loss: 1.73; acc: 0.47
Batch: 300; loss: 1.46; acc: 0.69
Batch: 320; loss: 1.59; acc: 0.59
Batch: 340; loss: 1.63; acc: 0.52
Batch: 360; loss: 1.53; acc: 0.61
Batch: 380; loss: 1.44; acc: 0.64
Batch: 400; loss: 1.6; acc: 0.56
Batch: 420; loss: 1.59; acc: 0.64
Batch: 440; loss: 1.48; acc: 0.67
Batch: 460; loss: 1.57; acc: 0.62
Batch: 480; loss: 1.43; acc: 0.7
Batch: 500; loss: 1.46; acc: 0.62
Batch: 520; loss: 1.5; acc: 0.61
Batch: 540; loss: 1.5; acc: 0.67
Batch: 560; loss: 1.36; acc: 0.72
Batch: 580; loss: 1.42; acc: 0.7
Batch: 600; loss: 1.41; acc: 0.69
Batch: 620; loss: 1.44; acc: 0.59
Batch: 640; loss: 1.39; acc: 0.72
Batch: 660; loss: 1.46; acc: 0.62
Batch: 680; loss: 1.41; acc: 0.7
Batch: 700; loss: 1.44; acc: 0.62
Batch: 720; loss: 1.42; acc: 0.62
Batch: 740; loss: 1.46; acc: 0.67
Batch: 760; loss: 1.34; acc: 0.72
Batch: 780; loss: 1.54; acc: 0.66
Train Epoch over. train_loss: 1.6; train_accuracy: 0.59 

6.748393934685737e-05
6.307666626526043e-05
Batch: 0; loss: 1.41; acc: 0.61
Batch: 20; loss: 1.47; acc: 0.59
Batch: 40; loss: 1.1; acc: 0.83
Batch: 60; loss: 1.21; acc: 0.75
Batch: 80; loss: 1.21; acc: 0.77
Batch: 100; loss: 1.29; acc: 0.77
Batch: 120; loss: 1.44; acc: 0.61
Batch: 140; loss: 1.24; acc: 0.72
Val Epoch over. val_loss: 1.3290781541994423; val_accuracy: 0.7066082802547771 

The current subspace-distance is: 6.307666626526043e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.66
Batch: 20; loss: 1.43; acc: 0.62
Batch: 40; loss: 1.44; acc: 0.58
Batch: 60; loss: 1.42; acc: 0.7
Batch: 80; loss: 1.34; acc: 0.69
Batch: 100; loss: 1.33; acc: 0.64
Batch: 120; loss: 1.4; acc: 0.67
Batch: 140; loss: 1.41; acc: 0.62
Batch: 160; loss: 1.38; acc: 0.72
Batch: 180; loss: 1.34; acc: 0.67
Batch: 200; loss: 1.25; acc: 0.73
Batch: 220; loss: 1.31; acc: 0.66
Batch: 240; loss: 1.29; acc: 0.69
Batch: 260; loss: 1.4; acc: 0.66
Batch: 280; loss: 1.34; acc: 0.67
Batch: 300; loss: 1.23; acc: 0.72
Batch: 320; loss: 1.41; acc: 0.56
Batch: 340; loss: 1.25; acc: 0.77
Batch: 360; loss: 1.33; acc: 0.73
Batch: 380; loss: 1.3; acc: 0.67
Batch: 400; loss: 1.3; acc: 0.7
Batch: 420; loss: 1.24; acc: 0.7
Batch: 440; loss: 1.48; acc: 0.64
Batch: 460; loss: 1.27; acc: 0.75
Batch: 480; loss: 1.39; acc: 0.64
Batch: 500; loss: 1.4; acc: 0.62
Batch: 520; loss: 1.24; acc: 0.75
Batch: 540; loss: 1.34; acc: 0.66
Batch: 560; loss: 1.29; acc: 0.7
Batch: 580; loss: 1.33; acc: 0.55
Batch: 600; loss: 1.3; acc: 0.69
Batch: 620; loss: 1.37; acc: 0.64
Batch: 640; loss: 1.28; acc: 0.66
Batch: 660; loss: 1.27; acc: 0.75
Batch: 680; loss: 1.12; acc: 0.81
Batch: 700; loss: 1.21; acc: 0.67
Batch: 720; loss: 1.38; acc: 0.66
Batch: 740; loss: 1.09; acc: 0.8
Batch: 760; loss: 1.17; acc: 0.8
Batch: 780; loss: 1.14; acc: 0.72
Train Epoch over. train_loss: 1.3; train_accuracy: 0.69 

8.897965017240494e-05
8.517075184499845e-05
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.41; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.89
Batch: 60; loss: 1.05; acc: 0.84
Batch: 80; loss: 1.07; acc: 0.83
Batch: 100; loss: 1.21; acc: 0.77
Batch: 120; loss: 1.39; acc: 0.59
Batch: 140; loss: 1.07; acc: 0.73
Val Epoch over. val_loss: 1.1949031694679504; val_accuracy: 0.7310907643312102 

The current subspace-distance is: 8.517075184499845e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.59
Batch: 20; loss: 1.22; acc: 0.75
Batch: 40; loss: 1.13; acc: 0.75
Batch: 60; loss: 1.29; acc: 0.66
Batch: 80; loss: 1.3; acc: 0.7
Batch: 100; loss: 1.25; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.8
Batch: 140; loss: 1.36; acc: 0.66
Batch: 160; loss: 1.13; acc: 0.77
Batch: 180; loss: 1.19; acc: 0.78
Batch: 200; loss: 1.41; acc: 0.64
Batch: 220; loss: 1.14; acc: 0.7
Batch: 240; loss: 1.23; acc: 0.73
Batch: 260; loss: 1.22; acc: 0.77
Batch: 280; loss: 1.31; acc: 0.66
Batch: 300; loss: 1.17; acc: 0.7
Batch: 320; loss: 1.19; acc: 0.72
Batch: 340; loss: 1.18; acc: 0.72
Batch: 360; loss: 1.19; acc: 0.7
Batch: 380; loss: 1.04; acc: 0.8
Batch: 400; loss: 1.3; acc: 0.69
Batch: 420; loss: 1.2; acc: 0.7
Batch: 440; loss: 1.22; acc: 0.7
Batch: 460; loss: 1.13; acc: 0.81
Batch: 480; loss: 1.24; acc: 0.69
Batch: 500; loss: 1.15; acc: 0.8
Batch: 520; loss: 1.15; acc: 0.78
Batch: 540; loss: 1.26; acc: 0.7
Batch: 560; loss: 1.18; acc: 0.7
Batch: 580; loss: 1.2; acc: 0.66
Batch: 600; loss: 1.18; acc: 0.73
Batch: 620; loss: 1.16; acc: 0.77
Batch: 640; loss: 1.19; acc: 0.72
Batch: 660; loss: 1.18; acc: 0.73
Batch: 680; loss: 1.08; acc: 0.78
Batch: 700; loss: 1.14; acc: 0.73
Batch: 720; loss: 1.17; acc: 0.72
Batch: 740; loss: 1.18; acc: 0.66
Batch: 760; loss: 1.32; acc: 0.61
Batch: 780; loss: 1.18; acc: 0.7
Train Epoch over. train_loss: 1.21; train_accuracy: 0.71 

0.0001058048801496625
0.00010217781527899206
Batch: 0; loss: 1.25; acc: 0.64
Batch: 20; loss: 1.35; acc: 0.61
Batch: 40; loss: 0.79; acc: 0.91
Batch: 60; loss: 0.99; acc: 0.89
Batch: 80; loss: 1.01; acc: 0.81
Batch: 100; loss: 1.16; acc: 0.75
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 1.1209120632736547; val_accuracy: 0.7421377388535032 

The current subspace-distance is: 0.00010217781527899206 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.8
Batch: 20; loss: 1.13; acc: 0.72
Batch: 40; loss: 1.08; acc: 0.8
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.0; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.64
Batch: 160; loss: 1.18; acc: 0.69
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.08; acc: 0.8
Batch: 220; loss: 1.25; acc: 0.67
Batch: 240; loss: 1.21; acc: 0.69
Batch: 260; loss: 1.06; acc: 0.75
Batch: 280; loss: 1.02; acc: 0.81
Batch: 300; loss: 1.18; acc: 0.69
Batch: 320; loss: 1.11; acc: 0.78
Batch: 340; loss: 1.23; acc: 0.61
Batch: 360; loss: 1.12; acc: 0.73
Batch: 380; loss: 1.09; acc: 0.72
Batch: 400; loss: 1.15; acc: 0.72
Batch: 420; loss: 1.06; acc: 0.75
Batch: 440; loss: 1.12; acc: 0.77
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 1.11; acc: 0.77
Batch: 500; loss: 1.16; acc: 0.72
Batch: 520; loss: 1.09; acc: 0.73
Batch: 540; loss: 1.11; acc: 0.73
Batch: 560; loss: 1.06; acc: 0.73
Batch: 580; loss: 1.14; acc: 0.75
Batch: 600; loss: 1.14; acc: 0.7
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.14; acc: 0.75
Batch: 660; loss: 1.12; acc: 0.75
Batch: 680; loss: 1.1; acc: 0.7
Batch: 700; loss: 1.08; acc: 0.77
Batch: 720; loss: 1.06; acc: 0.77
Batch: 740; loss: 1.14; acc: 0.7
Batch: 760; loss: 1.15; acc: 0.78
Batch: 780; loss: 1.08; acc: 0.75
Train Epoch over. train_loss: 1.15; train_accuracy: 0.73 

0.0001193455173051916
0.0001148729061242193
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 0.72; acc: 0.92
Batch: 60; loss: 0.94; acc: 0.86
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.87; acc: 0.78
Val Epoch over. val_loss: 1.0577911628279717; val_accuracy: 0.7575636942675159 

The current subspace-distance is: 0.0001148729061242193 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 1.0; acc: 0.73
Batch: 60; loss: 1.25; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 1.16; acc: 0.75
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.12; acc: 0.77
Batch: 200; loss: 1.06; acc: 0.77
Batch: 220; loss: 1.16; acc: 0.73
Batch: 240; loss: 1.14; acc: 0.64
Batch: 260; loss: 1.15; acc: 0.72
Batch: 280; loss: 1.15; acc: 0.69
Batch: 300; loss: 1.09; acc: 0.78
Batch: 320; loss: 1.29; acc: 0.61
Batch: 340; loss: 1.18; acc: 0.75
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.05; acc: 0.78
Batch: 400; loss: 1.05; acc: 0.73
Batch: 420; loss: 1.05; acc: 0.75
Batch: 440; loss: 1.19; acc: 0.72
Batch: 460; loss: 1.14; acc: 0.67
Batch: 480; loss: 1.04; acc: 0.77
Batch: 500; loss: 1.05; acc: 0.73
Batch: 520; loss: 1.08; acc: 0.75
Batch: 540; loss: 1.18; acc: 0.64
Batch: 560; loss: 1.23; acc: 0.72
Batch: 580; loss: 1.01; acc: 0.81
Batch: 600; loss: 0.98; acc: 0.8
Batch: 620; loss: 1.1; acc: 0.77
Batch: 640; loss: 1.18; acc: 0.7
Batch: 660; loss: 1.2; acc: 0.69
Batch: 680; loss: 1.16; acc: 0.7
Batch: 700; loss: 1.13; acc: 0.69
Batch: 720; loss: 0.98; acc: 0.8
Batch: 740; loss: 0.9; acc: 0.84
Batch: 760; loss: 1.1; acc: 0.66
Batch: 780; loss: 1.0; acc: 0.77
Train Epoch over. train_loss: 1.1; train_accuracy: 0.74 

0.00013299009879119694
0.00012850538769271225
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.19; acc: 0.69
Batch: 40; loss: 0.7; acc: 0.92
Batch: 60; loss: 0.94; acc: 0.83
Batch: 80; loss: 0.97; acc: 0.84
Batch: 100; loss: 1.04; acc: 0.8
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.86
Val Epoch over. val_loss: 1.021355116822917; val_accuracy: 0.7693073248407644 

The current subspace-distance is: 0.00012850538769271225 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.05; acc: 0.75
Batch: 20; loss: 1.07; acc: 0.72
Batch: 40; loss: 1.13; acc: 0.72
Batch: 60; loss: 0.91; acc: 0.84
Batch: 80; loss: 1.02; acc: 0.78
Batch: 100; loss: 1.24; acc: 0.67
Batch: 120; loss: 0.93; acc: 0.84
Batch: 140; loss: 1.02; acc: 0.81
Batch: 160; loss: 0.98; acc: 0.8
Batch: 180; loss: 1.09; acc: 0.73
Batch: 200; loss: 1.03; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.78
Batch: 240; loss: 1.07; acc: 0.73
Batch: 260; loss: 0.87; acc: 0.88
Batch: 280; loss: 1.02; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.69
Batch: 320; loss: 1.13; acc: 0.72
Batch: 340; loss: 1.07; acc: 0.7
Batch: 360; loss: 1.05; acc: 0.75
Batch: 380; loss: 1.0; acc: 0.75
Batch: 400; loss: 0.99; acc: 0.78
Batch: 420; loss: 0.97; acc: 0.8
Batch: 440; loss: 1.04; acc: 0.77
Batch: 460; loss: 1.02; acc: 0.72
Batch: 480; loss: 0.89; acc: 0.81
Batch: 500; loss: 1.18; acc: 0.72
Batch: 520; loss: 1.1; acc: 0.72
Batch: 540; loss: 1.14; acc: 0.73
Batch: 560; loss: 1.03; acc: 0.78
Batch: 580; loss: 0.92; acc: 0.86
Batch: 600; loss: 1.01; acc: 0.8
Batch: 620; loss: 0.94; acc: 0.75
Batch: 640; loss: 0.95; acc: 0.77
Batch: 660; loss: 1.12; acc: 0.73
Batch: 680; loss: 0.92; acc: 0.81
Batch: 700; loss: 0.98; acc: 0.78
Batch: 720; loss: 0.99; acc: 0.78
Batch: 740; loss: 1.08; acc: 0.69
Batch: 760; loss: 1.07; acc: 0.75
Batch: 780; loss: 0.93; acc: 0.81
Train Epoch over. train_loss: 1.05; train_accuracy: 0.75 

0.00014502214617095888
0.00014117066166363657
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.67; acc: 0.92
Batch: 60; loss: 0.91; acc: 0.84
Batch: 80; loss: 0.9; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.8
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 0.78; acc: 0.86
Val Epoch over. val_loss: 0.9736638809465299; val_accuracy: 0.7809514331210191 

The current subspace-distance is: 0.00014117066166363657 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.23; acc: 0.69
Batch: 40; loss: 0.99; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.73
Batch: 100; loss: 0.93; acc: 0.78
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.91; acc: 0.84
Batch: 160; loss: 1.1; acc: 0.73
Batch: 180; loss: 0.96; acc: 0.73
Batch: 200; loss: 0.95; acc: 0.81
Batch: 220; loss: 0.87; acc: 0.84
Batch: 240; loss: 1.0; acc: 0.73
Batch: 260; loss: 0.9; acc: 0.78
Batch: 280; loss: 0.93; acc: 0.77
Batch: 300; loss: 0.99; acc: 0.73
Batch: 320; loss: 1.02; acc: 0.72
Batch: 340; loss: 1.15; acc: 0.64
Batch: 360; loss: 0.93; acc: 0.81
Batch: 380; loss: 0.89; acc: 0.86
Batch: 400; loss: 1.08; acc: 0.73
Batch: 420; loss: 1.1; acc: 0.67
Batch: 440; loss: 0.97; acc: 0.83
Batch: 460; loss: 1.04; acc: 0.67
Batch: 480; loss: 0.97; acc: 0.77
Batch: 500; loss: 0.98; acc: 0.75
Batch: 520; loss: 0.98; acc: 0.73
Batch: 540; loss: 1.06; acc: 0.78
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 1.02; acc: 0.75
Batch: 600; loss: 1.07; acc: 0.73
Batch: 620; loss: 1.13; acc: 0.7
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 0.99; acc: 0.77
Batch: 680; loss: 0.87; acc: 0.84
Batch: 700; loss: 0.9; acc: 0.81
Batch: 720; loss: 1.13; acc: 0.73
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 1.18; acc: 0.7
Batch: 780; loss: 0.91; acc: 0.78
Train Epoch over. train_loss: 1.01; train_accuracy: 0.76 

0.0001581101241754368
0.00015319469093810767
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.1; acc: 0.72
Batch: 40; loss: 0.66; acc: 0.91
Batch: 60; loss: 0.89; acc: 0.83
Batch: 80; loss: 0.87; acc: 0.84
Batch: 100; loss: 0.95; acc: 0.78
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 0.77; acc: 0.88
Val Epoch over. val_loss: 0.9482337665406002; val_accuracy: 0.7857285031847133 

The current subspace-distance is: 0.00015319469093810767 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.84
Batch: 20; loss: 1.06; acc: 0.7
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 1.0; acc: 0.77
Batch: 80; loss: 1.0; acc: 0.78
Batch: 100; loss: 0.93; acc: 0.81
Batch: 120; loss: 0.86; acc: 0.86
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 0.93; acc: 0.83
Batch: 180; loss: 1.1; acc: 0.7
Batch: 200; loss: 0.92; acc: 0.77
Batch: 220; loss: 0.98; acc: 0.78
Batch: 240; loss: 0.91; acc: 0.77
Batch: 260; loss: 0.96; acc: 0.72
Batch: 280; loss: 0.88; acc: 0.86
Batch: 300; loss: 1.1; acc: 0.7
Batch: 320; loss: 1.17; acc: 0.69
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 0.88; acc: 0.84
Batch: 380; loss: 1.0; acc: 0.73
Batch: 400; loss: 1.16; acc: 0.59
Batch: 420; loss: 0.89; acc: 0.81
Batch: 440; loss: 0.96; acc: 0.78
Batch: 460; loss: 0.88; acc: 0.81
Batch: 480; loss: 0.92; acc: 0.78
Batch: 500; loss: 1.0; acc: 0.83
Batch: 520; loss: 0.96; acc: 0.75
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 0.91; acc: 0.8
Batch: 580; loss: 0.91; acc: 0.75
Batch: 600; loss: 0.89; acc: 0.83
Batch: 620; loss: 0.79; acc: 0.83
Batch: 640; loss: 0.82; acc: 0.83
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 0.91; acc: 0.88
Batch: 700; loss: 0.94; acc: 0.78
Batch: 720; loss: 0.96; acc: 0.78
Batch: 740; loss: 0.9; acc: 0.78
Batch: 760; loss: 0.91; acc: 0.86
Batch: 780; loss: 0.93; acc: 0.81
Train Epoch over. train_loss: 0.97; train_accuracy: 0.77 

0.0001710731885395944
0.00016626993601676077
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 1.03; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.89
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.77; acc: 0.86
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 1.18; acc: 0.7
Batch: 140; loss: 0.73; acc: 0.88
Val Epoch over. val_loss: 0.8921745275236239; val_accuracy: 0.7994625796178344 

The current subspace-distance is: 0.00016626993601676077 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.91
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.98; acc: 0.77
Batch: 60; loss: 0.88; acc: 0.81
Batch: 80; loss: 0.88; acc: 0.78
Batch: 100; loss: 0.81; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.83
Batch: 140; loss: 1.01; acc: 0.77
Batch: 160; loss: 0.97; acc: 0.72
Batch: 180; loss: 0.89; acc: 0.84
Batch: 200; loss: 0.87; acc: 0.84
Batch: 220; loss: 0.81; acc: 0.86
Batch: 240; loss: 0.92; acc: 0.78
Batch: 260; loss: 1.01; acc: 0.75
Batch: 280; loss: 1.06; acc: 0.67
Batch: 300; loss: 0.93; acc: 0.78
Batch: 320; loss: 0.95; acc: 0.77
Batch: 340; loss: 0.96; acc: 0.77
Batch: 360; loss: 1.07; acc: 0.67
Batch: 380; loss: 0.9; acc: 0.83
Batch: 400; loss: 0.94; acc: 0.84
Batch: 420; loss: 0.88; acc: 0.78
Batch: 440; loss: 0.91; acc: 0.8
Batch: 460; loss: 1.01; acc: 0.75
Batch: 480; loss: 0.87; acc: 0.84
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.83; acc: 0.84
Batch: 540; loss: 1.1; acc: 0.67
Batch: 560; loss: 0.89; acc: 0.81
Batch: 580; loss: 0.91; acc: 0.77
Batch: 600; loss: 0.91; acc: 0.77
Batch: 620; loss: 0.93; acc: 0.77
Batch: 640; loss: 0.92; acc: 0.78
Batch: 660; loss: 0.91; acc: 0.81
Batch: 680; loss: 0.88; acc: 0.8
Batch: 700; loss: 0.91; acc: 0.75
Batch: 720; loss: 1.02; acc: 0.8
Batch: 740; loss: 0.95; acc: 0.77
Batch: 760; loss: 0.93; acc: 0.77
Batch: 780; loss: 0.85; acc: 0.81
Train Epoch over. train_loss: 0.93; train_accuracy: 0.78 

0.00018725998234003782
0.00018105354683939368
Batch: 0; loss: 0.86; acc: 0.86
Batch: 20; loss: 0.97; acc: 0.8
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.81; acc: 0.83
Batch: 80; loss: 0.69; acc: 0.89
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 0.69; acc: 0.88
Val Epoch over. val_loss: 0.8480028639173811; val_accuracy: 0.8115047770700637 

The current subspace-distance is: 0.00018105354683939368 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.89
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 0.88; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.81
Batch: 100; loss: 0.86; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.91; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.88
Batch: 180; loss: 0.87; acc: 0.78
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.84; acc: 0.8
Batch: 240; loss: 0.91; acc: 0.77
Batch: 260; loss: 0.78; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.91
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.94; acc: 0.75
Batch: 360; loss: 0.84; acc: 0.81
Batch: 380; loss: 0.79; acc: 0.77
Batch: 400; loss: 0.83; acc: 0.8
Batch: 420; loss: 0.82; acc: 0.84
Batch: 440; loss: 0.87; acc: 0.84
Batch: 460; loss: 0.9; acc: 0.81
Batch: 480; loss: 0.95; acc: 0.77
Batch: 500; loss: 0.73; acc: 0.89
Batch: 520; loss: 0.97; acc: 0.73
Batch: 540; loss: 0.79; acc: 0.88
Batch: 560; loss: 0.91; acc: 0.78
Batch: 580; loss: 0.93; acc: 0.72
Batch: 600; loss: 0.97; acc: 0.7
Batch: 620; loss: 0.85; acc: 0.84
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.78; acc: 0.83
Batch: 680; loss: 0.78; acc: 0.84
Batch: 700; loss: 0.9; acc: 0.75
Batch: 720; loss: 0.85; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.86
Batch: 760; loss: 0.86; acc: 0.8
Batch: 780; loss: 0.67; acc: 0.89
Train Epoch over. train_loss: 0.88; train_accuracy: 0.8 

0.00019855999562423676
0.00019256299128755927
Batch: 0; loss: 0.78; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.89
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 1.1; acc: 0.7
Batch: 140; loss: 0.65; acc: 0.91
Val Epoch over. val_loss: 0.7971680338974971; val_accuracy: 0.8211584394904459 

The current subspace-distance is: 0.00019256299128755927 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.94; acc: 0.78
Batch: 60; loss: 1.03; acc: 0.72
Batch: 80; loss: 0.87; acc: 0.75
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 0.76; acc: 0.84
Batch: 140; loss: 0.89; acc: 0.75
Batch: 160; loss: 0.87; acc: 0.78
Batch: 180; loss: 0.91; acc: 0.72
Batch: 200; loss: 0.8; acc: 0.86
Batch: 220; loss: 0.94; acc: 0.75
Batch: 240; loss: 0.97; acc: 0.73
Batch: 260; loss: 0.98; acc: 0.72
Batch: 280; loss: 0.93; acc: 0.77
Batch: 300; loss: 0.74; acc: 0.83
Batch: 320; loss: 0.89; acc: 0.7
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.89; acc: 0.8
Batch: 380; loss: 0.8; acc: 0.8
Batch: 400; loss: 1.14; acc: 0.66
Batch: 420; loss: 0.8; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.88
Batch: 460; loss: 0.87; acc: 0.78
Batch: 480; loss: 0.87; acc: 0.7
Batch: 500; loss: 0.83; acc: 0.8
Batch: 520; loss: 0.92; acc: 0.75
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.93; acc: 0.77
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.76; acc: 0.86
Batch: 620; loss: 0.9; acc: 0.72
Batch: 640; loss: 0.81; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.84
Batch: 680; loss: 0.82; acc: 0.81
Batch: 700; loss: 0.92; acc: 0.69
Batch: 720; loss: 0.87; acc: 0.77
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.95
Batch: 780; loss: 0.77; acc: 0.86
Train Epoch over. train_loss: 0.84; train_accuracy: 0.8 

0.00020157641847617924
0.0001946378906723112
Batch: 0; loss: 0.75; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.62; acc: 0.92
Val Epoch over. val_loss: 0.7780775788483346; val_accuracy: 0.8271297770700637 

The current subspace-distance is: 0.0001946378906723112 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.96; acc: 0.78
Batch: 60; loss: 0.91; acc: 0.75
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 1.02; acc: 0.69
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.8; acc: 0.75
Batch: 160; loss: 0.79; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.84; acc: 0.78
Batch: 240; loss: 0.89; acc: 0.75
Batch: 260; loss: 0.84; acc: 0.73
Batch: 280; loss: 0.76; acc: 0.84
Batch: 300; loss: 0.83; acc: 0.83
Batch: 320; loss: 0.75; acc: 0.88
Batch: 340; loss: 0.89; acc: 0.78
Batch: 360; loss: 0.88; acc: 0.75
Batch: 380; loss: 0.88; acc: 0.8
Batch: 400; loss: 0.96; acc: 0.77
Batch: 420; loss: 0.84; acc: 0.8
Batch: 440; loss: 0.96; acc: 0.73
Batch: 460; loss: 0.74; acc: 0.83
Batch: 480; loss: 0.94; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.84
Batch: 520; loss: 0.76; acc: 0.8
Batch: 540; loss: 0.95; acc: 0.73
Batch: 560; loss: 0.8; acc: 0.86
Batch: 580; loss: 0.83; acc: 0.78
Batch: 600; loss: 0.85; acc: 0.8
Batch: 620; loss: 0.93; acc: 0.75
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.71; acc: 0.86
Batch: 680; loss: 0.79; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.89
Batch: 720; loss: 0.79; acc: 0.8
Batch: 740; loss: 0.85; acc: 0.73
Batch: 760; loss: 0.85; acc: 0.75
Batch: 780; loss: 0.83; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

0.00020523667626548558
0.00019966662512160838
Batch: 0; loss: 0.73; acc: 0.88
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.83
Batch: 120; loss: 1.08; acc: 0.7
Batch: 140; loss: 0.6; acc: 0.91
Val Epoch over. val_loss: 0.7586951637343996; val_accuracy: 0.8277269108280255 

The current subspace-distance is: 0.00019966662512160838 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.86; acc: 0.84
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 1.04; acc: 0.72
Batch: 60; loss: 1.03; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.75
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.83
Batch: 140; loss: 0.67; acc: 0.88
Batch: 160; loss: 0.88; acc: 0.75
Batch: 180; loss: 0.81; acc: 0.84
Batch: 200; loss: 0.96; acc: 0.75
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.78; acc: 0.78
Batch: 260; loss: 0.7; acc: 0.88
Batch: 280; loss: 0.97; acc: 0.73
Batch: 300; loss: 0.82; acc: 0.83
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.77
Batch: 360; loss: 0.99; acc: 0.75
Batch: 380; loss: 0.73; acc: 0.84
Batch: 400; loss: 0.81; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.81
Batch: 480; loss: 0.96; acc: 0.72
Batch: 500; loss: 0.77; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 1.01; acc: 0.75
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 0.92; acc: 0.77
Batch: 600; loss: 1.09; acc: 0.67
Batch: 620; loss: 0.71; acc: 0.88
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.79; acc: 0.78
Batch: 680; loss: 0.77; acc: 0.78
Batch: 700; loss: 0.76; acc: 0.84
Batch: 720; loss: 0.77; acc: 0.81
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.83; acc: 0.84
Batch: 780; loss: 0.77; acc: 0.83
Train Epoch over. train_loss: 0.81; train_accuracy: 0.81 

0.00021139821910765022
0.000205396834644489
Batch: 0; loss: 0.72; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.83
Batch: 120; loss: 1.08; acc: 0.7
Batch: 140; loss: 0.6; acc: 0.91
Val Epoch over. val_loss: 0.7561681794512803; val_accuracy: 0.8267316878980892 

The current subspace-distance is: 0.000205396834644489 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.86; acc: 0.77
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.92; acc: 0.73
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.86
Batch: 160; loss: 0.91; acc: 0.77
Batch: 180; loss: 0.74; acc: 0.83
Batch: 200; loss: 0.79; acc: 0.8
Batch: 220; loss: 0.84; acc: 0.77
Batch: 240; loss: 0.76; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.81
Batch: 280; loss: 0.81; acc: 0.86
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.78; acc: 0.84
Batch: 340; loss: 0.85; acc: 0.77
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.86
Batch: 400; loss: 0.72; acc: 0.88
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.93; acc: 0.78
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.84; acc: 0.77
Batch: 580; loss: 0.82; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.86
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.83; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.86
Batch: 700; loss: 0.7; acc: 0.83
Batch: 720; loss: 0.71; acc: 0.86
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.84; acc: 0.81
Batch: 780; loss: 0.95; acc: 0.77
Train Epoch over. train_loss: 0.8; train_accuracy: 0.81 

0.00021593828569166362
0.00020796808530576527
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 0.59; acc: 0.92
Val Epoch over. val_loss: 0.74058695507657; val_accuracy: 0.8298168789808917 

The current subspace-distance is: 0.00020796808530576527 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.84
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 0.8; acc: 0.84
Batch: 140; loss: 0.94; acc: 0.77
Batch: 160; loss: 0.75; acc: 0.84
Batch: 180; loss: 0.92; acc: 0.72
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.81
Batch: 240; loss: 0.87; acc: 0.77
Batch: 260; loss: 0.83; acc: 0.77
Batch: 280; loss: 0.83; acc: 0.8
Batch: 300; loss: 0.78; acc: 0.83
Batch: 320; loss: 0.81; acc: 0.81
Batch: 340; loss: 0.77; acc: 0.84
Batch: 360; loss: 0.73; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.74; acc: 0.84
Batch: 440; loss: 0.88; acc: 0.8
Batch: 460; loss: 0.75; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.75; acc: 0.84
Batch: 540; loss: 0.84; acc: 0.8
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.75; acc: 0.77
Batch: 600; loss: 0.91; acc: 0.66
Batch: 620; loss: 0.84; acc: 0.8
Batch: 640; loss: 0.75; acc: 0.84
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 0.85; acc: 0.73
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.78; acc: 0.81
Train Epoch over. train_loss: 0.79; train_accuracy: 0.81 

0.0002156891132472083
0.00020978630345780402
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.77; acc: 0.84
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 0.59; acc: 0.92
Val Epoch over. val_loss: 0.7328062619373297; val_accuracy: 0.8305135350318471 

The current subspace-distance is: 0.00020978630345780402 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.87; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.74; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.91; acc: 0.78
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.86; acc: 0.8
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.86; acc: 0.81
Batch: 340; loss: 0.88; acc: 0.78
Batch: 360; loss: 0.68; acc: 0.86
Batch: 380; loss: 0.78; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.88
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.93; acc: 0.77
Batch: 480; loss: 0.86; acc: 0.8
Batch: 500; loss: 0.7; acc: 0.88
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.86
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.78; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.73
Batch: 620; loss: 0.64; acc: 0.94
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.88; acc: 0.75
Batch: 680; loss: 0.83; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.83; acc: 0.78
Batch: 760; loss: 0.81; acc: 0.77
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00022040891053620726
0.00021374088828451931
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.86
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 0.56; acc: 0.92
Val Epoch over. val_loss: 0.7187713969285321; val_accuracy: 0.832703025477707 

The current subspace-distance is: 0.00021374088828451931 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.71; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 0.62; acc: 0.91
Batch: 140; loss: 0.7; acc: 0.86
Batch: 160; loss: 0.75; acc: 0.84
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.78; acc: 0.8
Batch: 260; loss: 0.77; acc: 0.83
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.71; acc: 0.83
Batch: 340; loss: 0.78; acc: 0.83
Batch: 360; loss: 0.78; acc: 0.83
Batch: 380; loss: 0.57; acc: 0.92
Batch: 400; loss: 0.75; acc: 0.78
Batch: 420; loss: 0.73; acc: 0.8
Batch: 440; loss: 0.73; acc: 0.86
Batch: 460; loss: 0.77; acc: 0.86
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.67; acc: 0.84
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.91; acc: 0.73
Batch: 620; loss: 0.92; acc: 0.75
Batch: 640; loss: 0.82; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.79; acc: 0.8
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.82 

0.00022316028480418026
0.00021646730601787567
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.84
Batch: 120; loss: 1.03; acc: 0.7
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.7063782702965341; val_accuracy: 0.8363853503184714 

The current subspace-distance is: 0.00021646730601787567 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.69; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.86
Batch: 180; loss: 0.83; acc: 0.77
Batch: 200; loss: 0.83; acc: 0.81
Batch: 220; loss: 0.77; acc: 0.84
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.9; acc: 0.75
Batch: 300; loss: 0.6; acc: 0.91
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.91; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.88
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.83
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.84
Batch: 520; loss: 0.86; acc: 0.78
Batch: 540; loss: 0.69; acc: 0.84
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.86
Batch: 620; loss: 0.62; acc: 0.89
Batch: 640; loss: 0.85; acc: 0.78
Batch: 660; loss: 0.75; acc: 0.8
Batch: 680; loss: 0.84; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.86
Batch: 760; loss: 0.88; acc: 0.78
Batch: 780; loss: 0.84; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00022685303702019155
0.0002214729756815359
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6949572331586461; val_accuracy: 0.8369824840764332 

The current subspace-distance is: 0.0002214729756815359 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.88; acc: 0.73
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.8; acc: 0.75
Batch: 200; loss: 0.8; acc: 0.83
Batch: 220; loss: 0.63; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.87; acc: 0.78
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.78; acc: 0.8
Batch: 400; loss: 0.88; acc: 0.75
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.76; acc: 0.78
Batch: 460; loss: 0.86; acc: 0.75
Batch: 480; loss: 0.92; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.87; acc: 0.73
Batch: 620; loss: 0.71; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.67; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.92
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.85; acc: 0.8
Batch: 760; loss: 0.73; acc: 0.84
Batch: 780; loss: 0.78; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.0002306346141267568
0.0002247104566777125
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6768260805090521; val_accuracy: 0.8405652866242038 

The current subspace-distance is: 0.0002247104566777125 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.82; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.64; acc: 0.89
Batch: 140; loss: 0.81; acc: 0.8
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.91
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.94; acc: 0.75
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.78; acc: 0.78
Batch: 280; loss: 0.71; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.7; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.91
Batch: 360; loss: 0.74; acc: 0.8
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.73; acc: 0.8
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.78; acc: 0.8
Batch: 520; loss: 0.81; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.95; acc: 0.72
Batch: 600; loss: 0.84; acc: 0.81
Batch: 620; loss: 0.75; acc: 0.83
Batch: 640; loss: 0.76; acc: 0.77
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.82; acc: 0.81
Batch: 700; loss: 0.92; acc: 0.73
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.0002325673121958971
0.00022719298431184143
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6783637744226273; val_accuracy: 0.8415605095541401 

The current subspace-distance is: 0.00022719298431184143 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.88
Batch: 40; loss: 0.71; acc: 0.78
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.8; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.89
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.86; acc: 0.78
Batch: 220; loss: 0.86; acc: 0.72
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.88
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.84
Batch: 340; loss: 0.82; acc: 0.83
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.64; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.91
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.71; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.84
Batch: 500; loss: 0.78; acc: 0.8
Batch: 520; loss: 0.76; acc: 0.8
Batch: 540; loss: 0.86; acc: 0.78
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.78; acc: 0.83
Batch: 600; loss: 0.73; acc: 0.77
Batch: 620; loss: 0.78; acc: 0.83
Batch: 640; loss: 0.87; acc: 0.78
Batch: 660; loss: 0.68; acc: 0.88
Batch: 680; loss: 0.65; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.77
Batch: 720; loss: 0.59; acc: 0.88
Batch: 740; loss: 0.89; acc: 0.72
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.8; acc: 0.72
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.0002339496131753549
0.00022811637609265745
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.84
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 0.52; acc: 0.94
Val Epoch over. val_loss: 0.6830125993983761; val_accuracy: 0.8390724522292994 

The current subspace-distance is: 0.00022811637609265745 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.67
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.94
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.48; acc: 0.92
Batch: 260; loss: 0.88; acc: 0.8
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.8; acc: 0.78
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.73; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.92
Batch: 440; loss: 0.72; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 0.81; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.76; acc: 0.8
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.86
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.83; acc: 0.77
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00023925780260469764
0.0002303545770701021
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.6680930429583143; val_accuracy: 0.8433519108280255 

The current subspace-distance is: 0.0002303545770701021 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.8; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.76; acc: 0.78
Batch: 240; loss: 0.69; acc: 0.75
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.83; acc: 0.78
Batch: 300; loss: 0.78; acc: 0.78
Batch: 320; loss: 0.63; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.75
Batch: 380; loss: 0.81; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.91
Batch: 420; loss: 0.94; acc: 0.8
Batch: 440; loss: 0.61; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.69; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.91
Batch: 520; loss: 0.73; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.89
Batch: 560; loss: 0.87; acc: 0.69
Batch: 580; loss: 0.66; acc: 0.84
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.7; acc: 0.83
Batch: 720; loss: 0.55; acc: 0.92
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.88
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00023664832406211644
0.00022897466260474175
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.84
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6711939282857688; val_accuracy: 0.8440485668789809 

The current subspace-distance is: 0.00022897466260474175 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.87; acc: 0.77
Batch: 20; loss: 0.63; acc: 0.89
Batch: 40; loss: 0.75; acc: 0.77
Batch: 60; loss: 0.57; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.76; acc: 0.8
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.78; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.71; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.84
Batch: 300; loss: 0.6; acc: 0.91
Batch: 320; loss: 0.74; acc: 0.84
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.74; acc: 0.83
Batch: 420; loss: 0.83; acc: 0.78
Batch: 440; loss: 0.81; acc: 0.8
Batch: 460; loss: 0.88; acc: 0.78
Batch: 480; loss: 0.79; acc: 0.8
Batch: 500; loss: 0.56; acc: 0.91
Batch: 520; loss: 0.78; acc: 0.77
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.79; acc: 0.75
Batch: 780; loss: 0.76; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00023905588022898883
0.00022995292965788394
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6567910535700002; val_accuracy: 0.8451433121019108 

The current subspace-distance is: 0.00022995292965788394 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 0.85; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.91
Batch: 220; loss: 0.8; acc: 0.8
Batch: 240; loss: 0.73; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.82; acc: 0.77
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.82; acc: 0.8
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.89; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.9; acc: 0.75
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.86; acc: 0.77
Batch: 580; loss: 0.74; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.77; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.81
Batch: 660; loss: 0.59; acc: 0.91
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.86; acc: 0.78
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.91
Batch: 760; loss: 0.77; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00024044934252742678
0.00023233593674376607
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6522688746072685; val_accuracy: 0.845640923566879 

The current subspace-distance is: 0.00023233593674376607 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.73
Batch: 100; loss: 0.8; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.72
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.96; acc: 0.72
Batch: 180; loss: 0.75; acc: 0.81
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.79; acc: 0.84
Batch: 340; loss: 0.8; acc: 0.73
Batch: 360; loss: 0.65; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.84
Batch: 420; loss: 0.77; acc: 0.75
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.84; acc: 0.73
Batch: 480; loss: 0.92; acc: 0.73
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.91
Batch: 580; loss: 0.74; acc: 0.75
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.76; acc: 0.81
Batch: 640; loss: 0.73; acc: 0.81
Batch: 660; loss: 0.79; acc: 0.81
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.9; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.0002395765477558598
0.00023137734388001263
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 1.0; acc: 0.7
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6649783204315575; val_accuracy: 0.8411624203821656 

The current subspace-distance is: 0.00023137734388001263 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.91
Batch: 140; loss: 0.74; acc: 0.86
Batch: 160; loss: 0.82; acc: 0.81
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.81; acc: 0.8
Batch: 260; loss: 0.5; acc: 0.92
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.77
Batch: 320; loss: 0.75; acc: 0.75
Batch: 340; loss: 0.8; acc: 0.8
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.75; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.8; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.91
Batch: 480; loss: 0.8; acc: 0.77
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.95
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.66; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.78; acc: 0.8
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.86; acc: 0.73
Batch: 700; loss: 0.84; acc: 0.78
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.8; acc: 0.78
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00024005105660762638
0.00023350627452600747
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6447913018381519; val_accuracy: 0.848328025477707 

The current subspace-distance is: 0.00023350627452600747 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.91
Batch: 220; loss: 0.78; acc: 0.73
Batch: 240; loss: 0.93; acc: 0.7
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.65; acc: 0.88
Batch: 400; loss: 0.8; acc: 0.77
Batch: 420; loss: 0.95; acc: 0.72
Batch: 440; loss: 0.83; acc: 0.77
Batch: 460; loss: 0.67; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.8; acc: 0.81
Batch: 520; loss: 0.71; acc: 0.77
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.84; acc: 0.73
Batch: 600; loss: 0.64; acc: 0.91
Batch: 620; loss: 0.75; acc: 0.81
Batch: 640; loss: 0.62; acc: 0.89
Batch: 660; loss: 0.76; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.95
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00024068725178949535
0.00023262832837644964
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.6464841367712446; val_accuracy: 0.8482285031847133 

The current subspace-distance is: 0.00023262832837644964 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.91; acc: 0.73
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.86
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.75
Batch: 320; loss: 0.77; acc: 0.81
Batch: 340; loss: 0.66; acc: 0.89
Batch: 360; loss: 0.91; acc: 0.66
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.89
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.76; acc: 0.81
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.76; acc: 0.81
Batch: 660; loss: 0.72; acc: 0.81
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.74; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00024244184896815568
0.00023488527222070843
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.6509620256864341; val_accuracy: 0.8481289808917197 

The current subspace-distance is: 0.00023488527222070843 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.81
Batch: 160; loss: 0.83; acc: 0.78
Batch: 180; loss: 0.68; acc: 0.89
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.7; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.89; acc: 0.73
Batch: 300; loss: 0.74; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.65; acc: 0.88
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.72; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.8; acc: 0.78
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.72; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.89
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.95
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00024137747823260725
0.000234692168305628
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.95
Val Epoch over. val_loss: 0.6547608688758437; val_accuracy: 0.8470342356687898 

The current subspace-distance is: 0.000234692168305628 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_15_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.03

The number of parameters is: 284685

The number of individual parameters is:

9
162
9
9
13
35802
13
13
25
99450
25
25
64
144000
64
64
4096
64
640
10
64
64

nonzero elements in E: 85405493
elements in E: 85405500
fraction nonzero: 0.9999999180380654
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.49; acc: 0.08
Batch: 20; loss: 2.1; acc: 0.3
Batch: 40; loss: 1.9; acc: 0.48
Batch: 60; loss: 1.86; acc: 0.5
Batch: 80; loss: 1.85; acc: 0.39
Batch: 100; loss: 1.77; acc: 0.52
Batch: 120; loss: 1.62; acc: 0.61
Batch: 140; loss: 1.52; acc: 0.69
Batch: 160; loss: 1.65; acc: 0.58
Batch: 180; loss: 1.59; acc: 0.67
Batch: 200; loss: 1.57; acc: 0.66
Batch: 220; loss: 1.53; acc: 0.67
Batch: 240; loss: 1.4; acc: 0.73
Batch: 260; loss: 1.51; acc: 0.64
Batch: 280; loss: 1.47; acc: 0.62
Batch: 300; loss: 1.37; acc: 0.73
Batch: 320; loss: 1.55; acc: 0.66
Batch: 340; loss: 1.49; acc: 0.66
Batch: 360; loss: 1.41; acc: 0.69
Batch: 380; loss: 1.42; acc: 0.69
Batch: 400; loss: 1.29; acc: 0.83
Batch: 420; loss: 1.35; acc: 0.77
Batch: 440; loss: 1.45; acc: 0.7
Batch: 460; loss: 1.44; acc: 0.64
Batch: 480; loss: 1.24; acc: 0.88
Batch: 500; loss: 1.22; acc: 0.81
Batch: 520; loss: 1.18; acc: 0.81
Batch: 540; loss: 1.32; acc: 0.8
Batch: 560; loss: 1.29; acc: 0.73
Batch: 580; loss: 1.24; acc: 0.8
Batch: 600; loss: 1.28; acc: 0.78
Batch: 620; loss: 1.3; acc: 0.75
Batch: 640; loss: 1.34; acc: 0.7
Batch: 660; loss: 1.29; acc: 0.75
Batch: 680; loss: 1.26; acc: 0.8
Batch: 700; loss: 1.17; acc: 0.86
Batch: 720; loss: 1.28; acc: 0.69
Batch: 740; loss: 1.33; acc: 0.64
Batch: 760; loss: 1.24; acc: 0.73
Batch: 780; loss: 1.27; acc: 0.69
Train Epoch over. train_loss: 1.46; train_accuracy: 0.67 

6.538753223139793e-05
6.0345566453179345e-05
Batch: 0; loss: 1.29; acc: 0.75
Batch: 20; loss: 1.36; acc: 0.62
Batch: 40; loss: 0.97; acc: 0.92
Batch: 60; loss: 1.17; acc: 0.8
Batch: 80; loss: 1.07; acc: 0.83
Batch: 100; loss: 1.18; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.69
Batch: 140; loss: 1.1; acc: 0.83
Val Epoch over. val_loss: 1.1880808092985944; val_accuracy: 0.7810509554140127 

The current subspace-distance is: 6.0345566453179345e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.84
Batch: 20; loss: 1.27; acc: 0.77
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.83
Batch: 80; loss: 1.17; acc: 0.78
Batch: 100; loss: 1.22; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.88
Batch: 160; loss: 1.11; acc: 0.81
Batch: 180; loss: 1.25; acc: 0.69
Batch: 200; loss: 1.26; acc: 0.64
Batch: 220; loss: 1.26; acc: 0.73
Batch: 240; loss: 1.19; acc: 0.73
Batch: 260; loss: 1.13; acc: 0.77
Batch: 280; loss: 1.22; acc: 0.73
Batch: 300; loss: 1.18; acc: 0.81
Batch: 320; loss: 1.1; acc: 0.73
Batch: 340; loss: 1.29; acc: 0.73
Batch: 360; loss: 1.12; acc: 0.77
Batch: 380; loss: 1.08; acc: 0.8
Batch: 400; loss: 1.27; acc: 0.72
Batch: 420; loss: 1.35; acc: 0.66
Batch: 440; loss: 1.21; acc: 0.75
Batch: 460; loss: 1.16; acc: 0.78
Batch: 480; loss: 1.13; acc: 0.73
Batch: 500; loss: 1.14; acc: 0.8
Batch: 520; loss: 1.07; acc: 0.8
Batch: 540; loss: 1.12; acc: 0.78
Batch: 560; loss: 1.09; acc: 0.8
Batch: 580; loss: 1.13; acc: 0.8
Batch: 600; loss: 1.17; acc: 0.72
Batch: 620; loss: 0.99; acc: 0.83
Batch: 640; loss: 1.11; acc: 0.73
Batch: 660; loss: 0.97; acc: 0.84
Batch: 680; loss: 0.96; acc: 0.88
Batch: 700; loss: 1.13; acc: 0.77
Batch: 720; loss: 1.02; acc: 0.81
Batch: 740; loss: 1.09; acc: 0.8
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 0.98; acc: 0.86
Train Epoch over. train_loss: 1.14; train_accuracy: 0.77 

8.977379911812022e-05
8.593283564550802e-05
Batch: 0; loss: 1.07; acc: 0.81
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 0.73; acc: 0.95
Batch: 60; loss: 1.01; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.92
Batch: 100; loss: 1.04; acc: 0.83
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 0.9; acc: 0.88
Val Epoch over. val_loss: 1.0038182898691506; val_accuracy: 0.8168789808917197 

The current subspace-distance is: 8.593283564550802e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.05; acc: 0.78
Batch: 20; loss: 1.04; acc: 0.8
Batch: 40; loss: 0.99; acc: 0.78
Batch: 60; loss: 0.99; acc: 0.86
Batch: 80; loss: 1.13; acc: 0.75
Batch: 100; loss: 1.06; acc: 0.8
Batch: 120; loss: 1.07; acc: 0.78
Batch: 140; loss: 1.11; acc: 0.73
Batch: 160; loss: 1.13; acc: 0.7
Batch: 180; loss: 1.01; acc: 0.84
Batch: 200; loss: 1.03; acc: 0.75
Batch: 220; loss: 1.05; acc: 0.78
Batch: 240; loss: 1.04; acc: 0.78
Batch: 260; loss: 0.93; acc: 0.84
Batch: 280; loss: 0.96; acc: 0.83
Batch: 300; loss: 0.99; acc: 0.8
Batch: 320; loss: 0.93; acc: 0.88
Batch: 340; loss: 1.0; acc: 0.8
Batch: 360; loss: 1.08; acc: 0.77
Batch: 380; loss: 1.2; acc: 0.67
Batch: 400; loss: 0.79; acc: 0.89
Batch: 420; loss: 1.01; acc: 0.8
Batch: 440; loss: 0.95; acc: 0.81
Batch: 460; loss: 0.91; acc: 0.84
Batch: 480; loss: 0.96; acc: 0.81
Batch: 500; loss: 0.98; acc: 0.84
Batch: 520; loss: 0.88; acc: 0.84
Batch: 540; loss: 1.13; acc: 0.72
Batch: 560; loss: 0.96; acc: 0.81
Batch: 580; loss: 0.88; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.88
Batch: 620; loss: 0.98; acc: 0.78
Batch: 640; loss: 0.94; acc: 0.81
Batch: 660; loss: 0.94; acc: 0.8
Batch: 680; loss: 1.16; acc: 0.67
Batch: 700; loss: 0.86; acc: 0.89
Batch: 720; loss: 0.91; acc: 0.78
Batch: 740; loss: 0.93; acc: 0.81
Batch: 760; loss: 0.83; acc: 0.89
Batch: 780; loss: 0.93; acc: 0.78
Train Epoch over. train_loss: 0.98; train_accuracy: 0.8 

0.00010925494279945269
0.00010430758266011253
Batch: 0; loss: 0.93; acc: 0.8
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 0.59; acc: 0.95
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.74; acc: 0.94
Batch: 100; loss: 0.88; acc: 0.89
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.73; acc: 0.92
Val Epoch over. val_loss: 0.8708583184867907; val_accuracy: 0.8367834394904459 

The current subspace-distance is: 0.00010430758266011253 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.75
Batch: 20; loss: 0.8; acc: 0.86
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 0.84; acc: 0.86
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.93; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.81
Batch: 140; loss: 0.93; acc: 0.8
Batch: 160; loss: 0.94; acc: 0.78
Batch: 180; loss: 0.92; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.81
Batch: 220; loss: 0.82; acc: 0.83
Batch: 240; loss: 1.01; acc: 0.73
Batch: 260; loss: 0.96; acc: 0.8
Batch: 280; loss: 1.04; acc: 0.8
Batch: 300; loss: 1.0; acc: 0.83
Batch: 320; loss: 0.74; acc: 0.92
Batch: 340; loss: 0.94; acc: 0.75
Batch: 360; loss: 0.9; acc: 0.84
Batch: 380; loss: 0.86; acc: 0.83
Batch: 400; loss: 0.89; acc: 0.8
Batch: 420; loss: 0.86; acc: 0.83
Batch: 440; loss: 0.88; acc: 0.77
Batch: 460; loss: 0.9; acc: 0.81
Batch: 480; loss: 0.87; acc: 0.83
Batch: 500; loss: 0.92; acc: 0.73
Batch: 520; loss: 0.94; acc: 0.81
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.79; acc: 0.89
Batch: 580; loss: 0.89; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.91
Batch: 620; loss: 0.91; acc: 0.78
Batch: 640; loss: 0.87; acc: 0.83
Batch: 660; loss: 0.86; acc: 0.78
Batch: 680; loss: 0.76; acc: 0.84
Batch: 700; loss: 0.77; acc: 0.83
Batch: 720; loss: 0.81; acc: 0.86
Batch: 740; loss: 0.99; acc: 0.7
Batch: 760; loss: 0.72; acc: 0.89
Batch: 780; loss: 0.68; acc: 0.89
Train Epoch over. train_loss: 0.87; train_accuracy: 0.82 

0.0001272696681553498
0.00012266905105207115
Batch: 0; loss: 0.83; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.51; acc: 0.97
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.94
Batch: 100; loss: 0.77; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.62; acc: 0.94
Val Epoch over. val_loss: 0.7791128614146239; val_accuracy: 0.8552945859872612 

The current subspace-distance is: 0.00012266905105207115 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 0.82; acc: 0.81
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.86
Batch: 140; loss: 0.69; acc: 0.91
Batch: 160; loss: 0.8; acc: 0.86
Batch: 180; loss: 0.79; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.91
Batch: 220; loss: 0.82; acc: 0.78
Batch: 240; loss: 0.88; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.86
Batch: 280; loss: 0.91; acc: 0.81
Batch: 300; loss: 0.77; acc: 0.86
Batch: 320; loss: 0.92; acc: 0.75
Batch: 340; loss: 1.0; acc: 0.69
Batch: 360; loss: 0.66; acc: 0.89
Batch: 380; loss: 0.83; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.85; acc: 0.86
Batch: 440; loss: 0.83; acc: 0.89
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.83; acc: 0.84
Batch: 500; loss: 0.79; acc: 0.86
Batch: 520; loss: 0.97; acc: 0.77
Batch: 540; loss: 0.94; acc: 0.78
Batch: 560; loss: 0.77; acc: 0.86
Batch: 580; loss: 0.94; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.84
Batch: 620; loss: 0.76; acc: 0.88
Batch: 640; loss: 0.79; acc: 0.73
Batch: 660; loss: 0.79; acc: 0.78
Batch: 680; loss: 0.82; acc: 0.75
Batch: 700; loss: 0.89; acc: 0.75
Batch: 720; loss: 0.65; acc: 0.86
Batch: 740; loss: 0.73; acc: 0.88
Batch: 760; loss: 0.7; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.86
Train Epoch over. train_loss: 0.8; train_accuracy: 0.83 

0.0001412189012626186
0.0001348662335658446
Batch: 0; loss: 0.75; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.95
Batch: 100; loss: 0.69; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.94
Val Epoch over. val_loss: 0.7113479432786346; val_accuracy: 0.8634554140127388 

The current subspace-distance is: 0.0001348662335658446 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.83; acc: 0.81
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.86
Batch: 160; loss: 0.89; acc: 0.81
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.88
Batch: 260; loss: 0.86; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.89
Batch: 300; loss: 0.81; acc: 0.83
Batch: 320; loss: 0.85; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.91
Batch: 360; loss: 0.67; acc: 0.88
Batch: 380; loss: 0.64; acc: 0.91
Batch: 400; loss: 0.74; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.91
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.7; acc: 0.84
Batch: 500; loss: 0.82; acc: 0.81
Batch: 520; loss: 0.74; acc: 0.83
Batch: 540; loss: 0.81; acc: 0.84
Batch: 560; loss: 0.8; acc: 0.84
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.72; acc: 0.91
Batch: 620; loss: 0.74; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.89
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.88
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.83; acc: 0.84
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.74; train_accuracy: 0.84 

0.00015639762568753213
0.0001515827316325158
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.95
Val Epoch over. val_loss: 0.6596200525001356; val_accuracy: 0.8698248407643312 

The current subspace-distance is: 0.0001515827316325158 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.92
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.84
Batch: 120; loss: 0.57; acc: 0.94
Batch: 140; loss: 0.7; acc: 0.89
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 0.79; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.88
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.51; acc: 0.95
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.71; acc: 0.83
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.8; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.91
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.66; acc: 0.88
Batch: 520; loss: 0.7; acc: 0.88
Batch: 540; loss: 0.78; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.86
Batch: 580; loss: 0.66; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.74; acc: 0.86
Batch: 640; loss: 0.84; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.88
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.85 

0.00016950172721408308
0.00016348596545867622
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.97
Val Epoch over. val_loss: 0.6215143463793834; val_accuracy: 0.8765923566878981 

The current subspace-distance is: 0.00016348596545867622 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.88
Batch: 140; loss: 0.75; acc: 0.84
Batch: 160; loss: 0.74; acc: 0.77
Batch: 180; loss: 0.73; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.88
Batch: 220; loss: 0.77; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.92
Batch: 360; loss: 0.81; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.84
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.91
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.73; acc: 0.78
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.54; acc: 0.91
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.73; acc: 0.81
Batch: 660; loss: 0.76; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.95
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.94
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.85 

0.00018113641999661922
0.00017303941422142088
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.98
Val Epoch over. val_loss: 0.5851917128274395; val_accuracy: 0.8820660828025477 

The current subspace-distance is: 0.00017303941422142088 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.58; acc: 0.94
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.77; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.89
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.92
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.91
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.69; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.63; train_accuracy: 0.86 

0.00019092705042567104
0.00018484740576241165
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.98
Val Epoch over. val_loss: 0.5581478003863316; val_accuracy: 0.8826632165605095 

The current subspace-distance is: 0.00018484740576241165 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.43; acc: 0.98
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.73; acc: 0.83
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.89
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.92
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.62; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.00020110525656491518
0.00019502380746416748
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.97
Val Epoch over. val_loss: 0.5295244068097157; val_accuracy: 0.8863455414012739 

The current subspace-distance is: 0.00019502380746416748 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.95
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.78; acc: 0.8
Batch: 440; loss: 0.54; acc: 0.92
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.66; acc: 0.77
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.94
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.91
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.0002043918939307332
0.00019680237164720893
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.98
Val Epoch over. val_loss: 0.5238876561070703; val_accuracy: 0.8888335987261147 

The current subspace-distance is: 0.00019680237164720893 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.73; acc: 0.77
Batch: 180; loss: 0.81; acc: 0.83
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.76; acc: 0.83
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.89
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.94
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.0002068336179945618
0.000198688154341653
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.97
Val Epoch over. val_loss: 0.5218133981440477; val_accuracy: 0.8873407643312102 

The current subspace-distance is: 0.000198688154341653 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.94
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.78
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.87; acc: 0.73
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00021064204338472337
0.0002044903812929988
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.5119950182878288; val_accuracy: 0.8873407643312102 

The current subspace-distance is: 0.0002044903812929988 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.77
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.72; acc: 0.78
Batch: 240; loss: 0.39; acc: 0.97
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.55; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.94
Batch: 540; loss: 0.55; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.0002099205885315314
0.00020470228628255427
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.502047917835272; val_accuracy: 0.8902269108280255 

The current subspace-distance is: 0.00020470228628255427 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.47; acc: 0.92
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.94
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.91
Batch: 520; loss: 0.73; acc: 0.7
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.76; acc: 0.8
Batch: 620; loss: 0.51; acc: 0.92
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00021524008479900658
0.0002085052547045052
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.5032477660733423; val_accuracy: 0.8918192675159236 

The current subspace-distance is: 0.0002085052547045052 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.94
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.68; acc: 0.8
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.66; acc: 0.8
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.66; acc: 0.77
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.95
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0002187314530601725
0.00021276499319355935
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.95
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.4910660848306243; val_accuracy: 0.8929140127388535 

The current subspace-distance is: 0.00021276499319355935 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.42; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.94
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.66; acc: 0.78
Batch: 720; loss: 0.41; acc: 0.95
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.95
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00022069510305300355
0.0002146874030586332
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.95
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.4853361282189181; val_accuracy: 0.8953025477707006 

The current subspace-distance is: 0.0002146874030586332 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.78
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.67; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.6; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00022298572002910078
0.00021571735851466656
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.4822832711373165; val_accuracy: 0.8946058917197452 

The current subspace-distance is: 0.00021571735851466656 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.97
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.7; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00022639549570158124
0.00022129529679659754
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.98
Val Epoch over. val_loss: 0.47255653522576496; val_accuracy: 0.8948049363057324 

The current subspace-distance is: 0.00022129529679659754 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.95
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.49; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.94
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.83
Batch: 640; loss: 0.37; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.94
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00022593232279177755
0.00021883906447328627
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.98
Val Epoch over. val_loss: 0.4688753035797435; val_accuracy: 0.8982882165605095 

The current subspace-distance is: 0.00021883906447328627 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.94
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.94
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00022971148428041488
0.00022345864272210747
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.4681810660726705; val_accuracy: 0.8979896496815286 

The current subspace-distance is: 0.00022345864272210747 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.75; acc: 0.75
Batch: 380; loss: 0.46; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00023078243248164654
0.000222610731725581
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.98
Val Epoch over. val_loss: 0.46675768352238234; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.000222610731725581 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.94
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.97
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.92
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0002297004102729261
0.00022204122797120363
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4650637911763161; val_accuracy: 0.897093949044586 

The current subspace-distance is: 0.00022204122797120363 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.66; acc: 0.8
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.52; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0002304701047251001
0.0002239373861812055
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.4552319163729431; val_accuracy: 0.8999800955414012 

The current subspace-distance is: 0.0002239373861812055 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.92
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00023295238497667015
0.00022420717868953943
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.4596161656318956; val_accuracy: 0.8988853503184714 

The current subspace-distance is: 0.00022420717868953943 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.94
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00023231285740621388
0.00022695168445352465
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.98
Val Epoch over. val_loss: 0.460352694437762; val_accuracy: 0.8979896496815286 

The current subspace-distance is: 0.00022695168445352465 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.94
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.36; acc: 0.95
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.95
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00023261093883775175
0.00022416973661165684
Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4572257510605891; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.00022416973661165684 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.62; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.97
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.000235042127314955
0.00022962754883337766
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.45555660973308953; val_accuracy: 0.898984872611465 

The current subspace-distance is: 0.00022962754883337766 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.95
Batch: 140; loss: 0.67; acc: 0.8
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.77
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.38; acc: 0.95
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.95
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.61; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.0002343663218198344
0.0002258304157294333
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.45289699581398324; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.0002258304157294333 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.95
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.95
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.95
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.67; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.65; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00023509636230301112
0.00022760606952942908
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.44660582929659803; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 0.00022760606952942908 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_15_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.03

The number of parameters is: 284685

The number of individual parameters is:

9
162
9
9
13
35802
13
13
25
99450
25
25
64
144000
64
64
4096
64
640
10
64
64

nonzero elements in E: 113873991
elements in E: 113874000
fraction nonzero: 0.9999999209652775
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.45; acc: 0.08
Batch: 20; loss: 2.05; acc: 0.38
Batch: 40; loss: 1.95; acc: 0.44
Batch: 60; loss: 1.75; acc: 0.56
Batch: 80; loss: 1.68; acc: 0.64
Batch: 100; loss: 1.67; acc: 0.59
Batch: 120; loss: 1.6; acc: 0.61
Batch: 140; loss: 1.6; acc: 0.61
Batch: 160; loss: 1.46; acc: 0.75
Batch: 180; loss: 1.49; acc: 0.64
Batch: 200; loss: 1.54; acc: 0.66
Batch: 220; loss: 1.55; acc: 0.59
Batch: 240; loss: 1.48; acc: 0.72
Batch: 260; loss: 1.46; acc: 0.73
Batch: 280; loss: 1.25; acc: 0.86
Batch: 300; loss: 1.36; acc: 0.7
Batch: 320; loss: 1.31; acc: 0.78
Batch: 340; loss: 1.3; acc: 0.7
Batch: 360; loss: 1.34; acc: 0.73
Batch: 380; loss: 1.16; acc: 0.84
Batch: 400; loss: 1.28; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.88
Batch: 440; loss: 1.23; acc: 0.8
Batch: 460; loss: 1.15; acc: 0.84
Batch: 480; loss: 1.2; acc: 0.83
Batch: 500; loss: 1.12; acc: 0.86
Batch: 520; loss: 1.1; acc: 0.88
Batch: 540; loss: 1.09; acc: 0.89
Batch: 560; loss: 1.19; acc: 0.73
Batch: 580; loss: 1.25; acc: 0.77
Batch: 600; loss: 1.06; acc: 0.86
Batch: 620; loss: 1.1; acc: 0.86
Batch: 640; loss: 1.02; acc: 0.91
Batch: 660; loss: 1.13; acc: 0.75
Batch: 680; loss: 1.07; acc: 0.83
Batch: 700; loss: 1.04; acc: 0.81
Batch: 720; loss: 0.97; acc: 0.89
Batch: 740; loss: 1.1; acc: 0.77
Batch: 760; loss: 0.96; acc: 0.89
Batch: 780; loss: 1.0; acc: 0.83
Train Epoch over. train_loss: 1.32; train_accuracy: 0.73 

2.5944416847778484e-05
8.67078506416874e-06
Batch: 0; loss: 1.0; acc: 0.88
Batch: 20; loss: 1.12; acc: 0.81
Batch: 40; loss: 0.72; acc: 0.92
Batch: 60; loss: 1.0; acc: 0.83
Batch: 80; loss: 0.96; acc: 0.89
Batch: 100; loss: 0.96; acc: 0.91
Batch: 120; loss: 1.15; acc: 0.73
Batch: 140; loss: 0.87; acc: 0.91
Val Epoch over. val_loss: 0.9970736492211651; val_accuracy: 0.8561902866242038 

The current subspace-distance is: 8.67078506416874e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.78
Batch: 20; loss: 1.09; acc: 0.77
Batch: 40; loss: 0.92; acc: 0.89
Batch: 60; loss: 1.08; acc: 0.75
Batch: 80; loss: 1.04; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.83
Batch: 120; loss: 0.98; acc: 0.86
Batch: 140; loss: 1.06; acc: 0.78
Batch: 160; loss: 1.03; acc: 0.83
Batch: 180; loss: 0.94; acc: 0.89
Batch: 200; loss: 0.88; acc: 0.89
Batch: 220; loss: 0.98; acc: 0.81
Batch: 240; loss: 1.15; acc: 0.72
Batch: 260; loss: 1.07; acc: 0.75
Batch: 280; loss: 0.95; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.91
Batch: 320; loss: 0.87; acc: 0.91
Batch: 340; loss: 0.95; acc: 0.83
Batch: 360; loss: 0.88; acc: 0.86
Batch: 380; loss: 0.88; acc: 0.91
Batch: 400; loss: 1.05; acc: 0.84
Batch: 420; loss: 0.94; acc: 0.83
Batch: 440; loss: 0.94; acc: 0.86
Batch: 460; loss: 1.01; acc: 0.81
Batch: 480; loss: 0.89; acc: 0.86
Batch: 500; loss: 0.99; acc: 0.8
Batch: 520; loss: 0.88; acc: 0.89
Batch: 540; loss: 0.89; acc: 0.84
Batch: 560; loss: 0.96; acc: 0.83
Batch: 580; loss: 0.96; acc: 0.8
Batch: 600; loss: 0.85; acc: 0.84
Batch: 620; loss: 0.94; acc: 0.88
Batch: 640; loss: 0.97; acc: 0.77
Batch: 660; loss: 0.98; acc: 0.78
Batch: 680; loss: 0.83; acc: 0.91
Batch: 700; loss: 0.96; acc: 0.8
Batch: 720; loss: 0.82; acc: 0.83
Batch: 740; loss: 0.88; acc: 0.8
Batch: 760; loss: 0.9; acc: 0.78
Batch: 780; loss: 0.85; acc: 0.84
Train Epoch over. train_loss: 0.95; train_accuracy: 0.84 

3.114413266303018e-05
1.1719774192897603e-05
Batch: 0; loss: 0.78; acc: 0.91
Batch: 20; loss: 0.88; acc: 0.81
Batch: 40; loss: 0.5; acc: 0.95
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.77; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.94
Batch: 120; loss: 1.01; acc: 0.78
Batch: 140; loss: 0.65; acc: 0.94
Val Epoch over. val_loss: 0.8049845230427517; val_accuracy: 0.87390525477707 

The current subspace-distance is: 1.1719774192897603e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.88
Batch: 20; loss: 0.76; acc: 0.86
Batch: 40; loss: 0.92; acc: 0.84
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.91
Batch: 100; loss: 0.87; acc: 0.83
Batch: 120; loss: 0.79; acc: 0.84
Batch: 140; loss: 0.79; acc: 0.88
Batch: 160; loss: 0.85; acc: 0.83
Batch: 180; loss: 0.77; acc: 0.89
Batch: 200; loss: 0.9; acc: 0.8
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 0.77; acc: 0.86
Batch: 260; loss: 0.75; acc: 0.91
Batch: 280; loss: 1.08; acc: 0.67
Batch: 300; loss: 0.69; acc: 0.94
Batch: 320; loss: 0.86; acc: 0.8
Batch: 340; loss: 0.92; acc: 0.78
Batch: 360; loss: 0.86; acc: 0.89
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.77; acc: 0.84
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.83; acc: 0.81
Batch: 460; loss: 0.84; acc: 0.86
Batch: 480; loss: 0.8; acc: 0.86
Batch: 500; loss: 0.78; acc: 0.92
Batch: 520; loss: 0.74; acc: 0.8
Batch: 540; loss: 0.77; acc: 0.88
Batch: 560; loss: 0.74; acc: 0.89
Batch: 580; loss: 0.68; acc: 0.89
Batch: 600; loss: 0.66; acc: 0.91
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.76; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.89; acc: 0.81
Batch: 700; loss: 0.84; acc: 0.84
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.91
Batch: 760; loss: 0.7; acc: 0.89
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.79; train_accuracy: 0.86 

3.577497409423813e-05
1.459871964470949e-05
Batch: 0; loss: 0.67; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.95
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6790263500942546; val_accuracy: 0.8861464968152867 

The current subspace-distance is: 1.459871964470949e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.94
Batch: 20; loss: 0.83; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.86; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.95
Batch: 100; loss: 0.7; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.92
Batch: 140; loss: 0.8; acc: 0.83
Batch: 160; loss: 0.73; acc: 0.92
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 0.91; acc: 0.78
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.81; acc: 0.81
Batch: 300; loss: 0.74; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.95
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.88
Batch: 440; loss: 0.74; acc: 0.86
Batch: 460; loss: 0.78; acc: 0.8
Batch: 480; loss: 0.67; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.65; acc: 0.91
Batch: 600; loss: 0.82; acc: 0.8
Batch: 620; loss: 0.74; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.89
Batch: 680; loss: 0.77; acc: 0.81
Batch: 700; loss: 0.63; acc: 0.91
Batch: 720; loss: 0.68; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.88
Batch: 760; loss: 0.59; acc: 0.94
Batch: 780; loss: 0.76; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.87 

4.0150633140001446e-05
1.5760890164528973e-05
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.94
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.95
Val Epoch over. val_loss: 0.6155832295964478; val_accuracy: 0.8922173566878981 

The current subspace-distance is: 1.5760890164528973e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.88
Batch: 140; loss: 0.77; acc: 0.8
Batch: 160; loss: 0.73; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.61; acc: 0.89
Batch: 220; loss: 0.64; acc: 0.92
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.95
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.76; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.57; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.8; acc: 0.83
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.92
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.68; acc: 0.88
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.92
Batch: 620; loss: 0.58; acc: 0.89
Batch: 640; loss: 0.76; acc: 0.83
Batch: 660; loss: 0.63; acc: 0.88
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.72; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.64; train_accuracy: 0.88 

4.400722900754772e-05
1.8214272131444886e-05
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.97
Val Epoch over. val_loss: 0.5638448049308388; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 1.8214272131444886e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.97
Batch: 140; loss: 0.68; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.92
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.95
Batch: 280; loss: 0.69; acc: 0.78
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.94
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.94
Batch: 420; loss: 0.73; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.63; acc: 0.92
Batch: 540; loss: 0.66; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.98
Batch: 680; loss: 0.79; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

4.657312456401996e-05
1.983816036954522e-05
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.98
Val Epoch over. val_loss: 0.5332552570446282; val_accuracy: 0.9022691082802548 

The current subspace-distance is: 1.983816036954522e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.8; acc: 0.8
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.78; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.94
Batch: 240; loss: 0.8; acc: 0.78
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.81
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.97
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.94
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.63; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.97
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.81
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.95
Batch: 640; loss: 0.51; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.94
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.56; train_accuracy: 0.88 

4.969300425727852e-05
2.089934605464805e-05
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.34; acc: 0.95
Val Epoch over. val_loss: 0.4993522535463807; val_accuracy: 0.9070461783439491 

The current subspace-distance is: 2.089934605464805e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.71; acc: 0.81
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.94
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.94
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

5.2066316129639745e-05
2.2808348148828372e-05
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.98
Val Epoch over. val_loss: 0.4723632248343935; val_accuracy: 0.9088375796178344 

The current subspace-distance is: 2.2808348148828372e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.97
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.98
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.94
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

5.484263238031417e-05
2.4445700546493754e-05
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.44493636147231813; val_accuracy: 0.9116242038216561 

The current subspace-distance is: 2.4445700546493754e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.34; acc: 0.97
Batch: 240; loss: 0.49; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.66; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.94
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.95
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.81
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

5.649655213346705e-05
2.652088187460322e-05
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.4246579647823504; val_accuracy: 0.9132165605095541 

The current subspace-distance is: 2.652088187460322e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.95
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.97
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.94
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.95
Batch: 760; loss: 0.51; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

5.8653433370636776e-05
2.713277717703022e-05
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4138250031099198; val_accuracy: 0.9158041401273885 

The current subspace-distance is: 2.713277717703022e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.97
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.943887299508788e-05
2.6486741262488067e-05
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.4081856505886005; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.6486741262488067e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.95
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.94
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.9144738770555705e-05
2.6111601982847787e-05
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.4015998583120905; val_accuracy: 0.9175955414012739 

The current subspace-distance is: 2.6111601982847787e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.98
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.945254088146612e-05
2.6022447855211794e-05
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3945098746164589; val_accuracy: 0.9170979299363057 

The current subspace-distance is: 2.6022447855211794e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.95
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.95
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.95
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

6.0234458942431957e-05
2.6420502763357945e-05
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3960415519726504; val_accuracy: 0.9165007961783439 

The current subspace-distance is: 2.6420502763357945e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.97
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.94
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.95
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.98
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

6.134736759122461e-05
2.6817591788130812e-05
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.38378010832580034; val_accuracy: 0.9192874203821656 

The current subspace-distance is: 2.6817591788130812e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.97
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.28; acc: 1.0
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.98
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.08943410043139e-05
2.653880073921755e-05
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3803358505106276; val_accuracy: 0.9195859872611465 

The current subspace-distance is: 2.653880073921755e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.94
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.95
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.123930506873876e-05
2.6356854505138472e-05
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.37667714638315186; val_accuracy: 0.92078025477707 

The current subspace-distance is: 2.6356854505138472e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.181212665978819e-05
2.7633277568384074e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3710093265695936; val_accuracy: 0.9213773885350318 

The current subspace-distance is: 2.7633277568384074e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.98
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.97
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.98
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.293363549048081e-05
2.808983663271647e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3658655728124509; val_accuracy: 0.92078025477707 

The current subspace-distance is: 2.808983663271647e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.357899110298604e-05
2.894916724471841e-05
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3635936706878577; val_accuracy: 0.9204816878980892 

The current subspace-distance is: 2.894916724471841e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.59; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.97
Batch: 160; loss: 0.35; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.347946327878162e-05
2.896600381063763e-05
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.36500549857403825; val_accuracy: 0.9213773885350318 

The current subspace-distance is: 2.896600381063763e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.459835276473314e-05
2.994866190419998e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3600074909864717; val_accuracy: 0.921875 

The current subspace-distance is: 2.994866190419998e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.52; acc: 0.8
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.294412742136046e-05
2.8838067009928636e-05
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3617425500207646; val_accuracy: 0.9213773885350318 

The current subspace-distance is: 2.8838067009928636e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.32979063084349e-05
2.7555586711969227e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.35719744576390383; val_accuracy: 0.9217754777070064 

The current subspace-distance is: 2.7555586711969227e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.405878229998052e-05
2.966275678772945e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3606884337154923; val_accuracy: 0.9215764331210191 

The current subspace-distance is: 2.966275678772945e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.78
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.92
Batch: 140; loss: 0.26; acc: 1.0
Batch: 160; loss: 0.26; acc: 0.98
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.97
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.98
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.422901060432196e-05
2.914576543844305e-05
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.36005106777142565; val_accuracy: 0.921875 

The current subspace-distance is: 2.914576543844305e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.98
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.98
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.51781156193465e-05
3.0189410608727485e-05
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.35504876694102194; val_accuracy: 0.9212778662420382 

The current subspace-distance is: 3.0189410608727485e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.64; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.55295661999844e-05
3.062305768253282e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.35537886458217716; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 3.062305768253282e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.98
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.454368849517778e-05
2.8476315492298454e-05
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3519548889558027; val_accuracy: 0.9222730891719745 

The current subspace-distance is: 2.8476315492298454e-05 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_15_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.03

The number of parameters is: 284685

The number of individual parameters is:

9
162
9
9
13
35802
13
13
25
99450
25
25
64
144000
64
64
4096
64
640
10
64
64

nonzero elements in E: 142342489
elements in E: 142342500
fraction nonzero: 0.9999999227216045
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.49; acc: 0.06
Batch: 20; loss: 1.93; acc: 0.42
Batch: 40; loss: 1.78; acc: 0.52
Batch: 60; loss: 1.78; acc: 0.55
Batch: 80; loss: 1.63; acc: 0.59
Batch: 100; loss: 1.57; acc: 0.62
Batch: 120; loss: 1.33; acc: 0.73
Batch: 140; loss: 1.35; acc: 0.75
Batch: 160; loss: 1.35; acc: 0.8
Batch: 180; loss: 1.46; acc: 0.69
Batch: 200; loss: 1.38; acc: 0.7
Batch: 220; loss: 1.37; acc: 0.66
Batch: 240; loss: 1.3; acc: 0.72
Batch: 260; loss: 1.38; acc: 0.61
Batch: 280; loss: 1.36; acc: 0.72
Batch: 300; loss: 1.17; acc: 0.86
Batch: 320; loss: 1.27; acc: 0.78
Batch: 340; loss: 1.17; acc: 0.73
Batch: 360; loss: 1.19; acc: 0.8
Batch: 380; loss: 1.18; acc: 0.75
Batch: 400; loss: 1.06; acc: 0.86
Batch: 420; loss: 1.09; acc: 0.83
Batch: 440; loss: 1.12; acc: 0.8
Batch: 460; loss: 1.05; acc: 0.83
Batch: 480; loss: 1.06; acc: 0.78
Batch: 500; loss: 1.14; acc: 0.8
Batch: 520; loss: 1.08; acc: 0.77
Batch: 540; loss: 1.13; acc: 0.77
Batch: 560; loss: 0.97; acc: 0.88
Batch: 580; loss: 1.06; acc: 0.83
Batch: 600; loss: 1.03; acc: 0.84
Batch: 620; loss: 0.94; acc: 0.84
Batch: 640; loss: 1.12; acc: 0.8
Batch: 660; loss: 1.11; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.86
Batch: 700; loss: 0.93; acc: 0.91
Batch: 720; loss: 0.99; acc: 0.84
Batch: 740; loss: 0.83; acc: 0.97
Batch: 760; loss: 0.94; acc: 0.86
Batch: 780; loss: 1.02; acc: 0.73
Train Epoch over. train_loss: 1.25; train_accuracy: 0.75 

2.61851710092742e-05
8.97647078090813e-06
Batch: 0; loss: 0.97; acc: 0.88
Batch: 20; loss: 1.04; acc: 0.75
Batch: 40; loss: 0.7; acc: 0.89
Batch: 60; loss: 0.9; acc: 0.84
Batch: 80; loss: 0.8; acc: 0.92
Batch: 100; loss: 0.87; acc: 0.89
Batch: 120; loss: 1.02; acc: 0.81
Batch: 140; loss: 0.87; acc: 0.88
Val Epoch over. val_loss: 0.9157287318995044; val_accuracy: 0.8546974522292994 

The current subspace-distance is: 8.97647078090813e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.88
Batch: 20; loss: 0.96; acc: 0.84
Batch: 40; loss: 0.97; acc: 0.81
Batch: 60; loss: 1.04; acc: 0.86
Batch: 80; loss: 0.86; acc: 0.92
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.85; acc: 0.81
Batch: 160; loss: 0.84; acc: 0.89
Batch: 180; loss: 0.87; acc: 0.89
Batch: 200; loss: 0.96; acc: 0.75
Batch: 220; loss: 0.91; acc: 0.81
Batch: 240; loss: 0.9; acc: 0.81
Batch: 260; loss: 0.9; acc: 0.81
Batch: 280; loss: 0.91; acc: 0.77
Batch: 300; loss: 0.92; acc: 0.86
Batch: 320; loss: 0.74; acc: 0.91
Batch: 340; loss: 1.02; acc: 0.73
Batch: 360; loss: 0.84; acc: 0.81
Batch: 380; loss: 0.84; acc: 0.86
Batch: 400; loss: 0.83; acc: 0.86
Batch: 420; loss: 0.8; acc: 0.89
Batch: 440; loss: 0.82; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.92
Batch: 480; loss: 0.76; acc: 0.92
Batch: 500; loss: 0.83; acc: 0.88
Batch: 520; loss: 0.92; acc: 0.78
Batch: 540; loss: 0.73; acc: 0.91
Batch: 560; loss: 0.86; acc: 0.8
Batch: 580; loss: 0.82; acc: 0.86
Batch: 600; loss: 0.85; acc: 0.86
Batch: 620; loss: 0.85; acc: 0.78
Batch: 640; loss: 0.82; acc: 0.81
Batch: 660; loss: 0.77; acc: 0.88
Batch: 680; loss: 0.79; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.92
Batch: 720; loss: 0.71; acc: 0.84
Batch: 740; loss: 0.81; acc: 0.84
Batch: 760; loss: 0.77; acc: 0.89
Batch: 780; loss: 0.71; acc: 0.89
Train Epoch over. train_loss: 0.85; train_accuracy: 0.85 

3.184611341566779e-05
1.2816238267987501e-05
Batch: 0; loss: 0.77; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.94
Val Epoch over. val_loss: 0.7047174121164212; val_accuracy: 0.8798765923566879 

The current subspace-distance is: 1.2816238267987501e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.88
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 0.66; acc: 0.89
Batch: 80; loss: 0.73; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.77; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.71; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.71; acc: 0.91
Batch: 260; loss: 0.65; acc: 0.89
Batch: 280; loss: 0.8; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.89
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.61; acc: 0.92
Batch: 380; loss: 0.73; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.89
Batch: 420; loss: 0.84; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.92
Batch: 460; loss: 0.71; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.92
Batch: 500; loss: 0.64; acc: 0.89
Batch: 520; loss: 0.79; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.89
Batch: 580; loss: 0.7; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.92
Batch: 620; loss: 0.62; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.86
Batch: 680; loss: 0.66; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.71; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.7; acc: 0.94
Batch: 780; loss: 0.64; acc: 0.94
Train Epoch over. train_loss: 0.71; train_accuracy: 0.87 

3.613331864471547e-05
1.5257533959811553e-05
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.43; acc: 0.95
Val Epoch over. val_loss: 0.601985086111506; val_accuracy: 0.8942078025477707 

The current subspace-distance is: 1.5257533959811553e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.64; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.92
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.92
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.84; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.84
Batch: 280; loss: 0.63; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.94
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.7; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.54; acc: 0.94
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.97
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.61; train_accuracy: 0.88 

4.069529313710518e-05
1.705723479972221e-05
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.5022210220622408; val_accuracy: 0.9067476114649682 

The current subspace-distance is: 1.705723479972221e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.71; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.92
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.98
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.97
Batch: 660; loss: 0.69; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.442574208951555e-05
1.967396201507654e-05
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.44498981307646274; val_accuracy: 0.9114251592356688 

The current subspace-distance is: 1.967396201507654e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.97
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.94
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

4.735710899694823e-05
2.0406223484314978e-05
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.40457788850091825; val_accuracy: 0.916202229299363 

The current subspace-distance is: 2.0406223484314978e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.060779585619457e-05
2.3034270270727575e-05
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3764634440848782; val_accuracy: 0.9182921974522293 

The current subspace-distance is: 2.3034270270727575e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.62; acc: 0.78
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.97
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.311495624482632e-05
2.4172488338081166e-05
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3515399352760072; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.4172488338081166e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.21; acc: 1.0
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.98
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.5781401897547767e-05
2.6375251763965935e-05
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.32600435975250924; val_accuracy: 0.9263535031847133 

The current subspace-distance is: 2.6375251763965935e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.727156894863583e-05
2.7398347810958512e-05
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3128818571092976; val_accuracy: 0.9274482484076433 

The current subspace-distance is: 2.7398347810958512e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.25; acc: 1.0
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.8372494095237926e-05
2.754307206487283e-05
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.312216502845667; val_accuracy: 0.9281449044585988 

The current subspace-distance is: 2.754307206487283e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.98
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.39; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.940465780440718e-05
2.709405089262873e-05
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3068835012567271; val_accuracy: 0.9311305732484076 

The current subspace-distance is: 2.709405089262873e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.77
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.9872476413147524e-05
2.7613856218522415e-05
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3040327807046046; val_accuracy: 0.9299363057324841 

The current subspace-distance is: 2.7613856218522415e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.061786916689016e-05
2.7908232368645258e-05
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.298438421005656; val_accuracy: 0.9327229299363057 

The current subspace-distance is: 2.7908232368645258e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.98
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.099177699070424e-05
2.856309583876282e-05
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.29064327511627963; val_accuracy: 0.9319267515923567 

The current subspace-distance is: 2.856309583876282e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.98
Batch: 200; loss: 0.21; acc: 1.0
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.98
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.154790753498673e-05
2.8433767511160113e-05
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.29297117200816514; val_accuracy: 0.931031050955414 

The current subspace-distance is: 2.8433767511160113e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.83
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.281171226873994e-05
3.025191108463332e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.28961455005749015; val_accuracy: 0.9326234076433121 

The current subspace-distance is: 3.025191108463332e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.98
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.241983646759763e-05
3.083540286752395e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.28779112447978583; val_accuracy: 0.9328224522292994 

The current subspace-distance is: 3.083540286752395e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.98
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.372464122250676e-05
2.9460501536959782e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.28242267971965157; val_accuracy: 0.9344148089171974 

The current subspace-distance is: 2.9460501536959782e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.365474837366492e-05
3.1139355996856466e-05
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2794994527272358; val_accuracy: 0.933718152866242 

The current subspace-distance is: 3.1139355996856466e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.378519901772961e-05
2.9035658371867612e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.28336188111715255; val_accuracy: 0.9314291401273885 

The current subspace-distance is: 2.9035658371867612e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.402391591109335e-05
3.0042219805181958e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.27943834709893367; val_accuracy: 0.9357085987261147 

The current subspace-distance is: 3.0042219805181958e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.98
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.98
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.97
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.38205383438617e-05
2.9809547413606197e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2807054019942405; val_accuracy: 0.9339171974522293 

The current subspace-distance is: 2.9809547413606197e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.399694393621758e-05
2.8911044864798896e-05
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2778887950405953; val_accuracy: 0.9340167197452229 

The current subspace-distance is: 2.8911044864798896e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.98
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.339517130982131e-05
2.854023659892846e-05
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2742842848228801; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.854023659892846e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.398442928912118e-05
2.8968488550162874e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2750490971241787; val_accuracy: 0.9338176751592356 

The current subspace-distance is: 2.8968488550162874e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.17; acc: 1.0
Batch: 580; loss: 0.23; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.97
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.449415377574041e-05
3.0502913432428613e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2765757656496042; val_accuracy: 0.9332205414012739 

The current subspace-distance is: 3.0502913432428613e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.98
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.511007086373866e-05
3.09281240333803e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2747545854965593; val_accuracy: 0.9347133757961783 

The current subspace-distance is: 3.09281240333803e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.496273272205144e-05
3.1878200388746336e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.2728861642965845; val_accuracy: 0.9349124203821656 

The current subspace-distance is: 3.1878200388746336e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.537228910019621e-05
3.107424345216714e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.27387813858355686; val_accuracy: 0.9351114649681529 

The current subspace-distance is: 3.107424345216714e-05 

plots/subspace_training/table13slim/2020-01-29 16:23:19/N_15_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 16:23:19/N_15_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
