model : table13slim
N : 2
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:47

Channel scaling factor: 7.203731671848972

The number of parameters is: 274066

The number of individual parameters is:

58
580
58
58
87
50460
87
87
173
150510
173
173
64
66432
64
64
4096
64
640
10
64
64

nonzero elements in E: 13703299
elements in E: 13703300
fraction nonzero: 0.9999999270248772
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.57; acc: 0.06
Batch: 20; loss: 2.53; acc: 0.08
Batch: 40; loss: 2.55; acc: 0.08
Batch: 60; loss: 2.51; acc: 0.05
Batch: 80; loss: 2.36; acc: 0.19
Batch: 100; loss: 2.47; acc: 0.08
Batch: 120; loss: 2.48; acc: 0.03
Batch: 140; loss: 2.39; acc: 0.17
Batch: 160; loss: 2.33; acc: 0.14
Batch: 180; loss: 2.46; acc: 0.03
Batch: 200; loss: 2.34; acc: 0.11
Batch: 220; loss: 2.32; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.27; acc: 0.17
Batch: 280; loss: 2.34; acc: 0.14
Batch: 300; loss: 2.21; acc: 0.22
Batch: 320; loss: 2.22; acc: 0.19
Batch: 340; loss: 2.14; acc: 0.19
Batch: 360; loss: 2.18; acc: 0.2
Batch: 380; loss: 2.08; acc: 0.25
Batch: 400; loss: 2.14; acc: 0.23
Batch: 420; loss: 2.19; acc: 0.2
Batch: 440; loss: 2.15; acc: 0.19
Batch: 460; loss: 2.09; acc: 0.28
Batch: 480; loss: 2.2; acc: 0.2
Batch: 500; loss: 2.18; acc: 0.2
Batch: 520; loss: 2.21; acc: 0.2
Batch: 540; loss: 2.06; acc: 0.3
Batch: 560; loss: 2.17; acc: 0.2
Batch: 580; loss: 2.22; acc: 0.19
Batch: 600; loss: 1.99; acc: 0.31
Batch: 620; loss: 2.13; acc: 0.23
Batch: 640; loss: 2.03; acc: 0.3
Batch: 660; loss: 2.09; acc: 0.28
Batch: 680; loss: 2.09; acc: 0.28
Batch: 700; loss: 2.08; acc: 0.33
Batch: 720; loss: 2.04; acc: 0.36
Batch: 740; loss: 2.0; acc: 0.38
Batch: 760; loss: 2.04; acc: 0.28
Batch: 780; loss: 1.88; acc: 0.44
Train Epoch over. train_loss: 2.21; train_accuracy: 0.21 

2.6565494408714585e-05
4.040547537442762e-06
Batch: 0; loss: 2.13; acc: 0.31
Batch: 20; loss: 2.11; acc: 0.22
Batch: 40; loss: 1.88; acc: 0.44
Batch: 60; loss: 2.0; acc: 0.31
Batch: 80; loss: 1.92; acc: 0.34
Batch: 100; loss: 2.06; acc: 0.34
Batch: 120; loss: 2.13; acc: 0.25
Batch: 140; loss: 1.94; acc: 0.44
Val Epoch over. val_loss: 2.010851610238385; val_accuracy: 0.33250398089171973 

The current subspace-distance is: 4.040547537442762e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.13; acc: 0.19
Batch: 20; loss: 2.04; acc: 0.3
Batch: 40; loss: 1.98; acc: 0.34
Batch: 60; loss: 2.16; acc: 0.3
Batch: 80; loss: 2.04; acc: 0.31
Batch: 100; loss: 1.99; acc: 0.3
Batch: 120; loss: 2.0; acc: 0.3
Batch: 140; loss: 2.13; acc: 0.28
Batch: 160; loss: 1.94; acc: 0.44
Batch: 180; loss: 1.98; acc: 0.41
Batch: 200; loss: 1.94; acc: 0.36
Batch: 220; loss: 2.08; acc: 0.34
Batch: 240; loss: 2.1; acc: 0.27
Batch: 260; loss: 2.1; acc: 0.28
Batch: 280; loss: 1.97; acc: 0.36
Batch: 300; loss: 2.13; acc: 0.25
Batch: 320; loss: 1.89; acc: 0.44
Batch: 340; loss: 1.94; acc: 0.31
Batch: 360; loss: 1.97; acc: 0.34
Batch: 380; loss: 2.0; acc: 0.28
Batch: 400; loss: 1.97; acc: 0.38
Batch: 420; loss: 1.95; acc: 0.34
Batch: 440; loss: 1.92; acc: 0.38
Batch: 460; loss: 1.99; acc: 0.3
Batch: 480; loss: 1.94; acc: 0.27
Batch: 500; loss: 2.06; acc: 0.25
Batch: 520; loss: 1.96; acc: 0.34
Batch: 540; loss: 2.02; acc: 0.31
Batch: 560; loss: 1.88; acc: 0.34
Batch: 580; loss: 1.98; acc: 0.44
Batch: 600; loss: 2.02; acc: 0.33
Batch: 620; loss: 1.84; acc: 0.47
Batch: 640; loss: 1.99; acc: 0.31
Batch: 660; loss: 1.95; acc: 0.44
Batch: 680; loss: 1.86; acc: 0.44
Batch: 700; loss: 1.89; acc: 0.42
Batch: 720; loss: 1.97; acc: 0.34
Batch: 740; loss: 1.85; acc: 0.52
Batch: 760; loss: 1.94; acc: 0.39
Batch: 780; loss: 1.95; acc: 0.36
Train Epoch over. train_loss: 1.98; train_accuracy: 0.35 

2.848809162969701e-05
4.389015884953551e-06
Batch: 0; loss: 1.9; acc: 0.44
Batch: 20; loss: 1.98; acc: 0.3
Batch: 40; loss: 1.73; acc: 0.56
Batch: 60; loss: 1.91; acc: 0.39
Batch: 80; loss: 1.8; acc: 0.44
Batch: 100; loss: 1.94; acc: 0.39
Batch: 120; loss: 1.95; acc: 0.44
Batch: 140; loss: 1.85; acc: 0.45
Val Epoch over. val_loss: 1.8852507278418085; val_accuracy: 0.41789410828025475 

The current subspace-distance is: 4.389015884953551e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.91; acc: 0.31
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.92; acc: 0.34
Batch: 60; loss: 1.99; acc: 0.34
Batch: 80; loss: 1.92; acc: 0.39
Batch: 100; loss: 1.96; acc: 0.34
Batch: 120; loss: 1.91; acc: 0.38
Batch: 140; loss: 1.89; acc: 0.31
Batch: 160; loss: 1.88; acc: 0.41
Batch: 180; loss: 1.96; acc: 0.36
Batch: 200; loss: 1.85; acc: 0.42
Batch: 220; loss: 1.88; acc: 0.52
Batch: 240; loss: 1.86; acc: 0.39
Batch: 260; loss: 1.89; acc: 0.36
Batch: 280; loss: 1.93; acc: 0.38
Batch: 300; loss: 1.85; acc: 0.42
Batch: 320; loss: 1.92; acc: 0.34
Batch: 340; loss: 1.85; acc: 0.44
Batch: 360; loss: 1.87; acc: 0.48
Batch: 380; loss: 1.87; acc: 0.42
Batch: 400; loss: 1.88; acc: 0.5
Batch: 420; loss: 1.84; acc: 0.36
Batch: 440; loss: 1.75; acc: 0.48
Batch: 460; loss: 1.83; acc: 0.42
Batch: 480; loss: 1.83; acc: 0.44
Batch: 500; loss: 1.85; acc: 0.44
Batch: 520; loss: 1.99; acc: 0.33
Batch: 540; loss: 1.91; acc: 0.41
Batch: 560; loss: 1.86; acc: 0.36
Batch: 580; loss: 1.87; acc: 0.5
Batch: 600; loss: 1.86; acc: 0.38
Batch: 620; loss: 1.86; acc: 0.48
Batch: 640; loss: 1.9; acc: 0.39
Batch: 660; loss: 1.99; acc: 0.28
Batch: 680; loss: 1.88; acc: 0.34
Batch: 700; loss: 1.88; acc: 0.44
Batch: 720; loss: 1.94; acc: 0.39
Batch: 740; loss: 1.83; acc: 0.45
Batch: 760; loss: 1.87; acc: 0.5
Batch: 780; loss: 1.76; acc: 0.52
Train Epoch over. train_loss: 1.9; train_accuracy: 0.4 

2.991679502883926e-05
6.651781859545736e-06
Batch: 0; loss: 1.85; acc: 0.45
Batch: 20; loss: 1.95; acc: 0.3
Batch: 40; loss: 1.71; acc: 0.56
Batch: 60; loss: 1.9; acc: 0.39
Batch: 80; loss: 1.77; acc: 0.44
Batch: 100; loss: 1.91; acc: 0.38
Batch: 120; loss: 1.89; acc: 0.5
Batch: 140; loss: 1.86; acc: 0.39
Val Epoch over. val_loss: 1.8651918036163233; val_accuracy: 0.43053343949044587 

The current subspace-distance is: 6.651781859545736e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.5
Batch: 20; loss: 1.92; acc: 0.34
Batch: 40; loss: 1.9; acc: 0.36
Batch: 60; loss: 1.97; acc: 0.3
Batch: 80; loss: 1.79; acc: 0.5
Batch: 100; loss: 1.89; acc: 0.41
Batch: 120; loss: 1.8; acc: 0.53
Batch: 140; loss: 1.87; acc: 0.44
Batch: 160; loss: 1.82; acc: 0.45
Batch: 180; loss: 1.92; acc: 0.34
Batch: 200; loss: 1.86; acc: 0.39
Batch: 220; loss: 1.96; acc: 0.36
Batch: 240; loss: 1.86; acc: 0.45
Batch: 260; loss: 1.89; acc: 0.45
Batch: 280; loss: 1.97; acc: 0.36
Batch: 300; loss: 1.82; acc: 0.47
Batch: 320; loss: 1.84; acc: 0.44
Batch: 340; loss: 1.84; acc: 0.42
Batch: 360; loss: 1.86; acc: 0.47
Batch: 380; loss: 1.89; acc: 0.44
Batch: 400; loss: 1.82; acc: 0.45
Batch: 420; loss: 1.91; acc: 0.45
Batch: 440; loss: 1.97; acc: 0.38
Batch: 460; loss: 1.85; acc: 0.44
Batch: 480; loss: 1.86; acc: 0.42
Batch: 500; loss: 1.86; acc: 0.38
Batch: 520; loss: 1.94; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.86; acc: 0.5
Batch: 580; loss: 1.87; acc: 0.38
Batch: 600; loss: 1.95; acc: 0.39
Batch: 620; loss: 1.94; acc: 0.33
Batch: 640; loss: 1.88; acc: 0.36
Batch: 660; loss: 1.81; acc: 0.44
Batch: 680; loss: 1.89; acc: 0.42
Batch: 700; loss: 1.99; acc: 0.31
Batch: 720; loss: 1.75; acc: 0.48
Batch: 740; loss: 1.75; acc: 0.52
Batch: 760; loss: 1.88; acc: 0.47
Batch: 780; loss: 1.82; acc: 0.52
Train Epoch over. train_loss: 1.87; train_accuracy: 0.41 

3.094262501690537e-05
6.6171164689876605e-06
Batch: 0; loss: 1.85; acc: 0.42
Batch: 20; loss: 1.96; acc: 0.31
Batch: 40; loss: 1.68; acc: 0.66
Batch: 60; loss: 1.83; acc: 0.5
Batch: 80; loss: 1.78; acc: 0.45
Batch: 100; loss: 1.88; acc: 0.44
Batch: 120; loss: 1.88; acc: 0.42
Batch: 140; loss: 1.85; acc: 0.39
Val Epoch over. val_loss: 1.8328823587697023; val_accuracy: 0.4518312101910828 

The current subspace-distance is: 6.6171164689876605e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.86; acc: 0.41
Batch: 20; loss: 1.8; acc: 0.53
Batch: 40; loss: 1.86; acc: 0.44
Batch: 60; loss: 1.82; acc: 0.39
Batch: 80; loss: 1.86; acc: 0.5
Batch: 100; loss: 1.81; acc: 0.45
Batch: 120; loss: 1.77; acc: 0.41
Batch: 140; loss: 1.91; acc: 0.34
Batch: 160; loss: 1.89; acc: 0.33
Batch: 180; loss: 1.75; acc: 0.47
Batch: 200; loss: 1.86; acc: 0.38
Batch: 220; loss: 1.87; acc: 0.36
Batch: 240; loss: 1.81; acc: 0.44
Batch: 260; loss: 1.84; acc: 0.42
Batch: 280; loss: 1.83; acc: 0.44
Batch: 300; loss: 1.88; acc: 0.39
Batch: 320; loss: 1.76; acc: 0.44
Batch: 340; loss: 1.86; acc: 0.39
Batch: 360; loss: 1.84; acc: 0.42
Batch: 380; loss: 1.91; acc: 0.36
Batch: 400; loss: 1.92; acc: 0.38
Batch: 420; loss: 1.87; acc: 0.45
Batch: 440; loss: 1.8; acc: 0.45
Batch: 460; loss: 1.79; acc: 0.48
Batch: 480; loss: 1.72; acc: 0.52
Batch: 500; loss: 1.83; acc: 0.36
Batch: 520; loss: 1.83; acc: 0.45
Batch: 540; loss: 1.84; acc: 0.39
Batch: 560; loss: 1.86; acc: 0.33
Batch: 580; loss: 1.87; acc: 0.38
Batch: 600; loss: 1.95; acc: 0.42
Batch: 620; loss: 1.81; acc: 0.45
Batch: 640; loss: 1.75; acc: 0.44
Batch: 660; loss: 1.86; acc: 0.44
Batch: 680; loss: 1.89; acc: 0.44
Batch: 700; loss: 1.82; acc: 0.44
Batch: 720; loss: 1.79; acc: 0.44
Batch: 740; loss: 1.69; acc: 0.52
Batch: 760; loss: 1.82; acc: 0.47
Batch: 780; loss: 1.85; acc: 0.41
Train Epoch over. train_loss: 1.84; train_accuracy: 0.42 

3.26323279296048e-05
7.0117284849402495e-06
Batch: 0; loss: 1.78; acc: 0.44
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.63; acc: 0.62
Batch: 60; loss: 1.77; acc: 0.47
Batch: 80; loss: 1.75; acc: 0.47
Batch: 100; loss: 1.83; acc: 0.44
Batch: 120; loss: 1.88; acc: 0.39
Batch: 140; loss: 1.82; acc: 0.41
Val Epoch over. val_loss: 1.7784827864094146; val_accuracy: 0.4645700636942675 

The current subspace-distance is: 7.0117284849402495e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.62
Batch: 20; loss: 1.8; acc: 0.48
Batch: 40; loss: 1.78; acc: 0.41
Batch: 60; loss: 1.83; acc: 0.53
Batch: 80; loss: 1.76; acc: 0.45
Batch: 100; loss: 1.79; acc: 0.45
Batch: 120; loss: 1.86; acc: 0.44
Batch: 140; loss: 1.66; acc: 0.55
Batch: 160; loss: 1.93; acc: 0.33
Batch: 180; loss: 1.77; acc: 0.41
Batch: 200; loss: 1.78; acc: 0.48
Batch: 220; loss: 1.79; acc: 0.45
Batch: 240; loss: 1.72; acc: 0.44
Batch: 260; loss: 1.75; acc: 0.47
Batch: 280; loss: 1.79; acc: 0.47
Batch: 300; loss: 1.81; acc: 0.31
Batch: 320; loss: 1.9; acc: 0.47
Batch: 340; loss: 1.85; acc: 0.41
Batch: 360; loss: 1.85; acc: 0.36
Batch: 380; loss: 1.83; acc: 0.52
Batch: 400; loss: 1.78; acc: 0.45
Batch: 420; loss: 1.94; acc: 0.34
Batch: 440; loss: 1.86; acc: 0.39
Batch: 460; loss: 1.71; acc: 0.53
Batch: 480; loss: 1.74; acc: 0.47
Batch: 500; loss: 1.65; acc: 0.56
Batch: 520; loss: 1.75; acc: 0.5
Batch: 540; loss: 1.84; acc: 0.41
Batch: 560; loss: 1.83; acc: 0.45
Batch: 580; loss: 1.76; acc: 0.48
Batch: 600; loss: 1.72; acc: 0.48
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.63; acc: 0.56
Batch: 660; loss: 1.73; acc: 0.5
Batch: 680; loss: 1.74; acc: 0.39
Batch: 700; loss: 1.78; acc: 0.45
Batch: 720; loss: 1.69; acc: 0.53
Batch: 740; loss: 1.7; acc: 0.44
Batch: 760; loss: 1.67; acc: 0.58
Batch: 780; loss: 1.7; acc: 0.53
Train Epoch over. train_loss: 1.77; train_accuracy: 0.46 

3.392001599422656e-05
6.637137175857788e-06
Batch: 0; loss: 1.66; acc: 0.55
Batch: 20; loss: 1.81; acc: 0.42
Batch: 40; loss: 1.58; acc: 0.64
Batch: 60; loss: 1.71; acc: 0.58
Batch: 80; loss: 1.68; acc: 0.48
Batch: 100; loss: 1.74; acc: 0.52
Batch: 120; loss: 1.81; acc: 0.47
Batch: 140; loss: 1.76; acc: 0.48
Val Epoch over. val_loss: 1.699178198340592; val_accuracy: 0.5242834394904459 

The current subspace-distance is: 6.637137175857788e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.81; acc: 0.45
Batch: 20; loss: 1.68; acc: 0.5
Batch: 40; loss: 1.65; acc: 0.55
Batch: 60; loss: 1.72; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.66; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.58
Batch: 140; loss: 1.67; acc: 0.56
Batch: 160; loss: 1.67; acc: 0.59
Batch: 180; loss: 1.75; acc: 0.47
Batch: 200; loss: 1.78; acc: 0.45
Batch: 220; loss: 1.64; acc: 0.58
Batch: 240; loss: 1.73; acc: 0.52
Batch: 260; loss: 1.7; acc: 0.48
Batch: 280; loss: 1.62; acc: 0.48
Batch: 300; loss: 1.72; acc: 0.47
Batch: 320; loss: 1.8; acc: 0.47
Batch: 340; loss: 1.69; acc: 0.56
Batch: 360; loss: 1.63; acc: 0.59
Batch: 380; loss: 1.63; acc: 0.53
Batch: 400; loss: 1.77; acc: 0.48
Batch: 420; loss: 1.66; acc: 0.58
Batch: 440; loss: 1.79; acc: 0.44
Batch: 460; loss: 1.63; acc: 0.56
Batch: 480; loss: 1.78; acc: 0.45
Batch: 500; loss: 1.64; acc: 0.53
Batch: 520; loss: 1.8; acc: 0.44
Batch: 540; loss: 1.69; acc: 0.53
Batch: 560; loss: 1.73; acc: 0.45
Batch: 580; loss: 1.74; acc: 0.52
Batch: 600; loss: 1.72; acc: 0.39
Batch: 620; loss: 1.65; acc: 0.52
Batch: 640; loss: 1.7; acc: 0.55
Batch: 660; loss: 1.65; acc: 0.52
Batch: 680; loss: 1.66; acc: 0.52
Batch: 700; loss: 1.57; acc: 0.56
Batch: 720; loss: 1.67; acc: 0.56
Batch: 740; loss: 1.76; acc: 0.53
Batch: 760; loss: 1.63; acc: 0.56
Batch: 780; loss: 1.62; acc: 0.59
Train Epoch over. train_loss: 1.7; train_accuracy: 0.51 

3.535777432261966e-05
9.489980584476143e-06
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.74; acc: 0.42
Batch: 40; loss: 1.45; acc: 0.72
Batch: 60; loss: 1.65; acc: 0.59
Batch: 80; loss: 1.59; acc: 0.53
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.74; acc: 0.55
Batch: 140; loss: 1.66; acc: 0.61
Val Epoch over. val_loss: 1.6287809693889252; val_accuracy: 0.5562300955414012 

The current subspace-distance is: 9.489980584476143e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.44
Batch: 20; loss: 1.59; acc: 0.55
Batch: 40; loss: 1.68; acc: 0.5
Batch: 60; loss: 1.7; acc: 0.41
Batch: 80; loss: 1.65; acc: 0.56
Batch: 100; loss: 1.63; acc: 0.53
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.59; acc: 0.61
Batch: 160; loss: 1.75; acc: 0.5
Batch: 180; loss: 1.66; acc: 0.48
Batch: 200; loss: 1.66; acc: 0.5
Batch: 220; loss: 1.72; acc: 0.47
Batch: 240; loss: 1.69; acc: 0.58
Batch: 260; loss: 1.67; acc: 0.5
Batch: 280; loss: 1.65; acc: 0.52
Batch: 300; loss: 1.65; acc: 0.47
Batch: 320; loss: 1.8; acc: 0.45
Batch: 340; loss: 1.62; acc: 0.59
Batch: 360; loss: 1.7; acc: 0.55
Batch: 380; loss: 1.74; acc: 0.42
Batch: 400; loss: 1.68; acc: 0.5
Batch: 420; loss: 1.72; acc: 0.41
Batch: 440; loss: 1.72; acc: 0.47
Batch: 460; loss: 1.55; acc: 0.62
Batch: 480; loss: 1.69; acc: 0.52
Batch: 500; loss: 1.68; acc: 0.53
Batch: 520; loss: 1.49; acc: 0.73
Batch: 540; loss: 1.58; acc: 0.52
Batch: 560; loss: 1.77; acc: 0.38
Batch: 580; loss: 1.62; acc: 0.58
Batch: 600; loss: 1.53; acc: 0.56
Batch: 620; loss: 1.61; acc: 0.5
Batch: 640; loss: 1.58; acc: 0.62
Batch: 660; loss: 1.67; acc: 0.52
Batch: 680; loss: 1.72; acc: 0.47
Batch: 700; loss: 1.68; acc: 0.53
Batch: 720; loss: 1.58; acc: 0.59
Batch: 740; loss: 1.73; acc: 0.44
Batch: 760; loss: 1.66; acc: 0.52
Batch: 780; loss: 1.56; acc: 0.58
Train Epoch over. train_loss: 1.66; train_accuracy: 0.52 

3.6093439121032134e-05
9.32502189243678e-06
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.73; acc: 0.44
Batch: 40; loss: 1.41; acc: 0.69
Batch: 60; loss: 1.64; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.5
Batch: 140; loss: 1.64; acc: 0.55
Val Epoch over. val_loss: 1.6126804230319467; val_accuracy: 0.5473726114649682 

The current subspace-distance is: 9.32502189243678e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.59; acc: 0.61
Batch: 60; loss: 1.7; acc: 0.47
Batch: 80; loss: 1.62; acc: 0.55
Batch: 100; loss: 1.76; acc: 0.42
Batch: 120; loss: 1.59; acc: 0.52
Batch: 140; loss: 1.7; acc: 0.5
Batch: 160; loss: 1.73; acc: 0.47
Batch: 180; loss: 1.56; acc: 0.66
Batch: 200; loss: 1.58; acc: 0.64
Batch: 220; loss: 1.61; acc: 0.53
Batch: 240; loss: 1.72; acc: 0.45
Batch: 260; loss: 1.77; acc: 0.48
Batch: 280; loss: 1.77; acc: 0.48
Batch: 300; loss: 1.74; acc: 0.42
Batch: 320; loss: 1.64; acc: 0.52
Batch: 340; loss: 1.67; acc: 0.42
Batch: 360; loss: 1.67; acc: 0.53
Batch: 380; loss: 1.68; acc: 0.52
Batch: 400; loss: 1.63; acc: 0.52
Batch: 420; loss: 1.67; acc: 0.56
Batch: 440; loss: 1.59; acc: 0.56
Batch: 460; loss: 1.72; acc: 0.42
Batch: 480; loss: 1.66; acc: 0.42
Batch: 500; loss: 1.7; acc: 0.52
Batch: 520; loss: 1.82; acc: 0.44
Batch: 540; loss: 1.67; acc: 0.5
Batch: 560; loss: 1.57; acc: 0.59
Batch: 580; loss: 1.72; acc: 0.5
Batch: 600; loss: 1.78; acc: 0.42
Batch: 620; loss: 1.76; acc: 0.42
Batch: 640; loss: 1.7; acc: 0.5
Batch: 660; loss: 1.61; acc: 0.48
Batch: 680; loss: 1.7; acc: 0.45
Batch: 700; loss: 1.62; acc: 0.53
Batch: 720; loss: 1.74; acc: 0.42
Batch: 740; loss: 1.66; acc: 0.41
Batch: 760; loss: 1.66; acc: 0.5
Batch: 780; loss: 1.64; acc: 0.52
Train Epoch over. train_loss: 1.66; train_accuracy: 0.51 

3.7286772567313164e-05
9.374265573569573e-06
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.41; acc: 0.69
Batch: 60; loss: 1.63; acc: 0.58
Batch: 80; loss: 1.57; acc: 0.52
Batch: 100; loss: 1.63; acc: 0.58
Batch: 120; loss: 1.76; acc: 0.48
Batch: 140; loss: 1.63; acc: 0.56
Val Epoch over. val_loss: 1.6068041537218034; val_accuracy: 0.5486664012738853 

The current subspace-distance is: 9.374265573569573e-06 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.65; acc: 0.59
Batch: 60; loss: 1.7; acc: 0.44
Batch: 80; loss: 1.74; acc: 0.41
Batch: 100; loss: 1.67; acc: 0.39
Batch: 120; loss: 1.8; acc: 0.36
Batch: 140; loss: 1.68; acc: 0.5
Batch: 160; loss: 1.51; acc: 0.58
Batch: 180; loss: 1.68; acc: 0.44
Batch: 200; loss: 1.72; acc: 0.47
Batch: 220; loss: 1.51; acc: 0.56
Batch: 240; loss: 1.78; acc: 0.42
Batch: 260; loss: 1.67; acc: 0.48
Batch: 280; loss: 1.71; acc: 0.42
Batch: 300; loss: 1.58; acc: 0.45
Batch: 320; loss: 1.6; acc: 0.47
Batch: 340; loss: 1.61; acc: 0.58
Batch: 360; loss: 1.69; acc: 0.39
Batch: 380; loss: 1.65; acc: 0.47
Batch: 400; loss: 1.61; acc: 0.5
Batch: 420; loss: 1.59; acc: 0.56
Batch: 440; loss: 1.67; acc: 0.48
Batch: 460; loss: 1.76; acc: 0.53
Batch: 480; loss: 1.54; acc: 0.58
Batch: 500; loss: 1.71; acc: 0.39
Batch: 520; loss: 1.66; acc: 0.52
Batch: 540; loss: 1.64; acc: 0.59
Batch: 560; loss: 1.67; acc: 0.5
Batch: 580; loss: 1.64; acc: 0.55
Batch: 600; loss: 1.78; acc: 0.36
Batch: 620; loss: 1.55; acc: 0.58
Batch: 640; loss: 1.64; acc: 0.48
Batch: 660; loss: 1.74; acc: 0.44
Batch: 680; loss: 1.68; acc: 0.5
Batch: 700; loss: 1.65; acc: 0.5
Batch: 720; loss: 1.71; acc: 0.42
Batch: 740; loss: 1.65; acc: 0.53
Batch: 760; loss: 1.48; acc: 0.66
Batch: 780; loss: 1.65; acc: 0.55
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.7928683013888076e-05
9.01479143067263e-06
Batch: 0; loss: 1.58; acc: 0.58
Batch: 20; loss: 1.69; acc: 0.5
Batch: 40; loss: 1.4; acc: 0.69
Batch: 60; loss: 1.61; acc: 0.58
Batch: 80; loss: 1.56; acc: 0.48
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.76; acc: 0.45
Batch: 140; loss: 1.62; acc: 0.58
Val Epoch over. val_loss: 1.6000837558394025; val_accuracy: 0.5460788216560509 

The current subspace-distance is: 9.01479143067263e-06 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.52
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.76; acc: 0.42
Batch: 60; loss: 1.62; acc: 0.52
Batch: 80; loss: 1.61; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.56
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.72; acc: 0.52
Batch: 160; loss: 1.8; acc: 0.48
Batch: 180; loss: 1.63; acc: 0.52
Batch: 200; loss: 1.64; acc: 0.56
Batch: 220; loss: 1.6; acc: 0.48
Batch: 240; loss: 1.59; acc: 0.56
Batch: 260; loss: 1.69; acc: 0.45
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.65; acc: 0.5
Batch: 320; loss: 1.74; acc: 0.45
Batch: 340; loss: 1.59; acc: 0.56
Batch: 360; loss: 1.61; acc: 0.48
Batch: 380; loss: 1.7; acc: 0.5
Batch: 400; loss: 1.69; acc: 0.44
Batch: 420; loss: 1.67; acc: 0.53
Batch: 440; loss: 1.64; acc: 0.52
Batch: 460; loss: 1.77; acc: 0.41
Batch: 480; loss: 1.56; acc: 0.61
Batch: 500; loss: 1.66; acc: 0.45
Batch: 520; loss: 1.73; acc: 0.42
Batch: 540; loss: 1.69; acc: 0.47
Batch: 560; loss: 1.62; acc: 0.53
Batch: 580; loss: 1.52; acc: 0.64
Batch: 600; loss: 1.67; acc: 0.59
Batch: 620; loss: 1.6; acc: 0.55
Batch: 640; loss: 1.54; acc: 0.52
Batch: 660; loss: 1.74; acc: 0.42
Batch: 680; loss: 1.59; acc: 0.55
Batch: 700; loss: 1.72; acc: 0.42
Batch: 720; loss: 1.74; acc: 0.52
Batch: 740; loss: 1.56; acc: 0.59
Batch: 760; loss: 1.68; acc: 0.52
Batch: 780; loss: 1.72; acc: 0.48
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.841364377876744e-05
1.134821923187701e-05
Batch: 0; loss: 1.59; acc: 0.56
Batch: 20; loss: 1.72; acc: 0.41
Batch: 40; loss: 1.39; acc: 0.67
Batch: 60; loss: 1.62; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.47
Batch: 140; loss: 1.62; acc: 0.56
Val Epoch over. val_loss: 1.6041571888954016; val_accuracy: 0.5357285031847133 

The current subspace-distance is: 1.134821923187701e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.57; acc: 0.62
Batch: 20; loss: 1.61; acc: 0.53
Batch: 40; loss: 1.54; acc: 0.61
Batch: 60; loss: 1.68; acc: 0.59
Batch: 80; loss: 1.67; acc: 0.42
Batch: 100; loss: 1.65; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.56
Batch: 140; loss: 1.71; acc: 0.48
Batch: 160; loss: 1.59; acc: 0.48
Batch: 180; loss: 1.68; acc: 0.53
Batch: 200; loss: 1.69; acc: 0.53
Batch: 220; loss: 1.67; acc: 0.47
Batch: 240; loss: 1.58; acc: 0.58
Batch: 260; loss: 1.62; acc: 0.52
Batch: 280; loss: 1.64; acc: 0.56
Batch: 300; loss: 1.68; acc: 0.52
Batch: 320; loss: 1.64; acc: 0.5
Batch: 340; loss: 1.63; acc: 0.42
Batch: 360; loss: 1.59; acc: 0.61
Batch: 380; loss: 1.61; acc: 0.39
Batch: 400; loss: 1.59; acc: 0.59
Batch: 420; loss: 1.66; acc: 0.56
Batch: 440; loss: 1.59; acc: 0.53
Batch: 460; loss: 1.73; acc: 0.53
Batch: 480; loss: 1.55; acc: 0.58
Batch: 500; loss: 1.6; acc: 0.48
Batch: 520; loss: 1.7; acc: 0.44
Batch: 540; loss: 1.67; acc: 0.44
Batch: 560; loss: 1.72; acc: 0.47
Batch: 580; loss: 1.63; acc: 0.56
Batch: 600; loss: 1.74; acc: 0.45
Batch: 620; loss: 1.62; acc: 0.58
Batch: 640; loss: 1.58; acc: 0.55
Batch: 660; loss: 1.69; acc: 0.5
Batch: 680; loss: 1.67; acc: 0.48
Batch: 700; loss: 1.71; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.42
Batch: 740; loss: 1.65; acc: 0.55
Batch: 760; loss: 1.61; acc: 0.55
Batch: 780; loss: 1.71; acc: 0.44
Train Epoch over. train_loss: 1.65; train_accuracy: 0.5 

3.750356336240657e-05
7.412813374685356e-06
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.44
Batch: 40; loss: 1.39; acc: 0.7
Batch: 60; loss: 1.62; acc: 0.53
Batch: 80; loss: 1.56; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.63; acc: 0.58
Val Epoch over. val_loss: 1.6017456548229145; val_accuracy: 0.5419984076433121 

The current subspace-distance is: 7.412813374685356e-06 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.55; acc: 0.58
Batch: 20; loss: 1.68; acc: 0.38
Batch: 40; loss: 1.64; acc: 0.52
Batch: 60; loss: 1.74; acc: 0.38
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.7; acc: 0.45
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.72; acc: 0.44
Batch: 160; loss: 1.6; acc: 0.58
Batch: 180; loss: 1.7; acc: 0.38
Batch: 200; loss: 1.64; acc: 0.45
Batch: 220; loss: 1.71; acc: 0.44
Batch: 240; loss: 1.56; acc: 0.55
Batch: 260; loss: 1.68; acc: 0.45
Batch: 280; loss: 1.68; acc: 0.52
Batch: 300; loss: 1.65; acc: 0.45
Batch: 320; loss: 1.61; acc: 0.64
Batch: 340; loss: 1.6; acc: 0.53
Batch: 360; loss: 1.66; acc: 0.36
Batch: 380; loss: 1.73; acc: 0.38
Batch: 400; loss: 1.63; acc: 0.55
Batch: 420; loss: 1.66; acc: 0.52
Batch: 440; loss: 1.5; acc: 0.64
Batch: 460; loss: 1.5; acc: 0.61
Batch: 480; loss: 1.61; acc: 0.58
Batch: 500; loss: 1.78; acc: 0.38
Batch: 520; loss: 1.75; acc: 0.47
Batch: 540; loss: 1.62; acc: 0.47
Batch: 560; loss: 1.65; acc: 0.55
Batch: 580; loss: 1.74; acc: 0.36
Batch: 600; loss: 1.67; acc: 0.52
Batch: 620; loss: 1.7; acc: 0.55
Batch: 640; loss: 1.68; acc: 0.48
Batch: 660; loss: 1.63; acc: 0.5
Batch: 680; loss: 1.78; acc: 0.38
Batch: 700; loss: 1.68; acc: 0.5
Batch: 720; loss: 1.73; acc: 0.52
Batch: 740; loss: 1.61; acc: 0.45
Batch: 760; loss: 1.73; acc: 0.44
Batch: 780; loss: 1.7; acc: 0.5
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.769368777284399e-05
8.310413250001147e-06
Batch: 0; loss: 1.58; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.39; acc: 0.7
Batch: 60; loss: 1.61; acc: 0.56
Batch: 80; loss: 1.55; acc: 0.48
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.59
Val Epoch over. val_loss: 1.5988552896839798; val_accuracy: 0.5402070063694268 

The current subspace-distance is: 8.310413250001147e-06 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.67; acc: 0.5
Batch: 40; loss: 1.82; acc: 0.36
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.65; acc: 0.56
Batch: 100; loss: 1.64; acc: 0.47
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.59
Batch: 160; loss: 1.71; acc: 0.47
Batch: 180; loss: 1.67; acc: 0.53
Batch: 200; loss: 1.65; acc: 0.44
Batch: 220; loss: 1.71; acc: 0.42
Batch: 240; loss: 1.73; acc: 0.44
Batch: 260; loss: 1.74; acc: 0.53
Batch: 280; loss: 1.73; acc: 0.41
Batch: 300; loss: 1.61; acc: 0.55
Batch: 320; loss: 1.73; acc: 0.41
Batch: 340; loss: 1.63; acc: 0.58
Batch: 360; loss: 1.78; acc: 0.38
Batch: 380; loss: 1.62; acc: 0.58
Batch: 400; loss: 1.65; acc: 0.55
Batch: 420; loss: 1.55; acc: 0.58
Batch: 440; loss: 1.76; acc: 0.47
Batch: 460; loss: 1.51; acc: 0.61
Batch: 480; loss: 1.64; acc: 0.48
Batch: 500; loss: 1.65; acc: 0.48
Batch: 520; loss: 1.64; acc: 0.5
Batch: 540; loss: 1.71; acc: 0.52
Batch: 560; loss: 1.67; acc: 0.48
Batch: 580; loss: 1.75; acc: 0.41
Batch: 600; loss: 1.63; acc: 0.48
Batch: 620; loss: 1.55; acc: 0.59
Batch: 640; loss: 1.7; acc: 0.45
Batch: 660; loss: 1.62; acc: 0.58
Batch: 680; loss: 1.68; acc: 0.42
Batch: 700; loss: 1.57; acc: 0.47
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.73; acc: 0.44
Batch: 760; loss: 1.69; acc: 0.5
Batch: 780; loss: 1.64; acc: 0.44
Train Epoch over. train_loss: 1.65; train_accuracy: 0.5 

3.871319131576456e-05
1.0735341675172094e-05
Batch: 0; loss: 1.6; acc: 0.56
Batch: 20; loss: 1.69; acc: 0.44
Batch: 40; loss: 1.39; acc: 0.72
Batch: 60; loss: 1.61; acc: 0.56
Batch: 80; loss: 1.56; acc: 0.48
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.77; acc: 0.42
Batch: 140; loss: 1.62; acc: 0.59
Val Epoch over. val_loss: 1.6031014676306659; val_accuracy: 0.5478702229299363 

The current subspace-distance is: 1.0735341675172094e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.61; acc: 0.48
Batch: 60; loss: 1.59; acc: 0.48
Batch: 80; loss: 1.61; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.53
Batch: 120; loss: 1.65; acc: 0.52
Batch: 140; loss: 1.68; acc: 0.45
Batch: 160; loss: 1.73; acc: 0.47
Batch: 180; loss: 1.72; acc: 0.47
Batch: 200; loss: 1.52; acc: 0.58
Batch: 220; loss: 1.7; acc: 0.45
Batch: 240; loss: 1.57; acc: 0.61
Batch: 260; loss: 1.64; acc: 0.56
Batch: 280; loss: 1.6; acc: 0.52
Batch: 300; loss: 1.7; acc: 0.47
Batch: 320; loss: 1.63; acc: 0.55
Batch: 340; loss: 1.65; acc: 0.45
Batch: 360; loss: 1.77; acc: 0.41
Batch: 380; loss: 1.71; acc: 0.47
Batch: 400; loss: 1.68; acc: 0.53
Batch: 420; loss: 1.57; acc: 0.53
Batch: 440; loss: 1.67; acc: 0.5
Batch: 460; loss: 1.75; acc: 0.36
Batch: 480; loss: 1.65; acc: 0.55
Batch: 500; loss: 1.57; acc: 0.58
Batch: 520; loss: 1.6; acc: 0.58
Batch: 540; loss: 1.67; acc: 0.47
Batch: 560; loss: 1.79; acc: 0.47
Batch: 580; loss: 1.7; acc: 0.5
Batch: 600; loss: 1.7; acc: 0.55
Batch: 620; loss: 1.58; acc: 0.47
Batch: 640; loss: 1.58; acc: 0.61
Batch: 660; loss: 1.52; acc: 0.69
Batch: 680; loss: 1.67; acc: 0.48
Batch: 700; loss: 1.67; acc: 0.48
Batch: 720; loss: 1.76; acc: 0.47
Batch: 740; loss: 1.7; acc: 0.55
Batch: 760; loss: 1.69; acc: 0.44
Batch: 780; loss: 1.68; acc: 0.53
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.8587033486692235e-05
1.008310300676385e-05
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.39; acc: 0.72
Batch: 60; loss: 1.6; acc: 0.58
Batch: 80; loss: 1.56; acc: 0.47
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.58
Val Epoch over. val_loss: 1.6036824504281306; val_accuracy: 0.5423964968152867 

The current subspace-distance is: 1.008310300676385e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.56
Batch: 20; loss: 1.8; acc: 0.34
Batch: 40; loss: 1.66; acc: 0.45
Batch: 60; loss: 1.64; acc: 0.53
Batch: 80; loss: 1.59; acc: 0.58
Batch: 100; loss: 1.69; acc: 0.47
Batch: 120; loss: 1.68; acc: 0.53
Batch: 140; loss: 1.63; acc: 0.53
Batch: 160; loss: 1.59; acc: 0.59
Batch: 180; loss: 1.6; acc: 0.52
Batch: 200; loss: 1.63; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.5
Batch: 240; loss: 1.6; acc: 0.56
Batch: 260; loss: 1.69; acc: 0.53
Batch: 280; loss: 1.66; acc: 0.45
Batch: 300; loss: 1.56; acc: 0.58
Batch: 320; loss: 1.71; acc: 0.52
Batch: 340; loss: 1.52; acc: 0.61
Batch: 360; loss: 1.61; acc: 0.55
Batch: 380; loss: 1.75; acc: 0.44
Batch: 400; loss: 1.66; acc: 0.47
Batch: 420; loss: 1.58; acc: 0.58
Batch: 440; loss: 1.47; acc: 0.67
Batch: 460; loss: 1.55; acc: 0.59
Batch: 480; loss: 1.68; acc: 0.55
Batch: 500; loss: 1.66; acc: 0.48
Batch: 520; loss: 1.59; acc: 0.55
Batch: 540; loss: 1.61; acc: 0.52
Batch: 560; loss: 1.51; acc: 0.66
Batch: 580; loss: 1.59; acc: 0.53
Batch: 600; loss: 1.64; acc: 0.55
Batch: 620; loss: 1.64; acc: 0.5
Batch: 640; loss: 1.7; acc: 0.44
Batch: 660; loss: 1.66; acc: 0.5
Batch: 680; loss: 1.63; acc: 0.47
Batch: 700; loss: 1.71; acc: 0.47
Batch: 720; loss: 1.74; acc: 0.45
Batch: 740; loss: 1.57; acc: 0.56
Batch: 760; loss: 1.68; acc: 0.41
Batch: 780; loss: 1.56; acc: 0.58
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.888251376338303e-05
9.29977250052616e-06
Batch: 0; loss: 1.58; acc: 0.58
Batch: 20; loss: 1.7; acc: 0.44
Batch: 40; loss: 1.38; acc: 0.7
Batch: 60; loss: 1.6; acc: 0.55
Batch: 80; loss: 1.54; acc: 0.47
Batch: 100; loss: 1.6; acc: 0.58
Batch: 120; loss: 1.77; acc: 0.45
Batch: 140; loss: 1.61; acc: 0.56
Val Epoch over. val_loss: 1.5972747294006833; val_accuracy: 0.5411027070063694 

The current subspace-distance is: 9.29977250052616e-06 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.5
Batch: 20; loss: 1.58; acc: 0.53
Batch: 40; loss: 1.6; acc: 0.52
Batch: 60; loss: 1.69; acc: 0.42
Batch: 80; loss: 1.63; acc: 0.55
Batch: 100; loss: 1.61; acc: 0.5
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.74; acc: 0.45
Batch: 160; loss: 1.56; acc: 0.59
Batch: 180; loss: 1.75; acc: 0.44
Batch: 200; loss: 1.71; acc: 0.53
Batch: 220; loss: 1.54; acc: 0.61
Batch: 240; loss: 1.68; acc: 0.44
Batch: 260; loss: 1.71; acc: 0.44
Batch: 280; loss: 1.65; acc: 0.55
Batch: 300; loss: 1.73; acc: 0.48
Batch: 320; loss: 1.58; acc: 0.56
Batch: 340; loss: 1.62; acc: 0.56
Batch: 360; loss: 1.67; acc: 0.47
Batch: 380; loss: 1.73; acc: 0.52
Batch: 400; loss: 1.67; acc: 0.47
Batch: 420; loss: 1.64; acc: 0.55
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.65; acc: 0.48
Batch: 480; loss: 1.68; acc: 0.47
Batch: 500; loss: 1.6; acc: 0.55
Batch: 520; loss: 1.73; acc: 0.47
Batch: 540; loss: 1.62; acc: 0.47
Batch: 560; loss: 1.68; acc: 0.45
Batch: 580; loss: 1.58; acc: 0.56
Batch: 600; loss: 1.63; acc: 0.58
Batch: 620; loss: 1.62; acc: 0.55
Batch: 640; loss: 1.7; acc: 0.48
Batch: 660; loss: 1.5; acc: 0.56
Batch: 680; loss: 1.65; acc: 0.45
Batch: 700; loss: 1.66; acc: 0.45
Batch: 720; loss: 1.69; acc: 0.47
Batch: 740; loss: 1.55; acc: 0.61
Batch: 760; loss: 1.6; acc: 0.59
Batch: 780; loss: 1.66; acc: 0.5
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.8334914279403165e-05
9.011634574562777e-06
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.7; acc: 0.44
Batch: 40; loss: 1.39; acc: 0.72
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.59
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.61
Val Epoch over. val_loss: 1.607722797211568; val_accuracy: 0.5383160828025477 

The current subspace-distance is: 9.011634574562777e-06 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.66; acc: 0.47
Batch: 60; loss: 1.65; acc: 0.47
Batch: 80; loss: 1.61; acc: 0.52
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.66; acc: 0.47
Batch: 140; loss: 1.61; acc: 0.45
Batch: 160; loss: 1.64; acc: 0.53
Batch: 180; loss: 1.72; acc: 0.47
Batch: 200; loss: 1.71; acc: 0.52
Batch: 220; loss: 1.5; acc: 0.64
Batch: 240; loss: 1.74; acc: 0.53
Batch: 260; loss: 1.6; acc: 0.53
Batch: 280; loss: 1.71; acc: 0.5
Batch: 300; loss: 1.44; acc: 0.61
Batch: 320; loss: 1.68; acc: 0.44
Batch: 340; loss: 1.5; acc: 0.62
Batch: 360; loss: 1.72; acc: 0.48
Batch: 380; loss: 1.52; acc: 0.64
Batch: 400; loss: 1.73; acc: 0.56
Batch: 420; loss: 1.66; acc: 0.5
Batch: 440; loss: 1.61; acc: 0.47
Batch: 460; loss: 1.59; acc: 0.58
Batch: 480; loss: 1.64; acc: 0.48
Batch: 500; loss: 1.53; acc: 0.55
Batch: 520; loss: 1.66; acc: 0.44
Batch: 540; loss: 1.64; acc: 0.52
Batch: 560; loss: 1.65; acc: 0.45
Batch: 580; loss: 1.61; acc: 0.5
Batch: 600; loss: 1.59; acc: 0.56
Batch: 620; loss: 1.59; acc: 0.56
Batch: 640; loss: 1.62; acc: 0.55
Batch: 660; loss: 1.68; acc: 0.48
Batch: 680; loss: 1.55; acc: 0.56
Batch: 700; loss: 1.62; acc: 0.5
Batch: 720; loss: 1.58; acc: 0.61
Batch: 740; loss: 1.69; acc: 0.47
Batch: 760; loss: 1.7; acc: 0.53
Batch: 780; loss: 1.68; acc: 0.45
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.961906986660324e-05
1.0410567483631894e-05
Batch: 0; loss: 1.6; acc: 0.56
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.38; acc: 0.72
Batch: 60; loss: 1.61; acc: 0.56
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.78; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.59
Val Epoch over. val_loss: 1.606264440876663; val_accuracy: 0.5310509554140127 

The current subspace-distance is: 1.0410567483631894e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.48; acc: 0.66
Batch: 40; loss: 1.7; acc: 0.56
Batch: 60; loss: 1.6; acc: 0.58
Batch: 80; loss: 1.74; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.52
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.66; acc: 0.5
Batch: 160; loss: 1.77; acc: 0.47
Batch: 180; loss: 1.47; acc: 0.66
Batch: 200; loss: 1.67; acc: 0.47
Batch: 220; loss: 1.78; acc: 0.44
Batch: 240; loss: 1.67; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.55
Batch: 280; loss: 1.69; acc: 0.52
Batch: 300; loss: 1.65; acc: 0.52
Batch: 320; loss: 1.6; acc: 0.56
Batch: 340; loss: 1.65; acc: 0.52
Batch: 360; loss: 1.79; acc: 0.39
Batch: 380; loss: 1.55; acc: 0.59
Batch: 400; loss: 1.57; acc: 0.52
Batch: 420; loss: 1.7; acc: 0.48
Batch: 440; loss: 1.67; acc: 0.55
Batch: 460; loss: 1.7; acc: 0.52
Batch: 480; loss: 1.69; acc: 0.53
Batch: 500; loss: 1.63; acc: 0.47
Batch: 520; loss: 1.62; acc: 0.59
Batch: 540; loss: 1.64; acc: 0.47
Batch: 560; loss: 1.68; acc: 0.5
Batch: 580; loss: 1.64; acc: 0.53
Batch: 600; loss: 1.63; acc: 0.53
Batch: 620; loss: 1.72; acc: 0.52
Batch: 640; loss: 1.65; acc: 0.53
Batch: 660; loss: 1.58; acc: 0.52
Batch: 680; loss: 1.67; acc: 0.45
Batch: 700; loss: 1.68; acc: 0.5
Batch: 720; loss: 1.69; acc: 0.45
Batch: 740; loss: 1.6; acc: 0.52
Batch: 760; loss: 1.56; acc: 0.53
Batch: 780; loss: 1.66; acc: 0.53
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.876283881254494e-05
1.0817892871273216e-05
Batch: 0; loss: 1.6; acc: 0.59
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.38; acc: 0.72
Batch: 60; loss: 1.6; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.77; acc: 0.45
Batch: 140; loss: 1.63; acc: 0.59
Val Epoch over. val_loss: 1.6051379085346391; val_accuracy: 0.5352308917197452 

The current subspace-distance is: 1.0817892871273216e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.55; acc: 0.53
Batch: 20; loss: 1.72; acc: 0.5
Batch: 40; loss: 1.69; acc: 0.47
Batch: 60; loss: 1.66; acc: 0.52
Batch: 80; loss: 1.69; acc: 0.47
Batch: 100; loss: 1.68; acc: 0.5
Batch: 120; loss: 1.82; acc: 0.44
Batch: 140; loss: 1.67; acc: 0.45
Batch: 160; loss: 1.58; acc: 0.58
Batch: 180; loss: 1.54; acc: 0.56
Batch: 200; loss: 1.64; acc: 0.5
Batch: 220; loss: 1.62; acc: 0.48
Batch: 240; loss: 1.59; acc: 0.52
Batch: 260; loss: 1.62; acc: 0.58
Batch: 280; loss: 1.57; acc: 0.59
Batch: 300; loss: 1.61; acc: 0.53
Batch: 320; loss: 1.61; acc: 0.56
Batch: 340; loss: 1.64; acc: 0.52
Batch: 360; loss: 1.67; acc: 0.55
Batch: 380; loss: 1.64; acc: 0.5
Batch: 400; loss: 1.74; acc: 0.47
Batch: 420; loss: 1.7; acc: 0.5
Batch: 440; loss: 1.73; acc: 0.41
Batch: 460; loss: 1.53; acc: 0.59
Batch: 480; loss: 1.58; acc: 0.56
Batch: 500; loss: 1.8; acc: 0.39
Batch: 520; loss: 1.56; acc: 0.59
Batch: 540; loss: 1.59; acc: 0.48
Batch: 560; loss: 1.67; acc: 0.44
Batch: 580; loss: 1.67; acc: 0.53
Batch: 600; loss: 1.56; acc: 0.58
Batch: 620; loss: 1.57; acc: 0.5
Batch: 640; loss: 1.7; acc: 0.45
Batch: 660; loss: 1.59; acc: 0.58
Batch: 680; loss: 1.76; acc: 0.44
Batch: 700; loss: 1.68; acc: 0.42
Batch: 720; loss: 1.71; acc: 0.39
Batch: 740; loss: 1.72; acc: 0.45
Batch: 760; loss: 1.76; acc: 0.39
Batch: 780; loss: 1.78; acc: 0.42
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.8688383938279e-05
1.026498921419261e-05
Batch: 0; loss: 1.61; acc: 0.56
Batch: 20; loss: 1.69; acc: 0.42
Batch: 40; loss: 1.38; acc: 0.7
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.61
Batch: 120; loss: 1.76; acc: 0.42
Batch: 140; loss: 1.63; acc: 0.59
Val Epoch over. val_loss: 1.6055686807936165; val_accuracy: 0.5381170382165605 

The current subspace-distance is: 1.026498921419261e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.58
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.58; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.66; acc: 0.56
Batch: 120; loss: 1.53; acc: 0.52
Batch: 140; loss: 1.62; acc: 0.52
Batch: 160; loss: 1.63; acc: 0.55
Batch: 180; loss: 1.72; acc: 0.53
Batch: 200; loss: 1.69; acc: 0.45
Batch: 220; loss: 1.65; acc: 0.53
Batch: 240; loss: 1.82; acc: 0.47
Batch: 260; loss: 1.61; acc: 0.53
Batch: 280; loss: 1.6; acc: 0.53
Batch: 300; loss: 1.7; acc: 0.39
Batch: 320; loss: 1.67; acc: 0.44
Batch: 340; loss: 1.6; acc: 0.56
Batch: 360; loss: 1.59; acc: 0.53
Batch: 380; loss: 1.64; acc: 0.45
Batch: 400; loss: 1.76; acc: 0.34
Batch: 420; loss: 1.65; acc: 0.5
Batch: 440; loss: 1.54; acc: 0.59
Batch: 460; loss: 1.58; acc: 0.5
Batch: 480; loss: 1.65; acc: 0.48
Batch: 500; loss: 1.73; acc: 0.45
Batch: 520; loss: 1.76; acc: 0.44
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.51; acc: 0.58
Batch: 580; loss: 1.61; acc: 0.45
Batch: 600; loss: 1.68; acc: 0.5
Batch: 620; loss: 1.57; acc: 0.55
Batch: 640; loss: 1.61; acc: 0.58
Batch: 660; loss: 1.59; acc: 0.53
Batch: 680; loss: 1.65; acc: 0.52
Batch: 700; loss: 1.53; acc: 0.66
Batch: 720; loss: 1.63; acc: 0.53
Batch: 740; loss: 1.68; acc: 0.55
Batch: 760; loss: 1.64; acc: 0.42
Batch: 780; loss: 1.52; acc: 0.58
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.8436512113548815e-05
9.757625775819179e-06
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.7
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.45
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.76; acc: 0.47
Batch: 140; loss: 1.61; acc: 0.58
Val Epoch over. val_loss: 1.600809809508597; val_accuracy: 0.5415007961783439 

The current subspace-distance is: 9.757625775819179e-06 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.6; acc: 0.53
Batch: 60; loss: 1.66; acc: 0.48
Batch: 80; loss: 1.66; acc: 0.5
Batch: 100; loss: 1.73; acc: 0.47
Batch: 120; loss: 1.61; acc: 0.53
Batch: 140; loss: 1.66; acc: 0.56
Batch: 160; loss: 1.81; acc: 0.36
Batch: 180; loss: 1.67; acc: 0.52
Batch: 200; loss: 1.65; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.53
Batch: 240; loss: 1.5; acc: 0.59
Batch: 260; loss: 1.6; acc: 0.48
Batch: 280; loss: 1.61; acc: 0.58
Batch: 300; loss: 1.74; acc: 0.47
Batch: 320; loss: 1.65; acc: 0.5
Batch: 340; loss: 1.66; acc: 0.5
Batch: 360; loss: 1.63; acc: 0.5
Batch: 380; loss: 1.6; acc: 0.48
Batch: 400; loss: 1.55; acc: 0.56
Batch: 420; loss: 1.6; acc: 0.58
Batch: 440; loss: 1.58; acc: 0.58
Batch: 460; loss: 1.7; acc: 0.55
Batch: 480; loss: 1.61; acc: 0.53
Batch: 500; loss: 1.55; acc: 0.66
Batch: 520; loss: 1.64; acc: 0.45
Batch: 540; loss: 1.62; acc: 0.58
Batch: 560; loss: 1.68; acc: 0.44
Batch: 580; loss: 1.6; acc: 0.56
Batch: 600; loss: 1.67; acc: 0.56
Batch: 620; loss: 1.57; acc: 0.53
Batch: 640; loss: 1.62; acc: 0.53
Batch: 660; loss: 1.51; acc: 0.66
Batch: 680; loss: 1.7; acc: 0.44
Batch: 700; loss: 1.64; acc: 0.55
Batch: 720; loss: 1.54; acc: 0.56
Batch: 740; loss: 1.83; acc: 0.33
Batch: 760; loss: 1.68; acc: 0.5
Batch: 780; loss: 1.5; acc: 0.61
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.874681715387851e-05
9.043071258929558e-06
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.72
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.6; acc: 0.58
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.59
Val Epoch over. val_loss: 1.6020752016905766; val_accuracy: 0.5341361464968153 

The current subspace-distance is: 9.043071258929558e-06 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.74; acc: 0.45
Batch: 20; loss: 1.58; acc: 0.5
Batch: 40; loss: 1.63; acc: 0.53
Batch: 60; loss: 1.67; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.47
Batch: 100; loss: 1.58; acc: 0.62
Batch: 120; loss: 1.51; acc: 0.66
Batch: 140; loss: 1.67; acc: 0.53
Batch: 160; loss: 1.68; acc: 0.58
Batch: 180; loss: 1.66; acc: 0.45
Batch: 200; loss: 1.71; acc: 0.47
Batch: 220; loss: 1.54; acc: 0.61
Batch: 240; loss: 1.71; acc: 0.45
Batch: 260; loss: 1.51; acc: 0.64
Batch: 280; loss: 1.67; acc: 0.47
Batch: 300; loss: 1.76; acc: 0.39
Batch: 320; loss: 1.62; acc: 0.52
Batch: 340; loss: 1.67; acc: 0.5
Batch: 360; loss: 1.8; acc: 0.36
Batch: 380; loss: 1.66; acc: 0.5
Batch: 400; loss: 1.79; acc: 0.34
Batch: 420; loss: 1.64; acc: 0.55
Batch: 440; loss: 1.58; acc: 0.58
Batch: 460; loss: 1.63; acc: 0.48
Batch: 480; loss: 1.72; acc: 0.48
Batch: 500; loss: 1.71; acc: 0.53
Batch: 520; loss: 1.76; acc: 0.47
Batch: 540; loss: 1.58; acc: 0.5
Batch: 560; loss: 1.7; acc: 0.47
Batch: 580; loss: 1.51; acc: 0.56
Batch: 600; loss: 1.66; acc: 0.47
Batch: 620; loss: 1.6; acc: 0.48
Batch: 640; loss: 1.7; acc: 0.55
Batch: 660; loss: 1.64; acc: 0.47
Batch: 680; loss: 1.67; acc: 0.55
Batch: 700; loss: 1.53; acc: 0.62
Batch: 720; loss: 1.6; acc: 0.59
Batch: 740; loss: 1.59; acc: 0.62
Batch: 760; loss: 1.63; acc: 0.53
Batch: 780; loss: 1.69; acc: 0.44
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.9308040868490934e-05
1.2871543731307611e-05
Batch: 0; loss: 1.6; acc: 0.56
Batch: 20; loss: 1.7; acc: 0.44
Batch: 40; loss: 1.37; acc: 0.73
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.47
Batch: 100; loss: 1.59; acc: 0.59
Batch: 120; loss: 1.76; acc: 0.44
Batch: 140; loss: 1.61; acc: 0.56
Val Epoch over. val_loss: 1.5962615164981526; val_accuracy: 0.5419984076433121 

The current subspace-distance is: 1.2871543731307611e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.55
Batch: 20; loss: 1.58; acc: 0.52
Batch: 40; loss: 1.66; acc: 0.59
Batch: 60; loss: 1.74; acc: 0.52
Batch: 80; loss: 1.56; acc: 0.53
Batch: 100; loss: 1.62; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.69; acc: 0.47
Batch: 160; loss: 1.64; acc: 0.5
Batch: 180; loss: 1.72; acc: 0.45
Batch: 200; loss: 1.67; acc: 0.44
Batch: 220; loss: 1.66; acc: 0.55
Batch: 240; loss: 1.68; acc: 0.42
Batch: 260; loss: 1.72; acc: 0.52
Batch: 280; loss: 1.63; acc: 0.48
Batch: 300; loss: 1.65; acc: 0.39
Batch: 320; loss: 1.67; acc: 0.5
Batch: 340; loss: 1.7; acc: 0.48
Batch: 360; loss: 1.69; acc: 0.45
Batch: 380; loss: 1.63; acc: 0.55
Batch: 400; loss: 1.71; acc: 0.47
Batch: 420; loss: 1.71; acc: 0.52
Batch: 440; loss: 1.72; acc: 0.47
Batch: 460; loss: 1.63; acc: 0.44
Batch: 480; loss: 1.77; acc: 0.34
Batch: 500; loss: 1.67; acc: 0.52
Batch: 520; loss: 1.61; acc: 0.53
Batch: 540; loss: 1.73; acc: 0.42
Batch: 560; loss: 1.62; acc: 0.58
Batch: 580; loss: 1.58; acc: 0.48
Batch: 600; loss: 1.61; acc: 0.5
Batch: 620; loss: 1.58; acc: 0.56
Batch: 640; loss: 1.66; acc: 0.42
Batch: 660; loss: 1.61; acc: 0.56
Batch: 680; loss: 1.67; acc: 0.45
Batch: 700; loss: 1.63; acc: 0.52
Batch: 720; loss: 1.63; acc: 0.47
Batch: 740; loss: 1.62; acc: 0.53
Batch: 760; loss: 1.54; acc: 0.59
Batch: 780; loss: 1.65; acc: 0.53
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.8713635149179026e-05
9.788932402443606e-06
Batch: 0; loss: 1.61; acc: 0.56
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.7
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.56
Val Epoch over. val_loss: 1.5992924688727992; val_accuracy: 0.5401074840764332 

The current subspace-distance is: 9.788932402443606e-06 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.52
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.57; acc: 0.59
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.47
Batch: 100; loss: 1.69; acc: 0.45
Batch: 120; loss: 1.7; acc: 0.47
Batch: 140; loss: 1.76; acc: 0.42
Batch: 160; loss: 1.7; acc: 0.44
Batch: 180; loss: 1.64; acc: 0.55
Batch: 200; loss: 1.72; acc: 0.48
Batch: 220; loss: 1.71; acc: 0.41
Batch: 240; loss: 1.74; acc: 0.41
Batch: 260; loss: 1.57; acc: 0.56
Batch: 280; loss: 1.66; acc: 0.52
Batch: 300; loss: 1.64; acc: 0.48
Batch: 320; loss: 1.68; acc: 0.48
Batch: 340; loss: 1.58; acc: 0.55
Batch: 360; loss: 1.58; acc: 0.59
Batch: 380; loss: 1.69; acc: 0.52
Batch: 400; loss: 1.69; acc: 0.48
Batch: 420; loss: 1.7; acc: 0.41
Batch: 440; loss: 1.83; acc: 0.41
Batch: 460; loss: 1.58; acc: 0.52
Batch: 480; loss: 1.67; acc: 0.45
Batch: 500; loss: 1.59; acc: 0.55
Batch: 520; loss: 1.6; acc: 0.61
Batch: 540; loss: 1.58; acc: 0.58
Batch: 560; loss: 1.63; acc: 0.61
Batch: 580; loss: 1.73; acc: 0.47
Batch: 600; loss: 1.59; acc: 0.5
Batch: 620; loss: 1.64; acc: 0.47
Batch: 640; loss: 1.74; acc: 0.39
Batch: 660; loss: 1.74; acc: 0.47
Batch: 680; loss: 1.76; acc: 0.42
Batch: 700; loss: 1.56; acc: 0.62
Batch: 720; loss: 1.49; acc: 0.62
Batch: 740; loss: 1.67; acc: 0.47
Batch: 760; loss: 1.67; acc: 0.48
Batch: 780; loss: 1.71; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.8038662751205266e-05
9.045086699188687e-06
Batch: 0; loss: 1.6; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.44
Batch: 40; loss: 1.37; acc: 0.73
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.47
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.77; acc: 0.47
Batch: 140; loss: 1.61; acc: 0.53
Val Epoch over. val_loss: 1.600005700330066; val_accuracy: 0.5405055732484076 

The current subspace-distance is: 9.045086699188687e-06 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.79; acc: 0.39
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.71; acc: 0.44
Batch: 60; loss: 1.75; acc: 0.5
Batch: 80; loss: 1.63; acc: 0.58
Batch: 100; loss: 1.66; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.42
Batch: 140; loss: 1.67; acc: 0.45
Batch: 160; loss: 1.57; acc: 0.53
Batch: 180; loss: 1.47; acc: 0.62
Batch: 200; loss: 1.77; acc: 0.38
Batch: 220; loss: 1.63; acc: 0.53
Batch: 240; loss: 1.55; acc: 0.59
Batch: 260; loss: 1.64; acc: 0.5
Batch: 280; loss: 1.6; acc: 0.52
Batch: 300; loss: 1.61; acc: 0.45
Batch: 320; loss: 1.68; acc: 0.48
Batch: 340; loss: 1.68; acc: 0.39
Batch: 360; loss: 1.72; acc: 0.47
Batch: 380; loss: 1.62; acc: 0.52
Batch: 400; loss: 1.76; acc: 0.45
Batch: 420; loss: 1.73; acc: 0.45
Batch: 440; loss: 1.67; acc: 0.53
Batch: 460; loss: 1.56; acc: 0.55
Batch: 480; loss: 1.57; acc: 0.55
Batch: 500; loss: 1.67; acc: 0.45
Batch: 520; loss: 1.77; acc: 0.38
Batch: 540; loss: 1.6; acc: 0.52
Batch: 560; loss: 1.82; acc: 0.38
Batch: 580; loss: 1.81; acc: 0.44
Batch: 600; loss: 1.69; acc: 0.48
Batch: 620; loss: 1.58; acc: 0.53
Batch: 640; loss: 1.67; acc: 0.52
Batch: 660; loss: 1.55; acc: 0.56
Batch: 680; loss: 1.76; acc: 0.41
Batch: 700; loss: 1.72; acc: 0.48
Batch: 720; loss: 1.51; acc: 0.62
Batch: 740; loss: 1.64; acc: 0.48
Batch: 760; loss: 1.69; acc: 0.39
Batch: 780; loss: 1.61; acc: 0.55
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.856206967611797e-05
9.510492418485228e-06
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.36; acc: 0.72
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.53; acc: 0.48
Batch: 100; loss: 1.59; acc: 0.59
Batch: 120; loss: 1.76; acc: 0.45
Batch: 140; loss: 1.61; acc: 0.55
Val Epoch over. val_loss: 1.5937339439513578; val_accuracy: 0.5363256369426752 

The current subspace-distance is: 9.510492418485228e-06 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.38
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.57; acc: 0.58
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.6; acc: 0.55
Batch: 100; loss: 1.66; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.39
Batch: 140; loss: 1.58; acc: 0.61
Batch: 160; loss: 1.69; acc: 0.44
Batch: 180; loss: 1.56; acc: 0.64
Batch: 200; loss: 1.64; acc: 0.53
Batch: 220; loss: 1.67; acc: 0.48
Batch: 240; loss: 1.68; acc: 0.38
Batch: 260; loss: 1.73; acc: 0.45
Batch: 280; loss: 1.65; acc: 0.53
Batch: 300; loss: 1.69; acc: 0.52
Batch: 320; loss: 1.56; acc: 0.56
Batch: 340; loss: 1.61; acc: 0.47
Batch: 360; loss: 1.64; acc: 0.48
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.57; acc: 0.56
Batch: 420; loss: 1.69; acc: 0.48
Batch: 440; loss: 1.56; acc: 0.58
Batch: 460; loss: 1.68; acc: 0.5
Batch: 480; loss: 1.56; acc: 0.5
Batch: 500; loss: 1.64; acc: 0.5
Batch: 520; loss: 1.57; acc: 0.5
Batch: 540; loss: 1.65; acc: 0.52
Batch: 560; loss: 1.58; acc: 0.53
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.71; acc: 0.52
Batch: 620; loss: 1.64; acc: 0.55
Batch: 640; loss: 1.71; acc: 0.45
Batch: 660; loss: 1.75; acc: 0.45
Batch: 680; loss: 1.68; acc: 0.5
Batch: 700; loss: 1.64; acc: 0.45
Batch: 720; loss: 1.74; acc: 0.52
Batch: 740; loss: 1.69; acc: 0.52
Batch: 760; loss: 1.76; acc: 0.45
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.872918023262173e-05
9.171943020191975e-06
Batch: 0; loss: 1.62; acc: 0.58
Batch: 20; loss: 1.69; acc: 0.44
Batch: 40; loss: 1.37; acc: 0.72
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.77; acc: 0.45
Batch: 140; loss: 1.62; acc: 0.55
Val Epoch over. val_loss: 1.6031493077612227; val_accuracy: 0.541202229299363 

The current subspace-distance is: 9.171943020191975e-06 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.76; acc: 0.5
Batch: 20; loss: 1.63; acc: 0.47
Batch: 40; loss: 1.65; acc: 0.47
Batch: 60; loss: 1.65; acc: 0.52
Batch: 80; loss: 1.57; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.55
Batch: 120; loss: 1.76; acc: 0.44
Batch: 140; loss: 1.71; acc: 0.44
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.54; acc: 0.58
Batch: 200; loss: 1.7; acc: 0.45
Batch: 220; loss: 1.59; acc: 0.5
Batch: 240; loss: 1.62; acc: 0.55
Batch: 260; loss: 1.6; acc: 0.52
Batch: 280; loss: 1.63; acc: 0.5
Batch: 300; loss: 1.76; acc: 0.44
Batch: 320; loss: 1.68; acc: 0.48
Batch: 340; loss: 1.68; acc: 0.48
Batch: 360; loss: 1.72; acc: 0.45
Batch: 380; loss: 1.71; acc: 0.53
Batch: 400; loss: 1.71; acc: 0.52
Batch: 420; loss: 1.68; acc: 0.5
Batch: 440; loss: 1.81; acc: 0.39
Batch: 460; loss: 1.58; acc: 0.58
Batch: 480; loss: 1.64; acc: 0.52
Batch: 500; loss: 1.66; acc: 0.41
Batch: 520; loss: 1.66; acc: 0.53
Batch: 540; loss: 1.62; acc: 0.52
Batch: 560; loss: 1.58; acc: 0.55
Batch: 580; loss: 1.49; acc: 0.62
Batch: 600; loss: 1.57; acc: 0.55
Batch: 620; loss: 1.75; acc: 0.48
Batch: 640; loss: 1.71; acc: 0.48
Batch: 660; loss: 1.59; acc: 0.58
Batch: 680; loss: 1.54; acc: 0.48
Batch: 700; loss: 1.75; acc: 0.44
Batch: 720; loss: 1.5; acc: 0.58
Batch: 740; loss: 1.71; acc: 0.53
Batch: 760; loss: 1.82; acc: 0.31
Batch: 780; loss: 1.82; acc: 0.3
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.825523526757024e-05
8.993649316835217e-06
Batch: 0; loss: 1.63; acc: 0.56
Batch: 20; loss: 1.71; acc: 0.41
Batch: 40; loss: 1.38; acc: 0.72
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.48
Batch: 100; loss: 1.61; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.45
Batch: 140; loss: 1.61; acc: 0.58
Val Epoch over. val_loss: 1.6085304583713507; val_accuracy: 0.5347332802547771 

The current subspace-distance is: 8.993649316835217e-06 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.5
Batch: 20; loss: 1.72; acc: 0.41
Batch: 40; loss: 1.65; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.62; acc: 0.52
Batch: 100; loss: 1.64; acc: 0.59
Batch: 120; loss: 1.7; acc: 0.52
Batch: 140; loss: 1.67; acc: 0.52
Batch: 160; loss: 1.6; acc: 0.47
Batch: 180; loss: 1.72; acc: 0.41
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.56; acc: 0.5
Batch: 240; loss: 1.62; acc: 0.52
Batch: 260; loss: 1.73; acc: 0.52
Batch: 280; loss: 1.59; acc: 0.5
Batch: 300; loss: 1.55; acc: 0.53
Batch: 320; loss: 1.67; acc: 0.44
Batch: 340; loss: 1.52; acc: 0.58
Batch: 360; loss: 1.48; acc: 0.64
Batch: 380; loss: 1.63; acc: 0.47
Batch: 400; loss: 1.63; acc: 0.45
Batch: 420; loss: 1.62; acc: 0.53
Batch: 440; loss: 1.66; acc: 0.47
Batch: 460; loss: 1.55; acc: 0.53
Batch: 480; loss: 1.63; acc: 0.58
Batch: 500; loss: 1.53; acc: 0.62
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.61; acc: 0.5
Batch: 560; loss: 1.58; acc: 0.53
Batch: 580; loss: 1.59; acc: 0.53
Batch: 600; loss: 1.62; acc: 0.53
Batch: 620; loss: 1.74; acc: 0.52
Batch: 640; loss: 1.87; acc: 0.36
Batch: 660; loss: 1.57; acc: 0.53
Batch: 680; loss: 1.69; acc: 0.5
Batch: 700; loss: 1.64; acc: 0.48
Batch: 720; loss: 1.51; acc: 0.61
Batch: 740; loss: 1.61; acc: 0.52
Batch: 760; loss: 1.67; acc: 0.52
Batch: 780; loss: 1.58; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.9149974327301607e-05
1.0563134310359601e-05
Batch: 0; loss: 1.62; acc: 0.56
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.73
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.48
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.77; acc: 0.45
Batch: 140; loss: 1.61; acc: 0.56
Val Epoch over. val_loss: 1.5993617879357307; val_accuracy: 0.5418988853503185 

The current subspace-distance is: 1.0563134310359601e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.58; acc: 0.56
Batch: 60; loss: 1.73; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.69; acc: 0.45
Batch: 120; loss: 1.52; acc: 0.64
Batch: 140; loss: 1.65; acc: 0.45
Batch: 160; loss: 1.64; acc: 0.53
Batch: 180; loss: 1.61; acc: 0.45
Batch: 200; loss: 1.54; acc: 0.62
Batch: 220; loss: 1.64; acc: 0.53
Batch: 240; loss: 1.67; acc: 0.55
Batch: 260; loss: 1.73; acc: 0.34
Batch: 280; loss: 1.6; acc: 0.55
Batch: 300; loss: 1.74; acc: 0.41
Batch: 320; loss: 1.69; acc: 0.44
Batch: 340; loss: 1.57; acc: 0.56
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.59; acc: 0.61
Batch: 400; loss: 1.67; acc: 0.47
Batch: 420; loss: 1.66; acc: 0.48
Batch: 440; loss: 1.69; acc: 0.5
Batch: 460; loss: 1.55; acc: 0.53
Batch: 480; loss: 1.71; acc: 0.39
Batch: 500; loss: 1.56; acc: 0.5
Batch: 520; loss: 1.66; acc: 0.48
Batch: 540; loss: 1.66; acc: 0.5
Batch: 560; loss: 1.66; acc: 0.45
Batch: 580; loss: 1.58; acc: 0.59
Batch: 600; loss: 1.6; acc: 0.55
Batch: 620; loss: 1.55; acc: 0.61
Batch: 640; loss: 1.71; acc: 0.47
Batch: 660; loss: 1.74; acc: 0.44
Batch: 680; loss: 1.59; acc: 0.52
Batch: 700; loss: 1.66; acc: 0.47
Batch: 720; loss: 1.66; acc: 0.45
Batch: 740; loss: 1.74; acc: 0.55
Batch: 760; loss: 1.65; acc: 0.52
Batch: 780; loss: 1.56; acc: 0.47
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.9553440728923306e-05
1.1540295417944435e-05
Batch: 0; loss: 1.6; acc: 0.56
Batch: 20; loss: 1.69; acc: 0.42
Batch: 40; loss: 1.36; acc: 0.7
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.48
Batch: 100; loss: 1.59; acc: 0.59
Batch: 120; loss: 1.76; acc: 0.44
Batch: 140; loss: 1.61; acc: 0.55
Val Epoch over. val_loss: 1.590468885032994; val_accuracy: 0.5450835987261147 

The current subspace-distance is: 1.1540295417944435e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 7.203731671848972

The number of parameters is: 274066

The number of individual parameters is:

58
580
58
58
87
50460
87
87
173
150510
173
173
64
66432
64
64
4096
64
640
10
64
64

nonzero elements in E: 27406597
elements in E: 27406600
fraction nonzero: 0.9999998905373159
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.49; acc: 0.05
Batch: 20; loss: 2.48; acc: 0.05
Batch: 40; loss: 2.35; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.14; acc: 0.22
Batch: 140; loss: 2.13; acc: 0.23
Batch: 160; loss: 2.13; acc: 0.23
Batch: 180; loss: 2.13; acc: 0.28
Batch: 200; loss: 2.1; acc: 0.16
Batch: 220; loss: 2.03; acc: 0.34
Batch: 240; loss: 2.13; acc: 0.23
Batch: 260; loss: 2.16; acc: 0.33
Batch: 280; loss: 2.03; acc: 0.31
Batch: 300; loss: 1.99; acc: 0.34
Batch: 320; loss: 2.05; acc: 0.28
Batch: 340; loss: 1.87; acc: 0.47
Batch: 360; loss: 1.98; acc: 0.34
Batch: 380; loss: 1.84; acc: 0.48
Batch: 400; loss: 1.9; acc: 0.45
Batch: 420; loss: 1.88; acc: 0.52
Batch: 440; loss: 1.85; acc: 0.47
Batch: 460; loss: 1.89; acc: 0.41
Batch: 480; loss: 1.88; acc: 0.47
Batch: 500; loss: 1.81; acc: 0.55
Batch: 520; loss: 1.87; acc: 0.47
Batch: 540; loss: 1.87; acc: 0.53
Batch: 560; loss: 1.98; acc: 0.42
Batch: 580; loss: 1.78; acc: 0.48
Batch: 600; loss: 1.83; acc: 0.55
Batch: 620; loss: 1.68; acc: 0.66
Batch: 640; loss: 1.78; acc: 0.56
Batch: 660; loss: 1.84; acc: 0.5
Batch: 680; loss: 1.75; acc: 0.53
Batch: 700; loss: 1.76; acc: 0.64
Batch: 720; loss: 1.79; acc: 0.52
Batch: 740; loss: 1.76; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.62
Batch: 780; loss: 1.81; acc: 0.44
Train Epoch over. train_loss: 1.97; train_accuracy: 0.38 

5.87501963309478e-05
5.2388484618859366e-05
Batch: 0; loss: 1.79; acc: 0.52
Batch: 20; loss: 1.94; acc: 0.47
Batch: 40; loss: 1.5; acc: 0.73
Batch: 60; loss: 1.66; acc: 0.62
Batch: 80; loss: 1.62; acc: 0.64
Batch: 100; loss: 1.69; acc: 0.61
Batch: 120; loss: 1.85; acc: 0.42
Batch: 140; loss: 1.63; acc: 0.58
Val Epoch over. val_loss: 1.7246213111148518; val_accuracy: 0.5566281847133758 

The current subspace-distance is: 5.2388484618859366e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.77; acc: 0.52
Batch: 20; loss: 1.75; acc: 0.52
Batch: 40; loss: 1.69; acc: 0.62
Batch: 60; loss: 1.75; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.56
Batch: 100; loss: 1.69; acc: 0.61
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.85; acc: 0.48
Batch: 160; loss: 1.66; acc: 0.61
Batch: 180; loss: 1.59; acc: 0.62
Batch: 200; loss: 1.67; acc: 0.61
Batch: 220; loss: 1.72; acc: 0.5
Batch: 240; loss: 1.69; acc: 0.56
Batch: 260; loss: 1.69; acc: 0.47
Batch: 280; loss: 1.77; acc: 0.52
Batch: 300; loss: 1.67; acc: 0.58
Batch: 320; loss: 1.73; acc: 0.47
Batch: 340; loss: 1.68; acc: 0.59
Batch: 360; loss: 1.67; acc: 0.53
Batch: 380; loss: 1.7; acc: 0.48
Batch: 400; loss: 1.69; acc: 0.55
Batch: 420; loss: 1.81; acc: 0.5
Batch: 440; loss: 1.67; acc: 0.58
Batch: 460; loss: 1.62; acc: 0.56
Batch: 480; loss: 1.7; acc: 0.5
Batch: 500; loss: 1.67; acc: 0.58
Batch: 520; loss: 1.71; acc: 0.55
Batch: 540; loss: 1.61; acc: 0.66
Batch: 560; loss: 1.69; acc: 0.42
Batch: 580; loss: 1.65; acc: 0.48
Batch: 600; loss: 1.74; acc: 0.47
Batch: 620; loss: 1.43; acc: 0.77
Batch: 640; loss: 1.59; acc: 0.64
Batch: 660; loss: 1.73; acc: 0.5
Batch: 680; loss: 1.59; acc: 0.66
Batch: 700; loss: 1.52; acc: 0.62
Batch: 720; loss: 1.73; acc: 0.47
Batch: 740; loss: 1.58; acc: 0.64
Batch: 760; loss: 1.52; acc: 0.7
Batch: 780; loss: 1.57; acc: 0.55
Train Epoch over. train_loss: 1.68; train_accuracy: 0.55 

7.896230090409517e-05
7.467671093763784e-05
Batch: 0; loss: 1.65; acc: 0.56
Batch: 20; loss: 1.81; acc: 0.47
Batch: 40; loss: 1.33; acc: 0.72
Batch: 60; loss: 1.54; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.69
Batch: 100; loss: 1.53; acc: 0.64
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.69
Val Epoch over. val_loss: 1.571587693919042; val_accuracy: 0.6041998407643312 

The current subspace-distance is: 7.467671093763784e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.56
Batch: 20; loss: 1.52; acc: 0.66
Batch: 40; loss: 1.59; acc: 0.61
Batch: 60; loss: 1.6; acc: 0.5
Batch: 80; loss: 1.6; acc: 0.64
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.65; acc: 0.58
Batch: 160; loss: 1.54; acc: 0.59
Batch: 180; loss: 1.51; acc: 0.64
Batch: 200; loss: 1.71; acc: 0.48
Batch: 220; loss: 1.73; acc: 0.48
Batch: 240; loss: 1.7; acc: 0.5
Batch: 260; loss: 1.55; acc: 0.56
Batch: 280; loss: 1.5; acc: 0.67
Batch: 300; loss: 1.55; acc: 0.59
Batch: 320; loss: 1.58; acc: 0.61
Batch: 340; loss: 1.53; acc: 0.64
Batch: 360; loss: 1.66; acc: 0.55
Batch: 380; loss: 1.53; acc: 0.64
Batch: 400; loss: 1.69; acc: 0.45
Batch: 420; loss: 1.6; acc: 0.59
Batch: 440; loss: 1.46; acc: 0.72
Batch: 460; loss: 1.61; acc: 0.47
Batch: 480; loss: 1.48; acc: 0.62
Batch: 500; loss: 1.63; acc: 0.55
Batch: 520; loss: 1.64; acc: 0.52
Batch: 540; loss: 1.69; acc: 0.58
Batch: 560; loss: 1.64; acc: 0.58
Batch: 580; loss: 1.58; acc: 0.62
Batch: 600; loss: 1.63; acc: 0.53
Batch: 620; loss: 1.45; acc: 0.67
Batch: 640; loss: 1.56; acc: 0.61
Batch: 660; loss: 1.6; acc: 0.58
Batch: 680; loss: 1.71; acc: 0.53
Batch: 700; loss: 1.59; acc: 0.61
Batch: 720; loss: 1.48; acc: 0.67
Batch: 740; loss: 1.51; acc: 0.59
Batch: 760; loss: 1.62; acc: 0.62
Batch: 780; loss: 1.53; acc: 0.55
Train Epoch over. train_loss: 1.58; train_accuracy: 0.59 

9.214807505486533e-05
8.695941505720839e-05
Batch: 0; loss: 1.58; acc: 0.55
Batch: 20; loss: 1.72; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.62
Batch: 80; loss: 1.34; acc: 0.72
Batch: 100; loss: 1.49; acc: 0.69
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.42; acc: 0.7
Val Epoch over. val_loss: 1.4996273305006087; val_accuracy: 0.6167396496815286 

The current subspace-distance is: 8.695941505720839e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.57; acc: 0.61
Batch: 40; loss: 1.68; acc: 0.48
Batch: 60; loss: 1.53; acc: 0.58
Batch: 80; loss: 1.5; acc: 0.61
Batch: 100; loss: 1.54; acc: 0.61
Batch: 120; loss: 1.48; acc: 0.61
Batch: 140; loss: 1.53; acc: 0.59
Batch: 160; loss: 1.59; acc: 0.5
Batch: 180; loss: 1.46; acc: 0.59
Batch: 200; loss: 1.62; acc: 0.52
Batch: 220; loss: 1.41; acc: 0.7
Batch: 240; loss: 1.45; acc: 0.56
Batch: 260; loss: 1.43; acc: 0.64
Batch: 280; loss: 1.8; acc: 0.44
Batch: 300; loss: 1.5; acc: 0.58
Batch: 320; loss: 1.59; acc: 0.47
Batch: 340; loss: 1.46; acc: 0.64
Batch: 360; loss: 1.56; acc: 0.59
Batch: 380; loss: 1.7; acc: 0.42
Batch: 400; loss: 1.48; acc: 0.55
Batch: 420; loss: 1.53; acc: 0.55
Batch: 440; loss: 1.56; acc: 0.59
Batch: 460; loss: 1.38; acc: 0.69
Batch: 480; loss: 1.47; acc: 0.53
Batch: 500; loss: 1.48; acc: 0.66
Batch: 520; loss: 1.44; acc: 0.58
Batch: 540; loss: 1.62; acc: 0.48
Batch: 560; loss: 1.56; acc: 0.5
Batch: 580; loss: 1.44; acc: 0.55
Batch: 600; loss: 1.5; acc: 0.58
Batch: 620; loss: 1.41; acc: 0.66
Batch: 640; loss: 1.47; acc: 0.64
Batch: 660; loss: 1.43; acc: 0.75
Batch: 680; loss: 1.47; acc: 0.56
Batch: 700; loss: 1.47; acc: 0.61
Batch: 720; loss: 1.65; acc: 0.58
Batch: 740; loss: 1.51; acc: 0.62
Batch: 760; loss: 1.47; acc: 0.61
Batch: 780; loss: 1.47; acc: 0.61
Train Epoch over. train_loss: 1.51; train_accuracy: 0.59 

0.00010565374395810068
0.00010042441863333806
Batch: 0; loss: 1.53; acc: 0.62
Batch: 20; loss: 1.68; acc: 0.52
Batch: 40; loss: 1.16; acc: 0.77
Batch: 60; loss: 1.42; acc: 0.59
Batch: 80; loss: 1.28; acc: 0.73
Batch: 100; loss: 1.47; acc: 0.69
Batch: 120; loss: 1.65; acc: 0.44
Batch: 140; loss: 1.38; acc: 0.66
Val Epoch over. val_loss: 1.442658523845065; val_accuracy: 0.6298765923566879 

The current subspace-distance is: 0.00010042441863333806 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.51; acc: 0.47
Batch: 20; loss: 1.52; acc: 0.59
Batch: 40; loss: 1.45; acc: 0.58
Batch: 60; loss: 1.46; acc: 0.66
Batch: 80; loss: 1.48; acc: 0.61
Batch: 100; loss: 1.39; acc: 0.69
Batch: 120; loss: 1.41; acc: 0.64
Batch: 140; loss: 1.36; acc: 0.69
Batch: 160; loss: 1.6; acc: 0.55
Batch: 180; loss: 1.39; acc: 0.69
Batch: 200; loss: 1.55; acc: 0.56
Batch: 220; loss: 1.48; acc: 0.61
Batch: 240; loss: 1.49; acc: 0.7
Batch: 260; loss: 1.35; acc: 0.69
Batch: 280; loss: 1.56; acc: 0.58
Batch: 300; loss: 1.41; acc: 0.64
Batch: 320; loss: 1.38; acc: 0.56
Batch: 340; loss: 1.54; acc: 0.5
Batch: 360; loss: 1.39; acc: 0.64
Batch: 380; loss: 1.4; acc: 0.69
Batch: 400; loss: 1.42; acc: 0.64
Batch: 420; loss: 1.43; acc: 0.53
Batch: 440; loss: 1.48; acc: 0.53
Batch: 460; loss: 1.4; acc: 0.67
Batch: 480; loss: 1.47; acc: 0.62
Batch: 500; loss: 1.56; acc: 0.55
Batch: 520; loss: 1.46; acc: 0.64
Batch: 540; loss: 1.34; acc: 0.67
Batch: 560; loss: 1.34; acc: 0.69
Batch: 580; loss: 1.62; acc: 0.5
Batch: 600; loss: 1.33; acc: 0.69
Batch: 620; loss: 1.52; acc: 0.55
Batch: 640; loss: 1.28; acc: 0.78
Batch: 660; loss: 1.45; acc: 0.56
Batch: 680; loss: 1.33; acc: 0.72
Batch: 700; loss: 1.41; acc: 0.61
Batch: 720; loss: 1.62; acc: 0.56
Batch: 740; loss: 1.58; acc: 0.58
Batch: 760; loss: 1.51; acc: 0.56
Batch: 780; loss: 1.3; acc: 0.7
Train Epoch over. train_loss: 1.46; train_accuracy: 0.61 

0.00011530982737895101
0.00010947292321361601
Batch: 0; loss: 1.4; acc: 0.64
Batch: 20; loss: 1.6; acc: 0.59
Batch: 40; loss: 1.1; acc: 0.78
Batch: 60; loss: 1.38; acc: 0.61
Batch: 80; loss: 1.22; acc: 0.78
Batch: 100; loss: 1.42; acc: 0.64
Batch: 120; loss: 1.58; acc: 0.5
Batch: 140; loss: 1.31; acc: 0.66
Val Epoch over. val_loss: 1.3800994772819957; val_accuracy: 0.647890127388535 

The current subspace-distance is: 0.00010947292321361601 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.64
Batch: 20; loss: 1.38; acc: 0.66
Batch: 40; loss: 1.42; acc: 0.59
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.38; acc: 0.66
Batch: 100; loss: 1.48; acc: 0.55
Batch: 120; loss: 1.36; acc: 0.7
Batch: 140; loss: 1.46; acc: 0.53
Batch: 160; loss: 1.42; acc: 0.67
Batch: 180; loss: 1.45; acc: 0.59
Batch: 200; loss: 1.41; acc: 0.61
Batch: 220; loss: 1.54; acc: 0.61
Batch: 240; loss: 1.29; acc: 0.67
Batch: 260; loss: 1.46; acc: 0.53
Batch: 280; loss: 1.41; acc: 0.61
Batch: 300; loss: 1.36; acc: 0.62
Batch: 320; loss: 1.37; acc: 0.64
Batch: 340; loss: 1.38; acc: 0.59
Batch: 360; loss: 1.4; acc: 0.59
Batch: 380; loss: 1.34; acc: 0.59
Batch: 400; loss: 1.46; acc: 0.59
Batch: 420; loss: 1.5; acc: 0.55
Batch: 440; loss: 1.43; acc: 0.56
Batch: 460; loss: 1.47; acc: 0.56
Batch: 480; loss: 1.34; acc: 0.7
Batch: 500; loss: 1.28; acc: 0.7
Batch: 520; loss: 1.55; acc: 0.55
Batch: 540; loss: 1.47; acc: 0.58
Batch: 560; loss: 1.53; acc: 0.52
Batch: 580; loss: 1.56; acc: 0.56
Batch: 600; loss: 1.42; acc: 0.59
Batch: 620; loss: 1.35; acc: 0.62
Batch: 640; loss: 1.36; acc: 0.66
Batch: 660; loss: 1.44; acc: 0.61
Batch: 680; loss: 1.35; acc: 0.62
Batch: 700; loss: 1.45; acc: 0.61
Batch: 720; loss: 1.63; acc: 0.56
Batch: 740; loss: 1.44; acc: 0.64
Batch: 760; loss: 1.39; acc: 0.69
Batch: 780; loss: 1.23; acc: 0.73
Train Epoch over. train_loss: 1.41; train_accuracy: 0.62 

0.00012647216499317437
0.00012039813736919314
Batch: 0; loss: 1.37; acc: 0.64
Batch: 20; loss: 1.6; acc: 0.53
Batch: 40; loss: 1.07; acc: 0.78
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 1.22; acc: 0.77
Batch: 100; loss: 1.37; acc: 0.66
Batch: 120; loss: 1.56; acc: 0.55
Batch: 140; loss: 1.25; acc: 0.72
Val Epoch over. val_loss: 1.344248881765232; val_accuracy: 0.6512738853503185 

The current subspace-distance is: 0.00012039813736919314 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.37; acc: 0.66
Batch: 20; loss: 1.62; acc: 0.59
Batch: 40; loss: 1.39; acc: 0.64
Batch: 60; loss: 1.41; acc: 0.59
Batch: 80; loss: 1.33; acc: 0.73
Batch: 100; loss: 1.5; acc: 0.55
Batch: 120; loss: 1.36; acc: 0.64
Batch: 140; loss: 1.43; acc: 0.55
Batch: 160; loss: 1.41; acc: 0.53
Batch: 180; loss: 1.42; acc: 0.61
Batch: 200; loss: 1.49; acc: 0.62
Batch: 220; loss: 1.54; acc: 0.61
Batch: 240; loss: 1.4; acc: 0.61
Batch: 260; loss: 1.48; acc: 0.56
Batch: 280; loss: 1.3; acc: 0.66
Batch: 300; loss: 1.4; acc: 0.62
Batch: 320; loss: 1.25; acc: 0.66
Batch: 340; loss: 1.29; acc: 0.69
Batch: 360; loss: 1.27; acc: 0.69
Batch: 380; loss: 1.43; acc: 0.62
Batch: 400; loss: 1.33; acc: 0.7
Batch: 420; loss: 1.38; acc: 0.69
Batch: 440; loss: 1.34; acc: 0.64
Batch: 460; loss: 1.34; acc: 0.59
Batch: 480; loss: 1.21; acc: 0.7
Batch: 500; loss: 1.44; acc: 0.59
Batch: 520; loss: 1.23; acc: 0.67
Batch: 540; loss: 1.3; acc: 0.69
Batch: 560; loss: 1.28; acc: 0.69
Batch: 580; loss: 1.26; acc: 0.69
Batch: 600; loss: 1.38; acc: 0.62
Batch: 620; loss: 1.22; acc: 0.7
Batch: 640; loss: 1.26; acc: 0.67
Batch: 660; loss: 1.32; acc: 0.7
Batch: 680; loss: 1.39; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.56
Batch: 720; loss: 1.33; acc: 0.69
Batch: 740; loss: 1.31; acc: 0.67
Batch: 760; loss: 1.36; acc: 0.64
Batch: 780; loss: 1.39; acc: 0.58
Train Epoch over. train_loss: 1.38; train_accuracy: 0.63 

0.00013607188884634525
0.00012922986934427172
Batch: 0; loss: 1.31; acc: 0.72
Batch: 20; loss: 1.55; acc: 0.56
Batch: 40; loss: 1.06; acc: 0.8
Batch: 60; loss: 1.31; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.75
Batch: 100; loss: 1.33; acc: 0.69
Batch: 120; loss: 1.53; acc: 0.53
Batch: 140; loss: 1.17; acc: 0.8
Val Epoch over. val_loss: 1.30993987268703; val_accuracy: 0.6592356687898089 

The current subspace-distance is: 0.00012922986934427172 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.64
Batch: 20; loss: 1.48; acc: 0.58
Batch: 40; loss: 1.28; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.69
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.48; acc: 0.53
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 1.34; acc: 0.64
Batch: 160; loss: 1.34; acc: 0.64
Batch: 180; loss: 1.3; acc: 0.62
Batch: 200; loss: 1.17; acc: 0.8
Batch: 220; loss: 1.44; acc: 0.61
Batch: 240; loss: 1.34; acc: 0.62
Batch: 260; loss: 1.38; acc: 0.7
Batch: 280; loss: 1.27; acc: 0.73
Batch: 300; loss: 1.26; acc: 0.66
Batch: 320; loss: 1.47; acc: 0.59
Batch: 340; loss: 1.32; acc: 0.69
Batch: 360; loss: 1.29; acc: 0.56
Batch: 380; loss: 1.42; acc: 0.61
Batch: 400; loss: 1.34; acc: 0.61
Batch: 420; loss: 1.33; acc: 0.64
Batch: 440; loss: 1.33; acc: 0.67
Batch: 460; loss: 1.41; acc: 0.66
Batch: 480; loss: 1.3; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.69
Batch: 520; loss: 1.4; acc: 0.59
Batch: 540; loss: 1.34; acc: 0.69
Batch: 560; loss: 1.28; acc: 0.66
Batch: 580; loss: 1.29; acc: 0.75
Batch: 600; loss: 1.41; acc: 0.61
Batch: 620; loss: 1.19; acc: 0.77
Batch: 640; loss: 1.24; acc: 0.67
Batch: 660; loss: 1.43; acc: 0.53
Batch: 680; loss: 1.33; acc: 0.67
Batch: 700; loss: 1.41; acc: 0.62
Batch: 720; loss: 1.39; acc: 0.64
Batch: 740; loss: 1.25; acc: 0.69
Batch: 760; loss: 1.39; acc: 0.59
Batch: 780; loss: 1.43; acc: 0.59
Train Epoch over. train_loss: 1.34; train_accuracy: 0.64 

0.00014209383516572416
0.00014017889043316245
Batch: 0; loss: 1.28; acc: 0.73
Batch: 20; loss: 1.51; acc: 0.58
Batch: 40; loss: 1.08; acc: 0.72
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 1.19; acc: 0.73
Batch: 100; loss: 1.32; acc: 0.67
Batch: 120; loss: 1.52; acc: 0.52
Batch: 140; loss: 1.13; acc: 0.78
Val Epoch over. val_loss: 1.2887950919236346; val_accuracy: 0.6710788216560509 

The current subspace-distance is: 0.00014017889043316245 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.7
Batch: 20; loss: 1.43; acc: 0.56
Batch: 40; loss: 1.45; acc: 0.56
Batch: 60; loss: 1.21; acc: 0.7
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.31; acc: 0.66
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.1; acc: 0.77
Batch: 160; loss: 1.29; acc: 0.67
Batch: 180; loss: 1.28; acc: 0.59
Batch: 200; loss: 1.25; acc: 0.72
Batch: 220; loss: 1.43; acc: 0.56
Batch: 240; loss: 1.28; acc: 0.62
Batch: 260; loss: 1.47; acc: 0.58
Batch: 280; loss: 1.4; acc: 0.59
Batch: 300; loss: 1.22; acc: 0.7
Batch: 320; loss: 1.2; acc: 0.69
Batch: 340; loss: 1.23; acc: 0.7
Batch: 360; loss: 1.5; acc: 0.64
Batch: 380; loss: 1.22; acc: 0.73
Batch: 400; loss: 1.32; acc: 0.7
Batch: 420; loss: 1.41; acc: 0.62
Batch: 440; loss: 1.38; acc: 0.66
Batch: 460; loss: 1.28; acc: 0.75
Batch: 480; loss: 1.39; acc: 0.59
Batch: 500; loss: 1.16; acc: 0.75
Batch: 520; loss: 1.25; acc: 0.67
Batch: 540; loss: 1.29; acc: 0.69
Batch: 560; loss: 1.32; acc: 0.59
Batch: 580; loss: 1.36; acc: 0.62
Batch: 600; loss: 1.32; acc: 0.67
Batch: 620; loss: 1.37; acc: 0.53
Batch: 640; loss: 1.3; acc: 0.66
Batch: 660; loss: 1.37; acc: 0.64
Batch: 680; loss: 1.19; acc: 0.72
Batch: 700; loss: 1.42; acc: 0.66
Batch: 720; loss: 1.37; acc: 0.62
Batch: 740; loss: 1.25; acc: 0.62
Batch: 760; loss: 1.41; acc: 0.56
Batch: 780; loss: 1.22; acc: 0.75
Train Epoch over. train_loss: 1.33; train_accuracy: 0.64 

0.0001499326026532799
0.00014472768816631287
Batch: 0; loss: 1.25; acc: 0.78
Batch: 20; loss: 1.5; acc: 0.61
Batch: 40; loss: 1.08; acc: 0.75
Batch: 60; loss: 1.3; acc: 0.64
Batch: 80; loss: 1.18; acc: 0.72
Batch: 100; loss: 1.31; acc: 0.67
Batch: 120; loss: 1.53; acc: 0.52
Batch: 140; loss: 1.1; acc: 0.81
Val Epoch over. val_loss: 1.281611599360302; val_accuracy: 0.6724721337579618 

The current subspace-distance is: 0.00014472768816631287 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.22; acc: 0.73
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.5
Batch: 120; loss: 1.42; acc: 0.64
Batch: 140; loss: 1.33; acc: 0.67
Batch: 160; loss: 1.3; acc: 0.59
Batch: 180; loss: 1.29; acc: 0.66
Batch: 200; loss: 1.31; acc: 0.66
Batch: 220; loss: 1.36; acc: 0.61
Batch: 240; loss: 1.26; acc: 0.66
Batch: 260; loss: 1.32; acc: 0.61
Batch: 280; loss: 1.32; acc: 0.61
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 1.36; acc: 0.67
Batch: 340; loss: 1.23; acc: 0.59
Batch: 360; loss: 1.44; acc: 0.52
Batch: 380; loss: 1.22; acc: 0.72
Batch: 400; loss: 1.44; acc: 0.62
Batch: 420; loss: 1.34; acc: 0.66
Batch: 440; loss: 1.29; acc: 0.7
Batch: 460; loss: 1.32; acc: 0.64
Batch: 480; loss: 1.21; acc: 0.62
Batch: 500; loss: 1.33; acc: 0.75
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.66
Batch: 560; loss: 1.39; acc: 0.59
Batch: 580; loss: 1.29; acc: 0.67
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.46; acc: 0.52
Batch: 640; loss: 1.25; acc: 0.7
Batch: 660; loss: 1.26; acc: 0.73
Batch: 680; loss: 1.39; acc: 0.59
Batch: 700; loss: 1.32; acc: 0.66
Batch: 720; loss: 1.21; acc: 0.7
Batch: 740; loss: 1.27; acc: 0.64
Batch: 760; loss: 1.43; acc: 0.55
Batch: 780; loss: 1.3; acc: 0.61
Train Epoch over. train_loss: 1.31; train_accuracy: 0.64 

0.00015827821334823966
0.00015152004198171198
Batch: 0; loss: 1.18; acc: 0.77
Batch: 20; loss: 1.5; acc: 0.56
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 1.25; acc: 0.64
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.05; acc: 0.73
Val Epoch over. val_loss: 1.2471175740478904; val_accuracy: 0.6712778662420382 

The current subspace-distance is: 0.00015152004198171198 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.72
Batch: 20; loss: 1.24; acc: 0.7
Batch: 40; loss: 1.43; acc: 0.56
Batch: 60; loss: 1.46; acc: 0.53
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 1.39; acc: 0.61
Batch: 160; loss: 1.5; acc: 0.52
Batch: 180; loss: 1.41; acc: 0.64
Batch: 200; loss: 1.41; acc: 0.56
Batch: 220; loss: 1.18; acc: 0.73
Batch: 240; loss: 1.3; acc: 0.64
Batch: 260; loss: 1.19; acc: 0.7
Batch: 280; loss: 1.34; acc: 0.62
Batch: 300; loss: 1.3; acc: 0.64
Batch: 320; loss: 1.4; acc: 0.58
Batch: 340; loss: 1.37; acc: 0.64
Batch: 360; loss: 1.39; acc: 0.5
Batch: 380; loss: 1.32; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.66
Batch: 420; loss: 1.2; acc: 0.69
Batch: 440; loss: 1.27; acc: 0.69
Batch: 460; loss: 1.19; acc: 0.72
Batch: 480; loss: 1.35; acc: 0.66
Batch: 500; loss: 1.28; acc: 0.62
Batch: 520; loss: 1.49; acc: 0.61
Batch: 540; loss: 1.27; acc: 0.61
Batch: 560; loss: 1.33; acc: 0.58
Batch: 580; loss: 1.24; acc: 0.67
Batch: 600; loss: 1.25; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.7
Batch: 640; loss: 1.29; acc: 0.62
Batch: 660; loss: 1.25; acc: 0.7
Batch: 680; loss: 1.05; acc: 0.83
Batch: 700; loss: 1.18; acc: 0.67
Batch: 720; loss: 1.36; acc: 0.64
Batch: 740; loss: 1.22; acc: 0.61
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.26; acc: 0.66
Train Epoch over. train_loss: 1.3; train_accuracy: 0.64 

0.00016005545330699533
0.00015396438539028168
Batch: 0; loss: 1.17; acc: 0.77
Batch: 20; loss: 1.49; acc: 0.56
Batch: 40; loss: 1.04; acc: 0.72
Batch: 60; loss: 1.24; acc: 0.62
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.23; acc: 0.75
Batch: 120; loss: 1.49; acc: 0.5
Batch: 140; loss: 1.05; acc: 0.78
Val Epoch over. val_loss: 1.2406927863503718; val_accuracy: 0.6744625796178344 

The current subspace-distance is: 0.00015396438539028168 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.29; acc: 0.66
Batch: 20; loss: 1.14; acc: 0.73
Batch: 40; loss: 1.36; acc: 0.61
Batch: 60; loss: 1.38; acc: 0.62
Batch: 80; loss: 1.32; acc: 0.56
Batch: 100; loss: 1.42; acc: 0.48
Batch: 120; loss: 1.4; acc: 0.5
Batch: 140; loss: 1.2; acc: 0.7
Batch: 160; loss: 1.25; acc: 0.66
Batch: 180; loss: 1.19; acc: 0.7
Batch: 200; loss: 1.28; acc: 0.67
Batch: 220; loss: 1.28; acc: 0.64
Batch: 240; loss: 1.2; acc: 0.67
Batch: 260; loss: 1.17; acc: 0.67
Batch: 280; loss: 1.33; acc: 0.62
Batch: 300; loss: 1.3; acc: 0.69
Batch: 320; loss: 1.19; acc: 0.69
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.26; acc: 0.64
Batch: 380; loss: 1.27; acc: 0.61
Batch: 400; loss: 1.52; acc: 0.52
Batch: 420; loss: 1.34; acc: 0.62
Batch: 440; loss: 1.29; acc: 0.66
Batch: 460; loss: 1.25; acc: 0.73
Batch: 480; loss: 1.34; acc: 0.58
Batch: 500; loss: 1.26; acc: 0.64
Batch: 520; loss: 1.34; acc: 0.64
Batch: 540; loss: 1.35; acc: 0.58
Batch: 560; loss: 1.12; acc: 0.75
Batch: 580; loss: 1.28; acc: 0.61
Batch: 600; loss: 1.34; acc: 0.69
Batch: 620; loss: 1.33; acc: 0.66
Batch: 640; loss: 1.17; acc: 0.75
Batch: 660; loss: 1.28; acc: 0.64
Batch: 680; loss: 1.33; acc: 0.61
Batch: 700; loss: 1.37; acc: 0.59
Batch: 720; loss: 1.13; acc: 0.67
Batch: 740; loss: 1.33; acc: 0.52
Batch: 760; loss: 1.35; acc: 0.59
Batch: 780; loss: 1.32; acc: 0.56
Train Epoch over. train_loss: 1.29; train_accuracy: 0.64 

0.00015938817523419857
0.00015329860616475344
Batch: 0; loss: 1.15; acc: 0.72
Batch: 20; loss: 1.49; acc: 0.56
Batch: 40; loss: 1.02; acc: 0.78
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.11; acc: 0.72
Batch: 100; loss: 1.21; acc: 0.75
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 1.03; acc: 0.77
Val Epoch over. val_loss: 1.2238605861451215; val_accuracy: 0.6866042993630573 

The current subspace-distance is: 0.00015329860616475344 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.37; acc: 0.58
Batch: 20; loss: 1.47; acc: 0.55
Batch: 40; loss: 1.28; acc: 0.7
Batch: 60; loss: 1.31; acc: 0.69
Batch: 80; loss: 1.13; acc: 0.75
Batch: 100; loss: 1.26; acc: 0.67
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 1.4; acc: 0.66
Batch: 160; loss: 1.12; acc: 0.73
Batch: 180; loss: 1.24; acc: 0.62
Batch: 200; loss: 1.11; acc: 0.78
Batch: 220; loss: 1.21; acc: 0.7
Batch: 240; loss: 1.21; acc: 0.7
Batch: 260; loss: 1.32; acc: 0.61
Batch: 280; loss: 1.43; acc: 0.48
Batch: 300; loss: 1.28; acc: 0.67
Batch: 320; loss: 1.22; acc: 0.75
Batch: 340; loss: 1.34; acc: 0.67
Batch: 360; loss: 1.3; acc: 0.62
Batch: 380; loss: 1.29; acc: 0.61
Batch: 400; loss: 1.27; acc: 0.61
Batch: 420; loss: 1.21; acc: 0.73
Batch: 440; loss: 1.33; acc: 0.52
Batch: 460; loss: 1.29; acc: 0.64
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.39; acc: 0.66
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 1.34; acc: 0.69
Batch: 560; loss: 1.28; acc: 0.55
Batch: 580; loss: 1.33; acc: 0.59
Batch: 600; loss: 1.14; acc: 0.72
Batch: 620; loss: 1.21; acc: 0.7
Batch: 640; loss: 1.31; acc: 0.64
Batch: 660; loss: 1.36; acc: 0.62
Batch: 680; loss: 1.44; acc: 0.59
Batch: 700; loss: 1.22; acc: 0.7
Batch: 720; loss: 1.35; acc: 0.59
Batch: 740; loss: 1.31; acc: 0.64
Batch: 760; loss: 1.24; acc: 0.69
Batch: 780; loss: 1.51; acc: 0.52
Train Epoch over. train_loss: 1.28; train_accuracy: 0.65 

0.00016532358131371439
0.00015538388106506318
Batch: 0; loss: 1.14; acc: 0.77
Batch: 20; loss: 1.46; acc: 0.56
Batch: 40; loss: 1.02; acc: 0.8
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 1.21; acc: 0.73
Batch: 120; loss: 1.47; acc: 0.56
Batch: 140; loss: 1.03; acc: 0.8
Val Epoch over. val_loss: 1.2189400351730881; val_accuracy: 0.6819267515923567 

The current subspace-distance is: 0.00015538388106506318 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.72
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 1.17; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.67
Batch: 80; loss: 1.25; acc: 0.59
Batch: 100; loss: 1.22; acc: 0.64
Batch: 120; loss: 1.14; acc: 0.75
Batch: 140; loss: 1.28; acc: 0.66
Batch: 160; loss: 1.37; acc: 0.56
Batch: 180; loss: 1.47; acc: 0.56
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.3; acc: 0.64
Batch: 240; loss: 1.4; acc: 0.55
Batch: 260; loss: 1.33; acc: 0.59
Batch: 280; loss: 1.24; acc: 0.73
Batch: 300; loss: 1.36; acc: 0.61
Batch: 320; loss: 1.27; acc: 0.59
Batch: 340; loss: 1.25; acc: 0.66
Batch: 360; loss: 1.33; acc: 0.59
Batch: 380; loss: 1.22; acc: 0.66
Batch: 400; loss: 1.23; acc: 0.64
Batch: 420; loss: 1.46; acc: 0.52
Batch: 440; loss: 1.34; acc: 0.59
Batch: 460; loss: 1.32; acc: 0.67
Batch: 480; loss: 1.33; acc: 0.62
Batch: 500; loss: 1.22; acc: 0.72
Batch: 520; loss: 1.13; acc: 0.78
Batch: 540; loss: 1.42; acc: 0.58
Batch: 560; loss: 1.22; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.75
Batch: 600; loss: 1.31; acc: 0.66
Batch: 620; loss: 1.31; acc: 0.61
Batch: 640; loss: 1.29; acc: 0.62
Batch: 660; loss: 1.29; acc: 0.61
Batch: 680; loss: 1.26; acc: 0.62
Batch: 700; loss: 1.24; acc: 0.59
Batch: 720; loss: 1.41; acc: 0.58
Batch: 740; loss: 1.28; acc: 0.66
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.07; acc: 0.77
Train Epoch over. train_loss: 1.27; train_accuracy: 0.65 

0.00016838636656757444
0.0001617814414203167
Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.48; acc: 0.56
Batch: 40; loss: 1.01; acc: 0.78
Batch: 60; loss: 1.22; acc: 0.62
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.47; acc: 0.53
Batch: 140; loss: 1.02; acc: 0.78
Val Epoch over. val_loss: 1.2125901131872918; val_accuracy: 0.681031050955414 

The current subspace-distance is: 0.0001617814414203167 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.69
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 1.19; acc: 0.73
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.24; acc: 0.59
Batch: 100; loss: 1.27; acc: 0.62
Batch: 120; loss: 1.22; acc: 0.67
Batch: 140; loss: 1.42; acc: 0.55
Batch: 160; loss: 1.28; acc: 0.67
Batch: 180; loss: 1.17; acc: 0.7
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.16; acc: 0.66
Batch: 240; loss: 1.47; acc: 0.55
Batch: 260; loss: 1.22; acc: 0.73
Batch: 280; loss: 1.26; acc: 0.64
Batch: 300; loss: 1.35; acc: 0.61
Batch: 320; loss: 1.26; acc: 0.69
Batch: 340; loss: 1.44; acc: 0.53
Batch: 360; loss: 1.29; acc: 0.62
Batch: 380; loss: 1.17; acc: 0.75
Batch: 400; loss: 1.24; acc: 0.67
Batch: 420; loss: 1.24; acc: 0.67
Batch: 440; loss: 1.12; acc: 0.75
Batch: 460; loss: 1.25; acc: 0.67
Batch: 480; loss: 1.22; acc: 0.67
Batch: 500; loss: 1.19; acc: 0.67
Batch: 520; loss: 1.33; acc: 0.64
Batch: 540; loss: 1.13; acc: 0.69
Batch: 560; loss: 1.32; acc: 0.61
Batch: 580; loss: 1.41; acc: 0.66
Batch: 600; loss: 1.2; acc: 0.66
Batch: 620; loss: 1.23; acc: 0.66
Batch: 640; loss: 1.28; acc: 0.64
Batch: 660; loss: 1.29; acc: 0.61
Batch: 680; loss: 1.15; acc: 0.73
Batch: 700; loss: 1.19; acc: 0.69
Batch: 720; loss: 1.29; acc: 0.64
Batch: 740; loss: 1.25; acc: 0.67
Batch: 760; loss: 1.2; acc: 0.67
Batch: 780; loss: 1.16; acc: 0.72
Train Epoch over. train_loss: 1.27; train_accuracy: 0.65 

0.00016911614511627704
0.00016450620023533702
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 1.0; acc: 0.77
Batch: 60; loss: 1.21; acc: 0.62
Batch: 80; loss: 1.08; acc: 0.72
Batch: 100; loss: 1.19; acc: 0.72
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.01; acc: 0.77
Val Epoch over. val_loss: 1.197384498301585; val_accuracy: 0.6867038216560509 

The current subspace-distance is: 0.00016450620023533702 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.78
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.38; acc: 0.59
Batch: 80; loss: 1.24; acc: 0.69
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.34; acc: 0.59
Batch: 140; loss: 1.16; acc: 0.73
Batch: 160; loss: 1.17; acc: 0.67
Batch: 180; loss: 1.15; acc: 0.73
Batch: 200; loss: 1.26; acc: 0.64
Batch: 220; loss: 1.17; acc: 0.73
Batch: 240; loss: 1.15; acc: 0.67
Batch: 260; loss: 1.35; acc: 0.59
Batch: 280; loss: 1.2; acc: 0.66
Batch: 300; loss: 1.23; acc: 0.67
Batch: 320; loss: 1.27; acc: 0.67
Batch: 340; loss: 1.11; acc: 0.73
Batch: 360; loss: 1.45; acc: 0.58
Batch: 380; loss: 1.28; acc: 0.67
Batch: 400; loss: 1.32; acc: 0.69
Batch: 420; loss: 1.21; acc: 0.67
Batch: 440; loss: 1.23; acc: 0.67
Batch: 460; loss: 1.45; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.7
Batch: 500; loss: 1.32; acc: 0.58
Batch: 520; loss: 1.27; acc: 0.59
Batch: 540; loss: 1.22; acc: 0.66
Batch: 560; loss: 1.3; acc: 0.56
Batch: 580; loss: 1.39; acc: 0.66
Batch: 600; loss: 1.37; acc: 0.59
Batch: 620; loss: 1.33; acc: 0.55
Batch: 640; loss: 1.35; acc: 0.64
Batch: 660; loss: 1.14; acc: 0.78
Batch: 680; loss: 1.33; acc: 0.59
Batch: 700; loss: 1.34; acc: 0.66
Batch: 720; loss: 1.39; acc: 0.59
Batch: 740; loss: 1.22; acc: 0.61
Batch: 760; loss: 1.27; acc: 0.64
Batch: 780; loss: 1.43; acc: 0.59
Train Epoch over. train_loss: 1.26; train_accuracy: 0.65 

0.00017270910029765218
0.00016707497707102448
Batch: 0; loss: 1.12; acc: 0.75
Batch: 20; loss: 1.47; acc: 0.53
Batch: 40; loss: 1.0; acc: 0.77
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.69
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.45; acc: 0.58
Batch: 140; loss: 1.0; acc: 0.81
Val Epoch over. val_loss: 1.1990333260244626; val_accuracy: 0.6890923566878981 

The current subspace-distance is: 0.00016707497707102448 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.29; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 1.21; acc: 0.73
Batch: 60; loss: 1.13; acc: 0.78
Batch: 80; loss: 1.37; acc: 0.53
Batch: 100; loss: 1.35; acc: 0.64
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 1.23; acc: 0.75
Batch: 160; loss: 1.22; acc: 0.62
Batch: 180; loss: 1.36; acc: 0.59
Batch: 200; loss: 1.42; acc: 0.61
Batch: 220; loss: 1.37; acc: 0.62
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.43; acc: 0.48
Batch: 280; loss: 1.31; acc: 0.66
Batch: 300; loss: 1.27; acc: 0.64
Batch: 320; loss: 1.29; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.37; acc: 0.59
Batch: 380; loss: 1.2; acc: 0.69
Batch: 400; loss: 1.36; acc: 0.66
Batch: 420; loss: 1.27; acc: 0.67
Batch: 440; loss: 1.23; acc: 0.67
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.11; acc: 0.8
Batch: 500; loss: 1.23; acc: 0.69
Batch: 520; loss: 1.27; acc: 0.62
Batch: 540; loss: 1.19; acc: 0.67
Batch: 560; loss: 1.14; acc: 0.66
Batch: 580; loss: 1.12; acc: 0.8
Batch: 600; loss: 1.23; acc: 0.66
Batch: 620; loss: 1.25; acc: 0.62
Batch: 640; loss: 1.17; acc: 0.7
Batch: 660; loss: 1.34; acc: 0.55
Batch: 680; loss: 1.36; acc: 0.56
Batch: 700; loss: 1.18; acc: 0.67
Batch: 720; loss: 1.18; acc: 0.66
Batch: 740; loss: 1.21; acc: 0.69
Batch: 760; loss: 1.23; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.73
Train Epoch over. train_loss: 1.25; train_accuracy: 0.66 

0.00017433382163289934
0.00016871534171514213
Batch: 0; loss: 1.14; acc: 0.77
Batch: 20; loss: 1.48; acc: 0.52
Batch: 40; loss: 1.01; acc: 0.75
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.2; acc: 0.73
Batch: 120; loss: 1.47; acc: 0.53
Batch: 140; loss: 1.0; acc: 0.81
Val Epoch over. val_loss: 1.201677776066361; val_accuracy: 0.682921974522293 

The current subspace-distance is: 0.00016871534171514213 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.31; acc: 0.64
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.41; acc: 0.53
Batch: 80; loss: 1.18; acc: 0.73
Batch: 100; loss: 1.28; acc: 0.62
Batch: 120; loss: 1.25; acc: 0.7
Batch: 140; loss: 1.26; acc: 0.64
Batch: 160; loss: 1.28; acc: 0.58
Batch: 180; loss: 1.27; acc: 0.61
Batch: 200; loss: 1.49; acc: 0.59
Batch: 220; loss: 1.37; acc: 0.56
Batch: 240; loss: 1.2; acc: 0.67
Batch: 260; loss: 1.13; acc: 0.77
Batch: 280; loss: 1.34; acc: 0.55
Batch: 300; loss: 1.06; acc: 0.8
Batch: 320; loss: 1.28; acc: 0.64
Batch: 340; loss: 1.04; acc: 0.81
Batch: 360; loss: 1.09; acc: 0.75
Batch: 380; loss: 1.33; acc: 0.55
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.22; acc: 0.72
Batch: 440; loss: 1.17; acc: 0.67
Batch: 460; loss: 1.26; acc: 0.66
Batch: 480; loss: 1.5; acc: 0.47
Batch: 500; loss: 1.3; acc: 0.59
Batch: 520; loss: 1.21; acc: 0.59
Batch: 540; loss: 1.23; acc: 0.72
Batch: 560; loss: 1.31; acc: 0.62
Batch: 580; loss: 1.49; acc: 0.53
Batch: 600; loss: 1.16; acc: 0.69
Batch: 620; loss: 1.14; acc: 0.69
Batch: 640; loss: 1.28; acc: 0.62
Batch: 660; loss: 1.33; acc: 0.62
Batch: 680; loss: 1.48; acc: 0.64
Batch: 700; loss: 1.31; acc: 0.66
Batch: 720; loss: 1.19; acc: 0.69
Batch: 740; loss: 1.25; acc: 0.64
Batch: 760; loss: 1.26; acc: 0.58
Batch: 780; loss: 1.37; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.00017982271674554795
0.0001734632533043623
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 0.97; acc: 0.78
Batch: 60; loss: 1.18; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 0.99; acc: 0.81
Val Epoch over. val_loss: 1.1811491497762643; val_accuracy: 0.6887937898089171 

The current subspace-distance is: 0.0001734632533043623 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.35; acc: 0.59
Batch: 80; loss: 1.25; acc: 0.69
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 1.35; acc: 0.59
Batch: 160; loss: 1.15; acc: 0.78
Batch: 180; loss: 1.28; acc: 0.62
Batch: 200; loss: 1.26; acc: 0.61
Batch: 220; loss: 1.22; acc: 0.62
Batch: 240; loss: 1.24; acc: 0.7
Batch: 260; loss: 1.27; acc: 0.61
Batch: 280; loss: 1.22; acc: 0.69
Batch: 300; loss: 1.31; acc: 0.59
Batch: 320; loss: 1.2; acc: 0.64
Batch: 340; loss: 1.36; acc: 0.62
Batch: 360; loss: 1.25; acc: 0.69
Batch: 380; loss: 1.17; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.42; acc: 0.58
Batch: 440; loss: 1.13; acc: 0.67
Batch: 460; loss: 1.29; acc: 0.66
Batch: 480; loss: 1.16; acc: 0.69
Batch: 500; loss: 1.31; acc: 0.56
Batch: 520; loss: 1.2; acc: 0.66
Batch: 540; loss: 1.15; acc: 0.69
Batch: 560; loss: 1.26; acc: 0.64
Batch: 580; loss: 1.27; acc: 0.67
Batch: 600; loss: 1.2; acc: 0.64
Batch: 620; loss: 1.47; acc: 0.53
Batch: 640; loss: 1.48; acc: 0.5
Batch: 660; loss: 1.29; acc: 0.59
Batch: 680; loss: 1.27; acc: 0.59
Batch: 700; loss: 1.23; acc: 0.7
Batch: 720; loss: 1.27; acc: 0.69
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.26; acc: 0.59
Batch: 780; loss: 1.25; acc: 0.59
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.00017915427451953292
0.00017372566799167544
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.46; acc: 0.55
Batch: 40; loss: 0.97; acc: 0.77
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 1.18; acc: 0.7
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.98; acc: 0.81
Val Epoch over. val_loss: 1.174464513921434; val_accuracy: 0.6932722929936306 

The current subspace-distance is: 0.00017372566799167544 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.66
Batch: 20; loss: 1.33; acc: 0.56
Batch: 40; loss: 1.24; acc: 0.67
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.61
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.58
Batch: 160; loss: 1.19; acc: 0.7
Batch: 180; loss: 1.31; acc: 0.61
Batch: 200; loss: 1.27; acc: 0.64
Batch: 220; loss: 1.26; acc: 0.62
Batch: 240; loss: 1.25; acc: 0.62
Batch: 260; loss: 1.11; acc: 0.72
Batch: 280; loss: 1.4; acc: 0.55
Batch: 300; loss: 1.19; acc: 0.69
Batch: 320; loss: 1.17; acc: 0.72
Batch: 340; loss: 1.26; acc: 0.67
Batch: 360; loss: 1.29; acc: 0.61
Batch: 380; loss: 1.18; acc: 0.72
Batch: 400; loss: 1.3; acc: 0.62
Batch: 420; loss: 1.16; acc: 0.62
Batch: 440; loss: 1.32; acc: 0.56
Batch: 460; loss: 1.26; acc: 0.67
Batch: 480; loss: 1.35; acc: 0.56
Batch: 500; loss: 1.25; acc: 0.58
Batch: 520; loss: 1.13; acc: 0.69
Batch: 540; loss: 1.42; acc: 0.56
Batch: 560; loss: 1.18; acc: 0.73
Batch: 580; loss: 1.22; acc: 0.67
Batch: 600; loss: 1.16; acc: 0.72
Batch: 620; loss: 1.18; acc: 0.7
Batch: 640; loss: 1.24; acc: 0.66
Batch: 660; loss: 1.18; acc: 0.72
Batch: 680; loss: 1.2; acc: 0.69
Batch: 700; loss: 1.12; acc: 0.72
Batch: 720; loss: 1.27; acc: 0.61
Batch: 740; loss: 1.14; acc: 0.72
Batch: 760; loss: 1.27; acc: 0.64
Batch: 780; loss: 1.37; acc: 0.62
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.00018307619029656053
0.00017549790209159255
Batch: 0; loss: 1.09; acc: 0.72
Batch: 20; loss: 1.47; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.78
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.41; acc: 0.53
Batch: 140; loss: 0.97; acc: 0.81
Val Epoch over. val_loss: 1.1613977547663792; val_accuracy: 0.6946656050955414 

The current subspace-distance is: 0.00017549790209159255 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.24; acc: 0.66
Batch: 20; loss: 1.09; acc: 0.78
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 1.3; acc: 0.59
Batch: 120; loss: 1.29; acc: 0.61
Batch: 140; loss: 1.11; acc: 0.75
Batch: 160; loss: 1.27; acc: 0.66
Batch: 180; loss: 1.22; acc: 0.64
Batch: 200; loss: 1.25; acc: 0.7
Batch: 220; loss: 1.48; acc: 0.52
Batch: 240; loss: 1.24; acc: 0.78
Batch: 260; loss: 1.08; acc: 0.75
Batch: 280; loss: 1.22; acc: 0.69
Batch: 300; loss: 1.24; acc: 0.67
Batch: 320; loss: 1.1; acc: 0.77
Batch: 340; loss: 1.27; acc: 0.61
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.34; acc: 0.55
Batch: 400; loss: 1.31; acc: 0.56
Batch: 420; loss: 1.21; acc: 0.64
Batch: 440; loss: 1.14; acc: 0.64
Batch: 460; loss: 1.17; acc: 0.75
Batch: 480; loss: 1.19; acc: 0.62
Batch: 500; loss: 1.23; acc: 0.64
Batch: 520; loss: 1.17; acc: 0.67
Batch: 540; loss: 1.17; acc: 0.66
Batch: 560; loss: 1.17; acc: 0.69
Batch: 580; loss: 1.16; acc: 0.7
Batch: 600; loss: 1.21; acc: 0.66
Batch: 620; loss: 1.26; acc: 0.67
Batch: 640; loss: 1.22; acc: 0.61
Batch: 660; loss: 1.13; acc: 0.67
Batch: 680; loss: 1.17; acc: 0.69
Batch: 700; loss: 1.22; acc: 0.7
Batch: 720; loss: 1.31; acc: 0.53
Batch: 740; loss: 1.36; acc: 0.66
Batch: 760; loss: 1.22; acc: 0.7
Batch: 780; loss: 1.34; acc: 0.64
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.0001815094583434984
0.0001760818704497069
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 1.18; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.18; acc: 0.69
Batch: 120; loss: 1.45; acc: 0.55
Batch: 140; loss: 0.99; acc: 0.84
Val Epoch over. val_loss: 1.1799001640574946; val_accuracy: 0.6851114649681529 

The current subspace-distance is: 0.0001760818704497069 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.21; acc: 0.69
Batch: 20; loss: 1.16; acc: 0.75
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.39; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.22; acc: 0.7
Batch: 140; loss: 1.28; acc: 0.59
Batch: 160; loss: 1.15; acc: 0.73
Batch: 180; loss: 1.31; acc: 0.64
Batch: 200; loss: 1.14; acc: 0.7
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 1.34; acc: 0.62
Batch: 260; loss: 1.08; acc: 0.72
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 1.18; acc: 0.72
Batch: 340; loss: 1.16; acc: 0.69
Batch: 360; loss: 1.4; acc: 0.58
Batch: 380; loss: 1.28; acc: 0.7
Batch: 400; loss: 1.3; acc: 0.62
Batch: 420; loss: 1.2; acc: 0.73
Batch: 440; loss: 1.15; acc: 0.75
Batch: 460; loss: 1.26; acc: 0.66
Batch: 480; loss: 1.22; acc: 0.67
Batch: 500; loss: 1.36; acc: 0.58
Batch: 520; loss: 1.18; acc: 0.69
Batch: 540; loss: 1.3; acc: 0.58
Batch: 560; loss: 1.19; acc: 0.67
Batch: 580; loss: 1.18; acc: 0.69
Batch: 600; loss: 1.17; acc: 0.7
Batch: 620; loss: 1.05; acc: 0.84
Batch: 640; loss: 1.23; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.62
Batch: 680; loss: 1.36; acc: 0.53
Batch: 700; loss: 1.28; acc: 0.59
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.17; acc: 0.69
Batch: 760; loss: 1.23; acc: 0.67
Batch: 780; loss: 1.32; acc: 0.64
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.0001829806133173406
0.00017583476437721401
Batch: 0; loss: 1.1; acc: 0.78
Batch: 20; loss: 1.45; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 0.97; acc: 0.81
Val Epoch over. val_loss: 1.1692536374565903; val_accuracy: 0.6913813694267515 

The current subspace-distance is: 0.00017583476437721401 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.25; acc: 0.7
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 1.31; acc: 0.67
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.26; acc: 0.58
Batch: 140; loss: 1.08; acc: 0.67
Batch: 160; loss: 1.26; acc: 0.67
Batch: 180; loss: 1.3; acc: 0.69
Batch: 200; loss: 1.1; acc: 0.73
Batch: 220; loss: 1.32; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.75
Batch: 260; loss: 1.32; acc: 0.58
Batch: 280; loss: 1.13; acc: 0.7
Batch: 300; loss: 1.21; acc: 0.72
Batch: 320; loss: 1.26; acc: 0.62
Batch: 340; loss: 1.49; acc: 0.52
Batch: 360; loss: 1.4; acc: 0.55
Batch: 380; loss: 1.32; acc: 0.69
Batch: 400; loss: 1.24; acc: 0.59
Batch: 420; loss: 1.21; acc: 0.8
Batch: 440; loss: 1.34; acc: 0.59
Batch: 460; loss: 1.23; acc: 0.7
Batch: 480; loss: 1.3; acc: 0.59
Batch: 500; loss: 1.13; acc: 0.67
Batch: 520; loss: 1.36; acc: 0.56
Batch: 540; loss: 1.12; acc: 0.75
Batch: 560; loss: 1.13; acc: 0.72
Batch: 580; loss: 1.37; acc: 0.53
Batch: 600; loss: 1.16; acc: 0.7
Batch: 620; loss: 1.16; acc: 0.72
Batch: 640; loss: 1.21; acc: 0.7
Batch: 660; loss: 1.1; acc: 0.7
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.18; acc: 0.62
Batch: 720; loss: 1.15; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.67
Batch: 760; loss: 1.09; acc: 0.75
Batch: 780; loss: 1.33; acc: 0.56
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.00018383724091108888
0.00017735153960529715
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.46; acc: 0.58
Batch: 40; loss: 0.96; acc: 0.77
Batch: 60; loss: 1.17; acc: 0.72
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.69
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.98; acc: 0.83
Val Epoch over. val_loss: 1.1712073600216277; val_accuracy: 0.6870023885350318 

The current subspace-distance is: 0.00017735153960529715 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.34; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.18; acc: 0.69
Batch: 80; loss: 1.22; acc: 0.61
Batch: 100; loss: 1.37; acc: 0.61
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 1.25; acc: 0.55
Batch: 160; loss: 1.2; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.69
Batch: 200; loss: 1.23; acc: 0.64
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.17; acc: 0.69
Batch: 260; loss: 1.15; acc: 0.7
Batch: 280; loss: 1.34; acc: 0.62
Batch: 300; loss: 1.09; acc: 0.72
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.18; acc: 0.64
Batch: 360; loss: 1.12; acc: 0.69
Batch: 380; loss: 1.3; acc: 0.64
Batch: 400; loss: 1.13; acc: 0.66
Batch: 420; loss: 1.04; acc: 0.72
Batch: 440; loss: 1.22; acc: 0.69
Batch: 460; loss: 1.37; acc: 0.59
Batch: 480; loss: 1.08; acc: 0.64
Batch: 500; loss: 0.95; acc: 0.86
Batch: 520; loss: 1.14; acc: 0.72
Batch: 540; loss: 1.32; acc: 0.67
Batch: 560; loss: 1.26; acc: 0.64
Batch: 580; loss: 1.17; acc: 0.75
Batch: 600; loss: 1.04; acc: 0.78
Batch: 620; loss: 1.35; acc: 0.61
Batch: 640; loss: 1.2; acc: 0.62
Batch: 660; loss: 1.15; acc: 0.67
Batch: 680; loss: 1.18; acc: 0.7
Batch: 700; loss: 1.17; acc: 0.64
Batch: 720; loss: 1.27; acc: 0.64
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.23; acc: 0.66
Batch: 780; loss: 1.33; acc: 0.62
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.00018599579925648868
0.0001792035182006657
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.18; acc: 0.72
Batch: 80; loss: 1.04; acc: 0.77
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.42; acc: 0.53
Batch: 140; loss: 0.99; acc: 0.84
Val Epoch over. val_loss: 1.1668120083535554; val_accuracy: 0.6892914012738853 

The current subspace-distance is: 0.0001792035182006657 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.17; acc: 0.67
Batch: 40; loss: 1.12; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.28; acc: 0.64
Batch: 100; loss: 1.14; acc: 0.78
Batch: 120; loss: 1.35; acc: 0.53
Batch: 140; loss: 1.13; acc: 0.77
Batch: 160; loss: 1.38; acc: 0.64
Batch: 180; loss: 1.41; acc: 0.52
Batch: 200; loss: 1.21; acc: 0.58
Batch: 220; loss: 1.44; acc: 0.59
Batch: 240; loss: 1.37; acc: 0.58
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.25; acc: 0.67
Batch: 300; loss: 1.38; acc: 0.59
Batch: 320; loss: 1.18; acc: 0.69
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.26; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.69
Batch: 420; loss: 1.25; acc: 0.67
Batch: 440; loss: 1.26; acc: 0.67
Batch: 460; loss: 1.23; acc: 0.61
Batch: 480; loss: 1.21; acc: 0.72
Batch: 500; loss: 1.22; acc: 0.66
Batch: 520; loss: 1.46; acc: 0.62
Batch: 540; loss: 1.24; acc: 0.67
Batch: 560; loss: 1.12; acc: 0.7
Batch: 580; loss: 1.31; acc: 0.59
Batch: 600; loss: 1.29; acc: 0.59
Batch: 620; loss: 1.64; acc: 0.52
Batch: 640; loss: 1.23; acc: 0.69
Batch: 660; loss: 1.17; acc: 0.69
Batch: 680; loss: 1.17; acc: 0.64
Batch: 700; loss: 1.2; acc: 0.72
Batch: 720; loss: 1.3; acc: 0.59
Batch: 740; loss: 1.17; acc: 0.73
Batch: 760; loss: 1.21; acc: 0.66
Batch: 780; loss: 1.25; acc: 0.67
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00018240253848489374
0.00017671383102424443
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.18; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 0.97; acc: 0.83
Val Epoch over. val_loss: 1.1644157182638812; val_accuracy: 0.6895899681528662 

The current subspace-distance is: 0.00017671383102424443 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.34; acc: 0.61
Batch: 20; loss: 1.21; acc: 0.7
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 1.23; acc: 0.7
Batch: 100; loss: 1.4; acc: 0.56
Batch: 120; loss: 1.14; acc: 0.77
Batch: 140; loss: 1.13; acc: 0.7
Batch: 160; loss: 1.3; acc: 0.62
Batch: 180; loss: 1.26; acc: 0.66
Batch: 200; loss: 1.29; acc: 0.59
Batch: 220; loss: 1.11; acc: 0.67
Batch: 240; loss: 1.25; acc: 0.67
Batch: 260; loss: 1.29; acc: 0.64
Batch: 280; loss: 1.25; acc: 0.62
Batch: 300; loss: 1.3; acc: 0.64
Batch: 320; loss: 1.23; acc: 0.69
Batch: 340; loss: 1.08; acc: 0.75
Batch: 360; loss: 1.21; acc: 0.7
Batch: 380; loss: 1.09; acc: 0.78
Batch: 400; loss: 1.18; acc: 0.72
Batch: 420; loss: 1.31; acc: 0.61
Batch: 440; loss: 1.19; acc: 0.66
Batch: 460; loss: 1.15; acc: 0.67
Batch: 480; loss: 1.44; acc: 0.59
Batch: 500; loss: 1.35; acc: 0.59
Batch: 520; loss: 1.24; acc: 0.59
Batch: 540; loss: 1.38; acc: 0.59
Batch: 560; loss: 1.2; acc: 0.64
Batch: 580; loss: 1.28; acc: 0.64
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 1.25; acc: 0.7
Batch: 640; loss: 1.32; acc: 0.59
Batch: 660; loss: 1.23; acc: 0.64
Batch: 680; loss: 1.28; acc: 0.61
Batch: 700; loss: 1.23; acc: 0.62
Batch: 720; loss: 1.24; acc: 0.66
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.38; acc: 0.61
Batch: 780; loss: 1.37; acc: 0.64
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.00018629679107107222
0.0001805977226467803
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.46; acc: 0.55
Batch: 40; loss: 0.97; acc: 0.77
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.97; acc: 0.83
Val Epoch over. val_loss: 1.1679134653632048; val_accuracy: 0.6915804140127388 

The current subspace-distance is: 0.0001805977226467803 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.73
Batch: 20; loss: 1.15; acc: 0.72
Batch: 40; loss: 1.34; acc: 0.59
Batch: 60; loss: 1.32; acc: 0.56
Batch: 80; loss: 1.32; acc: 0.58
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 1.3; acc: 0.66
Batch: 160; loss: 1.3; acc: 0.64
Batch: 180; loss: 1.39; acc: 0.62
Batch: 200; loss: 1.04; acc: 0.77
Batch: 220; loss: 1.22; acc: 0.69
Batch: 240; loss: 1.34; acc: 0.64
Batch: 260; loss: 1.16; acc: 0.73
Batch: 280; loss: 1.21; acc: 0.59
Batch: 300; loss: 1.16; acc: 0.72
Batch: 320; loss: 1.42; acc: 0.56
Batch: 340; loss: 1.22; acc: 0.66
Batch: 360; loss: 1.28; acc: 0.64
Batch: 380; loss: 1.22; acc: 0.64
Batch: 400; loss: 1.2; acc: 0.66
Batch: 420; loss: 1.21; acc: 0.64
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 1.17; acc: 0.7
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.39; acc: 0.59
Batch: 520; loss: 1.48; acc: 0.59
Batch: 540; loss: 1.18; acc: 0.67
Batch: 560; loss: 1.07; acc: 0.78
Batch: 580; loss: 1.13; acc: 0.67
Batch: 600; loss: 1.29; acc: 0.64
Batch: 620; loss: 1.28; acc: 0.62
Batch: 640; loss: 1.15; acc: 0.7
Batch: 660; loss: 1.28; acc: 0.64
Batch: 680; loss: 1.18; acc: 0.69
Batch: 700; loss: 1.2; acc: 0.7
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 1.26; acc: 0.56
Batch: 760; loss: 1.37; acc: 0.56
Batch: 780; loss: 1.14; acc: 0.75
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00018702047236729413
0.0001796705910237506
Batch: 0; loss: 1.09; acc: 0.72
Batch: 20; loss: 1.47; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 0.98; acc: 0.83
Val Epoch over. val_loss: 1.166524525660618; val_accuracy: 0.6880971337579618 

The current subspace-distance is: 0.0001796705910237506 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.73
Batch: 40; loss: 1.06; acc: 0.78
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 1.3; acc: 0.62
Batch: 100; loss: 1.28; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 1.21; acc: 0.7
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 1.21; acc: 0.67
Batch: 200; loss: 1.23; acc: 0.64
Batch: 220; loss: 1.16; acc: 0.69
Batch: 240; loss: 1.24; acc: 0.67
Batch: 260; loss: 1.33; acc: 0.66
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.38; acc: 0.56
Batch: 320; loss: 1.28; acc: 0.56
Batch: 340; loss: 1.24; acc: 0.66
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.12; acc: 0.64
Batch: 400; loss: 1.28; acc: 0.64
Batch: 420; loss: 1.52; acc: 0.59
Batch: 440; loss: 1.16; acc: 0.69
Batch: 460; loss: 1.35; acc: 0.67
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.4; acc: 0.56
Batch: 520; loss: 1.16; acc: 0.72
Batch: 540; loss: 1.3; acc: 0.7
Batch: 560; loss: 1.16; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.72
Batch: 600; loss: 1.42; acc: 0.66
Batch: 620; loss: 1.2; acc: 0.66
Batch: 640; loss: 1.01; acc: 0.73
Batch: 660; loss: 1.18; acc: 0.7
Batch: 680; loss: 1.22; acc: 0.62
Batch: 700; loss: 1.31; acc: 0.61
Batch: 720; loss: 1.18; acc: 0.73
Batch: 740; loss: 1.3; acc: 0.69
Batch: 760; loss: 1.19; acc: 0.66
Batch: 780; loss: 1.34; acc: 0.66
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00018294104665983468
0.0001749859657138586
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.47; acc: 0.55
Batch: 40; loss: 0.97; acc: 0.75
Batch: 60; loss: 1.17; acc: 0.73
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.19; acc: 0.62
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.97; acc: 0.81
Val Epoch over. val_loss: 1.1672662914178933; val_accuracy: 0.6899880573248408 

The current subspace-distance is: 0.0001749859657138586 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.64
Batch: 20; loss: 1.33; acc: 0.72
Batch: 40; loss: 1.14; acc: 0.7
Batch: 60; loss: 1.25; acc: 0.61
Batch: 80; loss: 1.19; acc: 0.7
Batch: 100; loss: 1.45; acc: 0.58
Batch: 120; loss: 1.31; acc: 0.66
Batch: 140; loss: 1.24; acc: 0.55
Batch: 160; loss: 1.31; acc: 0.67
Batch: 180; loss: 1.46; acc: 0.55
Batch: 200; loss: 1.24; acc: 0.61
Batch: 220; loss: 1.38; acc: 0.56
Batch: 240; loss: 1.33; acc: 0.59
Batch: 260; loss: 1.18; acc: 0.67
Batch: 280; loss: 1.23; acc: 0.59
Batch: 300; loss: 1.3; acc: 0.61
Batch: 320; loss: 1.17; acc: 0.62
Batch: 340; loss: 1.16; acc: 0.64
Batch: 360; loss: 1.09; acc: 0.66
Batch: 380; loss: 1.27; acc: 0.61
Batch: 400; loss: 1.28; acc: 0.55
Batch: 420; loss: 1.08; acc: 0.78
Batch: 440; loss: 1.24; acc: 0.56
Batch: 460; loss: 1.3; acc: 0.64
Batch: 480; loss: 1.16; acc: 0.7
Batch: 500; loss: 1.04; acc: 0.77
Batch: 520; loss: 1.25; acc: 0.7
Batch: 540; loss: 1.05; acc: 0.77
Batch: 560; loss: 1.36; acc: 0.62
Batch: 580; loss: 1.23; acc: 0.72
Batch: 600; loss: 1.19; acc: 0.69
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 1.3; acc: 0.62
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.2; acc: 0.69
Batch: 700; loss: 1.32; acc: 0.64
Batch: 720; loss: 1.05; acc: 0.7
Batch: 740; loss: 1.12; acc: 0.7
Batch: 760; loss: 1.13; acc: 0.69
Batch: 780; loss: 1.25; acc: 0.62
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.0001879500923678279
0.00018107914365828037
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.47; acc: 0.53
Batch: 40; loss: 0.97; acc: 0.77
Batch: 60; loss: 1.17; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 0.97; acc: 0.81
Val Epoch over. val_loss: 1.1672018728438456; val_accuracy: 0.6855095541401274 

The current subspace-distance is: 0.00018107914365828037 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.72
Batch: 40; loss: 1.37; acc: 0.61
Batch: 60; loss: 1.29; acc: 0.59
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.11; acc: 0.66
Batch: 120; loss: 1.17; acc: 0.77
Batch: 140; loss: 1.27; acc: 0.64
Batch: 160; loss: 1.19; acc: 0.75
Batch: 180; loss: 1.26; acc: 0.61
Batch: 200; loss: 1.08; acc: 0.72
Batch: 220; loss: 1.22; acc: 0.67
Batch: 240; loss: 1.19; acc: 0.64
Batch: 260; loss: 1.26; acc: 0.7
Batch: 280; loss: 1.12; acc: 0.73
Batch: 300; loss: 1.26; acc: 0.61
Batch: 320; loss: 1.26; acc: 0.64
Batch: 340; loss: 1.04; acc: 0.78
Batch: 360; loss: 1.31; acc: 0.56
Batch: 380; loss: 1.23; acc: 0.64
Batch: 400; loss: 1.12; acc: 0.72
Batch: 420; loss: 1.25; acc: 0.64
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 1.16; acc: 0.72
Batch: 480; loss: 1.23; acc: 0.66
Batch: 500; loss: 1.31; acc: 0.66
Batch: 520; loss: 1.34; acc: 0.62
Batch: 540; loss: 1.2; acc: 0.69
Batch: 560; loss: 1.23; acc: 0.69
Batch: 580; loss: 1.34; acc: 0.56
Batch: 600; loss: 1.16; acc: 0.72
Batch: 620; loss: 1.24; acc: 0.7
Batch: 640; loss: 1.21; acc: 0.72
Batch: 660; loss: 1.26; acc: 0.55
Batch: 680; loss: 1.23; acc: 0.67
Batch: 700; loss: 1.35; acc: 0.61
Batch: 720; loss: 1.32; acc: 0.56
Batch: 740; loss: 1.16; acc: 0.75
Batch: 760; loss: 1.12; acc: 0.73
Batch: 780; loss: 1.22; acc: 0.62
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00018755831115413457
0.00017985334852710366
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.48; acc: 0.52
Batch: 40; loss: 0.96; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.1636159260561512; val_accuracy: 0.6924761146496815 

The current subspace-distance is: 0.00017985334852710366 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 7.203731671848972

The number of parameters is: 274066

The number of individual parameters is:

58
580
58
58
87
50460
87
87
173
150510
173
173
64
66432
64
64
4096
64
640
10
64
64

nonzero elements in E: 54813196
elements in E: 54813200
fraction nonzero: 0.9999999270248772
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.63; acc: 0.05
Batch: 20; loss: 2.39; acc: 0.08
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.12; acc: 0.23
Batch: 80; loss: 2.1; acc: 0.2
Batch: 100; loss: 2.06; acc: 0.28
Batch: 120; loss: 1.93; acc: 0.44
Batch: 140; loss: 2.02; acc: 0.42
Batch: 160; loss: 1.88; acc: 0.45
Batch: 180; loss: 1.91; acc: 0.45
Batch: 200; loss: 1.83; acc: 0.52
Batch: 220; loss: 1.83; acc: 0.52
Batch: 240; loss: 1.77; acc: 0.59
Batch: 260; loss: 1.86; acc: 0.47
Batch: 280; loss: 1.98; acc: 0.31
Batch: 300; loss: 1.94; acc: 0.42
Batch: 320; loss: 1.91; acc: 0.45
Batch: 340; loss: 1.83; acc: 0.47
Batch: 360; loss: 1.9; acc: 0.42
Batch: 380; loss: 1.7; acc: 0.62
Batch: 400; loss: 1.75; acc: 0.47
Batch: 420; loss: 1.71; acc: 0.59
Batch: 440; loss: 1.69; acc: 0.61
Batch: 460; loss: 1.79; acc: 0.52
Batch: 480; loss: 1.76; acc: 0.5
Batch: 500; loss: 1.75; acc: 0.47
Batch: 520; loss: 1.75; acc: 0.5
Batch: 540; loss: 1.64; acc: 0.62
Batch: 560; loss: 1.68; acc: 0.52
Batch: 580; loss: 1.61; acc: 0.62
Batch: 600; loss: 1.61; acc: 0.55
Batch: 620; loss: 1.6; acc: 0.61
Batch: 640; loss: 1.57; acc: 0.61
Batch: 660; loss: 1.44; acc: 0.72
Batch: 680; loss: 1.61; acc: 0.62
Batch: 700; loss: 1.54; acc: 0.66
Batch: 720; loss: 1.51; acc: 0.66
Batch: 740; loss: 1.62; acc: 0.61
Batch: 760; loss: 1.5; acc: 0.59
Batch: 780; loss: 1.46; acc: 0.69
Train Epoch over. train_loss: 1.8; train_accuracy: 0.49 

6.199903873493895e-05
5.6710308854235336e-05
Batch: 0; loss: 1.47; acc: 0.66
Batch: 20; loss: 1.73; acc: 0.47
Batch: 40; loss: 1.3; acc: 0.78
Batch: 60; loss: 1.41; acc: 0.73
Batch: 80; loss: 1.43; acc: 0.67
Batch: 100; loss: 1.62; acc: 0.58
Batch: 120; loss: 1.66; acc: 0.69
Batch: 140; loss: 1.45; acc: 0.72
Val Epoch over. val_loss: 1.5166507253221646; val_accuracy: 0.6471934713375797 

The current subspace-distance is: 5.6710308854235336e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.46; acc: 0.7
Batch: 20; loss: 1.39; acc: 0.72
Batch: 40; loss: 1.55; acc: 0.62
Batch: 60; loss: 1.46; acc: 0.69
Batch: 80; loss: 1.44; acc: 0.72
Batch: 100; loss: 1.66; acc: 0.55
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.66
Batch: 160; loss: 1.52; acc: 0.62
Batch: 180; loss: 1.47; acc: 0.67
Batch: 200; loss: 1.48; acc: 0.62
Batch: 220; loss: 1.53; acc: 0.66
Batch: 240; loss: 1.36; acc: 0.73
Batch: 260; loss: 1.44; acc: 0.77
Batch: 280; loss: 1.52; acc: 0.61
Batch: 300; loss: 1.44; acc: 0.7
Batch: 320; loss: 1.34; acc: 0.8
Batch: 340; loss: 1.53; acc: 0.64
Batch: 360; loss: 1.39; acc: 0.69
Batch: 380; loss: 1.4; acc: 0.61
Batch: 400; loss: 1.44; acc: 0.72
Batch: 420; loss: 1.32; acc: 0.77
Batch: 440; loss: 1.34; acc: 0.69
Batch: 460; loss: 1.53; acc: 0.58
Batch: 480; loss: 1.55; acc: 0.52
Batch: 500; loss: 1.36; acc: 0.73
Batch: 520; loss: 1.54; acc: 0.61
Batch: 540; loss: 1.34; acc: 0.72
Batch: 560; loss: 1.27; acc: 0.77
Batch: 580; loss: 1.42; acc: 0.66
Batch: 600; loss: 1.46; acc: 0.62
Batch: 620; loss: 1.27; acc: 0.72
Batch: 640; loss: 1.35; acc: 0.59
Batch: 660; loss: 1.38; acc: 0.66
Batch: 680; loss: 1.43; acc: 0.7
Batch: 700; loss: 1.35; acc: 0.7
Batch: 720; loss: 1.48; acc: 0.61
Batch: 740; loss: 1.37; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.64
Batch: 780; loss: 1.38; acc: 0.67
Train Epoch over. train_loss: 1.44; train_accuracy: 0.66 

8.165504550561309e-05
7.661359995836392e-05
Batch: 0; loss: 1.29; acc: 0.73
Batch: 20; loss: 1.6; acc: 0.53
Batch: 40; loss: 1.09; acc: 0.84
Batch: 60; loss: 1.28; acc: 0.73
Batch: 80; loss: 1.2; acc: 0.8
Batch: 100; loss: 1.38; acc: 0.67
Batch: 120; loss: 1.48; acc: 0.7
Batch: 140; loss: 1.27; acc: 0.77
Val Epoch over. val_loss: 1.3192636325101184; val_accuracy: 0.6961584394904459 

The current subspace-distance is: 7.661359995836392e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.81
Batch: 20; loss: 1.25; acc: 0.73
Batch: 40; loss: 1.37; acc: 0.7
Batch: 60; loss: 1.49; acc: 0.62
Batch: 80; loss: 1.27; acc: 0.73
Batch: 100; loss: 1.34; acc: 0.64
Batch: 120; loss: 1.34; acc: 0.7
Batch: 140; loss: 1.48; acc: 0.58
Batch: 160; loss: 1.36; acc: 0.67
Batch: 180; loss: 1.36; acc: 0.66
Batch: 200; loss: 1.33; acc: 0.77
Batch: 220; loss: 1.2; acc: 0.8
Batch: 240; loss: 1.21; acc: 0.77
Batch: 260; loss: 1.35; acc: 0.67
Batch: 280; loss: 1.31; acc: 0.7
Batch: 300; loss: 1.28; acc: 0.78
Batch: 320; loss: 1.23; acc: 0.77
Batch: 340; loss: 1.31; acc: 0.77
Batch: 360; loss: 1.33; acc: 0.64
Batch: 380; loss: 1.36; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.73
Batch: 420; loss: 1.17; acc: 0.81
Batch: 440; loss: 1.31; acc: 0.69
Batch: 460; loss: 1.25; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.7
Batch: 500; loss: 1.34; acc: 0.7
Batch: 520; loss: 1.3; acc: 0.72
Batch: 540; loss: 1.37; acc: 0.66
Batch: 560; loss: 1.4; acc: 0.59
Batch: 580; loss: 1.62; acc: 0.47
Batch: 600; loss: 1.25; acc: 0.67
Batch: 620; loss: 1.32; acc: 0.62
Batch: 640; loss: 1.17; acc: 0.84
Batch: 660; loss: 1.35; acc: 0.69
Batch: 680; loss: 1.26; acc: 0.7
Batch: 700; loss: 1.19; acc: 0.69
Batch: 720; loss: 1.15; acc: 0.73
Batch: 740; loss: 1.3; acc: 0.67
Batch: 760; loss: 1.34; acc: 0.59
Batch: 780; loss: 1.24; acc: 0.72
Train Epoch over. train_loss: 1.31; train_accuracy: 0.69 

9.638841584092006e-05
9.143297938862815e-05
Batch: 0; loss: 1.19; acc: 0.73
Batch: 20; loss: 1.49; acc: 0.61
Batch: 40; loss: 0.97; acc: 0.83
Batch: 60; loss: 1.21; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.86
Batch: 100; loss: 1.27; acc: 0.77
Batch: 120; loss: 1.39; acc: 0.69
Batch: 140; loss: 1.2; acc: 0.69
Val Epoch over. val_loss: 1.212156195929096; val_accuracy: 0.7265127388535032 

The current subspace-distance is: 9.143297938862815e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.77
Batch: 20; loss: 1.35; acc: 0.59
Batch: 40; loss: 1.25; acc: 0.72
Batch: 60; loss: 1.26; acc: 0.7
Batch: 80; loss: 1.29; acc: 0.72
Batch: 100; loss: 1.31; acc: 0.67
Batch: 120; loss: 1.16; acc: 0.75
Batch: 140; loss: 1.38; acc: 0.58
Batch: 160; loss: 1.39; acc: 0.64
Batch: 180; loss: 1.15; acc: 0.75
Batch: 200; loss: 1.3; acc: 0.64
Batch: 220; loss: 1.3; acc: 0.62
Batch: 240; loss: 1.17; acc: 0.69
Batch: 260; loss: 1.32; acc: 0.61
Batch: 280; loss: 1.16; acc: 0.72
Batch: 300; loss: 1.09; acc: 0.77
Batch: 320; loss: 1.22; acc: 0.72
Batch: 340; loss: 1.18; acc: 0.73
Batch: 360; loss: 1.15; acc: 0.75
Batch: 380; loss: 1.21; acc: 0.7
Batch: 400; loss: 1.24; acc: 0.77
Batch: 420; loss: 1.26; acc: 0.62
Batch: 440; loss: 1.2; acc: 0.69
Batch: 460; loss: 1.22; acc: 0.69
Batch: 480; loss: 1.19; acc: 0.72
Batch: 500; loss: 1.24; acc: 0.69
Batch: 520; loss: 1.1; acc: 0.78
Batch: 540; loss: 1.26; acc: 0.69
Batch: 560; loss: 1.14; acc: 0.78
Batch: 580; loss: 1.18; acc: 0.73
Batch: 600; loss: 1.18; acc: 0.78
Batch: 620; loss: 1.14; acc: 0.72
Batch: 640; loss: 1.17; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.67
Batch: 680; loss: 1.07; acc: 0.8
Batch: 700; loss: 1.3; acc: 0.66
Batch: 720; loss: 1.15; acc: 0.78
Batch: 740; loss: 1.14; acc: 0.81
Batch: 760; loss: 1.21; acc: 0.75
Batch: 780; loss: 1.22; acc: 0.72
Train Epoch over. train_loss: 1.21; train_accuracy: 0.71 

0.00011107260070275515
0.00010486505925655365
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.44; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.91
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.24; acc: 0.69
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.09; acc: 0.67
Val Epoch over. val_loss: 1.1395419323520295; val_accuracy: 0.7466162420382165 

The current subspace-distance is: 0.00010486505925655365 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.21; acc: 0.73
Batch: 80; loss: 1.19; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.12; acc: 0.73
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.12; acc: 0.77
Batch: 180; loss: 1.08; acc: 0.78
Batch: 200; loss: 1.22; acc: 0.72
Batch: 220; loss: 1.11; acc: 0.69
Batch: 240; loss: 1.09; acc: 0.8
Batch: 260; loss: 1.02; acc: 0.8
Batch: 280; loss: 1.16; acc: 0.73
Batch: 300; loss: 1.17; acc: 0.62
Batch: 320; loss: 1.26; acc: 0.73
Batch: 340; loss: 1.14; acc: 0.73
Batch: 360; loss: 1.12; acc: 0.69
Batch: 380; loss: 1.12; acc: 0.67
Batch: 400; loss: 1.12; acc: 0.66
Batch: 420; loss: 0.91; acc: 0.86
Batch: 440; loss: 1.17; acc: 0.73
Batch: 460; loss: 1.2; acc: 0.75
Batch: 480; loss: 1.14; acc: 0.72
Batch: 500; loss: 1.29; acc: 0.64
Batch: 520; loss: 1.08; acc: 0.77
Batch: 540; loss: 1.1; acc: 0.73
Batch: 560; loss: 1.05; acc: 0.81
Batch: 580; loss: 0.97; acc: 0.8
Batch: 600; loss: 1.16; acc: 0.69
Batch: 620; loss: 1.09; acc: 0.77
Batch: 640; loss: 0.89; acc: 0.86
Batch: 660; loss: 1.05; acc: 0.7
Batch: 680; loss: 1.01; acc: 0.78
Batch: 700; loss: 1.24; acc: 0.73
Batch: 720; loss: 1.12; acc: 0.7
Batch: 740; loss: 1.08; acc: 0.78
Batch: 760; loss: 1.11; acc: 0.78
Batch: 780; loss: 1.21; acc: 0.69
Train Epoch over. train_loss: 1.12; train_accuracy: 0.74 

0.00012779720418620855
0.00012131204130128026
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 1.39; acc: 0.62
Batch: 40; loss: 0.8; acc: 0.91
Batch: 60; loss: 1.08; acc: 0.73
Batch: 80; loss: 0.9; acc: 0.86
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.0452514480633341; val_accuracy: 0.7710987261146497 

The current subspace-distance is: 0.00012131204130128026 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.78
Batch: 20; loss: 1.12; acc: 0.7
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 1.2; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.84
Batch: 100; loss: 0.92; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.81
Batch: 140; loss: 1.02; acc: 0.84
Batch: 160; loss: 1.06; acc: 0.78
Batch: 180; loss: 0.97; acc: 0.8
Batch: 200; loss: 0.91; acc: 0.84
Batch: 220; loss: 0.98; acc: 0.78
Batch: 240; loss: 1.02; acc: 0.77
Batch: 260; loss: 1.12; acc: 0.7
Batch: 280; loss: 1.01; acc: 0.84
Batch: 300; loss: 0.99; acc: 0.8
Batch: 320; loss: 1.01; acc: 0.8
Batch: 340; loss: 0.95; acc: 0.83
Batch: 360; loss: 1.1; acc: 0.8
Batch: 380; loss: 1.16; acc: 0.7
Batch: 400; loss: 1.2; acc: 0.72
Batch: 420; loss: 0.96; acc: 0.83
Batch: 440; loss: 1.19; acc: 0.72
Batch: 460; loss: 0.94; acc: 0.77
Batch: 480; loss: 1.19; acc: 0.62
Batch: 500; loss: 0.98; acc: 0.8
Batch: 520; loss: 0.96; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.8
Batch: 560; loss: 1.09; acc: 0.73
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.12; acc: 0.7
Batch: 620; loss: 1.03; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.75
Batch: 660; loss: 1.1; acc: 0.72
Batch: 680; loss: 0.99; acc: 0.84
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 1.24; acc: 0.69
Batch: 760; loss: 0.85; acc: 0.84
Batch: 780; loss: 1.08; acc: 0.73
Train Epoch over. train_loss: 1.05; train_accuracy: 0.76 

0.00014119810657575727
0.00013458677858579904
Batch: 0; loss: 0.91; acc: 0.8
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 0.71; acc: 0.97
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 0.83; acc: 0.86
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 0.956040637128672; val_accuracy: 0.7973726114649682 

The current subspace-distance is: 0.00013458677858579904 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.75
Batch: 20; loss: 0.93; acc: 0.81
Batch: 40; loss: 1.1; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.8
Batch: 80; loss: 0.97; acc: 0.83
Batch: 100; loss: 1.13; acc: 0.66
Batch: 120; loss: 0.99; acc: 0.81
Batch: 140; loss: 0.95; acc: 0.78
Batch: 160; loss: 0.92; acc: 0.83
Batch: 180; loss: 1.01; acc: 0.72
Batch: 200; loss: 1.09; acc: 0.69
Batch: 220; loss: 0.95; acc: 0.78
Batch: 240; loss: 0.89; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.81
Batch: 280; loss: 1.06; acc: 0.72
Batch: 300; loss: 0.91; acc: 0.78
Batch: 320; loss: 0.99; acc: 0.8
Batch: 340; loss: 0.94; acc: 0.77
Batch: 360; loss: 1.1; acc: 0.77
Batch: 380; loss: 0.94; acc: 0.8
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 0.86; acc: 0.83
Batch: 440; loss: 0.97; acc: 0.83
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.92; acc: 0.8
Batch: 520; loss: 1.04; acc: 0.72
Batch: 540; loss: 0.96; acc: 0.8
Batch: 560; loss: 0.96; acc: 0.75
Batch: 580; loss: 0.91; acc: 0.8
Batch: 600; loss: 0.89; acc: 0.83
Batch: 620; loss: 0.94; acc: 0.81
Batch: 640; loss: 1.0; acc: 0.81
Batch: 660; loss: 1.17; acc: 0.64
Batch: 680; loss: 1.11; acc: 0.7
Batch: 700; loss: 1.04; acc: 0.67
Batch: 720; loss: 0.95; acc: 0.8
Batch: 740; loss: 1.03; acc: 0.8
Batch: 760; loss: 1.01; acc: 0.73
Batch: 780; loss: 1.02; acc: 0.8
Train Epoch over. train_loss: 0.98; train_accuracy: 0.77 

0.00015510233060922474
0.0001488403941038996
Batch: 0; loss: 0.81; acc: 0.88
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.63; acc: 0.95
Batch: 60; loss: 0.99; acc: 0.7
Batch: 80; loss: 0.77; acc: 0.89
Batch: 100; loss: 0.97; acc: 0.81
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 0.84; acc: 0.81
Val Epoch over. val_loss: 0.8941559191722019; val_accuracy: 0.8038415605095541 

The current subspace-distance is: 0.0001488403941038996 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.88
Batch: 20; loss: 0.87; acc: 0.8
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 0.85; acc: 0.88
Batch: 100; loss: 0.94; acc: 0.77
Batch: 120; loss: 0.79; acc: 0.83
Batch: 140; loss: 0.97; acc: 0.8
Batch: 160; loss: 0.9; acc: 0.78
Batch: 180; loss: 1.07; acc: 0.73
Batch: 200; loss: 0.85; acc: 0.86
Batch: 220; loss: 1.06; acc: 0.75
Batch: 240; loss: 1.15; acc: 0.67
Batch: 260; loss: 1.1; acc: 0.7
Batch: 280; loss: 0.93; acc: 0.78
Batch: 300; loss: 1.09; acc: 0.75
Batch: 320; loss: 0.83; acc: 0.78
Batch: 340; loss: 0.88; acc: 0.84
Batch: 360; loss: 0.84; acc: 0.88
Batch: 380; loss: 0.94; acc: 0.78
Batch: 400; loss: 0.9; acc: 0.81
Batch: 420; loss: 0.87; acc: 0.88
Batch: 440; loss: 0.85; acc: 0.84
Batch: 460; loss: 0.86; acc: 0.81
Batch: 480; loss: 1.04; acc: 0.67
Batch: 500; loss: 0.88; acc: 0.75
Batch: 520; loss: 0.99; acc: 0.77
Batch: 540; loss: 0.73; acc: 0.89
Batch: 560; loss: 0.96; acc: 0.72
Batch: 580; loss: 1.08; acc: 0.7
Batch: 600; loss: 0.93; acc: 0.81
Batch: 620; loss: 1.1; acc: 0.7
Batch: 640; loss: 0.96; acc: 0.81
Batch: 660; loss: 0.94; acc: 0.78
Batch: 680; loss: 0.8; acc: 0.84
Batch: 700; loss: 0.81; acc: 0.75
Batch: 720; loss: 0.9; acc: 0.78
Batch: 740; loss: 0.85; acc: 0.83
Batch: 760; loss: 1.15; acc: 0.67
Batch: 780; loss: 0.83; acc: 0.84
Train Epoch over. train_loss: 0.92; train_accuracy: 0.78 

0.0001664205192355439
0.00016090630379039794
Batch: 0; loss: 0.74; acc: 0.92
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.57; acc: 0.94
Batch: 60; loss: 0.92; acc: 0.73
Batch: 80; loss: 0.74; acc: 0.88
Batch: 100; loss: 0.91; acc: 0.84
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.8334699387003661; val_accuracy: 0.8167794585987261 

The current subspace-distance is: 0.00016090630379039794 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 1.05; acc: 0.7
Batch: 40; loss: 0.92; acc: 0.77
Batch: 60; loss: 0.76; acc: 0.89
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 0.77; acc: 0.84
Batch: 120; loss: 0.85; acc: 0.83
Batch: 140; loss: 0.8; acc: 0.83
Batch: 160; loss: 0.98; acc: 0.8
Batch: 180; loss: 0.96; acc: 0.73
Batch: 200; loss: 0.91; acc: 0.77
Batch: 220; loss: 0.87; acc: 0.84
Batch: 240; loss: 0.87; acc: 0.78
Batch: 260; loss: 0.88; acc: 0.77
Batch: 280; loss: 1.05; acc: 0.73
Batch: 300; loss: 0.88; acc: 0.8
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.88
Batch: 360; loss: 1.02; acc: 0.66
Batch: 380; loss: 1.04; acc: 0.75
Batch: 400; loss: 0.91; acc: 0.78
Batch: 420; loss: 1.03; acc: 0.73
Batch: 440; loss: 0.82; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 0.89; acc: 0.78
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.73; acc: 0.88
Batch: 540; loss: 0.83; acc: 0.81
Batch: 560; loss: 0.88; acc: 0.77
Batch: 580; loss: 1.08; acc: 0.69
Batch: 600; loss: 1.01; acc: 0.72
Batch: 620; loss: 0.84; acc: 0.78
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.92; acc: 0.72
Batch: 680; loss: 0.77; acc: 0.83
Batch: 700; loss: 0.83; acc: 0.81
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.75; acc: 0.84
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.95; acc: 0.78
Train Epoch over. train_loss: 0.87; train_accuracy: 0.79 

0.00017573100922163576
0.00016861756739672273
Batch: 0; loss: 0.68; acc: 0.92
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 0.52; acc: 0.95
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 0.73; acc: 0.83
Val Epoch over. val_loss: 0.7966433152271684; val_accuracy: 0.8238455414012739 

The current subspace-distance is: 0.00016861756739672273 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.78
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.91; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.88
Batch: 140; loss: 0.93; acc: 0.75
Batch: 160; loss: 0.63; acc: 0.89
Batch: 180; loss: 0.91; acc: 0.75
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.95; acc: 0.72
Batch: 240; loss: 0.86; acc: 0.81
Batch: 260; loss: 1.02; acc: 0.72
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.92; acc: 0.8
Batch: 320; loss: 0.94; acc: 0.72
Batch: 340; loss: 0.85; acc: 0.78
Batch: 360; loss: 0.99; acc: 0.66
Batch: 380; loss: 0.76; acc: 0.91
Batch: 400; loss: 0.99; acc: 0.73
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.81
Batch: 480; loss: 0.89; acc: 0.73
Batch: 500; loss: 0.82; acc: 0.81
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.82; acc: 0.81
Batch: 560; loss: 0.89; acc: 0.8
Batch: 580; loss: 0.87; acc: 0.81
Batch: 600; loss: 0.8; acc: 0.81
Batch: 620; loss: 0.68; acc: 0.88
Batch: 640; loss: 0.88; acc: 0.8
Batch: 660; loss: 0.86; acc: 0.8
Batch: 680; loss: 0.87; acc: 0.75
Batch: 700; loss: 0.95; acc: 0.77
Batch: 720; loss: 0.91; acc: 0.75
Batch: 740; loss: 0.82; acc: 0.83
Batch: 760; loss: 0.76; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.84; train_accuracy: 0.8 

0.00018797941447701305
0.00017984042642638087
Batch: 0; loss: 0.63; acc: 0.91
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 0.47; acc: 0.97
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.62; acc: 0.88
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.68; acc: 0.88
Val Epoch over. val_loss: 0.7546706305947274; val_accuracy: 0.8269307324840764 

The current subspace-distance is: 0.00017984042642638087 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.83; acc: 0.84
Batch: 40; loss: 0.83; acc: 0.81
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.82; acc: 0.84
Batch: 180; loss: 0.81; acc: 0.78
Batch: 200; loss: 0.84; acc: 0.77
Batch: 220; loss: 0.77; acc: 0.86
Batch: 240; loss: 0.91; acc: 0.77
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.86; acc: 0.83
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 0.98; acc: 0.77
Batch: 340; loss: 0.88; acc: 0.73
Batch: 360; loss: 0.72; acc: 0.86
Batch: 380; loss: 1.01; acc: 0.69
Batch: 400; loss: 0.87; acc: 0.69
Batch: 420; loss: 0.77; acc: 0.84
Batch: 440; loss: 0.89; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.92; acc: 0.75
Batch: 500; loss: 0.91; acc: 0.73
Batch: 520; loss: 0.9; acc: 0.75
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.77; acc: 0.83
Batch: 580; loss: 0.96; acc: 0.7
Batch: 600; loss: 0.8; acc: 0.81
Batch: 620; loss: 0.92; acc: 0.77
Batch: 640; loss: 0.89; acc: 0.77
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.83; acc: 0.83
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.91
Batch: 740; loss: 0.85; acc: 0.78
Batch: 760; loss: 0.87; acc: 0.66
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.82; train_accuracy: 0.8 

0.00018878381524700671
0.00017945693980436772
Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.47; acc: 0.97
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.63; acc: 0.91
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.67; acc: 0.88
Val Epoch over. val_loss: 0.7581818377136424; val_accuracy: 0.8226512738853503 

The current subspace-distance is: 0.00017945693980436772 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.84; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.84
Batch: 140; loss: 0.79; acc: 0.8
Batch: 160; loss: 0.73; acc: 0.88
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.78
Batch: 220; loss: 0.74; acc: 0.86
Batch: 240; loss: 0.73; acc: 0.83
Batch: 260; loss: 0.69; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.86
Batch: 320; loss: 0.81; acc: 0.78
Batch: 340; loss: 0.91; acc: 0.73
Batch: 360; loss: 0.75; acc: 0.8
Batch: 380; loss: 0.69; acc: 0.88
Batch: 400; loss: 0.86; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.87; acc: 0.75
Batch: 480; loss: 0.85; acc: 0.8
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.89; acc: 0.72
Batch: 540; loss: 0.95; acc: 0.75
Batch: 560; loss: 0.86; acc: 0.77
Batch: 580; loss: 0.83; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 1.03; acc: 0.78
Batch: 640; loss: 0.8; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 0.79; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.92; acc: 0.75
Batch: 780; loss: 0.77; acc: 0.83
Train Epoch over. train_loss: 0.81; train_accuracy: 0.8 

0.00019256661471445113
0.00018342991825193167
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 0.47; acc: 0.97
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.66; acc: 0.88
Val Epoch over. val_loss: 0.7481850091439144; val_accuracy: 0.8264331210191083 

The current subspace-distance is: 0.00018342991825193167 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.87; acc: 0.72
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.81; acc: 0.81
Batch: 180; loss: 0.9; acc: 0.73
Batch: 200; loss: 0.85; acc: 0.81
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.8
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.74; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.88
Batch: 320; loss: 0.96; acc: 0.72
Batch: 340; loss: 0.79; acc: 0.86
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 0.93; acc: 0.77
Batch: 400; loss: 0.82; acc: 0.78
Batch: 420; loss: 0.79; acc: 0.8
Batch: 440; loss: 0.84; acc: 0.78
Batch: 460; loss: 0.75; acc: 0.83
Batch: 480; loss: 0.88; acc: 0.75
Batch: 500; loss: 0.85; acc: 0.77
Batch: 520; loss: 0.82; acc: 0.78
Batch: 540; loss: 0.76; acc: 0.8
Batch: 560; loss: 0.86; acc: 0.75
Batch: 580; loss: 0.84; acc: 0.77
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.76; acc: 0.84
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.9; acc: 0.75
Batch: 760; loss: 0.84; acc: 0.75
Batch: 780; loss: 0.88; acc: 0.8
Train Epoch over. train_loss: 0.8; train_accuracy: 0.8 

0.00019416477880440652
0.0001866284292191267
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.46; acc: 0.97
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.91
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.65; acc: 0.88
Val Epoch over. val_loss: 0.7388542622897276; val_accuracy: 0.8283240445859873 

The current subspace-distance is: 0.0001866284292191267 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 0.89; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.76; acc: 0.8
Batch: 180; loss: 0.85; acc: 0.78
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.85; acc: 0.83
Batch: 240; loss: 0.78; acc: 0.83
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 0.78; acc: 0.8
Batch: 320; loss: 0.73; acc: 0.81
Batch: 340; loss: 0.87; acc: 0.73
Batch: 360; loss: 0.85; acc: 0.78
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.81
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.9; acc: 0.72
Batch: 500; loss: 0.85; acc: 0.75
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.79; acc: 0.81
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.82; acc: 0.73
Batch: 620; loss: 0.81; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.87; acc: 0.8
Batch: 680; loss: 0.8; acc: 0.77
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.63; acc: 0.84
Batch: 780; loss: 1.05; acc: 0.67
Train Epoch over. train_loss: 0.79; train_accuracy: 0.8 

0.0001968531432794407
0.00018894912500400096
Batch: 0; loss: 0.59; acc: 0.92
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 0.46; acc: 0.95
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.61; acc: 0.91
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.86
Val Epoch over. val_loss: 0.737488920711408; val_accuracy: 0.825437898089172 

The current subspace-distance is: 0.00018894912500400096 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.0; acc: 0.66
Batch: 20; loss: 0.73; acc: 0.88
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.85; acc: 0.75
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.94; acc: 0.7
Batch: 160; loss: 0.65; acc: 0.89
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.85; acc: 0.81
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.75; acc: 0.78
Batch: 300; loss: 0.83; acc: 0.78
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.9; acc: 0.8
Batch: 360; loss: 0.77; acc: 0.86
Batch: 380; loss: 0.83; acc: 0.78
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.91
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.67; acc: 0.88
Batch: 500; loss: 0.92; acc: 0.73
Batch: 520; loss: 0.83; acc: 0.78
Batch: 540; loss: 0.75; acc: 0.88
Batch: 560; loss: 0.85; acc: 0.72
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.84
Batch: 640; loss: 0.75; acc: 0.81
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.8; acc: 0.77
Batch: 700; loss: 0.9; acc: 0.73
Batch: 720; loss: 0.95; acc: 0.72
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.8
Train Epoch over. train_loss: 0.79; train_accuracy: 0.8 

0.00020090262114536017
0.00019195390632376075
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.88
Val Epoch over. val_loss: 0.7156480763368546; val_accuracy: 0.8263335987261147 

The current subspace-distance is: 0.00019195390632376075 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.78; acc: 0.84
Batch: 20; loss: 0.94; acc: 0.8
Batch: 40; loss: 0.84; acc: 0.7
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.8; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 1.01; acc: 0.7
Batch: 160; loss: 0.8; acc: 0.77
Batch: 180; loss: 0.73; acc: 0.88
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.87; acc: 0.8
Batch: 240; loss: 0.87; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.78
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.7; acc: 0.89
Batch: 400; loss: 0.8; acc: 0.8
Batch: 420; loss: 0.97; acc: 0.81
Batch: 440; loss: 0.87; acc: 0.77
Batch: 460; loss: 0.9; acc: 0.69
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.78; acc: 0.8
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.96; acc: 0.73
Batch: 560; loss: 0.79; acc: 0.84
Batch: 580; loss: 0.91; acc: 0.75
Batch: 600; loss: 0.83; acc: 0.72
Batch: 620; loss: 0.87; acc: 0.7
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.85; acc: 0.8
Batch: 680; loss: 0.89; acc: 0.78
Batch: 700; loss: 0.63; acc: 0.89
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.75; acc: 0.83
Batch: 760; loss: 0.76; acc: 0.84
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.78; train_accuracy: 0.8 

0.00020047463476657867
0.0001938307104865089
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 1.08; acc: 0.7
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.89
Val Epoch over. val_loss: 0.7184991760618368; val_accuracy: 0.8321058917197452 

The current subspace-distance is: 0.0001938307104865089 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.62
Batch: 80; loss: 0.72; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.91
Batch: 180; loss: 0.83; acc: 0.78
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.84
Batch: 240; loss: 0.78; acc: 0.78
Batch: 260; loss: 0.69; acc: 0.84
Batch: 280; loss: 1.0; acc: 0.67
Batch: 300; loss: 0.89; acc: 0.77
Batch: 320; loss: 0.85; acc: 0.81
Batch: 340; loss: 0.84; acc: 0.8
Batch: 360; loss: 0.86; acc: 0.75
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.8; acc: 0.75
Batch: 460; loss: 0.79; acc: 0.84
Batch: 480; loss: 0.74; acc: 0.78
Batch: 500; loss: 0.64; acc: 0.94
Batch: 520; loss: 0.84; acc: 0.78
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.9; acc: 0.77
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.72; acc: 0.84
Batch: 740; loss: 0.78; acc: 0.8
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00020356569439172745
0.0001982623798539862
Batch: 0; loss: 0.56; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 0.7030173386358152; val_accuracy: 0.8338972929936306 

The current subspace-distance is: 0.0001982623798539862 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.78
Batch: 20; loss: 0.59; acc: 0.91
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.66; acc: 0.91
Batch: 100; loss: 0.94; acc: 0.73
Batch: 120; loss: 0.74; acc: 0.88
Batch: 140; loss: 0.78; acc: 0.84
Batch: 160; loss: 0.84; acc: 0.73
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.74; acc: 0.86
Batch: 260; loss: 0.89; acc: 0.77
Batch: 280; loss: 0.81; acc: 0.83
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 0.73; acc: 0.86
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.87; acc: 0.78
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.67; acc: 0.91
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.67; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.89
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.87; acc: 0.8
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.83; acc: 0.8
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.98; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.83
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.75; acc: 0.75
Batch: 680; loss: 0.96; acc: 0.73
Batch: 700; loss: 0.83; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.0002051682968158275
0.00019927765242755413
Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.88
Val Epoch over. val_loss: 0.7064536975067892; val_accuracy: 0.8302149681528662 

The current subspace-distance is: 0.00019927765242755413 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.91; acc: 0.69
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.73
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 1.03; acc: 0.66
Batch: 140; loss: 0.74; acc: 0.81
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.77; acc: 0.8
Batch: 200; loss: 0.81; acc: 0.75
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.84; acc: 0.77
Batch: 280; loss: 0.53; acc: 0.94
Batch: 300; loss: 0.91; acc: 0.75
Batch: 320; loss: 0.8; acc: 0.8
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.82; acc: 0.81
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.89
Batch: 440; loss: 0.75; acc: 0.84
Batch: 460; loss: 0.86; acc: 0.73
Batch: 480; loss: 0.76; acc: 0.84
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.74; acc: 0.84
Batch: 540; loss: 0.9; acc: 0.77
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.89
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.77
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.75; acc: 0.81
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.75; acc: 0.8
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.73; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00020633739768527448
0.0001988467702176422
Batch: 0; loss: 0.55; acc: 0.92
Batch: 20; loss: 1.08; acc: 0.67
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.61; acc: 0.88
Val Epoch over. val_loss: 0.7023633993734979; val_accuracy: 0.8303144904458599 

The current subspace-distance is: 0.0001988467702176422 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.83; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.95; acc: 0.75
Batch: 160; loss: 0.78; acc: 0.8
Batch: 180; loss: 0.8; acc: 0.77
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.86; acc: 0.8
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.71; acc: 0.84
Batch: 300; loss: 0.88; acc: 0.77
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.83
Batch: 360; loss: 0.73; acc: 0.86
Batch: 380; loss: 0.78; acc: 0.83
Batch: 400; loss: 0.94; acc: 0.72
Batch: 420; loss: 0.72; acc: 0.86
Batch: 440; loss: 0.93; acc: 0.73
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.92; acc: 0.72
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.78; acc: 0.8
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.86; acc: 0.75
Batch: 600; loss: 0.71; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.8
Batch: 640; loss: 0.88; acc: 0.73
Batch: 660; loss: 0.75; acc: 0.86
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 0.9; acc: 0.77
Batch: 740; loss: 0.77; acc: 0.77
Batch: 760; loss: 0.83; acc: 0.78
Batch: 780; loss: 0.84; acc: 0.77
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00021187504171393812
0.0002037099184235558
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.84
Val Epoch over. val_loss: 0.6917146195651619; val_accuracy: 0.8351910828025477 

The current subspace-distance is: 0.0002037099184235558 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.97; acc: 0.77
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.86; acc: 0.83
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.81
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.8; acc: 0.8
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.76; acc: 0.84
Batch: 320; loss: 0.99; acc: 0.72
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.86; acc: 0.8
Batch: 380; loss: 0.61; acc: 0.92
Batch: 400; loss: 0.67; acc: 0.81
Batch: 420; loss: 0.77; acc: 0.77
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.77; acc: 0.8
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.77; acc: 0.75
Batch: 540; loss: 0.71; acc: 0.88
Batch: 560; loss: 0.77; acc: 0.73
Batch: 580; loss: 0.73; acc: 0.81
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.79; acc: 0.77
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.97; acc: 0.73
Batch: 680; loss: 0.76; acc: 0.78
Batch: 700; loss: 0.87; acc: 0.75
Batch: 720; loss: 0.8; acc: 0.8
Batch: 740; loss: 0.76; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021034249220974743
0.00020344453514553607
Batch: 0; loss: 0.55; acc: 0.92
Batch: 20; loss: 1.06; acc: 0.66
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.6909881674560012; val_accuracy: 0.8334992038216561 

The current subspace-distance is: 0.00020344453514553607 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.78
Batch: 160; loss: 0.7; acc: 0.84
Batch: 180; loss: 0.67; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.74; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.77; acc: 0.78
Batch: 280; loss: 0.9; acc: 0.75
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.75; acc: 0.83
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.73; acc: 0.81
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 0.8; acc: 0.78
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.74; acc: 0.83
Batch: 500; loss: 0.85; acc: 0.75
Batch: 520; loss: 1.04; acc: 0.75
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.74; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.91; acc: 0.7
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.92; acc: 0.7
Batch: 720; loss: 0.74; acc: 0.77
Batch: 740; loss: 0.79; acc: 0.8
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.83; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021020520944148302
0.00020446983398869634
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.66
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.56; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.86
Val Epoch over. val_loss: 0.6853109112211094; val_accuracy: 0.832703025477707 

The current subspace-distance is: 0.00020446983398869634 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.84
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.75
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.83; acc: 0.77
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.83; acc: 0.81
Batch: 280; loss: 0.94; acc: 0.7
Batch: 300; loss: 0.75; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 0.84; acc: 0.78
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.81
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.96; acc: 0.73
Batch: 520; loss: 0.5; acc: 0.92
Batch: 540; loss: 0.56; acc: 0.92
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 0.6; acc: 0.89
Batch: 620; loss: 0.85; acc: 0.8
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.78; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.95; acc: 0.77
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.91
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00020907967700622976
0.00020150518685113639
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 1.05; acc: 0.66
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.6836533924196936; val_accuracy: 0.8343949044585988 

The current subspace-distance is: 0.00020150518685113639 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.91
Batch: 140; loss: 0.85; acc: 0.7
Batch: 160; loss: 0.66; acc: 0.78
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.76; acc: 0.78
Batch: 240; loss: 0.72; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.85; acc: 0.77
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.62; acc: 0.88
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.89; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.89; acc: 0.73
Batch: 440; loss: 0.69; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.87; acc: 0.77
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.82; acc: 0.77
Batch: 660; loss: 0.82; acc: 0.77
Batch: 680; loss: 0.79; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.72; acc: 0.83
Batch: 740; loss: 0.93; acc: 0.75
Batch: 760; loss: 0.76; acc: 0.84
Batch: 780; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.0002125571627402678
0.0002031470212386921
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 1.05; acc: 0.69
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.59; acc: 0.86
Val Epoch over. val_loss: 0.6845228347429044; val_accuracy: 0.8382762738853503 

The current subspace-distance is: 0.0002031470212386921 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.94
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.88; acc: 0.73
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.86
Batch: 140; loss: 0.73; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.93; acc: 0.75
Batch: 220; loss: 0.74; acc: 0.77
Batch: 240; loss: 0.91; acc: 0.75
Batch: 260; loss: 0.71; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.86
Batch: 340; loss: 0.77; acc: 0.78
Batch: 360; loss: 0.67; acc: 0.88
Batch: 380; loss: 0.78; acc: 0.81
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.86; acc: 0.72
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 0.77; acc: 0.86
Batch: 500; loss: 0.75; acc: 0.81
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.8; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.78
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.78; acc: 0.83
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.93; acc: 0.72
Batch: 740; loss: 0.75; acc: 0.84
Batch: 760; loss: 0.76; acc: 0.77
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021106631902512163
0.00020196742843836546
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.56; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.89
Val Epoch over. val_loss: 0.6857434541556486; val_accuracy: 0.8343949044585988 

The current subspace-distance is: 0.00020196742843836546 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.89
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.73; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.8; acc: 0.77
Batch: 440; loss: 0.59; acc: 0.89
Batch: 460; loss: 0.79; acc: 0.75
Batch: 480; loss: 0.8; acc: 0.73
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.73; acc: 0.75
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 0.96; acc: 0.72
Batch: 580; loss: 0.87; acc: 0.8
Batch: 600; loss: 1.03; acc: 0.69
Batch: 620; loss: 0.75; acc: 0.8
Batch: 640; loss: 0.81; acc: 0.83
Batch: 660; loss: 0.76; acc: 0.81
Batch: 680; loss: 0.73; acc: 0.73
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.82; acc: 0.72
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.97; acc: 0.67
Batch: 780; loss: 0.73; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.0002142781304428354
0.00020590763597283512
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 1.06; acc: 0.7
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.86
Val Epoch over. val_loss: 0.6792613171088467; val_accuracy: 0.8365843949044586 

The current subspace-distance is: 0.00020590763597283512 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.7; acc: 0.86
Batch: 160; loss: 0.81; acc: 0.75
Batch: 180; loss: 0.75; acc: 0.72
Batch: 200; loss: 0.92; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.7; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.81
Batch: 320; loss: 0.68; acc: 0.86
Batch: 340; loss: 0.91; acc: 0.8
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.78; acc: 0.81
Batch: 460; loss: 0.8; acc: 0.75
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.84
Batch: 560; loss: 0.84; acc: 0.78
Batch: 580; loss: 0.6; acc: 0.91
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.78; acc: 0.8
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.76; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.89; acc: 0.78
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00021441359422169626
0.00020710084936581552
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.88
Val Epoch over. val_loss: 0.6898747306720466; val_accuracy: 0.8322054140127388 

The current subspace-distance is: 0.00020710084936581552 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.76; acc: 0.88
Batch: 80; loss: 0.78; acc: 0.81
Batch: 100; loss: 0.87; acc: 0.72
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.85; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.89
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.84; acc: 0.8
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.78; acc: 0.81
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.72; acc: 0.77
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.93; acc: 0.73
Batch: 460; loss: 0.84; acc: 0.77
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.84; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.74; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.8
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 0.95; acc: 0.72
Batch: 700; loss: 0.99; acc: 0.73
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.83
Batch: 760; loss: 0.8; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.92
Train Epoch over. train_loss: 0.74; train_accuracy: 0.81 

0.0002115750830853358
0.000203996678465046
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.6889422220788943; val_accuracy: 0.833797770700637 

The current subspace-distance is: 0.000203996678465046 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.92
Batch: 140; loss: 0.83; acc: 0.72
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.92; acc: 0.73
Batch: 220; loss: 0.89; acc: 0.73
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.67; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.8
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 1.01; acc: 0.69
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.89
Batch: 440; loss: 0.85; acc: 0.75
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.95; acc: 0.7
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.86
Batch: 580; loss: 0.73; acc: 0.83
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 0.87; acc: 0.73
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.81; acc: 0.78
Batch: 680; loss: 0.79; acc: 0.8
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.78
Batch: 740; loss: 0.78; acc: 0.8
Batch: 760; loss: 0.78; acc: 0.72
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.74; train_accuracy: 0.81 

0.00021490732615347952
0.00020622281590476632
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.73
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.58; acc: 0.86
Val Epoch over. val_loss: 0.6823662828867603; val_accuracy: 0.8336982484076433 

The current subspace-distance is: 0.00020622281590476632 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 1.09; acc: 0.7
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.86
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.91
Batch: 140; loss: 0.91; acc: 0.75
Batch: 160; loss: 0.81; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.84
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 0.77; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.92
Batch: 260; loss: 0.64; acc: 0.88
Batch: 280; loss: 0.86; acc: 0.78
Batch: 300; loss: 0.92; acc: 0.77
Batch: 320; loss: 0.65; acc: 0.86
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.65; acc: 0.89
Batch: 380; loss: 0.89; acc: 0.78
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.74; acc: 0.88
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.98; acc: 0.73
Batch: 520; loss: 1.03; acc: 0.7
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.8; acc: 0.75
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.94; acc: 0.72
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.74; train_accuracy: 0.81 

0.0002179406292270869
0.0002104213199345395
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.675306308231536; val_accuracy: 0.8373805732484076 

The current subspace-distance is: 0.0002104213199345395 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 7.203731671848972

The number of parameters is: 274066

The number of individual parameters is:

58
580
58
58
87
50460
87
87
173
150510
173
173
64
66432
64
64
4096
64
640
10
64
64

nonzero elements in E: 82219792
elements in E: 82219800
fraction nonzero: 0.9999999026998363
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.55; acc: 0.09
Batch: 20; loss: 2.38; acc: 0.11
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.04; acc: 0.39
Batch: 80; loss: 1.92; acc: 0.38
Batch: 100; loss: 1.82; acc: 0.5
Batch: 120; loss: 1.9; acc: 0.41
Batch: 140; loss: 1.78; acc: 0.44
Batch: 160; loss: 1.72; acc: 0.56
Batch: 180; loss: 1.75; acc: 0.52
Batch: 200; loss: 1.71; acc: 0.58
Batch: 220; loss: 1.56; acc: 0.69
Batch: 240; loss: 1.6; acc: 0.66
Batch: 260; loss: 1.58; acc: 0.62
Batch: 280; loss: 1.56; acc: 0.72
Batch: 300; loss: 1.63; acc: 0.61
Batch: 320; loss: 1.53; acc: 0.64
Batch: 340; loss: 1.55; acc: 0.66
Batch: 360; loss: 1.51; acc: 0.64
Batch: 380; loss: 1.52; acc: 0.72
Batch: 400; loss: 1.59; acc: 0.53
Batch: 420; loss: 1.35; acc: 0.75
Batch: 440; loss: 1.47; acc: 0.64
Batch: 460; loss: 1.43; acc: 0.66
Batch: 480; loss: 1.4; acc: 0.73
Batch: 500; loss: 1.52; acc: 0.64
Batch: 520; loss: 1.48; acc: 0.61
Batch: 540; loss: 1.41; acc: 0.66
Batch: 560; loss: 1.45; acc: 0.67
Batch: 580; loss: 1.28; acc: 0.73
Batch: 600; loss: 1.47; acc: 0.66
Batch: 620; loss: 1.35; acc: 0.75
Batch: 640; loss: 1.38; acc: 0.66
Batch: 660; loss: 1.33; acc: 0.64
Batch: 680; loss: 1.37; acc: 0.62
Batch: 700; loss: 1.35; acc: 0.69
Batch: 720; loss: 1.35; acc: 0.72
Batch: 740; loss: 1.32; acc: 0.69
Batch: 760; loss: 1.51; acc: 0.59
Batch: 780; loss: 1.29; acc: 0.77
Train Epoch over. train_loss: 1.58; train_accuracy: 0.6 

6.38119745417498e-05
5.814600444864482e-05
Batch: 0; loss: 1.31; acc: 0.7
Batch: 20; loss: 1.54; acc: 0.56
Batch: 40; loss: 0.99; acc: 0.91
Batch: 60; loss: 1.16; acc: 0.83
Batch: 80; loss: 1.13; acc: 0.83
Batch: 100; loss: 1.19; acc: 0.77
Batch: 120; loss: 1.36; acc: 0.69
Batch: 140; loss: 1.13; acc: 0.81
Val Epoch over. val_loss: 1.2530373217193944; val_accuracy: 0.7573646496815286 

The current subspace-distance is: 5.814600444864482e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.39; acc: 0.66
Batch: 20; loss: 1.25; acc: 0.75
Batch: 40; loss: 1.33; acc: 0.78
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.17; acc: 0.75
Batch: 100; loss: 1.19; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 1.16; acc: 0.75
Batch: 160; loss: 1.27; acc: 0.7
Batch: 180; loss: 1.27; acc: 0.69
Batch: 200; loss: 1.25; acc: 0.7
Batch: 220; loss: 1.26; acc: 0.73
Batch: 240; loss: 1.17; acc: 0.78
Batch: 260; loss: 1.29; acc: 0.67
Batch: 280; loss: 1.1; acc: 0.78
Batch: 300; loss: 1.3; acc: 0.69
Batch: 320; loss: 1.06; acc: 0.86
Batch: 340; loss: 1.24; acc: 0.69
Batch: 360; loss: 1.26; acc: 0.73
Batch: 380; loss: 1.28; acc: 0.67
Batch: 400; loss: 1.19; acc: 0.73
Batch: 420; loss: 1.13; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.77
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.04; acc: 0.83
Batch: 500; loss: 1.21; acc: 0.7
Batch: 520; loss: 1.08; acc: 0.83
Batch: 540; loss: 1.14; acc: 0.81
Batch: 560; loss: 1.15; acc: 0.7
Batch: 580; loss: 1.25; acc: 0.64
Batch: 600; loss: 1.24; acc: 0.66
Batch: 620; loss: 1.09; acc: 0.8
Batch: 640; loss: 1.04; acc: 0.78
Batch: 660; loss: 1.14; acc: 0.75
Batch: 680; loss: 1.11; acc: 0.78
Batch: 700; loss: 1.09; acc: 0.83
Batch: 720; loss: 1.07; acc: 0.83
Batch: 740; loss: 1.21; acc: 0.72
Batch: 760; loss: 1.09; acc: 0.75
Batch: 780; loss: 1.01; acc: 0.83
Train Epoch over. train_loss: 1.19; train_accuracy: 0.75 

8.59789943206124e-05
8.030629396671429e-05
Batch: 0; loss: 1.05; acc: 0.8
Batch: 20; loss: 1.36; acc: 0.59
Batch: 40; loss: 0.8; acc: 0.95
Batch: 60; loss: 0.97; acc: 0.88
Batch: 80; loss: 0.94; acc: 0.89
Batch: 100; loss: 1.01; acc: 0.88
Batch: 120; loss: 1.22; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.91
Val Epoch over. val_loss: 1.0552617683532133; val_accuracy: 0.8031449044585988 

The current subspace-distance is: 8.030629396671429e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.83
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.73
Batch: 60; loss: 1.1; acc: 0.78
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 0.96; acc: 0.84
Batch: 120; loss: 1.1; acc: 0.78
Batch: 140; loss: 1.14; acc: 0.75
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 0.98; acc: 0.84
Batch: 200; loss: 1.05; acc: 0.78
Batch: 220; loss: 1.1; acc: 0.81
Batch: 240; loss: 1.07; acc: 0.8
Batch: 260; loss: 1.17; acc: 0.72
Batch: 280; loss: 1.02; acc: 0.81
Batch: 300; loss: 0.93; acc: 0.8
Batch: 320; loss: 1.18; acc: 0.67
Batch: 340; loss: 1.15; acc: 0.78
Batch: 360; loss: 1.04; acc: 0.75
Batch: 380; loss: 1.03; acc: 0.77
Batch: 400; loss: 0.88; acc: 0.92
Batch: 420; loss: 0.98; acc: 0.8
Batch: 440; loss: 1.0; acc: 0.73
Batch: 460; loss: 1.11; acc: 0.77
Batch: 480; loss: 1.1; acc: 0.75
Batch: 500; loss: 1.03; acc: 0.8
Batch: 520; loss: 1.07; acc: 0.73
Batch: 540; loss: 0.98; acc: 0.8
Batch: 560; loss: 1.01; acc: 0.78
Batch: 580; loss: 1.0; acc: 0.73
Batch: 600; loss: 1.29; acc: 0.64
Batch: 620; loss: 0.95; acc: 0.83
Batch: 640; loss: 0.95; acc: 0.84
Batch: 660; loss: 0.91; acc: 0.89
Batch: 680; loss: 0.9; acc: 0.88
Batch: 700; loss: 0.86; acc: 0.88
Batch: 720; loss: 1.0; acc: 0.83
Batch: 740; loss: 0.96; acc: 0.78
Batch: 760; loss: 0.93; acc: 0.81
Batch: 780; loss: 0.97; acc: 0.83
Train Epoch over. train_loss: 1.03; train_accuracy: 0.79 

0.0001038944537867792
9.757919906405732e-05
Batch: 0; loss: 0.9; acc: 0.86
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.65; acc: 0.92
Batch: 60; loss: 0.84; acc: 0.83
Batch: 80; loss: 0.78; acc: 0.89
Batch: 100; loss: 0.89; acc: 0.89
Batch: 120; loss: 1.1; acc: 0.7
Batch: 140; loss: 0.83; acc: 0.88
Val Epoch over. val_loss: 0.9225853465165302; val_accuracy: 0.8138933121019108 

The current subspace-distance is: 9.757919906405732e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.88
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 0.83; acc: 0.89
Batch: 100; loss: 0.93; acc: 0.75
Batch: 120; loss: 0.93; acc: 0.84
Batch: 140; loss: 0.92; acc: 0.83
Batch: 160; loss: 0.79; acc: 0.94
Batch: 180; loss: 0.83; acc: 0.88
Batch: 200; loss: 0.96; acc: 0.8
Batch: 220; loss: 0.99; acc: 0.8
Batch: 240; loss: 0.99; acc: 0.75
Batch: 260; loss: 0.95; acc: 0.75
Batch: 280; loss: 1.18; acc: 0.64
Batch: 300; loss: 0.93; acc: 0.81
Batch: 320; loss: 0.93; acc: 0.8
Batch: 340; loss: 1.09; acc: 0.69
Batch: 360; loss: 0.97; acc: 0.8
Batch: 380; loss: 1.03; acc: 0.77
Batch: 400; loss: 1.07; acc: 0.73
Batch: 420; loss: 0.96; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.83
Batch: 460; loss: 0.99; acc: 0.83
Batch: 480; loss: 1.0; acc: 0.77
Batch: 500; loss: 0.72; acc: 0.94
Batch: 520; loss: 0.83; acc: 0.83
Batch: 540; loss: 0.89; acc: 0.81
Batch: 560; loss: 1.0; acc: 0.75
Batch: 580; loss: 1.06; acc: 0.77
Batch: 600; loss: 0.98; acc: 0.78
Batch: 620; loss: 0.92; acc: 0.8
Batch: 640; loss: 0.92; acc: 0.78
Batch: 660; loss: 0.88; acc: 0.86
Batch: 680; loss: 0.92; acc: 0.81
Batch: 700; loss: 0.86; acc: 0.84
Batch: 720; loss: 0.86; acc: 0.83
Batch: 740; loss: 0.93; acc: 0.8
Batch: 760; loss: 0.93; acc: 0.8
Batch: 780; loss: 0.94; acc: 0.77
Train Epoch over. train_loss: 0.94; train_accuracy: 0.8 

0.00011770931450882927
0.0001122134635807015
Batch: 0; loss: 0.83; acc: 0.84
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.59; acc: 0.95
Batch: 60; loss: 0.79; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.94
Batch: 100; loss: 0.82; acc: 0.92
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.89
Val Epoch over. val_loss: 0.8440844507733728; val_accuracy: 0.835390127388535 

The current subspace-distance is: 0.0001122134635807015 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.73
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 0.94; acc: 0.8
Batch: 80; loss: 0.8; acc: 0.91
Batch: 100; loss: 0.81; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.91
Batch: 140; loss: 0.86; acc: 0.84
Batch: 160; loss: 0.93; acc: 0.8
Batch: 180; loss: 0.96; acc: 0.8
Batch: 200; loss: 0.92; acc: 0.84
Batch: 220; loss: 0.94; acc: 0.8
Batch: 240; loss: 1.01; acc: 0.8
Batch: 260; loss: 0.87; acc: 0.78
Batch: 280; loss: 0.85; acc: 0.83
Batch: 300; loss: 0.95; acc: 0.81
Batch: 320; loss: 0.9; acc: 0.78
Batch: 340; loss: 0.96; acc: 0.8
Batch: 360; loss: 0.89; acc: 0.83
Batch: 380; loss: 0.86; acc: 0.86
Batch: 400; loss: 0.86; acc: 0.84
Batch: 420; loss: 0.89; acc: 0.8
Batch: 440; loss: 0.85; acc: 0.83
Batch: 460; loss: 0.96; acc: 0.73
Batch: 480; loss: 0.77; acc: 0.88
Batch: 500; loss: 0.81; acc: 0.86
Batch: 520; loss: 0.85; acc: 0.88
Batch: 540; loss: 0.78; acc: 0.86
Batch: 560; loss: 0.98; acc: 0.72
Batch: 580; loss: 0.93; acc: 0.81
Batch: 600; loss: 0.92; acc: 0.77
Batch: 620; loss: 0.82; acc: 0.83
Batch: 640; loss: 0.9; acc: 0.77
Batch: 660; loss: 0.9; acc: 0.78
Batch: 680; loss: 0.8; acc: 0.88
Batch: 700; loss: 0.79; acc: 0.86
Batch: 720; loss: 0.86; acc: 0.81
Batch: 740; loss: 0.85; acc: 0.8
Batch: 760; loss: 0.85; acc: 0.83
Batch: 780; loss: 0.84; acc: 0.81
Train Epoch over. train_loss: 0.87; train_accuracy: 0.82 

0.00013153541658539325
0.0001272662921110168
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 0.54; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.92
Batch: 100; loss: 0.77; acc: 0.92
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.89
Val Epoch over. val_loss: 0.7829829224735309; val_accuracy: 0.8416600318471338 

The current subspace-distance is: 0.0001272662921110168 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.89
Batch: 40; loss: 0.83; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.91
Batch: 80; loss: 0.84; acc: 0.86
Batch: 100; loss: 0.79; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.86; acc: 0.8
Batch: 160; loss: 0.97; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.91
Batch: 200; loss: 0.75; acc: 0.88
Batch: 220; loss: 0.84; acc: 0.84
Batch: 240; loss: 0.78; acc: 0.86
Batch: 260; loss: 0.85; acc: 0.81
Batch: 280; loss: 0.86; acc: 0.78
Batch: 300; loss: 0.75; acc: 0.84
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 0.85; acc: 0.83
Batch: 380; loss: 0.93; acc: 0.78
Batch: 400; loss: 0.69; acc: 0.86
Batch: 420; loss: 0.91; acc: 0.81
Batch: 440; loss: 0.79; acc: 0.86
Batch: 460; loss: 0.83; acc: 0.84
Batch: 480; loss: 0.96; acc: 0.75
Batch: 500; loss: 0.93; acc: 0.77
Batch: 520; loss: 0.74; acc: 0.89
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.84; acc: 0.8
Batch: 580; loss: 0.78; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.88
Batch: 620; loss: 0.78; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.86
Batch: 660; loss: 0.83; acc: 0.78
Batch: 680; loss: 0.94; acc: 0.77
Batch: 700; loss: 0.83; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.88
Batch: 740; loss: 0.81; acc: 0.81
Batch: 760; loss: 0.93; acc: 0.83
Batch: 780; loss: 0.92; acc: 0.8
Train Epoch over. train_loss: 0.82; train_accuracy: 0.82 

0.00014359396300278604
0.00013818756269756705
Batch: 0; loss: 0.72; acc: 0.88
Batch: 20; loss: 1.09; acc: 0.75
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.92
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.64; acc: 0.88
Val Epoch over. val_loss: 0.7444166258262221; val_accuracy: 0.8495222929936306 

The current subspace-distance is: 0.00013818756269756705 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.92
Batch: 20; loss: 0.83; acc: 0.83
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.8; acc: 0.78
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.89
Batch: 160; loss: 0.69; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.88
Batch: 220; loss: 0.89; acc: 0.8
Batch: 240; loss: 0.89; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.95; acc: 0.78
Batch: 300; loss: 0.83; acc: 0.84
Batch: 320; loss: 0.79; acc: 0.86
Batch: 340; loss: 0.79; acc: 0.8
Batch: 360; loss: 0.81; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.88; acc: 0.86
Batch: 420; loss: 0.83; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.91
Batch: 460; loss: 0.8; acc: 0.81
Batch: 480; loss: 0.73; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.71; acc: 0.86
Batch: 540; loss: 0.83; acc: 0.84
Batch: 560; loss: 0.79; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.8
Batch: 600; loss: 0.87; acc: 0.75
Batch: 620; loss: 0.74; acc: 0.78
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.93; acc: 0.77
Batch: 680; loss: 0.72; acc: 0.91
Batch: 700; loss: 0.84; acc: 0.78
Batch: 720; loss: 0.81; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.81
Batch: 760; loss: 1.08; acc: 0.67
Batch: 780; loss: 0.8; acc: 0.83
Train Epoch over. train_loss: 0.79; train_accuracy: 0.83 

0.00015370809705927968
0.0001463614753447473
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 0.46; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.7; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.89
Val Epoch over. val_loss: 0.7089580623966873; val_accuracy: 0.8526074840764332 

The current subspace-distance is: 0.0001463614753447473 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.88
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.83
Batch: 140; loss: 0.73; acc: 0.84
Batch: 160; loss: 0.76; acc: 0.88
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.91; acc: 0.8
Batch: 220; loss: 0.76; acc: 0.86
Batch: 240; loss: 0.75; acc: 0.81
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.89; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.84
Batch: 320; loss: 0.88; acc: 0.75
Batch: 340; loss: 0.97; acc: 0.77
Batch: 360; loss: 0.72; acc: 0.88
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.92
Batch: 420; loss: 0.6; acc: 0.89
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.75; acc: 0.83
Batch: 480; loss: 0.77; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.89
Batch: 520; loss: 0.8; acc: 0.78
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.79; acc: 0.78
Batch: 620; loss: 0.68; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.88
Batch: 660; loss: 0.66; acc: 0.88
Batch: 680; loss: 0.86; acc: 0.75
Batch: 700; loss: 0.67; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.92
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.83 

0.00016338196292053908
0.0001572337350808084
Batch: 0; loss: 0.62; acc: 0.94
Batch: 20; loss: 0.95; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.6; acc: 0.89
Val Epoch over. val_loss: 0.674369557647948; val_accuracy: 0.8539012738853503 

The current subspace-distance is: 0.0001572337350808084 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.94
Batch: 180; loss: 0.71; acc: 0.84
Batch: 200; loss: 0.75; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.81
Batch: 240; loss: 0.61; acc: 0.92
Batch: 260; loss: 0.8; acc: 0.81
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.71; acc: 0.88
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.75; acc: 0.83
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.64; acc: 0.91
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.74; acc: 0.81
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.75; acc: 0.84
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.98; acc: 0.72
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.91
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.62; acc: 0.89
Batch: 700; loss: 0.78; acc: 0.83
Batch: 720; loss: 0.54; acc: 0.88
Batch: 740; loss: 0.88; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.92
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.84 

0.00017307905363850296
0.00016509689157828689
Batch: 0; loss: 0.63; acc: 0.92
Batch: 20; loss: 0.91; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.74; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.57; acc: 0.88
Val Epoch over. val_loss: 0.6528049834594605; val_accuracy: 0.8609673566878981 

The current subspace-distance is: 0.00016509689157828689 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.89
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.85; acc: 0.78
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.83; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.79; acc: 0.8
Batch: 400; loss: 0.63; acc: 0.89
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.92
Batch: 500; loss: 0.78; acc: 0.8
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.68; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.88
Batch: 620; loss: 0.67; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.51; acc: 0.94
Batch: 680; loss: 0.81; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.73; acc: 0.8
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.00018311677558813244
0.00017455416673328727
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.55; acc: 0.86
Val Epoch over. val_loss: 0.6155985352719665; val_accuracy: 0.866640127388535 

The current subspace-distance is: 0.00017455416673328727 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.89; acc: 0.66
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.8; acc: 0.83
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.77; acc: 0.81
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.88
Batch: 440; loss: 0.72; acc: 0.77
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.76; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.91
Batch: 640; loss: 0.77; acc: 0.8
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.75; acc: 0.83
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.67; train_accuracy: 0.85 

0.00018510324298404157
0.00017841754015535116
Batch: 0; loss: 0.56; acc: 0.92
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.59; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.89
Val Epoch over. val_loss: 0.5955824320483359; val_accuracy: 0.8676353503184714 

The current subspace-distance is: 0.00017841754015535116 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.82; acc: 0.77
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.91
Batch: 180; loss: 0.71; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.92
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.76; acc: 0.83
Batch: 320; loss: 0.71; acc: 0.86
Batch: 340; loss: 0.76; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.92
Batch: 380; loss: 0.66; acc: 0.89
Batch: 400; loss: 0.74; acc: 0.8
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.48; acc: 0.94
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.78
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.67; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.66; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.92
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.7; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.66; train_accuracy: 0.85 

0.00018666773394215852
0.0001793447881937027
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.5921373149012304; val_accuracy: 0.8701234076433121 

The current subspace-distance is: 0.0001793447881937027 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.91
Batch: 200; loss: 0.68; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.62; acc: 0.88
Batch: 280; loss: 0.8; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.88
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.69; acc: 0.88
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.7; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.54; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.64; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.75
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.67; acc: 0.86
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.98; acc: 0.67
Train Epoch over. train_loss: 0.65; train_accuracy: 0.85 

0.00018909793288912624
0.00018219534831587225
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.97
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.5831029014602588; val_accuracy: 0.8698248407643312 

The current subspace-distance is: 0.00018219534831587225 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.89
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.8; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.92; acc: 0.78
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.87; acc: 0.7
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.67; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.7; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.0001909348793560639
0.00018601429474074394
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.88
Val Epoch over. val_loss: 0.5761166672418072; val_accuracy: 0.8729100318471338 

The current subspace-distance is: 0.00018601429474074394 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.65; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.91
Batch: 240; loss: 0.8; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.97; acc: 0.72
Batch: 300; loss: 0.66; acc: 0.89
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.62; acc: 0.89
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.95
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.88
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.91
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00019348511705175042
0.0001874542940640822
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.81; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.97
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.89
Val Epoch over. val_loss: 0.5720513254214245; val_accuracy: 0.8711186305732485 

The current subspace-distance is: 0.0001874542940640822 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.72; acc: 0.75
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.59; acc: 0.91
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.94
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.72; acc: 0.78
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00019646546570584178
0.0001889604754978791
Batch: 0; loss: 0.51; acc: 0.94
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.97
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.91
Val Epoch over. val_loss: 0.5534063744696842; val_accuracy: 0.8763933121019108 

The current subspace-distance is: 0.0001889604754978791 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.69; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.94
Batch: 220; loss: 0.72; acc: 0.77
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.73; acc: 0.75
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.74; acc: 0.77
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.81
Batch: 500; loss: 0.54; acc: 0.94
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.92
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.48; acc: 0.95
Train Epoch over. train_loss: 0.62; train_accuracy: 0.85 

0.0001994505146285519
0.0001935159816639498
Batch: 0; loss: 0.5; acc: 0.95
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.89
Val Epoch over. val_loss: 0.5499627523741145; val_accuracy: 0.8757961783439491 

The current subspace-distance is: 0.0001935159816639498 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.8; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.69; acc: 0.86
Batch: 380; loss: 0.72; acc: 0.78
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.74; acc: 0.77
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.53; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.63; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.91
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.88; acc: 0.78
Batch: 740; loss: 0.72; acc: 0.8
Batch: 760; loss: 0.73; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.62; train_accuracy: 0.85 

0.00019999399955850095
0.00019419607997406274
Batch: 0; loss: 0.49; acc: 0.95
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.5440213909954023; val_accuracy: 0.8805732484076433 

The current subspace-distance is: 0.00019419607997406274 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.91
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.91
Batch: 340; loss: 0.77; acc: 0.78
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.56; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.61; train_accuracy: 0.86 

0.0002047187153948471
0.0001976319181267172
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.89
Val Epoch over. val_loss: 0.543859467764569; val_accuracy: 0.8799761146496815 

The current subspace-distance is: 0.0001976319181267172 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.64; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.71; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.88
Batch: 540; loss: 0.79; acc: 0.73
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.94
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.74; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.0002039255778072402
0.00019691007037181407
Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.89
Val Epoch over. val_loss: 0.5421539550753915; val_accuracy: 0.8780851910828026 

The current subspace-distance is: 0.00019691007037181407 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.89
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.73; acc: 0.78
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.94
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.65; acc: 0.77
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.0002093309158226475
0.00020050036255270243
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.89
Val Epoch over. val_loss: 0.5342300386185859; val_accuracy: 0.8784832802547771 

The current subspace-distance is: 0.00020050036255270243 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.74; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.94
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.79; acc: 0.77
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.71; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.7; acc: 0.78
Batch: 460; loss: 0.45; acc: 0.95
Batch: 480; loss: 0.84; acc: 0.73
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.62; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.65; acc: 0.8
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.0002029706083703786
0.00019726192113012075
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.5318055838156658; val_accuracy: 0.879578025477707 

The current subspace-distance is: 0.00019726192113012075 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.45; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.95
Batch: 340; loss: 0.69; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.94
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.42; acc: 0.95
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.89
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.97
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.78; acc: 0.77
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.94
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00020548228349070996
0.00019749898638110608
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.5398883612672235; val_accuracy: 0.8785828025477707 

The current subspace-distance is: 0.00019749898638110608 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.87; acc: 0.73
Batch: 240; loss: 0.38; acc: 0.97
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.95
Batch: 440; loss: 0.58; acc: 0.92
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.71; acc: 0.77
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.65; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.91
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00020955436048097908
0.00020189433416817337
Batch: 0; loss: 0.46; acc: 0.95
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.5221506619149712; val_accuracy: 0.8835589171974523 

The current subspace-distance is: 0.00020189433416817337 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.73; acc: 0.83
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.84; acc: 0.75
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.49; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.94
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.83
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.77; acc: 0.84
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00020646859775297344
0.00019954376330133528
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.5324877314506822; val_accuracy: 0.8810708598726115 

The current subspace-distance is: 0.00019954376330133528 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.92
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.95
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.88
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.94
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00020851056615356356
0.00020249704539310187
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.5202673069990364; val_accuracy: 0.8817675159235668 

The current subspace-distance is: 0.00020249704539310187 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.79; acc: 0.77
Batch: 300; loss: 0.73; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.77
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.89
Batch: 420; loss: 0.66; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.68; acc: 0.75
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.95
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.95
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00021045180619694293
0.00020208273781463504
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.5284629638787288; val_accuracy: 0.8810708598726115 

The current subspace-distance is: 0.00020208273781463504 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.95
Batch: 280; loss: 0.54; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.7; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.7; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.95
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.58; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.94
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00021044320601504296
0.00020047981524839997
Batch: 0; loss: 0.45; acc: 0.95
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.88
Val Epoch over. val_loss: 0.521194648400993; val_accuracy: 0.8842555732484076 

The current subspace-distance is: 0.00020047981524839997 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.73; acc: 0.75
Batch: 200; loss: 0.47; acc: 0.94
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.92
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.52; acc: 0.92
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.68; acc: 0.78
Batch: 540; loss: 0.62; acc: 0.8
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.72; acc: 0.77
Batch: 720; loss: 0.6; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.84; acc: 0.77
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00021123890473973006
0.00020352195133455098
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.5141807544003626; val_accuracy: 0.8835589171974523 

The current subspace-distance is: 0.00020352195133455098 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.78
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.94
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.66; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.92
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.59; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.0002119525452144444
0.00020264438353478909
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.89
Val Epoch over. val_loss: 0.5200490473182338; val_accuracy: 0.883359872611465 

The current subspace-distance is: 0.00020264438353478909 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 7.203731671848972

The number of parameters is: 274066

The number of individual parameters is:

58
580
58
58
87
50460
87
87
173
150510
173
173
64
66432
64
64
4096
64
640
10
64
64

nonzero elements in E: 109626388
elements in E: 109626400
fraction nonzero: 0.9999998905373159
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.55; acc: 0.05
Batch: 20; loss: 2.17; acc: 0.12
Batch: 40; loss: 1.97; acc: 0.41
Batch: 60; loss: 1.94; acc: 0.39
Batch: 80; loss: 1.86; acc: 0.45
Batch: 100; loss: 1.73; acc: 0.5
Batch: 120; loss: 1.7; acc: 0.56
Batch: 140; loss: 1.64; acc: 0.64
Batch: 160; loss: 1.62; acc: 0.56
Batch: 180; loss: 1.65; acc: 0.55
Batch: 200; loss: 1.5; acc: 0.69
Batch: 220; loss: 1.52; acc: 0.64
Batch: 240; loss: 1.44; acc: 0.72
Batch: 260; loss: 1.52; acc: 0.62
Batch: 280; loss: 1.36; acc: 0.75
Batch: 300; loss: 1.56; acc: 0.56
Batch: 320; loss: 1.46; acc: 0.61
Batch: 340; loss: 1.57; acc: 0.53
Batch: 360; loss: 1.35; acc: 0.67
Batch: 380; loss: 1.31; acc: 0.73
Batch: 400; loss: 1.21; acc: 0.81
Batch: 420; loss: 1.32; acc: 0.72
Batch: 440; loss: 1.37; acc: 0.66
Batch: 460; loss: 1.24; acc: 0.78
Batch: 480; loss: 1.23; acc: 0.78
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.23; acc: 0.75
Batch: 540; loss: 1.17; acc: 0.8
Batch: 560; loss: 1.15; acc: 0.83
Batch: 580; loss: 1.19; acc: 0.88
Batch: 600; loss: 1.19; acc: 0.81
Batch: 620; loss: 1.19; acc: 0.77
Batch: 640; loss: 1.09; acc: 0.86
Batch: 660; loss: 1.13; acc: 0.73
Batch: 680; loss: 1.08; acc: 0.78
Batch: 700; loss: 1.07; acc: 0.88
Batch: 720; loss: 1.27; acc: 0.69
Batch: 740; loss: 1.02; acc: 0.84
Batch: 760; loss: 1.18; acc: 0.72
Batch: 780; loss: 0.95; acc: 0.83
Train Epoch over. train_loss: 1.43; train_accuracy: 0.66 

2.8186534109408967e-05
8.154745955835097e-06
Batch: 0; loss: 1.03; acc: 0.84
Batch: 20; loss: 1.29; acc: 0.69
Batch: 40; loss: 0.79; acc: 0.92
Batch: 60; loss: 0.96; acc: 0.84
Batch: 80; loss: 0.93; acc: 0.89
Batch: 100; loss: 1.03; acc: 0.86
Batch: 120; loss: 1.1; acc: 0.81
Batch: 140; loss: 0.91; acc: 0.86
Val Epoch over. val_loss: 1.0291894719858838; val_accuracy: 0.8306130573248408 

The current subspace-distance is: 8.154745955835097e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.83
Batch: 20; loss: 1.04; acc: 0.84
Batch: 40; loss: 1.08; acc: 0.81
Batch: 60; loss: 1.02; acc: 0.83
Batch: 80; loss: 1.06; acc: 0.81
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.8
Batch: 140; loss: 1.02; acc: 0.8
Batch: 160; loss: 0.92; acc: 0.86
Batch: 180; loss: 0.91; acc: 0.88
Batch: 200; loss: 0.96; acc: 0.86
Batch: 220; loss: 0.98; acc: 0.83
Batch: 240; loss: 0.98; acc: 0.78
Batch: 260; loss: 1.1; acc: 0.73
Batch: 280; loss: 0.89; acc: 0.89
Batch: 300; loss: 0.96; acc: 0.83
Batch: 320; loss: 1.03; acc: 0.77
Batch: 340; loss: 0.95; acc: 0.8
Batch: 360; loss: 0.88; acc: 0.83
Batch: 380; loss: 0.84; acc: 0.89
Batch: 400; loss: 0.86; acc: 0.89
Batch: 420; loss: 1.05; acc: 0.73
Batch: 440; loss: 0.99; acc: 0.78
Batch: 460; loss: 0.91; acc: 0.83
Batch: 480; loss: 0.85; acc: 0.86
Batch: 500; loss: 1.03; acc: 0.81
Batch: 520; loss: 0.95; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.88
Batch: 560; loss: 0.96; acc: 0.83
Batch: 580; loss: 0.78; acc: 0.88
Batch: 600; loss: 1.06; acc: 0.78
Batch: 620; loss: 0.83; acc: 0.88
Batch: 640; loss: 0.9; acc: 0.83
Batch: 660; loss: 0.94; acc: 0.77
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 1.07; acc: 0.72
Batch: 720; loss: 0.83; acc: 0.84
Batch: 740; loss: 0.97; acc: 0.83
Batch: 760; loss: 0.92; acc: 0.75
Batch: 780; loss: 0.84; acc: 0.86
Train Epoch over. train_loss: 0.95; train_accuracy: 0.83 

3.338811075082049e-05
1.1171588084835093e-05
Batch: 0; loss: 0.75; acc: 0.89
Batch: 20; loss: 1.07; acc: 0.7
Batch: 40; loss: 0.59; acc: 0.95
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.94
Batch: 100; loss: 0.77; acc: 0.89
Batch: 120; loss: 0.91; acc: 0.81
Batch: 140; loss: 0.64; acc: 0.91
Val Epoch over. val_loss: 0.7954300607845282; val_accuracy: 0.8614649681528662 

The current subspace-distance is: 1.1171588084835093e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.92
Batch: 60; loss: 0.81; acc: 0.91
Batch: 80; loss: 0.83; acc: 0.86
Batch: 100; loss: 0.87; acc: 0.8
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.86; acc: 0.78
Batch: 160; loss: 0.83; acc: 0.88
Batch: 180; loss: 0.88; acc: 0.81
Batch: 200; loss: 0.88; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.89
Batch: 240; loss: 0.83; acc: 0.81
Batch: 260; loss: 0.87; acc: 0.77
Batch: 280; loss: 0.68; acc: 0.91
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 1.0; acc: 0.73
Batch: 340; loss: 0.75; acc: 0.89
Batch: 360; loss: 0.78; acc: 0.84
Batch: 380; loss: 0.71; acc: 0.91
Batch: 400; loss: 0.8; acc: 0.86
Batch: 420; loss: 0.86; acc: 0.83
Batch: 440; loss: 0.7; acc: 0.89
Batch: 460; loss: 0.85; acc: 0.8
Batch: 480; loss: 0.88; acc: 0.78
Batch: 500; loss: 0.66; acc: 0.92
Batch: 520; loss: 0.87; acc: 0.81
Batch: 540; loss: 0.9; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 0.76; acc: 0.88
Batch: 600; loss: 0.81; acc: 0.86
Batch: 620; loss: 0.76; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.82; acc: 0.84
Batch: 680; loss: 0.76; acc: 0.88
Batch: 700; loss: 0.7; acc: 0.84
Batch: 720; loss: 0.77; acc: 0.83
Batch: 740; loss: 0.66; acc: 0.89
Batch: 760; loss: 0.73; acc: 0.89
Batch: 780; loss: 0.79; acc: 0.84
Train Epoch over. train_loss: 0.79; train_accuracy: 0.85 

3.770849798456766e-05
1.3273315744299907e-05
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.46; acc: 0.97
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6684358417987823; val_accuracy: 0.880672770700637 

The current subspace-distance is: 1.3273315744299907e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.89
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.76; acc: 0.84
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.74; acc: 0.86
Batch: 200; loss: 0.79; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.91
Batch: 240; loss: 0.83; acc: 0.77
Batch: 260; loss: 0.73; acc: 0.91
Batch: 280; loss: 0.78; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.83
Batch: 320; loss: 0.65; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.91
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 0.6; acc: 0.92
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.92
Batch: 480; loss: 0.63; acc: 0.92
Batch: 500; loss: 0.63; acc: 0.88
Batch: 520; loss: 0.69; acc: 0.88
Batch: 540; loss: 0.67; acc: 0.88
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.71; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.88
Batch: 640; loss: 0.69; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.89
Batch: 720; loss: 0.79; acc: 0.81
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.86 

4.1325994970975444e-05
1.652731589274481e-05
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.98
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.95
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.6015723343867405; val_accuracy: 0.8924164012738853 

The current subspace-distance is: 1.652731589274481e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.7; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.92
Batch: 180; loss: 0.8; acc: 0.77
Batch: 200; loss: 0.84; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.94
Batch: 280; loss: 0.55; acc: 0.92
Batch: 300; loss: 0.6; acc: 0.91
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.66; acc: 0.89
Batch: 360; loss: 0.68; acc: 0.86
Batch: 380; loss: 0.62; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.94
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.6; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.95
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.63; acc: 0.91
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.94
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.91
Batch: 700; loss: 0.68; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.95
Batch: 740; loss: 0.69; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.88 

4.521167284110561e-05
1.8426255337544717e-05
Batch: 0; loss: 0.45; acc: 0.95
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.95
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.91
Val Epoch over. val_loss: 0.5346746962920875; val_accuracy: 0.9009753184713376 

The current subspace-distance is: 1.8426255337544717e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.92
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.6; acc: 0.88
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.95
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.68; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.95
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.92
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.46; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.56; train_accuracy: 0.89 

4.742355667985976e-05
1.9222497940063477e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.91
Val Epoch over. val_loss: 0.4869432692315168; val_accuracy: 0.9036624203821656 

The current subspace-distance is: 1.9222497940063477e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.95
Batch: 320; loss: 0.57; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

4.982573591405526e-05
2.058247810055036e-05
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.98
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.4541987747333612; val_accuracy: 0.9076433121019108 

The current subspace-distance is: 2.058247810055036e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.97
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.92
Batch: 260; loss: 0.74; acc: 0.8
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.95
Batch: 520; loss: 0.49; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.7; acc: 0.78
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.97
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

5.227737710811198e-05
2.137786395906005e-05
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.97
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.43198396369909786; val_accuracy: 0.9079418789808917 

The current subspace-distance is: 2.137786395906005e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.62; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.94
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.95
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.92
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

5.379881986300461e-05
2.1969226509099826e-05
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.23; acc: 1.0
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4135799997361602; val_accuracy: 0.9106289808917197 

The current subspace-distance is: 2.1969226509099826e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.94
Batch: 160; loss: 0.59; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.44; acc: 0.95
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.97
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

5.61836532142479e-05
2.380767182330601e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.40675444122712323; val_accuracy: 0.9096337579617835 

The current subspace-distance is: 2.380767182330601e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.94
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.69641160836909e-05
2.413927359157242e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.39100211934678875; val_accuracy: 0.9126194267515924 

The current subspace-distance is: 2.413927359157242e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.72; acc: 0.81
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.97
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.97
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.98
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.764378875028342e-05
2.4181568733183667e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3861647396330621; val_accuracy: 0.9128184713375797 

The current subspace-distance is: 2.4181568733183667e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.97
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.815178155899048e-05
2.4901888536987826e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.2; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3838703951258568; val_accuracy: 0.9138136942675159 

The current subspace-distance is: 2.4901888536987826e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.95
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.7882392866304144e-05
2.473328095220495e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.38211575824363975; val_accuracy: 0.9128184713375797 

The current subspace-distance is: 2.473328095220495e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.97
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.98
Batch: 460; loss: 0.37; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.8994704886572435e-05
2.5773786546778865e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.38539700959898104; val_accuracy: 0.9122213375796179 

The current subspace-distance is: 2.5773786546778865e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.965298987575807e-05
2.5267361706937663e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3770977702869731; val_accuracy: 0.9136146496815286 

The current subspace-distance is: 2.5267361706937663e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.98
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.34; acc: 0.97
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.98
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.9804875490954146e-05
2.5743704100023024e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3687983487442041; val_accuracy: 0.9159036624203821 

The current subspace-distance is: 2.5743704100023024e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.97
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.98
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.97
Batch: 720; loss: 0.52; acc: 0.92
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.039821892045438e-05
2.6234583856421523e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.37122265073903804; val_accuracy: 0.9144108280254777 

The current subspace-distance is: 2.6234583856421523e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.78
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.055373160052113e-05
2.5298948457930237e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3690770003636172; val_accuracy: 0.9133160828025477 

The current subspace-distance is: 2.5298948457930237e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.0300018958514556e-05
2.581355147412978e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.3641991197683249; val_accuracy: 0.9147093949044586 

The current subspace-distance is: 2.581355147412978e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.47; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.95
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.25; acc: 1.0
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.6; acc: 0.8
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.121149635873735e-05
2.5961247956729494e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.36075650658577113; val_accuracy: 0.915406050955414 

The current subspace-distance is: 2.5961247956729494e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.98
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.95
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.8
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.193455192260444e-05
2.5833453037193976e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3535510853027842; val_accuracy: 0.9174960191082803 

The current subspace-distance is: 2.5833453037193976e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.174355075927451e-05
2.5512315914966166e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.3574790113670811; val_accuracy: 0.916202229299363 

The current subspace-distance is: 2.5512315914966166e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.98
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.95
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.134797149570659e-05
2.655373464222066e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3590439079673427; val_accuracy: 0.9142117834394905 

The current subspace-distance is: 2.655373464222066e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.54; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.158100586617365e-05
2.7673671866068617e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.35628459408025076; val_accuracy: 0.9159036624203821 

The current subspace-distance is: 2.7673671866068617e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.160455814097077e-05
2.6418825655127876e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.35463894182329725; val_accuracy: 0.9161027070063694 

The current subspace-distance is: 2.6418825655127876e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.98
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.218311318662018e-05
2.647903238539584e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3527127544211734; val_accuracy: 0.9176950636942676 

The current subspace-distance is: 2.647903238539584e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.31; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.98
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.231830775504932e-05
2.6397870897199027e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3510025534660194; val_accuracy: 0.9171974522292994 

The current subspace-distance is: 2.6397870897199027e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.97
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.98
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.2073246226646e-05
2.596307786006946e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.34932147113570744; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.596307786006946e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.98
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.248366844374686e-05
2.7056201361119747e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3578761539831283; val_accuracy: 0.9156050955414012 

The current subspace-distance is: 2.7056201361119747e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 7.203731671848972

The number of parameters is: 274066

The number of individual parameters is:

58
580
58
58
87
50460
87
87
173
150510
173
173
64
66432
64
64
4096
64
640
10
64
64

nonzero elements in E: 137032987
elements in E: 137033000
fraction nonzero: 0.9999999051323404
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.48; acc: 0.09
Batch: 20; loss: 2.19; acc: 0.12
Batch: 40; loss: 1.93; acc: 0.38
Batch: 60; loss: 1.87; acc: 0.42
Batch: 80; loss: 1.76; acc: 0.56
Batch: 100; loss: 1.63; acc: 0.62
Batch: 120; loss: 1.55; acc: 0.67
Batch: 140; loss: 1.6; acc: 0.62
Batch: 160; loss: 1.41; acc: 0.77
Batch: 180; loss: 1.36; acc: 0.83
Batch: 200; loss: 1.47; acc: 0.66
Batch: 220; loss: 1.41; acc: 0.67
Batch: 240; loss: 1.22; acc: 0.81
Batch: 260; loss: 1.2; acc: 0.83
Batch: 280; loss: 1.31; acc: 0.72
Batch: 300; loss: 1.24; acc: 0.73
Batch: 320; loss: 1.22; acc: 0.77
Batch: 340; loss: 1.19; acc: 0.8
Batch: 360; loss: 1.19; acc: 0.78
Batch: 380; loss: 1.15; acc: 0.72
Batch: 400; loss: 1.18; acc: 0.77
Batch: 420; loss: 1.19; acc: 0.78
Batch: 440; loss: 1.19; acc: 0.8
Batch: 460; loss: 1.18; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.78
Batch: 500; loss: 1.18; acc: 0.77
Batch: 520; loss: 1.13; acc: 0.78
Batch: 540; loss: 0.96; acc: 0.86
Batch: 560; loss: 1.0; acc: 0.84
Batch: 580; loss: 1.0; acc: 0.81
Batch: 600; loss: 1.09; acc: 0.78
Batch: 620; loss: 0.94; acc: 0.86
Batch: 640; loss: 1.09; acc: 0.78
Batch: 660; loss: 0.93; acc: 0.88
Batch: 680; loss: 0.95; acc: 0.88
Batch: 700; loss: 1.07; acc: 0.81
Batch: 720; loss: 0.99; acc: 0.75
Batch: 740; loss: 0.96; acc: 0.78
Batch: 760; loss: 0.89; acc: 0.94
Batch: 780; loss: 0.84; acc: 0.89
Train Epoch over. train_loss: 1.28; train_accuracy: 0.72 

2.9278664442244917e-05
9.398587280884385e-06
Batch: 0; loss: 0.88; acc: 0.88
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 0.67; acc: 0.92
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.81; acc: 0.88
Batch: 100; loss: 0.9; acc: 0.88
Batch: 120; loss: 1.1; acc: 0.72
Batch: 140; loss: 0.88; acc: 0.83
Val Epoch over. val_loss: 0.898315502200157; val_accuracy: 0.8457404458598726 

The current subspace-distance is: 9.398587280884385e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.83
Batch: 20; loss: 0.97; acc: 0.8
Batch: 40; loss: 0.94; acc: 0.84
Batch: 60; loss: 0.92; acc: 0.84
Batch: 80; loss: 0.87; acc: 0.84
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 0.85; acc: 0.86
Batch: 140; loss: 0.93; acc: 0.81
Batch: 160; loss: 0.78; acc: 0.89
Batch: 180; loss: 0.84; acc: 0.83
Batch: 200; loss: 0.89; acc: 0.88
Batch: 220; loss: 0.79; acc: 0.86
Batch: 240; loss: 0.92; acc: 0.8
Batch: 260; loss: 0.77; acc: 0.88
Batch: 280; loss: 0.8; acc: 0.84
Batch: 300; loss: 0.81; acc: 0.91
Batch: 320; loss: 0.9; acc: 0.86
Batch: 340; loss: 0.93; acc: 0.75
Batch: 360; loss: 0.8; acc: 0.84
Batch: 380; loss: 1.13; acc: 0.64
Batch: 400; loss: 0.76; acc: 0.84
Batch: 420; loss: 0.91; acc: 0.77
Batch: 440; loss: 0.9; acc: 0.83
Batch: 460; loss: 0.89; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.97
Batch: 500; loss: 0.83; acc: 0.81
Batch: 520; loss: 0.75; acc: 0.88
Batch: 540; loss: 0.83; acc: 0.8
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.86; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.88
Batch: 640; loss: 0.77; acc: 0.88
Batch: 660; loss: 0.73; acc: 0.86
Batch: 680; loss: 0.64; acc: 0.92
Batch: 700; loss: 0.65; acc: 0.92
Batch: 720; loss: 0.73; acc: 0.88
Batch: 740; loss: 0.6; acc: 0.95
Batch: 760; loss: 0.68; acc: 0.89
Batch: 780; loss: 0.77; acc: 0.83
Train Epoch over. train_loss: 0.83; train_accuracy: 0.85 

3.4307602618355304e-05
1.2147915185778402e-05
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.92
Batch: 100; loss: 0.7; acc: 0.94
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.71; acc: 0.88
Val Epoch over. val_loss: 0.7211852142005969; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 1.2147915185778402e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.95
Batch: 20; loss: 0.81; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.89
Batch: 80; loss: 0.72; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.8; acc: 0.8
Batch: 160; loss: 0.65; acc: 0.92
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.78; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.73; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.88
Batch: 280; loss: 0.79; acc: 0.81
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.68; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.95
Batch: 420; loss: 0.87; acc: 0.78
Batch: 440; loss: 0.75; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.94
Batch: 480; loss: 0.62; acc: 0.92
Batch: 500; loss: 0.8; acc: 0.78
Batch: 520; loss: 0.62; acc: 0.92
Batch: 540; loss: 0.61; acc: 0.92
Batch: 560; loss: 0.53; acc: 0.92
Batch: 580; loss: 0.65; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.86
Batch: 620; loss: 0.7; acc: 0.89
Batch: 640; loss: 0.82; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.78; acc: 0.84
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.89
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.94
Train Epoch over. train_loss: 0.69; train_accuracy: 0.86 

3.776010271394625e-05
1.4543546058121137e-05
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.91
Val Epoch over. val_loss: 0.6100499485708346; val_accuracy: 0.882265127388535 

The current subspace-distance is: 1.4543546058121137e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.95
Batch: 100; loss: 0.71; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.97
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.62; acc: 0.88
Batch: 280; loss: 0.77; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.94
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.94
Batch: 440; loss: 0.53; acc: 0.94
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.62; acc: 0.92
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.75; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.61; train_accuracy: 0.87 

4.18274212279357e-05
1.6078909538919106e-05
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.5488736078997326; val_accuracy: 0.8867436305732485 

The current subspace-distance is: 1.6078909538919106e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.94
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.95
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.95
Batch: 480; loss: 0.49; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.95
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

4.495518442126922e-05
1.92308198165847e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.47108189904006426; val_accuracy: 0.9015724522292994 

The current subspace-distance is: 1.92308198165847e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.95
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.35; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

4.8115420213434845e-05
2.077511999232229e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.4198411014998794; val_accuracy: 0.9103304140127388 

The current subspace-distance is: 2.077511999232229e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.98
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.95
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.009278902434744e-05
2.1675057723768987e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3761460094884702; val_accuracy: 0.9157046178343949 

The current subspace-distance is: 2.1675057723768987e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.273083661450073e-05
2.317793041584082e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3677401161117918; val_accuracy: 0.9179936305732485 

The current subspace-distance is: 2.317793041584082e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.3509353165281937e-05
2.2956895918468945e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3412331401542493; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.2956895918468945e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.598489588010125e-05
2.446950202283915e-05
Batch: 0; loss: 0.24; acc: 1.0
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3299005045822472; val_accuracy: 0.9248606687898089 

The current subspace-distance is: 2.446950202283915e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.81
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.95
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.619295916403644e-05
2.3549619072582573e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3250769152288224; val_accuracy: 0.9263535031847133 

The current subspace-distance is: 2.3549619072582573e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.43; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.98
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.95
Batch: 660; loss: 0.6; acc: 0.81
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.98
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.718525790143758e-05
2.5665205612313002e-05
Batch: 0; loss: 0.24; acc: 1.0
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3238268503148085; val_accuracy: 0.925656847133758 

The current subspace-distance is: 2.5665205612313002e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.98
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.6; acc: 0.78
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.58; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.6876579037634656e-05
2.4170614778995514e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3241352808608371; val_accuracy: 0.9249601910828026 

The current subspace-distance is: 2.4170614778995514e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.97
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.765658715972677e-05
2.5937712052837014e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3171203500905614; val_accuracy: 0.9273487261146497 

The current subspace-distance is: 2.5937712052837014e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.915297879255377e-05
2.7110214432468638e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.321849293674633; val_accuracy: 0.9260549363057324 

The current subspace-distance is: 2.7110214432468638e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.97
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.8301957324147224e-05
2.4924922399804927e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3056571585167745; val_accuracy: 0.9279458598726115 

The current subspace-distance is: 2.4924922399804927e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.8092067774850875e-05
2.4224278604378924e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3050422475311407; val_accuracy: 0.9280453821656051 

The current subspace-distance is: 2.4224278604378924e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.97
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

5.918241004110314e-05
2.540865352784749e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.30762016521707464; val_accuracy: 0.9258558917197452 

The current subspace-distance is: 2.540865352784749e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.012560334056616e-05
2.6678126232582144e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.3033955044996966; val_accuracy: 0.9288415605095541 

The current subspace-distance is: 2.6678126232582144e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.98018777964171e-05
2.595141268102452e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.3001406875195777; val_accuracy: 0.9276472929936306 

The current subspace-distance is: 2.595141268102452e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.59; acc: 0.78
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.115408177720383e-05
2.664974090293981e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.30084700948873144; val_accuracy: 0.9282444267515924 

The current subspace-distance is: 2.664974090293981e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.98
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.98927799728699e-05
2.5538651243550703e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2966792707800106; val_accuracy: 0.9307324840764332 

The current subspace-distance is: 2.5538651243550703e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.032465535099618e-05
2.5745368475327268e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.29239467256198265; val_accuracy: 0.9306329617834395 

The current subspace-distance is: 2.5745368475327268e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.98
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.0249098169151694e-05
2.686682819330599e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2969611838080321; val_accuracy: 0.9301353503184714 

The current subspace-distance is: 2.686682819330599e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.078936712583527e-05
2.652619150467217e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.29800706176431313; val_accuracy: 0.9297372611464968 

The current subspace-distance is: 2.652619150467217e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.98
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

6.063267574063502e-05
2.75648581009591e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2992453279958409; val_accuracy: 0.9284434713375797 

The current subspace-distance is: 2.75648581009591e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.22; acc: 1.0
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.98
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.98
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.0917540395166725e-05
2.6529272872721776e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.29731647010631623; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.6529272872721776e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.1028196796542034e-05
2.5916942831827328e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2893416594452919; val_accuracy: 0.9319267515923567 

The current subspace-distance is: 2.5916942831827328e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.107577792135999e-05
2.5288001779699698e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2935748183803194; val_accuracy: 0.9292396496815286 

The current subspace-distance is: 2.5288001779699698e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.21; acc: 1.0
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.98
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.148691318230703e-05
2.709220461838413e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2927663853498781; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.709220461838413e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:47/N_2_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
