model : table13slim
N : 13
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 1.6833983485794441

The number of parameters is: 259251

The number of individual parameters is:

14
252
14
14
21
38514
21
21
41
112791
41
41
64
102336
64
64
4096
64
640
10
64
64

nonzero elements in E: 12962548
elements in E: 12962550
fraction nonzero: 0.9999998457093705
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.19; acc: 0.16
Batch: 40; loss: 2.15; acc: 0.19
Batch: 60; loss: 1.98; acc: 0.33
Batch: 80; loss: 2.06; acc: 0.34
Batch: 100; loss: 2.07; acc: 0.23
Batch: 120; loss: 2.11; acc: 0.28
Batch: 140; loss: 1.96; acc: 0.25
Batch: 160; loss: 2.06; acc: 0.31
Batch: 180; loss: 1.93; acc: 0.38
Batch: 200; loss: 2.08; acc: 0.28
Batch: 220; loss: 1.96; acc: 0.36
Batch: 240; loss: 1.84; acc: 0.47
Batch: 260; loss: 2.09; acc: 0.3
Batch: 280; loss: 1.99; acc: 0.31
Batch: 300; loss: 1.9; acc: 0.42
Batch: 320; loss: 1.86; acc: 0.42
Batch: 340; loss: 1.9; acc: 0.42
Batch: 360; loss: 2.0; acc: 0.31
Batch: 380; loss: 2.01; acc: 0.33
Batch: 400; loss: 1.94; acc: 0.41
Batch: 420; loss: 1.87; acc: 0.44
Batch: 440; loss: 1.94; acc: 0.3
Batch: 460; loss: 1.87; acc: 0.42
Batch: 480; loss: 1.89; acc: 0.34
Batch: 500; loss: 1.79; acc: 0.5
Batch: 520; loss: 1.87; acc: 0.41
Batch: 540; loss: 1.87; acc: 0.44
Batch: 560; loss: 1.88; acc: 0.44
Batch: 580; loss: 1.86; acc: 0.39
Batch: 600; loss: 1.93; acc: 0.36
Batch: 620; loss: 1.81; acc: 0.5
Batch: 640; loss: 1.81; acc: 0.47
Batch: 660; loss: 1.87; acc: 0.39
Batch: 680; loss: 1.76; acc: 0.47
Batch: 700; loss: 1.83; acc: 0.52
Batch: 720; loss: 1.9; acc: 0.41
Batch: 740; loss: 1.84; acc: 0.41
Batch: 760; loss: 1.78; acc: 0.53
Batch: 780; loss: 1.81; acc: 0.38
Train Epoch over. train_loss: 1.93; train_accuracy: 0.36 

2.1356388970161788e-05
4.051586984132882e-06
Batch: 0; loss: 1.82; acc: 0.38
Batch: 20; loss: 1.88; acc: 0.42
Batch: 40; loss: 1.6; acc: 0.61
Batch: 60; loss: 1.74; acc: 0.45
Batch: 80; loss: 1.74; acc: 0.45
Batch: 100; loss: 1.86; acc: 0.42
Batch: 120; loss: 1.86; acc: 0.5
Batch: 140; loss: 1.77; acc: 0.55
Val Epoch over. val_loss: 1.796913929805634; val_accuracy: 0.45710589171974525 

The current subspace-distance is: 4.051586984132882e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.5
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.79; acc: 0.48
Batch: 60; loss: 1.88; acc: 0.38
Batch: 80; loss: 1.87; acc: 0.48
Batch: 100; loss: 1.8; acc: 0.45
Batch: 120; loss: 1.8; acc: 0.45
Batch: 140; loss: 1.88; acc: 0.44
Batch: 160; loss: 1.91; acc: 0.39
Batch: 180; loss: 1.88; acc: 0.34
Batch: 200; loss: 1.73; acc: 0.52
Batch: 220; loss: 1.72; acc: 0.48
Batch: 240; loss: 1.76; acc: 0.55
Batch: 260; loss: 2.0; acc: 0.34
Batch: 280; loss: 1.78; acc: 0.52
Batch: 300; loss: 1.88; acc: 0.41
Batch: 320; loss: 1.79; acc: 0.36
Batch: 340; loss: 1.81; acc: 0.47
Batch: 360; loss: 1.7; acc: 0.52
Batch: 380; loss: 1.87; acc: 0.45
Batch: 400; loss: 1.72; acc: 0.53
Batch: 420; loss: 1.84; acc: 0.41
Batch: 440; loss: 1.85; acc: 0.41
Batch: 460; loss: 1.74; acc: 0.53
Batch: 480; loss: 1.7; acc: 0.48
Batch: 500; loss: 1.76; acc: 0.47
Batch: 520; loss: 1.78; acc: 0.42
Batch: 540; loss: 1.79; acc: 0.45
Batch: 560; loss: 1.65; acc: 0.58
Batch: 580; loss: 1.74; acc: 0.53
Batch: 600; loss: 1.68; acc: 0.66
Batch: 620; loss: 1.81; acc: 0.5
Batch: 640; loss: 1.83; acc: 0.39
Batch: 660; loss: 1.68; acc: 0.55
Batch: 680; loss: 1.68; acc: 0.5
Batch: 700; loss: 1.63; acc: 0.58
Batch: 720; loss: 1.81; acc: 0.41
Batch: 740; loss: 1.76; acc: 0.56
Batch: 760; loss: 1.8; acc: 0.48
Batch: 780; loss: 1.93; acc: 0.38
Train Epoch over. train_loss: 1.78; train_accuracy: 0.47 

2.3973934730747715e-05
4.959371835866477e-06
Batch: 0; loss: 1.82; acc: 0.41
Batch: 20; loss: 1.82; acc: 0.41
Batch: 40; loss: 1.49; acc: 0.64
Batch: 60; loss: 1.71; acc: 0.5
Batch: 80; loss: 1.72; acc: 0.5
Batch: 100; loss: 1.83; acc: 0.45
Batch: 120; loss: 1.79; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.59
Val Epoch over. val_loss: 1.752452583070014; val_accuracy: 0.491640127388535 

The current subspace-distance is: 4.959371835866477e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.42
Batch: 20; loss: 1.72; acc: 0.52
Batch: 40; loss: 1.78; acc: 0.47
Batch: 60; loss: 1.83; acc: 0.5
Batch: 80; loss: 1.69; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.45
Batch: 120; loss: 1.78; acc: 0.47
Batch: 140; loss: 1.84; acc: 0.44
Batch: 160; loss: 1.66; acc: 0.5
Batch: 180; loss: 1.75; acc: 0.52
Batch: 200; loss: 1.76; acc: 0.5
Batch: 220; loss: 1.71; acc: 0.53
Batch: 240; loss: 1.75; acc: 0.52
Batch: 260; loss: 1.76; acc: 0.45
Batch: 280; loss: 1.77; acc: 0.48
Batch: 300; loss: 1.82; acc: 0.42
Batch: 320; loss: 1.78; acc: 0.45
Batch: 340; loss: 1.79; acc: 0.45
Batch: 360; loss: 1.75; acc: 0.47
Batch: 380; loss: 1.73; acc: 0.53
Batch: 400; loss: 1.73; acc: 0.47
Batch: 420; loss: 1.88; acc: 0.47
Batch: 440; loss: 1.79; acc: 0.41
Batch: 460; loss: 1.77; acc: 0.5
Batch: 480; loss: 1.6; acc: 0.55
Batch: 500; loss: 1.68; acc: 0.56
Batch: 520; loss: 1.75; acc: 0.52
Batch: 540; loss: 1.74; acc: 0.53
Batch: 560; loss: 1.71; acc: 0.53
Batch: 580; loss: 1.76; acc: 0.5
Batch: 600; loss: 1.8; acc: 0.52
Batch: 620; loss: 1.69; acc: 0.52
Batch: 640; loss: 1.67; acc: 0.56
Batch: 660; loss: 1.73; acc: 0.48
Batch: 680; loss: 1.73; acc: 0.52
Batch: 700; loss: 1.78; acc: 0.45
Batch: 720; loss: 1.67; acc: 0.59
Batch: 740; loss: 1.81; acc: 0.44
Batch: 760; loss: 1.73; acc: 0.42
Batch: 780; loss: 1.76; acc: 0.45
Train Epoch over. train_loss: 1.74; train_accuracy: 0.49 

2.6311257897759788e-05
5.952562332822708e-06
Batch: 0; loss: 1.8; acc: 0.41
Batch: 20; loss: 1.8; acc: 0.42
Batch: 40; loss: 1.45; acc: 0.69
Batch: 60; loss: 1.68; acc: 0.53
Batch: 80; loss: 1.73; acc: 0.44
Batch: 100; loss: 1.78; acc: 0.44
Batch: 120; loss: 1.74; acc: 0.5
Batch: 140; loss: 1.65; acc: 0.55
Val Epoch over. val_loss: 1.7309467944369954; val_accuracy: 0.49980095541401276 

The current subspace-distance is: 5.952562332822708e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.45
Batch: 20; loss: 1.73; acc: 0.48
Batch: 40; loss: 1.66; acc: 0.55
Batch: 60; loss: 1.72; acc: 0.5
Batch: 80; loss: 1.78; acc: 0.47
Batch: 100; loss: 1.69; acc: 0.56
Batch: 120; loss: 1.73; acc: 0.52
Batch: 140; loss: 1.71; acc: 0.45
Batch: 160; loss: 1.64; acc: 0.5
Batch: 180; loss: 1.82; acc: 0.39
Batch: 200; loss: 1.84; acc: 0.5
Batch: 220; loss: 1.83; acc: 0.39
Batch: 240; loss: 1.75; acc: 0.52
Batch: 260; loss: 1.78; acc: 0.44
Batch: 280; loss: 1.76; acc: 0.45
Batch: 300; loss: 1.76; acc: 0.47
Batch: 320; loss: 1.7; acc: 0.55
Batch: 340; loss: 1.6; acc: 0.61
Batch: 360; loss: 1.78; acc: 0.53
Batch: 380; loss: 1.71; acc: 0.47
Batch: 400; loss: 1.74; acc: 0.44
Batch: 420; loss: 1.82; acc: 0.48
Batch: 440; loss: 1.79; acc: 0.47
Batch: 460; loss: 1.77; acc: 0.47
Batch: 480; loss: 1.59; acc: 0.53
Batch: 500; loss: 1.7; acc: 0.5
Batch: 520; loss: 1.79; acc: 0.42
Batch: 540; loss: 1.79; acc: 0.44
Batch: 560; loss: 1.79; acc: 0.45
Batch: 580; loss: 1.72; acc: 0.52
Batch: 600; loss: 1.84; acc: 0.42
Batch: 620; loss: 1.65; acc: 0.56
Batch: 640; loss: 1.63; acc: 0.59
Batch: 660; loss: 1.68; acc: 0.5
Batch: 680; loss: 1.73; acc: 0.47
Batch: 700; loss: 1.74; acc: 0.45
Batch: 720; loss: 1.71; acc: 0.47
Batch: 740; loss: 1.74; acc: 0.5
Batch: 760; loss: 1.76; acc: 0.45
Batch: 780; loss: 1.71; acc: 0.5
Train Epoch over. train_loss: 1.72; train_accuracy: 0.5 

2.7332043828209862e-05
8.676388461026363e-06
Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.75; acc: 0.47
Batch: 40; loss: 1.41; acc: 0.67
Batch: 60; loss: 1.67; acc: 0.52
Batch: 80; loss: 1.73; acc: 0.38
Batch: 100; loss: 1.77; acc: 0.52
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.52
Val Epoch over. val_loss: 1.6973588747583377; val_accuracy: 0.5154259554140127 

The current subspace-distance is: 8.676388461026363e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.55
Batch: 20; loss: 1.57; acc: 0.59
Batch: 40; loss: 1.7; acc: 0.5
Batch: 60; loss: 1.79; acc: 0.42
Batch: 80; loss: 1.76; acc: 0.44
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.67; acc: 0.53
Batch: 160; loss: 1.61; acc: 0.59
Batch: 180; loss: 1.55; acc: 0.59
Batch: 200; loss: 1.77; acc: 0.5
Batch: 220; loss: 1.79; acc: 0.47
Batch: 240; loss: 1.7; acc: 0.52
Batch: 260; loss: 1.8; acc: 0.52
Batch: 280; loss: 1.77; acc: 0.45
Batch: 300; loss: 1.64; acc: 0.48
Batch: 320; loss: 1.63; acc: 0.55
Batch: 340; loss: 1.68; acc: 0.53
Batch: 360; loss: 1.84; acc: 0.39
Batch: 380; loss: 1.79; acc: 0.45
Batch: 400; loss: 1.7; acc: 0.56
Batch: 420; loss: 1.74; acc: 0.5
Batch: 440; loss: 1.76; acc: 0.47
Batch: 460; loss: 1.78; acc: 0.44
Batch: 480; loss: 1.62; acc: 0.62
Batch: 500; loss: 1.69; acc: 0.53
Batch: 520; loss: 1.75; acc: 0.47
Batch: 540; loss: 1.61; acc: 0.59
Batch: 560; loss: 1.62; acc: 0.56
Batch: 580; loss: 1.63; acc: 0.56
Batch: 600; loss: 1.71; acc: 0.55
Batch: 620; loss: 1.64; acc: 0.53
Batch: 640; loss: 1.73; acc: 0.52
Batch: 660; loss: 1.67; acc: 0.55
Batch: 680; loss: 1.7; acc: 0.53
Batch: 700; loss: 1.59; acc: 0.59
Batch: 720; loss: 1.73; acc: 0.42
Batch: 740; loss: 1.66; acc: 0.52
Batch: 760; loss: 1.57; acc: 0.58
Batch: 780; loss: 1.65; acc: 0.48
Train Epoch over. train_loss: 1.69; train_accuracy: 0.51 

2.8037253287038766e-05
7.848057975934353e-06
Batch: 0; loss: 1.77; acc: 0.38
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.42; acc: 0.72
Batch: 60; loss: 1.67; acc: 0.52
Batch: 80; loss: 1.72; acc: 0.44
Batch: 100; loss: 1.77; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.65; acc: 0.53
Val Epoch over. val_loss: 1.6866050389162295; val_accuracy: 0.5240843949044586 

The current subspace-distance is: 7.848057975934353e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.59
Batch: 20; loss: 1.58; acc: 0.62
Batch: 40; loss: 1.73; acc: 0.53
Batch: 60; loss: 1.57; acc: 0.61
Batch: 80; loss: 1.62; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.62
Batch: 120; loss: 1.79; acc: 0.48
Batch: 140; loss: 1.68; acc: 0.52
Batch: 160; loss: 1.85; acc: 0.41
Batch: 180; loss: 1.72; acc: 0.48
Batch: 200; loss: 1.7; acc: 0.52
Batch: 220; loss: 1.72; acc: 0.52
Batch: 240; loss: 1.65; acc: 0.53
Batch: 260; loss: 1.69; acc: 0.59
Batch: 280; loss: 1.73; acc: 0.47
Batch: 300; loss: 1.78; acc: 0.44
Batch: 320; loss: 1.69; acc: 0.44
Batch: 340; loss: 1.64; acc: 0.61
Batch: 360; loss: 1.62; acc: 0.56
Batch: 380; loss: 1.66; acc: 0.5
Batch: 400; loss: 1.68; acc: 0.52
Batch: 420; loss: 1.63; acc: 0.48
Batch: 440; loss: 1.63; acc: 0.52
Batch: 460; loss: 1.64; acc: 0.52
Batch: 480; loss: 1.64; acc: 0.58
Batch: 500; loss: 1.62; acc: 0.58
Batch: 520; loss: 1.71; acc: 0.48
Batch: 540; loss: 1.74; acc: 0.48
Batch: 560; loss: 1.64; acc: 0.5
Batch: 580; loss: 1.63; acc: 0.52
Batch: 600; loss: 1.63; acc: 0.58
Batch: 620; loss: 1.6; acc: 0.62
Batch: 640; loss: 1.66; acc: 0.55
Batch: 660; loss: 1.61; acc: 0.53
Batch: 680; loss: 1.79; acc: 0.39
Batch: 700; loss: 1.7; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.48
Batch: 740; loss: 1.67; acc: 0.47
Batch: 760; loss: 1.64; acc: 0.58
Batch: 780; loss: 1.55; acc: 0.66
Train Epoch over. train_loss: 1.67; train_accuracy: 0.53 

2.998947093146853e-05
8.685664397489745e-06
Batch: 0; loss: 1.76; acc: 0.36
Batch: 20; loss: 1.68; acc: 0.47
Batch: 40; loss: 1.42; acc: 0.75
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.7; acc: 0.47
Batch: 100; loss: 1.75; acc: 0.53
Batch: 120; loss: 1.7; acc: 0.41
Batch: 140; loss: 1.65; acc: 0.52
Val Epoch over. val_loss: 1.6678455999702404; val_accuracy: 0.5328423566878981 

The current subspace-distance is: 8.685664397489745e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.59
Batch: 20; loss: 1.73; acc: 0.38
Batch: 40; loss: 1.78; acc: 0.45
Batch: 60; loss: 1.65; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.55
Batch: 100; loss: 1.66; acc: 0.55
Batch: 120; loss: 1.73; acc: 0.53
Batch: 140; loss: 1.72; acc: 0.52
Batch: 160; loss: 1.69; acc: 0.47
Batch: 180; loss: 1.61; acc: 0.45
Batch: 200; loss: 1.63; acc: 0.58
Batch: 220; loss: 1.55; acc: 0.64
Batch: 240; loss: 1.59; acc: 0.56
Batch: 260; loss: 1.57; acc: 0.56
Batch: 280; loss: 1.62; acc: 0.59
Batch: 300; loss: 1.59; acc: 0.56
Batch: 320; loss: 1.81; acc: 0.52
Batch: 340; loss: 1.55; acc: 0.55
Batch: 360; loss: 1.63; acc: 0.52
Batch: 380; loss: 1.71; acc: 0.48
Batch: 400; loss: 1.58; acc: 0.58
Batch: 420; loss: 1.73; acc: 0.53
Batch: 440; loss: 1.74; acc: 0.45
Batch: 460; loss: 1.69; acc: 0.48
Batch: 480; loss: 1.69; acc: 0.52
Batch: 500; loss: 1.7; acc: 0.53
Batch: 520; loss: 1.69; acc: 0.52
Batch: 540; loss: 1.73; acc: 0.44
Batch: 560; loss: 1.7; acc: 0.47
Batch: 580; loss: 1.87; acc: 0.34
Batch: 600; loss: 1.78; acc: 0.44
Batch: 620; loss: 1.71; acc: 0.53
Batch: 640; loss: 1.66; acc: 0.55
Batch: 660; loss: 1.57; acc: 0.67
Batch: 680; loss: 1.63; acc: 0.56
Batch: 700; loss: 1.57; acc: 0.58
Batch: 720; loss: 1.69; acc: 0.56
Batch: 740; loss: 1.75; acc: 0.48
Batch: 760; loss: 1.69; acc: 0.52
Batch: 780; loss: 1.77; acc: 0.44
Train Epoch over. train_loss: 1.66; train_accuracy: 0.53 

3.0382618206203915e-05
9.00030408956809e-06
Batch: 0; loss: 1.74; acc: 0.44
Batch: 20; loss: 1.65; acc: 0.47
Batch: 40; loss: 1.42; acc: 0.7
Batch: 60; loss: 1.65; acc: 0.55
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.73; acc: 0.5
Batch: 120; loss: 1.71; acc: 0.42
Batch: 140; loss: 1.64; acc: 0.5
Val Epoch over. val_loss: 1.6570471942804421; val_accuracy: 0.5427945859872612 

The current subspace-distance is: 9.00030408956809e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.51; acc: 0.62
Batch: 20; loss: 1.63; acc: 0.55
Batch: 40; loss: 1.62; acc: 0.56
Batch: 60; loss: 1.6; acc: 0.61
Batch: 80; loss: 1.65; acc: 0.55
Batch: 100; loss: 1.72; acc: 0.44
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.55; acc: 0.62
Batch: 160; loss: 1.71; acc: 0.48
Batch: 180; loss: 1.66; acc: 0.55
Batch: 200; loss: 1.82; acc: 0.39
Batch: 220; loss: 1.61; acc: 0.59
Batch: 240; loss: 1.72; acc: 0.47
Batch: 260; loss: 1.67; acc: 0.52
Batch: 280; loss: 1.61; acc: 0.56
Batch: 300; loss: 1.66; acc: 0.52
Batch: 320; loss: 1.55; acc: 0.58
Batch: 340; loss: 1.69; acc: 0.5
Batch: 360; loss: 1.63; acc: 0.61
Batch: 380; loss: 1.6; acc: 0.64
Batch: 400; loss: 1.77; acc: 0.48
Batch: 420; loss: 1.75; acc: 0.47
Batch: 440; loss: 1.74; acc: 0.53
Batch: 460; loss: 1.84; acc: 0.44
Batch: 480; loss: 1.71; acc: 0.5
Batch: 500; loss: 1.6; acc: 0.59
Batch: 520; loss: 1.68; acc: 0.55
Batch: 540; loss: 1.69; acc: 0.52
Batch: 560; loss: 1.74; acc: 0.42
Batch: 580; loss: 1.77; acc: 0.5
Batch: 600; loss: 1.66; acc: 0.52
Batch: 620; loss: 1.59; acc: 0.56
Batch: 640; loss: 1.7; acc: 0.47
Batch: 660; loss: 1.73; acc: 0.53
Batch: 680; loss: 1.6; acc: 0.53
Batch: 700; loss: 1.74; acc: 0.44
Batch: 720; loss: 1.44; acc: 0.75
Batch: 740; loss: 1.64; acc: 0.56
Batch: 760; loss: 1.49; acc: 0.53
Batch: 780; loss: 1.63; acc: 0.61
Train Epoch over. train_loss: 1.65; train_accuracy: 0.54 

3.1734423828311265e-05
1.0348465366405435e-05
Batch: 0; loss: 1.74; acc: 0.45
Batch: 20; loss: 1.63; acc: 0.55
Batch: 40; loss: 1.4; acc: 0.7
Batch: 60; loss: 1.62; acc: 0.59
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.7; acc: 0.52
Batch: 120; loss: 1.71; acc: 0.42
Batch: 140; loss: 1.62; acc: 0.52
Val Epoch over. val_loss: 1.6466365308518622; val_accuracy: 0.5494625796178344 

The current subspace-distance is: 1.0348465366405435e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.56
Batch: 20; loss: 1.67; acc: 0.5
Batch: 40; loss: 1.79; acc: 0.45
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.64; acc: 0.53
Batch: 100; loss: 1.59; acc: 0.64
Batch: 120; loss: 1.88; acc: 0.41
Batch: 140; loss: 1.47; acc: 0.62
Batch: 160; loss: 1.55; acc: 0.56
Batch: 180; loss: 1.66; acc: 0.48
Batch: 200; loss: 1.6; acc: 0.58
Batch: 220; loss: 1.51; acc: 0.62
Batch: 240; loss: 1.66; acc: 0.61
Batch: 260; loss: 1.63; acc: 0.55
Batch: 280; loss: 1.57; acc: 0.62
Batch: 300; loss: 1.74; acc: 0.48
Batch: 320; loss: 1.62; acc: 0.45
Batch: 340; loss: 1.58; acc: 0.7
Batch: 360; loss: 1.5; acc: 0.61
Batch: 380; loss: 1.71; acc: 0.48
Batch: 400; loss: 1.6; acc: 0.56
Batch: 420; loss: 1.67; acc: 0.47
Batch: 440; loss: 1.55; acc: 0.61
Batch: 460; loss: 1.72; acc: 0.48
Batch: 480; loss: 1.79; acc: 0.47
Batch: 500; loss: 1.66; acc: 0.47
Batch: 520; loss: 1.64; acc: 0.58
Batch: 540; loss: 1.59; acc: 0.61
Batch: 560; loss: 1.78; acc: 0.42
Batch: 580; loss: 1.72; acc: 0.5
Batch: 600; loss: 1.8; acc: 0.42
Batch: 620; loss: 1.74; acc: 0.52
Batch: 640; loss: 1.75; acc: 0.47
Batch: 660; loss: 1.68; acc: 0.5
Batch: 680; loss: 1.63; acc: 0.5
Batch: 700; loss: 1.61; acc: 0.61
Batch: 720; loss: 1.69; acc: 0.47
Batch: 740; loss: 1.67; acc: 0.53
Batch: 760; loss: 1.55; acc: 0.53
Batch: 780; loss: 1.57; acc: 0.61
Train Epoch over. train_loss: 1.65; train_accuracy: 0.54 

3.257849675719626e-05
9.78269144980004e-06
Batch: 0; loss: 1.73; acc: 0.5
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.41; acc: 0.66
Batch: 60; loss: 1.61; acc: 0.58
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.72; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.61; acc: 0.48
Val Epoch over. val_loss: 1.6456168335714159; val_accuracy: 0.5421974522292994 

The current subspace-distance is: 9.78269144980004e-06 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.62
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.61; acc: 0.61
Batch: 60; loss: 1.54; acc: 0.5
Batch: 80; loss: 1.55; acc: 0.62
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.81; acc: 0.48
Batch: 140; loss: 1.66; acc: 0.55
Batch: 160; loss: 1.69; acc: 0.53
Batch: 180; loss: 1.7; acc: 0.5
Batch: 200; loss: 1.68; acc: 0.48
Batch: 220; loss: 1.66; acc: 0.47
Batch: 240; loss: 1.5; acc: 0.66
Batch: 260; loss: 1.7; acc: 0.55
Batch: 280; loss: 1.67; acc: 0.52
Batch: 300; loss: 1.71; acc: 0.47
Batch: 320; loss: 1.62; acc: 0.59
Batch: 340; loss: 1.59; acc: 0.52
Batch: 360; loss: 1.59; acc: 0.61
Batch: 380; loss: 1.71; acc: 0.59
Batch: 400; loss: 1.56; acc: 0.62
Batch: 420; loss: 1.53; acc: 0.62
Batch: 440; loss: 1.64; acc: 0.45
Batch: 460; loss: 1.74; acc: 0.45
Batch: 480; loss: 1.67; acc: 0.5
Batch: 500; loss: 1.52; acc: 0.62
Batch: 520; loss: 1.51; acc: 0.61
Batch: 540; loss: 1.57; acc: 0.59
Batch: 560; loss: 1.61; acc: 0.58
Batch: 580; loss: 1.49; acc: 0.64
Batch: 600; loss: 1.55; acc: 0.62
Batch: 620; loss: 1.71; acc: 0.48
Batch: 640; loss: 1.57; acc: 0.58
Batch: 660; loss: 1.58; acc: 0.62
Batch: 680; loss: 1.69; acc: 0.52
Batch: 700; loss: 1.71; acc: 0.44
Batch: 720; loss: 1.61; acc: 0.58
Batch: 740; loss: 1.85; acc: 0.5
Batch: 760; loss: 1.71; acc: 0.47
Batch: 780; loss: 1.67; acc: 0.56
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.35433469444979e-05
9.903256795951165e-06
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.63; acc: 0.53
Batch: 40; loss: 1.41; acc: 0.66
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.52
Batch: 100; loss: 1.71; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.61; acc: 0.47
Val Epoch over. val_loss: 1.64547101783145; val_accuracy: 0.5394108280254777 

The current subspace-distance is: 9.903256795951165e-06 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.79; acc: 0.47
Batch: 40; loss: 1.7; acc: 0.53
Batch: 60; loss: 1.8; acc: 0.44
Batch: 80; loss: 1.84; acc: 0.39
Batch: 100; loss: 1.52; acc: 0.66
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.59; acc: 0.58
Batch: 160; loss: 1.55; acc: 0.58
Batch: 180; loss: 1.59; acc: 0.58
Batch: 200; loss: 1.51; acc: 0.61
Batch: 220; loss: 1.54; acc: 0.66
Batch: 240; loss: 1.56; acc: 0.67
Batch: 260; loss: 1.64; acc: 0.53
Batch: 280; loss: 1.57; acc: 0.58
Batch: 300; loss: 1.61; acc: 0.56
Batch: 320; loss: 1.54; acc: 0.58
Batch: 340; loss: 1.64; acc: 0.55
Batch: 360; loss: 1.64; acc: 0.58
Batch: 380; loss: 1.62; acc: 0.52
Batch: 400; loss: 1.61; acc: 0.56
Batch: 420; loss: 1.5; acc: 0.61
Batch: 440; loss: 1.63; acc: 0.55
Batch: 460; loss: 1.78; acc: 0.58
Batch: 480; loss: 1.57; acc: 0.55
Batch: 500; loss: 1.57; acc: 0.64
Batch: 520; loss: 1.62; acc: 0.5
Batch: 540; loss: 1.69; acc: 0.48
Batch: 560; loss: 1.76; acc: 0.45
Batch: 580; loss: 1.56; acc: 0.48
Batch: 600; loss: 1.61; acc: 0.58
Batch: 620; loss: 1.68; acc: 0.55
Batch: 640; loss: 1.61; acc: 0.5
Batch: 660; loss: 1.72; acc: 0.52
Batch: 680; loss: 1.63; acc: 0.56
Batch: 700; loss: 1.57; acc: 0.66
Batch: 720; loss: 1.56; acc: 0.61
Batch: 740; loss: 1.62; acc: 0.59
Batch: 760; loss: 1.59; acc: 0.61
Batch: 780; loss: 1.49; acc: 0.59
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.398079934413545e-05
9.802082786336541e-06
Batch: 0; loss: 1.72; acc: 0.5
Batch: 20; loss: 1.63; acc: 0.53
Batch: 40; loss: 1.4; acc: 0.67
Batch: 60; loss: 1.59; acc: 0.56
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.69; acc: 0.53
Batch: 120; loss: 1.72; acc: 0.45
Batch: 140; loss: 1.6; acc: 0.45
Val Epoch over. val_loss: 1.6365179559987062; val_accuracy: 0.5439888535031847 

The current subspace-distance is: 9.802082786336541e-06 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.59
Batch: 20; loss: 1.75; acc: 0.45
Batch: 40; loss: 1.7; acc: 0.47
Batch: 60; loss: 1.66; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.53
Batch: 100; loss: 1.61; acc: 0.53
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.59; acc: 0.59
Batch: 160; loss: 1.61; acc: 0.53
Batch: 180; loss: 1.55; acc: 0.59
Batch: 200; loss: 1.73; acc: 0.45
Batch: 220; loss: 1.68; acc: 0.5
Batch: 240; loss: 1.58; acc: 0.56
Batch: 260; loss: 1.55; acc: 0.61
Batch: 280; loss: 1.54; acc: 0.62
Batch: 300; loss: 1.63; acc: 0.53
Batch: 320; loss: 1.71; acc: 0.48
Batch: 340; loss: 1.65; acc: 0.5
Batch: 360; loss: 1.71; acc: 0.55
Batch: 380; loss: 1.6; acc: 0.53
Batch: 400; loss: 1.69; acc: 0.5
Batch: 420; loss: 1.63; acc: 0.53
Batch: 440; loss: 1.64; acc: 0.48
Batch: 460; loss: 1.56; acc: 0.55
Batch: 480; loss: 1.62; acc: 0.56
Batch: 500; loss: 1.57; acc: 0.58
Batch: 520; loss: 1.55; acc: 0.59
Batch: 540; loss: 1.73; acc: 0.53
Batch: 560; loss: 1.79; acc: 0.45
Batch: 580; loss: 1.62; acc: 0.56
Batch: 600; loss: 1.73; acc: 0.45
Batch: 620; loss: 1.77; acc: 0.5
Batch: 640; loss: 1.63; acc: 0.56
Batch: 660; loss: 1.62; acc: 0.58
Batch: 680; loss: 1.69; acc: 0.5
Batch: 700; loss: 1.7; acc: 0.36
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.55; acc: 0.58
Batch: 760; loss: 1.63; acc: 0.61
Batch: 780; loss: 1.67; acc: 0.47
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.404796734685078e-05
1.0296538675902411e-05
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.63; acc: 0.56
Batch: 40; loss: 1.4; acc: 0.67
Batch: 60; loss: 1.59; acc: 0.56
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.7; acc: 0.5
Batch: 120; loss: 1.73; acc: 0.45
Batch: 140; loss: 1.59; acc: 0.45
Val Epoch over. val_loss: 1.638416937202405; val_accuracy: 0.5416003184713376 

The current subspace-distance is: 1.0296538675902411e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.53; acc: 0.62
Batch: 40; loss: 1.72; acc: 0.52
Batch: 60; loss: 1.65; acc: 0.48
Batch: 80; loss: 1.49; acc: 0.59
Batch: 100; loss: 1.65; acc: 0.47
Batch: 120; loss: 1.68; acc: 0.47
Batch: 140; loss: 1.62; acc: 0.55
Batch: 160; loss: 1.54; acc: 0.61
Batch: 180; loss: 1.66; acc: 0.44
Batch: 200; loss: 1.69; acc: 0.53
Batch: 220; loss: 1.55; acc: 0.58
Batch: 240; loss: 1.57; acc: 0.58
Batch: 260; loss: 1.63; acc: 0.52
Batch: 280; loss: 1.48; acc: 0.64
Batch: 300; loss: 1.58; acc: 0.62
Batch: 320; loss: 1.6; acc: 0.53
Batch: 340; loss: 1.82; acc: 0.45
Batch: 360; loss: 1.76; acc: 0.47
Batch: 380; loss: 1.58; acc: 0.59
Batch: 400; loss: 1.61; acc: 0.59
Batch: 420; loss: 1.74; acc: 0.5
Batch: 440; loss: 1.66; acc: 0.52
Batch: 460; loss: 1.52; acc: 0.62
Batch: 480; loss: 1.59; acc: 0.62
Batch: 500; loss: 1.69; acc: 0.5
Batch: 520; loss: 1.65; acc: 0.5
Batch: 540; loss: 1.67; acc: 0.56
Batch: 560; loss: 1.6; acc: 0.55
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.67; acc: 0.55
Batch: 620; loss: 1.6; acc: 0.52
Batch: 640; loss: 1.59; acc: 0.61
Batch: 660; loss: 1.56; acc: 0.52
Batch: 680; loss: 1.7; acc: 0.45
Batch: 700; loss: 1.52; acc: 0.66
Batch: 720; loss: 1.64; acc: 0.53
Batch: 740; loss: 1.67; acc: 0.52
Batch: 760; loss: 1.63; acc: 0.56
Batch: 780; loss: 1.52; acc: 0.59
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.4113560104742646e-05
9.290554771723691e-06
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.5
Batch: 40; loss: 1.41; acc: 0.67
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.55
Batch: 100; loss: 1.71; acc: 0.53
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.59; acc: 0.45
Val Epoch over. val_loss: 1.6391935325731897; val_accuracy: 0.5409036624203821 

The current subspace-distance is: 9.290554771723691e-06 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.52
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.51; acc: 0.56
Batch: 60; loss: 1.72; acc: 0.45
Batch: 80; loss: 1.79; acc: 0.45
Batch: 100; loss: 1.69; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.55; acc: 0.66
Batch: 160; loss: 1.67; acc: 0.44
Batch: 180; loss: 1.71; acc: 0.48
Batch: 200; loss: 1.7; acc: 0.5
Batch: 220; loss: 1.6; acc: 0.48
Batch: 240; loss: 1.49; acc: 0.62
Batch: 260; loss: 1.62; acc: 0.53
Batch: 280; loss: 1.63; acc: 0.56
Batch: 300; loss: 1.5; acc: 0.66
Batch: 320; loss: 1.63; acc: 0.55
Batch: 340; loss: 1.62; acc: 0.64
Batch: 360; loss: 1.69; acc: 0.53
Batch: 380; loss: 1.59; acc: 0.61
Batch: 400; loss: 1.78; acc: 0.47
Batch: 420; loss: 1.51; acc: 0.58
Batch: 440; loss: 1.68; acc: 0.45
Batch: 460; loss: 1.72; acc: 0.44
Batch: 480; loss: 1.73; acc: 0.48
Batch: 500; loss: 1.6; acc: 0.61
Batch: 520; loss: 1.7; acc: 0.47
Batch: 540; loss: 1.63; acc: 0.47
Batch: 560; loss: 1.7; acc: 0.48
Batch: 580; loss: 1.71; acc: 0.5
Batch: 600; loss: 1.56; acc: 0.59
Batch: 620; loss: 1.68; acc: 0.48
Batch: 640; loss: 1.68; acc: 0.52
Batch: 660; loss: 1.73; acc: 0.47
Batch: 680; loss: 1.65; acc: 0.61
Batch: 700; loss: 1.52; acc: 0.62
Batch: 720; loss: 1.66; acc: 0.58
Batch: 740; loss: 1.67; acc: 0.48
Batch: 760; loss: 1.52; acc: 0.62
Batch: 780; loss: 1.75; acc: 0.47
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.4933051210828125e-05
1.194701690110378e-05
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.48
Batch: 40; loss: 1.4; acc: 0.66
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.7; acc: 0.55
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.59; acc: 0.5
Val Epoch over. val_loss: 1.6358176773520792; val_accuracy: 0.5461783439490446 

The current subspace-distance is: 1.194701690110378e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.62; acc: 0.58
Batch: 40; loss: 1.64; acc: 0.58
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.53
Batch: 100; loss: 1.62; acc: 0.59
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.67; acc: 0.55
Batch: 160; loss: 1.68; acc: 0.48
Batch: 180; loss: 1.55; acc: 0.7
Batch: 200; loss: 1.61; acc: 0.56
Batch: 220; loss: 1.55; acc: 0.62
Batch: 240; loss: 1.73; acc: 0.42
Batch: 260; loss: 1.61; acc: 0.64
Batch: 280; loss: 1.78; acc: 0.48
Batch: 300; loss: 1.75; acc: 0.44
Batch: 320; loss: 1.53; acc: 0.69
Batch: 340; loss: 1.74; acc: 0.48
Batch: 360; loss: 1.69; acc: 0.47
Batch: 380; loss: 1.66; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.55
Batch: 420; loss: 1.57; acc: 0.58
Batch: 440; loss: 1.72; acc: 0.44
Batch: 460; loss: 1.53; acc: 0.64
Batch: 480; loss: 1.68; acc: 0.55
Batch: 500; loss: 1.63; acc: 0.55
Batch: 520; loss: 1.78; acc: 0.44
Batch: 540; loss: 1.69; acc: 0.5
Batch: 560; loss: 1.67; acc: 0.52
Batch: 580; loss: 1.63; acc: 0.56
Batch: 600; loss: 1.56; acc: 0.59
Batch: 620; loss: 1.64; acc: 0.53
Batch: 640; loss: 1.58; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.55
Batch: 680; loss: 1.54; acc: 0.61
Batch: 700; loss: 1.6; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.48
Batch: 740; loss: 1.67; acc: 0.59
Batch: 760; loss: 1.61; acc: 0.56
Batch: 780; loss: 1.54; acc: 0.59
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.454385660006665e-05
8.631286618765444e-06
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.39; acc: 0.66
Batch: 60; loss: 1.58; acc: 0.58
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.57; acc: 0.5
Val Epoch over. val_loss: 1.6296746358749972; val_accuracy: 0.54578025477707 

The current subspace-distance is: 8.631286618765444e-06 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.56
Batch: 40; loss: 1.77; acc: 0.47
Batch: 60; loss: 1.61; acc: 0.56
Batch: 80; loss: 1.54; acc: 0.59
Batch: 100; loss: 1.71; acc: 0.52
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.69; acc: 0.56
Batch: 160; loss: 1.58; acc: 0.56
Batch: 180; loss: 1.65; acc: 0.47
Batch: 200; loss: 1.65; acc: 0.55
Batch: 220; loss: 1.63; acc: 0.59
Batch: 240; loss: 1.65; acc: 0.56
Batch: 260; loss: 1.47; acc: 0.67
Batch: 280; loss: 1.71; acc: 0.53
Batch: 300; loss: 1.58; acc: 0.61
Batch: 320; loss: 1.68; acc: 0.48
Batch: 340; loss: 1.64; acc: 0.53
Batch: 360; loss: 1.78; acc: 0.47
Batch: 380; loss: 1.66; acc: 0.52
Batch: 400; loss: 1.77; acc: 0.44
Batch: 420; loss: 1.71; acc: 0.52
Batch: 440; loss: 1.8; acc: 0.44
Batch: 460; loss: 1.74; acc: 0.47
Batch: 480; loss: 1.67; acc: 0.5
Batch: 500; loss: 1.54; acc: 0.56
Batch: 520; loss: 1.63; acc: 0.55
Batch: 540; loss: 1.64; acc: 0.64
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.63; acc: 0.48
Batch: 600; loss: 1.7; acc: 0.55
Batch: 620; loss: 1.8; acc: 0.44
Batch: 640; loss: 1.62; acc: 0.62
Batch: 660; loss: 1.72; acc: 0.5
Batch: 680; loss: 1.65; acc: 0.55
Batch: 700; loss: 1.68; acc: 0.52
Batch: 720; loss: 1.56; acc: 0.59
Batch: 740; loss: 1.69; acc: 0.47
Batch: 760; loss: 1.69; acc: 0.5
Batch: 780; loss: 1.68; acc: 0.52
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.486200512270443e-05
1.154028814198682e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.66; acc: 0.5
Batch: 40; loss: 1.4; acc: 0.67
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.71; acc: 0.53
Batch: 120; loss: 1.74; acc: 0.47
Batch: 140; loss: 1.58; acc: 0.53
Val Epoch over. val_loss: 1.6364125863761658; val_accuracy: 0.5409036624203821 

The current subspace-distance is: 1.154028814198682e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.61
Batch: 20; loss: 1.77; acc: 0.45
Batch: 40; loss: 1.69; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.53
Batch: 120; loss: 1.56; acc: 0.67
Batch: 140; loss: 1.67; acc: 0.52
Batch: 160; loss: 1.63; acc: 0.53
Batch: 180; loss: 1.63; acc: 0.58
Batch: 200; loss: 1.63; acc: 0.48
Batch: 220; loss: 1.51; acc: 0.61
Batch: 240; loss: 1.7; acc: 0.55
Batch: 260; loss: 1.6; acc: 0.48
Batch: 280; loss: 1.54; acc: 0.61
Batch: 300; loss: 1.57; acc: 0.55
Batch: 320; loss: 1.73; acc: 0.52
Batch: 340; loss: 1.59; acc: 0.55
Batch: 360; loss: 1.69; acc: 0.5
Batch: 380; loss: 1.54; acc: 0.59
Batch: 400; loss: 1.6; acc: 0.58
Batch: 420; loss: 1.7; acc: 0.48
Batch: 440; loss: 1.72; acc: 0.5
Batch: 460; loss: 1.61; acc: 0.56
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.68; acc: 0.53
Batch: 520; loss: 1.62; acc: 0.55
Batch: 540; loss: 1.69; acc: 0.5
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.63; acc: 0.48
Batch: 600; loss: 1.62; acc: 0.55
Batch: 620; loss: 1.61; acc: 0.56
Batch: 640; loss: 1.6; acc: 0.56
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.66; acc: 0.45
Batch: 700; loss: 1.52; acc: 0.53
Batch: 720; loss: 1.64; acc: 0.5
Batch: 740; loss: 1.66; acc: 0.53
Batch: 760; loss: 1.7; acc: 0.53
Batch: 780; loss: 1.78; acc: 0.34
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.56327036570292e-05
1.141373832069803e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.38; acc: 0.7
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.69; acc: 0.53
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.55
Val Epoch over. val_loss: 1.623600458643239; val_accuracy: 0.5486664012738853 

The current subspace-distance is: 1.141373832069803e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.55; acc: 0.61
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.64; acc: 0.61
Batch: 60; loss: 1.54; acc: 0.58
Batch: 80; loss: 1.69; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.47
Batch: 120; loss: 1.53; acc: 0.67
Batch: 140; loss: 1.63; acc: 0.58
Batch: 160; loss: 1.66; acc: 0.53
Batch: 180; loss: 1.64; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.59
Batch: 220; loss: 1.56; acc: 0.62
Batch: 240; loss: 1.82; acc: 0.38
Batch: 260; loss: 1.59; acc: 0.58
Batch: 280; loss: 1.76; acc: 0.47
Batch: 300; loss: 1.58; acc: 0.53
Batch: 320; loss: 1.69; acc: 0.55
Batch: 340; loss: 1.64; acc: 0.58
Batch: 360; loss: 1.52; acc: 0.61
Batch: 380; loss: 1.46; acc: 0.64
Batch: 400; loss: 1.66; acc: 0.52
Batch: 420; loss: 1.67; acc: 0.52
Batch: 440; loss: 1.64; acc: 0.55
Batch: 460; loss: 1.62; acc: 0.47
Batch: 480; loss: 1.63; acc: 0.53
Batch: 500; loss: 1.74; acc: 0.47
Batch: 520; loss: 1.72; acc: 0.53
Batch: 540; loss: 1.66; acc: 0.47
Batch: 560; loss: 1.58; acc: 0.59
Batch: 580; loss: 1.63; acc: 0.52
Batch: 600; loss: 1.7; acc: 0.52
Batch: 620; loss: 1.6; acc: 0.61
Batch: 640; loss: 1.7; acc: 0.55
Batch: 660; loss: 1.54; acc: 0.64
Batch: 680; loss: 1.64; acc: 0.52
Batch: 700; loss: 1.72; acc: 0.48
Batch: 720; loss: 1.69; acc: 0.55
Batch: 740; loss: 1.63; acc: 0.55
Batch: 760; loss: 1.66; acc: 0.5
Batch: 780; loss: 1.62; acc: 0.48
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.547607411746867e-05
9.905720617098268e-06
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.5
Batch: 40; loss: 1.39; acc: 0.66
Batch: 60; loss: 1.58; acc: 0.56
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.68; acc: 0.5
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.57; acc: 0.52
Val Epoch over. val_loss: 1.63116771324425; val_accuracy: 0.5431926751592356 

The current subspace-distance is: 9.905720617098268e-06 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.55
Batch: 20; loss: 1.71; acc: 0.45
Batch: 40; loss: 1.64; acc: 0.47
Batch: 60; loss: 1.57; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.58; acc: 0.56
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.74; acc: 0.47
Batch: 160; loss: 1.64; acc: 0.52
Batch: 180; loss: 1.59; acc: 0.52
Batch: 200; loss: 1.72; acc: 0.47
Batch: 220; loss: 1.62; acc: 0.56
Batch: 240; loss: 1.59; acc: 0.5
Batch: 260; loss: 1.58; acc: 0.5
Batch: 280; loss: 1.63; acc: 0.56
Batch: 300; loss: 1.56; acc: 0.55
Batch: 320; loss: 1.75; acc: 0.45
Batch: 340; loss: 1.63; acc: 0.5
Batch: 360; loss: 1.6; acc: 0.56
Batch: 380; loss: 1.52; acc: 0.62
Batch: 400; loss: 1.56; acc: 0.62
Batch: 420; loss: 1.59; acc: 0.56
Batch: 440; loss: 1.74; acc: 0.47
Batch: 460; loss: 1.73; acc: 0.47
Batch: 480; loss: 1.64; acc: 0.53
Batch: 500; loss: 1.63; acc: 0.47
Batch: 520; loss: 1.52; acc: 0.59
Batch: 540; loss: 1.65; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.58
Batch: 580; loss: 1.6; acc: 0.59
Batch: 600; loss: 1.65; acc: 0.52
Batch: 620; loss: 1.59; acc: 0.56
Batch: 640; loss: 1.73; acc: 0.48
Batch: 660; loss: 1.66; acc: 0.55
Batch: 680; loss: 1.69; acc: 0.5
Batch: 700; loss: 1.71; acc: 0.52
Batch: 720; loss: 1.55; acc: 0.53
Batch: 740; loss: 1.58; acc: 0.55
Batch: 760; loss: 1.58; acc: 0.58
Batch: 780; loss: 1.61; acc: 0.59
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.606025711633265e-05
1.1789858945121523e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.4; acc: 0.7
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.53
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.57; acc: 0.52
Val Epoch over. val_loss: 1.6325113401291476; val_accuracy: 0.5466759554140127 

The current subspace-distance is: 1.1789858945121523e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.69; acc: 0.5
Batch: 40; loss: 1.73; acc: 0.48
Batch: 60; loss: 1.51; acc: 0.62
Batch: 80; loss: 1.67; acc: 0.55
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.58
Batch: 140; loss: 1.58; acc: 0.64
Batch: 160; loss: 1.83; acc: 0.44
Batch: 180; loss: 1.49; acc: 0.64
Batch: 200; loss: 1.65; acc: 0.45
Batch: 220; loss: 1.47; acc: 0.62
Batch: 240; loss: 1.76; acc: 0.5
Batch: 260; loss: 1.54; acc: 0.59
Batch: 280; loss: 1.68; acc: 0.5
Batch: 300; loss: 1.69; acc: 0.55
Batch: 320; loss: 1.72; acc: 0.45
Batch: 340; loss: 1.65; acc: 0.55
Batch: 360; loss: 1.7; acc: 0.48
Batch: 380; loss: 1.64; acc: 0.53
Batch: 400; loss: 1.59; acc: 0.52
Batch: 420; loss: 1.66; acc: 0.48
Batch: 440; loss: 1.55; acc: 0.61
Batch: 460; loss: 1.66; acc: 0.52
Batch: 480; loss: 1.6; acc: 0.58
Batch: 500; loss: 1.65; acc: 0.52
Batch: 520; loss: 1.63; acc: 0.53
Batch: 540; loss: 1.61; acc: 0.55
Batch: 560; loss: 1.72; acc: 0.5
Batch: 580; loss: 1.67; acc: 0.5
Batch: 600; loss: 1.69; acc: 0.47
Batch: 620; loss: 1.59; acc: 0.55
Batch: 640; loss: 1.63; acc: 0.56
Batch: 660; loss: 1.69; acc: 0.44
Batch: 680; loss: 1.5; acc: 0.62
Batch: 700; loss: 1.7; acc: 0.41
Batch: 720; loss: 1.63; acc: 0.53
Batch: 740; loss: 1.63; acc: 0.59
Batch: 760; loss: 1.75; acc: 0.45
Batch: 780; loss: 1.52; acc: 0.58
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.624944292823784e-05
1.2301554306759499e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.39; acc: 0.69
Batch: 60; loss: 1.57; acc: 0.58
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.59
Val Epoch over. val_loss: 1.6302390933796098; val_accuracy: 0.5484673566878981 

The current subspace-distance is: 1.2301554306759499e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.56
Batch: 20; loss: 1.53; acc: 0.58
Batch: 40; loss: 1.67; acc: 0.56
Batch: 60; loss: 1.62; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.52
Batch: 100; loss: 1.55; acc: 0.55
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 1.71; acc: 0.5
Batch: 160; loss: 1.5; acc: 0.64
Batch: 180; loss: 1.66; acc: 0.48
Batch: 200; loss: 1.56; acc: 0.56
Batch: 220; loss: 1.55; acc: 0.58
Batch: 240; loss: 1.66; acc: 0.5
Batch: 260; loss: 1.72; acc: 0.44
Batch: 280; loss: 1.6; acc: 0.59
Batch: 300; loss: 1.58; acc: 0.53
Batch: 320; loss: 1.63; acc: 0.53
Batch: 340; loss: 1.75; acc: 0.48
Batch: 360; loss: 1.7; acc: 0.52
Batch: 380; loss: 1.73; acc: 0.39
Batch: 400; loss: 1.61; acc: 0.56
Batch: 420; loss: 1.66; acc: 0.56
Batch: 440; loss: 1.76; acc: 0.5
Batch: 460; loss: 1.67; acc: 0.5
Batch: 480; loss: 1.5; acc: 0.62
Batch: 500; loss: 1.57; acc: 0.62
Batch: 520; loss: 1.58; acc: 0.61
Batch: 540; loss: 1.54; acc: 0.61
Batch: 560; loss: 1.53; acc: 0.58
Batch: 580; loss: 1.62; acc: 0.47
Batch: 600; loss: 1.69; acc: 0.47
Batch: 620; loss: 1.66; acc: 0.59
Batch: 640; loss: 1.69; acc: 0.59
Batch: 660; loss: 1.62; acc: 0.55
Batch: 680; loss: 1.62; acc: 0.56
Batch: 700; loss: 1.51; acc: 0.62
Batch: 720; loss: 1.63; acc: 0.5
Batch: 740; loss: 1.64; acc: 0.52
Batch: 760; loss: 1.61; acc: 0.56
Batch: 780; loss: 1.64; acc: 0.52
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.625854878919199e-05
1.3319124263944104e-05
Batch: 0; loss: 1.71; acc: 0.44
Batch: 20; loss: 1.66; acc: 0.5
Batch: 40; loss: 1.38; acc: 0.67
Batch: 60; loss: 1.57; acc: 0.55
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.69; acc: 0.5
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.55
Val Epoch over. val_loss: 1.6291552417597193; val_accuracy: 0.5440883757961783 

The current subspace-distance is: 1.3319124263944104e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.58; acc: 0.59
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.63; acc: 0.5
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.48; acc: 0.62
Batch: 140; loss: 1.59; acc: 0.47
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.59; acc: 0.59
Batch: 200; loss: 1.53; acc: 0.59
Batch: 220; loss: 1.65; acc: 0.53
Batch: 240; loss: 1.54; acc: 0.58
Batch: 260; loss: 1.75; acc: 0.47
Batch: 280; loss: 1.57; acc: 0.55
Batch: 300; loss: 1.62; acc: 0.56
Batch: 320; loss: 1.66; acc: 0.53
Batch: 340; loss: 1.67; acc: 0.52
Batch: 360; loss: 1.69; acc: 0.45
Batch: 380; loss: 1.74; acc: 0.48
Batch: 400; loss: 1.63; acc: 0.55
Batch: 420; loss: 1.49; acc: 0.62
Batch: 440; loss: 1.68; acc: 0.52
Batch: 460; loss: 1.49; acc: 0.62
Batch: 480; loss: 1.53; acc: 0.61
Batch: 500; loss: 1.68; acc: 0.45
Batch: 520; loss: 1.64; acc: 0.58
Batch: 540; loss: 1.49; acc: 0.66
Batch: 560; loss: 1.69; acc: 0.55
Batch: 580; loss: 1.8; acc: 0.5
Batch: 600; loss: 1.57; acc: 0.61
Batch: 620; loss: 1.63; acc: 0.45
Batch: 640; loss: 1.57; acc: 0.56
Batch: 660; loss: 1.66; acc: 0.58
Batch: 680; loss: 1.61; acc: 0.5
Batch: 700; loss: 1.68; acc: 0.59
Batch: 720; loss: 1.65; acc: 0.56
Batch: 740; loss: 1.77; acc: 0.5
Batch: 760; loss: 1.7; acc: 0.38
Batch: 780; loss: 1.53; acc: 0.62
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.55074334947858e-05
9.938063158188015e-06
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.38; acc: 0.69
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.71; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.53
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.57; acc: 0.52
Val Epoch over. val_loss: 1.633666106849719; val_accuracy: 0.5439888535031847 

The current subspace-distance is: 9.938063158188015e-06 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.51; acc: 0.59
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.65; acc: 0.53
Batch: 60; loss: 1.67; acc: 0.52
Batch: 80; loss: 1.49; acc: 0.73
Batch: 100; loss: 1.49; acc: 0.55
Batch: 120; loss: 1.63; acc: 0.5
Batch: 140; loss: 1.64; acc: 0.5
Batch: 160; loss: 1.5; acc: 0.59
Batch: 180; loss: 1.6; acc: 0.58
Batch: 200; loss: 1.66; acc: 0.52
Batch: 220; loss: 1.65; acc: 0.5
Batch: 240; loss: 1.59; acc: 0.58
Batch: 260; loss: 1.6; acc: 0.52
Batch: 280; loss: 1.5; acc: 0.62
Batch: 300; loss: 1.63; acc: 0.59
Batch: 320; loss: 1.63; acc: 0.53
Batch: 340; loss: 1.64; acc: 0.48
Batch: 360; loss: 1.63; acc: 0.55
Batch: 380; loss: 1.62; acc: 0.52
Batch: 400; loss: 1.53; acc: 0.59
Batch: 420; loss: 1.73; acc: 0.52
Batch: 440; loss: 1.58; acc: 0.5
Batch: 460; loss: 1.63; acc: 0.59
Batch: 480; loss: 1.75; acc: 0.45
Batch: 500; loss: 1.59; acc: 0.61
Batch: 520; loss: 1.7; acc: 0.62
Batch: 540; loss: 1.68; acc: 0.53
Batch: 560; loss: 1.69; acc: 0.5
Batch: 580; loss: 1.68; acc: 0.48
Batch: 600; loss: 1.53; acc: 0.59
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.51; acc: 0.55
Batch: 660; loss: 1.83; acc: 0.41
Batch: 680; loss: 1.6; acc: 0.61
Batch: 700; loss: 1.81; acc: 0.42
Batch: 720; loss: 1.61; acc: 0.58
Batch: 740; loss: 1.57; acc: 0.56
Batch: 760; loss: 1.52; acc: 0.53
Batch: 780; loss: 1.64; acc: 0.47
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.6247187381377444e-05
1.2874085768999066e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.68; acc: 0.5
Batch: 40; loss: 1.38; acc: 0.67
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.68; acc: 0.53
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.56
Val Epoch over. val_loss: 1.6274057383749896; val_accuracy: 0.5415007961783439 

The current subspace-distance is: 1.2874085768999066e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.64; acc: 0.56
Batch: 20; loss: 1.63; acc: 0.55
Batch: 40; loss: 1.65; acc: 0.52
Batch: 60; loss: 1.64; acc: 0.47
Batch: 80; loss: 1.72; acc: 0.53
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.57; acc: 0.61
Batch: 140; loss: 1.55; acc: 0.62
Batch: 160; loss: 1.67; acc: 0.5
Batch: 180; loss: 1.61; acc: 0.53
Batch: 200; loss: 1.58; acc: 0.53
Batch: 220; loss: 1.7; acc: 0.53
Batch: 240; loss: 1.64; acc: 0.56
Batch: 260; loss: 1.5; acc: 0.62
Batch: 280; loss: 1.75; acc: 0.52
Batch: 300; loss: 1.76; acc: 0.47
Batch: 320; loss: 1.64; acc: 0.52
Batch: 340; loss: 1.5; acc: 0.61
Batch: 360; loss: 1.59; acc: 0.61
Batch: 380; loss: 1.57; acc: 0.55
Batch: 400; loss: 1.47; acc: 0.66
Batch: 420; loss: 1.51; acc: 0.64
Batch: 440; loss: 1.69; acc: 0.53
Batch: 460; loss: 1.56; acc: 0.58
Batch: 480; loss: 1.54; acc: 0.58
Batch: 500; loss: 1.68; acc: 0.45
Batch: 520; loss: 1.73; acc: 0.45
Batch: 540; loss: 1.73; acc: 0.44
Batch: 560; loss: 1.73; acc: 0.48
Batch: 580; loss: 1.56; acc: 0.59
Batch: 600; loss: 1.68; acc: 0.59
Batch: 620; loss: 1.69; acc: 0.62
Batch: 640; loss: 1.54; acc: 0.56
Batch: 660; loss: 1.52; acc: 0.64
Batch: 680; loss: 1.7; acc: 0.48
Batch: 700; loss: 1.58; acc: 0.53
Batch: 720; loss: 1.62; acc: 0.58
Batch: 740; loss: 1.51; acc: 0.55
Batch: 760; loss: 1.67; acc: 0.55
Batch: 780; loss: 1.68; acc: 0.53
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.621006544562988e-05
1.0695689525164198e-05
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.67; acc: 0.5
Batch: 40; loss: 1.38; acc: 0.69
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.71; acc: 0.53
Batch: 100; loss: 1.69; acc: 0.53
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.57; acc: 0.55
Val Epoch over. val_loss: 1.6310817390490489; val_accuracy: 0.5438893312101911 

The current subspace-distance is: 1.0695689525164198e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.52
Batch: 20; loss: 1.53; acc: 0.59
Batch: 40; loss: 1.62; acc: 0.61
Batch: 60; loss: 1.75; acc: 0.47
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.58; acc: 0.53
Batch: 120; loss: 1.65; acc: 0.52
Batch: 140; loss: 1.72; acc: 0.5
Batch: 160; loss: 1.64; acc: 0.59
Batch: 180; loss: 1.61; acc: 0.59
Batch: 200; loss: 1.63; acc: 0.53
Batch: 220; loss: 1.57; acc: 0.55
Batch: 240; loss: 1.61; acc: 0.53
Batch: 260; loss: 1.56; acc: 0.59
Batch: 280; loss: 1.55; acc: 0.59
Batch: 300; loss: 1.54; acc: 0.59
Batch: 320; loss: 1.59; acc: 0.55
Batch: 340; loss: 1.67; acc: 0.5
Batch: 360; loss: 1.62; acc: 0.58
Batch: 380; loss: 1.69; acc: 0.42
Batch: 400; loss: 1.72; acc: 0.47
Batch: 420; loss: 1.54; acc: 0.52
Batch: 440; loss: 1.72; acc: 0.45
Batch: 460; loss: 1.68; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.5
Batch: 500; loss: 1.59; acc: 0.58
Batch: 520; loss: 1.53; acc: 0.56
Batch: 540; loss: 1.48; acc: 0.59
Batch: 560; loss: 1.52; acc: 0.59
Batch: 580; loss: 1.6; acc: 0.55
Batch: 600; loss: 1.64; acc: 0.55
Batch: 620; loss: 1.6; acc: 0.58
Batch: 640; loss: 1.72; acc: 0.47
Batch: 660; loss: 1.77; acc: 0.45
Batch: 680; loss: 1.58; acc: 0.58
Batch: 700; loss: 1.67; acc: 0.59
Batch: 720; loss: 1.62; acc: 0.59
Batch: 740; loss: 1.54; acc: 0.64
Batch: 760; loss: 1.71; acc: 0.47
Batch: 780; loss: 1.6; acc: 0.48
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.6484769225353375e-05
1.1630940207396634e-05
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.39; acc: 0.69
Batch: 60; loss: 1.58; acc: 0.58
Batch: 80; loss: 1.71; acc: 0.5
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.59
Val Epoch over. val_loss: 1.633469033393131; val_accuracy: 0.5447850318471338 

The current subspace-distance is: 1.1630940207396634e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.48
Batch: 20; loss: 1.58; acc: 0.53
Batch: 40; loss: 1.66; acc: 0.55
Batch: 60; loss: 1.83; acc: 0.45
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.56; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.56
Batch: 140; loss: 1.56; acc: 0.55
Batch: 160; loss: 1.57; acc: 0.62
Batch: 180; loss: 1.66; acc: 0.52
Batch: 200; loss: 1.6; acc: 0.56
Batch: 220; loss: 1.71; acc: 0.5
Batch: 240; loss: 1.64; acc: 0.58
Batch: 260; loss: 1.61; acc: 0.52
Batch: 280; loss: 1.58; acc: 0.56
Batch: 300; loss: 1.69; acc: 0.55
Batch: 320; loss: 1.49; acc: 0.64
Batch: 340; loss: 1.66; acc: 0.59
Batch: 360; loss: 1.54; acc: 0.53
Batch: 380; loss: 1.67; acc: 0.55
Batch: 400; loss: 1.57; acc: 0.61
Batch: 420; loss: 1.63; acc: 0.58
Batch: 440; loss: 1.58; acc: 0.64
Batch: 460; loss: 1.62; acc: 0.52
Batch: 480; loss: 1.61; acc: 0.5
Batch: 500; loss: 1.65; acc: 0.53
Batch: 520; loss: 1.56; acc: 0.58
Batch: 540; loss: 1.67; acc: 0.47
Batch: 560; loss: 1.54; acc: 0.58
Batch: 580; loss: 1.44; acc: 0.67
Batch: 600; loss: 1.6; acc: 0.55
Batch: 620; loss: 1.56; acc: 0.53
Batch: 640; loss: 1.73; acc: 0.45
Batch: 660; loss: 1.58; acc: 0.59
Batch: 680; loss: 1.75; acc: 0.47
Batch: 700; loss: 1.61; acc: 0.59
Batch: 720; loss: 1.54; acc: 0.66
Batch: 740; loss: 1.76; acc: 0.5
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.65; acc: 0.53
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.729564923560247e-05
1.238193544850219e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.66; acc: 0.52
Batch: 40; loss: 1.38; acc: 0.66
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.56
Val Epoch over. val_loss: 1.6258791844556286; val_accuracy: 0.5465764331210191 

The current subspace-distance is: 1.238193544850219e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.68; acc: 0.44
Batch: 20; loss: 1.63; acc: 0.56
Batch: 40; loss: 1.71; acc: 0.45
Batch: 60; loss: 1.58; acc: 0.62
Batch: 80; loss: 1.62; acc: 0.56
Batch: 100; loss: 1.53; acc: 0.64
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.55; acc: 0.56
Batch: 160; loss: 1.49; acc: 0.69
Batch: 180; loss: 1.74; acc: 0.45
Batch: 200; loss: 1.71; acc: 0.45
Batch: 220; loss: 1.59; acc: 0.53
Batch: 240; loss: 1.74; acc: 0.52
Batch: 260; loss: 1.47; acc: 0.67
Batch: 280; loss: 1.49; acc: 0.66
Batch: 300; loss: 1.54; acc: 0.61
Batch: 320; loss: 1.67; acc: 0.52
Batch: 340; loss: 1.58; acc: 0.5
Batch: 360; loss: 1.6; acc: 0.52
Batch: 380; loss: 1.7; acc: 0.48
Batch: 400; loss: 1.78; acc: 0.45
Batch: 420; loss: 1.65; acc: 0.45
Batch: 440; loss: 1.51; acc: 0.64
Batch: 460; loss: 1.62; acc: 0.53
Batch: 480; loss: 1.72; acc: 0.5
Batch: 500; loss: 1.69; acc: 0.5
Batch: 520; loss: 1.64; acc: 0.61
Batch: 540; loss: 1.51; acc: 0.58
Batch: 560; loss: 1.56; acc: 0.58
Batch: 580; loss: 1.75; acc: 0.45
Batch: 600; loss: 1.6; acc: 0.52
Batch: 620; loss: 1.76; acc: 0.48
Batch: 640; loss: 1.66; acc: 0.47
Batch: 660; loss: 1.68; acc: 0.47
Batch: 680; loss: 1.58; acc: 0.62
Batch: 700; loss: 1.69; acc: 0.53
Batch: 720; loss: 1.71; acc: 0.45
Batch: 740; loss: 1.64; acc: 0.56
Batch: 760; loss: 1.61; acc: 0.56
Batch: 780; loss: 1.73; acc: 0.47
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.5742847103392705e-05
9.77585841610562e-06
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.67; acc: 0.53
Batch: 40; loss: 1.38; acc: 0.67
Batch: 60; loss: 1.56; acc: 0.58
Batch: 80; loss: 1.71; acc: 0.5
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.55; acc: 0.61
Val Epoch over. val_loss: 1.6275261663327552; val_accuracy: 0.5450835987261147 

The current subspace-distance is: 9.77585841610562e-06 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.61
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.67; acc: 0.45
Batch: 60; loss: 1.7; acc: 0.41
Batch: 80; loss: 1.51; acc: 0.62
Batch: 100; loss: 1.73; acc: 0.52
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.53; acc: 0.64
Batch: 160; loss: 1.72; acc: 0.5
Batch: 180; loss: 1.75; acc: 0.5
Batch: 200; loss: 1.65; acc: 0.5
Batch: 220; loss: 1.63; acc: 0.55
Batch: 240; loss: 1.6; acc: 0.58
Batch: 260; loss: 1.61; acc: 0.55
Batch: 280; loss: 1.44; acc: 0.66
Batch: 300; loss: 1.64; acc: 0.53
Batch: 320; loss: 1.53; acc: 0.56
Batch: 340; loss: 1.52; acc: 0.56
Batch: 360; loss: 1.56; acc: 0.56
Batch: 380; loss: 1.69; acc: 0.53
Batch: 400; loss: 1.58; acc: 0.62
Batch: 420; loss: 1.59; acc: 0.58
Batch: 440; loss: 1.77; acc: 0.44
Batch: 460; loss: 1.68; acc: 0.52
Batch: 480; loss: 1.65; acc: 0.56
Batch: 500; loss: 1.61; acc: 0.48
Batch: 520; loss: 1.64; acc: 0.48
Batch: 540; loss: 1.55; acc: 0.61
Batch: 560; loss: 1.56; acc: 0.59
Batch: 580; loss: 1.59; acc: 0.5
Batch: 600; loss: 1.55; acc: 0.62
Batch: 620; loss: 1.71; acc: 0.48
Batch: 640; loss: 1.66; acc: 0.47
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.6; acc: 0.55
Batch: 700; loss: 1.58; acc: 0.55
Batch: 720; loss: 1.62; acc: 0.56
Batch: 740; loss: 1.61; acc: 0.52
Batch: 760; loss: 1.72; acc: 0.48
Batch: 780; loss: 1.73; acc: 0.45
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.641598596004769e-05
1.2783562851836905e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.67; acc: 0.45
Batch: 40; loss: 1.38; acc: 0.67
Batch: 60; loss: 1.57; acc: 0.56
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.68; acc: 0.53
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.53
Val Epoch over. val_loss: 1.6269067533456596; val_accuracy: 0.5458797770700637 

The current subspace-distance is: 1.2783562851836905e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.61; acc: 0.53
Batch: 40; loss: 1.64; acc: 0.52
Batch: 60; loss: 1.71; acc: 0.45
Batch: 80; loss: 1.63; acc: 0.53
Batch: 100; loss: 1.65; acc: 0.59
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.55; acc: 0.61
Batch: 160; loss: 1.77; acc: 0.44
Batch: 180; loss: 1.69; acc: 0.48
Batch: 200; loss: 1.66; acc: 0.45
Batch: 220; loss: 1.7; acc: 0.48
Batch: 240; loss: 1.58; acc: 0.53
Batch: 260; loss: 1.52; acc: 0.58
Batch: 280; loss: 1.55; acc: 0.61
Batch: 300; loss: 1.51; acc: 0.64
Batch: 320; loss: 1.68; acc: 0.55
Batch: 340; loss: 1.6; acc: 0.55
Batch: 360; loss: 1.73; acc: 0.53
Batch: 380; loss: 1.51; acc: 0.59
Batch: 400; loss: 1.5; acc: 0.64
Batch: 420; loss: 1.62; acc: 0.55
Batch: 440; loss: 1.58; acc: 0.61
Batch: 460; loss: 1.59; acc: 0.58
Batch: 480; loss: 1.69; acc: 0.5
Batch: 500; loss: 1.74; acc: 0.42
Batch: 520; loss: 1.75; acc: 0.47
Batch: 540; loss: 1.62; acc: 0.48
Batch: 560; loss: 1.68; acc: 0.44
Batch: 580; loss: 1.65; acc: 0.52
Batch: 600; loss: 1.5; acc: 0.59
Batch: 620; loss: 1.56; acc: 0.59
Batch: 640; loss: 1.63; acc: 0.55
Batch: 660; loss: 1.7; acc: 0.44
Batch: 680; loss: 1.59; acc: 0.61
Batch: 700; loss: 1.56; acc: 0.59
Batch: 720; loss: 1.6; acc: 0.58
Batch: 740; loss: 1.76; acc: 0.41
Batch: 760; loss: 1.7; acc: 0.52
Batch: 780; loss: 1.73; acc: 0.42
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.696510611916892e-05
1.3105808648106176e-05
Batch: 0; loss: 1.69; acc: 0.48
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.37; acc: 0.67
Batch: 60; loss: 1.57; acc: 0.58
Batch: 80; loss: 1.71; acc: 0.5
Batch: 100; loss: 1.67; acc: 0.56
Batch: 120; loss: 1.71; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.58
Val Epoch over. val_loss: 1.6272442743277094; val_accuracy: 0.5452826433121019 

The current subspace-distance is: 1.3105808648106176e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.71; acc: 0.55
Batch: 60; loss: 1.62; acc: 0.55
Batch: 80; loss: 1.57; acc: 0.64
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.59; acc: 0.56
Batch: 160; loss: 1.64; acc: 0.44
Batch: 180; loss: 1.82; acc: 0.45
Batch: 200; loss: 1.72; acc: 0.52
Batch: 220; loss: 1.7; acc: 0.52
Batch: 240; loss: 1.67; acc: 0.5
Batch: 260; loss: 1.69; acc: 0.45
Batch: 280; loss: 1.54; acc: 0.62
Batch: 300; loss: 1.82; acc: 0.36
Batch: 320; loss: 1.7; acc: 0.55
Batch: 340; loss: 1.63; acc: 0.53
Batch: 360; loss: 1.6; acc: 0.58
Batch: 380; loss: 1.63; acc: 0.53
Batch: 400; loss: 1.61; acc: 0.48
Batch: 420; loss: 1.67; acc: 0.59
Batch: 440; loss: 1.69; acc: 0.45
Batch: 460; loss: 1.82; acc: 0.41
Batch: 480; loss: 1.64; acc: 0.56
Batch: 500; loss: 1.54; acc: 0.58
Batch: 520; loss: 1.55; acc: 0.66
Batch: 540; loss: 1.57; acc: 0.55
Batch: 560; loss: 1.7; acc: 0.55
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.54; acc: 0.53
Batch: 620; loss: 1.66; acc: 0.44
Batch: 640; loss: 1.49; acc: 0.66
Batch: 660; loss: 1.61; acc: 0.55
Batch: 680; loss: 1.67; acc: 0.48
Batch: 700; loss: 1.61; acc: 0.58
Batch: 720; loss: 1.58; acc: 0.58
Batch: 740; loss: 1.69; acc: 0.56
Batch: 760; loss: 1.61; acc: 0.56
Batch: 780; loss: 1.8; acc: 0.41
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.738819214049727e-05
1.4141977771942038e-05
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.5
Batch: 40; loss: 1.39; acc: 0.67
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.53
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.74; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.58
Val Epoch over. val_loss: 1.6321112631232875; val_accuracy: 0.5392117834394905 

The current subspace-distance is: 1.4141977771942038e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_13_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6833983485794441

The number of parameters is: 259251

The number of individual parameters is:

14
252
14
14
21
38514
21
21
41
112791
41
41
64
102336
64
64
4096
64
640
10
64
64

nonzero elements in E: 25925097
elements in E: 25925100
fraction nonzero: 0.9999998842820278
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.34; acc: 0.14
Batch: 20; loss: 2.22; acc: 0.2
Batch: 40; loss: 2.03; acc: 0.34
Batch: 60; loss: 2.06; acc: 0.28
Batch: 80; loss: 1.93; acc: 0.39
Batch: 100; loss: 1.94; acc: 0.34
Batch: 120; loss: 1.91; acc: 0.34
Batch: 140; loss: 1.84; acc: 0.48
Batch: 160; loss: 1.78; acc: 0.56
Batch: 180; loss: 1.82; acc: 0.41
Batch: 200; loss: 1.7; acc: 0.59
Batch: 220; loss: 1.87; acc: 0.39
Batch: 240; loss: 1.83; acc: 0.45
Batch: 260; loss: 1.76; acc: 0.48
Batch: 280; loss: 1.68; acc: 0.53
Batch: 300; loss: 1.82; acc: 0.38
Batch: 320; loss: 1.78; acc: 0.53
Batch: 340; loss: 1.87; acc: 0.42
Batch: 360; loss: 1.67; acc: 0.48
Batch: 380; loss: 1.81; acc: 0.41
Batch: 400; loss: 1.78; acc: 0.42
Batch: 420; loss: 1.67; acc: 0.47
Batch: 440; loss: 1.79; acc: 0.39
Batch: 460; loss: 1.66; acc: 0.5
Batch: 480; loss: 1.66; acc: 0.52
Batch: 500; loss: 1.6; acc: 0.58
Batch: 520; loss: 1.7; acc: 0.52
Batch: 540; loss: 1.5; acc: 0.64
Batch: 560; loss: 1.68; acc: 0.55
Batch: 580; loss: 1.68; acc: 0.5
Batch: 600; loss: 1.67; acc: 0.48
Batch: 620; loss: 1.65; acc: 0.48
Batch: 640; loss: 1.68; acc: 0.53
Batch: 660; loss: 1.68; acc: 0.52
Batch: 680; loss: 1.65; acc: 0.52
Batch: 700; loss: 1.64; acc: 0.56
Batch: 720; loss: 1.59; acc: 0.56
Batch: 740; loss: 1.74; acc: 0.47
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.5; acc: 0.67
Train Epoch over. train_loss: 1.76; train_accuracy: 0.46 

4.663755316869356e-05
4.0937724406830966e-05
Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.75; acc: 0.41
Batch: 40; loss: 1.4; acc: 0.61
Batch: 60; loss: 1.53; acc: 0.58
Batch: 80; loss: 1.47; acc: 0.62
Batch: 100; loss: 1.59; acc: 0.53
Batch: 120; loss: 1.74; acc: 0.44
Batch: 140; loss: 1.49; acc: 0.59
Val Epoch over. val_loss: 1.587036174573716; val_accuracy: 0.5416998407643312 

The current subspace-distance is: 4.0937724406830966e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.62
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.64; acc: 0.5
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.6; acc: 0.53
Batch: 100; loss: 1.53; acc: 0.62
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.5; acc: 0.67
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.65; acc: 0.52
Batch: 200; loss: 1.58; acc: 0.52
Batch: 220; loss: 1.7; acc: 0.47
Batch: 240; loss: 1.58; acc: 0.62
Batch: 260; loss: 1.49; acc: 0.61
Batch: 280; loss: 1.53; acc: 0.55
Batch: 300; loss: 1.55; acc: 0.58
Batch: 320; loss: 1.71; acc: 0.47
Batch: 340; loss: 1.59; acc: 0.56
Batch: 360; loss: 1.53; acc: 0.62
Batch: 380; loss: 1.63; acc: 0.58
Batch: 400; loss: 1.54; acc: 0.62
Batch: 420; loss: 1.55; acc: 0.61
Batch: 440; loss: 1.52; acc: 0.62
Batch: 460; loss: 1.62; acc: 0.58
Batch: 480; loss: 1.53; acc: 0.52
Batch: 500; loss: 1.49; acc: 0.55
Batch: 520; loss: 1.68; acc: 0.45
Batch: 540; loss: 1.56; acc: 0.62
Batch: 560; loss: 1.55; acc: 0.61
Batch: 580; loss: 1.47; acc: 0.66
Batch: 600; loss: 1.58; acc: 0.48
Batch: 620; loss: 1.62; acc: 0.52
Batch: 640; loss: 1.59; acc: 0.53
Batch: 660; loss: 1.49; acc: 0.61
Batch: 680; loss: 1.5; acc: 0.67
Batch: 700; loss: 1.5; acc: 0.58
Batch: 720; loss: 1.62; acc: 0.5
Batch: 740; loss: 1.5; acc: 0.62
Batch: 760; loss: 1.53; acc: 0.61
Batch: 780; loss: 1.49; acc: 0.67
Train Epoch over. train_loss: 1.57; train_accuracy: 0.58 

5.952401988906786e-05
5.3436877351487055e-05
Batch: 0; loss: 1.58; acc: 0.56
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.3; acc: 0.73
Batch: 60; loss: 1.42; acc: 0.66
Batch: 80; loss: 1.4; acc: 0.66
Batch: 100; loss: 1.52; acc: 0.56
Batch: 120; loss: 1.7; acc: 0.52
Batch: 140; loss: 1.41; acc: 0.56
Val Epoch over. val_loss: 1.5018014809128586; val_accuracy: 0.6148487261146497 

The current subspace-distance is: 5.3436877351487055e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.62
Batch: 20; loss: 1.57; acc: 0.55
Batch: 40; loss: 1.52; acc: 0.61
Batch: 60; loss: 1.46; acc: 0.69
Batch: 80; loss: 1.52; acc: 0.62
Batch: 100; loss: 1.61; acc: 0.53
Batch: 120; loss: 1.56; acc: 0.59
Batch: 140; loss: 1.65; acc: 0.45
Batch: 160; loss: 1.55; acc: 0.59
Batch: 180; loss: 1.52; acc: 0.55
Batch: 200; loss: 1.53; acc: 0.61
Batch: 220; loss: 1.51; acc: 0.59
Batch: 240; loss: 1.4; acc: 0.66
Batch: 260; loss: 1.65; acc: 0.53
Batch: 280; loss: 1.56; acc: 0.53
Batch: 300; loss: 1.5; acc: 0.59
Batch: 320; loss: 1.49; acc: 0.59
Batch: 340; loss: 1.5; acc: 0.59
Batch: 360; loss: 1.56; acc: 0.55
Batch: 380; loss: 1.46; acc: 0.61
Batch: 400; loss: 1.49; acc: 0.58
Batch: 420; loss: 1.52; acc: 0.58
Batch: 440; loss: 1.43; acc: 0.67
Batch: 460; loss: 1.46; acc: 0.64
Batch: 480; loss: 1.57; acc: 0.45
Batch: 500; loss: 1.57; acc: 0.61
Batch: 520; loss: 1.52; acc: 0.64
Batch: 540; loss: 1.54; acc: 0.58
Batch: 560; loss: 1.54; acc: 0.53
Batch: 580; loss: 1.44; acc: 0.67
Batch: 600; loss: 1.55; acc: 0.64
Batch: 620; loss: 1.44; acc: 0.62
Batch: 640; loss: 1.49; acc: 0.56
Batch: 660; loss: 1.53; acc: 0.59
Batch: 680; loss: 1.4; acc: 0.75
Batch: 700; loss: 1.38; acc: 0.66
Batch: 720; loss: 1.49; acc: 0.62
Batch: 740; loss: 1.59; acc: 0.64
Batch: 760; loss: 1.63; acc: 0.48
Batch: 780; loss: 1.48; acc: 0.62
Train Epoch over. train_loss: 1.51; train_accuracy: 0.61 

6.856840627733618e-05
6.333642522804439e-05
Batch: 0; loss: 1.56; acc: 0.62
Batch: 20; loss: 1.55; acc: 0.64
Batch: 40; loss: 1.23; acc: 0.8
Batch: 60; loss: 1.37; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.69
Batch: 100; loss: 1.45; acc: 0.58
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.69
Val Epoch over. val_loss: 1.4489859676664802; val_accuracy: 0.6426154458598726 

The current subspace-distance is: 6.333642522804439e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.66
Batch: 20; loss: 1.52; acc: 0.56
Batch: 40; loss: 1.52; acc: 0.66
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.56; acc: 0.56
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.42; acc: 0.67
Batch: 140; loss: 1.43; acc: 0.72
Batch: 160; loss: 1.59; acc: 0.59
Batch: 180; loss: 1.42; acc: 0.59
Batch: 200; loss: 1.4; acc: 0.72
Batch: 220; loss: 1.58; acc: 0.58
Batch: 240; loss: 1.45; acc: 0.62
Batch: 260; loss: 1.5; acc: 0.56
Batch: 280; loss: 1.37; acc: 0.69
Batch: 300; loss: 1.52; acc: 0.62
Batch: 320; loss: 1.41; acc: 0.67
Batch: 340; loss: 1.37; acc: 0.66
Batch: 360; loss: 1.41; acc: 0.64
Batch: 380; loss: 1.54; acc: 0.67
Batch: 400; loss: 1.59; acc: 0.58
Batch: 420; loss: 1.49; acc: 0.59
Batch: 440; loss: 1.52; acc: 0.56
Batch: 460; loss: 1.56; acc: 0.52
Batch: 480; loss: 1.47; acc: 0.61
Batch: 500; loss: 1.46; acc: 0.61
Batch: 520; loss: 1.46; acc: 0.72
Batch: 540; loss: 1.59; acc: 0.59
Batch: 560; loss: 1.59; acc: 0.53
Batch: 580; loss: 1.64; acc: 0.59
Batch: 600; loss: 1.51; acc: 0.55
Batch: 620; loss: 1.45; acc: 0.66
Batch: 640; loss: 1.53; acc: 0.5
Batch: 660; loss: 1.55; acc: 0.56
Batch: 680; loss: 1.42; acc: 0.67
Batch: 700; loss: 1.39; acc: 0.7
Batch: 720; loss: 1.35; acc: 0.72
Batch: 740; loss: 1.33; acc: 0.69
Batch: 760; loss: 1.46; acc: 0.61
Batch: 780; loss: 1.36; acc: 0.69
Train Epoch over. train_loss: 1.47; train_accuracy: 0.63 

7.758010906400159e-05
7.124929834390059e-05
Batch: 0; loss: 1.55; acc: 0.61
Batch: 20; loss: 1.53; acc: 0.66
Batch: 40; loss: 1.17; acc: 0.81
Batch: 60; loss: 1.34; acc: 0.78
Batch: 80; loss: 1.27; acc: 0.77
Batch: 100; loss: 1.41; acc: 0.69
Batch: 120; loss: 1.6; acc: 0.55
Batch: 140; loss: 1.3; acc: 0.81
Val Epoch over. val_loss: 1.4136445636202575; val_accuracy: 0.6731687898089171 

The current subspace-distance is: 7.124929834390059e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.67
Batch: 40; loss: 1.44; acc: 0.61
Batch: 60; loss: 1.45; acc: 0.64
Batch: 80; loss: 1.5; acc: 0.58
Batch: 100; loss: 1.54; acc: 0.59
Batch: 120; loss: 1.33; acc: 0.7
Batch: 140; loss: 1.48; acc: 0.62
Batch: 160; loss: 1.46; acc: 0.61
Batch: 180; loss: 1.5; acc: 0.58
Batch: 200; loss: 1.41; acc: 0.67
Batch: 220; loss: 1.46; acc: 0.58
Batch: 240; loss: 1.4; acc: 0.64
Batch: 260; loss: 1.39; acc: 0.7
Batch: 280; loss: 1.52; acc: 0.66
Batch: 300; loss: 1.59; acc: 0.45
Batch: 320; loss: 1.41; acc: 0.62
Batch: 340; loss: 1.32; acc: 0.75
Batch: 360; loss: 1.41; acc: 0.69
Batch: 380; loss: 1.48; acc: 0.67
Batch: 400; loss: 1.4; acc: 0.7
Batch: 420; loss: 1.25; acc: 0.78
Batch: 440; loss: 1.54; acc: 0.55
Batch: 460; loss: 1.44; acc: 0.59
Batch: 480; loss: 1.44; acc: 0.72
Batch: 500; loss: 1.54; acc: 0.61
Batch: 520; loss: 1.36; acc: 0.67
Batch: 540; loss: 1.48; acc: 0.64
Batch: 560; loss: 1.28; acc: 0.69
Batch: 580; loss: 1.43; acc: 0.62
Batch: 600; loss: 1.55; acc: 0.59
Batch: 620; loss: 1.41; acc: 0.55
Batch: 640; loss: 1.44; acc: 0.61
Batch: 660; loss: 1.42; acc: 0.7
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.42; acc: 0.62
Batch: 720; loss: 1.4; acc: 0.69
Batch: 740; loss: 1.25; acc: 0.73
Batch: 760; loss: 1.47; acc: 0.62
Batch: 780; loss: 1.23; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.64 

8.704042556928471e-05
8.089132461464033e-05
Batch: 0; loss: 1.55; acc: 0.52
Batch: 20; loss: 1.5; acc: 0.64
Batch: 40; loss: 1.11; acc: 0.86
Batch: 60; loss: 1.3; acc: 0.75
Batch: 80; loss: 1.22; acc: 0.78
Batch: 100; loss: 1.38; acc: 0.67
Batch: 120; loss: 1.55; acc: 0.56
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3735269596622248; val_accuracy: 0.6826234076433121 

The current subspace-distance is: 8.089132461464033e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.7
Batch: 20; loss: 1.37; acc: 0.72
Batch: 40; loss: 1.44; acc: 0.62
Batch: 60; loss: 1.37; acc: 0.62
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.32; acc: 0.67
Batch: 120; loss: 1.41; acc: 0.62
Batch: 140; loss: 1.42; acc: 0.62
Batch: 160; loss: 1.32; acc: 0.67
Batch: 180; loss: 1.33; acc: 0.69
Batch: 200; loss: 1.49; acc: 0.52
Batch: 220; loss: 1.35; acc: 0.72
Batch: 240; loss: 1.37; acc: 0.73
Batch: 260; loss: 1.47; acc: 0.56
Batch: 280; loss: 1.39; acc: 0.62
Batch: 300; loss: 1.41; acc: 0.64
Batch: 320; loss: 1.39; acc: 0.64
Batch: 340; loss: 1.36; acc: 0.7
Batch: 360; loss: 1.35; acc: 0.64
Batch: 380; loss: 1.39; acc: 0.62
Batch: 400; loss: 1.22; acc: 0.81
Batch: 420; loss: 1.36; acc: 0.64
Batch: 440; loss: 1.36; acc: 0.67
Batch: 460; loss: 1.45; acc: 0.58
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.31; acc: 0.78
Batch: 520; loss: 1.3; acc: 0.75
Batch: 540; loss: 1.37; acc: 0.66
Batch: 560; loss: 1.24; acc: 0.77
Batch: 580; loss: 1.35; acc: 0.69
Batch: 600; loss: 1.37; acc: 0.62
Batch: 620; loss: 1.37; acc: 0.66
Batch: 640; loss: 1.33; acc: 0.66
Batch: 660; loss: 1.4; acc: 0.67
Batch: 680; loss: 1.25; acc: 0.72
Batch: 700; loss: 1.36; acc: 0.64
Batch: 720; loss: 1.32; acc: 0.67
Batch: 740; loss: 1.36; acc: 0.7
Batch: 760; loss: 1.36; acc: 0.7
Batch: 780; loss: 1.43; acc: 0.62
Train Epoch over. train_loss: 1.38; train_accuracy: 0.66 

9.752847108757123e-05
9.15301643544808e-05
Batch: 0; loss: 1.51; acc: 0.56
Batch: 20; loss: 1.44; acc: 0.64
Batch: 40; loss: 1.03; acc: 0.84
Batch: 60; loss: 1.24; acc: 0.73
Batch: 80; loss: 1.16; acc: 0.81
Batch: 100; loss: 1.33; acc: 0.69
Batch: 120; loss: 1.52; acc: 0.55
Batch: 140; loss: 1.19; acc: 0.83
Val Epoch over. val_loss: 1.3227338168271787; val_accuracy: 0.689390923566879 

The current subspace-distance is: 9.15301643544808e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.64
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.34; acc: 0.64
Batch: 80; loss: 1.37; acc: 0.62
Batch: 100; loss: 1.34; acc: 0.7
Batch: 120; loss: 1.36; acc: 0.67
Batch: 140; loss: 1.38; acc: 0.72
Batch: 160; loss: 1.35; acc: 0.59
Batch: 180; loss: 1.27; acc: 0.73
Batch: 200; loss: 1.27; acc: 0.7
Batch: 220; loss: 1.4; acc: 0.61
Batch: 240; loss: 1.33; acc: 0.72
Batch: 260; loss: 1.39; acc: 0.62
Batch: 280; loss: 1.47; acc: 0.56
Batch: 300; loss: 1.32; acc: 0.66
Batch: 320; loss: 1.32; acc: 0.7
Batch: 340; loss: 1.38; acc: 0.7
Batch: 360; loss: 1.43; acc: 0.64
Batch: 380; loss: 1.39; acc: 0.64
Batch: 400; loss: 1.27; acc: 0.73
Batch: 420; loss: 1.31; acc: 0.67
Batch: 440; loss: 1.25; acc: 0.73
Batch: 460; loss: 1.41; acc: 0.66
Batch: 480; loss: 1.35; acc: 0.66
Batch: 500; loss: 1.39; acc: 0.61
Batch: 520; loss: 1.28; acc: 0.73
Batch: 540; loss: 1.28; acc: 0.7
Batch: 560; loss: 1.29; acc: 0.73
Batch: 580; loss: 1.38; acc: 0.72
Batch: 600; loss: 1.3; acc: 0.69
Batch: 620; loss: 1.42; acc: 0.67
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.29; acc: 0.75
Batch: 680; loss: 1.22; acc: 0.75
Batch: 700; loss: 1.24; acc: 0.72
Batch: 720; loss: 1.4; acc: 0.58
Batch: 740; loss: 1.36; acc: 0.62
Batch: 760; loss: 1.32; acc: 0.61
Batch: 780; loss: 1.27; acc: 0.77
Train Epoch over. train_loss: 1.35; train_accuracy: 0.66 

0.00010886080417549238
0.00010142012615688145
Batch: 0; loss: 1.49; acc: 0.58
Batch: 20; loss: 1.42; acc: 0.62
Batch: 40; loss: 1.01; acc: 0.86
Batch: 60; loss: 1.22; acc: 0.73
Batch: 80; loss: 1.13; acc: 0.81
Batch: 100; loss: 1.32; acc: 0.7
Batch: 120; loss: 1.5; acc: 0.59
Batch: 140; loss: 1.17; acc: 0.81
Val Epoch over. val_loss: 1.3017640637744003; val_accuracy: 0.6979498407643312 

The current subspace-distance is: 0.00010142012615688145 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.67
Batch: 20; loss: 1.33; acc: 0.72
Batch: 40; loss: 1.24; acc: 0.72
Batch: 60; loss: 1.41; acc: 0.67
Batch: 80; loss: 1.57; acc: 0.55
Batch: 100; loss: 1.43; acc: 0.59
Batch: 120; loss: 1.25; acc: 0.7
Batch: 140; loss: 1.29; acc: 0.69
Batch: 160; loss: 1.26; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.67
Batch: 200; loss: 1.21; acc: 0.72
Batch: 220; loss: 1.32; acc: 0.7
Batch: 240; loss: 1.23; acc: 0.7
Batch: 260; loss: 1.37; acc: 0.61
Batch: 280; loss: 1.4; acc: 0.59
Batch: 300; loss: 1.17; acc: 0.75
Batch: 320; loss: 1.38; acc: 0.66
Batch: 340; loss: 1.37; acc: 0.72
Batch: 360; loss: 1.31; acc: 0.64
Batch: 380; loss: 1.39; acc: 0.61
Batch: 400; loss: 1.21; acc: 0.77
Batch: 420; loss: 1.24; acc: 0.62
Batch: 440; loss: 1.36; acc: 0.64
Batch: 460; loss: 1.24; acc: 0.72
Batch: 480; loss: 1.33; acc: 0.59
Batch: 500; loss: 1.2; acc: 0.69
Batch: 520; loss: 1.39; acc: 0.72
Batch: 540; loss: 1.25; acc: 0.72
Batch: 560; loss: 1.46; acc: 0.56
Batch: 580; loss: 1.09; acc: 0.78
Batch: 600; loss: 1.25; acc: 0.73
Batch: 620; loss: 1.24; acc: 0.73
Batch: 640; loss: 1.18; acc: 0.75
Batch: 660; loss: 1.29; acc: 0.7
Batch: 680; loss: 1.36; acc: 0.64
Batch: 700; loss: 1.33; acc: 0.7
Batch: 720; loss: 1.42; acc: 0.59
Batch: 740; loss: 1.36; acc: 0.62
Batch: 760; loss: 1.34; acc: 0.66
Batch: 780; loss: 1.42; acc: 0.56
Train Epoch over. train_loss: 1.32; train_accuracy: 0.66 

0.00011355625611031428
0.00010801666940096766
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.42; acc: 0.66
Batch: 40; loss: 0.97; acc: 0.89
Batch: 60; loss: 1.18; acc: 0.77
Batch: 80; loss: 1.11; acc: 0.78
Batch: 100; loss: 1.31; acc: 0.69
Batch: 120; loss: 1.5; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.84
Val Epoch over. val_loss: 1.2789866316850018; val_accuracy: 0.6949641719745223 

The current subspace-distance is: 0.00010801666940096766 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.66
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 1.34; acc: 0.62
Batch: 60; loss: 1.34; acc: 0.66
Batch: 80; loss: 1.31; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.66
Batch: 120; loss: 1.34; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.56
Batch: 160; loss: 1.38; acc: 0.59
Batch: 180; loss: 1.32; acc: 0.67
Batch: 200; loss: 1.38; acc: 0.61
Batch: 220; loss: 1.39; acc: 0.61
Batch: 240; loss: 1.34; acc: 0.66
Batch: 260; loss: 1.37; acc: 0.62
Batch: 280; loss: 1.37; acc: 0.66
Batch: 300; loss: 1.3; acc: 0.67
Batch: 320; loss: 1.31; acc: 0.67
Batch: 340; loss: 1.34; acc: 0.67
Batch: 360; loss: 1.24; acc: 0.66
Batch: 380; loss: 1.31; acc: 0.66
Batch: 400; loss: 1.4; acc: 0.61
Batch: 420; loss: 1.28; acc: 0.72
Batch: 440; loss: 1.4; acc: 0.56
Batch: 460; loss: 1.37; acc: 0.69
Batch: 480; loss: 1.41; acc: 0.69
Batch: 500; loss: 1.25; acc: 0.72
Batch: 520; loss: 1.31; acc: 0.61
Batch: 540; loss: 1.28; acc: 0.64
Batch: 560; loss: 1.37; acc: 0.64
Batch: 580; loss: 1.21; acc: 0.72
Batch: 600; loss: 1.33; acc: 0.66
Batch: 620; loss: 1.32; acc: 0.75
Batch: 640; loss: 1.16; acc: 0.75
Batch: 660; loss: 1.29; acc: 0.67
Batch: 680; loss: 1.41; acc: 0.59
Batch: 700; loss: 1.27; acc: 0.66
Batch: 720; loss: 1.16; acc: 0.81
Batch: 740; loss: 1.25; acc: 0.67
Batch: 760; loss: 1.12; acc: 0.75
Batch: 780; loss: 1.21; acc: 0.73
Train Epoch over. train_loss: 1.31; train_accuracy: 0.67 

0.00011957425158470869
0.00011472817277535796
Batch: 0; loss: 1.46; acc: 0.62
Batch: 20; loss: 1.39; acc: 0.64
Batch: 40; loss: 0.97; acc: 0.86
Batch: 60; loss: 1.19; acc: 0.75
Batch: 80; loss: 1.09; acc: 0.8
Batch: 100; loss: 1.32; acc: 0.69
Batch: 120; loss: 1.51; acc: 0.59
Batch: 140; loss: 1.13; acc: 0.83
Val Epoch over. val_loss: 1.2717926900857572; val_accuracy: 0.701234076433121 

The current subspace-distance is: 0.00011472817277535796 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.72
Batch: 20; loss: 1.34; acc: 0.69
Batch: 40; loss: 1.37; acc: 0.58
Batch: 60; loss: 1.47; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.72
Batch: 100; loss: 1.41; acc: 0.67
Batch: 120; loss: 1.26; acc: 0.72
Batch: 140; loss: 1.2; acc: 0.81
Batch: 160; loss: 1.31; acc: 0.75
Batch: 180; loss: 1.39; acc: 0.67
Batch: 200; loss: 1.17; acc: 0.75
Batch: 220; loss: 1.34; acc: 0.66
Batch: 240; loss: 1.32; acc: 0.62
Batch: 260; loss: 1.26; acc: 0.75
Batch: 280; loss: 1.32; acc: 0.66
Batch: 300; loss: 1.28; acc: 0.67
Batch: 320; loss: 1.53; acc: 0.52
Batch: 340; loss: 1.37; acc: 0.59
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.24; acc: 0.67
Batch: 400; loss: 1.18; acc: 0.72
Batch: 420; loss: 1.33; acc: 0.67
Batch: 440; loss: 1.26; acc: 0.72
Batch: 460; loss: 1.28; acc: 0.69
Batch: 480; loss: 1.53; acc: 0.61
Batch: 500; loss: 1.21; acc: 0.72
Batch: 520; loss: 1.43; acc: 0.61
Batch: 540; loss: 1.17; acc: 0.7
Batch: 560; loss: 1.33; acc: 0.59
Batch: 580; loss: 1.32; acc: 0.66
Batch: 600; loss: 1.19; acc: 0.75
Batch: 620; loss: 1.24; acc: 0.72
Batch: 640; loss: 1.54; acc: 0.5
Batch: 660; loss: 1.4; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.62
Batch: 700; loss: 1.35; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.75
Batch: 740; loss: 1.4; acc: 0.64
Batch: 760; loss: 1.16; acc: 0.78
Batch: 780; loss: 1.32; acc: 0.61
Train Epoch over. train_loss: 1.29; train_accuracy: 0.67 

0.0001280061260331422
0.00012331610196270049
Batch: 0; loss: 1.44; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.69
Batch: 40; loss: 0.94; acc: 0.88
Batch: 60; loss: 1.16; acc: 0.8
Batch: 80; loss: 1.06; acc: 0.78
Batch: 100; loss: 1.31; acc: 0.72
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.08; acc: 0.84
Val Epoch over. val_loss: 1.2463785322608463; val_accuracy: 0.7101910828025477 

The current subspace-distance is: 0.00012331610196270049 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.28; acc: 0.67
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 1.28; acc: 0.59
Batch: 60; loss: 1.14; acc: 0.75
Batch: 80; loss: 1.33; acc: 0.59
Batch: 100; loss: 1.13; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.62
Batch: 140; loss: 1.37; acc: 0.64
Batch: 160; loss: 1.29; acc: 0.72
Batch: 180; loss: 1.21; acc: 0.69
Batch: 200; loss: 1.35; acc: 0.67
Batch: 220; loss: 1.22; acc: 0.67
Batch: 240; loss: 1.35; acc: 0.67
Batch: 260; loss: 1.18; acc: 0.67
Batch: 280; loss: 1.34; acc: 0.66
Batch: 300; loss: 1.27; acc: 0.62
Batch: 320; loss: 1.29; acc: 0.67
Batch: 340; loss: 1.19; acc: 0.69
Batch: 360; loss: 1.21; acc: 0.75
Batch: 380; loss: 1.39; acc: 0.62
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 1.31; acc: 0.64
Batch: 440; loss: 1.22; acc: 0.77
Batch: 460; loss: 1.23; acc: 0.67
Batch: 480; loss: 1.32; acc: 0.62
Batch: 500; loss: 1.3; acc: 0.67
Batch: 520; loss: 1.23; acc: 0.7
Batch: 540; loss: 1.33; acc: 0.62
Batch: 560; loss: 1.25; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.72
Batch: 600; loss: 1.18; acc: 0.73
Batch: 620; loss: 1.39; acc: 0.52
Batch: 640; loss: 1.27; acc: 0.73
Batch: 660; loss: 1.36; acc: 0.64
Batch: 680; loss: 1.42; acc: 0.59
Batch: 700; loss: 1.24; acc: 0.69
Batch: 720; loss: 1.29; acc: 0.61
Batch: 740; loss: 1.29; acc: 0.67
Batch: 760; loss: 1.23; acc: 0.72
Batch: 780; loss: 1.3; acc: 0.67
Train Epoch over. train_loss: 1.28; train_accuracy: 0.67 

0.00012904360482934862
0.00012254720786586404
Batch: 0; loss: 1.43; acc: 0.62
Batch: 20; loss: 1.37; acc: 0.67
Batch: 40; loss: 0.93; acc: 0.89
Batch: 60; loss: 1.17; acc: 0.77
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 1.3; acc: 0.72
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.08; acc: 0.88
Val Epoch over. val_loss: 1.2402285732281435; val_accuracy: 0.713077229299363 

The current subspace-distance is: 0.00012254720786586404 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.28; acc: 0.72
Batch: 40; loss: 1.42; acc: 0.59
Batch: 60; loss: 1.23; acc: 0.7
Batch: 80; loss: 1.18; acc: 0.72
Batch: 100; loss: 1.38; acc: 0.66
Batch: 120; loss: 1.23; acc: 0.75
Batch: 140; loss: 1.36; acc: 0.61
Batch: 160; loss: 1.22; acc: 0.73
Batch: 180; loss: 1.3; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.67
Batch: 220; loss: 1.27; acc: 0.62
Batch: 240; loss: 1.2; acc: 0.73
Batch: 260; loss: 1.3; acc: 0.66
Batch: 280; loss: 1.2; acc: 0.78
Batch: 300; loss: 1.31; acc: 0.66
Batch: 320; loss: 1.4; acc: 0.58
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.19; acc: 0.72
Batch: 380; loss: 1.31; acc: 0.69
Batch: 400; loss: 1.26; acc: 0.67
Batch: 420; loss: 1.15; acc: 0.77
Batch: 440; loss: 1.28; acc: 0.66
Batch: 460; loss: 1.39; acc: 0.67
Batch: 480; loss: 1.23; acc: 0.66
Batch: 500; loss: 1.4; acc: 0.66
Batch: 520; loss: 1.18; acc: 0.73
Batch: 540; loss: 1.25; acc: 0.67
Batch: 560; loss: 1.3; acc: 0.59
Batch: 580; loss: 1.22; acc: 0.75
Batch: 600; loss: 1.23; acc: 0.73
Batch: 620; loss: 1.29; acc: 0.66
Batch: 640; loss: 1.24; acc: 0.69
Batch: 660; loss: 1.26; acc: 0.66
Batch: 680; loss: 1.2; acc: 0.73
Batch: 700; loss: 1.25; acc: 0.7
Batch: 720; loss: 1.27; acc: 0.75
Batch: 740; loss: 1.13; acc: 0.78
Batch: 760; loss: 1.28; acc: 0.69
Batch: 780; loss: 1.3; acc: 0.69
Train Epoch over. train_loss: 1.28; train_accuracy: 0.67 

0.00013165581913199276
0.00012509214866440743
Batch: 0; loss: 1.42; acc: 0.61
Batch: 20; loss: 1.35; acc: 0.66
Batch: 40; loss: 0.92; acc: 0.88
Batch: 60; loss: 1.16; acc: 0.77
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 1.29; acc: 0.72
Batch: 120; loss: 1.48; acc: 0.52
Batch: 140; loss: 1.06; acc: 0.91
Val Epoch over. val_loss: 1.2298024319539405; val_accuracy: 0.714171974522293 

The current subspace-distance is: 0.00012509214866440743 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.7
Batch: 20; loss: 1.27; acc: 0.7
Batch: 40; loss: 1.18; acc: 0.7
Batch: 60; loss: 1.25; acc: 0.7
Batch: 80; loss: 1.31; acc: 0.62
Batch: 100; loss: 1.23; acc: 0.69
Batch: 120; loss: 1.35; acc: 0.62
Batch: 140; loss: 1.36; acc: 0.66
Batch: 160; loss: 1.38; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.62
Batch: 200; loss: 1.39; acc: 0.69
Batch: 220; loss: 1.4; acc: 0.61
Batch: 240; loss: 1.36; acc: 0.62
Batch: 260; loss: 1.27; acc: 0.64
Batch: 280; loss: 1.3; acc: 0.66
Batch: 300; loss: 1.12; acc: 0.69
Batch: 320; loss: 1.37; acc: 0.62
Batch: 340; loss: 1.21; acc: 0.64
Batch: 360; loss: 1.24; acc: 0.7
Batch: 380; loss: 1.18; acc: 0.77
Batch: 400; loss: 1.37; acc: 0.61
Batch: 420; loss: 1.09; acc: 0.78
Batch: 440; loss: 1.17; acc: 0.72
Batch: 460; loss: 1.21; acc: 0.73
Batch: 480; loss: 1.25; acc: 0.7
Batch: 500; loss: 1.35; acc: 0.59
Batch: 520; loss: 1.31; acc: 0.64
Batch: 540; loss: 1.28; acc: 0.72
Batch: 560; loss: 1.34; acc: 0.67
Batch: 580; loss: 1.25; acc: 0.69
Batch: 600; loss: 1.2; acc: 0.67
Batch: 620; loss: 1.23; acc: 0.67
Batch: 640; loss: 1.34; acc: 0.59
Batch: 660; loss: 1.26; acc: 0.66
Batch: 680; loss: 1.4; acc: 0.64
Batch: 700; loss: 1.32; acc: 0.62
Batch: 720; loss: 1.26; acc: 0.62
Batch: 740; loss: 1.36; acc: 0.58
Batch: 760; loss: 1.19; acc: 0.7
Batch: 780; loss: 1.18; acc: 0.7
Train Epoch over. train_loss: 1.27; train_accuracy: 0.67 

0.00013216270599514246
0.00012600049376487732
Batch: 0; loss: 1.42; acc: 0.58
Batch: 20; loss: 1.35; acc: 0.64
Batch: 40; loss: 0.91; acc: 0.89
Batch: 60; loss: 1.15; acc: 0.78
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 1.3; acc: 0.73
Batch: 120; loss: 1.48; acc: 0.53
Batch: 140; loss: 1.06; acc: 0.86
Val Epoch over. val_loss: 1.2235098158477977; val_accuracy: 0.7109872611464968 

The current subspace-distance is: 0.00012600049376487732 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.28; acc: 0.61
Batch: 20; loss: 1.24; acc: 0.64
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.45; acc: 0.5
Batch: 80; loss: 1.16; acc: 0.75
Batch: 100; loss: 1.24; acc: 0.69
Batch: 120; loss: 1.25; acc: 0.72
Batch: 140; loss: 1.19; acc: 0.67
Batch: 160; loss: 1.12; acc: 0.75
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.33; acc: 0.64
Batch: 220; loss: 1.19; acc: 0.73
Batch: 240; loss: 1.25; acc: 0.7
Batch: 260; loss: 1.36; acc: 0.64
Batch: 280; loss: 1.16; acc: 0.72
Batch: 300; loss: 1.22; acc: 0.72
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.41; acc: 0.62
Batch: 360; loss: 1.13; acc: 0.77
Batch: 380; loss: 1.25; acc: 0.72
Batch: 400; loss: 1.21; acc: 0.75
Batch: 420; loss: 1.3; acc: 0.64
Batch: 440; loss: 1.36; acc: 0.73
Batch: 460; loss: 1.46; acc: 0.55
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 1.25; acc: 0.62
Batch: 520; loss: 1.24; acc: 0.66
Batch: 540; loss: 1.2; acc: 0.72
Batch: 560; loss: 1.23; acc: 0.66
Batch: 580; loss: 1.32; acc: 0.61
Batch: 600; loss: 1.13; acc: 0.77
Batch: 620; loss: 1.21; acc: 0.7
Batch: 640; loss: 1.27; acc: 0.67
Batch: 660; loss: 1.33; acc: 0.58
Batch: 680; loss: 1.25; acc: 0.67
Batch: 700; loss: 1.2; acc: 0.7
Batch: 720; loss: 1.26; acc: 0.72
Batch: 740; loss: 1.19; acc: 0.75
Batch: 760; loss: 1.29; acc: 0.66
Batch: 780; loss: 1.27; acc: 0.66
Train Epoch over. train_loss: 1.26; train_accuracy: 0.68 

0.00013606183347292244
0.00013121972733642906
Batch: 0; loss: 1.41; acc: 0.64
Batch: 20; loss: 1.34; acc: 0.69
Batch: 40; loss: 0.91; acc: 0.88
Batch: 60; loss: 1.15; acc: 0.78
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.29; acc: 0.72
Batch: 120; loss: 1.47; acc: 0.56
Batch: 140; loss: 1.05; acc: 0.84
Val Epoch over. val_loss: 1.2217799626338255; val_accuracy: 0.7163614649681529 

The current subspace-distance is: 0.00013121972733642906 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.8
Batch: 20; loss: 1.17; acc: 0.72
Batch: 40; loss: 1.32; acc: 0.62
Batch: 60; loss: 1.27; acc: 0.69
Batch: 80; loss: 1.27; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.7
Batch: 120; loss: 1.22; acc: 0.7
Batch: 140; loss: 1.36; acc: 0.64
Batch: 160; loss: 1.2; acc: 0.7
Batch: 180; loss: 1.32; acc: 0.67
Batch: 200; loss: 1.18; acc: 0.73
Batch: 220; loss: 1.34; acc: 0.61
Batch: 240; loss: 1.29; acc: 0.64
Batch: 260; loss: 1.17; acc: 0.72
Batch: 280; loss: 1.21; acc: 0.69
Batch: 300; loss: 1.2; acc: 0.7
Batch: 320; loss: 1.33; acc: 0.62
Batch: 340; loss: 1.15; acc: 0.73
Batch: 360; loss: 1.16; acc: 0.73
Batch: 380; loss: 1.23; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.69
Batch: 420; loss: 1.26; acc: 0.61
Batch: 440; loss: 1.24; acc: 0.73
Batch: 460; loss: 1.28; acc: 0.64
Batch: 480; loss: 1.22; acc: 0.69
Batch: 500; loss: 1.26; acc: 0.66
Batch: 520; loss: 1.16; acc: 0.8
Batch: 540; loss: 1.32; acc: 0.62
Batch: 560; loss: 1.36; acc: 0.62
Batch: 580; loss: 1.3; acc: 0.67
Batch: 600; loss: 1.18; acc: 0.73
Batch: 620; loss: 1.27; acc: 0.66
Batch: 640; loss: 1.32; acc: 0.67
Batch: 660; loss: 1.42; acc: 0.52
Batch: 680; loss: 1.43; acc: 0.56
Batch: 700; loss: 1.1; acc: 0.77
Batch: 720; loss: 1.34; acc: 0.7
Batch: 740; loss: 1.32; acc: 0.64
Batch: 760; loss: 1.28; acc: 0.66
Batch: 780; loss: 1.42; acc: 0.59
Train Epoch over. train_loss: 1.26; train_accuracy: 0.68 

0.00013857678277418017
0.00013284651504363865
Batch: 0; loss: 1.4; acc: 0.64
Batch: 20; loss: 1.32; acc: 0.69
Batch: 40; loss: 0.9; acc: 0.88
Batch: 60; loss: 1.14; acc: 0.78
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.72
Batch: 120; loss: 1.47; acc: 0.59
Batch: 140; loss: 1.03; acc: 0.83
Val Epoch over. val_loss: 1.2108374302554283; val_accuracy: 0.7144705414012739 

The current subspace-distance is: 0.00013284651504363865 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.29; acc: 0.7
Batch: 20; loss: 1.28; acc: 0.64
Batch: 40; loss: 1.4; acc: 0.59
Batch: 60; loss: 1.11; acc: 0.78
Batch: 80; loss: 1.09; acc: 0.8
Batch: 100; loss: 1.4; acc: 0.61
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 1.19; acc: 0.7
Batch: 160; loss: 1.3; acc: 0.64
Batch: 180; loss: 1.22; acc: 0.72
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.24; acc: 0.64
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.41; acc: 0.59
Batch: 280; loss: 1.32; acc: 0.58
Batch: 300; loss: 1.33; acc: 0.67
Batch: 320; loss: 1.21; acc: 0.7
Batch: 340; loss: 1.19; acc: 0.69
Batch: 360; loss: 1.35; acc: 0.61
Batch: 380; loss: 1.31; acc: 0.64
Batch: 400; loss: 1.37; acc: 0.61
Batch: 420; loss: 1.28; acc: 0.62
Batch: 440; loss: 1.28; acc: 0.75
Batch: 460; loss: 1.36; acc: 0.7
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 1.28; acc: 0.61
Batch: 520; loss: 1.21; acc: 0.62
Batch: 540; loss: 1.12; acc: 0.73
Batch: 560; loss: 1.37; acc: 0.61
Batch: 580; loss: 1.35; acc: 0.67
Batch: 600; loss: 1.28; acc: 0.72
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.31; acc: 0.69
Batch: 660; loss: 1.24; acc: 0.64
Batch: 680; loss: 1.23; acc: 0.69
Batch: 700; loss: 1.37; acc: 0.67
Batch: 720; loss: 1.22; acc: 0.72
Batch: 740; loss: 1.3; acc: 0.66
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.2; acc: 0.69
Train Epoch over. train_loss: 1.25; train_accuracy: 0.68 

0.0001400154287694022
0.00013300174032337964
Batch: 0; loss: 1.38; acc: 0.64
Batch: 20; loss: 1.31; acc: 0.69
Batch: 40; loss: 0.88; acc: 0.84
Batch: 60; loss: 1.12; acc: 0.78
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.72
Batch: 120; loss: 1.47; acc: 0.59
Batch: 140; loss: 1.01; acc: 0.84
Val Epoch over. val_loss: 1.1937319651530807; val_accuracy: 0.7138734076433121 

The current subspace-distance is: 0.00013300174032337964 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.31; acc: 0.7
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 1.35; acc: 0.56
Batch: 60; loss: 1.13; acc: 0.75
Batch: 80; loss: 1.27; acc: 0.64
Batch: 100; loss: 1.23; acc: 0.77
Batch: 120; loss: 1.35; acc: 0.67
Batch: 140; loss: 1.28; acc: 0.67
Batch: 160; loss: 1.35; acc: 0.58
Batch: 180; loss: 1.16; acc: 0.75
Batch: 200; loss: 1.22; acc: 0.67
Batch: 220; loss: 1.32; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.72
Batch: 260; loss: 1.14; acc: 0.75
Batch: 280; loss: 1.16; acc: 0.72
Batch: 300; loss: 1.27; acc: 0.66
Batch: 320; loss: 1.17; acc: 0.78
Batch: 340; loss: 1.13; acc: 0.78
Batch: 360; loss: 1.22; acc: 0.69
Batch: 380; loss: 1.27; acc: 0.67
Batch: 400; loss: 1.17; acc: 0.67
Batch: 420; loss: 1.37; acc: 0.59
Batch: 440; loss: 1.26; acc: 0.66
Batch: 460; loss: 1.19; acc: 0.77
Batch: 480; loss: 1.22; acc: 0.67
Batch: 500; loss: 1.28; acc: 0.67
Batch: 520; loss: 1.29; acc: 0.7
Batch: 540; loss: 1.09; acc: 0.77
Batch: 560; loss: 1.11; acc: 0.69
Batch: 580; loss: 1.38; acc: 0.59
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.38; acc: 0.62
Batch: 640; loss: 1.14; acc: 0.75
Batch: 660; loss: 1.12; acc: 0.73
Batch: 680; loss: 1.27; acc: 0.67
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.41; acc: 0.62
Batch: 760; loss: 1.31; acc: 0.58
Batch: 780; loss: 1.08; acc: 0.75
Train Epoch over. train_loss: 1.25; train_accuracy: 0.68 

0.00014280801406130195
0.00013818375009577721
Batch: 0; loss: 1.39; acc: 0.61
Batch: 20; loss: 1.34; acc: 0.67
Batch: 40; loss: 0.89; acc: 0.88
Batch: 60; loss: 1.14; acc: 0.77
Batch: 80; loss: 1.01; acc: 0.77
Batch: 100; loss: 1.29; acc: 0.7
Batch: 120; loss: 1.47; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.83
Val Epoch over. val_loss: 1.2013926935044064; val_accuracy: 0.7090963375796179 

The current subspace-distance is: 0.00013818375009577721 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.73
Batch: 20; loss: 1.32; acc: 0.64
Batch: 40; loss: 1.3; acc: 0.62
Batch: 60; loss: 1.32; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.75
Batch: 120; loss: 1.17; acc: 0.72
Batch: 140; loss: 1.23; acc: 0.73
Batch: 160; loss: 1.36; acc: 0.62
Batch: 180; loss: 1.21; acc: 0.62
Batch: 200; loss: 1.25; acc: 0.66
Batch: 220; loss: 1.29; acc: 0.58
Batch: 240; loss: 1.18; acc: 0.72
Batch: 260; loss: 1.27; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.75
Batch: 300; loss: 1.15; acc: 0.7
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.3; acc: 0.66
Batch: 360; loss: 1.28; acc: 0.66
Batch: 380; loss: 1.28; acc: 0.69
Batch: 400; loss: 1.14; acc: 0.7
Batch: 420; loss: 1.29; acc: 0.7
Batch: 440; loss: 1.2; acc: 0.73
Batch: 460; loss: 1.27; acc: 0.62
Batch: 480; loss: 1.22; acc: 0.67
Batch: 500; loss: 1.3; acc: 0.69
Batch: 520; loss: 1.3; acc: 0.69
Batch: 540; loss: 1.32; acc: 0.72
Batch: 560; loss: 1.19; acc: 0.69
Batch: 580; loss: 1.24; acc: 0.72
Batch: 600; loss: 1.08; acc: 0.86
Batch: 620; loss: 1.16; acc: 0.73
Batch: 640; loss: 1.22; acc: 0.69
Batch: 660; loss: 1.31; acc: 0.59
Batch: 680; loss: 1.12; acc: 0.8
Batch: 700; loss: 1.11; acc: 0.83
Batch: 720; loss: 1.45; acc: 0.62
Batch: 740; loss: 1.42; acc: 0.56
Batch: 760; loss: 1.07; acc: 0.78
Batch: 780; loss: 1.33; acc: 0.67
Train Epoch over. train_loss: 1.24; train_accuracy: 0.68 

0.0001468713307986036
0.0001396777224726975
Batch: 0; loss: 1.38; acc: 0.62
Batch: 20; loss: 1.34; acc: 0.62
Batch: 40; loss: 0.87; acc: 0.84
Batch: 60; loss: 1.14; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.78
Batch: 100; loss: 1.28; acc: 0.69
Batch: 120; loss: 1.46; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.81
Val Epoch over. val_loss: 1.191624903375176; val_accuracy: 0.7106886942675159 

The current subspace-distance is: 0.0001396777224726975 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.72
Batch: 40; loss: 1.2; acc: 0.73
Batch: 60; loss: 1.1; acc: 0.69
Batch: 80; loss: 1.28; acc: 0.61
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.13; acc: 0.81
Batch: 140; loss: 1.24; acc: 0.62
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 1.16; acc: 0.7
Batch: 200; loss: 1.29; acc: 0.66
Batch: 220; loss: 1.26; acc: 0.67
Batch: 240; loss: 1.3; acc: 0.66
Batch: 260; loss: 1.14; acc: 0.75
Batch: 280; loss: 1.27; acc: 0.64
Batch: 300; loss: 1.22; acc: 0.69
Batch: 320; loss: 1.2; acc: 0.66
Batch: 340; loss: 1.39; acc: 0.61
Batch: 360; loss: 1.24; acc: 0.7
Batch: 380; loss: 1.28; acc: 0.64
Batch: 400; loss: 1.23; acc: 0.67
Batch: 420; loss: 1.37; acc: 0.61
Batch: 440; loss: 1.2; acc: 0.58
Batch: 460; loss: 1.38; acc: 0.61
Batch: 480; loss: 1.2; acc: 0.72
Batch: 500; loss: 1.1; acc: 0.67
Batch: 520; loss: 1.22; acc: 0.67
Batch: 540; loss: 1.19; acc: 0.73
Batch: 560; loss: 1.24; acc: 0.7
Batch: 580; loss: 1.19; acc: 0.75
Batch: 600; loss: 1.27; acc: 0.66
Batch: 620; loss: 1.15; acc: 0.67
Batch: 640; loss: 1.19; acc: 0.64
Batch: 660; loss: 1.21; acc: 0.73
Batch: 680; loss: 1.23; acc: 0.7
Batch: 700; loss: 1.19; acc: 0.72
Batch: 720; loss: 1.15; acc: 0.66
Batch: 740; loss: 1.15; acc: 0.7
Batch: 760; loss: 1.28; acc: 0.58
Batch: 780; loss: 1.29; acc: 0.67
Train Epoch over. train_loss: 1.23; train_accuracy: 0.68 

0.0001448055263608694
0.0001387023221468553
Batch: 0; loss: 1.37; acc: 0.61
Batch: 20; loss: 1.34; acc: 0.64
Batch: 40; loss: 0.87; acc: 0.84
Batch: 60; loss: 1.14; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.72
Batch: 120; loss: 1.45; acc: 0.59
Batch: 140; loss: 1.02; acc: 0.83
Val Epoch over. val_loss: 1.1861003250073476; val_accuracy: 0.7099920382165605 

The current subspace-distance is: 0.0001387023221468553 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.31; acc: 0.64
Batch: 20; loss: 1.22; acc: 0.66
Batch: 40; loss: 1.21; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.61
Batch: 80; loss: 1.27; acc: 0.66
Batch: 100; loss: 1.2; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.72
Batch: 140; loss: 1.29; acc: 0.56
Batch: 160; loss: 1.29; acc: 0.67
Batch: 180; loss: 1.36; acc: 0.69
Batch: 200; loss: 1.27; acc: 0.62
Batch: 220; loss: 1.41; acc: 0.59
Batch: 240; loss: 1.25; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.7
Batch: 280; loss: 1.24; acc: 0.64
Batch: 300; loss: 1.27; acc: 0.67
Batch: 320; loss: 1.04; acc: 0.8
Batch: 340; loss: 1.23; acc: 0.64
Batch: 360; loss: 1.09; acc: 0.78
Batch: 380; loss: 1.13; acc: 0.7
Batch: 400; loss: 1.12; acc: 0.77
Batch: 420; loss: 1.25; acc: 0.67
Batch: 440; loss: 1.03; acc: 0.8
Batch: 460; loss: 1.22; acc: 0.73
Batch: 480; loss: 1.12; acc: 0.73
Batch: 500; loss: 1.06; acc: 0.73
Batch: 520; loss: 1.18; acc: 0.72
Batch: 540; loss: 1.11; acc: 0.78
Batch: 560; loss: 1.23; acc: 0.72
Batch: 580; loss: 1.27; acc: 0.64
Batch: 600; loss: 1.27; acc: 0.69
Batch: 620; loss: 1.31; acc: 0.56
Batch: 640; loss: 1.22; acc: 0.64
Batch: 660; loss: 1.13; acc: 0.7
Batch: 680; loss: 1.29; acc: 0.62
Batch: 700; loss: 1.15; acc: 0.75
Batch: 720; loss: 1.24; acc: 0.59
Batch: 740; loss: 1.04; acc: 0.73
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.28; acc: 0.61
Train Epoch over. train_loss: 1.22; train_accuracy: 0.68 

0.00014939135871827602
0.00014270014071371406
Batch: 0; loss: 1.36; acc: 0.62
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 1.12; acc: 0.77
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 1.26; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.62
Batch: 140; loss: 1.01; acc: 0.8
Val Epoch over. val_loss: 1.174953937910165; val_accuracy: 0.7109872611464968 

The current subspace-distance is: 0.00014270014071371406 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.19; acc: 0.75
Batch: 40; loss: 1.14; acc: 0.73
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 1.19; acc: 0.78
Batch: 100; loss: 1.27; acc: 0.62
Batch: 120; loss: 1.45; acc: 0.55
Batch: 140; loss: 1.12; acc: 0.7
Batch: 160; loss: 1.19; acc: 0.69
Batch: 180; loss: 1.15; acc: 0.64
Batch: 200; loss: 1.13; acc: 0.73
Batch: 220; loss: 1.13; acc: 0.77
Batch: 240; loss: 1.14; acc: 0.67
Batch: 260; loss: 1.17; acc: 0.72
Batch: 280; loss: 1.17; acc: 0.75
Batch: 300; loss: 1.13; acc: 0.69
Batch: 320; loss: 1.25; acc: 0.67
Batch: 340; loss: 1.22; acc: 0.75
Batch: 360; loss: 1.3; acc: 0.62
Batch: 380; loss: 1.35; acc: 0.59
Batch: 400; loss: 1.39; acc: 0.66
Batch: 420; loss: 1.19; acc: 0.73
Batch: 440; loss: 1.19; acc: 0.67
Batch: 460; loss: 1.16; acc: 0.77
Batch: 480; loss: 1.13; acc: 0.78
Batch: 500; loss: 1.27; acc: 0.61
Batch: 520; loss: 1.34; acc: 0.56
Batch: 540; loss: 1.22; acc: 0.66
Batch: 560; loss: 1.23; acc: 0.62
Batch: 580; loss: 1.14; acc: 0.7
Batch: 600; loss: 1.17; acc: 0.62
Batch: 620; loss: 1.22; acc: 0.66
Batch: 640; loss: 1.26; acc: 0.66
Batch: 660; loss: 1.17; acc: 0.69
Batch: 680; loss: 1.29; acc: 0.69
Batch: 700; loss: 1.24; acc: 0.69
Batch: 720; loss: 1.28; acc: 0.61
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.24; acc: 0.73
Batch: 780; loss: 1.16; acc: 0.78
Train Epoch over. train_loss: 1.22; train_accuracy: 0.68 

0.00015019025886431336
0.0001450443669455126
Batch: 0; loss: 1.33; acc: 0.61
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 0.96; acc: 0.81
Batch: 100; loss: 1.25; acc: 0.73
Batch: 120; loss: 1.43; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.81
Val Epoch over. val_loss: 1.1638066138431524; val_accuracy: 0.716062898089172 

The current subspace-distance is: 0.0001450443669455126 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.14; acc: 0.69
Batch: 80; loss: 1.38; acc: 0.53
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.18; acc: 0.73
Batch: 160; loss: 1.11; acc: 0.7
Batch: 180; loss: 1.3; acc: 0.62
Batch: 200; loss: 1.14; acc: 0.73
Batch: 220; loss: 1.21; acc: 0.67
Batch: 240; loss: 1.19; acc: 0.67
Batch: 260; loss: 1.09; acc: 0.77
Batch: 280; loss: 1.29; acc: 0.7
Batch: 300; loss: 1.25; acc: 0.61
Batch: 320; loss: 1.16; acc: 0.75
Batch: 340; loss: 1.08; acc: 0.78
Batch: 360; loss: 1.21; acc: 0.73
Batch: 380; loss: 1.37; acc: 0.58
Batch: 400; loss: 1.19; acc: 0.72
Batch: 420; loss: 1.14; acc: 0.73
Batch: 440; loss: 1.19; acc: 0.72
Batch: 460; loss: 1.36; acc: 0.66
Batch: 480; loss: 1.28; acc: 0.56
Batch: 500; loss: 1.31; acc: 0.61
Batch: 520; loss: 1.54; acc: 0.5
Batch: 540; loss: 1.24; acc: 0.72
Batch: 560; loss: 1.29; acc: 0.59
Batch: 580; loss: 1.3; acc: 0.61
Batch: 600; loss: 1.17; acc: 0.67
Batch: 620; loss: 1.04; acc: 0.77
Batch: 640; loss: 1.21; acc: 0.69
Batch: 660; loss: 1.14; acc: 0.75
Batch: 680; loss: 1.11; acc: 0.77
Batch: 700; loss: 1.22; acc: 0.7
Batch: 720; loss: 1.18; acc: 0.67
Batch: 740; loss: 1.16; acc: 0.64
Batch: 760; loss: 1.34; acc: 0.61
Batch: 780; loss: 1.17; acc: 0.7
Train Epoch over. train_loss: 1.22; train_accuracy: 0.68 

0.00015225607785396278
0.00014432548778131604
Batch: 0; loss: 1.35; acc: 0.59
Batch: 20; loss: 1.34; acc: 0.59
Batch: 40; loss: 0.83; acc: 0.86
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 0.96; acc: 0.78
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.42; acc: 0.62
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1619382944835979; val_accuracy: 0.7136743630573248 

The current subspace-distance is: 0.00014432548778131604 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.75
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 1.14; acc: 0.75
Batch: 60; loss: 1.25; acc: 0.72
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.3; acc: 0.59
Batch: 120; loss: 1.16; acc: 0.78
Batch: 140; loss: 1.3; acc: 0.59
Batch: 160; loss: 1.16; acc: 0.67
Batch: 180; loss: 1.06; acc: 0.8
Batch: 200; loss: 1.21; acc: 0.67
Batch: 220; loss: 1.22; acc: 0.67
Batch: 240; loss: 1.22; acc: 0.7
Batch: 260; loss: 1.25; acc: 0.67
Batch: 280; loss: 1.28; acc: 0.64
Batch: 300; loss: 1.34; acc: 0.56
Batch: 320; loss: 1.19; acc: 0.69
Batch: 340; loss: 1.41; acc: 0.56
Batch: 360; loss: 1.2; acc: 0.67
Batch: 380; loss: 1.3; acc: 0.67
Batch: 400; loss: 1.27; acc: 0.69
Batch: 420; loss: 1.11; acc: 0.78
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.22; acc: 0.66
Batch: 480; loss: 1.21; acc: 0.64
Batch: 500; loss: 1.25; acc: 0.62
Batch: 520; loss: 1.05; acc: 0.8
Batch: 540; loss: 1.25; acc: 0.67
Batch: 560; loss: 1.22; acc: 0.73
Batch: 580; loss: 0.99; acc: 0.83
Batch: 600; loss: 1.24; acc: 0.66
Batch: 620; loss: 1.24; acc: 0.7
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 0.98; acc: 0.78
Batch: 680; loss: 1.22; acc: 0.64
Batch: 700; loss: 1.27; acc: 0.64
Batch: 720; loss: 1.14; acc: 0.73
Batch: 740; loss: 1.42; acc: 0.67
Batch: 760; loss: 1.25; acc: 0.62
Batch: 780; loss: 1.29; acc: 0.58
Train Epoch over. train_loss: 1.21; train_accuracy: 0.68 

0.00015150182298384607
0.0001471907744416967
Batch: 0; loss: 1.35; acc: 0.62
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.86
Batch: 60; loss: 1.13; acc: 0.75
Batch: 80; loss: 0.97; acc: 0.83
Batch: 100; loss: 1.26; acc: 0.72
Batch: 120; loss: 1.44; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1732914508528012; val_accuracy: 0.7156648089171974 

The current subspace-distance is: 0.0001471907744416967 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.01; acc: 0.73
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.29; acc: 0.67
Batch: 80; loss: 1.21; acc: 0.69
Batch: 100; loss: 1.2; acc: 0.66
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.36; acc: 0.64
Batch: 160; loss: 1.13; acc: 0.66
Batch: 180; loss: 1.07; acc: 0.77
Batch: 200; loss: 1.24; acc: 0.72
Batch: 220; loss: 1.19; acc: 0.62
Batch: 240; loss: 1.36; acc: 0.61
Batch: 260; loss: 1.16; acc: 0.72
Batch: 280; loss: 1.11; acc: 0.72
Batch: 300; loss: 1.17; acc: 0.77
Batch: 320; loss: 1.17; acc: 0.83
Batch: 340; loss: 1.17; acc: 0.72
Batch: 360; loss: 1.34; acc: 0.58
Batch: 380; loss: 1.33; acc: 0.62
Batch: 400; loss: 1.29; acc: 0.61
Batch: 420; loss: 1.13; acc: 0.73
Batch: 440; loss: 1.29; acc: 0.61
Batch: 460; loss: 1.24; acc: 0.67
Batch: 480; loss: 1.24; acc: 0.77
Batch: 500; loss: 1.17; acc: 0.7
Batch: 520; loss: 1.15; acc: 0.7
Batch: 540; loss: 1.27; acc: 0.66
Batch: 560; loss: 1.18; acc: 0.72
Batch: 580; loss: 1.16; acc: 0.69
Batch: 600; loss: 1.18; acc: 0.66
Batch: 620; loss: 1.19; acc: 0.66
Batch: 640; loss: 1.25; acc: 0.67
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.29; acc: 0.62
Batch: 720; loss: 1.17; acc: 0.69
Batch: 740; loss: 1.23; acc: 0.67
Batch: 760; loss: 1.41; acc: 0.64
Batch: 780; loss: 1.15; acc: 0.75
Train Epoch over. train_loss: 1.21; train_accuracy: 0.68 

0.00015007550246082246
0.00014472653856500983
Batch: 0; loss: 1.35; acc: 0.59
Batch: 20; loss: 1.35; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 1.26; acc: 0.73
Batch: 120; loss: 1.42; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.8
Val Epoch over. val_loss: 1.17328658377289; val_accuracy: 0.7156648089171974 

The current subspace-distance is: 0.00014472653856500983 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.66
Batch: 20; loss: 1.1; acc: 0.78
Batch: 40; loss: 1.01; acc: 0.73
Batch: 60; loss: 1.27; acc: 0.62
Batch: 80; loss: 1.36; acc: 0.62
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.15; acc: 0.72
Batch: 140; loss: 1.29; acc: 0.66
Batch: 160; loss: 1.28; acc: 0.62
Batch: 180; loss: 1.28; acc: 0.66
Batch: 200; loss: 1.17; acc: 0.72
Batch: 220; loss: 1.17; acc: 0.7
Batch: 240; loss: 1.19; acc: 0.66
Batch: 260; loss: 1.29; acc: 0.64
Batch: 280; loss: 1.14; acc: 0.69
Batch: 300; loss: 1.16; acc: 0.67
Batch: 320; loss: 1.13; acc: 0.77
Batch: 340; loss: 1.29; acc: 0.66
Batch: 360; loss: 1.2; acc: 0.66
Batch: 380; loss: 1.23; acc: 0.66
Batch: 400; loss: 1.28; acc: 0.64
Batch: 420; loss: 1.23; acc: 0.69
Batch: 440; loss: 1.21; acc: 0.72
Batch: 460; loss: 1.31; acc: 0.61
Batch: 480; loss: 1.1; acc: 0.7
Batch: 500; loss: 1.17; acc: 0.73
Batch: 520; loss: 1.22; acc: 0.66
Batch: 540; loss: 1.38; acc: 0.59
Batch: 560; loss: 1.27; acc: 0.69
Batch: 580; loss: 1.1; acc: 0.73
Batch: 600; loss: 1.16; acc: 0.72
Batch: 620; loss: 1.23; acc: 0.7
Batch: 640; loss: 1.08; acc: 0.75
Batch: 660; loss: 1.22; acc: 0.69
Batch: 680; loss: 1.21; acc: 0.66
Batch: 700; loss: 1.29; acc: 0.64
Batch: 720; loss: 1.15; acc: 0.66
Batch: 740; loss: 1.23; acc: 0.72
Batch: 760; loss: 1.36; acc: 0.55
Batch: 780; loss: 1.18; acc: 0.72
Train Epoch over. train_loss: 1.21; train_accuracy: 0.68 

0.00015459743735846132
0.00014624139294028282
Batch: 0; loss: 1.33; acc: 0.61
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 0.96; acc: 0.83
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.42; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1617840133654844; val_accuracy: 0.7164609872611465 

The current subspace-distance is: 0.00014624139294028282 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.69
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 1.24; acc: 0.69
Batch: 60; loss: 1.18; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.69
Batch: 100; loss: 1.22; acc: 0.7
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 1.22; acc: 0.7
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 1.28; acc: 0.72
Batch: 200; loss: 1.4; acc: 0.53
Batch: 220; loss: 1.29; acc: 0.67
Batch: 240; loss: 1.37; acc: 0.59
Batch: 260; loss: 1.13; acc: 0.72
Batch: 280; loss: 1.12; acc: 0.75
Batch: 300; loss: 1.32; acc: 0.56
Batch: 320; loss: 1.14; acc: 0.73
Batch: 340; loss: 1.27; acc: 0.66
Batch: 360; loss: 1.24; acc: 0.67
Batch: 380; loss: 1.13; acc: 0.77
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.23; acc: 0.66
Batch: 440; loss: 1.29; acc: 0.62
Batch: 460; loss: 1.24; acc: 0.7
Batch: 480; loss: 1.11; acc: 0.77
Batch: 500; loss: 1.41; acc: 0.53
Batch: 520; loss: 1.31; acc: 0.66
Batch: 540; loss: 1.36; acc: 0.61
Batch: 560; loss: 1.25; acc: 0.7
Batch: 580; loss: 1.14; acc: 0.67
Batch: 600; loss: 1.25; acc: 0.7
Batch: 620; loss: 1.16; acc: 0.7
Batch: 640; loss: 1.21; acc: 0.7
Batch: 660; loss: 1.11; acc: 0.67
Batch: 680; loss: 1.23; acc: 0.75
Batch: 700; loss: 1.21; acc: 0.75
Batch: 720; loss: 1.3; acc: 0.67
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 1.06; acc: 0.7
Batch: 780; loss: 1.33; acc: 0.62
Train Epoch over. train_loss: 1.21; train_accuracy: 0.68 

0.00015200940833892673
0.0001475026219850406
Batch: 0; loss: 1.33; acc: 0.61
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 0.96; acc: 0.8
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1628591444841616; val_accuracy: 0.7172571656050956 

The current subspace-distance is: 0.0001475026219850406 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 1.07; acc: 0.83
Batch: 60; loss: 1.37; acc: 0.58
Batch: 80; loss: 1.15; acc: 0.67
Batch: 100; loss: 1.21; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.14; acc: 0.72
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 1.14; acc: 0.69
Batch: 220; loss: 1.25; acc: 0.64
Batch: 240; loss: 1.24; acc: 0.62
Batch: 260; loss: 1.28; acc: 0.64
Batch: 280; loss: 1.25; acc: 0.67
Batch: 300; loss: 1.45; acc: 0.53
Batch: 320; loss: 1.18; acc: 0.7
Batch: 340; loss: 1.29; acc: 0.61
Batch: 360; loss: 1.24; acc: 0.67
Batch: 380; loss: 1.24; acc: 0.61
Batch: 400; loss: 1.19; acc: 0.72
Batch: 420; loss: 1.24; acc: 0.62
Batch: 440; loss: 1.13; acc: 0.64
Batch: 460; loss: 1.18; acc: 0.73
Batch: 480; loss: 1.12; acc: 0.7
Batch: 500; loss: 1.14; acc: 0.75
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 1.22; acc: 0.72
Batch: 560; loss: 1.07; acc: 0.75
Batch: 580; loss: 1.31; acc: 0.61
Batch: 600; loss: 1.17; acc: 0.77
Batch: 620; loss: 1.22; acc: 0.61
Batch: 640; loss: 1.39; acc: 0.55
Batch: 660; loss: 1.27; acc: 0.64
Batch: 680; loss: 1.14; acc: 0.73
Batch: 700; loss: 1.25; acc: 0.59
Batch: 720; loss: 1.23; acc: 0.7
Batch: 740; loss: 1.3; acc: 0.61
Batch: 760; loss: 1.22; acc: 0.67
Batch: 780; loss: 1.16; acc: 0.72
Train Epoch over. train_loss: 1.2; train_accuracy: 0.68 

0.00015624093066435307
0.00014924399147275835
Batch: 0; loss: 1.31; acc: 0.64
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 1.12; acc: 0.73
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.42; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.81
Val Epoch over. val_loss: 1.1627435953753769; val_accuracy: 0.7172571656050956 

The current subspace-distance is: 0.00014924399147275835 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.29; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.73
Batch: 40; loss: 1.16; acc: 0.7
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.29; acc: 0.69
Batch: 100; loss: 1.27; acc: 0.61
Batch: 120; loss: 1.17; acc: 0.66
Batch: 140; loss: 1.23; acc: 0.73
Batch: 160; loss: 1.23; acc: 0.72
Batch: 180; loss: 1.35; acc: 0.61
Batch: 200; loss: 1.36; acc: 0.58
Batch: 220; loss: 1.25; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.69
Batch: 260; loss: 1.16; acc: 0.7
Batch: 280; loss: 1.11; acc: 0.69
Batch: 300; loss: 1.3; acc: 0.66
Batch: 320; loss: 1.27; acc: 0.64
Batch: 340; loss: 1.3; acc: 0.64
Batch: 360; loss: 1.24; acc: 0.64
Batch: 380; loss: 1.23; acc: 0.64
Batch: 400; loss: 1.13; acc: 0.67
Batch: 420; loss: 1.22; acc: 0.69
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.04; acc: 0.78
Batch: 500; loss: 1.33; acc: 0.55
Batch: 520; loss: 1.38; acc: 0.55
Batch: 540; loss: 1.5; acc: 0.48
Batch: 560; loss: 1.15; acc: 0.73
Batch: 580; loss: 1.15; acc: 0.69
Batch: 600; loss: 1.19; acc: 0.73
Batch: 620; loss: 1.3; acc: 0.59
Batch: 640; loss: 1.42; acc: 0.58
Batch: 660; loss: 1.23; acc: 0.72
Batch: 680; loss: 1.23; acc: 0.69
Batch: 700; loss: 1.24; acc: 0.69
Batch: 720; loss: 1.23; acc: 0.73
Batch: 740; loss: 1.23; acc: 0.72
Batch: 760; loss: 1.28; acc: 0.58
Batch: 780; loss: 1.08; acc: 0.75
Train Epoch over. train_loss: 1.2; train_accuracy: 0.68 

0.0001543508842587471
0.00014782969083171338
Batch: 0; loss: 1.33; acc: 0.59
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.7
Batch: 80; loss: 0.95; acc: 0.78
Batch: 100; loss: 1.25; acc: 0.72
Batch: 120; loss: 1.41; acc: 0.59
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1572842043676195; val_accuracy: 0.7144705414012739 

The current subspace-distance is: 0.00014782969083171338 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.72
Batch: 20; loss: 1.45; acc: 0.59
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.28; acc: 0.61
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.04; acc: 0.8
Batch: 140; loss: 1.15; acc: 0.7
Batch: 160; loss: 1.26; acc: 0.67
Batch: 180; loss: 1.35; acc: 0.55
Batch: 200; loss: 1.08; acc: 0.72
Batch: 220; loss: 1.19; acc: 0.66
Batch: 240; loss: 1.22; acc: 0.66
Batch: 260; loss: 1.07; acc: 0.73
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.13; acc: 0.75
Batch: 320; loss: 1.12; acc: 0.72
Batch: 340; loss: 1.09; acc: 0.75
Batch: 360; loss: 1.27; acc: 0.7
Batch: 380; loss: 1.24; acc: 0.73
Batch: 400; loss: 1.24; acc: 0.7
Batch: 420; loss: 1.08; acc: 0.78
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 1.16; acc: 0.72
Batch: 480; loss: 1.31; acc: 0.58
Batch: 500; loss: 1.09; acc: 0.72
Batch: 520; loss: 1.24; acc: 0.62
Batch: 540; loss: 1.16; acc: 0.7
Batch: 560; loss: 1.18; acc: 0.7
Batch: 580; loss: 1.13; acc: 0.72
Batch: 600; loss: 1.3; acc: 0.62
Batch: 620; loss: 1.14; acc: 0.72
Batch: 640; loss: 1.15; acc: 0.69
Batch: 660; loss: 1.27; acc: 0.61
Batch: 680; loss: 1.06; acc: 0.78
Batch: 700; loss: 1.25; acc: 0.61
Batch: 720; loss: 1.24; acc: 0.69
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.17; acc: 0.66
Batch: 780; loss: 1.21; acc: 0.64
Train Epoch over. train_loss: 1.2; train_accuracy: 0.68 

0.00015690171858295798
0.00014981927233748138
Batch: 0; loss: 1.33; acc: 0.62
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 0.95; acc: 0.8
Batch: 100; loss: 1.24; acc: 0.7
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.1507801265473578; val_accuracy: 0.7140724522292994 

The current subspace-distance is: 0.00014981927233748138 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.26; acc: 0.62
Batch: 20; loss: 1.2; acc: 0.73
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.17; acc: 0.78
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 1.25; acc: 0.62
Batch: 160; loss: 1.25; acc: 0.69
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 1.18; acc: 0.73
Batch: 220; loss: 1.13; acc: 0.73
Batch: 240; loss: 1.29; acc: 0.67
Batch: 260; loss: 1.12; acc: 0.7
Batch: 280; loss: 1.04; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.73
Batch: 320; loss: 1.07; acc: 0.73
Batch: 340; loss: 1.19; acc: 0.7
Batch: 360; loss: 1.09; acc: 0.78
Batch: 380; loss: 1.35; acc: 0.58
Batch: 400; loss: 1.11; acc: 0.75
Batch: 420; loss: 1.37; acc: 0.55
Batch: 440; loss: 1.29; acc: 0.61
Batch: 460; loss: 1.24; acc: 0.66
Batch: 480; loss: 1.23; acc: 0.62
Batch: 500; loss: 1.33; acc: 0.58
Batch: 520; loss: 1.25; acc: 0.59
Batch: 540; loss: 1.2; acc: 0.66
Batch: 560; loss: 1.19; acc: 0.67
Batch: 580; loss: 1.32; acc: 0.64
Batch: 600; loss: 1.25; acc: 0.67
Batch: 620; loss: 1.24; acc: 0.59
Batch: 640; loss: 1.23; acc: 0.7
Batch: 660; loss: 1.46; acc: 0.58
Batch: 680; loss: 1.24; acc: 0.66
Batch: 700; loss: 1.27; acc: 0.61
Batch: 720; loss: 1.29; acc: 0.69
Batch: 740; loss: 1.15; acc: 0.72
Batch: 760; loss: 1.22; acc: 0.69
Batch: 780; loss: 1.19; acc: 0.73
Train Epoch over. train_loss: 1.2; train_accuracy: 0.68 

0.00015610788250342011
0.00014851732703391463
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 1.1; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.81
Batch: 100; loss: 1.22; acc: 0.7
Batch: 120; loss: 1.38; acc: 0.61
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.1386212634432846; val_accuracy: 0.7199442675159236 

The current subspace-distance is: 0.00014851732703391463 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_13_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6833983485794441

The number of parameters is: 259251

The number of individual parameters is:

14
252
14
14
21
38514
21
21
41
112791
41
41
64
102336
64
64
4096
64
640
10
64
64

nonzero elements in E: 51850195
elements in E: 51850200
fraction nonzero: 0.9999999035683566
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.11
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 2.05; acc: 0.2
Batch: 60; loss: 1.94; acc: 0.38
Batch: 80; loss: 1.84; acc: 0.5
Batch: 100; loss: 1.9; acc: 0.34
Batch: 120; loss: 1.85; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.56
Batch: 160; loss: 1.8; acc: 0.45
Batch: 180; loss: 1.78; acc: 0.47
Batch: 200; loss: 1.76; acc: 0.56
Batch: 220; loss: 1.72; acc: 0.47
Batch: 240; loss: 1.61; acc: 0.56
Batch: 260; loss: 1.56; acc: 0.53
Batch: 280; loss: 1.62; acc: 0.52
Batch: 300; loss: 1.67; acc: 0.53
Batch: 320; loss: 1.52; acc: 0.61
Batch: 340; loss: 1.46; acc: 0.67
Batch: 360; loss: 1.52; acc: 0.58
Batch: 380; loss: 1.54; acc: 0.64
Batch: 400; loss: 1.51; acc: 0.55
Batch: 420; loss: 1.61; acc: 0.59
Batch: 440; loss: 1.45; acc: 0.56
Batch: 460; loss: 1.61; acc: 0.58
Batch: 480; loss: 1.62; acc: 0.55
Batch: 500; loss: 1.5; acc: 0.58
Batch: 520; loss: 1.45; acc: 0.67
Batch: 540; loss: 1.37; acc: 0.72
Batch: 560; loss: 1.43; acc: 0.67
Batch: 580; loss: 1.4; acc: 0.78
Batch: 600; loss: 1.31; acc: 0.77
Batch: 620; loss: 1.29; acc: 0.77
Batch: 640; loss: 1.55; acc: 0.53
Batch: 660; loss: 1.5; acc: 0.62
Batch: 680; loss: 1.4; acc: 0.69
Batch: 700; loss: 1.52; acc: 0.58
Batch: 720; loss: 1.43; acc: 0.62
Batch: 740; loss: 1.27; acc: 0.8
Batch: 760; loss: 1.42; acc: 0.58
Batch: 780; loss: 1.3; acc: 0.77
Train Epoch over. train_loss: 1.58; train_accuracy: 0.58 

5.838201104779728e-05
5.394360778154805e-05
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.53; acc: 0.56
Batch: 40; loss: 1.08; acc: 0.83
Batch: 60; loss: 1.28; acc: 0.75
Batch: 80; loss: 1.23; acc: 0.75
Batch: 100; loss: 1.33; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 1.16; acc: 0.83
Val Epoch over. val_loss: 1.3029178008911715; val_accuracy: 0.7251194267515924 

The current subspace-distance is: 5.394360778154805e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.73
Batch: 20; loss: 1.32; acc: 0.73
Batch: 40; loss: 1.39; acc: 0.7
Batch: 60; loss: 1.28; acc: 0.72
Batch: 80; loss: 1.5; acc: 0.59
Batch: 100; loss: 1.19; acc: 0.78
Batch: 120; loss: 1.33; acc: 0.75
Batch: 140; loss: 1.33; acc: 0.67
Batch: 160; loss: 1.2; acc: 0.8
Batch: 180; loss: 1.38; acc: 0.66
Batch: 200; loss: 1.29; acc: 0.77
Batch: 220; loss: 1.36; acc: 0.66
Batch: 240; loss: 1.34; acc: 0.7
Batch: 260; loss: 1.23; acc: 0.77
Batch: 280; loss: 1.22; acc: 0.77
Batch: 300; loss: 1.24; acc: 0.73
Batch: 320; loss: 1.17; acc: 0.78
Batch: 340; loss: 1.34; acc: 0.69
Batch: 360; loss: 1.21; acc: 0.78
Batch: 380; loss: 1.28; acc: 0.75
Batch: 400; loss: 1.28; acc: 0.67
Batch: 420; loss: 1.26; acc: 0.72
Batch: 440; loss: 1.2; acc: 0.75
Batch: 460; loss: 1.2; acc: 0.78
Batch: 480; loss: 1.24; acc: 0.7
Batch: 500; loss: 1.32; acc: 0.73
Batch: 520; loss: 1.17; acc: 0.8
Batch: 540; loss: 1.2; acc: 0.73
Batch: 560; loss: 1.35; acc: 0.66
Batch: 580; loss: 1.35; acc: 0.72
Batch: 600; loss: 1.23; acc: 0.72
Batch: 620; loss: 1.19; acc: 0.7
Batch: 640; loss: 1.15; acc: 0.81
Batch: 660; loss: 1.16; acc: 0.75
Batch: 680; loss: 1.11; acc: 0.81
Batch: 700; loss: 1.04; acc: 0.83
Batch: 720; loss: 1.27; acc: 0.7
Batch: 740; loss: 1.25; acc: 0.75
Batch: 760; loss: 1.12; acc: 0.75
Batch: 780; loss: 1.13; acc: 0.8
Train Epoch over. train_loss: 1.25; train_accuracy: 0.73 

7.902337529230863e-05
7.422324415529147e-05
Batch: 0; loss: 1.22; acc: 0.77
Batch: 20; loss: 1.31; acc: 0.64
Batch: 40; loss: 0.85; acc: 0.92
Batch: 60; loss: 1.14; acc: 0.77
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.14; acc: 0.86
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.94; acc: 0.89
Val Epoch over. val_loss: 1.117475830825271; val_accuracy: 0.7908041401273885 

The current subspace-distance is: 7.422324415529147e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.05; acc: 0.78
Batch: 40; loss: 1.14; acc: 0.81
Batch: 60; loss: 1.32; acc: 0.66
Batch: 80; loss: 1.16; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.77
Batch: 140; loss: 1.15; acc: 0.78
Batch: 160; loss: 1.19; acc: 0.75
Batch: 180; loss: 1.03; acc: 0.83
Batch: 200; loss: 1.12; acc: 0.78
Batch: 220; loss: 1.19; acc: 0.75
Batch: 240; loss: 1.04; acc: 0.81
Batch: 260; loss: 1.19; acc: 0.73
Batch: 280; loss: 1.09; acc: 0.86
Batch: 300; loss: 1.11; acc: 0.77
Batch: 320; loss: 1.07; acc: 0.8
Batch: 340; loss: 1.1; acc: 0.81
Batch: 360; loss: 1.03; acc: 0.88
Batch: 380; loss: 1.03; acc: 0.8
Batch: 400; loss: 1.06; acc: 0.73
Batch: 420; loss: 1.27; acc: 0.64
Batch: 440; loss: 0.98; acc: 0.77
Batch: 460; loss: 1.03; acc: 0.84
Batch: 480; loss: 0.98; acc: 0.81
Batch: 500; loss: 1.16; acc: 0.7
Batch: 520; loss: 1.0; acc: 0.83
Batch: 540; loss: 1.01; acc: 0.81
Batch: 560; loss: 0.96; acc: 0.83
Batch: 580; loss: 1.09; acc: 0.77
Batch: 600; loss: 1.12; acc: 0.73
Batch: 620; loss: 1.17; acc: 0.73
Batch: 640; loss: 1.17; acc: 0.77
Batch: 660; loss: 1.04; acc: 0.8
Batch: 680; loss: 1.15; acc: 0.75
Batch: 700; loss: 0.99; acc: 0.8
Batch: 720; loss: 1.04; acc: 0.81
Batch: 740; loss: 1.07; acc: 0.78
Batch: 760; loss: 1.07; acc: 0.72
Batch: 780; loss: 1.14; acc: 0.77
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

9.610370761947706e-05
9.146533557213843e-05
Batch: 0; loss: 1.07; acc: 0.8
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 0.67; acc: 0.94
Batch: 60; loss: 1.06; acc: 0.8
Batch: 80; loss: 0.81; acc: 0.88
Batch: 100; loss: 1.04; acc: 0.84
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 0.89; acc: 0.86
Val Epoch over. val_loss: 1.0030618503594855; val_accuracy: 0.8040406050955414 

The current subspace-distance is: 9.146533557213843e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.83
Batch: 20; loss: 1.14; acc: 0.8
Batch: 40; loss: 0.86; acc: 0.86
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 1.05; acc: 0.78
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.03; acc: 0.77
Batch: 140; loss: 1.0; acc: 0.75
Batch: 160; loss: 1.08; acc: 0.77
Batch: 180; loss: 0.99; acc: 0.77
Batch: 200; loss: 1.02; acc: 0.81
Batch: 220; loss: 0.96; acc: 0.8
Batch: 240; loss: 0.99; acc: 0.77
Batch: 260; loss: 0.98; acc: 0.84
Batch: 280; loss: 0.95; acc: 0.8
Batch: 300; loss: 0.97; acc: 0.77
Batch: 320; loss: 1.06; acc: 0.78
Batch: 340; loss: 1.02; acc: 0.75
Batch: 360; loss: 0.91; acc: 0.84
Batch: 380; loss: 1.06; acc: 0.69
Batch: 400; loss: 0.99; acc: 0.78
Batch: 420; loss: 1.03; acc: 0.78
Batch: 440; loss: 0.95; acc: 0.84
Batch: 460; loss: 0.89; acc: 0.81
Batch: 480; loss: 1.05; acc: 0.77
Batch: 500; loss: 1.08; acc: 0.77
Batch: 520; loss: 0.99; acc: 0.8
Batch: 540; loss: 1.06; acc: 0.7
Batch: 560; loss: 0.96; acc: 0.83
Batch: 580; loss: 1.01; acc: 0.81
Batch: 600; loss: 1.19; acc: 0.67
Batch: 620; loss: 1.0; acc: 0.8
Batch: 640; loss: 0.95; acc: 0.84
Batch: 660; loss: 1.08; acc: 0.75
Batch: 680; loss: 0.97; acc: 0.75
Batch: 700; loss: 0.97; acc: 0.78
Batch: 720; loss: 0.95; acc: 0.77
Batch: 740; loss: 0.85; acc: 0.88
Batch: 760; loss: 0.97; acc: 0.81
Batch: 780; loss: 1.1; acc: 0.78
Train Epoch over. train_loss: 1.0; train_accuracy: 0.79 

0.00010936509352177382
0.0001032593208947219
Batch: 0; loss: 0.94; acc: 0.83
Batch: 20; loss: 1.12; acc: 0.72
Batch: 40; loss: 0.54; acc: 1.0
Batch: 60; loss: 0.94; acc: 0.83
Batch: 80; loss: 0.68; acc: 0.92
Batch: 100; loss: 0.89; acc: 0.89
Batch: 120; loss: 1.02; acc: 0.78
Batch: 140; loss: 0.78; acc: 0.88
Val Epoch over. val_loss: 0.8985438115277867; val_accuracy: 0.8171775477707006 

The current subspace-distance is: 0.0001032593208947219 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 1.06; acc: 0.72
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 0.88; acc: 0.86
Batch: 80; loss: 0.93; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.93; acc: 0.81
Batch: 140; loss: 0.99; acc: 0.8
Batch: 160; loss: 0.82; acc: 0.84
Batch: 180; loss: 1.04; acc: 0.77
Batch: 200; loss: 0.95; acc: 0.81
Batch: 220; loss: 0.81; acc: 0.83
Batch: 240; loss: 0.9; acc: 0.81
Batch: 260; loss: 0.92; acc: 0.78
Batch: 280; loss: 0.95; acc: 0.81
Batch: 300; loss: 0.92; acc: 0.84
Batch: 320; loss: 1.03; acc: 0.86
Batch: 340; loss: 0.89; acc: 0.8
Batch: 360; loss: 0.85; acc: 0.8
Batch: 380; loss: 0.83; acc: 0.83
Batch: 400; loss: 0.87; acc: 0.83
Batch: 420; loss: 0.92; acc: 0.81
Batch: 440; loss: 0.88; acc: 0.83
Batch: 460; loss: 1.0; acc: 0.83
Batch: 480; loss: 1.09; acc: 0.73
Batch: 500; loss: 0.9; acc: 0.81
Batch: 520; loss: 0.98; acc: 0.78
Batch: 540; loss: 0.86; acc: 0.83
Batch: 560; loss: 0.92; acc: 0.8
Batch: 580; loss: 0.84; acc: 0.86
Batch: 600; loss: 0.87; acc: 0.83
Batch: 620; loss: 1.07; acc: 0.72
Batch: 640; loss: 0.97; acc: 0.75
Batch: 660; loss: 0.8; acc: 0.89
Batch: 680; loss: 0.88; acc: 0.78
Batch: 700; loss: 0.89; acc: 0.8
Batch: 720; loss: 1.01; acc: 0.8
Batch: 740; loss: 0.88; acc: 0.81
Batch: 760; loss: 0.87; acc: 0.8
Batch: 780; loss: 0.76; acc: 0.88
Train Epoch over. train_loss: 0.94; train_accuracy: 0.79 

0.0001225604210048914
0.00011606939369812608
Batch: 0; loss: 0.89; acc: 0.86
Batch: 20; loss: 1.1; acc: 0.69
Batch: 40; loss: 0.5; acc: 0.95
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 0.63; acc: 0.94
Batch: 100; loss: 0.84; acc: 0.89
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 0.75; acc: 0.88
Val Epoch over. val_loss: 0.8678575093579141; val_accuracy: 0.8118033439490446 

The current subspace-distance is: 0.00011606939369812608 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.69
Batch: 20; loss: 0.94; acc: 0.81
Batch: 40; loss: 0.9; acc: 0.8
Batch: 60; loss: 0.89; acc: 0.77
Batch: 80; loss: 0.92; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 1.05; acc: 0.73
Batch: 160; loss: 0.89; acc: 0.81
Batch: 180; loss: 0.91; acc: 0.8
Batch: 200; loss: 1.01; acc: 0.8
Batch: 220; loss: 1.03; acc: 0.75
Batch: 240; loss: 1.09; acc: 0.73
Batch: 260; loss: 0.88; acc: 0.81
Batch: 280; loss: 0.82; acc: 0.77
Batch: 300; loss: 0.93; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.81
Batch: 340; loss: 0.97; acc: 0.77
Batch: 360; loss: 0.98; acc: 0.7
Batch: 380; loss: 0.86; acc: 0.84
Batch: 400; loss: 0.99; acc: 0.78
Batch: 420; loss: 0.98; acc: 0.73
Batch: 440; loss: 1.05; acc: 0.72
Batch: 460; loss: 1.01; acc: 0.75
Batch: 480; loss: 0.75; acc: 0.86
Batch: 500; loss: 0.82; acc: 0.88
Batch: 520; loss: 0.91; acc: 0.78
Batch: 540; loss: 0.87; acc: 0.78
Batch: 560; loss: 0.75; acc: 0.86
Batch: 580; loss: 1.07; acc: 0.75
Batch: 600; loss: 0.83; acc: 0.8
Batch: 620; loss: 1.01; acc: 0.67
Batch: 640; loss: 1.01; acc: 0.67
Batch: 660; loss: 0.89; acc: 0.8
Batch: 680; loss: 0.86; acc: 0.77
Batch: 700; loss: 0.91; acc: 0.73
Batch: 720; loss: 0.88; acc: 0.78
Batch: 740; loss: 0.98; acc: 0.73
Batch: 760; loss: 0.89; acc: 0.75
Batch: 780; loss: 0.88; acc: 0.77
Train Epoch over. train_loss: 0.91; train_accuracy: 0.79 

0.00012834307563025504
0.00012406853784341365
Batch: 0; loss: 0.86; acc: 0.86
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.92
Batch: 100; loss: 0.8; acc: 0.88
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.7; acc: 0.83
Val Epoch over. val_loss: 0.8322947731443272; val_accuracy: 0.818172770700637 

The current subspace-distance is: 0.00012406853784341365 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.8
Batch: 20; loss: 0.88; acc: 0.78
Batch: 40; loss: 1.1; acc: 0.75
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.9; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.91; acc: 0.81
Batch: 160; loss: 0.81; acc: 0.86
Batch: 180; loss: 0.96; acc: 0.72
Batch: 200; loss: 0.91; acc: 0.77
Batch: 220; loss: 0.79; acc: 0.8
Batch: 240; loss: 0.78; acc: 0.86
Batch: 260; loss: 0.97; acc: 0.78
Batch: 280; loss: 0.97; acc: 0.77
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.8; acc: 0.8
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 1.0; acc: 0.77
Batch: 380; loss: 0.79; acc: 0.86
Batch: 400; loss: 0.84; acc: 0.78
Batch: 420; loss: 0.8; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.86
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.96; acc: 0.72
Batch: 500; loss: 0.88; acc: 0.8
Batch: 520; loss: 0.9; acc: 0.77
Batch: 540; loss: 0.88; acc: 0.77
Batch: 560; loss: 0.82; acc: 0.84
Batch: 580; loss: 0.76; acc: 0.86
Batch: 600; loss: 0.82; acc: 0.77
Batch: 620; loss: 0.85; acc: 0.8
Batch: 640; loss: 0.79; acc: 0.81
Batch: 660; loss: 0.89; acc: 0.8
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 0.91; acc: 0.73
Batch: 720; loss: 0.94; acc: 0.78
Batch: 740; loss: 0.85; acc: 0.75
Batch: 760; loss: 0.79; acc: 0.83
Batch: 780; loss: 0.75; acc: 0.86
Train Epoch over. train_loss: 0.88; train_accuracy: 0.79 

0.00013582340034190565
0.00013033123104833066
Batch: 0; loss: 0.83; acc: 0.84
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.56; acc: 0.91
Batch: 100; loss: 0.75; acc: 0.91
Batch: 120; loss: 0.92; acc: 0.7
Batch: 140; loss: 0.64; acc: 0.89
Val Epoch over. val_loss: 0.7844794373603383; val_accuracy: 0.8225517515923567 

The current subspace-distance is: 0.00013033123104833066 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.97; acc: 0.78
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.93; acc: 0.78
Batch: 120; loss: 0.86; acc: 0.84
Batch: 140; loss: 0.86; acc: 0.77
Batch: 160; loss: 0.81; acc: 0.83
Batch: 180; loss: 0.93; acc: 0.73
Batch: 200; loss: 0.82; acc: 0.8
Batch: 220; loss: 1.08; acc: 0.7
Batch: 240; loss: 0.93; acc: 0.77
Batch: 260; loss: 0.87; acc: 0.8
Batch: 280; loss: 0.85; acc: 0.77
Batch: 300; loss: 0.7; acc: 0.92
Batch: 320; loss: 0.96; acc: 0.7
Batch: 340; loss: 0.77; acc: 0.83
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.88; acc: 0.78
Batch: 400; loss: 0.87; acc: 0.84
Batch: 420; loss: 0.82; acc: 0.8
Batch: 440; loss: 0.85; acc: 0.77
Batch: 460; loss: 0.84; acc: 0.83
Batch: 480; loss: 0.97; acc: 0.73
Batch: 500; loss: 0.98; acc: 0.73
Batch: 520; loss: 0.85; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.85; acc: 0.8
Batch: 620; loss: 0.73; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.86; acc: 0.81
Batch: 680; loss: 0.76; acc: 0.8
Batch: 700; loss: 0.82; acc: 0.81
Batch: 720; loss: 0.87; acc: 0.78
Batch: 740; loss: 0.8; acc: 0.86
Batch: 760; loss: 0.83; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.84
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

0.00014603201998397708
0.00013995518384035677
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 1.01; acc: 0.72
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.87; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.74; acc: 0.91
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.89
Val Epoch over. val_loss: 0.7725434242540105; val_accuracy: 0.825437898089172 

The current subspace-distance is: 0.00013995518384035677 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.92; acc: 0.7
Batch: 60; loss: 0.8; acc: 0.81
Batch: 80; loss: 0.99; acc: 0.72
Batch: 100; loss: 0.83; acc: 0.81
Batch: 120; loss: 0.84; acc: 0.83
Batch: 140; loss: 0.84; acc: 0.78
Batch: 160; loss: 0.81; acc: 0.78
Batch: 180; loss: 0.86; acc: 0.8
Batch: 200; loss: 0.9; acc: 0.75
Batch: 220; loss: 0.92; acc: 0.7
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.68; acc: 0.88
Batch: 280; loss: 0.97; acc: 0.73
Batch: 300; loss: 0.81; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.79; acc: 0.8
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.66; acc: 0.88
Batch: 400; loss: 0.9; acc: 0.77
Batch: 420; loss: 0.8; acc: 0.83
Batch: 440; loss: 0.75; acc: 0.88
Batch: 460; loss: 0.74; acc: 0.86
Batch: 480; loss: 0.94; acc: 0.75
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 0.8; acc: 0.81
Batch: 540; loss: 0.9; acc: 0.78
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.96; acc: 0.7
Batch: 600; loss: 0.81; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 1.04; acc: 0.73
Batch: 720; loss: 1.0; acc: 0.75
Batch: 740; loss: 0.81; acc: 0.8
Batch: 760; loss: 0.76; acc: 0.86
Batch: 780; loss: 0.68; acc: 0.89
Train Epoch over. train_loss: 0.82; train_accuracy: 0.8 

0.0001524020335637033
0.00014651827223133296
Batch: 0; loss: 0.82; acc: 0.81
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 0.7383151346710837; val_accuracy: 0.8282245222929936 

The current subspace-distance is: 0.00014651827223133296 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.86; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.66; acc: 0.89
Batch: 160; loss: 0.74; acc: 0.8
Batch: 180; loss: 0.87; acc: 0.8
Batch: 200; loss: 1.02; acc: 0.78
Batch: 220; loss: 0.71; acc: 0.89
Batch: 240; loss: 0.88; acc: 0.75
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.81; acc: 0.8
Batch: 320; loss: 0.83; acc: 0.77
Batch: 340; loss: 0.64; acc: 0.92
Batch: 360; loss: 0.86; acc: 0.7
Batch: 380; loss: 0.77; acc: 0.78
Batch: 400; loss: 0.88; acc: 0.81
Batch: 420; loss: 0.83; acc: 0.78
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.86; acc: 0.8
Batch: 480; loss: 0.74; acc: 0.83
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 0.76; acc: 0.72
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.76; acc: 0.84
Batch: 580; loss: 0.82; acc: 0.77
Batch: 600; loss: 0.65; acc: 0.89
Batch: 620; loss: 0.8; acc: 0.83
Batch: 640; loss: 0.77; acc: 0.78
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.75; acc: 0.84
Batch: 720; loss: 0.85; acc: 0.8
Batch: 740; loss: 0.93; acc: 0.75
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.89; acc: 0.84
Train Epoch over. train_loss: 0.79; train_accuracy: 0.8 

0.00016028189565986395
0.00015361167606897652
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.85; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 0.59; acc: 0.91
Val Epoch over. val_loss: 0.7119174238982474; val_accuracy: 0.8397691082802548 

The current subspace-distance is: 0.00015361167606897652 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.82; acc: 0.8
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.97; acc: 0.7
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.73; acc: 0.84
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.77; acc: 0.81
Batch: 200; loss: 0.72; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.88; acc: 0.75
Batch: 280; loss: 0.72; acc: 0.8
Batch: 300; loss: 0.7; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.88
Batch: 340; loss: 0.79; acc: 0.78
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 1.0; acc: 0.72
Batch: 400; loss: 0.95; acc: 0.73
Batch: 420; loss: 0.85; acc: 0.77
Batch: 440; loss: 0.93; acc: 0.72
Batch: 460; loss: 0.82; acc: 0.77
Batch: 480; loss: 0.89; acc: 0.75
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.93; acc: 0.7
Batch: 540; loss: 0.93; acc: 0.75
Batch: 560; loss: 0.81; acc: 0.81
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.66; acc: 0.89
Batch: 640; loss: 0.81; acc: 0.78
Batch: 660; loss: 0.79; acc: 0.81
Batch: 680; loss: 0.81; acc: 0.81
Batch: 700; loss: 0.73; acc: 0.88
Batch: 720; loss: 0.92; acc: 0.7
Batch: 740; loss: 0.86; acc: 0.78
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.78; train_accuracy: 0.81 

0.00016400965978391469
0.0001567596336826682
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.94
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.7
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.7092790248667359; val_accuracy: 0.8369824840764332 

The current subspace-distance is: 0.0001567596336826682 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.78
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.73; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.93; acc: 0.72
Batch: 200; loss: 0.83; acc: 0.78
Batch: 220; loss: 0.81; acc: 0.83
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 0.68; acc: 0.88
Batch: 300; loss: 0.92; acc: 0.75
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.77; acc: 0.8
Batch: 360; loss: 0.91; acc: 0.67
Batch: 380; loss: 0.71; acc: 0.81
Batch: 400; loss: 0.8; acc: 0.78
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.79; acc: 0.78
Batch: 460; loss: 0.59; acc: 0.92
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.86
Batch: 520; loss: 0.85; acc: 0.78
Batch: 540; loss: 0.76; acc: 0.8
Batch: 560; loss: 0.87; acc: 0.73
Batch: 580; loss: 0.8; acc: 0.78
Batch: 600; loss: 0.8; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.78
Batch: 640; loss: 0.71; acc: 0.86
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 0.87; acc: 0.73
Batch: 700; loss: 0.74; acc: 0.86
Batch: 720; loss: 0.79; acc: 0.77
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.8; acc: 0.75
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.0001665635936660692
0.0001600991381565109
Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.7006879784878651; val_accuracy: 0.8415605095541401 

The current subspace-distance is: 0.0001600991381565109 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.73
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.73
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.81
Batch: 160; loss: 0.69; acc: 0.88
Batch: 180; loss: 0.83; acc: 0.78
Batch: 200; loss: 0.86; acc: 0.8
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.82; acc: 0.72
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.78
Batch: 380; loss: 0.73; acc: 0.81
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.85; acc: 0.8
Batch: 460; loss: 0.59; acc: 0.89
Batch: 480; loss: 0.77; acc: 0.84
Batch: 500; loss: 0.79; acc: 0.81
Batch: 520; loss: 0.84; acc: 0.8
Batch: 540; loss: 0.79; acc: 0.81
Batch: 560; loss: 0.7; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.77; acc: 0.8
Batch: 640; loss: 0.75; acc: 0.81
Batch: 660; loss: 0.79; acc: 0.8
Batch: 680; loss: 0.75; acc: 0.75
Batch: 700; loss: 0.76; acc: 0.89
Batch: 720; loss: 0.72; acc: 0.8
Batch: 740; loss: 0.71; acc: 0.88
Batch: 760; loss: 0.73; acc: 0.86
Batch: 780; loss: 0.81; acc: 0.75
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00016885185323189944
0.00016641165711916983
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.58; acc: 0.91
Val Epoch over. val_loss: 0.6990252678181715; val_accuracy: 0.838077229299363 

The current subspace-distance is: 0.00016641165711916983 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.87; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.78
Batch: 200; loss: 0.64; acc: 0.94
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.9; acc: 0.77
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.7; acc: 0.86
Batch: 300; loss: 0.84; acc: 0.77
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.8
Batch: 360; loss: 0.6; acc: 0.91
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.93; acc: 0.73
Batch: 440; loss: 0.67; acc: 0.86
Batch: 460; loss: 0.93; acc: 0.75
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.72; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.84
Batch: 540; loss: 0.86; acc: 0.8
Batch: 560; loss: 0.88; acc: 0.78
Batch: 580; loss: 0.73; acc: 0.86
Batch: 600; loss: 0.74; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.73
Batch: 640; loss: 0.88; acc: 0.77
Batch: 660; loss: 0.78; acc: 0.78
Batch: 680; loss: 0.77; acc: 0.83
Batch: 700; loss: 0.79; acc: 0.72
Batch: 720; loss: 0.83; acc: 0.77
Batch: 740; loss: 0.94; acc: 0.72
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.83; acc: 0.77
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00017030521121341735
0.000164491415489465
Batch: 0; loss: 0.79; acc: 0.8
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.57; acc: 0.89
Val Epoch over. val_loss: 0.6906932958751727; val_accuracy: 0.8416600318471338 

The current subspace-distance is: 0.000164491415489465 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.72; acc: 0.88
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.82; acc: 0.73
Batch: 220; loss: 0.8; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.77; acc: 0.84
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.79; acc: 0.77
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.8; acc: 0.83
Batch: 580; loss: 0.83; acc: 0.83
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.65; acc: 0.86
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.94
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.88; acc: 0.73
Batch: 740; loss: 0.85; acc: 0.77
Batch: 760; loss: 0.93; acc: 0.73
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00017518791719339788
0.0001689703785814345
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.91
Val Epoch over. val_loss: 0.6742690835788752; val_accuracy: 0.84375 

The current subspace-distance is: 0.0001689703785814345 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.86; acc: 0.75
Batch: 100; loss: 0.83; acc: 0.8
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.9; acc: 0.72
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 0.97; acc: 0.73
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.91; acc: 0.78
Batch: 300; loss: 0.87; acc: 0.69
Batch: 320; loss: 0.76; acc: 0.77
Batch: 340; loss: 0.54; acc: 0.94
Batch: 360; loss: 0.61; acc: 0.89
Batch: 380; loss: 0.69; acc: 0.83
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.87; acc: 0.73
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.81; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.75; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.92
Batch: 620; loss: 0.84; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.65; acc: 0.92
Batch: 680; loss: 0.9; acc: 0.78
Batch: 700; loss: 0.69; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.86
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.76; acc: 0.75
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00017689986270852387
0.00017065965221263468
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.91
Val Epoch over. val_loss: 0.6736364188087973; val_accuracy: 0.8432523885350318 

The current subspace-distance is: 0.00017065965221263468 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.79; acc: 0.77
Batch: 100; loss: 0.81; acc: 0.77
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.71; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.78; acc: 0.8
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 0.85; acc: 0.69
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.92; acc: 0.72
Batch: 400; loss: 0.73; acc: 0.84
Batch: 420; loss: 0.82; acc: 0.78
Batch: 440; loss: 0.82; acc: 0.8
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.73; acc: 0.78
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.77; acc: 0.84
Batch: 620; loss: 0.75; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.89
Batch: 680; loss: 0.74; acc: 0.83
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.67; acc: 0.88
Batch: 740; loss: 0.82; acc: 0.8
Batch: 760; loss: 0.81; acc: 0.81
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.0001794443669496104
0.00017203677271027118
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.91
Val Epoch over. val_loss: 0.6764215291685359; val_accuracy: 0.8427547770700637 

The current subspace-distance is: 0.00017203677271027118 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.75; acc: 0.81
Batch: 180; loss: 0.72; acc: 0.81
Batch: 200; loss: 0.64; acc: 0.88
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.77; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.94
Batch: 400; loss: 0.7; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.8
Batch: 520; loss: 0.73; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.73; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.69; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.83; acc: 0.78
Batch: 720; loss: 0.92; acc: 0.7
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.0001798077573766932
0.0001737795100780204
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.89
Val Epoch over. val_loss: 0.6662432264750171; val_accuracy: 0.8420581210191083 

The current subspace-distance is: 0.0001737795100780204 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.65; acc: 0.84
Batch: 200; loss: 0.8; acc: 0.78
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.75; acc: 0.88
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.78; acc: 0.81
Batch: 340; loss: 0.77; acc: 0.81
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.82; acc: 0.77
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.6; acc: 0.88
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.92; acc: 0.67
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.82; acc: 0.77
Batch: 720; loss: 0.68; acc: 0.88
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.00018466016626916826
0.0001772851828718558
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6597358128827089; val_accuracy: 0.841062898089172 

The current subspace-distance is: 0.0001772851828718558 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.54; acc: 0.91
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 0.76; acc: 0.84
Batch: 180; loss: 0.77; acc: 0.83
Batch: 200; loss: 0.81; acc: 0.77
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.82; acc: 0.83
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.81; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.87; acc: 0.69
Batch: 480; loss: 0.73; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.94
Batch: 560; loss: 0.81; acc: 0.78
Batch: 580; loss: 0.76; acc: 0.78
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.8
Batch: 660; loss: 0.79; acc: 0.81
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.77; acc: 0.78
Batch: 720; loss: 0.68; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 0.71; acc: 0.84
Batch: 780; loss: 0.7; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.0001833962887758389
0.00017916249635163695
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.87; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6527734751913957; val_accuracy: 0.8453423566878981 

The current subspace-distance is: 0.00017916249635163695 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.88
Batch: 80; loss: 0.86; acc: 0.75
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.92
Batch: 140; loss: 0.68; acc: 0.88
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.79; acc: 0.77
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.9; acc: 0.72
Batch: 280; loss: 0.77; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.91
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.75; acc: 0.83
Batch: 380; loss: 0.82; acc: 0.78
Batch: 400; loss: 0.8; acc: 0.8
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.79; acc: 0.83
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.81; acc: 0.73
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.77; acc: 0.75
Batch: 700; loss: 0.65; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.89; acc: 0.83
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00018791438196785748
0.00018020001880358905
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6552486594315547; val_accuracy: 0.8447452229299363 

The current subspace-distance is: 0.00018020001880358905 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.89
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.75
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.73; acc: 0.81
Batch: 180; loss: 0.9; acc: 0.78
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.64; acc: 0.86
Batch: 240; loss: 0.69; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.88
Batch: 280; loss: 0.78; acc: 0.8
Batch: 300; loss: 0.6; acc: 0.91
Batch: 320; loss: 0.68; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.91
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.81; acc: 0.78
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.85; acc: 0.78
Batch: 520; loss: 0.82; acc: 0.8
Batch: 540; loss: 0.78; acc: 0.78
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.83; acc: 0.75
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.87; acc: 0.75
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.76; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00018865724268835038
0.00018165065557695925
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6429181970228814; val_accuracy: 0.8467356687898089 

The current subspace-distance is: 0.00018165065557695925 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.84; acc: 0.72
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.71; acc: 0.81
Batch: 180; loss: 0.75; acc: 0.78
Batch: 200; loss: 0.77; acc: 0.8
Batch: 220; loss: 0.82; acc: 0.83
Batch: 240; loss: 0.77; acc: 0.8
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.68; acc: 0.89
Batch: 300; loss: 0.71; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.8; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.89
Batch: 400; loss: 0.8; acc: 0.8
Batch: 420; loss: 0.81; acc: 0.78
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.69; acc: 0.78
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.73; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.74; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.81
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.82; acc: 0.78
Batch: 700; loss: 0.9; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.81
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.79; acc: 0.8
Batch: 780; loss: 0.87; acc: 0.75
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00018752868345472962
0.0001823526108637452
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6566558301828469; val_accuracy: 0.8436504777070064 

The current subspace-distance is: 0.0001823526108637452 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.89
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.91
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.78; acc: 0.77
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.76; acc: 0.83
Batch: 260; loss: 0.8; acc: 0.7
Batch: 280; loss: 0.8; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.86
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.66; acc: 0.84
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.79; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.86; acc: 0.8
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.82; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.85; acc: 0.75
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.75; acc: 0.78
Batch: 620; loss: 0.78; acc: 0.8
Batch: 640; loss: 0.73; acc: 0.75
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.86; acc: 0.72
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.92; acc: 0.75
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00019118910131510347
0.00018408124742563814
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6473387565202774; val_accuracy: 0.8442476114649682 

The current subspace-distance is: 0.00018408124742563814 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.84; acc: 0.8
Batch: 180; loss: 0.86; acc: 0.7
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 0.8; acc: 0.77
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.78; acc: 0.75
Batch: 280; loss: 0.76; acc: 0.81
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.68; acc: 0.78
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.86; acc: 0.78
Batch: 440; loss: 0.86; acc: 0.77
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.71; acc: 0.88
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 0.63; acc: 0.91
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 0.55; acc: 0.95
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.89; acc: 0.77
Batch: 740; loss: 0.76; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.84; acc: 0.77
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00018780186655931175
0.00018033960077445954
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6421795386797303; val_accuracy: 0.8446457006369427 

The current subspace-distance is: 0.00018033960077445954 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.87; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.92
Batch: 40; loss: 0.66; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.91
Batch: 140; loss: 0.6; acc: 0.89
Batch: 160; loss: 0.71; acc: 0.81
Batch: 180; loss: 0.61; acc: 0.92
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.57; acc: 0.92
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.87; acc: 0.78
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.88; acc: 0.77
Batch: 380; loss: 0.9; acc: 0.73
Batch: 400; loss: 0.74; acc: 0.81
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.77; acc: 0.8
Batch: 480; loss: 0.72; acc: 0.83
Batch: 500; loss: 0.69; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.6; acc: 0.91
Batch: 600; loss: 0.76; acc: 0.78
Batch: 620; loss: 0.78; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.67; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 1.04; acc: 0.77
Batch: 720; loss: 0.76; acc: 0.77
Batch: 740; loss: 0.9; acc: 0.77
Batch: 760; loss: 0.66; acc: 0.89
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00018921506125479937
0.0001833470887504518
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.88
Val Epoch over. val_loss: 0.6515613465931764; val_accuracy: 0.8411624203821656 

The current subspace-distance is: 0.0001833470887504518 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.87; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.8; acc: 0.78
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.63; acc: 0.89
Batch: 180; loss: 0.81; acc: 0.8
Batch: 200; loss: 0.9; acc: 0.77
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.6; acc: 0.89
Batch: 260; loss: 0.7; acc: 0.86
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.73; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.84; acc: 0.73
Batch: 460; loss: 0.77; acc: 0.78
Batch: 480; loss: 0.81; acc: 0.75
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.89
Batch: 540; loss: 0.9; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.87; acc: 0.7
Batch: 600; loss: 0.74; acc: 0.77
Batch: 620; loss: 0.63; acc: 0.88
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.82; acc: 0.81
Batch: 680; loss: 0.67; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.76; acc: 0.73
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00019030741532333195
0.000182145566213876
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6422251301586248; val_accuracy: 0.8476313694267515 

The current subspace-distance is: 0.000182145566213876 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.84
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.92
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.73; acc: 0.78
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.76; acc: 0.8
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.67; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.73; acc: 0.78
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.81; acc: 0.78
Batch: 420; loss: 0.84; acc: 0.73
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.81
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.86
Batch: 600; loss: 0.79; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.83
Batch: 700; loss: 0.64; acc: 0.88
Batch: 720; loss: 0.85; acc: 0.73
Batch: 740; loss: 0.72; acc: 0.8
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.66; acc: 0.88
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.0001923949457705021
0.00018618583271745592
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.6441817532299431; val_accuracy: 0.8446457006369427 

The current subspace-distance is: 0.00018618583271745592 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.66; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.78; acc: 0.8
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.64; acc: 0.86
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.63; acc: 0.89
Batch: 420; loss: 0.72; acc: 0.86
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.81
Batch: 520; loss: 0.75; acc: 0.81
Batch: 540; loss: 0.77; acc: 0.73
Batch: 560; loss: 0.78; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.75
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.76; acc: 0.8
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.81
Batch: 680; loss: 0.62; acc: 0.91
Batch: 700; loss: 0.72; acc: 0.88
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.71; acc: 0.88
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00019279835396446288
0.00018684864335227758
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6407482369690184; val_accuracy: 0.8452428343949044 

The current subspace-distance is: 0.00018684864335227758 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.76; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.7; acc: 0.86
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.74; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.94
Batch: 240; loss: 0.88; acc: 0.73
Batch: 260; loss: 0.77; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.88
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.89
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.82; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.82; acc: 0.8
Batch: 540; loss: 0.62; acc: 0.88
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.73; acc: 0.77
Batch: 600; loss: 0.71; acc: 0.84
Batch: 620; loss: 0.92; acc: 0.69
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.75; acc: 0.83
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 0.94; acc: 0.77
Batch: 740; loss: 0.52; acc: 0.94
Batch: 760; loss: 0.77; acc: 0.77
Batch: 780; loss: 0.75; acc: 0.8
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00019494116713758558
0.00018931129307020456
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6405248598308321; val_accuracy: 0.8479299363057324 

The current subspace-distance is: 0.00018931129307020456 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_13_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6833983485794441

The number of parameters is: 259251

The number of individual parameters is:

14
252
14
14
21
38514
21
21
41
112791
41
41
64
102336
64
64
4096
64
640
10
64
64

nonzero elements in E: 77775292
elements in E: 77775300
fraction nonzero: 0.9999998971395803
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.12
Batch: 20; loss: 2.08; acc: 0.34
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.83; acc: 0.41
Batch: 80; loss: 1.61; acc: 0.62
Batch: 100; loss: 1.56; acc: 0.69
Batch: 120; loss: 1.64; acc: 0.47
Batch: 140; loss: 1.6; acc: 0.61
Batch: 160; loss: 1.49; acc: 0.67
Batch: 180; loss: 1.42; acc: 0.72
Batch: 200; loss: 1.38; acc: 0.72
Batch: 220; loss: 1.44; acc: 0.66
Batch: 240; loss: 1.46; acc: 0.69
Batch: 260; loss: 1.37; acc: 0.67
Batch: 280; loss: 1.45; acc: 0.62
Batch: 300; loss: 1.4; acc: 0.72
Batch: 320; loss: 1.32; acc: 0.72
Batch: 340; loss: 1.2; acc: 0.8
Batch: 360; loss: 1.27; acc: 0.73
Batch: 380; loss: 1.26; acc: 0.72
Batch: 400; loss: 1.39; acc: 0.66
Batch: 420; loss: 1.17; acc: 0.8
Batch: 440; loss: 1.17; acc: 0.8
Batch: 460; loss: 1.11; acc: 0.84
Batch: 480; loss: 1.14; acc: 0.78
Batch: 500; loss: 1.16; acc: 0.77
Batch: 520; loss: 1.08; acc: 0.86
Batch: 540; loss: 1.16; acc: 0.77
Batch: 560; loss: 1.27; acc: 0.69
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.25; acc: 0.75
Batch: 620; loss: 1.18; acc: 0.69
Batch: 640; loss: 1.15; acc: 0.78
Batch: 660; loss: 1.17; acc: 0.75
Batch: 680; loss: 1.35; acc: 0.61
Batch: 700; loss: 1.15; acc: 0.75
Batch: 720; loss: 0.98; acc: 0.81
Batch: 740; loss: 1.1; acc: 0.77
Batch: 760; loss: 1.14; acc: 0.77
Batch: 780; loss: 1.12; acc: 0.77
Train Epoch over. train_loss: 1.34; train_accuracy: 0.7 

6.078722435631789e-05
5.6876793678384274e-05
Batch: 0; loss: 1.16; acc: 0.73
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 0.77; acc: 0.91
Batch: 60; loss: 1.02; acc: 0.83
Batch: 80; loss: 0.89; acc: 0.92
Batch: 100; loss: 1.02; acc: 0.88
Batch: 120; loss: 1.13; acc: 0.75
Batch: 140; loss: 0.84; acc: 0.86
Val Epoch over. val_loss: 1.0247475106245394; val_accuracy: 0.8225517515923567 

The current subspace-distance is: 5.6876793678384274e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.88
Batch: 20; loss: 1.01; acc: 0.81
Batch: 40; loss: 0.99; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.86
Batch: 80; loss: 0.94; acc: 0.88
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 0.99; acc: 0.8
Batch: 140; loss: 1.03; acc: 0.84
Batch: 160; loss: 0.93; acc: 0.89
Batch: 180; loss: 1.08; acc: 0.75
Batch: 200; loss: 1.02; acc: 0.8
Batch: 220; loss: 1.05; acc: 0.8
Batch: 240; loss: 0.98; acc: 0.81
Batch: 260; loss: 1.06; acc: 0.73
Batch: 280; loss: 0.79; acc: 0.91
Batch: 300; loss: 0.99; acc: 0.83
Batch: 320; loss: 1.12; acc: 0.7
Batch: 340; loss: 0.87; acc: 0.89
Batch: 360; loss: 1.09; acc: 0.78
Batch: 380; loss: 1.0; acc: 0.81
Batch: 400; loss: 0.98; acc: 0.77
Batch: 420; loss: 0.88; acc: 0.84
Batch: 440; loss: 1.01; acc: 0.75
Batch: 460; loss: 0.92; acc: 0.83
Batch: 480; loss: 0.9; acc: 0.77
Batch: 500; loss: 0.9; acc: 0.91
Batch: 520; loss: 0.96; acc: 0.78
Batch: 540; loss: 0.94; acc: 0.78
Batch: 560; loss: 1.03; acc: 0.77
Batch: 580; loss: 1.01; acc: 0.86
Batch: 600; loss: 0.85; acc: 0.88
Batch: 620; loss: 0.88; acc: 0.86
Batch: 640; loss: 0.97; acc: 0.86
Batch: 660; loss: 1.13; acc: 0.75
Batch: 680; loss: 0.94; acc: 0.83
Batch: 700; loss: 1.0; acc: 0.81
Batch: 720; loss: 1.04; acc: 0.77
Batch: 740; loss: 0.92; acc: 0.81
Batch: 760; loss: 0.88; acc: 0.81
Batch: 780; loss: 0.93; acc: 0.81
Train Epoch over. train_loss: 0.97; train_accuracy: 0.81 

8.346699178218842e-05
7.872350397519767e-05
Batch: 0; loss: 0.92; acc: 0.84
Batch: 20; loss: 0.98; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.95
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.69; acc: 0.94
Batch: 100; loss: 0.84; acc: 0.91
Batch: 120; loss: 1.01; acc: 0.78
Batch: 140; loss: 0.65; acc: 0.92
Val Epoch over. val_loss: 0.841185174170573; val_accuracy: 0.8501194267515924 

The current subspace-distance is: 7.872350397519767e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.78
Batch: 20; loss: 0.87; acc: 0.88
Batch: 40; loss: 0.96; acc: 0.81
Batch: 60; loss: 0.84; acc: 0.89
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.93; acc: 0.77
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.81; acc: 0.84
Batch: 160; loss: 0.79; acc: 0.83
Batch: 180; loss: 0.88; acc: 0.8
Batch: 200; loss: 0.9; acc: 0.86
Batch: 220; loss: 0.9; acc: 0.83
Batch: 240; loss: 0.99; acc: 0.81
Batch: 260; loss: 0.8; acc: 0.88
Batch: 280; loss: 0.86; acc: 0.88
Batch: 300; loss: 0.83; acc: 0.88
Batch: 320; loss: 0.84; acc: 0.86
Batch: 340; loss: 0.87; acc: 0.8
Batch: 360; loss: 0.95; acc: 0.83
Batch: 380; loss: 0.88; acc: 0.83
Batch: 400; loss: 0.86; acc: 0.81
Batch: 420; loss: 1.04; acc: 0.8
Batch: 440; loss: 0.8; acc: 0.83
Batch: 460; loss: 0.86; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.84
Batch: 500; loss: 0.87; acc: 0.81
Batch: 520; loss: 0.89; acc: 0.84
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.98; acc: 0.75
Batch: 580; loss: 0.95; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.88
Batch: 620; loss: 0.8; acc: 0.84
Batch: 640; loss: 0.81; acc: 0.86
Batch: 660; loss: 0.71; acc: 0.88
Batch: 680; loss: 0.79; acc: 0.86
Batch: 700; loss: 0.86; acc: 0.83
Batch: 720; loss: 0.87; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.84
Batch: 760; loss: 0.8; acc: 0.88
Batch: 780; loss: 0.75; acc: 0.88
Train Epoch over. train_loss: 0.86; train_accuracy: 0.84 

9.789005707716569e-05
9.325439896201715e-05
Batch: 0; loss: 0.82; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.8
Batch: 40; loss: 0.54; acc: 0.95
Batch: 60; loss: 0.76; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.92
Batch: 100; loss: 0.75; acc: 0.95
Batch: 120; loss: 0.93; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.92
Val Epoch over. val_loss: 0.7722387382179309; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 9.325439896201715e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.91
Batch: 20; loss: 0.83; acc: 0.83
Batch: 40; loss: 0.85; acc: 0.75
Batch: 60; loss: 0.74; acc: 0.86
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.84
Batch: 140; loss: 0.76; acc: 0.81
Batch: 160; loss: 0.81; acc: 0.89
Batch: 180; loss: 0.83; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.86
Batch: 220; loss: 0.74; acc: 0.86
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.73; acc: 0.86
Batch: 280; loss: 0.67; acc: 0.91
Batch: 300; loss: 0.86; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.91
Batch: 340; loss: 0.76; acc: 0.91
Batch: 360; loss: 0.75; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.91
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.89
Batch: 440; loss: 0.82; acc: 0.81
Batch: 460; loss: 0.96; acc: 0.8
Batch: 480; loss: 0.77; acc: 0.89
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.81; acc: 0.86
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 0.75; acc: 0.81
Batch: 580; loss: 0.78; acc: 0.83
Batch: 600; loss: 0.91; acc: 0.8
Batch: 620; loss: 0.8; acc: 0.83
Batch: 640; loss: 0.75; acc: 0.89
Batch: 660; loss: 0.89; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.91
Batch: 700; loss: 0.76; acc: 0.89
Batch: 720; loss: 0.87; acc: 0.8
Batch: 740; loss: 0.86; acc: 0.83
Batch: 760; loss: 0.85; acc: 0.83
Batch: 780; loss: 0.81; acc: 0.83
Train Epoch over. train_loss: 0.79; train_accuracy: 0.84 

0.00011245197674725205
0.000107873413071502
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.92; acc: 0.81
Batch: 40; loss: 0.49; acc: 0.97
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.94
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.94
Val Epoch over. val_loss: 0.7165365856923874; val_accuracy: 0.8696257961783439 

The current subspace-distance is: 0.000107873413071502 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.91
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.77; acc: 0.84
Batch: 160; loss: 0.67; acc: 0.91
Batch: 180; loss: 0.75; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.88
Batch: 260; loss: 0.72; acc: 0.89
Batch: 280; loss: 0.7; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.89
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.82; acc: 0.78
Batch: 360; loss: 0.8; acc: 0.83
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.82; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.89
Batch: 440; loss: 0.74; acc: 0.86
Batch: 460; loss: 0.98; acc: 0.75
Batch: 480; loss: 0.72; acc: 0.88
Batch: 500; loss: 0.77; acc: 0.83
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.91
Batch: 560; loss: 0.76; acc: 0.84
Batch: 580; loss: 0.74; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.86
Batch: 620; loss: 0.57; acc: 0.89
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.78; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.86
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.86
Batch: 740; loss: 0.68; acc: 0.89
Batch: 760; loss: 0.77; acc: 0.88
Batch: 780; loss: 0.79; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.85 

0.00012542166223283857
0.00012040408182656392
Batch: 0; loss: 0.71; acc: 0.89
Batch: 20; loss: 0.89; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.97
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.94
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6552575228700213; val_accuracy: 0.8793789808917197 

The current subspace-distance is: 0.00012040408182656392 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.95
Batch: 20; loss: 0.76; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.84; acc: 0.8
Batch: 160; loss: 0.54; acc: 0.94
Batch: 180; loss: 0.86; acc: 0.83
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.75; acc: 0.81
Batch: 260; loss: 0.57; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.91
Batch: 300; loss: 0.79; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.94
Batch: 340; loss: 0.68; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.89
Batch: 400; loss: 0.68; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.89
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.76; acc: 0.83
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.65; acc: 0.89
Batch: 680; loss: 0.8; acc: 0.81
Batch: 700; loss: 0.78; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.92
Batch: 760; loss: 0.67; acc: 0.88
Batch: 780; loss: 0.78; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.86 

0.00013493324513547122
0.00013041449710726738
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.6152532265824118; val_accuracy: 0.8815684713375797 

The current subspace-distance is: 0.00013041449710726738 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.77
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.57; acc: 0.92
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.92
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.92
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.71; acc: 0.8
Batch: 360; loss: 0.46; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.95
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.88
Batch: 500; loss: 0.76; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.88
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.97
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.92
Batch: 700; loss: 0.68; acc: 0.89
Batch: 720; loss: 0.72; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.91
Batch: 760; loss: 0.8; acc: 0.8
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.65; train_accuracy: 0.87 

0.00014760161866433918
0.00014033145271241665
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.97
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5731310506535184; val_accuracy: 0.8853503184713376 

The current subspace-distance is: 0.00014033145271241665 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.58; acc: 0.89
Batch: 220; loss: 0.55; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.92
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.65; acc: 0.89
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.92
Batch: 460; loss: 0.65; acc: 0.91
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.86
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.78
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.73; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.62; train_accuracy: 0.87 

0.0001567410654388368
0.00014938376261852682
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.95
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5497692971472528; val_accuracy: 0.888734076433121 

The current subspace-distance is: 0.00014938376261852682 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.58; acc: 0.91
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.47; acc: 0.95
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.81; acc: 0.78
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.95
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.48; acc: 0.94
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.91
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.94
Batch: 620; loss: 0.56; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.6; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.92
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.59; train_accuracy: 0.87 

0.00016544577374588698
0.00015921832527965307
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.97
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.5191607949840036; val_accuracy: 0.8909235668789809 

The current subspace-distance is: 0.00015921832527965307 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.92
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.97
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.92
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.89
Batch: 420; loss: 0.79; acc: 0.77
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.95
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.84
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.8; acc: 0.77
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.66; acc: 0.84
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.0001736020640237257
0.00016649632016196847
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.97
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.49734137430312525; val_accuracy: 0.8916202229299363 

The current subspace-distance is: 0.00016649632016196847 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.52; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.95
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.5; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.6; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.0001757527788868174
0.000168447702890262
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.97
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.49120123807791694; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 0.000168447702890262 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.94
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.73; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.77; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.73; acc: 0.78
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.95
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.94
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.00017656784621067345
0.00017181671864818782
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.97
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4851069865143223; val_accuracy: 0.8950039808917197 

The current subspace-distance is: 0.00017181671864818782 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.95
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.67; acc: 0.8
Batch: 280; loss: 0.59; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.81
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.4; acc: 0.95
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.95
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.00018021318828687072
0.00017216736159753054
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.98
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4851806580450884; val_accuracy: 0.8932125796178344 

The current subspace-distance is: 0.00017216736159753054 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.54; acc: 0.91
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.94
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.64; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.00018092124082613736
0.00017365555686410517
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.97
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.47428617659647754; val_accuracy: 0.8972929936305732 

The current subspace-distance is: 0.00017365555686410517 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.56; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.52; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.91
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.58; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.94
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.97
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00018361520778853446
0.00017543726426083595
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.97
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4714184775473965; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 0.00017543726426083595 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.95
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.81
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00018696924962569028
0.0001780843158485368
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.98
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4693094347692599; val_accuracy: 0.8969944267515924 

The current subspace-distance is: 0.0001780843158485368 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.95
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.7; acc: 0.78
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00018748863658402115
0.00018140101747121662
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.97
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.46538962566169206; val_accuracy: 0.8963972929936306 

The current subspace-distance is: 0.00018140101747121662 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.92
Batch: 260; loss: 0.75; acc: 0.81
Batch: 280; loss: 0.34; acc: 0.97
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.78
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.94
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.65; acc: 0.81
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00018843873112928122
0.00018141315376851708
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.97
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.45738182961940765; val_accuracy: 0.8981886942675159 

The current subspace-distance is: 0.00018141315376851708 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.95
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.95
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.97
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.63; acc: 0.8
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00019177547073923051
0.00018404831644147635
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.45354449758484106; val_accuracy: 0.8973925159235668 

The current subspace-distance is: 0.00018404831644147635 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.95
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.74; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.78
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00019337827689014375
0.00018583080964162946
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4492608060123055; val_accuracy: 0.8993829617834395 

The current subspace-distance is: 0.00018583080964162946 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.66; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.72; acc: 0.73
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.000194029082194902
0.00018696545157581568
Batch: 0; loss: 0.42; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4455321035377539; val_accuracy: 0.9002786624203821 

The current subspace-distance is: 0.00018696545157581568 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.89
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.78
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.8
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001953967584995553
0.0001868562976596877
Batch: 0; loss: 0.42; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4505805824972262; val_accuracy: 0.897890127388535 

The current subspace-distance is: 0.0001868562976596877 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.94
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00019466372032184154
0.0001873829896794632
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4506367142223249; val_accuracy: 0.8986863057324841 

The current subspace-distance is: 0.0001873829896794632 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.35; acc: 0.97
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.75
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.98
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001953011960722506
0.00018850481137633324
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4444673760871219; val_accuracy: 0.9004777070063694 

The current subspace-distance is: 0.00018850481137633324 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.95
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.91
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001982491958187893
0.0001890972926048562
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.98
Val Epoch over. val_loss: 0.44797739320120233; val_accuracy: 0.8969944267515924 

The current subspace-distance is: 0.0001890972926048562 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.64; acc: 0.75
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00019550025172065943
0.00018845315207727253
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4404676231988676; val_accuracy: 0.8998805732484076 

The current subspace-distance is: 0.00018845315207727253 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.94
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001969719014596194
0.00018928461940959096
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4427776642286094; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.00018928461940959096 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.65; acc: 0.81
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00019759991846513003
0.00019099374185316265
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4378803634339837; val_accuracy: 0.9003781847133758 

The current subspace-distance is: 0.00019099374185316265 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.95
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.94
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.83
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.68; acc: 0.8
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.8
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001969259901670739
0.000189808095456101
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.44435062207234133; val_accuracy: 0.8996815286624203 

The current subspace-distance is: 0.000189808095456101 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.8
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.78
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0001996802893700078
0.00019317082478664815
Batch: 0; loss: 0.4; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.98
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4367703943495538; val_accuracy: 0.9001791401273885 

The current subspace-distance is: 0.00019317082478664815 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_13_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6833983485794441

The number of parameters is: 259251

The number of individual parameters is:

14
252
14
14
21
38514
21
21
41
112791
41
41
64
102336
64
64
4096
64
640
10
64
64

nonzero elements in E: 103700390
elements in E: 103700400
fraction nonzero: 0.9999999035683566
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.25; acc: 0.14
Batch: 20; loss: 1.94; acc: 0.44
Batch: 40; loss: 1.77; acc: 0.47
Batch: 60; loss: 1.7; acc: 0.42
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.57; acc: 0.61
Batch: 120; loss: 1.42; acc: 0.67
Batch: 140; loss: 1.32; acc: 0.75
Batch: 160; loss: 1.38; acc: 0.69
Batch: 180; loss: 1.37; acc: 0.7
Batch: 200; loss: 1.32; acc: 0.72
Batch: 220; loss: 1.26; acc: 0.8
Batch: 240; loss: 1.41; acc: 0.64
Batch: 260; loss: 1.35; acc: 0.73
Batch: 280; loss: 1.16; acc: 0.78
Batch: 300; loss: 1.18; acc: 0.78
Batch: 320; loss: 1.3; acc: 0.7
Batch: 340; loss: 1.28; acc: 0.7
Batch: 360; loss: 1.26; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.73
Batch: 400; loss: 1.14; acc: 0.8
Batch: 420; loss: 1.2; acc: 0.73
Batch: 440; loss: 1.16; acc: 0.75
Batch: 460; loss: 1.04; acc: 0.83
Batch: 480; loss: 1.15; acc: 0.75
Batch: 500; loss: 1.06; acc: 0.81
Batch: 520; loss: 1.08; acc: 0.81
Batch: 540; loss: 1.05; acc: 0.84
Batch: 560; loss: 0.99; acc: 0.81
Batch: 580; loss: 1.15; acc: 0.77
Batch: 600; loss: 1.08; acc: 0.78
Batch: 620; loss: 1.23; acc: 0.64
Batch: 640; loss: 1.08; acc: 0.75
Batch: 660; loss: 1.08; acc: 0.78
Batch: 680; loss: 1.12; acc: 0.69
Batch: 700; loss: 1.06; acc: 0.77
Batch: 720; loss: 0.99; acc: 0.83
Batch: 740; loss: 0.91; acc: 0.91
Batch: 760; loss: 1.04; acc: 0.84
Batch: 780; loss: 0.82; acc: 0.94
Train Epoch over. train_loss: 1.26; train_accuracy: 0.73 

2.4470520656905137e-05
8.336718565260526e-06
Batch: 0; loss: 1.05; acc: 0.84
Batch: 20; loss: 1.11; acc: 0.75
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.93; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.95
Batch: 100; loss: 0.93; acc: 0.86
Batch: 120; loss: 1.0; acc: 0.78
Batch: 140; loss: 0.76; acc: 0.89
Val Epoch over. val_loss: 0.9101664837758252; val_accuracy: 0.8486265923566879 

The current subspace-distance is: 8.336718565260526e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.77
Batch: 20; loss: 0.98; acc: 0.83
Batch: 40; loss: 0.92; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.88
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 0.86; acc: 0.86
Batch: 140; loss: 0.85; acc: 0.86
Batch: 160; loss: 0.93; acc: 0.83
Batch: 180; loss: 1.01; acc: 0.8
Batch: 200; loss: 0.78; acc: 0.92
Batch: 220; loss: 0.9; acc: 0.8
Batch: 240; loss: 0.88; acc: 0.78
Batch: 260; loss: 0.92; acc: 0.86
Batch: 280; loss: 0.83; acc: 0.86
Batch: 300; loss: 0.95; acc: 0.84
Batch: 320; loss: 0.89; acc: 0.8
Batch: 340; loss: 0.81; acc: 0.91
Batch: 360; loss: 0.82; acc: 0.84
Batch: 380; loss: 0.92; acc: 0.8
Batch: 400; loss: 0.8; acc: 0.81
Batch: 420; loss: 0.69; acc: 0.92
Batch: 440; loss: 0.89; acc: 0.83
Batch: 460; loss: 0.95; acc: 0.78
Batch: 480; loss: 0.78; acc: 0.86
Batch: 500; loss: 1.04; acc: 0.73
Batch: 520; loss: 0.8; acc: 0.83
Batch: 540; loss: 0.89; acc: 0.77
Batch: 560; loss: 0.75; acc: 0.88
Batch: 580; loss: 0.87; acc: 0.8
Batch: 600; loss: 0.73; acc: 0.91
Batch: 620; loss: 0.73; acc: 0.91
Batch: 640; loss: 0.83; acc: 0.83
Batch: 660; loss: 0.78; acc: 0.88
Batch: 680; loss: 0.76; acc: 0.89
Batch: 700; loss: 0.92; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.81
Batch: 740; loss: 0.71; acc: 0.88
Batch: 760; loss: 0.77; acc: 0.84
Batch: 780; loss: 0.73; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.84 

3.014071990037337e-05
1.186491317639593e-05
Batch: 0; loss: 0.84; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.49; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.91
Val Epoch over. val_loss: 0.7008460790488371; val_accuracy: 0.8793789808917197 

The current subspace-distance is: 1.186491317639593e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.95
Batch: 40; loss: 0.88; acc: 0.77
Batch: 60; loss: 0.65; acc: 0.92
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.89
Batch: 140; loss: 0.75; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.92
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.88
Batch: 220; loss: 0.72; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.94
Batch: 260; loss: 0.77; acc: 0.81
Batch: 280; loss: 0.78; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.94
Batch: 320; loss: 0.61; acc: 0.91
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.75; acc: 0.84
Batch: 380; loss: 0.64; acc: 0.94
Batch: 400; loss: 0.7; acc: 0.86
Batch: 420; loss: 0.67; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.92
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.95
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.89
Batch: 600; loss: 0.65; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.91
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.68; acc: 0.92
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.7; train_accuracy: 0.87 

3.4808050259016454e-05
1.4451248716795817e-05
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.97
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.94
Val Epoch over. val_loss: 0.5813524601565805; val_accuracy: 0.8983877388535032 

The current subspace-distance is: 1.4451248716795817e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.92
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.62; acc: 0.89
Batch: 140; loss: 0.65; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.91
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.49; acc: 0.97
Batch: 240; loss: 0.62; acc: 0.91
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.64; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.92
Batch: 360; loss: 0.69; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.94
Batch: 440; loss: 0.57; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.92
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.51; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.65; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.92
Batch: 660; loss: 0.67; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.92
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

3.8967518776189536e-05
1.7394719179719687e-05
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.95
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.92
Val Epoch over. val_loss: 0.5105095528493262; val_accuracy: 0.9026671974522293 

The current subspace-distance is: 1.7394719179719687e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.63; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.97
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.92
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.55; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.94
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.51; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.94
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

4.21822551288642e-05
1.7985548765864223e-05
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.4676435282275935; val_accuracy: 0.9019705414012739 

The current subspace-distance is: 1.7985548765864223e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.95
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.64; acc: 0.83
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

4.5054479414829984e-05
2.0479252270888537e-05
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.43391761819648134; val_accuracy: 0.9065485668789809 

The current subspace-distance is: 2.0479252270888537e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.23; acc: 1.0
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

4.802401235792786e-05
2.0941255570505746e-05
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.3965703479613468; val_accuracy: 0.9118232484076433 

The current subspace-distance is: 2.0941255570505746e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.97
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.092440915177576e-05
2.4172761186491698e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.3750168077505318; val_accuracy: 0.9143113057324841 

The current subspace-distance is: 2.4172761186491698e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.98
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.97
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.3643165301764384e-05
2.5980598366004415e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.3595901879535359; val_accuracy: 0.9190883757961783 

The current subspace-distance is: 2.5980598366004415e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

5.4734875448048115e-05
2.5158557036775164e-05
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.343807909376682; val_accuracy: 0.9202826433121019 

The current subspace-distance is: 2.5158557036775164e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.78
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.98
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.58756546524819e-05
2.5695464501040988e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3367421251193733; val_accuracy: 0.9197850318471338 

The current subspace-distance is: 2.5695464501040988e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.63; acc: 0.8
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.28; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.686246367986314e-05
2.5709130568429828e-05
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.331674635315397; val_accuracy: 0.9244625796178344 

The current subspace-distance is: 2.5709130568429828e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.78
Batch: 400; loss: 0.24; acc: 0.98
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.663128467858769e-05
2.490076622052584e-05
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.324183102720862; val_accuracy: 0.9227707006369427 

The current subspace-distance is: 2.490076622052584e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.98
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.818094723508693e-05
2.7495179892866872e-05
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.32477777546188635; val_accuracy: 0.9240644904458599 

The current subspace-distance is: 2.7495179892866872e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.55; acc: 0.8
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.845561463502236e-05
2.6001142032328062e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.32052477881027636; val_accuracy: 0.9244625796178344 

The current subspace-distance is: 2.6001142032328062e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.8
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.9948080888716504e-05
2.7112982934340835e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.32323795006533335; val_accuracy: 0.925656847133758 

The current subspace-distance is: 2.7112982934340835e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.98
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.940266419202089e-05
2.6769273972604424e-05
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.31494914151870523; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 2.6769273972604424e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.98
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.98
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.0299320466583595e-05
2.7286532713333145e-05
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3111329738786266; val_accuracy: 0.9268511146496815 

The current subspace-distance is: 2.7286532713333145e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.040628795744851e-05
2.7667854737956077e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3046066193443954; val_accuracy: 0.9280453821656051 

The current subspace-distance is: 2.7667854737956077e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.98
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.97
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.184933590702713e-05
2.8852069590357132e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.30583791491711976; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.8852069590357132e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.126546213636175e-05
2.663427403604146e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3032604326867753; val_accuracy: 0.9274482484076433 

The current subspace-distance is: 2.663427403604146e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.23; acc: 1.0
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.183861842146143e-05
2.826010859280359e-05
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3030615911172454; val_accuracy: 0.9278463375796179 

The current subspace-distance is: 2.826010859280359e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.98
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.55; acc: 0.8
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.98
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

6.210659194039181e-05
2.8950065825483762e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3028302944389878; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.8950065825483762e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.97
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.18; acc: 1.0
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.161810597404838e-05
2.8230353564140387e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.29980718496308967; val_accuracy: 0.9300358280254777 

The current subspace-distance is: 2.8230353564140387e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.18; acc: 0.98
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.29545611445792e-05
3.0007093300810084e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.2973846294887506; val_accuracy: 0.9300358280254777 

The current subspace-distance is: 3.0007093300810084e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.97
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.43; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.183194636832923e-05
2.882031003537122e-05
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.2986430913494651; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.882031003537122e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.98
Batch: 160; loss: 0.28; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.98
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.98
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.198961636982858e-05
2.875445170502644e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.2978550206134274; val_accuracy: 0.9304339171974523 

The current subspace-distance is: 2.875445170502644e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.98
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

6.30815266049467e-05
3.017349990841467e-05
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.2978145074407766; val_accuracy: 0.9281449044585988 

The current subspace-distance is: 3.017349990841467e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.241956725716591e-05
2.8325383027549833e-05
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.298432403356786; val_accuracy: 0.9297372611464968 

The current subspace-distance is: 2.8325383027549833e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.279831723077223e-05
2.809646503010299e-05
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.78
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.29984712923408313; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.809646503010299e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_13_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6833983485794441

The number of parameters is: 259251

The number of individual parameters is:

14
252
14
14
21
38514
21
21
41
112791
41
41
64
102336
64
64
4096
64
640
10
64
64

nonzero elements in E: 129625490
elements in E: 129625500
fraction nonzero: 0.9999999228546852
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 1.92; acc: 0.42
Batch: 40; loss: 1.65; acc: 0.59
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.4; acc: 0.72
Batch: 100; loss: 1.43; acc: 0.58
Batch: 120; loss: 1.5; acc: 0.62
Batch: 140; loss: 1.43; acc: 0.61
Batch: 160; loss: 1.26; acc: 0.77
Batch: 180; loss: 1.31; acc: 0.75
Batch: 200; loss: 1.19; acc: 0.78
Batch: 220; loss: 1.15; acc: 0.83
Batch: 240; loss: 1.24; acc: 0.8
Batch: 260; loss: 1.11; acc: 0.78
Batch: 280; loss: 1.14; acc: 0.84
Batch: 300; loss: 1.09; acc: 0.8
Batch: 320; loss: 1.02; acc: 0.89
Batch: 340; loss: 1.17; acc: 0.67
Batch: 360; loss: 0.99; acc: 0.78
Batch: 380; loss: 1.01; acc: 0.91
Batch: 400; loss: 0.93; acc: 0.89
Batch: 420; loss: 0.89; acc: 0.88
Batch: 440; loss: 1.02; acc: 0.84
Batch: 460; loss: 0.99; acc: 0.89
Batch: 480; loss: 0.86; acc: 0.89
Batch: 500; loss: 0.99; acc: 0.88
Batch: 520; loss: 0.89; acc: 0.88
Batch: 540; loss: 1.08; acc: 0.8
Batch: 560; loss: 0.96; acc: 0.86
Batch: 580; loss: 0.89; acc: 0.91
Batch: 600; loss: 0.9; acc: 0.84
Batch: 620; loss: 0.96; acc: 0.81
Batch: 640; loss: 0.81; acc: 0.88
Batch: 660; loss: 0.83; acc: 0.88
Batch: 680; loss: 0.98; acc: 0.77
Batch: 700; loss: 0.84; acc: 0.88
Batch: 720; loss: 0.8; acc: 0.89
Batch: 740; loss: 0.85; acc: 0.84
Batch: 760; loss: 0.84; acc: 0.91
Batch: 780; loss: 0.85; acc: 0.83
Train Epoch over. train_loss: 1.11; train_accuracy: 0.78 

2.463190867274534e-05
8.84415021573659e-06
Batch: 0; loss: 0.77; acc: 0.94
Batch: 20; loss: 0.96; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.98
Batch: 60; loss: 0.81; acc: 0.83
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.92
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.92
Val Epoch over. val_loss: 0.7629650861594328; val_accuracy: 0.8767914012738853 

The current subspace-distance is: 8.84415021573659e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.91
Batch: 20; loss: 0.85; acc: 0.86
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 0.93; acc: 0.84
Batch: 80; loss: 0.86; acc: 0.86
Batch: 100; loss: 0.87; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.89
Batch: 140; loss: 0.72; acc: 0.94
Batch: 160; loss: 0.82; acc: 0.84
Batch: 180; loss: 0.79; acc: 0.86
Batch: 200; loss: 0.75; acc: 0.86
Batch: 220; loss: 0.77; acc: 0.86
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.86; acc: 0.8
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.88
Batch: 360; loss: 0.69; acc: 0.92
Batch: 380; loss: 0.63; acc: 0.94
Batch: 400; loss: 0.76; acc: 0.86
Batch: 420; loss: 0.7; acc: 0.89
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.66; acc: 0.91
Batch: 480; loss: 0.71; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.88
Batch: 520; loss: 0.83; acc: 0.75
Batch: 540; loss: 0.72; acc: 0.88
Batch: 560; loss: 0.68; acc: 0.94
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.89
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.89
Batch: 760; loss: 0.72; acc: 0.86
Batch: 780; loss: 0.92; acc: 0.75
Train Epoch over. train_loss: 0.73; train_accuracy: 0.87 

2.986086838063784e-05
1.2565315046231262e-05
Batch: 0; loss: 0.6; acc: 0.92
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.6102857179702468; val_accuracy: 0.8934116242038217 

The current subspace-distance is: 1.2565315046231262e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.91
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.76; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.61; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.92
Batch: 400; loss: 0.59; acc: 0.91
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.77; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.81
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.94
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.71; acc: 0.88
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.95
Batch: 720; loss: 0.74; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.95
Batch: 780; loss: 0.59; acc: 0.91
Train Epoch over. train_loss: 0.62; train_accuracy: 0.88 

3.3746760891517624e-05
1.3390625099418685e-05
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.5347823257658891; val_accuracy: 0.9052547770700637 

The current subspace-distance is: 1.3390625099418685e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.95
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.94
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.86
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.94
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.59; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.91
Batch: 760; loss: 0.68; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.56; train_accuracy: 0.89 

3.704554546857253e-05
1.6432806660304777e-05
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.4858302079188596; val_accuracy: 0.9102308917197452 

The current subspace-distance is: 1.6432806660304777e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.48; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.95
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.6; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.94
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.97
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.95
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.9 

4.028127295896411e-05
1.6877362213563174e-05
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.43801365678857085; val_accuracy: 0.917296974522293 

The current subspace-distance is: 1.6877362213563174e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.36; acc: 1.0
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.92
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.91 

4.277393236407079e-05
1.820377838157583e-05
Batch: 0; loss: 0.4; acc: 0.98
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.40770428822298715; val_accuracy: 0.9194864649681529 

The current subspace-distance is: 1.820377838157583e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.97
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.98
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.95
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.97
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.91 

4.5326763938646764e-05
1.8614757209434174e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.22; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.38005492243037864; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 1.8614757209434174e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.98
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.94
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.80846174468752e-05
2.0211815353832208e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.35788985137726853; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.0211815353832208e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.97
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.92 

4.92739854962565e-05
2.1366457076510414e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.33684306700897826; val_accuracy: 0.9306329617834395 

The current subspace-distance is: 2.1366457076510414e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.97
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.97
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.98
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.98
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

5.176122431294061e-05
2.360767757636495e-05
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.19; acc: 1.0
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.32276038350952657; val_accuracy: 0.9333200636942676 

The current subspace-distance is: 2.360767757636495e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.97
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.98
Batch: 420; loss: 0.19; acc: 1.0
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.29; acc: 1.0
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.283420250634663e-05
2.276979648740962e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.31744703574545063; val_accuracy: 0.934812898089172 

The current subspace-distance is: 2.276979648740962e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.98
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.98
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.95
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.337873881217092e-05
2.176637099182699e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3158680467279094; val_accuracy: 0.9345143312101911 

The current subspace-distance is: 2.176637099182699e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.21; acc: 1.0
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.2; acc: 1.0
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.4174244723981246e-05
2.317601683898829e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.31213325252578517; val_accuracy: 0.9333200636942676 

The current subspace-distance is: 2.317601683898829e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.98
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.4224845371209085e-05
2.3261743990588002e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3046920938761371; val_accuracy: 0.93640525477707 

The current subspace-distance is: 2.3261743990588002e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.98
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.98
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.23; acc: 0.98
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.532418435905129e-05
2.3405327738146298e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.30151102715616773; val_accuracy: 0.9362062101910829 

The current subspace-distance is: 2.3405327738146298e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.98
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.5428688938263804e-05
2.3686474378337152e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.2956034565807148; val_accuracy: 0.9375 

The current subspace-distance is: 2.3686474378337152e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.93 

5.5996144510572776e-05
2.535251041990705e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.2923953353789202; val_accuracy: 0.9357085987261147 

The current subspace-distance is: 2.535251041990705e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.98
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.593001333181746e-05
2.3918886654428206e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.29104282023610584; val_accuracy: 0.9375 

The current subspace-distance is: 2.3918886654428206e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.98
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.746099122916348e-05
2.5913073841365986e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.2884860771951402; val_accuracy: 0.9375995222929936 

The current subspace-distance is: 2.5913073841365986e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.98
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.689377576345578e-05
2.429002597637009e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.28250549719401985; val_accuracy: 0.9372014331210191 

The current subspace-distance is: 2.429002597637009e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.22; acc: 1.0
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.97
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.98
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.7164135796483606e-05
2.541832327551674e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.2849886465793962; val_accuracy: 0.9376990445859873 

The current subspace-distance is: 2.541832327551674e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.98
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.734346268582158e-05
2.4175826183636673e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.28167131490957964; val_accuracy: 0.9365047770700637 

The current subspace-distance is: 2.4175826183636673e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.17; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.17; acc: 1.0
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.737890751333907e-05
2.4200866391765885e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.28334895568858287; val_accuracy: 0.9386942675159236 

The current subspace-distance is: 2.4200866391765885e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.98
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.98
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.98
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.725686423829757e-05
2.3556780433864333e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.28627458281198126; val_accuracy: 0.9368033439490446 

The current subspace-distance is: 2.3556780433864333e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.98
Batch: 100; loss: 0.17; acc: 1.0
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.8279354561818764e-05
2.5703922801767476e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.27900992049153445; val_accuracy: 0.9376990445859873 

The current subspace-distance is: 2.5703922801767476e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.97
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.837388744112104e-05
2.5993633244070224e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.2813724405161894; val_accuracy: 0.9376990445859873 

The current subspace-distance is: 2.5993633244070224e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.821648665005341e-05
2.5174957045237534e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.2807193641449995; val_accuracy: 0.9375 

The current subspace-distance is: 2.5174957045237534e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.98
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.862479156348854e-05
2.557920197432395e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.27681207315177675; val_accuracy: 0.939390923566879 

The current subspace-distance is: 2.557920197432395e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.98
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.19; acc: 1.0
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.821900049340911e-05
2.5019446184160188e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.2771271623813423; val_accuracy: 0.9383957006369427 

The current subspace-distance is: 2.5019446184160188e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.98
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.904518548049964e-05
2.5029841708601452e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.27764377244718513; val_accuracy: 0.9387937898089171 

The current subspace-distance is: 2.5029841708601452e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_13_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_13_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
