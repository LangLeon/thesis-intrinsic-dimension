model : table13slim
N : 11
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 1.835055857460475

The number of parameters is: 251109

The number of individual parameters is:

15
270
15
15
23
37605
23
23
45
112815
45
45
64
95040
64
64
4096
64
640
10
64
64

nonzero elements in E: 12555448
elements in E: 12555450
fraction nonzero: 0.9999998407066254
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.34; acc: 0.16
Batch: 20; loss: 2.28; acc: 0.16
Batch: 40; loss: 2.27; acc: 0.11
Batch: 60; loss: 2.22; acc: 0.2
Batch: 80; loss: 2.24; acc: 0.16
Batch: 100; loss: 2.23; acc: 0.2
Batch: 120; loss: 2.05; acc: 0.36
Batch: 140; loss: 2.19; acc: 0.19
Batch: 160; loss: 2.1; acc: 0.2
Batch: 180; loss: 2.08; acc: 0.3
Batch: 200; loss: 2.09; acc: 0.25
Batch: 220; loss: 2.06; acc: 0.31
Batch: 240; loss: 2.08; acc: 0.33
Batch: 260; loss: 1.89; acc: 0.47
Batch: 280; loss: 2.07; acc: 0.31
Batch: 300; loss: 1.98; acc: 0.34
Batch: 320; loss: 1.97; acc: 0.38
Batch: 340; loss: 1.91; acc: 0.45
Batch: 360; loss: 2.02; acc: 0.33
Batch: 380; loss: 2.06; acc: 0.3
Batch: 400; loss: 1.98; acc: 0.34
Batch: 420; loss: 1.87; acc: 0.47
Batch: 440; loss: 1.95; acc: 0.39
Batch: 460; loss: 1.96; acc: 0.44
Batch: 480; loss: 2.04; acc: 0.28
Batch: 500; loss: 1.92; acc: 0.38
Batch: 520; loss: 1.92; acc: 0.42
Batch: 540; loss: 1.9; acc: 0.44
Batch: 560; loss: 1.93; acc: 0.36
Batch: 580; loss: 1.92; acc: 0.42
Batch: 600; loss: 2.01; acc: 0.38
Batch: 620; loss: 1.81; acc: 0.44
Batch: 640; loss: 1.84; acc: 0.48
Batch: 660; loss: 1.94; acc: 0.38
Batch: 680; loss: 1.87; acc: 0.48
Batch: 700; loss: 1.92; acc: 0.41
Batch: 720; loss: 1.72; acc: 0.64
Batch: 740; loss: 1.91; acc: 0.41
Batch: 760; loss: 1.88; acc: 0.41
Batch: 780; loss: 1.94; acc: 0.38
Train Epoch over. train_loss: 2.02; train_accuracy: 0.34 

2.201987081207335e-05
3.95890401705401e-06
Batch: 0; loss: 2.05; acc: 0.42
Batch: 20; loss: 2.0; acc: 0.33
Batch: 40; loss: 1.69; acc: 0.55
Batch: 60; loss: 1.76; acc: 0.56
Batch: 80; loss: 1.8; acc: 0.45
Batch: 100; loss: 1.91; acc: 0.44
Batch: 120; loss: 1.95; acc: 0.33
Batch: 140; loss: 1.82; acc: 0.41
Val Epoch over. val_loss: 1.8897013436457155; val_accuracy: 0.428343949044586 

The current subspace-distance is: 3.95890401705401e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.89; acc: 0.41
Batch: 20; loss: 1.87; acc: 0.44
Batch: 40; loss: 1.91; acc: 0.45
Batch: 60; loss: 1.98; acc: 0.36
Batch: 80; loss: 1.9; acc: 0.47
Batch: 100; loss: 1.95; acc: 0.38
Batch: 120; loss: 1.89; acc: 0.48
Batch: 140; loss: 1.88; acc: 0.41
Batch: 160; loss: 1.78; acc: 0.47
Batch: 180; loss: 1.88; acc: 0.39
Batch: 200; loss: 1.99; acc: 0.31
Batch: 220; loss: 1.81; acc: 0.48
Batch: 240; loss: 1.72; acc: 0.64
Batch: 260; loss: 1.91; acc: 0.34
Batch: 280; loss: 1.89; acc: 0.45
Batch: 300; loss: 1.89; acc: 0.41
Batch: 320; loss: 1.92; acc: 0.41
Batch: 340; loss: 1.91; acc: 0.41
Batch: 360; loss: 1.88; acc: 0.44
Batch: 380; loss: 1.86; acc: 0.42
Batch: 400; loss: 1.85; acc: 0.42
Batch: 420; loss: 1.86; acc: 0.52
Batch: 440; loss: 1.81; acc: 0.44
Batch: 460; loss: 1.88; acc: 0.5
Batch: 480; loss: 1.84; acc: 0.47
Batch: 500; loss: 1.96; acc: 0.36
Batch: 520; loss: 1.8; acc: 0.5
Batch: 540; loss: 1.92; acc: 0.33
Batch: 560; loss: 1.93; acc: 0.44
Batch: 580; loss: 1.83; acc: 0.53
Batch: 600; loss: 1.89; acc: 0.47
Batch: 620; loss: 1.82; acc: 0.48
Batch: 640; loss: 1.9; acc: 0.41
Batch: 660; loss: 1.82; acc: 0.5
Batch: 680; loss: 1.82; acc: 0.5
Batch: 700; loss: 1.86; acc: 0.44
Batch: 720; loss: 1.87; acc: 0.42
Batch: 740; loss: 2.06; acc: 0.28
Batch: 760; loss: 1.85; acc: 0.47
Batch: 780; loss: 1.84; acc: 0.38
Train Epoch over. train_loss: 1.89; train_accuracy: 0.43 

2.3641598090762272e-05
4.5648475861526094e-06
Batch: 0; loss: 1.98; acc: 0.38
Batch: 20; loss: 1.93; acc: 0.42
Batch: 40; loss: 1.64; acc: 0.62
Batch: 60; loss: 1.69; acc: 0.58
Batch: 80; loss: 1.77; acc: 0.52
Batch: 100; loss: 1.84; acc: 0.48
Batch: 120; loss: 1.9; acc: 0.39
Batch: 140; loss: 1.81; acc: 0.52
Val Epoch over. val_loss: 1.8444311611211983; val_accuracy: 0.46765525477707004 

The current subspace-distance is: 4.5648475861526094e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.52
Batch: 20; loss: 1.87; acc: 0.47
Batch: 40; loss: 1.82; acc: 0.47
Batch: 60; loss: 1.79; acc: 0.5
Batch: 80; loss: 1.83; acc: 0.5
Batch: 100; loss: 1.9; acc: 0.42
Batch: 120; loss: 1.88; acc: 0.39
Batch: 140; loss: 1.82; acc: 0.41
Batch: 160; loss: 1.9; acc: 0.39
Batch: 180; loss: 1.98; acc: 0.31
Batch: 200; loss: 1.96; acc: 0.38
Batch: 220; loss: 1.91; acc: 0.33
Batch: 240; loss: 1.87; acc: 0.42
Batch: 260; loss: 1.92; acc: 0.41
Batch: 280; loss: 1.85; acc: 0.5
Batch: 300; loss: 1.89; acc: 0.39
Batch: 320; loss: 1.74; acc: 0.52
Batch: 340; loss: 1.91; acc: 0.48
Batch: 360; loss: 1.84; acc: 0.45
Batch: 380; loss: 1.81; acc: 0.48
Batch: 400; loss: 1.77; acc: 0.48
Batch: 420; loss: 1.88; acc: 0.42
Batch: 440; loss: 1.89; acc: 0.47
Batch: 460; loss: 1.85; acc: 0.42
Batch: 480; loss: 1.85; acc: 0.45
Batch: 500; loss: 1.83; acc: 0.48
Batch: 520; loss: 1.8; acc: 0.48
Batch: 540; loss: 1.89; acc: 0.41
Batch: 560; loss: 1.88; acc: 0.39
Batch: 580; loss: 1.81; acc: 0.56
Batch: 600; loss: 1.78; acc: 0.52
Batch: 620; loss: 1.87; acc: 0.45
Batch: 640; loss: 1.92; acc: 0.39
Batch: 660; loss: 1.9; acc: 0.39
Batch: 680; loss: 1.88; acc: 0.42
Batch: 700; loss: 1.86; acc: 0.56
Batch: 720; loss: 1.82; acc: 0.5
Batch: 740; loss: 1.8; acc: 0.52
Batch: 760; loss: 1.86; acc: 0.47
Batch: 780; loss: 1.82; acc: 0.42
Train Epoch over. train_loss: 1.85; train_accuracy: 0.46 

2.4497210688423365e-05
5.2009881983394735e-06
Batch: 0; loss: 1.9; acc: 0.44
Batch: 20; loss: 1.88; acc: 0.48
Batch: 40; loss: 1.58; acc: 0.67
Batch: 60; loss: 1.67; acc: 0.61
Batch: 80; loss: 1.74; acc: 0.5
Batch: 100; loss: 1.76; acc: 0.56
Batch: 120; loss: 1.85; acc: 0.38
Batch: 140; loss: 1.8; acc: 0.53
Val Epoch over. val_loss: 1.8081574895579344; val_accuracy: 0.49522292993630573 

The current subspace-distance is: 5.2009881983394735e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.85; acc: 0.47
Batch: 20; loss: 1.85; acc: 0.53
Batch: 40; loss: 1.91; acc: 0.41
Batch: 60; loss: 1.91; acc: 0.42
Batch: 80; loss: 1.81; acc: 0.53
Batch: 100; loss: 1.85; acc: 0.45
Batch: 120; loss: 1.85; acc: 0.41
Batch: 140; loss: 1.77; acc: 0.55
Batch: 160; loss: 1.83; acc: 0.45
Batch: 180; loss: 1.89; acc: 0.5
Batch: 200; loss: 1.83; acc: 0.45
Batch: 220; loss: 1.74; acc: 0.45
Batch: 240; loss: 1.88; acc: 0.44
Batch: 260; loss: 1.75; acc: 0.53
Batch: 280; loss: 1.71; acc: 0.55
Batch: 300; loss: 1.79; acc: 0.52
Batch: 320; loss: 1.8; acc: 0.45
Batch: 340; loss: 1.79; acc: 0.45
Batch: 360; loss: 1.82; acc: 0.47
Batch: 380; loss: 1.79; acc: 0.48
Batch: 400; loss: 1.87; acc: 0.39
Batch: 420; loss: 1.76; acc: 0.44
Batch: 440; loss: 1.77; acc: 0.52
Batch: 460; loss: 1.88; acc: 0.38
Batch: 480; loss: 1.76; acc: 0.5
Batch: 500; loss: 1.76; acc: 0.55
Batch: 520; loss: 1.82; acc: 0.5
Batch: 540; loss: 1.84; acc: 0.47
Batch: 560; loss: 1.74; acc: 0.55
Batch: 580; loss: 1.81; acc: 0.55
Batch: 600; loss: 1.83; acc: 0.47
Batch: 620; loss: 1.83; acc: 0.41
Batch: 640; loss: 1.7; acc: 0.58
Batch: 660; loss: 1.82; acc: 0.44
Batch: 680; loss: 1.71; acc: 0.59
Batch: 700; loss: 1.74; acc: 0.53
Batch: 720; loss: 1.74; acc: 0.47
Batch: 740; loss: 1.85; acc: 0.45
Batch: 760; loss: 1.8; acc: 0.55
Batch: 780; loss: 1.84; acc: 0.44
Train Epoch over. train_loss: 1.82; train_accuracy: 0.47 

2.5687690140330233e-05
5.8998839449486695e-06
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.86; acc: 0.52
Batch: 40; loss: 1.57; acc: 0.61
Batch: 60; loss: 1.67; acc: 0.59
Batch: 80; loss: 1.73; acc: 0.52
Batch: 100; loss: 1.73; acc: 0.61
Batch: 120; loss: 1.84; acc: 0.39
Batch: 140; loss: 1.8; acc: 0.56
Val Epoch over. val_loss: 1.7920271089881847; val_accuracy: 0.5044785031847133 

The current subspace-distance is: 5.8998839449486695e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.58
Batch: 20; loss: 1.83; acc: 0.42
Batch: 40; loss: 1.77; acc: 0.47
Batch: 60; loss: 1.89; acc: 0.44
Batch: 80; loss: 1.81; acc: 0.42
Batch: 100; loss: 1.75; acc: 0.53
Batch: 120; loss: 1.83; acc: 0.44
Batch: 140; loss: 1.89; acc: 0.41
Batch: 160; loss: 1.83; acc: 0.47
Batch: 180; loss: 1.86; acc: 0.38
Batch: 200; loss: 1.81; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.53
Batch: 240; loss: 1.79; acc: 0.48
Batch: 260; loss: 1.78; acc: 0.44
Batch: 280; loss: 1.78; acc: 0.47
Batch: 300; loss: 1.75; acc: 0.52
Batch: 320; loss: 1.84; acc: 0.44
Batch: 340; loss: 1.7; acc: 0.64
Batch: 360; loss: 1.93; acc: 0.36
Batch: 380; loss: 1.9; acc: 0.36
Batch: 400; loss: 1.76; acc: 0.47
Batch: 420; loss: 1.85; acc: 0.47
Batch: 440; loss: 1.82; acc: 0.41
Batch: 460; loss: 1.89; acc: 0.41
Batch: 480; loss: 1.89; acc: 0.36
Batch: 500; loss: 1.85; acc: 0.45
Batch: 520; loss: 1.97; acc: 0.33
Batch: 540; loss: 1.9; acc: 0.39
Batch: 560; loss: 1.73; acc: 0.58
Batch: 580; loss: 1.83; acc: 0.45
Batch: 600; loss: 1.82; acc: 0.5
Batch: 620; loss: 1.7; acc: 0.53
Batch: 640; loss: 1.78; acc: 0.48
Batch: 660; loss: 1.71; acc: 0.58
Batch: 680; loss: 1.79; acc: 0.45
Batch: 700; loss: 1.86; acc: 0.42
Batch: 720; loss: 1.77; acc: 0.5
Batch: 740; loss: 1.8; acc: 0.38
Batch: 760; loss: 1.76; acc: 0.52
Batch: 780; loss: 1.82; acc: 0.45
Train Epoch over. train_loss: 1.8; train_accuracy: 0.48 

2.6774594516609795e-05
7.109526904969243e-06
Batch: 0; loss: 1.85; acc: 0.39
Batch: 20; loss: 1.85; acc: 0.48
Batch: 40; loss: 1.56; acc: 0.62
Batch: 60; loss: 1.68; acc: 0.58
Batch: 80; loss: 1.71; acc: 0.58
Batch: 100; loss: 1.73; acc: 0.56
Batch: 120; loss: 1.85; acc: 0.38
Batch: 140; loss: 1.8; acc: 0.58
Val Epoch over. val_loss: 1.780469733438674; val_accuracy: 0.5017914012738853 

The current subspace-distance is: 7.109526904969243e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.77; acc: 0.5
Batch: 60; loss: 1.78; acc: 0.56
Batch: 80; loss: 1.92; acc: 0.34
Batch: 100; loss: 1.78; acc: 0.53
Batch: 120; loss: 1.79; acc: 0.52
Batch: 140; loss: 1.7; acc: 0.5
Batch: 160; loss: 1.88; acc: 0.55
Batch: 180; loss: 1.87; acc: 0.47
Batch: 200; loss: 1.86; acc: 0.44
Batch: 220; loss: 1.78; acc: 0.47
Batch: 240; loss: 1.83; acc: 0.47
Batch: 260; loss: 1.8; acc: 0.45
Batch: 280; loss: 1.7; acc: 0.62
Batch: 300; loss: 1.76; acc: 0.52
Batch: 320; loss: 1.77; acc: 0.55
Batch: 340; loss: 1.9; acc: 0.45
Batch: 360; loss: 1.84; acc: 0.44
Batch: 380; loss: 1.79; acc: 0.44
Batch: 400; loss: 1.8; acc: 0.56
Batch: 420; loss: 1.78; acc: 0.52
Batch: 440; loss: 1.85; acc: 0.41
Batch: 460; loss: 1.78; acc: 0.42
Batch: 480; loss: 1.82; acc: 0.47
Batch: 500; loss: 1.74; acc: 0.48
Batch: 520; loss: 1.75; acc: 0.5
Batch: 540; loss: 1.82; acc: 0.42
Batch: 560; loss: 1.84; acc: 0.47
Batch: 580; loss: 1.76; acc: 0.39
Batch: 600; loss: 1.86; acc: 0.41
Batch: 620; loss: 1.79; acc: 0.47
Batch: 640; loss: 1.77; acc: 0.44
Batch: 660; loss: 1.92; acc: 0.38
Batch: 680; loss: 1.68; acc: 0.53
Batch: 700; loss: 1.67; acc: 0.56
Batch: 720; loss: 1.72; acc: 0.58
Batch: 740; loss: 1.73; acc: 0.53
Batch: 760; loss: 1.92; acc: 0.38
Batch: 780; loss: 1.85; acc: 0.41
Train Epoch over. train_loss: 1.79; train_accuracy: 0.48 

2.722702811297495e-05
6.8221729634387884e-06
Batch: 0; loss: 1.82; acc: 0.44
Batch: 20; loss: 1.86; acc: 0.41
Batch: 40; loss: 1.55; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.53
Batch: 80; loss: 1.69; acc: 0.62
Batch: 100; loss: 1.74; acc: 0.55
Batch: 120; loss: 1.87; acc: 0.36
Batch: 140; loss: 1.8; acc: 0.53
Val Epoch over. val_loss: 1.768697335461902; val_accuracy: 0.4947253184713376 

The current subspace-distance is: 6.8221729634387884e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.64
Batch: 40; loss: 1.68; acc: 0.55
Batch: 60; loss: 1.7; acc: 0.48
Batch: 80; loss: 1.77; acc: 0.47
Batch: 100; loss: 1.7; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.55
Batch: 140; loss: 1.83; acc: 0.41
Batch: 160; loss: 1.68; acc: 0.52
Batch: 180; loss: 1.75; acc: 0.53
Batch: 200; loss: 1.78; acc: 0.42
Batch: 220; loss: 1.79; acc: 0.5
Batch: 240; loss: 1.75; acc: 0.52
Batch: 260; loss: 1.7; acc: 0.48
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.86; acc: 0.48
Batch: 320; loss: 1.7; acc: 0.55
Batch: 340; loss: 1.71; acc: 0.59
Batch: 360; loss: 1.84; acc: 0.53
Batch: 380; loss: 1.78; acc: 0.48
Batch: 400; loss: 1.91; acc: 0.39
Batch: 420; loss: 1.82; acc: 0.52
Batch: 440; loss: 1.69; acc: 0.55
Batch: 460; loss: 1.71; acc: 0.55
Batch: 480; loss: 1.74; acc: 0.52
Batch: 500; loss: 1.76; acc: 0.45
Batch: 520; loss: 1.7; acc: 0.59
Batch: 540; loss: 1.75; acc: 0.52
Batch: 560; loss: 1.69; acc: 0.52
Batch: 580; loss: 1.75; acc: 0.47
Batch: 600; loss: 1.81; acc: 0.48
Batch: 620; loss: 1.79; acc: 0.5
Batch: 640; loss: 1.83; acc: 0.41
Batch: 660; loss: 1.73; acc: 0.44
Batch: 680; loss: 1.74; acc: 0.48
Batch: 700; loss: 1.83; acc: 0.47
Batch: 720; loss: 1.71; acc: 0.52
Batch: 740; loss: 1.67; acc: 0.61
Batch: 760; loss: 1.66; acc: 0.59
Batch: 780; loss: 1.73; acc: 0.44
Train Epoch over. train_loss: 1.78; train_accuracy: 0.48 

2.806422889989335e-05
7.131645816116361e-06
Batch: 0; loss: 1.81; acc: 0.45
Batch: 20; loss: 1.86; acc: 0.39
Batch: 40; loss: 1.54; acc: 0.55
Batch: 60; loss: 1.67; acc: 0.55
Batch: 80; loss: 1.67; acc: 0.62
Batch: 100; loss: 1.7; acc: 0.55
Batch: 120; loss: 1.85; acc: 0.39
Batch: 140; loss: 1.79; acc: 0.53
Val Epoch over. val_loss: 1.747595377788422; val_accuracy: 0.5004976114649682 

The current subspace-distance is: 7.131645816116361e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.45
Batch: 20; loss: 1.66; acc: 0.62
Batch: 40; loss: 1.72; acc: 0.48
Batch: 60; loss: 1.85; acc: 0.39
Batch: 80; loss: 1.67; acc: 0.53
Batch: 100; loss: 1.81; acc: 0.48
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.87; acc: 0.41
Batch: 160; loss: 1.84; acc: 0.39
Batch: 180; loss: 1.79; acc: 0.44
Batch: 200; loss: 1.67; acc: 0.53
Batch: 220; loss: 1.76; acc: 0.45
Batch: 240; loss: 1.8; acc: 0.53
Batch: 260; loss: 1.73; acc: 0.5
Batch: 280; loss: 1.74; acc: 0.55
Batch: 300; loss: 1.72; acc: 0.52
Batch: 320; loss: 1.75; acc: 0.5
Batch: 340; loss: 1.74; acc: 0.48
Batch: 360; loss: 1.8; acc: 0.42
Batch: 380; loss: 1.88; acc: 0.42
Batch: 400; loss: 1.81; acc: 0.44
Batch: 420; loss: 1.91; acc: 0.34
Batch: 440; loss: 1.73; acc: 0.48
Batch: 460; loss: 1.74; acc: 0.44
Batch: 480; loss: 1.8; acc: 0.44
Batch: 500; loss: 1.68; acc: 0.56
Batch: 520; loss: 1.79; acc: 0.52
Batch: 540; loss: 1.7; acc: 0.52
Batch: 560; loss: 1.65; acc: 0.53
Batch: 580; loss: 1.79; acc: 0.48
Batch: 600; loss: 1.82; acc: 0.5
Batch: 620; loss: 1.76; acc: 0.44
Batch: 640; loss: 1.66; acc: 0.56
Batch: 660; loss: 1.73; acc: 0.52
Batch: 680; loss: 1.78; acc: 0.45
Batch: 700; loss: 1.84; acc: 0.44
Batch: 720; loss: 1.77; acc: 0.48
Batch: 740; loss: 1.85; acc: 0.42
Batch: 760; loss: 1.77; acc: 0.45
Batch: 780; loss: 1.83; acc: 0.48
Train Epoch over. train_loss: 1.77; train_accuracy: 0.48 

2.8427351935533807e-05
6.756339189450955e-06
Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 1.87; acc: 0.42
Batch: 40; loss: 1.55; acc: 0.56
Batch: 60; loss: 1.68; acc: 0.55
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.68; acc: 0.59
Batch: 120; loss: 1.85; acc: 0.38
Batch: 140; loss: 1.77; acc: 0.53
Val Epoch over. val_loss: 1.7392981348523668; val_accuracy: 0.5015923566878981 

The current subspace-distance is: 6.756339189450955e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.59
Batch: 20; loss: 1.62; acc: 0.62
Batch: 40; loss: 1.67; acc: 0.52
Batch: 60; loss: 1.9; acc: 0.44
Batch: 80; loss: 1.81; acc: 0.48
Batch: 100; loss: 1.82; acc: 0.47
Batch: 120; loss: 1.81; acc: 0.42
Batch: 140; loss: 1.7; acc: 0.53
Batch: 160; loss: 1.63; acc: 0.5
Batch: 180; loss: 1.71; acc: 0.53
Batch: 200; loss: 1.74; acc: 0.48
Batch: 220; loss: 1.85; acc: 0.38
Batch: 240; loss: 1.72; acc: 0.56
Batch: 260; loss: 1.82; acc: 0.44
Batch: 280; loss: 1.68; acc: 0.5
Batch: 300; loss: 1.73; acc: 0.48
Batch: 320; loss: 1.65; acc: 0.53
Batch: 340; loss: 1.8; acc: 0.48
Batch: 360; loss: 1.86; acc: 0.42
Batch: 380; loss: 1.67; acc: 0.58
Batch: 400; loss: 1.72; acc: 0.48
Batch: 420; loss: 1.71; acc: 0.52
Batch: 440; loss: 1.78; acc: 0.52
Batch: 460; loss: 1.76; acc: 0.48
Batch: 480; loss: 1.82; acc: 0.36
Batch: 500; loss: 1.76; acc: 0.48
Batch: 520; loss: 1.74; acc: 0.53
Batch: 540; loss: 1.65; acc: 0.59
Batch: 560; loss: 1.71; acc: 0.52
Batch: 580; loss: 1.8; acc: 0.47
Batch: 600; loss: 1.79; acc: 0.47
Batch: 620; loss: 1.76; acc: 0.45
Batch: 640; loss: 1.79; acc: 0.5
Batch: 660; loss: 1.65; acc: 0.5
Batch: 680; loss: 1.86; acc: 0.44
Batch: 700; loss: 1.75; acc: 0.52
Batch: 720; loss: 1.78; acc: 0.5
Batch: 740; loss: 1.94; acc: 0.42
Batch: 760; loss: 1.76; acc: 0.53
Batch: 780; loss: 1.69; acc: 0.47
Train Epoch over. train_loss: 1.76; train_accuracy: 0.48 

2.9348695534281433e-05
7.440291483362671e-06
Batch: 0; loss: 1.8; acc: 0.42
Batch: 20; loss: 1.88; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.62
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.55
Batch: 120; loss: 1.84; acc: 0.41
Batch: 140; loss: 1.77; acc: 0.58
Val Epoch over. val_loss: 1.7317602201631874; val_accuracy: 0.5016918789808917 

The current subspace-distance is: 7.440291483362671e-06 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.56
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.7; acc: 0.59
Batch: 60; loss: 1.93; acc: 0.47
Batch: 80; loss: 1.83; acc: 0.38
Batch: 100; loss: 1.76; acc: 0.47
Batch: 120; loss: 1.85; acc: 0.39
Batch: 140; loss: 1.81; acc: 0.48
Batch: 160; loss: 1.61; acc: 0.59
Batch: 180; loss: 1.82; acc: 0.45
Batch: 200; loss: 1.8; acc: 0.48
Batch: 220; loss: 1.92; acc: 0.36
Batch: 240; loss: 1.83; acc: 0.41
Batch: 260; loss: 1.67; acc: 0.5
Batch: 280; loss: 1.84; acc: 0.39
Batch: 300; loss: 1.77; acc: 0.44
Batch: 320; loss: 1.86; acc: 0.47
Batch: 340; loss: 1.73; acc: 0.48
Batch: 360; loss: 1.85; acc: 0.44
Batch: 380; loss: 1.74; acc: 0.47
Batch: 400; loss: 1.7; acc: 0.53
Batch: 420; loss: 1.76; acc: 0.5
Batch: 440; loss: 1.88; acc: 0.44
Batch: 460; loss: 1.75; acc: 0.44
Batch: 480; loss: 1.69; acc: 0.61
Batch: 500; loss: 1.82; acc: 0.47
Batch: 520; loss: 1.89; acc: 0.41
Batch: 540; loss: 1.69; acc: 0.52
Batch: 560; loss: 1.71; acc: 0.55
Batch: 580; loss: 1.7; acc: 0.53
Batch: 600; loss: 1.85; acc: 0.39
Batch: 620; loss: 1.7; acc: 0.5
Batch: 640; loss: 1.72; acc: 0.52
Batch: 660; loss: 1.69; acc: 0.47
Batch: 680; loss: 1.83; acc: 0.45
Batch: 700; loss: 1.79; acc: 0.47
Batch: 720; loss: 1.85; acc: 0.38
Batch: 740; loss: 1.81; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.48
Batch: 780; loss: 1.71; acc: 0.53
Train Epoch over. train_loss: 1.76; train_accuracy: 0.48 

2.9837601687177084e-05
6.883195965201594e-06
Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.57; acc: 0.61
Batch: 60; loss: 1.68; acc: 0.47
Batch: 80; loss: 1.69; acc: 0.52
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.83; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.56
Val Epoch over. val_loss: 1.7257817664723487; val_accuracy: 0.5017914012738853 

The current subspace-distance is: 6.883195965201594e-06 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.74; acc: 0.48
Batch: 20; loss: 1.9; acc: 0.39
Batch: 40; loss: 1.7; acc: 0.48
Batch: 60; loss: 1.77; acc: 0.53
Batch: 80; loss: 1.77; acc: 0.36
Batch: 100; loss: 1.68; acc: 0.5
Batch: 120; loss: 1.78; acc: 0.38
Batch: 140; loss: 1.75; acc: 0.48
Batch: 160; loss: 1.7; acc: 0.5
Batch: 180; loss: 1.82; acc: 0.45
Batch: 200; loss: 1.84; acc: 0.42
Batch: 220; loss: 1.79; acc: 0.44
Batch: 240; loss: 1.67; acc: 0.53
Batch: 260; loss: 1.68; acc: 0.56
Batch: 280; loss: 1.74; acc: 0.55
Batch: 300; loss: 1.66; acc: 0.48
Batch: 320; loss: 1.75; acc: 0.59
Batch: 340; loss: 1.7; acc: 0.55
Batch: 360; loss: 1.84; acc: 0.39
Batch: 380; loss: 1.81; acc: 0.42
Batch: 400; loss: 1.71; acc: 0.48
Batch: 420; loss: 1.64; acc: 0.58
Batch: 440; loss: 1.71; acc: 0.56
Batch: 460; loss: 1.87; acc: 0.42
Batch: 480; loss: 1.72; acc: 0.5
Batch: 500; loss: 1.83; acc: 0.48
Batch: 520; loss: 1.84; acc: 0.44
Batch: 540; loss: 1.76; acc: 0.45
Batch: 560; loss: 1.63; acc: 0.55
Batch: 580; loss: 1.77; acc: 0.47
Batch: 600; loss: 1.87; acc: 0.42
Batch: 620; loss: 1.74; acc: 0.55
Batch: 640; loss: 1.65; acc: 0.53
Batch: 660; loss: 1.78; acc: 0.36
Batch: 680; loss: 1.81; acc: 0.55
Batch: 700; loss: 1.69; acc: 0.55
Batch: 720; loss: 1.8; acc: 0.38
Batch: 740; loss: 1.66; acc: 0.58
Batch: 760; loss: 1.76; acc: 0.45
Batch: 780; loss: 1.81; acc: 0.44
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

2.989139647979755e-05
1.0101085536007304e-05
Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.58; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.52
Batch: 80; loss: 1.69; acc: 0.52
Batch: 100; loss: 1.66; acc: 0.56
Batch: 120; loss: 1.82; acc: 0.45
Batch: 140; loss: 1.74; acc: 0.58
Val Epoch over. val_loss: 1.7284312217858187; val_accuracy: 0.5043789808917197 

The current subspace-distance is: 1.0101085536007304e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.85; acc: 0.39
Batch: 20; loss: 1.68; acc: 0.55
Batch: 40; loss: 1.75; acc: 0.44
Batch: 60; loss: 1.65; acc: 0.52
Batch: 80; loss: 1.81; acc: 0.41
Batch: 100; loss: 1.68; acc: 0.58
Batch: 120; loss: 1.79; acc: 0.48
Batch: 140; loss: 1.72; acc: 0.47
Batch: 160; loss: 1.87; acc: 0.38
Batch: 180; loss: 1.71; acc: 0.53
Batch: 200; loss: 1.83; acc: 0.45
Batch: 220; loss: 1.8; acc: 0.5
Batch: 240; loss: 1.75; acc: 0.5
Batch: 260; loss: 1.65; acc: 0.59
Batch: 280; loss: 1.7; acc: 0.48
Batch: 300; loss: 1.84; acc: 0.44
Batch: 320; loss: 1.71; acc: 0.53
Batch: 340; loss: 1.88; acc: 0.45
Batch: 360; loss: 1.81; acc: 0.45
Batch: 380; loss: 1.82; acc: 0.44
Batch: 400; loss: 1.73; acc: 0.47
Batch: 420; loss: 1.79; acc: 0.48
Batch: 440; loss: 1.76; acc: 0.5
Batch: 460; loss: 1.77; acc: 0.42
Batch: 480; loss: 1.71; acc: 0.41
Batch: 500; loss: 1.74; acc: 0.52
Batch: 520; loss: 1.74; acc: 0.47
Batch: 540; loss: 1.59; acc: 0.59
Batch: 560; loss: 1.7; acc: 0.53
Batch: 580; loss: 1.67; acc: 0.55
Batch: 600; loss: 1.75; acc: 0.42
Batch: 620; loss: 1.75; acc: 0.47
Batch: 640; loss: 1.68; acc: 0.55
Batch: 660; loss: 1.8; acc: 0.48
Batch: 680; loss: 1.64; acc: 0.56
Batch: 700; loss: 1.79; acc: 0.47
Batch: 720; loss: 1.69; acc: 0.56
Batch: 740; loss: 1.79; acc: 0.45
Batch: 760; loss: 1.68; acc: 0.52
Batch: 780; loss: 1.9; acc: 0.38
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.112102785962634e-05
9.667318408901338e-06
Batch: 0; loss: 1.81; acc: 0.47
Batch: 20; loss: 1.89; acc: 0.38
Batch: 40; loss: 1.57; acc: 0.58
Batch: 60; loss: 1.67; acc: 0.5
Batch: 80; loss: 1.68; acc: 0.53
Batch: 100; loss: 1.67; acc: 0.56
Batch: 120; loss: 1.81; acc: 0.47
Batch: 140; loss: 1.73; acc: 0.58
Val Epoch over. val_loss: 1.722016186471198; val_accuracy: 0.5069665605095541 

The current subspace-distance is: 9.667318408901338e-06 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.69; acc: 0.52
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.75; acc: 0.53
Batch: 60; loss: 1.75; acc: 0.53
Batch: 80; loss: 1.77; acc: 0.53
Batch: 100; loss: 1.76; acc: 0.52
Batch: 120; loss: 1.74; acc: 0.53
Batch: 140; loss: 1.7; acc: 0.44
Batch: 160; loss: 1.81; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.52
Batch: 200; loss: 1.58; acc: 0.59
Batch: 220; loss: 1.77; acc: 0.45
Batch: 240; loss: 1.82; acc: 0.36
Batch: 260; loss: 1.68; acc: 0.48
Batch: 280; loss: 1.83; acc: 0.45
Batch: 300; loss: 1.67; acc: 0.52
Batch: 320; loss: 1.78; acc: 0.5
Batch: 340; loss: 1.88; acc: 0.41
Batch: 360; loss: 1.76; acc: 0.53
Batch: 380; loss: 1.78; acc: 0.44
Batch: 400; loss: 1.74; acc: 0.47
Batch: 420; loss: 1.85; acc: 0.36
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.66; acc: 0.62
Batch: 480; loss: 1.77; acc: 0.52
Batch: 500; loss: 1.87; acc: 0.34
Batch: 520; loss: 1.77; acc: 0.52
Batch: 540; loss: 1.79; acc: 0.45
Batch: 560; loss: 1.86; acc: 0.42
Batch: 580; loss: 1.8; acc: 0.56
Batch: 600; loss: 1.76; acc: 0.5
Batch: 620; loss: 1.81; acc: 0.47
Batch: 640; loss: 1.83; acc: 0.5
Batch: 660; loss: 1.7; acc: 0.58
Batch: 680; loss: 1.82; acc: 0.36
Batch: 700; loss: 1.88; acc: 0.36
Batch: 720; loss: 1.81; acc: 0.55
Batch: 740; loss: 1.74; acc: 0.42
Batch: 760; loss: 1.89; acc: 0.38
Batch: 780; loss: 1.75; acc: 0.47
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.091471080551855e-05
8.228902515838854e-06
Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.58; acc: 0.61
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.69; acc: 0.52
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.82; acc: 0.39
Batch: 140; loss: 1.72; acc: 0.58
Val Epoch over. val_loss: 1.723481405312848; val_accuracy: 0.5003980891719745 

The current subspace-distance is: 8.228902515838854e-06 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.52
Batch: 20; loss: 1.86; acc: 0.48
Batch: 40; loss: 1.61; acc: 0.62
Batch: 60; loss: 1.77; acc: 0.48
Batch: 80; loss: 1.72; acc: 0.5
Batch: 100; loss: 1.89; acc: 0.41
Batch: 120; loss: 1.8; acc: 0.45
Batch: 140; loss: 1.84; acc: 0.44
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.57; acc: 0.62
Batch: 200; loss: 1.79; acc: 0.48
Batch: 220; loss: 1.68; acc: 0.53
Batch: 240; loss: 1.7; acc: 0.52
Batch: 260; loss: 1.74; acc: 0.45
Batch: 280; loss: 1.84; acc: 0.48
Batch: 300; loss: 1.7; acc: 0.5
Batch: 320; loss: 1.61; acc: 0.59
Batch: 340; loss: 1.73; acc: 0.44
Batch: 360; loss: 1.82; acc: 0.41
Batch: 380; loss: 1.7; acc: 0.56
Batch: 400; loss: 1.79; acc: 0.48
Batch: 420; loss: 1.71; acc: 0.55
Batch: 440; loss: 1.63; acc: 0.53
Batch: 460; loss: 1.68; acc: 0.53
Batch: 480; loss: 1.83; acc: 0.38
Batch: 500; loss: 1.73; acc: 0.5
Batch: 520; loss: 1.84; acc: 0.38
Batch: 540; loss: 1.67; acc: 0.48
Batch: 560; loss: 1.74; acc: 0.47
Batch: 580; loss: 1.67; acc: 0.58
Batch: 600; loss: 1.7; acc: 0.52
Batch: 620; loss: 1.77; acc: 0.45
Batch: 640; loss: 1.85; acc: 0.38
Batch: 660; loss: 1.74; acc: 0.5
Batch: 680; loss: 1.68; acc: 0.56
Batch: 700; loss: 1.82; acc: 0.47
Batch: 720; loss: 1.68; acc: 0.55
Batch: 740; loss: 1.74; acc: 0.56
Batch: 760; loss: 1.85; acc: 0.41
Batch: 780; loss: 1.71; acc: 0.52
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.1020510505186394e-05
8.552465260436293e-06
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.9; acc: 0.36
Batch: 40; loss: 1.58; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.68; acc: 0.52
Batch: 100; loss: 1.67; acc: 0.56
Batch: 120; loss: 1.82; acc: 0.41
Batch: 140; loss: 1.72; acc: 0.58
Val Epoch over. val_loss: 1.7231285412600086; val_accuracy: 0.504578025477707 

The current subspace-distance is: 8.552465260436293e-06 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.52
Batch: 20; loss: 1.82; acc: 0.52
Batch: 40; loss: 1.73; acc: 0.48
Batch: 60; loss: 1.81; acc: 0.38
Batch: 80; loss: 1.89; acc: 0.42
Batch: 100; loss: 1.78; acc: 0.52
Batch: 120; loss: 1.79; acc: 0.44
Batch: 140; loss: 1.83; acc: 0.38
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.77; acc: 0.53
Batch: 200; loss: 1.84; acc: 0.45
Batch: 220; loss: 1.84; acc: 0.44
Batch: 240; loss: 1.72; acc: 0.45
Batch: 260; loss: 1.69; acc: 0.52
Batch: 280; loss: 1.78; acc: 0.42
Batch: 300; loss: 1.72; acc: 0.44
Batch: 320; loss: 1.75; acc: 0.45
Batch: 340; loss: 1.76; acc: 0.53
Batch: 360; loss: 1.69; acc: 0.5
Batch: 380; loss: 1.75; acc: 0.47
Batch: 400; loss: 1.75; acc: 0.5
Batch: 420; loss: 1.74; acc: 0.44
Batch: 440; loss: 1.8; acc: 0.39
Batch: 460; loss: 1.69; acc: 0.5
Batch: 480; loss: 1.77; acc: 0.53
Batch: 500; loss: 1.81; acc: 0.45
Batch: 520; loss: 1.63; acc: 0.55
Batch: 540; loss: 1.72; acc: 0.53
Batch: 560; loss: 1.7; acc: 0.44
Batch: 580; loss: 1.85; acc: 0.45
Batch: 600; loss: 1.77; acc: 0.47
Batch: 620; loss: 1.84; acc: 0.41
Batch: 640; loss: 1.59; acc: 0.53
Batch: 660; loss: 1.84; acc: 0.44
Batch: 680; loss: 1.81; acc: 0.41
Batch: 700; loss: 1.66; acc: 0.56
Batch: 720; loss: 1.74; acc: 0.5
Batch: 740; loss: 1.89; acc: 0.36
Batch: 760; loss: 1.65; acc: 0.58
Batch: 780; loss: 1.7; acc: 0.47
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.152840872644447e-05
8.348702067451086e-06
Batch: 0; loss: 1.8; acc: 0.45
Batch: 20; loss: 1.91; acc: 0.33
Batch: 40; loss: 1.59; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.68; acc: 0.52
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.56
Val Epoch over. val_loss: 1.723508367113247; val_accuracy: 0.5022890127388535 

The current subspace-distance is: 8.348702067451086e-06 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.67; acc: 0.56
Batch: 20; loss: 1.79; acc: 0.38
Batch: 40; loss: 1.82; acc: 0.41
Batch: 60; loss: 1.86; acc: 0.44
Batch: 80; loss: 1.7; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.53
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.7; acc: 0.45
Batch: 160; loss: 1.76; acc: 0.44
Batch: 180; loss: 1.79; acc: 0.42
Batch: 200; loss: 1.79; acc: 0.44
Batch: 220; loss: 1.61; acc: 0.59
Batch: 240; loss: 1.81; acc: 0.44
Batch: 260; loss: 1.67; acc: 0.48
Batch: 280; loss: 1.68; acc: 0.55
Batch: 300; loss: 1.82; acc: 0.41
Batch: 320; loss: 1.87; acc: 0.33
Batch: 340; loss: 1.82; acc: 0.42
Batch: 360; loss: 1.69; acc: 0.53
Batch: 380; loss: 1.76; acc: 0.47
Batch: 400; loss: 1.72; acc: 0.59
Batch: 420; loss: 1.79; acc: 0.42
Batch: 440; loss: 1.61; acc: 0.56
Batch: 460; loss: 1.8; acc: 0.44
Batch: 480; loss: 1.77; acc: 0.47
Batch: 500; loss: 1.78; acc: 0.47
Batch: 520; loss: 1.9; acc: 0.34
Batch: 540; loss: 1.75; acc: 0.48
Batch: 560; loss: 1.73; acc: 0.5
Batch: 580; loss: 1.69; acc: 0.52
Batch: 600; loss: 1.74; acc: 0.47
Batch: 620; loss: 1.72; acc: 0.47
Batch: 640; loss: 1.76; acc: 0.52
Batch: 660; loss: 1.78; acc: 0.47
Batch: 680; loss: 1.71; acc: 0.44
Batch: 700; loss: 1.79; acc: 0.55
Batch: 720; loss: 1.83; acc: 0.45
Batch: 740; loss: 1.8; acc: 0.44
Batch: 760; loss: 1.77; acc: 0.44
Batch: 780; loss: 1.84; acc: 0.42
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.123174610664137e-05
8.066667760431301e-06
Batch: 0; loss: 1.81; acc: 0.41
Batch: 20; loss: 1.9; acc: 0.3
Batch: 40; loss: 1.58; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.68; acc: 0.5
Batch: 100; loss: 1.68; acc: 0.55
Batch: 120; loss: 1.82; acc: 0.41
Batch: 140; loss: 1.71; acc: 0.55
Val Epoch over. val_loss: 1.7219113195018403; val_accuracy: 0.4951234076433121 

The current subspace-distance is: 8.066667760431301e-06 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.75; acc: 0.48
Batch: 60; loss: 1.73; acc: 0.5
Batch: 80; loss: 1.6; acc: 0.62
Batch: 100; loss: 1.67; acc: 0.55
Batch: 120; loss: 1.76; acc: 0.47
Batch: 140; loss: 1.7; acc: 0.55
Batch: 160; loss: 1.83; acc: 0.42
Batch: 180; loss: 1.8; acc: 0.48
Batch: 200; loss: 1.65; acc: 0.5
Batch: 220; loss: 1.79; acc: 0.39
Batch: 240; loss: 1.72; acc: 0.5
Batch: 260; loss: 1.75; acc: 0.5
Batch: 280; loss: 1.75; acc: 0.42
Batch: 300; loss: 1.78; acc: 0.44
Batch: 320; loss: 1.72; acc: 0.47
Batch: 340; loss: 1.59; acc: 0.59
Batch: 360; loss: 1.74; acc: 0.47
Batch: 380; loss: 1.74; acc: 0.45
Batch: 400; loss: 1.83; acc: 0.38
Batch: 420; loss: 1.73; acc: 0.52
Batch: 440; loss: 1.62; acc: 0.52
Batch: 460; loss: 1.84; acc: 0.52
Batch: 480; loss: 1.58; acc: 0.64
Batch: 500; loss: 1.66; acc: 0.61
Batch: 520; loss: 1.7; acc: 0.5
Batch: 540; loss: 1.8; acc: 0.45
Batch: 560; loss: 1.85; acc: 0.38
Batch: 580; loss: 1.74; acc: 0.47
Batch: 600; loss: 1.78; acc: 0.44
Batch: 620; loss: 1.79; acc: 0.5
Batch: 640; loss: 1.71; acc: 0.58
Batch: 660; loss: 1.75; acc: 0.47
Batch: 680; loss: 1.75; acc: 0.47
Batch: 700; loss: 1.79; acc: 0.47
Batch: 720; loss: 1.75; acc: 0.5
Batch: 740; loss: 1.79; acc: 0.48
Batch: 760; loss: 1.74; acc: 0.5
Batch: 780; loss: 1.88; acc: 0.44
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.1328741897596046e-05
9.107810001296457e-06
Batch: 0; loss: 1.81; acc: 0.41
Batch: 20; loss: 1.89; acc: 0.31
Batch: 40; loss: 1.59; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.52
Batch: 80; loss: 1.69; acc: 0.52
Batch: 100; loss: 1.67; acc: 0.58
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.71; acc: 0.58
Val Epoch over. val_loss: 1.7209691659660096; val_accuracy: 0.5002985668789809 

The current subspace-distance is: 9.107810001296457e-06 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.5
Batch: 20; loss: 1.79; acc: 0.47
Batch: 40; loss: 1.76; acc: 0.47
Batch: 60; loss: 1.73; acc: 0.52
Batch: 80; loss: 1.81; acc: 0.34
Batch: 100; loss: 1.69; acc: 0.55
Batch: 120; loss: 1.78; acc: 0.47
Batch: 140; loss: 1.93; acc: 0.33
Batch: 160; loss: 1.8; acc: 0.45
Batch: 180; loss: 1.71; acc: 0.5
Batch: 200; loss: 1.78; acc: 0.48
Batch: 220; loss: 1.79; acc: 0.47
Batch: 240; loss: 1.75; acc: 0.41
Batch: 260; loss: 1.76; acc: 0.48
Batch: 280; loss: 1.67; acc: 0.58
Batch: 300; loss: 1.7; acc: 0.5
Batch: 320; loss: 1.91; acc: 0.39
Batch: 340; loss: 1.65; acc: 0.55
Batch: 360; loss: 1.63; acc: 0.56
Batch: 380; loss: 1.67; acc: 0.42
Batch: 400; loss: 1.65; acc: 0.55
Batch: 420; loss: 1.8; acc: 0.38
Batch: 440; loss: 1.64; acc: 0.56
Batch: 460; loss: 1.77; acc: 0.52
Batch: 480; loss: 1.7; acc: 0.48
Batch: 500; loss: 1.76; acc: 0.48
Batch: 520; loss: 1.74; acc: 0.52
Batch: 540; loss: 1.7; acc: 0.52
Batch: 560; loss: 1.65; acc: 0.52
Batch: 580; loss: 1.68; acc: 0.58
Batch: 600; loss: 1.78; acc: 0.45
Batch: 620; loss: 1.74; acc: 0.47
Batch: 640; loss: 1.71; acc: 0.53
Batch: 660; loss: 1.86; acc: 0.36
Batch: 680; loss: 1.77; acc: 0.45
Batch: 700; loss: 1.82; acc: 0.39
Batch: 720; loss: 1.73; acc: 0.48
Batch: 740; loss: 1.71; acc: 0.52
Batch: 760; loss: 1.78; acc: 0.48
Batch: 780; loss: 1.79; acc: 0.45
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.218568963347934e-05
1.0088964700116776e-05
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.31
Batch: 40; loss: 1.58; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.69; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.55
Batch: 120; loss: 1.81; acc: 0.39
Batch: 140; loss: 1.7; acc: 0.56
Val Epoch over. val_loss: 1.7177742507047713; val_accuracy: 0.4971138535031847 

The current subspace-distance is: 1.0088964700116776e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.75; acc: 0.52
Batch: 20; loss: 1.71; acc: 0.52
Batch: 40; loss: 1.76; acc: 0.52
Batch: 60; loss: 1.69; acc: 0.56
Batch: 80; loss: 1.71; acc: 0.56
Batch: 100; loss: 1.69; acc: 0.59
Batch: 120; loss: 1.82; acc: 0.48
Batch: 140; loss: 1.62; acc: 0.58
Batch: 160; loss: 1.66; acc: 0.55
Batch: 180; loss: 1.78; acc: 0.39
Batch: 200; loss: 1.66; acc: 0.53
Batch: 220; loss: 1.76; acc: 0.45
Batch: 240; loss: 1.79; acc: 0.45
Batch: 260; loss: 1.74; acc: 0.58
Batch: 280; loss: 1.77; acc: 0.44
Batch: 300; loss: 1.78; acc: 0.45
Batch: 320; loss: 1.86; acc: 0.39
Batch: 340; loss: 1.69; acc: 0.53
Batch: 360; loss: 1.79; acc: 0.39
Batch: 380; loss: 1.87; acc: 0.36
Batch: 400; loss: 1.78; acc: 0.44
Batch: 420; loss: 1.83; acc: 0.42
Batch: 440; loss: 1.95; acc: 0.38
Batch: 460; loss: 1.77; acc: 0.47
Batch: 480; loss: 1.7; acc: 0.58
Batch: 500; loss: 1.81; acc: 0.42
Batch: 520; loss: 1.84; acc: 0.38
Batch: 540; loss: 1.79; acc: 0.42
Batch: 560; loss: 1.66; acc: 0.48
Batch: 580; loss: 1.64; acc: 0.52
Batch: 600; loss: 1.66; acc: 0.56
Batch: 620; loss: 1.78; acc: 0.48
Batch: 640; loss: 1.71; acc: 0.52
Batch: 660; loss: 1.75; acc: 0.45
Batch: 680; loss: 1.67; acc: 0.55
Batch: 700; loss: 1.6; acc: 0.59
Batch: 720; loss: 1.65; acc: 0.52
Batch: 740; loss: 1.74; acc: 0.44
Batch: 760; loss: 1.84; acc: 0.52
Batch: 780; loss: 1.85; acc: 0.41
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.128635580651462e-05
8.590272045694292e-06
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.58; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.68; acc: 0.47
Batch: 100; loss: 1.68; acc: 0.58
Batch: 120; loss: 1.81; acc: 0.42
Batch: 140; loss: 1.69; acc: 0.56
Val Epoch over. val_loss: 1.7164733432660437; val_accuracy: 0.5009952229299363 

The current subspace-distance is: 8.590272045694292e-06 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.42
Batch: 20; loss: 1.79; acc: 0.52
Batch: 40; loss: 1.61; acc: 0.64
Batch: 60; loss: 1.84; acc: 0.45
Batch: 80; loss: 1.6; acc: 0.64
Batch: 100; loss: 1.65; acc: 0.55
Batch: 120; loss: 1.71; acc: 0.47
Batch: 140; loss: 1.78; acc: 0.45
Batch: 160; loss: 1.78; acc: 0.52
Batch: 180; loss: 1.77; acc: 0.44
Batch: 200; loss: 1.77; acc: 0.45
Batch: 220; loss: 1.78; acc: 0.39
Batch: 240; loss: 1.75; acc: 0.41
Batch: 260; loss: 1.72; acc: 0.44
Batch: 280; loss: 1.63; acc: 0.52
Batch: 300; loss: 1.78; acc: 0.52
Batch: 320; loss: 1.82; acc: 0.39
Batch: 340; loss: 1.75; acc: 0.48
Batch: 360; loss: 1.71; acc: 0.41
Batch: 380; loss: 1.69; acc: 0.53
Batch: 400; loss: 1.71; acc: 0.45
Batch: 420; loss: 1.68; acc: 0.56
Batch: 440; loss: 1.73; acc: 0.48
Batch: 460; loss: 1.67; acc: 0.52
Batch: 480; loss: 1.74; acc: 0.55
Batch: 500; loss: 1.66; acc: 0.5
Batch: 520; loss: 1.8; acc: 0.39
Batch: 540; loss: 1.66; acc: 0.55
Batch: 560; loss: 1.63; acc: 0.52
Batch: 580; loss: 1.71; acc: 0.45
Batch: 600; loss: 1.75; acc: 0.48
Batch: 620; loss: 1.75; acc: 0.5
Batch: 640; loss: 1.81; acc: 0.39
Batch: 660; loss: 1.75; acc: 0.45
Batch: 680; loss: 1.77; acc: 0.48
Batch: 700; loss: 1.78; acc: 0.44
Batch: 720; loss: 1.82; acc: 0.47
Batch: 740; loss: 1.7; acc: 0.47
Batch: 760; loss: 1.87; acc: 0.42
Batch: 780; loss: 1.84; acc: 0.41
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.217836274416186e-05
1.0081095751957037e-05
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.9; acc: 0.3
Batch: 40; loss: 1.59; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.68; acc: 0.47
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.81; acc: 0.42
Batch: 140; loss: 1.7; acc: 0.53
Val Epoch over. val_loss: 1.7156489631932252; val_accuracy: 0.4971138535031847 

The current subspace-distance is: 1.0081095751957037e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.68; acc: 0.5
Batch: 40; loss: 1.61; acc: 0.53
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.75; acc: 0.45
Batch: 100; loss: 1.9; acc: 0.39
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.84; acc: 0.44
Batch: 160; loss: 1.73; acc: 0.56
Batch: 180; loss: 1.76; acc: 0.45
Batch: 200; loss: 1.71; acc: 0.53
Batch: 220; loss: 1.67; acc: 0.48
Batch: 240; loss: 1.78; acc: 0.47
Batch: 260; loss: 1.79; acc: 0.48
Batch: 280; loss: 1.8; acc: 0.39
Batch: 300; loss: 1.67; acc: 0.59
Batch: 320; loss: 1.92; acc: 0.36
Batch: 340; loss: 1.79; acc: 0.5
Batch: 360; loss: 1.79; acc: 0.48
Batch: 380; loss: 1.63; acc: 0.53
Batch: 400; loss: 1.66; acc: 0.59
Batch: 420; loss: 1.85; acc: 0.41
Batch: 440; loss: 1.71; acc: 0.55
Batch: 460; loss: 1.81; acc: 0.41
Batch: 480; loss: 1.72; acc: 0.47
Batch: 500; loss: 1.79; acc: 0.45
Batch: 520; loss: 1.77; acc: 0.48
Batch: 540; loss: 1.68; acc: 0.55
Batch: 560; loss: 1.75; acc: 0.5
Batch: 580; loss: 1.77; acc: 0.42
Batch: 600; loss: 1.7; acc: 0.48
Batch: 620; loss: 1.85; acc: 0.36
Batch: 640; loss: 1.73; acc: 0.44
Batch: 660; loss: 1.72; acc: 0.53
Batch: 680; loss: 1.77; acc: 0.44
Batch: 700; loss: 1.8; acc: 0.47
Batch: 720; loss: 1.66; acc: 0.52
Batch: 740; loss: 1.83; acc: 0.39
Batch: 760; loss: 1.59; acc: 0.59
Batch: 780; loss: 1.71; acc: 0.47
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

3.2158641261048615e-05
8.903150956030004e-06
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.59; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.69; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.58
Batch: 120; loss: 1.8; acc: 0.39
Batch: 140; loss: 1.69; acc: 0.5
Val Epoch over. val_loss: 1.716222273316353; val_accuracy: 0.4984076433121019 

The current subspace-distance is: 8.903150956030004e-06 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.81; acc: 0.36
Batch: 40; loss: 1.8; acc: 0.38
Batch: 60; loss: 1.9; acc: 0.38
Batch: 80; loss: 1.63; acc: 0.61
Batch: 100; loss: 1.78; acc: 0.38
Batch: 120; loss: 1.77; acc: 0.45
Batch: 140; loss: 1.74; acc: 0.44
Batch: 160; loss: 1.83; acc: 0.47
Batch: 180; loss: 1.82; acc: 0.44
Batch: 200; loss: 1.65; acc: 0.58
Batch: 220; loss: 1.79; acc: 0.47
Batch: 240; loss: 1.61; acc: 0.62
Batch: 260; loss: 1.75; acc: 0.48
Batch: 280; loss: 1.66; acc: 0.48
Batch: 300; loss: 1.73; acc: 0.5
Batch: 320; loss: 1.87; acc: 0.44
Batch: 340; loss: 1.72; acc: 0.52
Batch: 360; loss: 1.83; acc: 0.44
Batch: 380; loss: 1.64; acc: 0.55
Batch: 400; loss: 1.89; acc: 0.36
Batch: 420; loss: 1.7; acc: 0.53
Batch: 440; loss: 1.7; acc: 0.5
Batch: 460; loss: 1.8; acc: 0.38
Batch: 480; loss: 1.82; acc: 0.41
Batch: 500; loss: 1.74; acc: 0.53
Batch: 520; loss: 1.74; acc: 0.5
Batch: 540; loss: 1.67; acc: 0.61
Batch: 560; loss: 1.7; acc: 0.48
Batch: 580; loss: 1.77; acc: 0.47
Batch: 600; loss: 1.68; acc: 0.48
Batch: 620; loss: 1.76; acc: 0.5
Batch: 640; loss: 1.81; acc: 0.39
Batch: 660; loss: 1.86; acc: 0.42
Batch: 680; loss: 1.75; acc: 0.52
Batch: 700; loss: 1.69; acc: 0.5
Batch: 720; loss: 1.75; acc: 0.48
Batch: 740; loss: 1.84; acc: 0.36
Batch: 760; loss: 1.66; acc: 0.55
Batch: 780; loss: 1.6; acc: 0.58
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.255584670114331e-05
9.176425010082312e-06
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.28
Batch: 40; loss: 1.58; acc: 0.56
Batch: 60; loss: 1.69; acc: 0.48
Batch: 80; loss: 1.68; acc: 0.45
Batch: 100; loss: 1.7; acc: 0.58
Batch: 120; loss: 1.8; acc: 0.42
Batch: 140; loss: 1.7; acc: 0.52
Val Epoch over. val_loss: 1.7181370501305646; val_accuracy: 0.49920382165605093 

The current subspace-distance is: 9.176425010082312e-06 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.82; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.58
Batch: 40; loss: 1.76; acc: 0.48
Batch: 60; loss: 1.65; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.53
Batch: 100; loss: 1.8; acc: 0.41
Batch: 120; loss: 1.71; acc: 0.56
Batch: 140; loss: 1.81; acc: 0.39
Batch: 160; loss: 1.71; acc: 0.52
Batch: 180; loss: 1.81; acc: 0.48
Batch: 200; loss: 1.73; acc: 0.53
Batch: 220; loss: 1.76; acc: 0.44
Batch: 240; loss: 1.73; acc: 0.45
Batch: 260; loss: 1.67; acc: 0.58
Batch: 280; loss: 1.68; acc: 0.53
Batch: 300; loss: 1.72; acc: 0.5
Batch: 320; loss: 1.63; acc: 0.59
Batch: 340; loss: 1.6; acc: 0.58
Batch: 360; loss: 1.73; acc: 0.5
Batch: 380; loss: 1.66; acc: 0.5
Batch: 400; loss: 1.72; acc: 0.52
Batch: 420; loss: 1.66; acc: 0.55
Batch: 440; loss: 1.77; acc: 0.39
Batch: 460; loss: 1.78; acc: 0.44
Batch: 480; loss: 1.84; acc: 0.39
Batch: 500; loss: 1.8; acc: 0.42
Batch: 520; loss: 1.66; acc: 0.5
Batch: 540; loss: 1.72; acc: 0.48
Batch: 560; loss: 1.76; acc: 0.55
Batch: 580; loss: 1.77; acc: 0.47
Batch: 600; loss: 1.75; acc: 0.52
Batch: 620; loss: 1.82; acc: 0.44
Batch: 640; loss: 1.64; acc: 0.56
Batch: 660; loss: 1.64; acc: 0.56
Batch: 680; loss: 1.79; acc: 0.44
Batch: 700; loss: 1.75; acc: 0.56
Batch: 720; loss: 1.82; acc: 0.45
Batch: 740; loss: 1.81; acc: 0.41
Batch: 760; loss: 1.8; acc: 0.42
Batch: 780; loss: 1.91; acc: 0.38
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.26443332596682e-05
9.15844339033356e-06
Batch: 0; loss: 1.81; acc: 0.39
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 1.58; acc: 0.56
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.48
Batch: 100; loss: 1.7; acc: 0.56
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.69; acc: 0.5
Val Epoch over. val_loss: 1.7107930783253567; val_accuracy: 0.49751194267515925 

The current subspace-distance is: 9.15844339033356e-06 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.74; acc: 0.5
Batch: 20; loss: 1.74; acc: 0.45
Batch: 40; loss: 1.75; acc: 0.48
Batch: 60; loss: 1.78; acc: 0.42
Batch: 80; loss: 1.69; acc: 0.5
Batch: 100; loss: 1.84; acc: 0.44
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.87; acc: 0.42
Batch: 160; loss: 1.78; acc: 0.47
Batch: 180; loss: 1.66; acc: 0.52
Batch: 200; loss: 1.81; acc: 0.44
Batch: 220; loss: 1.69; acc: 0.55
Batch: 240; loss: 1.74; acc: 0.48
Batch: 260; loss: 1.62; acc: 0.61
Batch: 280; loss: 1.78; acc: 0.52
Batch: 300; loss: 1.7; acc: 0.53
Batch: 320; loss: 1.66; acc: 0.61
Batch: 340; loss: 1.7; acc: 0.47
Batch: 360; loss: 1.9; acc: 0.33
Batch: 380; loss: 1.72; acc: 0.44
Batch: 400; loss: 1.73; acc: 0.5
Batch: 420; loss: 1.77; acc: 0.47
Batch: 440; loss: 1.78; acc: 0.44
Batch: 460; loss: 1.74; acc: 0.41
Batch: 480; loss: 1.83; acc: 0.44
Batch: 500; loss: 1.8; acc: 0.42
Batch: 520; loss: 1.72; acc: 0.44
Batch: 540; loss: 1.72; acc: 0.55
Batch: 560; loss: 1.88; acc: 0.38
Batch: 580; loss: 1.77; acc: 0.42
Batch: 600; loss: 1.8; acc: 0.36
Batch: 620; loss: 1.89; acc: 0.41
Batch: 640; loss: 1.66; acc: 0.56
Batch: 660; loss: 1.84; acc: 0.41
Batch: 680; loss: 1.64; acc: 0.55
Batch: 700; loss: 1.64; acc: 0.58
Batch: 720; loss: 1.8; acc: 0.44
Batch: 740; loss: 1.64; acc: 0.58
Batch: 760; loss: 1.74; acc: 0.47
Batch: 780; loss: 1.71; acc: 0.5
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.27059933624696e-05
1.010216601571301e-05
Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.58; acc: 0.61
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.69; acc: 0.52
Val Epoch over. val_loss: 1.7118964984918097; val_accuracy: 0.5042794585987261 

The current subspace-distance is: 1.010216601571301e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.67; acc: 0.53
Batch: 20; loss: 1.77; acc: 0.45
Batch: 40; loss: 1.78; acc: 0.45
Batch: 60; loss: 1.63; acc: 0.59
Batch: 80; loss: 1.73; acc: 0.39
Batch: 100; loss: 1.67; acc: 0.55
Batch: 120; loss: 1.71; acc: 0.47
Batch: 140; loss: 1.77; acc: 0.41
Batch: 160; loss: 1.76; acc: 0.5
Batch: 180; loss: 1.89; acc: 0.42
Batch: 200; loss: 1.91; acc: 0.34
Batch: 220; loss: 1.68; acc: 0.55
Batch: 240; loss: 1.8; acc: 0.42
Batch: 260; loss: 1.65; acc: 0.56
Batch: 280; loss: 1.73; acc: 0.52
Batch: 300; loss: 1.82; acc: 0.39
Batch: 320; loss: 1.84; acc: 0.48
Batch: 340; loss: 1.65; acc: 0.48
Batch: 360; loss: 1.59; acc: 0.61
Batch: 380; loss: 1.74; acc: 0.48
Batch: 400; loss: 1.85; acc: 0.38
Batch: 420; loss: 1.91; acc: 0.36
Batch: 440; loss: 1.81; acc: 0.42
Batch: 460; loss: 1.75; acc: 0.48
Batch: 480; loss: 1.79; acc: 0.47
Batch: 500; loss: 1.76; acc: 0.5
Batch: 520; loss: 1.81; acc: 0.38
Batch: 540; loss: 1.73; acc: 0.52
Batch: 560; loss: 1.75; acc: 0.47
Batch: 580; loss: 1.73; acc: 0.53
Batch: 600; loss: 1.7; acc: 0.47
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.89; acc: 0.39
Batch: 660; loss: 1.8; acc: 0.42
Batch: 680; loss: 1.69; acc: 0.55
Batch: 700; loss: 1.74; acc: 0.45
Batch: 720; loss: 1.68; acc: 0.55
Batch: 740; loss: 1.82; acc: 0.42
Batch: 760; loss: 1.78; acc: 0.5
Batch: 780; loss: 1.71; acc: 0.42
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.220294456696138e-05
9.144514478975907e-06
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.59; acc: 0.61
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.67; acc: 0.47
Batch: 100; loss: 1.69; acc: 0.58
Batch: 120; loss: 1.81; acc: 0.42
Batch: 140; loss: 1.69; acc: 0.56
Val Epoch over. val_loss: 1.7163175473547285; val_accuracy: 0.4997014331210191 

The current subspace-distance is: 9.144514478975907e-06 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.8; acc: 0.45
Batch: 20; loss: 1.86; acc: 0.44
Batch: 40; loss: 1.8; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.53
Batch: 80; loss: 1.9; acc: 0.34
Batch: 100; loss: 1.81; acc: 0.42
Batch: 120; loss: 1.73; acc: 0.48
Batch: 140; loss: 1.86; acc: 0.33
Batch: 160; loss: 1.63; acc: 0.56
Batch: 180; loss: 1.8; acc: 0.44
Batch: 200; loss: 1.8; acc: 0.44
Batch: 220; loss: 1.7; acc: 0.47
Batch: 240; loss: 1.77; acc: 0.47
Batch: 260; loss: 1.67; acc: 0.53
Batch: 280; loss: 1.75; acc: 0.52
Batch: 300; loss: 1.73; acc: 0.5
Batch: 320; loss: 1.79; acc: 0.45
Batch: 340; loss: 1.72; acc: 0.47
Batch: 360; loss: 1.74; acc: 0.45
Batch: 380; loss: 1.76; acc: 0.5
Batch: 400; loss: 1.82; acc: 0.39
Batch: 420; loss: 1.67; acc: 0.53
Batch: 440; loss: 1.71; acc: 0.48
Batch: 460; loss: 1.8; acc: 0.45
Batch: 480; loss: 1.69; acc: 0.55
Batch: 500; loss: 1.72; acc: 0.48
Batch: 520; loss: 1.8; acc: 0.45
Batch: 540; loss: 1.67; acc: 0.53
Batch: 560; loss: 1.84; acc: 0.47
Batch: 580; loss: 1.77; acc: 0.48
Batch: 600; loss: 1.76; acc: 0.48
Batch: 620; loss: 1.72; acc: 0.41
Batch: 640; loss: 1.7; acc: 0.52
Batch: 660; loss: 1.76; acc: 0.39
Batch: 680; loss: 1.64; acc: 0.58
Batch: 700; loss: 1.74; acc: 0.42
Batch: 720; loss: 1.6; acc: 0.53
Batch: 740; loss: 1.8; acc: 0.48
Batch: 760; loss: 1.85; acc: 0.45
Batch: 780; loss: 1.72; acc: 0.55
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.181397187290713e-05
8.17431282484904e-06
Batch: 0; loss: 1.81; acc: 0.41
Batch: 20; loss: 1.88; acc: 0.31
Batch: 40; loss: 1.57; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.47
Batch: 100; loss: 1.68; acc: 0.58
Batch: 120; loss: 1.8; acc: 0.41
Batch: 140; loss: 1.68; acc: 0.53
Val Epoch over. val_loss: 1.7056404868508601; val_accuracy: 0.501890923566879 

The current subspace-distance is: 8.17431282484904e-06 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.64
Batch: 20; loss: 1.76; acc: 0.47
Batch: 40; loss: 1.66; acc: 0.5
Batch: 60; loss: 1.82; acc: 0.41
Batch: 80; loss: 1.73; acc: 0.44
Batch: 100; loss: 1.64; acc: 0.52
Batch: 120; loss: 1.81; acc: 0.38
Batch: 140; loss: 1.72; acc: 0.44
Batch: 160; loss: 1.7; acc: 0.52
Batch: 180; loss: 1.74; acc: 0.47
Batch: 200; loss: 1.79; acc: 0.39
Batch: 220; loss: 1.81; acc: 0.44
Batch: 240; loss: 1.72; acc: 0.42
Batch: 260; loss: 1.74; acc: 0.47
Batch: 280; loss: 1.71; acc: 0.56
Batch: 300; loss: 1.68; acc: 0.55
Batch: 320; loss: 1.73; acc: 0.5
Batch: 340; loss: 1.75; acc: 0.5
Batch: 360; loss: 1.81; acc: 0.44
Batch: 380; loss: 1.76; acc: 0.45
Batch: 400; loss: 1.71; acc: 0.48
Batch: 420; loss: 1.78; acc: 0.45
Batch: 440; loss: 1.77; acc: 0.42
Batch: 460; loss: 1.92; acc: 0.36
Batch: 480; loss: 1.59; acc: 0.58
Batch: 500; loss: 1.7; acc: 0.5
Batch: 520; loss: 1.79; acc: 0.42
Batch: 540; loss: 1.77; acc: 0.47
Batch: 560; loss: 1.67; acc: 0.53
Batch: 580; loss: 1.7; acc: 0.45
Batch: 600; loss: 1.84; acc: 0.38
Batch: 620; loss: 1.74; acc: 0.45
Batch: 640; loss: 1.8; acc: 0.53
Batch: 660; loss: 1.79; acc: 0.34
Batch: 680; loss: 1.75; acc: 0.48
Batch: 700; loss: 1.71; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.53
Batch: 740; loss: 1.69; acc: 0.52
Batch: 760; loss: 1.64; acc: 0.59
Batch: 780; loss: 1.79; acc: 0.42
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.215029209968634e-05
9.574337127560284e-06
Batch: 0; loss: 1.8; acc: 0.41
Batch: 20; loss: 1.88; acc: 0.33
Batch: 40; loss: 1.58; acc: 0.61
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.47
Batch: 100; loss: 1.69; acc: 0.58
Batch: 120; loss: 1.8; acc: 0.42
Batch: 140; loss: 1.68; acc: 0.55
Val Epoch over. val_loss: 1.7125273282360878; val_accuracy: 0.5017914012738853 

The current subspace-distance is: 9.574337127560284e-06 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.48
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.74; acc: 0.53
Batch: 60; loss: 1.72; acc: 0.53
Batch: 80; loss: 1.86; acc: 0.42
Batch: 100; loss: 1.7; acc: 0.52
Batch: 120; loss: 1.79; acc: 0.44
Batch: 140; loss: 1.79; acc: 0.42
Batch: 160; loss: 1.73; acc: 0.5
Batch: 180; loss: 1.73; acc: 0.52
Batch: 200; loss: 1.74; acc: 0.45
Batch: 220; loss: 1.82; acc: 0.38
Batch: 240; loss: 1.9; acc: 0.34
Batch: 260; loss: 1.69; acc: 0.52
Batch: 280; loss: 1.64; acc: 0.56
Batch: 300; loss: 1.79; acc: 0.48
Batch: 320; loss: 1.77; acc: 0.44
Batch: 340; loss: 1.68; acc: 0.5
Batch: 360; loss: 1.79; acc: 0.36
Batch: 380; loss: 1.77; acc: 0.48
Batch: 400; loss: 1.73; acc: 0.41
Batch: 420; loss: 1.7; acc: 0.52
Batch: 440; loss: 1.69; acc: 0.55
Batch: 460; loss: 1.69; acc: 0.52
Batch: 480; loss: 1.69; acc: 0.5
Batch: 500; loss: 1.61; acc: 0.59
Batch: 520; loss: 1.8; acc: 0.5
Batch: 540; loss: 1.79; acc: 0.47
Batch: 560; loss: 1.82; acc: 0.52
Batch: 580; loss: 1.75; acc: 0.5
Batch: 600; loss: 1.76; acc: 0.44
Batch: 620; loss: 1.67; acc: 0.61
Batch: 640; loss: 1.75; acc: 0.44
Batch: 660; loss: 1.61; acc: 0.56
Batch: 680; loss: 1.71; acc: 0.5
Batch: 700; loss: 1.76; acc: 0.52
Batch: 720; loss: 1.69; acc: 0.42
Batch: 740; loss: 1.81; acc: 0.41
Batch: 760; loss: 1.75; acc: 0.53
Batch: 780; loss: 1.72; acc: 0.44
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.156106686219573e-05
8.192091627279297e-06
Batch: 0; loss: 1.81; acc: 0.39
Batch: 20; loss: 1.87; acc: 0.33
Batch: 40; loss: 1.58; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.8; acc: 0.41
Batch: 140; loss: 1.67; acc: 0.55
Val Epoch over. val_loss: 1.7100442124020523; val_accuracy: 0.4980095541401274 

The current subspace-distance is: 8.192091627279297e-06 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.66; acc: 0.56
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.94; acc: 0.28
Batch: 60; loss: 1.77; acc: 0.36
Batch: 80; loss: 1.74; acc: 0.44
Batch: 100; loss: 1.65; acc: 0.53
Batch: 120; loss: 1.82; acc: 0.34
Batch: 140; loss: 1.85; acc: 0.41
Batch: 160; loss: 1.81; acc: 0.52
Batch: 180; loss: 1.75; acc: 0.45
Batch: 200; loss: 1.75; acc: 0.47
Batch: 220; loss: 1.8; acc: 0.41
Batch: 240; loss: 1.64; acc: 0.52
Batch: 260; loss: 1.72; acc: 0.48
Batch: 280; loss: 1.79; acc: 0.47
Batch: 300; loss: 1.75; acc: 0.53
Batch: 320; loss: 1.67; acc: 0.55
Batch: 340; loss: 1.74; acc: 0.48
Batch: 360; loss: 1.82; acc: 0.41
Batch: 380; loss: 1.73; acc: 0.48
Batch: 400; loss: 1.62; acc: 0.55
Batch: 420; loss: 1.91; acc: 0.42
Batch: 440; loss: 1.73; acc: 0.47
Batch: 460; loss: 1.62; acc: 0.55
Batch: 480; loss: 1.68; acc: 0.53
Batch: 500; loss: 1.77; acc: 0.5
Batch: 520; loss: 1.7; acc: 0.47
Batch: 540; loss: 1.76; acc: 0.5
Batch: 560; loss: 1.81; acc: 0.42
Batch: 580; loss: 1.63; acc: 0.56
Batch: 600; loss: 1.73; acc: 0.48
Batch: 620; loss: 1.59; acc: 0.56
Batch: 640; loss: 1.88; acc: 0.39
Batch: 660; loss: 1.67; acc: 0.48
Batch: 680; loss: 1.75; acc: 0.45
Batch: 700; loss: 1.67; acc: 0.55
Batch: 720; loss: 1.74; acc: 0.42
Batch: 740; loss: 1.72; acc: 0.5
Batch: 760; loss: 1.75; acc: 0.5
Batch: 780; loss: 1.8; acc: 0.44
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.1927924283081666e-05
8.308553333336022e-06
Batch: 0; loss: 1.81; acc: 0.41
Batch: 20; loss: 1.88; acc: 0.39
Batch: 40; loss: 1.58; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.47
Batch: 80; loss: 1.67; acc: 0.48
Batch: 100; loss: 1.7; acc: 0.55
Batch: 120; loss: 1.8; acc: 0.41
Batch: 140; loss: 1.68; acc: 0.5
Val Epoch over. val_loss: 1.7144739855626585; val_accuracy: 0.4997014331210191 

The current subspace-distance is: 8.308553333336022e-06 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.79; acc: 0.52
Batch: 20; loss: 1.72; acc: 0.5
Batch: 40; loss: 1.79; acc: 0.42
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.69; acc: 0.5
Batch: 100; loss: 1.77; acc: 0.42
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.8; acc: 0.36
Batch: 160; loss: 1.76; acc: 0.52
Batch: 180; loss: 1.63; acc: 0.56
Batch: 200; loss: 1.63; acc: 0.52
Batch: 220; loss: 1.73; acc: 0.56
Batch: 240; loss: 1.78; acc: 0.42
Batch: 260; loss: 1.75; acc: 0.47
Batch: 280; loss: 1.69; acc: 0.52
Batch: 300; loss: 1.65; acc: 0.55
Batch: 320; loss: 1.75; acc: 0.55
Batch: 340; loss: 1.69; acc: 0.5
Batch: 360; loss: 1.79; acc: 0.44
Batch: 380; loss: 1.73; acc: 0.48
Batch: 400; loss: 1.73; acc: 0.52
Batch: 420; loss: 1.71; acc: 0.44
Batch: 440; loss: 1.96; acc: 0.38
Batch: 460; loss: 1.75; acc: 0.5
Batch: 480; loss: 1.75; acc: 0.44
Batch: 500; loss: 1.69; acc: 0.44
Batch: 520; loss: 1.85; acc: 0.38
Batch: 540; loss: 1.6; acc: 0.53
Batch: 560; loss: 1.74; acc: 0.5
Batch: 580; loss: 1.7; acc: 0.58
Batch: 600; loss: 1.78; acc: 0.48
Batch: 620; loss: 1.66; acc: 0.44
Batch: 640; loss: 1.61; acc: 0.53
Batch: 660; loss: 1.75; acc: 0.52
Batch: 680; loss: 1.65; acc: 0.58
Batch: 700; loss: 1.68; acc: 0.53
Batch: 720; loss: 1.78; acc: 0.47
Batch: 740; loss: 1.89; acc: 0.39
Batch: 760; loss: 1.69; acc: 0.44
Batch: 780; loss: 1.85; acc: 0.41
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.2204199669649825e-05
8.68802180775674e-06
Batch: 0; loss: 1.8; acc: 0.42
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.57; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.47
Batch: 80; loss: 1.67; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.58
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.67; acc: 0.5
Val Epoch over. val_loss: 1.7076708678227321; val_accuracy: 0.5001990445859873 

The current subspace-distance is: 8.68802180775674e-06 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_11_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.835055857460475

The number of parameters is: 251109

The number of individual parameters is:

15
270
15
15
23
37605
23
23
45
112815
45
45
64
95040
64
64
4096
64
640
10
64
64

nonzero elements in E: 25110897
elements in E: 25110900
fraction nonzero: 0.999999880529969
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.34; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.08; acc: 0.27
Batch: 60; loss: 2.11; acc: 0.23
Batch: 80; loss: 2.0; acc: 0.33
Batch: 100; loss: 2.02; acc: 0.28
Batch: 120; loss: 2.08; acc: 0.27
Batch: 140; loss: 2.0; acc: 0.33
Batch: 160; loss: 1.91; acc: 0.44
Batch: 180; loss: 1.9; acc: 0.41
Batch: 200; loss: 1.8; acc: 0.39
Batch: 220; loss: 1.88; acc: 0.39
Batch: 240; loss: 1.81; acc: 0.42
Batch: 260; loss: 1.92; acc: 0.34
Batch: 280; loss: 1.84; acc: 0.44
Batch: 300; loss: 1.74; acc: 0.53
Batch: 320; loss: 1.81; acc: 0.45
Batch: 340; loss: 1.87; acc: 0.5
Batch: 360; loss: 1.75; acc: 0.5
Batch: 380; loss: 1.77; acc: 0.45
Batch: 400; loss: 1.72; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.48
Batch: 440; loss: 1.71; acc: 0.52
Batch: 460; loss: 1.65; acc: 0.56
Batch: 480; loss: 1.74; acc: 0.53
Batch: 500; loss: 1.7; acc: 0.53
Batch: 520; loss: 1.71; acc: 0.56
Batch: 540; loss: 1.72; acc: 0.53
Batch: 560; loss: 1.61; acc: 0.62
Batch: 580; loss: 1.59; acc: 0.7
Batch: 600; loss: 1.62; acc: 0.61
Batch: 620; loss: 1.69; acc: 0.44
Batch: 640; loss: 1.7; acc: 0.58
Batch: 660; loss: 1.63; acc: 0.66
Batch: 680; loss: 1.77; acc: 0.36
Batch: 700; loss: 1.75; acc: 0.38
Batch: 720; loss: 1.74; acc: 0.52
Batch: 740; loss: 1.6; acc: 0.66
Batch: 760; loss: 1.64; acc: 0.58
Batch: 780; loss: 1.62; acc: 0.58
Train Epoch over. train_loss: 1.84; train_accuracy: 0.43 

4.6637916966574267e-05
4.073119998793118e-05
Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.82; acc: 0.45
Batch: 40; loss: 1.41; acc: 0.8
Batch: 60; loss: 1.63; acc: 0.58
Batch: 80; loss: 1.54; acc: 0.64
Batch: 100; loss: 1.66; acc: 0.61
Batch: 120; loss: 1.67; acc: 0.55
Batch: 140; loss: 1.5; acc: 0.59
Val Epoch over. val_loss: 1.6190296191318778; val_accuracy: 0.5896695859872612 

The current subspace-distance is: 4.073119998793118e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.59; acc: 0.62
Batch: 20; loss: 1.71; acc: 0.48
Batch: 40; loss: 1.7; acc: 0.44
Batch: 60; loss: 1.78; acc: 0.48
Batch: 80; loss: 1.63; acc: 0.58
Batch: 100; loss: 1.65; acc: 0.59
Batch: 120; loss: 1.75; acc: 0.55
Batch: 140; loss: 1.54; acc: 0.72
Batch: 160; loss: 1.79; acc: 0.48
Batch: 180; loss: 1.52; acc: 0.66
Batch: 200; loss: 1.67; acc: 0.55
Batch: 220; loss: 1.59; acc: 0.55
Batch: 240; loss: 1.82; acc: 0.42
Batch: 260; loss: 1.81; acc: 0.45
Batch: 280; loss: 1.46; acc: 0.73
Batch: 300; loss: 1.51; acc: 0.62
Batch: 320; loss: 1.65; acc: 0.53
Batch: 340; loss: 1.59; acc: 0.53
Batch: 360; loss: 1.53; acc: 0.61
Batch: 380; loss: 1.6; acc: 0.52
Batch: 400; loss: 1.53; acc: 0.64
Batch: 420; loss: 1.53; acc: 0.67
Batch: 440; loss: 1.67; acc: 0.52
Batch: 460; loss: 1.47; acc: 0.7
Batch: 480; loss: 1.48; acc: 0.59
Batch: 500; loss: 1.5; acc: 0.64
Batch: 520; loss: 1.49; acc: 0.69
Batch: 540; loss: 1.52; acc: 0.64
Batch: 560; loss: 1.57; acc: 0.59
Batch: 580; loss: 1.62; acc: 0.58
Batch: 600; loss: 1.46; acc: 0.7
Batch: 620; loss: 1.46; acc: 0.66
Batch: 640; loss: 1.71; acc: 0.5
Batch: 660; loss: 1.47; acc: 0.67
Batch: 680; loss: 1.6; acc: 0.53
Batch: 700; loss: 1.61; acc: 0.56
Batch: 720; loss: 1.59; acc: 0.55
Batch: 740; loss: 1.6; acc: 0.58
Batch: 760; loss: 1.61; acc: 0.53
Batch: 780; loss: 1.42; acc: 0.67
Train Epoch over. train_loss: 1.57; train_accuracy: 0.59 

6.720423698425293e-05
6.232360465219244e-05
Batch: 0; loss: 1.54; acc: 0.59
Batch: 20; loss: 1.64; acc: 0.59
Batch: 40; loss: 1.2; acc: 0.81
Batch: 60; loss: 1.47; acc: 0.64
Batch: 80; loss: 1.42; acc: 0.66
Batch: 100; loss: 1.44; acc: 0.7
Batch: 120; loss: 1.52; acc: 0.59
Batch: 140; loss: 1.3; acc: 0.8
Val Epoch over. val_loss: 1.458552102374423; val_accuracy: 0.6541600318471338 

The current subspace-distance is: 6.232360465219244e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.59
Batch: 20; loss: 1.45; acc: 0.58
Batch: 40; loss: 1.46; acc: 0.62
Batch: 60; loss: 1.5; acc: 0.67
Batch: 80; loss: 1.51; acc: 0.67
Batch: 100; loss: 1.48; acc: 0.67
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.62
Batch: 160; loss: 1.41; acc: 0.7
Batch: 180; loss: 1.48; acc: 0.61
Batch: 200; loss: 1.47; acc: 0.64
Batch: 220; loss: 1.53; acc: 0.64
Batch: 240; loss: 1.47; acc: 0.59
Batch: 260; loss: 1.5; acc: 0.67
Batch: 280; loss: 1.42; acc: 0.69
Batch: 300; loss: 1.52; acc: 0.58
Batch: 320; loss: 1.45; acc: 0.59
Batch: 340; loss: 1.56; acc: 0.56
Batch: 360; loss: 1.52; acc: 0.58
Batch: 380; loss: 1.43; acc: 0.62
Batch: 400; loss: 1.37; acc: 0.67
Batch: 420; loss: 1.48; acc: 0.64
Batch: 440; loss: 1.51; acc: 0.58
Batch: 460; loss: 1.43; acc: 0.66
Batch: 480; loss: 1.48; acc: 0.61
Batch: 500; loss: 1.39; acc: 0.59
Batch: 520; loss: 1.39; acc: 0.62
Batch: 540; loss: 1.4; acc: 0.62
Batch: 560; loss: 1.27; acc: 0.72
Batch: 580; loss: 1.43; acc: 0.61
Batch: 600; loss: 1.45; acc: 0.53
Batch: 620; loss: 1.42; acc: 0.69
Batch: 640; loss: 1.33; acc: 0.67
Batch: 660; loss: 1.43; acc: 0.66
Batch: 680; loss: 1.49; acc: 0.56
Batch: 700; loss: 1.4; acc: 0.67
Batch: 720; loss: 1.35; acc: 0.7
Batch: 740; loss: 1.38; acc: 0.66
Batch: 760; loss: 1.41; acc: 0.61
Batch: 780; loss: 1.5; acc: 0.52
Train Epoch over. train_loss: 1.46; train_accuracy: 0.62 

8.315562445204705e-05
7.783366891089827e-05
Batch: 0; loss: 1.42; acc: 0.58
Batch: 20; loss: 1.54; acc: 0.61
Batch: 40; loss: 1.15; acc: 0.75
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.34; acc: 0.69
Batch: 100; loss: 1.3; acc: 0.75
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 1.22; acc: 0.78
Val Epoch over. val_loss: 1.368214544217298; val_accuracy: 0.6638136942675159 

The current subspace-distance is: 7.783366891089827e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.7
Batch: 20; loss: 1.51; acc: 0.62
Batch: 40; loss: 1.35; acc: 0.66
Batch: 60; loss: 1.37; acc: 0.66
Batch: 80; loss: 1.42; acc: 0.66
Batch: 100; loss: 1.31; acc: 0.72
Batch: 120; loss: 1.33; acc: 0.69
Batch: 140; loss: 1.37; acc: 0.7
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.44; acc: 0.61
Batch: 200; loss: 1.37; acc: 0.61
Batch: 220; loss: 1.4; acc: 0.56
Batch: 240; loss: 1.43; acc: 0.56
Batch: 260; loss: 1.31; acc: 0.67
Batch: 280; loss: 1.35; acc: 0.61
Batch: 300; loss: 1.34; acc: 0.62
Batch: 320; loss: 1.29; acc: 0.75
Batch: 340; loss: 1.4; acc: 0.69
Batch: 360; loss: 1.48; acc: 0.55
Batch: 380; loss: 1.46; acc: 0.61
Batch: 400; loss: 1.37; acc: 0.61
Batch: 420; loss: 1.2; acc: 0.78
Batch: 440; loss: 1.38; acc: 0.64
Batch: 460; loss: 1.33; acc: 0.67
Batch: 480; loss: 1.34; acc: 0.69
Batch: 500; loss: 1.46; acc: 0.47
Batch: 520; loss: 1.43; acc: 0.56
Batch: 540; loss: 1.29; acc: 0.64
Batch: 560; loss: 1.24; acc: 0.62
Batch: 580; loss: 1.28; acc: 0.69
Batch: 600; loss: 1.32; acc: 0.59
Batch: 620; loss: 1.25; acc: 0.7
Batch: 640; loss: 1.25; acc: 0.73
Batch: 660; loss: 1.14; acc: 0.78
Batch: 680; loss: 1.48; acc: 0.55
Batch: 700; loss: 1.29; acc: 0.69
Batch: 720; loss: 1.35; acc: 0.58
Batch: 740; loss: 1.36; acc: 0.61
Batch: 760; loss: 1.45; acc: 0.64
Batch: 780; loss: 1.47; acc: 0.58
Train Epoch over. train_loss: 1.38; train_accuracy: 0.63 

9.90487023955211e-05
9.322953701484948e-05
Batch: 0; loss: 1.36; acc: 0.58
Batch: 20; loss: 1.44; acc: 0.55
Batch: 40; loss: 1.08; acc: 0.75
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.26; acc: 0.66
Batch: 100; loss: 1.24; acc: 0.69
Batch: 120; loss: 1.35; acc: 0.62
Batch: 140; loss: 1.15; acc: 0.81
Val Epoch over. val_loss: 1.2972410218730854; val_accuracy: 0.6639132165605095 

The current subspace-distance is: 9.322953701484948e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.59
Batch: 20; loss: 1.33; acc: 0.72
Batch: 40; loss: 1.32; acc: 0.64
Batch: 60; loss: 1.31; acc: 0.62
Batch: 80; loss: 1.51; acc: 0.52
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 1.33; acc: 0.61
Batch: 160; loss: 1.4; acc: 0.55
Batch: 180; loss: 1.28; acc: 0.73
Batch: 200; loss: 1.2; acc: 0.8
Batch: 220; loss: 1.44; acc: 0.53
Batch: 240; loss: 1.33; acc: 0.67
Batch: 260; loss: 1.28; acc: 0.69
Batch: 280; loss: 1.4; acc: 0.62
Batch: 300; loss: 1.32; acc: 0.62
Batch: 320; loss: 1.4; acc: 0.58
Batch: 340; loss: 1.35; acc: 0.55
Batch: 360; loss: 1.42; acc: 0.56
Batch: 380; loss: 1.32; acc: 0.59
Batch: 400; loss: 1.36; acc: 0.64
Batch: 420; loss: 1.28; acc: 0.61
Batch: 440; loss: 1.4; acc: 0.58
Batch: 460; loss: 1.46; acc: 0.53
Batch: 480; loss: 1.26; acc: 0.72
Batch: 500; loss: 1.23; acc: 0.7
Batch: 520; loss: 1.32; acc: 0.69
Batch: 540; loss: 1.24; acc: 0.72
Batch: 560; loss: 1.45; acc: 0.53
Batch: 580; loss: 1.29; acc: 0.69
Batch: 600; loss: 1.37; acc: 0.66
Batch: 620; loss: 1.32; acc: 0.72
Batch: 640; loss: 1.28; acc: 0.73
Batch: 660; loss: 1.27; acc: 0.64
Batch: 680; loss: 1.37; acc: 0.55
Batch: 700; loss: 1.38; acc: 0.61
Batch: 720; loss: 1.34; acc: 0.64
Batch: 740; loss: 1.28; acc: 0.61
Batch: 760; loss: 1.23; acc: 0.69
Batch: 780; loss: 1.27; acc: 0.66
Train Epoch over. train_loss: 1.32; train_accuracy: 0.65 

0.00011165210889885202
0.000106543127913028
Batch: 0; loss: 1.3; acc: 0.58
Batch: 20; loss: 1.34; acc: 0.69
Batch: 40; loss: 0.99; acc: 0.86
Batch: 60; loss: 1.19; acc: 0.72
Batch: 80; loss: 1.17; acc: 0.73
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 1.07; acc: 0.78
Val Epoch over. val_loss: 1.2278522173310542; val_accuracy: 0.6858081210191083 

The current subspace-distance is: 0.000106543127913028 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.69
Batch: 20; loss: 1.34; acc: 0.66
Batch: 40; loss: 1.39; acc: 0.69
Batch: 60; loss: 1.18; acc: 0.77
Batch: 80; loss: 1.32; acc: 0.64
Batch: 100; loss: 1.44; acc: 0.53
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 1.32; acc: 0.64
Batch: 160; loss: 1.23; acc: 0.69
Batch: 180; loss: 1.28; acc: 0.69
Batch: 200; loss: 1.24; acc: 0.69
Batch: 220; loss: 1.22; acc: 0.7
Batch: 240; loss: 1.37; acc: 0.59
Batch: 260; loss: 1.35; acc: 0.59
Batch: 280; loss: 1.25; acc: 0.64
Batch: 300; loss: 1.18; acc: 0.7
Batch: 320; loss: 1.27; acc: 0.69
Batch: 340; loss: 1.44; acc: 0.55
Batch: 360; loss: 1.3; acc: 0.61
Batch: 380; loss: 1.33; acc: 0.61
Batch: 400; loss: 1.24; acc: 0.62
Batch: 420; loss: 1.3; acc: 0.64
Batch: 440; loss: 1.28; acc: 0.66
Batch: 460; loss: 1.27; acc: 0.64
Batch: 480; loss: 1.32; acc: 0.64
Batch: 500; loss: 1.41; acc: 0.59
Batch: 520; loss: 1.14; acc: 0.67
Batch: 540; loss: 1.18; acc: 0.61
Batch: 560; loss: 1.12; acc: 0.77
Batch: 580; loss: 1.16; acc: 0.73
Batch: 600; loss: 1.32; acc: 0.64
Batch: 620; loss: 0.97; acc: 0.83
Batch: 640; loss: 1.47; acc: 0.53
Batch: 660; loss: 1.15; acc: 0.67
Batch: 680; loss: 1.18; acc: 0.69
Batch: 700; loss: 1.34; acc: 0.64
Batch: 720; loss: 1.17; acc: 0.7
Batch: 740; loss: 1.17; acc: 0.69
Batch: 760; loss: 1.11; acc: 0.7
Batch: 780; loss: 1.17; acc: 0.7
Train Epoch over. train_loss: 1.26; train_accuracy: 0.66 

0.00012884711031801999
0.00012159349716966972
Batch: 0; loss: 1.23; acc: 0.58
Batch: 20; loss: 1.24; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.84
Batch: 60; loss: 1.1; acc: 0.73
Batch: 80; loss: 1.11; acc: 0.75
Batch: 100; loss: 1.13; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 0.99; acc: 0.78
Val Epoch over. val_loss: 1.1584978502267484; val_accuracy: 0.7108877388535032 

The current subspace-distance is: 0.00012159349716966972 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.29; acc: 0.67
Batch: 80; loss: 1.14; acc: 0.73
Batch: 100; loss: 1.33; acc: 0.53
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.23; acc: 0.66
Batch: 200; loss: 1.32; acc: 0.62
Batch: 220; loss: 1.3; acc: 0.59
Batch: 240; loss: 1.24; acc: 0.7
Batch: 260; loss: 1.34; acc: 0.61
Batch: 280; loss: 1.18; acc: 0.67
Batch: 300; loss: 1.1; acc: 0.73
Batch: 320; loss: 1.27; acc: 0.66
Batch: 340; loss: 1.42; acc: 0.55
Batch: 360; loss: 1.18; acc: 0.67
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 1.14; acc: 0.73
Batch: 420; loss: 1.14; acc: 0.75
Batch: 440; loss: 1.35; acc: 0.62
Batch: 460; loss: 1.18; acc: 0.62
Batch: 480; loss: 1.08; acc: 0.72
Batch: 500; loss: 1.23; acc: 0.67
Batch: 520; loss: 1.05; acc: 0.69
Batch: 540; loss: 1.12; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.69
Batch: 600; loss: 1.17; acc: 0.7
Batch: 620; loss: 1.35; acc: 0.59
Batch: 640; loss: 1.23; acc: 0.69
Batch: 660; loss: 1.26; acc: 0.61
Batch: 680; loss: 1.32; acc: 0.61
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 1.3; acc: 0.55
Batch: 740; loss: 1.15; acc: 0.75
Batch: 760; loss: 1.17; acc: 0.69
Batch: 780; loss: 1.35; acc: 0.56
Train Epoch over. train_loss: 1.2; train_accuracy: 0.68 

0.00013745414617005736
0.00013281794963404536
Batch: 0; loss: 1.22; acc: 0.59
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 1.04; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.78
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 0.93; acc: 0.78
Val Epoch over. val_loss: 1.1191187015004977; val_accuracy: 0.7161624203821656 

The current subspace-distance is: 0.00013281794963404536 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 1.19; acc: 0.64
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.22; acc: 0.69
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 1.23; acc: 0.61
Batch: 160; loss: 1.12; acc: 0.75
Batch: 180; loss: 1.15; acc: 0.69
Batch: 200; loss: 1.13; acc: 0.69
Batch: 220; loss: 1.15; acc: 0.64
Batch: 240; loss: 1.16; acc: 0.7
Batch: 260; loss: 1.28; acc: 0.61
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 1.32; acc: 0.64
Batch: 320; loss: 1.16; acc: 0.66
Batch: 340; loss: 1.13; acc: 0.67
Batch: 360; loss: 1.27; acc: 0.61
Batch: 380; loss: 1.11; acc: 0.69
Batch: 400; loss: 1.31; acc: 0.59
Batch: 420; loss: 1.13; acc: 0.77
Batch: 440; loss: 1.16; acc: 0.72
Batch: 460; loss: 1.04; acc: 0.73
Batch: 480; loss: 1.05; acc: 0.69
Batch: 500; loss: 1.14; acc: 0.66
Batch: 520; loss: 1.07; acc: 0.73
Batch: 540; loss: 1.16; acc: 0.69
Batch: 560; loss: 1.22; acc: 0.64
Batch: 580; loss: 1.1; acc: 0.66
Batch: 600; loss: 1.07; acc: 0.72
Batch: 620; loss: 1.13; acc: 0.66
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.1; acc: 0.72
Batch: 680; loss: 1.27; acc: 0.56
Batch: 700; loss: 1.12; acc: 0.62
Batch: 720; loss: 1.18; acc: 0.66
Batch: 740; loss: 0.97; acc: 0.77
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.09; acc: 0.7
Train Epoch over. train_loss: 1.16; train_accuracy: 0.68 

0.00015129511302802712
0.0001451319403713569
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.75
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.8
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 0.88; acc: 0.78
Val Epoch over. val_loss: 1.0696832609784073; val_accuracy: 0.7248208598726115 

The current subspace-distance is: 0.0001451319403713569 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.72
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 1.18; acc: 0.61
Batch: 60; loss: 1.1; acc: 0.67
Batch: 80; loss: 1.3; acc: 0.55
Batch: 100; loss: 1.09; acc: 0.7
Batch: 120; loss: 1.05; acc: 0.69
Batch: 140; loss: 1.09; acc: 0.73
Batch: 160; loss: 1.1; acc: 0.7
Batch: 180; loss: 1.14; acc: 0.64
Batch: 200; loss: 1.14; acc: 0.72
Batch: 220; loss: 1.22; acc: 0.64
Batch: 240; loss: 1.16; acc: 0.7
Batch: 260; loss: 1.07; acc: 0.75
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 1.21; acc: 0.7
Batch: 320; loss: 1.22; acc: 0.62
Batch: 340; loss: 1.11; acc: 0.7
Batch: 360; loss: 1.32; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.7
Batch: 400; loss: 1.02; acc: 0.77
Batch: 420; loss: 1.11; acc: 0.67
Batch: 440; loss: 1.21; acc: 0.56
Batch: 460; loss: 1.24; acc: 0.67
Batch: 480; loss: 1.13; acc: 0.66
Batch: 500; loss: 1.05; acc: 0.69
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 1.24; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.64
Batch: 580; loss: 1.13; acc: 0.64
Batch: 600; loss: 1.02; acc: 0.72
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 1.04; acc: 0.66
Batch: 660; loss: 1.0; acc: 0.7
Batch: 680; loss: 1.07; acc: 0.72
Batch: 700; loss: 1.17; acc: 0.59
Batch: 720; loss: 1.17; acc: 0.67
Batch: 740; loss: 1.15; acc: 0.69
Batch: 760; loss: 1.15; acc: 0.7
Batch: 780; loss: 1.2; acc: 0.7
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00015941703168209642
0.00015302635438274592
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.84; acc: 0.83
Val Epoch over. val_loss: 1.0469365207252987; val_accuracy: 0.7269108280254777 

The current subspace-distance is: 0.00015302635438274592 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.7
Batch: 40; loss: 1.16; acc: 0.62
Batch: 60; loss: 0.93; acc: 0.78
Batch: 80; loss: 1.23; acc: 0.66
Batch: 100; loss: 1.1; acc: 0.66
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 1.13; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.67
Batch: 180; loss: 1.05; acc: 0.69
Batch: 200; loss: 1.13; acc: 0.69
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 1.02; acc: 0.75
Batch: 260; loss: 1.17; acc: 0.67
Batch: 280; loss: 1.09; acc: 0.69
Batch: 300; loss: 1.17; acc: 0.69
Batch: 320; loss: 1.12; acc: 0.58
Batch: 340; loss: 1.04; acc: 0.75
Batch: 360; loss: 0.98; acc: 0.72
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 1.09; acc: 0.69
Batch: 420; loss: 1.09; acc: 0.64
Batch: 440; loss: 1.02; acc: 0.75
Batch: 460; loss: 1.19; acc: 0.67
Batch: 480; loss: 1.19; acc: 0.64
Batch: 500; loss: 1.1; acc: 0.67
Batch: 520; loss: 0.94; acc: 0.75
Batch: 540; loss: 1.04; acc: 0.77
Batch: 560; loss: 1.13; acc: 0.62
Batch: 580; loss: 1.09; acc: 0.72
Batch: 600; loss: 1.09; acc: 0.7
Batch: 620; loss: 1.18; acc: 0.66
Batch: 640; loss: 1.21; acc: 0.61
Batch: 660; loss: 1.22; acc: 0.64
Batch: 680; loss: 1.16; acc: 0.69
Batch: 700; loss: 1.12; acc: 0.69
Batch: 720; loss: 1.08; acc: 0.64
Batch: 740; loss: 1.12; acc: 0.75
Batch: 760; loss: 1.26; acc: 0.58
Batch: 780; loss: 1.19; acc: 0.66
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00017241868772543967
0.0001666733151068911
Batch: 0; loss: 1.12; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.66
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.91; acc: 0.75
Batch: 80; loss: 0.89; acc: 0.73
Batch: 100; loss: 0.94; acc: 0.78
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 0.81; acc: 0.83
Val Epoch over. val_loss: 1.0267370878511173; val_accuracy: 0.731687898089172 

The current subspace-distance is: 0.0001666733151068911 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 1.19; acc: 0.61
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.2; acc: 0.56
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 1.17; acc: 0.66
Batch: 140; loss: 1.22; acc: 0.66
Batch: 160; loss: 1.01; acc: 0.67
Batch: 180; loss: 1.16; acc: 0.69
Batch: 200; loss: 0.95; acc: 0.75
Batch: 220; loss: 1.17; acc: 0.7
Batch: 240; loss: 1.02; acc: 0.77
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.08; acc: 0.7
Batch: 300; loss: 0.96; acc: 0.8
Batch: 320; loss: 1.06; acc: 0.66
Batch: 340; loss: 1.05; acc: 0.66
Batch: 360; loss: 1.08; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.69
Batch: 400; loss: 1.2; acc: 0.61
Batch: 420; loss: 1.23; acc: 0.69
Batch: 440; loss: 1.13; acc: 0.67
Batch: 460; loss: 1.03; acc: 0.7
Batch: 480; loss: 1.27; acc: 0.61
Batch: 500; loss: 1.03; acc: 0.73
Batch: 520; loss: 0.97; acc: 0.75
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.07; acc: 0.66
Batch: 580; loss: 1.18; acc: 0.67
Batch: 600; loss: 1.11; acc: 0.69
Batch: 620; loss: 1.17; acc: 0.66
Batch: 640; loss: 1.06; acc: 0.73
Batch: 660; loss: 0.94; acc: 0.8
Batch: 680; loss: 1.1; acc: 0.7
Batch: 700; loss: 1.16; acc: 0.64
Batch: 720; loss: 1.14; acc: 0.67
Batch: 740; loss: 1.23; acc: 0.59
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 1.04; acc: 0.73
Train Epoch over. train_loss: 1.1; train_accuracy: 0.69 

0.00017426094564143568
0.00016852948465384543
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.91; acc: 0.77
Batch: 80; loss: 0.9; acc: 0.73
Batch: 100; loss: 0.93; acc: 0.73
Batch: 120; loss: 1.13; acc: 0.62
Batch: 140; loss: 0.81; acc: 0.84
Val Epoch over. val_loss: 1.0282310129730565; val_accuracy: 0.7246218152866242 

The current subspace-distance is: 0.00016852948465384543 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.7
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 1.14; acc: 0.73
Batch: 60; loss: 1.2; acc: 0.61
Batch: 80; loss: 1.17; acc: 0.67
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 0.99; acc: 0.72
Batch: 140; loss: 0.95; acc: 0.8
Batch: 160; loss: 1.02; acc: 0.7
Batch: 180; loss: 1.02; acc: 0.73
Batch: 200; loss: 1.01; acc: 0.75
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.17; acc: 0.69
Batch: 260; loss: 1.12; acc: 0.61
Batch: 280; loss: 1.04; acc: 0.69
Batch: 300; loss: 1.18; acc: 0.62
Batch: 320; loss: 1.06; acc: 0.67
Batch: 340; loss: 1.01; acc: 0.73
Batch: 360; loss: 1.08; acc: 0.72
Batch: 380; loss: 1.21; acc: 0.62
Batch: 400; loss: 1.21; acc: 0.66
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 1.05; acc: 0.69
Batch: 460; loss: 1.18; acc: 0.66
Batch: 480; loss: 1.06; acc: 0.69
Batch: 500; loss: 1.21; acc: 0.67
Batch: 520; loss: 0.92; acc: 0.75
Batch: 540; loss: 1.06; acc: 0.78
Batch: 560; loss: 1.03; acc: 0.75
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 0.98; acc: 0.77
Batch: 620; loss: 1.16; acc: 0.61
Batch: 640; loss: 1.21; acc: 0.66
Batch: 660; loss: 0.97; acc: 0.78
Batch: 680; loss: 1.03; acc: 0.73
Batch: 700; loss: 1.13; acc: 0.67
Batch: 720; loss: 1.09; acc: 0.66
Batch: 740; loss: 1.11; acc: 0.7
Batch: 760; loss: 1.26; acc: 0.56
Batch: 780; loss: 1.07; acc: 0.72
Train Epoch over. train_loss: 1.1; train_accuracy: 0.69 

0.00018088477372657508
0.00017398335330653936
Batch: 0; loss: 1.14; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.91; acc: 0.77
Batch: 80; loss: 0.9; acc: 0.72
Batch: 100; loss: 0.94; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.62
Batch: 140; loss: 0.81; acc: 0.84
Val Epoch over. val_loss: 1.0282674925342488; val_accuracy: 0.7212380573248408 

The current subspace-distance is: 0.00017398335330653936 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 0.96; acc: 0.78
Batch: 40; loss: 1.07; acc: 0.61
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.26; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.66
Batch: 120; loss: 1.27; acc: 0.58
Batch: 140; loss: 1.16; acc: 0.69
Batch: 160; loss: 1.01; acc: 0.77
Batch: 180; loss: 1.1; acc: 0.67
Batch: 200; loss: 1.06; acc: 0.67
Batch: 220; loss: 1.07; acc: 0.69
Batch: 240; loss: 0.95; acc: 0.77
Batch: 260; loss: 1.12; acc: 0.69
Batch: 280; loss: 1.24; acc: 0.58
Batch: 300; loss: 1.14; acc: 0.67
Batch: 320; loss: 1.22; acc: 0.67
Batch: 340; loss: 1.3; acc: 0.56
Batch: 360; loss: 1.11; acc: 0.75
Batch: 380; loss: 1.14; acc: 0.69
Batch: 400; loss: 1.04; acc: 0.64
Batch: 420; loss: 1.07; acc: 0.75
Batch: 440; loss: 1.15; acc: 0.66
Batch: 460; loss: 1.24; acc: 0.61
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 1.25; acc: 0.59
Batch: 540; loss: 1.17; acc: 0.66
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 0.9; acc: 0.81
Batch: 600; loss: 1.21; acc: 0.62
Batch: 620; loss: 1.09; acc: 0.73
Batch: 640; loss: 1.21; acc: 0.59
Batch: 660; loss: 1.21; acc: 0.66
Batch: 680; loss: 1.27; acc: 0.62
Batch: 700; loss: 1.14; acc: 0.7
Batch: 720; loss: 1.11; acc: 0.64
Batch: 740; loss: 0.97; acc: 0.7
Batch: 760; loss: 1.22; acc: 0.66
Batch: 780; loss: 1.07; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.0001798585435608402
0.00017124603618867695
Batch: 0; loss: 1.12; acc: 0.69
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.9; acc: 0.8
Batch: 80; loss: 0.89; acc: 0.73
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 1.11; acc: 0.62
Batch: 140; loss: 0.79; acc: 0.84
Val Epoch over. val_loss: 1.0148060029479349; val_accuracy: 0.7311902866242038 

The current subspace-distance is: 0.00017124603618867695 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.92; acc: 0.77
Batch: 20; loss: 1.15; acc: 0.72
Batch: 40; loss: 1.16; acc: 0.59
Batch: 60; loss: 1.14; acc: 0.69
Batch: 80; loss: 1.19; acc: 0.66
Batch: 100; loss: 1.23; acc: 0.58
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 1.06; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 1.03; acc: 0.69
Batch: 200; loss: 1.08; acc: 0.67
Batch: 220; loss: 1.27; acc: 0.64
Batch: 240; loss: 1.1; acc: 0.72
Batch: 260; loss: 1.01; acc: 0.77
Batch: 280; loss: 1.12; acc: 0.59
Batch: 300; loss: 1.08; acc: 0.67
Batch: 320; loss: 1.1; acc: 0.66
Batch: 340; loss: 1.01; acc: 0.67
Batch: 360; loss: 1.33; acc: 0.59
Batch: 380; loss: 1.1; acc: 0.69
Batch: 400; loss: 0.9; acc: 0.78
Batch: 420; loss: 1.13; acc: 0.69
Batch: 440; loss: 1.14; acc: 0.67
Batch: 460; loss: 1.3; acc: 0.64
Batch: 480; loss: 1.13; acc: 0.7
Batch: 500; loss: 0.99; acc: 0.77
Batch: 520; loss: 1.15; acc: 0.58
Batch: 540; loss: 0.99; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.73
Batch: 580; loss: 1.08; acc: 0.7
Batch: 600; loss: 0.87; acc: 0.8
Batch: 620; loss: 1.05; acc: 0.73
Batch: 640; loss: 1.16; acc: 0.62
Batch: 660; loss: 1.11; acc: 0.62
Batch: 680; loss: 1.11; acc: 0.66
Batch: 700; loss: 1.21; acc: 0.66
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 1.18; acc: 0.73
Batch: 760; loss: 1.09; acc: 0.69
Batch: 780; loss: 1.14; acc: 0.62
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00018663280934561044
0.00017701915930956602
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.89; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.72
Batch: 100; loss: 0.94; acc: 0.78
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 0.78; acc: 0.84
Val Epoch over. val_loss: 1.0154025619197045; val_accuracy: 0.7301950636942676 

The current subspace-distance is: 0.00017701915930956602 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.7
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 1.12; acc: 0.69
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 1.02; acc: 0.73
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 1.01; acc: 0.69
Batch: 200; loss: 1.03; acc: 0.73
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 1.18; acc: 0.67
Batch: 260; loss: 1.0; acc: 0.77
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 1.02; acc: 0.64
Batch: 340; loss: 1.3; acc: 0.67
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.13; acc: 0.77
Batch: 400; loss: 1.09; acc: 0.62
Batch: 420; loss: 1.11; acc: 0.72
Batch: 440; loss: 1.24; acc: 0.62
Batch: 460; loss: 1.28; acc: 0.67
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 1.2; acc: 0.66
Batch: 520; loss: 1.12; acc: 0.72
Batch: 540; loss: 0.98; acc: 0.75
Batch: 560; loss: 0.99; acc: 0.75
Batch: 580; loss: 1.22; acc: 0.61
Batch: 600; loss: 0.95; acc: 0.75
Batch: 620; loss: 1.0; acc: 0.75
Batch: 640; loss: 1.16; acc: 0.61
Batch: 660; loss: 1.03; acc: 0.73
Batch: 680; loss: 1.08; acc: 0.67
Batch: 700; loss: 1.11; acc: 0.66
Batch: 720; loss: 1.36; acc: 0.56
Batch: 740; loss: 1.17; acc: 0.67
Batch: 760; loss: 0.91; acc: 0.75
Batch: 780; loss: 0.9; acc: 0.75
Train Epoch over. train_loss: 1.08; train_accuracy: 0.69 

0.00018564164929557592
0.0001776096469257027
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 0.88; acc: 0.73
Batch: 100; loss: 0.94; acc: 0.75
Batch: 120; loss: 1.14; acc: 0.62
Batch: 140; loss: 0.79; acc: 0.84
Val Epoch over. val_loss: 1.0133310203339643; val_accuracy: 0.7291003184713376 

The current subspace-distance is: 0.0001776096469257027 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 1.06; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.1; acc: 0.7
Batch: 140; loss: 1.21; acc: 0.62
Batch: 160; loss: 1.03; acc: 0.72
Batch: 180; loss: 1.03; acc: 0.75
Batch: 200; loss: 1.13; acc: 0.73
Batch: 220; loss: 1.2; acc: 0.66
Batch: 240; loss: 1.25; acc: 0.64
Batch: 260; loss: 1.15; acc: 0.64
Batch: 280; loss: 1.08; acc: 0.77
Batch: 300; loss: 1.11; acc: 0.62
Batch: 320; loss: 1.12; acc: 0.69
Batch: 340; loss: 1.0; acc: 0.73
Batch: 360; loss: 1.04; acc: 0.73
Batch: 380; loss: 1.0; acc: 0.72
Batch: 400; loss: 1.1; acc: 0.7
Batch: 420; loss: 1.16; acc: 0.59
Batch: 440; loss: 1.05; acc: 0.66
Batch: 460; loss: 0.98; acc: 0.75
Batch: 480; loss: 1.06; acc: 0.67
Batch: 500; loss: 1.19; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.13; acc: 0.66
Batch: 560; loss: 1.25; acc: 0.61
Batch: 580; loss: 0.88; acc: 0.75
Batch: 600; loss: 1.19; acc: 0.62
Batch: 620; loss: 1.12; acc: 0.67
Batch: 640; loss: 0.98; acc: 0.7
Batch: 660; loss: 0.98; acc: 0.72
Batch: 680; loss: 1.29; acc: 0.62
Batch: 700; loss: 1.21; acc: 0.56
Batch: 720; loss: 1.07; acc: 0.72
Batch: 740; loss: 1.25; acc: 0.61
Batch: 760; loss: 1.15; acc: 0.66
Batch: 780; loss: 1.16; acc: 0.62
Train Epoch over. train_loss: 1.08; train_accuracy: 0.7 

0.00018565423670224845
0.0001825267099775374
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.64
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.77
Batch: 120; loss: 1.12; acc: 0.66
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.9983440470543636; val_accuracy: 0.7347730891719745 

The current subspace-distance is: 0.0001825267099775374 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.59
Batch: 20; loss: 1.05; acc: 0.69
Batch: 40; loss: 1.03; acc: 0.72
Batch: 60; loss: 1.23; acc: 0.58
Batch: 80; loss: 1.18; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 1.02; acc: 0.7
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 1.0; acc: 0.7
Batch: 200; loss: 1.1; acc: 0.72
Batch: 220; loss: 1.07; acc: 0.67
Batch: 240; loss: 1.04; acc: 0.7
Batch: 260; loss: 1.05; acc: 0.73
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 1.28; acc: 0.61
Batch: 320; loss: 1.25; acc: 0.64
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 1.07; acc: 0.7
Batch: 380; loss: 0.99; acc: 0.7
Batch: 400; loss: 1.06; acc: 0.72
Batch: 420; loss: 1.02; acc: 0.72
Batch: 440; loss: 1.22; acc: 0.59
Batch: 460; loss: 1.1; acc: 0.64
Batch: 480; loss: 1.16; acc: 0.72
Batch: 500; loss: 1.25; acc: 0.67
Batch: 520; loss: 1.04; acc: 0.67
Batch: 540; loss: 1.07; acc: 0.73
Batch: 560; loss: 0.92; acc: 0.75
Batch: 580; loss: 0.94; acc: 0.78
Batch: 600; loss: 1.1; acc: 0.7
Batch: 620; loss: 1.1; acc: 0.67
Batch: 640; loss: 1.18; acc: 0.62
Batch: 660; loss: 1.09; acc: 0.69
Batch: 680; loss: 1.18; acc: 0.67
Batch: 700; loss: 0.9; acc: 0.83
Batch: 720; loss: 1.21; acc: 0.62
Batch: 740; loss: 1.05; acc: 0.66
Batch: 760; loss: 1.07; acc: 0.66
Batch: 780; loss: 0.97; acc: 0.73
Train Epoch over. train_loss: 1.08; train_accuracy: 0.69 

0.0001887688849819824
0.0001793533010641113
Batch: 0; loss: 1.1; acc: 0.69
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 0.93; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 0.76; acc: 0.84
Val Epoch over. val_loss: 1.0024891632377722; val_accuracy: 0.7308917197452229 

The current subspace-distance is: 0.0001793533010641113 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.08; acc: 0.73
Batch: 40; loss: 1.09; acc: 0.8
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.03; acc: 0.77
Batch: 100; loss: 1.13; acc: 0.66
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 1.36; acc: 0.56
Batch: 160; loss: 1.02; acc: 0.7
Batch: 180; loss: 1.07; acc: 0.67
Batch: 200; loss: 1.21; acc: 0.62
Batch: 220; loss: 1.06; acc: 0.66
Batch: 240; loss: 1.21; acc: 0.62
Batch: 260; loss: 1.07; acc: 0.66
Batch: 280; loss: 1.01; acc: 0.73
Batch: 300; loss: 1.11; acc: 0.59
Batch: 320; loss: 1.13; acc: 0.69
Batch: 340; loss: 0.93; acc: 0.77
Batch: 360; loss: 1.04; acc: 0.73
Batch: 380; loss: 1.31; acc: 0.61
Batch: 400; loss: 1.2; acc: 0.61
Batch: 420; loss: 0.94; acc: 0.78
Batch: 440; loss: 1.08; acc: 0.67
Batch: 460; loss: 1.05; acc: 0.7
Batch: 480; loss: 0.92; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.64
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.02; acc: 0.77
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 1.18; acc: 0.62
Batch: 600; loss: 0.97; acc: 0.7
Batch: 620; loss: 1.1; acc: 0.67
Batch: 640; loss: 1.19; acc: 0.58
Batch: 660; loss: 1.02; acc: 0.67
Batch: 680; loss: 1.2; acc: 0.64
Batch: 700; loss: 0.95; acc: 0.77
Batch: 720; loss: 1.11; acc: 0.69
Batch: 740; loss: 1.11; acc: 0.7
Batch: 760; loss: 0.99; acc: 0.67
Batch: 780; loss: 1.04; acc: 0.73
Train Epoch over. train_loss: 1.07; train_accuracy: 0.69 

0.00019280670676380396
0.0001833320566220209
Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.93; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.77; acc: 0.84
Val Epoch over. val_loss: 1.0075226342602142; val_accuracy: 0.7321855095541401 

The current subspace-distance is: 0.0001833320566220209 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.7
Batch: 40; loss: 0.92; acc: 0.72
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 1.16; acc: 0.59
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 1.15; acc: 0.61
Batch: 200; loss: 0.98; acc: 0.77
Batch: 220; loss: 1.11; acc: 0.72
Batch: 240; loss: 1.07; acc: 0.72
Batch: 260; loss: 1.19; acc: 0.61
Batch: 280; loss: 1.09; acc: 0.69
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 0.87; acc: 0.77
Batch: 340; loss: 0.95; acc: 0.78
Batch: 360; loss: 1.14; acc: 0.69
Batch: 380; loss: 1.39; acc: 0.61
Batch: 400; loss: 0.79; acc: 0.86
Batch: 420; loss: 0.92; acc: 0.8
Batch: 440; loss: 1.17; acc: 0.61
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 1.13; acc: 0.62
Batch: 500; loss: 1.06; acc: 0.73
Batch: 520; loss: 1.17; acc: 0.64
Batch: 540; loss: 0.94; acc: 0.83
Batch: 560; loss: 1.15; acc: 0.62
Batch: 580; loss: 1.02; acc: 0.66
Batch: 600; loss: 1.07; acc: 0.77
Batch: 620; loss: 1.25; acc: 0.61
Batch: 640; loss: 1.17; acc: 0.73
Batch: 660; loss: 1.12; acc: 0.77
Batch: 680; loss: 1.02; acc: 0.8
Batch: 700; loss: 1.17; acc: 0.61
Batch: 720; loss: 1.01; acc: 0.64
Batch: 740; loss: 1.02; acc: 0.69
Batch: 760; loss: 0.89; acc: 0.78
Batch: 780; loss: 1.09; acc: 0.69
Train Epoch over. train_loss: 1.07; train_accuracy: 0.7 

0.00019410726963542402
0.00018599139002617449
Batch: 0; loss: 1.09; acc: 0.7
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.85; acc: 0.73
Batch: 100; loss: 0.91; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 0.76; acc: 0.86
Val Epoch over. val_loss: 0.9882786285345722; val_accuracy: 0.7377587579617835 

The current subspace-distance is: 0.00018599139002617449 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.06; acc: 0.72
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 1.19; acc: 0.66
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.91; acc: 0.73
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 1.05; acc: 0.75
Batch: 200; loss: 1.09; acc: 0.69
Batch: 220; loss: 0.86; acc: 0.75
Batch: 240; loss: 1.01; acc: 0.66
Batch: 260; loss: 1.09; acc: 0.67
Batch: 280; loss: 1.08; acc: 0.67
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 1.07; acc: 0.69
Batch: 340; loss: 1.36; acc: 0.62
Batch: 360; loss: 1.05; acc: 0.67
Batch: 380; loss: 1.05; acc: 0.72
Batch: 400; loss: 1.0; acc: 0.72
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.06; acc: 0.69
Batch: 460; loss: 0.99; acc: 0.72
Batch: 480; loss: 0.98; acc: 0.73
Batch: 500; loss: 1.0; acc: 0.7
Batch: 520; loss: 0.91; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.67
Batch: 560; loss: 1.09; acc: 0.66
Batch: 580; loss: 1.1; acc: 0.62
Batch: 600; loss: 1.08; acc: 0.62
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 0.92; acc: 0.72
Batch: 660; loss: 1.01; acc: 0.66
Batch: 680; loss: 1.12; acc: 0.7
Batch: 700; loss: 1.19; acc: 0.61
Batch: 720; loss: 1.05; acc: 0.73
Batch: 740; loss: 1.13; acc: 0.7
Batch: 760; loss: 1.07; acc: 0.7
Batch: 780; loss: 1.04; acc: 0.72
Train Epoch over. train_loss: 1.07; train_accuracy: 0.69 

0.00019713345682248473
0.00019035105651710182
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 0.76; acc: 0.86
Val Epoch over. val_loss: 0.9847062135198313; val_accuracy: 0.7375597133757962 

The current subspace-distance is: 0.00019035105651710182 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.67
Batch: 20; loss: 1.04; acc: 0.69
Batch: 40; loss: 1.18; acc: 0.64
Batch: 60; loss: 1.15; acc: 0.61
Batch: 80; loss: 1.02; acc: 0.72
Batch: 100; loss: 1.11; acc: 0.69
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 1.09; acc: 0.67
Batch: 160; loss: 1.24; acc: 0.64
Batch: 180; loss: 0.98; acc: 0.75
Batch: 200; loss: 1.19; acc: 0.64
Batch: 220; loss: 1.06; acc: 0.66
Batch: 240; loss: 1.09; acc: 0.67
Batch: 260; loss: 1.05; acc: 0.73
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 1.09; acc: 0.67
Batch: 320; loss: 0.96; acc: 0.78
Batch: 340; loss: 1.26; acc: 0.58
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 1.09; acc: 0.69
Batch: 400; loss: 1.22; acc: 0.59
Batch: 420; loss: 0.92; acc: 0.78
Batch: 440; loss: 1.04; acc: 0.72
Batch: 460; loss: 1.18; acc: 0.62
Batch: 480; loss: 0.91; acc: 0.8
Batch: 500; loss: 1.11; acc: 0.69
Batch: 520; loss: 1.06; acc: 0.72
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.05; acc: 0.66
Batch: 580; loss: 1.06; acc: 0.73
Batch: 600; loss: 1.06; acc: 0.7
Batch: 620; loss: 1.14; acc: 0.66
Batch: 640; loss: 0.94; acc: 0.73
Batch: 660; loss: 1.02; acc: 0.66
Batch: 680; loss: 1.09; acc: 0.7
Batch: 700; loss: 1.2; acc: 0.66
Batch: 720; loss: 1.13; acc: 0.66
Batch: 740; loss: 0.86; acc: 0.86
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 1.12; acc: 0.7
Train Epoch over. train_loss: 1.07; train_accuracy: 0.7 

0.00019547436386346817
0.00018895060929935426
Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.09; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.8
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 0.75; acc: 0.86
Val Epoch over. val_loss: 0.9890742632234173; val_accuracy: 0.7320859872611465 

The current subspace-distance is: 0.00018895060929935426 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 1.0; acc: 0.66
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 0.89; acc: 0.72
Batch: 100; loss: 1.14; acc: 0.61
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 1.16; acc: 0.66
Batch: 160; loss: 1.15; acc: 0.64
Batch: 180; loss: 1.23; acc: 0.58
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.16; acc: 0.61
Batch: 240; loss: 1.04; acc: 0.7
Batch: 260; loss: 1.18; acc: 0.61
Batch: 280; loss: 1.0; acc: 0.73
Batch: 300; loss: 1.3; acc: 0.55
Batch: 320; loss: 1.06; acc: 0.7
Batch: 340; loss: 0.93; acc: 0.75
Batch: 360; loss: 1.1; acc: 0.69
Batch: 380; loss: 0.93; acc: 0.75
Batch: 400; loss: 1.19; acc: 0.7
Batch: 420; loss: 1.22; acc: 0.69
Batch: 440; loss: 1.08; acc: 0.69
Batch: 460; loss: 1.07; acc: 0.7
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 0.96; acc: 0.8
Batch: 520; loss: 1.1; acc: 0.73
Batch: 540; loss: 1.03; acc: 0.69
Batch: 560; loss: 1.26; acc: 0.61
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.03; acc: 0.66
Batch: 620; loss: 0.98; acc: 0.73
Batch: 640; loss: 1.04; acc: 0.67
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 1.14; acc: 0.66
Batch: 700; loss: 0.93; acc: 0.73
Batch: 720; loss: 1.06; acc: 0.69
Batch: 740; loss: 1.1; acc: 0.67
Batch: 760; loss: 0.89; acc: 0.88
Batch: 780; loss: 1.2; acc: 0.61
Train Epoch over. train_loss: 1.07; train_accuracy: 0.7 

0.0001969340373761952
0.00018908418132923543
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.83
Batch: 80; loss: 0.86; acc: 0.75
Batch: 100; loss: 0.95; acc: 0.73
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.77; acc: 0.86
Val Epoch over. val_loss: 1.0018430021917744; val_accuracy: 0.729796974522293 

The current subspace-distance is: 0.00018908418132923543 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 1.28; acc: 0.55
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 0.94; acc: 0.75
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 1.17; acc: 0.62
Batch: 160; loss: 0.95; acc: 0.77
Batch: 180; loss: 0.87; acc: 0.81
Batch: 200; loss: 1.22; acc: 0.59
Batch: 220; loss: 0.92; acc: 0.8
Batch: 240; loss: 1.11; acc: 0.64
Batch: 260; loss: 1.12; acc: 0.69
Batch: 280; loss: 1.18; acc: 0.7
Batch: 300; loss: 1.01; acc: 0.69
Batch: 320; loss: 0.9; acc: 0.77
Batch: 340; loss: 0.96; acc: 0.84
Batch: 360; loss: 1.2; acc: 0.64
Batch: 380; loss: 1.02; acc: 0.78
Batch: 400; loss: 1.2; acc: 0.69
Batch: 420; loss: 1.1; acc: 0.7
Batch: 440; loss: 1.09; acc: 0.62
Batch: 460; loss: 0.99; acc: 0.77
Batch: 480; loss: 1.2; acc: 0.58
Batch: 500; loss: 0.99; acc: 0.75
Batch: 520; loss: 1.06; acc: 0.73
Batch: 540; loss: 1.0; acc: 0.77
Batch: 560; loss: 1.11; acc: 0.7
Batch: 580; loss: 1.09; acc: 0.62
Batch: 600; loss: 1.13; acc: 0.67
Batch: 620; loss: 0.93; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.81
Batch: 660; loss: 1.15; acc: 0.64
Batch: 680; loss: 1.15; acc: 0.72
Batch: 700; loss: 1.01; acc: 0.69
Batch: 720; loss: 0.96; acc: 0.78
Batch: 740; loss: 0.89; acc: 0.84
Batch: 760; loss: 1.11; acc: 0.7
Batch: 780; loss: 1.1; acc: 0.64
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

0.00019242975395172834
0.00018416158854961395
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 0.75; acc: 0.84
Val Epoch over. val_loss: 0.9858781748516544; val_accuracy: 0.7371616242038217 

The current subspace-distance is: 0.00018416158854961395 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.06; acc: 0.66
Batch: 40; loss: 1.16; acc: 0.64
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.04; acc: 0.69
Batch: 140; loss: 1.04; acc: 0.7
Batch: 160; loss: 1.11; acc: 0.77
Batch: 180; loss: 1.01; acc: 0.75
Batch: 200; loss: 0.94; acc: 0.72
Batch: 220; loss: 0.97; acc: 0.73
Batch: 240; loss: 1.01; acc: 0.7
Batch: 260; loss: 1.07; acc: 0.69
Batch: 280; loss: 1.14; acc: 0.69
Batch: 300; loss: 0.96; acc: 0.67
Batch: 320; loss: 0.98; acc: 0.69
Batch: 340; loss: 1.06; acc: 0.64
Batch: 360; loss: 0.88; acc: 0.84
Batch: 380; loss: 1.01; acc: 0.73
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 1.11; acc: 0.67
Batch: 440; loss: 1.13; acc: 0.66
Batch: 460; loss: 1.18; acc: 0.61
Batch: 480; loss: 0.99; acc: 0.72
Batch: 500; loss: 0.98; acc: 0.77
Batch: 520; loss: 1.26; acc: 0.62
Batch: 540; loss: 1.0; acc: 0.69
Batch: 560; loss: 0.92; acc: 0.8
Batch: 580; loss: 0.92; acc: 0.75
Batch: 600; loss: 0.93; acc: 0.84
Batch: 620; loss: 1.02; acc: 0.7
Batch: 640; loss: 1.19; acc: 0.59
Batch: 660; loss: 0.86; acc: 0.83
Batch: 680; loss: 1.28; acc: 0.59
Batch: 700; loss: 0.96; acc: 0.78
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 1.18; acc: 0.59
Batch: 760; loss: 1.0; acc: 0.78
Batch: 780; loss: 0.98; acc: 0.67
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

0.00020013739413116127
0.00019069929840043187
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 0.84; acc: 0.73
Batch: 100; loss: 0.89; acc: 0.75
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 0.74; acc: 0.86
Val Epoch over. val_loss: 0.9776560850204177; val_accuracy: 0.737062101910828 

The current subspace-distance is: 0.00019069929840043187 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.7
Batch: 20; loss: 1.03; acc: 0.67
Batch: 40; loss: 1.05; acc: 0.77
Batch: 60; loss: 1.18; acc: 0.56
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 0.97; acc: 0.73
Batch: 120; loss: 1.06; acc: 0.75
Batch: 140; loss: 1.02; acc: 0.77
Batch: 160; loss: 1.09; acc: 0.66
Batch: 180; loss: 0.98; acc: 0.72
Batch: 200; loss: 0.91; acc: 0.83
Batch: 220; loss: 0.92; acc: 0.72
Batch: 240; loss: 1.19; acc: 0.69
Batch: 260; loss: 1.05; acc: 0.69
Batch: 280; loss: 1.01; acc: 0.78
Batch: 300; loss: 1.01; acc: 0.75
Batch: 320; loss: 1.07; acc: 0.61
Batch: 340; loss: 1.08; acc: 0.69
Batch: 360; loss: 0.92; acc: 0.75
Batch: 380; loss: 1.1; acc: 0.64
Batch: 400; loss: 1.05; acc: 0.67
Batch: 420; loss: 0.89; acc: 0.8
Batch: 440; loss: 1.06; acc: 0.78
Batch: 460; loss: 1.01; acc: 0.72
Batch: 480; loss: 1.06; acc: 0.66
Batch: 500; loss: 1.13; acc: 0.66
Batch: 520; loss: 0.92; acc: 0.78
Batch: 540; loss: 1.13; acc: 0.58
Batch: 560; loss: 1.2; acc: 0.66
Batch: 580; loss: 1.15; acc: 0.62
Batch: 600; loss: 1.16; acc: 0.56
Batch: 620; loss: 1.12; acc: 0.64
Batch: 640; loss: 1.07; acc: 0.69
Batch: 660; loss: 1.04; acc: 0.72
Batch: 680; loss: 1.15; acc: 0.66
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 1.05; acc: 0.67
Batch: 740; loss: 1.11; acc: 0.61
Batch: 760; loss: 1.03; acc: 0.69
Batch: 780; loss: 0.92; acc: 0.81
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

0.0001978729123948142
0.00018868174811359495
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.66
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 0.75; acc: 0.86
Val Epoch over. val_loss: 0.9842549410595256; val_accuracy: 0.7291003184713376 

The current subspace-distance is: 0.00018868174811359495 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.9; acc: 0.83
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 1.02; acc: 0.77
Batch: 60; loss: 0.86; acc: 0.78
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 1.09; acc: 0.73
Batch: 160; loss: 0.96; acc: 0.77
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 1.1; acc: 0.64
Batch: 240; loss: 1.04; acc: 0.72
Batch: 260; loss: 1.15; acc: 0.58
Batch: 280; loss: 1.15; acc: 0.7
Batch: 300; loss: 1.04; acc: 0.67
Batch: 320; loss: 0.93; acc: 0.7
Batch: 340; loss: 1.09; acc: 0.7
Batch: 360; loss: 1.2; acc: 0.66
Batch: 380; loss: 0.93; acc: 0.73
Batch: 400; loss: 1.12; acc: 0.67
Batch: 420; loss: 0.9; acc: 0.75
Batch: 440; loss: 1.18; acc: 0.64
Batch: 460; loss: 1.19; acc: 0.56
Batch: 480; loss: 1.29; acc: 0.62
Batch: 500; loss: 1.0; acc: 0.69
Batch: 520; loss: 0.96; acc: 0.77
Batch: 540; loss: 1.02; acc: 0.75
Batch: 560; loss: 1.09; acc: 0.7
Batch: 580; loss: 1.13; acc: 0.64
Batch: 600; loss: 0.98; acc: 0.69
Batch: 620; loss: 1.01; acc: 0.75
Batch: 640; loss: 0.92; acc: 0.8
Batch: 660; loss: 1.07; acc: 0.67
Batch: 680; loss: 1.04; acc: 0.67
Batch: 700; loss: 1.19; acc: 0.64
Batch: 720; loss: 0.95; acc: 0.75
Batch: 740; loss: 0.98; acc: 0.75
Batch: 760; loss: 0.93; acc: 0.78
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

0.00019143569807056338
0.00018606534285936505
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 0.85; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.66
Batch: 140; loss: 0.76; acc: 0.86
Val Epoch over. val_loss: 0.9851335628776793; val_accuracy: 0.73328025477707 

The current subspace-distance is: 0.00018606534285936505 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.95; acc: 0.8
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 1.08; acc: 0.64
Batch: 60; loss: 1.14; acc: 0.62
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 1.31; acc: 0.55
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 1.14; acc: 0.69
Batch: 160; loss: 1.0; acc: 0.75
Batch: 180; loss: 0.8; acc: 0.89
Batch: 200; loss: 1.03; acc: 0.69
Batch: 220; loss: 1.06; acc: 0.7
Batch: 240; loss: 1.19; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.64
Batch: 280; loss: 1.06; acc: 0.72
Batch: 300; loss: 0.91; acc: 0.8
Batch: 320; loss: 0.93; acc: 0.78
Batch: 340; loss: 1.05; acc: 0.73
Batch: 360; loss: 0.94; acc: 0.77
Batch: 380; loss: 0.98; acc: 0.81
Batch: 400; loss: 1.17; acc: 0.67
Batch: 420; loss: 1.24; acc: 0.61
Batch: 440; loss: 1.1; acc: 0.7
Batch: 460; loss: 0.87; acc: 0.77
Batch: 480; loss: 1.02; acc: 0.78
Batch: 500; loss: 1.03; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.69
Batch: 540; loss: 1.09; acc: 0.69
Batch: 560; loss: 0.96; acc: 0.77
Batch: 580; loss: 1.13; acc: 0.66
Batch: 600; loss: 1.18; acc: 0.61
Batch: 620; loss: 1.01; acc: 0.72
Batch: 640; loss: 1.14; acc: 0.61
Batch: 660; loss: 1.17; acc: 0.66
Batch: 680; loss: 1.19; acc: 0.67
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.1; acc: 0.73
Batch: 740; loss: 0.99; acc: 0.75
Batch: 760; loss: 0.98; acc: 0.75
Batch: 780; loss: 1.07; acc: 0.69
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

0.00019986627739854157
0.00019291964417789131
Batch: 0; loss: 1.09; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.83
Batch: 80; loss: 0.85; acc: 0.73
Batch: 100; loss: 0.93; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.75; acc: 0.86
Val Epoch over. val_loss: 0.9888333685838493; val_accuracy: 0.7361664012738853 

The current subspace-distance is: 0.00019291964417789131 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.14; acc: 0.69
Batch: 40; loss: 0.91; acc: 0.8
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 1.13; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.02; acc: 0.78
Batch: 160; loss: 1.06; acc: 0.75
Batch: 180; loss: 1.04; acc: 0.77
Batch: 200; loss: 1.03; acc: 0.72
Batch: 220; loss: 1.03; acc: 0.78
Batch: 240; loss: 1.1; acc: 0.64
Batch: 260; loss: 0.95; acc: 0.75
Batch: 280; loss: 0.99; acc: 0.67
Batch: 300; loss: 1.07; acc: 0.69
Batch: 320; loss: 0.91; acc: 0.83
Batch: 340; loss: 1.12; acc: 0.7
Batch: 360; loss: 1.0; acc: 0.81
Batch: 380; loss: 1.01; acc: 0.72
Batch: 400; loss: 0.9; acc: 0.77
Batch: 420; loss: 1.15; acc: 0.59
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 1.11; acc: 0.66
Batch: 500; loss: 1.03; acc: 0.72
Batch: 520; loss: 0.9; acc: 0.8
Batch: 540; loss: 1.08; acc: 0.7
Batch: 560; loss: 0.85; acc: 0.78
Batch: 580; loss: 1.03; acc: 0.73
Batch: 600; loss: 1.1; acc: 0.67
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 1.08; acc: 0.73
Batch: 680; loss: 0.88; acc: 0.78
Batch: 700; loss: 1.28; acc: 0.62
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 1.01; acc: 0.75
Batch: 760; loss: 1.01; acc: 0.77
Batch: 780; loss: 0.91; acc: 0.72
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

0.00019822725153062493
0.00018935320258606225
Batch: 0; loss: 1.08; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 0.76; acc: 0.86
Val Epoch over. val_loss: 0.9837650899674483; val_accuracy: 0.7341759554140127 

The current subspace-distance is: 0.00018935320258606225 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.19; acc: 0.58
Batch: 40; loss: 1.06; acc: 0.72
Batch: 60; loss: 1.05; acc: 0.66
Batch: 80; loss: 0.99; acc: 0.67
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.06; acc: 0.69
Batch: 140; loss: 1.05; acc: 0.67
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 1.24; acc: 0.58
Batch: 200; loss: 0.89; acc: 0.8
Batch: 220; loss: 1.02; acc: 0.69
Batch: 240; loss: 1.01; acc: 0.72
Batch: 260; loss: 0.93; acc: 0.8
Batch: 280; loss: 0.87; acc: 0.8
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 1.09; acc: 0.7
Batch: 340; loss: 0.98; acc: 0.77
Batch: 360; loss: 1.23; acc: 0.64
Batch: 380; loss: 1.13; acc: 0.67
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 0.79; acc: 0.83
Batch: 440; loss: 1.18; acc: 0.62
Batch: 460; loss: 1.06; acc: 0.73
Batch: 480; loss: 1.09; acc: 0.72
Batch: 500; loss: 1.14; acc: 0.73
Batch: 520; loss: 1.14; acc: 0.64
Batch: 540; loss: 1.04; acc: 0.75
Batch: 560; loss: 1.02; acc: 0.67
Batch: 580; loss: 1.05; acc: 0.77
Batch: 600; loss: 1.25; acc: 0.64
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 1.05; acc: 0.69
Batch: 660; loss: 0.93; acc: 0.75
Batch: 680; loss: 1.14; acc: 0.61
Batch: 700; loss: 1.06; acc: 0.66
Batch: 720; loss: 1.08; acc: 0.67
Batch: 740; loss: 1.1; acc: 0.64
Batch: 760; loss: 1.07; acc: 0.62
Batch: 780; loss: 1.09; acc: 0.64
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

0.00019846395298372954
0.00019205501303076744
Batch: 0; loss: 1.07; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.73
Batch: 100; loss: 0.92; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.67
Batch: 140; loss: 0.74; acc: 0.84
Val Epoch over. val_loss: 0.9873448048427607; val_accuracy: 0.7355692675159236 

The current subspace-distance is: 0.00019205501303076744 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 1.13; acc: 0.66
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.04; acc: 0.7
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 1.1; acc: 0.72
Batch: 160; loss: 1.08; acc: 0.61
Batch: 180; loss: 1.2; acc: 0.66
Batch: 200; loss: 1.09; acc: 0.62
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 1.15; acc: 0.66
Batch: 260; loss: 1.12; acc: 0.61
Batch: 280; loss: 1.02; acc: 0.72
Batch: 300; loss: 0.97; acc: 0.69
Batch: 320; loss: 1.16; acc: 0.67
Batch: 340; loss: 1.07; acc: 0.66
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 0.86; acc: 0.78
Batch: 400; loss: 1.19; acc: 0.62
Batch: 420; loss: 1.0; acc: 0.72
Batch: 440; loss: 1.01; acc: 0.75
Batch: 460; loss: 1.12; acc: 0.69
Batch: 480; loss: 1.15; acc: 0.72
Batch: 500; loss: 1.0; acc: 0.75
Batch: 520; loss: 1.08; acc: 0.67
Batch: 540; loss: 1.07; acc: 0.66
Batch: 560; loss: 0.98; acc: 0.7
Batch: 580; loss: 1.04; acc: 0.72
Batch: 600; loss: 1.0; acc: 0.83
Batch: 620; loss: 1.12; acc: 0.62
Batch: 640; loss: 1.02; acc: 0.72
Batch: 660; loss: 0.91; acc: 0.75
Batch: 680; loss: 1.1; acc: 0.67
Batch: 700; loss: 1.06; acc: 0.72
Batch: 720; loss: 1.09; acc: 0.7
Batch: 740; loss: 0.91; acc: 0.75
Batch: 760; loss: 0.94; acc: 0.78
Batch: 780; loss: 1.07; acc: 0.69
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

0.00020029705774504691
0.0001920032373163849
Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.64
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.78
Batch: 80; loss: 0.85; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.7
Batch: 140; loss: 0.74; acc: 0.86
Val Epoch over. val_loss: 0.9932428795820588; val_accuracy: 0.7331807324840764 

The current subspace-distance is: 0.0001920032373163849 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_11_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.835055857460475

The number of parameters is: 251109

The number of individual parameters is:

15
270
15
15
23
37605
23
23
45
112815
45
45
64
95040
64
64
4096
64
640
10
64
64

nonzero elements in E: 50221795
elements in E: 50221800
fraction nonzero: 0.9999999004416409
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.36; acc: 0.12
Batch: 20; loss: 2.19; acc: 0.22
Batch: 40; loss: 2.05; acc: 0.38
Batch: 60; loss: 2.03; acc: 0.31
Batch: 80; loss: 1.91; acc: 0.33
Batch: 100; loss: 1.83; acc: 0.47
Batch: 120; loss: 1.83; acc: 0.38
Batch: 140; loss: 1.77; acc: 0.59
Batch: 160; loss: 1.7; acc: 0.52
Batch: 180; loss: 1.73; acc: 0.5
Batch: 200; loss: 1.68; acc: 0.59
Batch: 220; loss: 1.61; acc: 0.61
Batch: 240; loss: 1.55; acc: 0.66
Batch: 260; loss: 1.62; acc: 0.58
Batch: 280; loss: 1.56; acc: 0.72
Batch: 300; loss: 1.58; acc: 0.62
Batch: 320; loss: 1.58; acc: 0.59
Batch: 340; loss: 1.57; acc: 0.62
Batch: 360; loss: 1.53; acc: 0.7
Batch: 380; loss: 1.59; acc: 0.56
Batch: 400; loss: 1.47; acc: 0.66
Batch: 420; loss: 1.51; acc: 0.61
Batch: 440; loss: 1.5; acc: 0.66
Batch: 460; loss: 1.52; acc: 0.64
Batch: 480; loss: 1.34; acc: 0.78
Batch: 500; loss: 1.54; acc: 0.64
Batch: 520; loss: 1.51; acc: 0.67
Batch: 540; loss: 1.48; acc: 0.62
Batch: 560; loss: 1.47; acc: 0.62
Batch: 580; loss: 1.42; acc: 0.73
Batch: 600; loss: 1.4; acc: 0.67
Batch: 620; loss: 1.48; acc: 0.62
Batch: 640; loss: 1.36; acc: 0.67
Batch: 660; loss: 1.38; acc: 0.66
Batch: 680; loss: 1.32; acc: 0.69
Batch: 700; loss: 1.41; acc: 0.7
Batch: 720; loss: 1.34; acc: 0.73
Batch: 740; loss: 1.5; acc: 0.67
Batch: 760; loss: 1.5; acc: 0.62
Batch: 780; loss: 1.38; acc: 0.67
Train Epoch over. train_loss: 1.59; train_accuracy: 0.6 

5.347288606571965e-05
4.907352558802813e-05
Batch: 0; loss: 1.46; acc: 0.55
Batch: 20; loss: 1.52; acc: 0.59
Batch: 40; loss: 1.07; acc: 0.89
Batch: 60; loss: 1.29; acc: 0.78
Batch: 80; loss: 1.29; acc: 0.7
Batch: 100; loss: 1.4; acc: 0.78
Batch: 120; loss: 1.42; acc: 0.67
Batch: 140; loss: 1.27; acc: 0.81
Val Epoch over. val_loss: 1.334267263199873; val_accuracy: 0.7125796178343949 

The current subspace-distance is: 4.907352558802813e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.37; acc: 0.67
Batch: 20; loss: 1.31; acc: 0.72
Batch: 40; loss: 1.33; acc: 0.7
Batch: 60; loss: 1.27; acc: 0.73
Batch: 80; loss: 1.24; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.62
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 1.34; acc: 0.69
Batch: 160; loss: 1.25; acc: 0.73
Batch: 180; loss: 1.38; acc: 0.66
Batch: 200; loss: 1.25; acc: 0.73
Batch: 220; loss: 1.29; acc: 0.73
Batch: 240; loss: 1.3; acc: 0.73
Batch: 260; loss: 1.32; acc: 0.69
Batch: 280; loss: 1.27; acc: 0.73
Batch: 300; loss: 1.2; acc: 0.72
Batch: 320; loss: 1.16; acc: 0.78
Batch: 340; loss: 1.29; acc: 0.66
Batch: 360; loss: 1.22; acc: 0.67
Batch: 380; loss: 1.15; acc: 0.75
Batch: 400; loss: 1.39; acc: 0.62
Batch: 420; loss: 1.21; acc: 0.69
Batch: 440; loss: 1.27; acc: 0.78
Batch: 460; loss: 1.18; acc: 0.75
Batch: 480; loss: 1.22; acc: 0.72
Batch: 500; loss: 1.1; acc: 0.77
Batch: 520; loss: 1.2; acc: 0.7
Batch: 540; loss: 1.28; acc: 0.62
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.11; acc: 0.78
Batch: 600; loss: 1.13; acc: 0.77
Batch: 620; loss: 1.11; acc: 0.67
Batch: 640; loss: 1.11; acc: 0.78
Batch: 660; loss: 1.11; acc: 0.84
Batch: 680; loss: 1.33; acc: 0.59
Batch: 700; loss: 1.08; acc: 0.8
Batch: 720; loss: 1.12; acc: 0.73
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.13; acc: 0.75
Batch: 780; loss: 1.24; acc: 0.64
Train Epoch over. train_loss: 1.24; train_accuracy: 0.72 

7.606112194480374e-05
7.096073386492208e-05
Batch: 0; loss: 1.21; acc: 0.7
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 0.77; acc: 0.91
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.67
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.0691360964137278; val_accuracy: 0.7581608280254777 

The current subspace-distance is: 7.096073386492208e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.06; acc: 0.77
Batch: 40; loss: 1.19; acc: 0.64
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 1.13; acc: 0.7
Batch: 100; loss: 1.02; acc: 0.77
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 1.14; acc: 0.66
Batch: 160; loss: 0.93; acc: 0.84
Batch: 180; loss: 1.1; acc: 0.72
Batch: 200; loss: 1.15; acc: 0.69
Batch: 220; loss: 1.06; acc: 0.75
Batch: 240; loss: 0.98; acc: 0.81
Batch: 260; loss: 1.12; acc: 0.75
Batch: 280; loss: 1.11; acc: 0.77
Batch: 300; loss: 0.97; acc: 0.8
Batch: 320; loss: 0.91; acc: 0.78
Batch: 340; loss: 1.02; acc: 0.78
Batch: 360; loss: 1.03; acc: 0.78
Batch: 380; loss: 1.14; acc: 0.72
Batch: 400; loss: 1.07; acc: 0.72
Batch: 420; loss: 0.91; acc: 0.8
Batch: 440; loss: 0.86; acc: 0.89
Batch: 460; loss: 1.05; acc: 0.72
Batch: 480; loss: 1.01; acc: 0.77
Batch: 500; loss: 1.01; acc: 0.75
Batch: 520; loss: 0.95; acc: 0.83
Batch: 540; loss: 0.91; acc: 0.81
Batch: 560; loss: 0.91; acc: 0.8
Batch: 580; loss: 1.04; acc: 0.67
Batch: 600; loss: 0.9; acc: 0.8
Batch: 620; loss: 0.98; acc: 0.77
Batch: 640; loss: 0.82; acc: 0.84
Batch: 660; loss: 0.98; acc: 0.73
Batch: 680; loss: 1.06; acc: 0.75
Batch: 700; loss: 0.91; acc: 0.86
Batch: 720; loss: 0.91; acc: 0.78
Batch: 740; loss: 0.93; acc: 0.77
Batch: 760; loss: 1.18; acc: 0.62
Batch: 780; loss: 1.03; acc: 0.73
Train Epoch over. train_loss: 1.04; train_accuracy: 0.75 

9.372524073114619e-05
8.859249646775424e-05
Batch: 0; loss: 1.03; acc: 0.72
Batch: 20; loss: 1.17; acc: 0.67
Batch: 40; loss: 0.64; acc: 0.92
Batch: 60; loss: 0.89; acc: 0.78
Batch: 80; loss: 0.8; acc: 0.84
Batch: 100; loss: 0.98; acc: 0.77
Batch: 120; loss: 1.1; acc: 0.69
Batch: 140; loss: 0.8; acc: 0.78
Val Epoch over. val_loss: 0.9257026056575167; val_accuracy: 0.7838375796178344 

The current subspace-distance is: 8.859249646775424e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.77
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 0.87; acc: 0.84
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.72
Batch: 100; loss: 0.96; acc: 0.8
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 1.05; acc: 0.77
Batch: 160; loss: 0.96; acc: 0.75
Batch: 180; loss: 0.91; acc: 0.8
Batch: 200; loss: 0.9; acc: 0.86
Batch: 220; loss: 0.87; acc: 0.86
Batch: 240; loss: 0.94; acc: 0.77
Batch: 260; loss: 0.9; acc: 0.77
Batch: 280; loss: 0.93; acc: 0.78
Batch: 300; loss: 1.01; acc: 0.67
Batch: 320; loss: 0.97; acc: 0.75
Batch: 340; loss: 0.89; acc: 0.8
Batch: 360; loss: 0.93; acc: 0.67
Batch: 380; loss: 0.95; acc: 0.73
Batch: 400; loss: 1.03; acc: 0.7
Batch: 420; loss: 0.86; acc: 0.83
Batch: 440; loss: 0.87; acc: 0.81
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 0.85; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.77
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 0.92; acc: 0.77
Batch: 560; loss: 0.89; acc: 0.78
Batch: 580; loss: 0.87; acc: 0.83
Batch: 600; loss: 0.88; acc: 0.84
Batch: 620; loss: 0.9; acc: 0.77
Batch: 640; loss: 1.03; acc: 0.77
Batch: 660; loss: 0.88; acc: 0.8
Batch: 680; loss: 0.95; acc: 0.78
Batch: 700; loss: 0.82; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.88
Batch: 740; loss: 0.98; acc: 0.73
Batch: 760; loss: 0.99; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.8
Train Epoch over. train_loss: 0.94; train_accuracy: 0.77 

0.00010739764547906816
0.00010352753452025354
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 1.11; acc: 0.69
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.89
Batch: 100; loss: 0.91; acc: 0.8
Batch: 120; loss: 1.06; acc: 0.72
Batch: 140; loss: 0.74; acc: 0.81
Val Epoch over. val_loss: 0.8542405397269377; val_accuracy: 0.7947850318471338 

The current subspace-distance is: 0.00010352753452025354 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.72
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 1.0; acc: 0.77
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 0.85; acc: 0.86
Batch: 140; loss: 0.89; acc: 0.77
Batch: 160; loss: 0.79; acc: 0.81
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.85; acc: 0.81
Batch: 220; loss: 0.98; acc: 0.78
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.9; acc: 0.8
Batch: 280; loss: 0.9; acc: 0.84
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.85; acc: 0.77
Batch: 340; loss: 0.85; acc: 0.78
Batch: 360; loss: 0.81; acc: 0.8
Batch: 380; loss: 0.94; acc: 0.78
Batch: 400; loss: 0.92; acc: 0.8
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 1.01; acc: 0.7
Batch: 460; loss: 0.87; acc: 0.8
Batch: 480; loss: 0.91; acc: 0.78
Batch: 500; loss: 0.87; acc: 0.81
Batch: 520; loss: 0.75; acc: 0.89
Batch: 540; loss: 0.78; acc: 0.84
Batch: 560; loss: 0.98; acc: 0.77
Batch: 580; loss: 0.8; acc: 0.8
Batch: 600; loss: 0.76; acc: 0.86
Batch: 620; loss: 0.97; acc: 0.72
Batch: 640; loss: 0.92; acc: 0.75
Batch: 660; loss: 0.89; acc: 0.8
Batch: 680; loss: 0.92; acc: 0.73
Batch: 700; loss: 0.92; acc: 0.78
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.97; acc: 0.75
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 0.88; acc: 0.75
Train Epoch over. train_loss: 0.88; train_accuracy: 0.78 

0.0001211548296851106
0.00011504592112032697
Batch: 0; loss: 0.91; acc: 0.77
Batch: 20; loss: 1.04; acc: 0.69
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.67; acc: 0.91
Batch: 100; loss: 0.86; acc: 0.84
Batch: 120; loss: 1.04; acc: 0.72
Batch: 140; loss: 0.68; acc: 0.81
Val Epoch over. val_loss: 0.7997693575111924; val_accuracy: 0.8084195859872612 

The current subspace-distance is: 0.00011504592112032697 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.73
Batch: 20; loss: 0.83; acc: 0.73
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.89; acc: 0.78
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.93; acc: 0.73
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 0.87; acc: 0.81
Batch: 200; loss: 0.91; acc: 0.72
Batch: 220; loss: 0.83; acc: 0.78
Batch: 240; loss: 0.85; acc: 0.78
Batch: 260; loss: 0.75; acc: 0.81
Batch: 280; loss: 0.9; acc: 0.75
Batch: 300; loss: 0.87; acc: 0.72
Batch: 320; loss: 0.92; acc: 0.77
Batch: 340; loss: 0.88; acc: 0.73
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.78; acc: 0.84
Batch: 400; loss: 1.05; acc: 0.72
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 0.94; acc: 0.69
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 0.71; acc: 0.84
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.97; acc: 0.75
Batch: 540; loss: 0.68; acc: 0.86
Batch: 560; loss: 0.9; acc: 0.72
Batch: 580; loss: 1.02; acc: 0.7
Batch: 600; loss: 0.89; acc: 0.78
Batch: 620; loss: 0.77; acc: 0.81
Batch: 640; loss: 0.74; acc: 0.84
Batch: 660; loss: 1.03; acc: 0.75
Batch: 680; loss: 0.76; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.74; acc: 0.81
Batch: 740; loss: 0.87; acc: 0.8
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.89; acc: 0.77
Train Epoch over. train_loss: 0.84; train_accuracy: 0.79 

0.00013042079808656126
0.00012361526023596525
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 0.52; acc: 0.92
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.65; acc: 0.89
Batch: 100; loss: 0.83; acc: 0.83
Batch: 120; loss: 1.02; acc: 0.7
Batch: 140; loss: 0.63; acc: 0.83
Val Epoch over. val_loss: 0.762720565127719; val_accuracy: 0.8161823248407644 

The current subspace-distance is: 0.00012361526023596525 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 1.03; acc: 0.66
Batch: 40; loss: 0.8; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.73
Batch: 100; loss: 0.83; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.88; acc: 0.77
Batch: 200; loss: 0.81; acc: 0.8
Batch: 220; loss: 0.81; acc: 0.81
Batch: 240; loss: 0.89; acc: 0.77
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.86; acc: 0.78
Batch: 300; loss: 0.76; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.85; acc: 0.77
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.94; acc: 0.64
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.82; acc: 0.78
Batch: 460; loss: 0.69; acc: 0.86
Batch: 480; loss: 0.81; acc: 0.8
Batch: 500; loss: 0.89; acc: 0.8
Batch: 520; loss: 0.74; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.84
Batch: 560; loss: 0.81; acc: 0.81
Batch: 580; loss: 0.9; acc: 0.73
Batch: 600; loss: 0.86; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.84
Batch: 640; loss: 0.84; acc: 0.8
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.84; acc: 0.75
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.79; acc: 0.8
Batch: 740; loss: 0.79; acc: 0.81
Batch: 760; loss: 0.84; acc: 0.75
Batch: 780; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.8; train_accuracy: 0.8 

0.0001390320248901844
0.00013298344856593758
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.91
Batch: 100; loss: 0.79; acc: 0.84
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.57; acc: 0.84
Val Epoch over. val_loss: 0.7133922320642289; val_accuracy: 0.8311106687898089 

The current subspace-distance is: 0.00013298344856593758 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 0.78; acc: 0.86
Batch: 80; loss: 0.7; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.83; acc: 0.81
Batch: 180; loss: 0.72; acc: 0.84
Batch: 200; loss: 0.8; acc: 0.83
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.72; acc: 0.86
Batch: 260; loss: 0.77; acc: 0.88
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.8
Batch: 320; loss: 0.8; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.75
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.78; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.87; acc: 0.73
Batch: 460; loss: 0.93; acc: 0.72
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.75; acc: 0.8
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.78; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.98; acc: 0.66
Batch: 620; loss: 0.74; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.91
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.83; acc: 0.8
Batch: 720; loss: 0.91; acc: 0.78
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.72; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00015093608817551285
0.00014401738008018583
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.88
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.6732504561448552; val_accuracy: 0.8406648089171974 

The current subspace-distance is: 0.00014401738008018583 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.94; acc: 0.67
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.78
Batch: 180; loss: 0.89; acc: 0.72
Batch: 200; loss: 0.74; acc: 0.83
Batch: 220; loss: 0.61; acc: 0.94
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.83; acc: 0.73
Batch: 280; loss: 0.76; acc: 0.88
Batch: 300; loss: 0.82; acc: 0.81
Batch: 320; loss: 0.74; acc: 0.88
Batch: 340; loss: 0.79; acc: 0.8
Batch: 360; loss: 0.73; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.77; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.75
Batch: 520; loss: 0.61; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.88; acc: 0.75
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.68; acc: 0.88
Batch: 640; loss: 0.8; acc: 0.83
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.89
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.83
Batch: 740; loss: 0.73; acc: 0.81
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.86
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.0001578421943122521
0.00015162391355261207
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.89
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.6544661956607916; val_accuracy: 0.8476313694267515 

The current subspace-distance is: 0.00015162391355261207 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.84
Batch: 180; loss: 0.79; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.71; acc: 0.81
Batch: 260; loss: 0.89; acc: 0.75
Batch: 280; loss: 0.8; acc: 0.81
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.88
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.91; acc: 0.72
Batch: 440; loss: 0.81; acc: 0.75
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.81; acc: 0.73
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.77; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.94
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.65; acc: 0.91
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.89
Batch: 740; loss: 0.75; acc: 0.78
Batch: 760; loss: 0.92; acc: 0.77
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0001692562800599262
0.00016188810695894063
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.72
Batch: 140; loss: 0.48; acc: 0.88
Val Epoch over. val_loss: 0.6234655687763433; val_accuracy: 0.8538017515923567 

The current subspace-distance is: 0.00016188810695894063 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.91
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.86
Batch: 180; loss: 0.93; acc: 0.77
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.75; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.68; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.77; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.86
Batch: 460; loss: 0.77; acc: 0.8
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.87; acc: 0.77
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.65; acc: 0.86
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.75
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.84
Batch: 780; loss: 0.85; acc: 0.77
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00017042653053067625
0.00016336917178705335
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.49; acc: 0.88
Val Epoch over. val_loss: 0.6239037031580688; val_accuracy: 0.853702229299363 

The current subspace-distance is: 0.00016336917178705335 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.87; acc: 0.78
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.77; acc: 0.73
Batch: 320; loss: 0.68; acc: 0.77
Batch: 340; loss: 0.89; acc: 0.8
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.84; acc: 0.75
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.68; acc: 0.91
Batch: 480; loss: 0.96; acc: 0.72
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.76; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.68; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.88
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.67; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.89
Batch: 720; loss: 0.72; acc: 0.8
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.86; acc: 0.73
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00017146836034953594
0.00016374015831388533
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.72
Batch: 140; loss: 0.49; acc: 0.88
Val Epoch over. val_loss: 0.6164556732223292; val_accuracy: 0.8551950636942676 

The current subspace-distance is: 0.00016374015831388533 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.89; acc: 0.73
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.85; acc: 0.83
Batch: 220; loss: 0.42; acc: 0.98
Batch: 240; loss: 0.73; acc: 0.8
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.77; acc: 0.72
Batch: 380; loss: 0.62; acc: 0.88
Batch: 400; loss: 0.79; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.59; acc: 0.91
Batch: 600; loss: 0.77; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.85; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.91
Batch: 720; loss: 0.78; acc: 0.77
Batch: 740; loss: 0.85; acc: 0.77
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00017371740250382572
0.00016848761879373342
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.47; acc: 0.88
Val Epoch over. val_loss: 0.6046759869642319; val_accuracy: 0.8580812101910829 

The current subspace-distance is: 0.00016848761879373342 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.63; acc: 0.88
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.86
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.71; acc: 0.77
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.72; acc: 0.75
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.74; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.79; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.00017692259280011058
0.0001691888173809275
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.88
Val Epoch over. val_loss: 0.6035536859825159; val_accuracy: 0.8576831210191083 

The current subspace-distance is: 0.0001691888173809275 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.74; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.77
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.86
Batch: 240; loss: 0.8; acc: 0.75
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.72; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.92
Batch: 340; loss: 0.78; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.79; acc: 0.77
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.88
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00017961546836886555
0.00017304143693763763
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.88
Val Epoch over. val_loss: 0.603259650954775; val_accuracy: 0.8586783439490446 

The current subspace-distance is: 0.00017304143693763763 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.63; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.64; acc: 0.89
Batch: 200; loss: 0.7; acc: 0.81
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.75; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.73; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.92
Batch: 480; loss: 0.66; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.56; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.79; acc: 0.81
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.87; acc: 0.72
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.0001831334229791537
0.00017279735766351223
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.88
Val Epoch over. val_loss: 0.597495235455264; val_accuracy: 0.8581807324840764 

The current subspace-distance is: 0.00017279735766351223 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.8; acc: 0.8
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.67; acc: 0.86
Batch: 260; loss: 0.78; acc: 0.8
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.81
Batch: 320; loss: 0.82; acc: 0.8
Batch: 340; loss: 0.75; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.8; acc: 0.83
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.63; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.7; acc: 0.84
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00018227427790407091
0.00017444297554902732
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.88
Val Epoch over. val_loss: 0.5956011509439748; val_accuracy: 0.8575835987261147 

The current subspace-distance is: 0.00017444297554902732 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.92
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.74; acc: 0.73
Batch: 180; loss: 0.57; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.81; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.77
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.91
Batch: 320; loss: 0.74; acc: 0.8
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.92
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.86
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 0.83; acc: 0.8
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.85; acc: 0.75
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.45; acc: 0.95
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.0001886560203274712
0.00017675031267572194
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.5892500970393989; val_accuracy: 0.8596735668789809 

The current subspace-distance is: 0.00017675031267572194 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.54; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.68; acc: 0.84
Batch: 200; loss: 0.65; acc: 0.8
Batch: 220; loss: 0.52; acc: 0.92
Batch: 240; loss: 0.62; acc: 0.89
Batch: 260; loss: 0.76; acc: 0.73
Batch: 280; loss: 0.82; acc: 0.83
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.71; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.92; acc: 0.75
Batch: 440; loss: 0.76; acc: 0.73
Batch: 460; loss: 0.67; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.84
Batch: 580; loss: 0.7; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.81; acc: 0.77
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.81; acc: 0.78
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.6; acc: 0.89
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.7; acc: 0.77
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.0001870658015832305
0.00017874875629786402
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.587449237039894; val_accuracy: 0.86046974522293 

The current subspace-distance is: 0.00017874875629786402 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.6; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.75
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.76; acc: 0.78
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.91
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.77; acc: 0.8
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.67; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00019026035442948341
0.0001817449665395543
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.5758449596584223; val_accuracy: 0.8576831210191083 

The current subspace-distance is: 0.0001817449665395543 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.63; acc: 0.89
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.91
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.92
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.58; acc: 0.89
Batch: 320; loss: 0.64; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.68; acc: 0.8
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.78
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.83
Batch: 520; loss: 0.9; acc: 0.69
Batch: 540; loss: 0.51; acc: 0.92
Batch: 560; loss: 0.75; acc: 0.81
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.68; acc: 0.86
Batch: 620; loss: 0.73; acc: 0.81
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.66; acc: 0.77
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00018869615450967103
0.00018060175352729857
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.97
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.43; acc: 0.89
Val Epoch over. val_loss: 0.5747291479900385; val_accuracy: 0.8613654458598726 

The current subspace-distance is: 0.00018060175352729857 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.78
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.73; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.79; acc: 0.81
Batch: 460; loss: 0.48; acc: 0.95
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.62; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.82; acc: 0.8
Batch: 640; loss: 0.72; acc: 0.8
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.68; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.73; acc: 0.78
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00019151752348989248
0.00018377452215645462
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.43; acc: 0.89
Val Epoch over. val_loss: 0.5794148858945081; val_accuracy: 0.862062101910828 

The current subspace-distance is: 0.00018377452215645462 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.92
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.77; acc: 0.77
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.71; acc: 0.8
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.88
Batch: 320; loss: 0.65; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.74; acc: 0.8
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.92
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.68; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.88
Batch: 540; loss: 0.64; acc: 0.89
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.91
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.64; acc: 0.91
Batch: 680; loss: 0.8; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.91
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00019150879234075546
0.0001826930820243433
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.5851110654651739; val_accuracy: 0.8601711783439491 

The current subspace-distance is: 0.0001826930820243433 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.95
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.77; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.79; acc: 0.77
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.73; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.77
Batch: 600; loss: 0.63; acc: 0.88
Batch: 620; loss: 0.87; acc: 0.75
Batch: 640; loss: 0.67; acc: 0.91
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00019354894175194204
0.0001839652977650985
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.97
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.5796561736589784; val_accuracy: 0.8626592356687898 

The current subspace-distance is: 0.0001839652977650985 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.73; acc: 0.72
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.53; acc: 0.91
Batch: 220; loss: 0.79; acc: 0.73
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.61; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.58; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.62; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.7; acc: 0.77
Batch: 560; loss: 0.77; acc: 0.78
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.88
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00019292581418994814
0.00018546241335570812
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5721077858262761; val_accuracy: 0.8606687898089171 

The current subspace-distance is: 0.00018546241335570812 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.7
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.69; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.84; acc: 0.75
Batch: 240; loss: 0.39; acc: 0.95
Batch: 260; loss: 0.74; acc: 0.77
Batch: 280; loss: 0.51; acc: 0.92
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.65; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.89
Batch: 460; loss: 0.71; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.78; acc: 0.75
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00019147424609400332
0.00018502181046642363
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.5729605122736305; val_accuracy: 0.8621616242038217 

The current subspace-distance is: 0.00018502181046642363 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.72
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.76; acc: 0.86
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.77; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.67; acc: 0.78
Batch: 420; loss: 0.71; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.85; acc: 0.77
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.68; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.84
Batch: 760; loss: 0.77; acc: 0.78
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00019486472592689097
0.00018723594257608056
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.5740035001639348; val_accuracy: 0.8598726114649682 

The current subspace-distance is: 0.00018723594257608056 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.81; acc: 0.83
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.65; acc: 0.78
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.7; acc: 0.78
Batch: 360; loss: 0.78; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.84; acc: 0.73
Batch: 480; loss: 0.58; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.83; acc: 0.78
Batch: 680; loss: 0.67; acc: 0.84
Batch: 700; loss: 0.83; acc: 0.8
Batch: 720; loss: 0.58; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00019292783690616488
0.0001846975355874747
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.5802268645945629; val_accuracy: 0.8605692675159236 

The current subspace-distance is: 0.0001846975355874747 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.79; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.81; acc: 0.78
Batch: 180; loss: 0.65; acc: 0.8
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.92
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.77
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.75; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.7; acc: 0.77
Batch: 520; loss: 0.66; acc: 0.77
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.61; acc: 0.8
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.75
Batch: 660; loss: 0.69; acc: 0.83
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.71; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.86
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.82; acc: 0.8
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00019273743964731693
0.0001867984829004854
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.88
Val Epoch over. val_loss: 0.5736324090486878; val_accuracy: 0.8625597133757962 

The current subspace-distance is: 0.0001867984829004854 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.86; acc: 0.78
Batch: 360; loss: 0.67; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.63; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.63; train_accuracy: 0.84 

0.0001929592399392277
0.0001861268247012049
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.5702823645370022; val_accuracy: 0.8640525477707006 

The current subspace-distance is: 0.0001861268247012049 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_11_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.835055857460475

The number of parameters is: 251109

The number of individual parameters is:

15
270
15
15
23
37605
23
23
45
112815
45
45
64
95040
64
64
4096
64
640
10
64
64

nonzero elements in E: 75332691
elements in E: 75332700
fraction nonzero: 0.999999880529969
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.36; acc: 0.06
Batch: 20; loss: 2.07; acc: 0.27
Batch: 40; loss: 1.87; acc: 0.47
Batch: 60; loss: 1.8; acc: 0.48
Batch: 80; loss: 1.84; acc: 0.45
Batch: 100; loss: 1.57; acc: 0.67
Batch: 120; loss: 1.54; acc: 0.61
Batch: 140; loss: 1.52; acc: 0.73
Batch: 160; loss: 1.49; acc: 0.67
Batch: 180; loss: 1.67; acc: 0.53
Batch: 200; loss: 1.53; acc: 0.66
Batch: 220; loss: 1.5; acc: 0.73
Batch: 240; loss: 1.36; acc: 0.78
Batch: 260; loss: 1.42; acc: 0.67
Batch: 280; loss: 1.35; acc: 0.78
Batch: 300; loss: 1.35; acc: 0.77
Batch: 320; loss: 1.35; acc: 0.73
Batch: 340; loss: 1.39; acc: 0.72
Batch: 360; loss: 1.38; acc: 0.69
Batch: 380; loss: 1.36; acc: 0.73
Batch: 400; loss: 1.26; acc: 0.77
Batch: 420; loss: 1.37; acc: 0.64
Batch: 440; loss: 1.47; acc: 0.56
Batch: 460; loss: 1.3; acc: 0.72
Batch: 480; loss: 1.19; acc: 0.78
Batch: 500; loss: 1.22; acc: 0.77
Batch: 520; loss: 1.24; acc: 0.73
Batch: 540; loss: 1.19; acc: 0.84
Batch: 560; loss: 1.2; acc: 0.83
Batch: 580; loss: 1.16; acc: 0.83
Batch: 600; loss: 1.3; acc: 0.7
Batch: 620; loss: 1.15; acc: 0.81
Batch: 640; loss: 1.16; acc: 0.83
Batch: 660; loss: 1.21; acc: 0.73
Batch: 680; loss: 1.24; acc: 0.72
Batch: 700; loss: 1.17; acc: 0.8
Batch: 720; loss: 1.3; acc: 0.72
Batch: 740; loss: 1.23; acc: 0.73
Batch: 760; loss: 1.22; acc: 0.72
Batch: 780; loss: 1.19; acc: 0.7
Train Epoch over. train_loss: 1.39; train_accuracy: 0.69 

5.709535253117792e-05
5.2180010243318975e-05
Batch: 0; loss: 1.16; acc: 0.84
Batch: 20; loss: 1.27; acc: 0.81
Batch: 40; loss: 0.84; acc: 0.89
Batch: 60; loss: 1.07; acc: 0.86
Batch: 80; loss: 1.01; acc: 0.89
Batch: 100; loss: 1.09; acc: 0.78
Batch: 120; loss: 1.19; acc: 0.7
Batch: 140; loss: 0.94; acc: 0.84
Val Epoch over. val_loss: 1.1002685674436532; val_accuracy: 0.8063296178343949 

The current subspace-distance is: 5.2180010243318975e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.88
Batch: 20; loss: 1.15; acc: 0.8
Batch: 40; loss: 1.09; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.78
Batch: 80; loss: 1.04; acc: 0.84
Batch: 100; loss: 1.07; acc: 0.83
Batch: 120; loss: 1.25; acc: 0.72
Batch: 140; loss: 1.22; acc: 0.72
Batch: 160; loss: 1.01; acc: 0.8
Batch: 180; loss: 1.1; acc: 0.81
Batch: 200; loss: 1.01; acc: 0.83
Batch: 220; loss: 1.02; acc: 0.88
Batch: 240; loss: 1.09; acc: 0.77
Batch: 260; loss: 1.06; acc: 0.77
Batch: 280; loss: 0.97; acc: 0.86
Batch: 300; loss: 1.19; acc: 0.73
Batch: 320; loss: 1.17; acc: 0.75
Batch: 340; loss: 1.09; acc: 0.78
Batch: 360; loss: 1.16; acc: 0.72
Batch: 380; loss: 0.95; acc: 0.86
Batch: 400; loss: 0.98; acc: 0.84
Batch: 420; loss: 1.02; acc: 0.83
Batch: 440; loss: 1.13; acc: 0.77
Batch: 460; loss: 1.05; acc: 0.77
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.96; acc: 0.84
Batch: 520; loss: 1.11; acc: 0.77
Batch: 540; loss: 1.01; acc: 0.83
Batch: 560; loss: 1.02; acc: 0.83
Batch: 580; loss: 1.05; acc: 0.73
Batch: 600; loss: 1.09; acc: 0.81
Batch: 620; loss: 0.9; acc: 0.8
Batch: 640; loss: 0.89; acc: 0.92
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.92; acc: 0.81
Batch: 700; loss: 0.9; acc: 0.84
Batch: 720; loss: 0.94; acc: 0.83
Batch: 740; loss: 0.98; acc: 0.8
Batch: 760; loss: 0.86; acc: 0.83
Batch: 780; loss: 0.99; acc: 0.83
Train Epoch over. train_loss: 1.05; train_accuracy: 0.8 

7.890225242590532e-05
7.270204514497891e-05
Batch: 0; loss: 0.91; acc: 0.84
Batch: 20; loss: 1.09; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.92
Batch: 60; loss: 0.85; acc: 0.86
Batch: 80; loss: 0.78; acc: 0.91
Batch: 100; loss: 0.91; acc: 0.89
Batch: 120; loss: 1.02; acc: 0.8
Batch: 140; loss: 0.72; acc: 0.91
Val Epoch over. val_loss: 0.9035773227928551; val_accuracy: 0.835390127388535 

The current subspace-distance is: 7.270204514497891e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.86
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 0.84; acc: 0.91
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 0.9; acc: 0.88
Batch: 100; loss: 1.01; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 1.03; acc: 0.81
Batch: 160; loss: 0.78; acc: 0.91
Batch: 180; loss: 0.89; acc: 0.84
Batch: 200; loss: 1.04; acc: 0.78
Batch: 220; loss: 0.84; acc: 0.83
Batch: 240; loss: 0.92; acc: 0.78
Batch: 260; loss: 0.95; acc: 0.77
Batch: 280; loss: 0.92; acc: 0.89
Batch: 300; loss: 0.83; acc: 0.89
Batch: 320; loss: 0.98; acc: 0.77
Batch: 340; loss: 0.98; acc: 0.77
Batch: 360; loss: 1.02; acc: 0.72
Batch: 380; loss: 0.97; acc: 0.73
Batch: 400; loss: 0.78; acc: 0.86
Batch: 420; loss: 0.98; acc: 0.81
Batch: 440; loss: 0.91; acc: 0.86
Batch: 460; loss: 0.87; acc: 0.78
Batch: 480; loss: 0.82; acc: 0.84
Batch: 500; loss: 0.84; acc: 0.89
Batch: 520; loss: 0.82; acc: 0.86
Batch: 540; loss: 0.83; acc: 0.84
Batch: 560; loss: 0.93; acc: 0.84
Batch: 580; loss: 0.8; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.89
Batch: 620; loss: 0.76; acc: 0.89
Batch: 640; loss: 0.91; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.77; acc: 0.91
Batch: 700; loss: 0.8; acc: 0.86
Batch: 720; loss: 0.84; acc: 0.83
Batch: 740; loss: 0.79; acc: 0.86
Batch: 760; loss: 1.0; acc: 0.8
Batch: 780; loss: 0.94; acc: 0.78
Train Epoch over. train_loss: 0.9; train_accuracy: 0.82 

9.672076703282073e-05
9.182983194477856e-05
Batch: 0; loss: 0.78; acc: 0.89
Batch: 20; loss: 0.92; acc: 0.8
Batch: 40; loss: 0.53; acc: 0.97
Batch: 60; loss: 0.73; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.94
Batch: 100; loss: 0.77; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 0.7789567731747962; val_accuracy: 0.8532046178343949 

The current subspace-distance is: 9.182983194477856e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.83
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.8; acc: 0.84
Batch: 100; loss: 0.74; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.83
Batch: 140; loss: 0.69; acc: 0.89
Batch: 160; loss: 0.87; acc: 0.81
Batch: 180; loss: 0.85; acc: 0.8
Batch: 200; loss: 0.82; acc: 0.88
Batch: 220; loss: 0.86; acc: 0.84
Batch: 240; loss: 0.84; acc: 0.8
Batch: 260; loss: 0.76; acc: 0.88
Batch: 280; loss: 0.76; acc: 0.89
Batch: 300; loss: 0.77; acc: 0.86
Batch: 320; loss: 0.85; acc: 0.8
Batch: 340; loss: 0.9; acc: 0.81
Batch: 360; loss: 0.73; acc: 0.92
Batch: 380; loss: 0.83; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.94; acc: 0.73
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 0.83; acc: 0.84
Batch: 480; loss: 0.89; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.89
Batch: 520; loss: 0.79; acc: 0.83
Batch: 540; loss: 0.79; acc: 0.83
Batch: 560; loss: 0.84; acc: 0.89
Batch: 580; loss: 0.74; acc: 0.89
Batch: 600; loss: 0.71; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.89
Batch: 640; loss: 0.77; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.91
Batch: 680; loss: 0.76; acc: 0.88
Batch: 700; loss: 0.66; acc: 0.88
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.75; acc: 0.86
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.79; train_accuracy: 0.84 

0.00011192091187695041
0.00010604541603242978
Batch: 0; loss: 0.73; acc: 0.95
Batch: 20; loss: 0.78; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.97
Batch: 60; loss: 0.68; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.94
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.91
Val Epoch over. val_loss: 0.6990390507278929; val_accuracy: 0.8633558917197452 

The current subspace-distance is: 0.00010604541603242978 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.91
Batch: 80; loss: 0.89; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.88
Batch: 140; loss: 0.76; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.91
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.59; acc: 0.94
Batch: 260; loss: 0.76; acc: 0.86
Batch: 280; loss: 0.78; acc: 0.84
Batch: 300; loss: 0.86; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.84; acc: 0.78
Batch: 380; loss: 0.82; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.89
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.74; acc: 0.83
Batch: 460; loss: 0.73; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.74; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.94
Batch: 620; loss: 0.76; acc: 0.81
Batch: 640; loss: 0.67; acc: 0.88
Batch: 660; loss: 0.73; acc: 0.83
Batch: 680; loss: 0.76; acc: 0.78
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.73; acc: 0.86
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.95
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.72; train_accuracy: 0.85 

0.00012617152242455631
0.0001207171444548294
Batch: 0; loss: 0.67; acc: 0.92
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.6342067002870475; val_accuracy: 0.8714171974522293 

The current subspace-distance is: 0.0001207171444548294 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.89
Batch: 180; loss: 0.67; acc: 0.86
Batch: 200; loss: 0.71; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.89
Batch: 280; loss: 0.65; acc: 0.89
Batch: 300; loss: 0.87; acc: 0.75
Batch: 320; loss: 0.89; acc: 0.72
Batch: 340; loss: 0.67; acc: 0.88
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.53; acc: 0.92
Batch: 400; loss: 0.72; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.7; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.72; acc: 0.75
Batch: 620; loss: 0.67; acc: 0.88
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.91
Batch: 700; loss: 0.59; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.66; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.66; train_accuracy: 0.86 

0.00013715754903387278
0.00013189297169446945
Batch: 0; loss: 0.61; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.94
Val Epoch over. val_loss: 0.5818271621776994; val_accuracy: 0.8804737261146497 

The current subspace-distance is: 0.00013189297169446945 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.91
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.91
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.8
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.94
Batch: 400; loss: 0.66; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.92
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.67; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.87 

0.0001470456481911242
0.00014031138562131673
Batch: 0; loss: 0.56; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.98
Batch: 60; loss: 0.57; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.5216350394069769; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.00014031138562131673 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.94
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.94
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.89
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

0.00015609936963301152
0.00014922076661605388
Batch: 0; loss: 0.52; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.484696999097326; val_accuracy: 0.9003781847133758 

The current subspace-distance is: 0.00014922076661605388 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.94
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.65; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.45; acc: 0.95
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0001646713790250942
0.00015730128507129848
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.46245073702684636; val_accuracy: 0.9037619426751592 

The current subspace-distance is: 0.00015730128507129848 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.51; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.97
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.95
Batch: 260; loss: 0.53; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.81
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.97
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

0.00017068108718376607
0.00016526601393707097
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.4415700820980558; val_accuracy: 0.9041600318471338 

The current subspace-distance is: 0.00016526601393707097 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.94
Batch: 180; loss: 0.63; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.0001742303866194561
0.00016533337475266308
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.4365392818002944; val_accuracy: 0.9041600318471338 

The current subspace-distance is: 0.00016533337475266308 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.95
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.89
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.0001773010299075395
0.00016795541159808636
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.4276278882649294; val_accuracy: 0.9078423566878981 

The current subspace-distance is: 0.00016795541159808636 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.97
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.65; acc: 0.78
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.8
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.95
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00017638200370129198
0.00016959352069534361
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.42228276334750425; val_accuracy: 0.9068471337579618 

The current subspace-distance is: 0.00016959352069534361 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.94
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.33; acc: 0.95
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.95
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00017921328253578395
0.00017102817946579307
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.41921830794234183; val_accuracy: 0.9070461783439491 

The current subspace-distance is: 0.00017102817946579307 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.97
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.92
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.95
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.0001810531539376825
0.0001735368132358417
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.4126575225667589; val_accuracy: 0.9083399681528662 

The current subspace-distance is: 0.0001735368132358417 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.95
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00018132792320102453
0.00017452413158025593
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.41384777322316624; val_accuracy: 0.9094347133757962 

The current subspace-distance is: 0.00017452413158025593 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.78
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.97
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.83
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00018551625544205308
0.00017723676864989102
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.40976094924340584; val_accuracy: 0.9096337579617835 

The current subspace-distance is: 0.00017723676864989102 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.95
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.71; acc: 0.77
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.95
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.68; acc: 0.75
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00018809882749337703
0.00017945622676052153
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.40476582308483733; val_accuracy: 0.9095342356687898 

The current subspace-distance is: 0.00017945622676052153 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.97
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00018868065671995282
0.00018058952991850674
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.3984289088636447; val_accuracy: 0.9115246815286624 

The current subspace-distance is: 0.00018058952991850674 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.36; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.98
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.97
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.98
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.95
Train Epoch over. train_loss: 0.45; train_accuracy: 0.89 

0.00019041186897084117
0.00018196742166765034
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.3978884742138492; val_accuracy: 0.9107285031847133 

The current subspace-distance is: 0.00018196742166765034 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.98
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.94
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.95
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.95
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.6; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

0.0001905068347696215
0.0001811355323297903
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.39301976542563954; val_accuracy: 0.911922770700637 

The current subspace-distance is: 0.0001811355323297903 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.68; acc: 0.73
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.98
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.67; acc: 0.8
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.98
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.98
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

0.0001906505785882473
0.00018244041712023318
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.39010857567665685; val_accuracy: 0.9122213375796179 

The current subspace-distance is: 0.00018244041712023318 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.97
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

0.0001896342873806134
0.00018126508803106844
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3931380592903514; val_accuracy: 0.913515127388535 

The current subspace-distance is: 0.00018126508803106844 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.37; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.97
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.97
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.97
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

0.00019094943127129227
0.00018388997705187649
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.39612197268540694; val_accuracy: 0.9114251592356688 

The current subspace-distance is: 0.00018388997705187649 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.64; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00019178393995389342
0.00018319941591471434
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3904460178818672; val_accuracy: 0.9116242038216561 

The current subspace-distance is: 0.00018319941591471434 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.97
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.56; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.95
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00019147606508340687
0.00018228095723316073
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3904856419677188; val_accuracy: 0.9128184713375797 

The current subspace-distance is: 0.00018228095723316073 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.98
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.3; acc: 0.97
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00019338467973284423
0.00018478647689335048
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.39169624685102206; val_accuracy: 0.911922770700637 

The current subspace-distance is: 0.00018478647689335048 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.98
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.98
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00019396106654312462
0.00018462850130163133
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.385424778434881; val_accuracy: 0.9133160828025477 

The current subspace-distance is: 0.00018462850130163133 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.46; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00019232132763136178
0.00018646899843588471
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3876099608316543; val_accuracy: 0.9126194267515924 

The current subspace-distance is: 0.00018646899843588471 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.98
Batch: 460; loss: 0.32; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.98
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00019312533549964428
0.00018590119725558907
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.38743320373213214; val_accuracy: 0.9144108280254777 

The current subspace-distance is: 0.00018590119725558907 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_11_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.835055857460475

The number of parameters is: 251109

The number of individual parameters is:

15
270
15
15
23
37605
23
23
45
112815
45
45
64
95040
64
64
4096
64
640
10
64
64

nonzero elements in E: 100443589
elements in E: 100443600
fraction nonzero: 0.999999890485805
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.34; acc: 0.08
Batch: 20; loss: 1.94; acc: 0.38
Batch: 40; loss: 1.93; acc: 0.36
Batch: 60; loss: 1.74; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.59
Batch: 100; loss: 1.54; acc: 0.64
Batch: 120; loss: 1.51; acc: 0.69
Batch: 140; loss: 1.6; acc: 0.61
Batch: 160; loss: 1.63; acc: 0.55
Batch: 180; loss: 1.4; acc: 0.69
Batch: 200; loss: 1.53; acc: 0.59
Batch: 220; loss: 1.36; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.78
Batch: 260; loss: 1.28; acc: 0.75
Batch: 280; loss: 1.26; acc: 0.73
Batch: 300; loss: 1.23; acc: 0.81
Batch: 320; loss: 1.17; acc: 0.78
Batch: 340; loss: 1.21; acc: 0.77
Batch: 360; loss: 1.16; acc: 0.75
Batch: 380; loss: 1.17; acc: 0.88
Batch: 400; loss: 1.33; acc: 0.69
Batch: 420; loss: 1.08; acc: 0.86
Batch: 440; loss: 1.28; acc: 0.73
Batch: 460; loss: 1.19; acc: 0.83
Batch: 480; loss: 1.25; acc: 0.75
Batch: 500; loss: 1.12; acc: 0.81
Batch: 520; loss: 1.27; acc: 0.67
Batch: 540; loss: 1.17; acc: 0.73
Batch: 560; loss: 1.21; acc: 0.72
Batch: 580; loss: 1.05; acc: 0.81
Batch: 600; loss: 1.06; acc: 0.83
Batch: 620; loss: 1.22; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.86
Batch: 660; loss: 1.09; acc: 0.78
Batch: 680; loss: 1.17; acc: 0.8
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 0.96; acc: 0.89
Batch: 740; loss: 1.03; acc: 0.8
Batch: 760; loss: 0.87; acc: 0.86
Batch: 780; loss: 1.03; acc: 0.84
Train Epoch over. train_loss: 1.28; train_accuracy: 0.72 

2.4556731659686193e-05
8.500188414473087e-06
Batch: 0; loss: 1.0; acc: 0.89
Batch: 20; loss: 1.11; acc: 0.69
Batch: 40; loss: 0.69; acc: 0.94
Batch: 60; loss: 0.86; acc: 0.83
Batch: 80; loss: 0.82; acc: 0.94
Batch: 100; loss: 0.95; acc: 0.84
Batch: 120; loss: 1.06; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.91
Val Epoch over. val_loss: 0.9313306470585477; val_accuracy: 0.8389729299363057 

The current subspace-distance is: 8.500188414473087e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.8
Batch: 20; loss: 1.02; acc: 0.78
Batch: 40; loss: 0.86; acc: 0.89
Batch: 60; loss: 1.01; acc: 0.83
Batch: 80; loss: 0.98; acc: 0.8
Batch: 100; loss: 0.99; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.88
Batch: 140; loss: 0.85; acc: 0.89
Batch: 160; loss: 1.02; acc: 0.75
Batch: 180; loss: 0.85; acc: 0.86
Batch: 200; loss: 0.94; acc: 0.8
Batch: 220; loss: 0.85; acc: 0.86
Batch: 240; loss: 0.98; acc: 0.77
Batch: 260; loss: 0.93; acc: 0.81
Batch: 280; loss: 0.83; acc: 0.83
Batch: 300; loss: 0.88; acc: 0.86
Batch: 320; loss: 0.9; acc: 0.86
Batch: 340; loss: 0.9; acc: 0.77
Batch: 360; loss: 0.89; acc: 0.8
Batch: 380; loss: 0.83; acc: 0.84
Batch: 400; loss: 0.89; acc: 0.84
Batch: 420; loss: 0.81; acc: 0.83
Batch: 440; loss: 0.81; acc: 0.86
Batch: 460; loss: 0.74; acc: 0.88
Batch: 480; loss: 0.73; acc: 0.89
Batch: 500; loss: 0.88; acc: 0.77
Batch: 520; loss: 1.0; acc: 0.81
Batch: 540; loss: 0.79; acc: 0.89
Batch: 560; loss: 0.85; acc: 0.83
Batch: 580; loss: 0.83; acc: 0.88
Batch: 600; loss: 0.85; acc: 0.8
Batch: 620; loss: 0.87; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.89
Batch: 660; loss: 0.86; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.8
Batch: 700; loss: 0.84; acc: 0.78
Batch: 720; loss: 0.64; acc: 0.95
Batch: 740; loss: 0.72; acc: 0.92
Batch: 760; loss: 0.84; acc: 0.84
Batch: 780; loss: 0.69; acc: 0.88
Train Epoch over. train_loss: 0.86; train_accuracy: 0.84 

3.051853855140507e-05
1.2681802218139637e-05
Batch: 0; loss: 0.83; acc: 0.91
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.95
Batch: 100; loss: 0.75; acc: 0.89
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.7312251648325829; val_accuracy: 0.8655453821656051 

The current subspace-distance is: 1.2681802218139637e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.8
Batch: 20; loss: 0.77; acc: 0.84
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.92
Batch: 80; loss: 0.75; acc: 0.84
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.89
Batch: 160; loss: 0.7; acc: 0.86
Batch: 180; loss: 0.74; acc: 0.88
Batch: 200; loss: 0.65; acc: 0.91
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 0.76; acc: 0.88
Batch: 260; loss: 0.84; acc: 0.86
Batch: 280; loss: 0.8; acc: 0.78
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.84; acc: 0.75
Batch: 340; loss: 0.87; acc: 0.77
Batch: 360; loss: 0.69; acc: 0.91
Batch: 380; loss: 0.81; acc: 0.77
Batch: 400; loss: 0.74; acc: 0.8
Batch: 420; loss: 0.89; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.95
Batch: 480; loss: 0.67; acc: 0.89
Batch: 500; loss: 0.76; acc: 0.83
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.89
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.82; acc: 0.78
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.73; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.92
Batch: 660; loss: 0.61; acc: 0.89
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.65; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.94
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.68; acc: 0.91
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.72; train_accuracy: 0.86 

3.536117583280429e-05
1.5124771380214952e-05
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.97
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.98
Val Epoch over. val_loss: 0.6028518891258604; val_accuracy: 0.8867436305732485 

The current subspace-distance is: 1.5124771380214952e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.78
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.76; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.91
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.67; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.64; acc: 0.91
Batch: 300; loss: 0.79; acc: 0.81
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.62; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.89
Batch: 420; loss: 0.64; acc: 0.91
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.95
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.65; acc: 0.88
Batch: 640; loss: 0.76; acc: 0.83
Batch: 660; loss: 0.52; acc: 0.95
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.67; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.65; acc: 0.88
Batch: 760; loss: 0.68; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.61; train_accuracy: 0.88 

3.9412152545992285e-05
1.715541111479979e-05
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.98
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.97
Val Epoch over. val_loss: 0.5303538783340697; val_accuracy: 0.8921178343949044 

The current subspace-distance is: 1.715541111479979e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.91
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.95
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.67; acc: 0.86
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.78
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.95
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.73; acc: 0.78
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.89 

4.3709133024094626e-05
1.920759677886963e-05
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.98
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.479305061090524; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 1.920759677886963e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.61; acc: 0.8
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.94
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

4.671323767979629e-05
2.028524068009574e-05
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.43822327417552853; val_accuracy: 0.9027667197452229 

The current subspace-distance is: 2.028524068009574e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.97
Batch: 660; loss: 0.43; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

4.99014240631368e-05
2.3364080334431492e-05
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4088525824296247; val_accuracy: 0.9096337579617835 

The current subspace-distance is: 2.3364080334431492e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.37; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.243315172265284e-05
2.5184077458106913e-05
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.3925519224944388; val_accuracy: 0.9098328025477707 

The current subspace-distance is: 2.5184077458106913e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.55; acc: 0.83
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.520014383364469e-05
2.5883413400151767e-05
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.37095775403034914; val_accuracy: 0.912718949044586 

The current subspace-distance is: 2.5883413400151767e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

5.6156975915655494e-05
2.5314198865089566e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.35247388918688344; val_accuracy: 0.9148089171974523 

The current subspace-distance is: 2.5314198865089566e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.813241659780033e-05
2.7197002054890618e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.34332565308376484; val_accuracy: 0.9164012738853503 

The current subspace-distance is: 2.7197002054890618e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.811911978526041e-05
2.595040677988436e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.34133462456001595; val_accuracy: 0.9165007961783439 

The current subspace-distance is: 2.595040677988436e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.8947072830051184e-05
2.7098280042991973e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3388556008505973; val_accuracy: 0.9173964968152867 

The current subspace-distance is: 2.7098280042991973e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.95
Batch: 240; loss: 0.25; acc: 1.0
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.97
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.966739990981296e-05
2.758719347184524e-05
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.33634637656864846; val_accuracy: 0.9185907643312102 

The current subspace-distance is: 2.758719347184524e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.9993901231791824e-05
2.703885365917813e-05
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3304326289398655; val_accuracy: 0.9197850318471338 

The current subspace-distance is: 2.703885365917813e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.81
Batch: 600; loss: 0.2; acc: 0.98
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.130243855295703e-05
2.6731860998552293e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3213700603242892; val_accuracy: 0.9217754777070064 

The current subspace-distance is: 2.6731860998552293e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.97
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.154636503197253e-05
2.7570287784328684e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3219160970988547; val_accuracy: 0.9217754777070064 

The current subspace-distance is: 2.7570287784328684e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.98
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.198795017553493e-05
2.9067274226690643e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3137902150962763; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 2.9067274226690643e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.187114922795445e-05
2.8009206289425492e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3170619975229737; val_accuracy: 0.9220740445859873 

The current subspace-distance is: 2.8009206289425492e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.265436240937561e-05
2.8339696655166335e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3125628644398823; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.8339696655166335e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.98
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.4; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.19; acc: 0.98
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.312764890026301e-05
3.029688014066778e-05
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.30937439611383305; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 3.029688014066778e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.318878149613738e-05
2.9228554922156036e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.30892151689073843; val_accuracy: 0.9241640127388535 

The current subspace-distance is: 2.9228554922156036e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.98
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.315995415206999e-05
2.9581835406133905e-05
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.30701751285677503; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 2.9581835406133905e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.98
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

6.332255725283176e-05
3.038973045477178e-05
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.30958874087045146; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 3.038973045477178e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.97
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

6.39374993625097e-05
2.961430982395541e-05
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.30882492903501363; val_accuracy: 0.9232683121019108 

The current subspace-distance is: 2.961430982395541e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.98
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

6.357832171488553e-05
2.9252218155306764e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3012969200588336; val_accuracy: 0.9241640127388535 

The current subspace-distance is: 2.9252218155306764e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.370196206262335e-05
2.977636358991731e-05
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3039961948421351; val_accuracy: 0.9249601910828026 

The current subspace-distance is: 2.977636358991731e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.97
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

6.364092405419797e-05
2.8634372938540764e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.30335889775661906; val_accuracy: 0.9250597133757962 

The current subspace-distance is: 2.8634372938540764e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

6.353812932502478e-05
2.7346910428605042e-05
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.30718916127825996; val_accuracy: 0.9231687898089171 

The current subspace-distance is: 2.7346910428605042e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.98
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

6.472426321124658e-05
3.1476662115892395e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.30293023742877756; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 3.1476662115892395e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_11_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.835055857460475

The number of parameters is: 251109

The number of individual parameters is:

15
270
15
15
23
37605
23
23
45
112815
45
45
64
95040
64
64
4096
64
640
10
64
64

nonzero elements in E: 125554489
elements in E: 125554500
fraction nonzero: 0.9999999123886439
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.38; acc: 0.06
Batch: 20; loss: 2.01; acc: 0.3
Batch: 40; loss: 1.74; acc: 0.59
Batch: 60; loss: 1.66; acc: 0.61
Batch: 80; loss: 1.67; acc: 0.58
Batch: 100; loss: 1.4; acc: 0.72
Batch: 120; loss: 1.24; acc: 0.84
Batch: 140; loss: 1.21; acc: 0.84
Batch: 160; loss: 1.26; acc: 0.78
Batch: 180; loss: 1.33; acc: 0.73
Batch: 200; loss: 1.28; acc: 0.75
Batch: 220; loss: 1.2; acc: 0.75
Batch: 240; loss: 1.31; acc: 0.61
Batch: 260; loss: 1.32; acc: 0.69
Batch: 280; loss: 1.21; acc: 0.75
Batch: 300; loss: 0.99; acc: 0.88
Batch: 320; loss: 1.07; acc: 0.81
Batch: 340; loss: 1.11; acc: 0.8
Batch: 360; loss: 1.19; acc: 0.77
Batch: 380; loss: 0.98; acc: 0.88
Batch: 400; loss: 1.03; acc: 0.83
Batch: 420; loss: 1.0; acc: 0.86
Batch: 440; loss: 1.01; acc: 0.81
Batch: 460; loss: 0.94; acc: 0.83
Batch: 480; loss: 1.01; acc: 0.88
Batch: 500; loss: 0.87; acc: 0.88
Batch: 520; loss: 0.95; acc: 0.84
Batch: 540; loss: 0.99; acc: 0.84
Batch: 560; loss: 0.9; acc: 0.84
Batch: 580; loss: 0.82; acc: 0.86
Batch: 600; loss: 0.97; acc: 0.81
Batch: 620; loss: 0.91; acc: 0.78
Batch: 640; loss: 0.96; acc: 0.78
Batch: 660; loss: 0.82; acc: 0.89
Batch: 680; loss: 0.83; acc: 0.86
Batch: 700; loss: 0.92; acc: 0.81
Batch: 720; loss: 0.86; acc: 0.86
Batch: 740; loss: 0.85; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.91
Batch: 780; loss: 0.78; acc: 0.89
Train Epoch over. train_loss: 1.14; train_accuracy: 0.77 

2.508098987163976e-05
9.070045962289441e-06
Batch: 0; loss: 0.86; acc: 0.89
Batch: 20; loss: 0.98; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.97
Batch: 60; loss: 0.79; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.97
Batch: 100; loss: 0.74; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.92
Val Epoch over. val_loss: 0.7779202146135318; val_accuracy: 0.8769904458598726 

The current subspace-distance is: 9.070045962289441e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.86
Batch: 40; loss: 0.9; acc: 0.77
Batch: 60; loss: 0.75; acc: 0.89
Batch: 80; loss: 0.92; acc: 0.8
Batch: 100; loss: 0.83; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.92
Batch: 160; loss: 0.76; acc: 0.84
Batch: 180; loss: 0.79; acc: 0.89
Batch: 200; loss: 0.73; acc: 0.92
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.89
Batch: 260; loss: 0.77; acc: 0.83
Batch: 280; loss: 0.79; acc: 0.81
Batch: 300; loss: 0.83; acc: 0.78
Batch: 320; loss: 0.74; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.92
Batch: 360; loss: 0.66; acc: 0.89
Batch: 380; loss: 0.82; acc: 0.83
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.75; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.73; acc: 0.88
Batch: 480; loss: 0.81; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.94
Batch: 560; loss: 0.62; acc: 0.94
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.63; acc: 0.91
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.74; acc: 0.83
Batch: 700; loss: 0.59; acc: 0.95
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.92
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.89
Train Epoch over. train_loss: 0.72; train_accuracy: 0.87 

3.059930168092251e-05
1.2513074580056127e-05
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.81; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.98
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.97
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.6022702227732178; val_accuracy: 0.898984872611465 

The current subspace-distance is: 1.2513074580056127e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.98
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.77; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.95
Batch: 160; loss: 0.64; acc: 0.89
Batch: 180; loss: 0.77; acc: 0.8
Batch: 200; loss: 0.58; acc: 0.92
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.65; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.95
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.94
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.92
Batch: 760; loss: 0.54; acc: 0.95
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.6; train_accuracy: 0.89 

3.5675824619829655e-05
1.469526432629209e-05
Batch: 0; loss: 0.58; acc: 0.95
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.5207727951988294; val_accuracy: 0.9065485668789809 

The current subspace-distance is: 1.469526432629209e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.97
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.95
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.95
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.92
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.95
Batch: 600; loss: 0.64; acc: 0.8
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

3.887170896632597e-05
1.5707606507930905e-05
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.46338290146961336; val_accuracy: 0.9106289808917197 

The current subspace-distance is: 1.5707606507930905e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.65; acc: 0.8
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.95
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.94
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.95
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.9 

4.218868707539514e-05
1.7773658328223974e-05
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.97
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42030015853559893; val_accuracy: 0.9157046178343949 

The current subspace-distance is: 1.7773658328223974e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.95
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.97
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

4.5337896153796464e-05
1.982810499612242e-05
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.39035084767706074; val_accuracy: 0.9212778662420382 

The current subspace-distance is: 1.982810499612242e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.24; acc: 1.0
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.76034838357009e-05
2.0012277673231438e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3594720146268796; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 2.0012277673231438e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.059698014520109e-05
2.3611441065440886e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.34430225468744896; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.3611441065440886e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.97
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.192249591345899e-05
2.3229777070810087e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.30909283497151296; val_accuracy: 0.9304339171974523 

The current subspace-distance is: 2.3229777070810087e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.480498293763958e-05
2.51957153523108e-05
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3008721197961242; val_accuracy: 0.9316281847133758 

The current subspace-distance is: 2.51957153523108e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.5694014008622617e-05
2.4773302357061766e-05
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.2954405202132881; val_accuracy: 0.9334195859872612 

The current subspace-distance is: 2.4773302357061766e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.566475374507718e-05
2.4719751309021376e-05
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.29258084975799936; val_accuracy: 0.9324243630573248 

The current subspace-distance is: 2.4719751309021376e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.22; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.97
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.633238470181823e-05
2.474208849889692e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.2856139488945341; val_accuracy: 0.9349124203821656 

The current subspace-distance is: 2.474208849889692e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.97
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.97
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.6705532188061625e-05
2.487452184141148e-05
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.283225623617886; val_accuracy: 0.934812898089172 

The current subspace-distance is: 2.487452184141148e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.97
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.97
Batch: 180; loss: 0.32; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.7782108342507854e-05
2.556071922299452e-05
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.28480221843643555; val_accuracy: 0.9351114649681529 

The current subspace-distance is: 2.556071922299452e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.98
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.839567165821791e-05
2.6130859623663127e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.27805130492160274; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.6130859623663127e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.954823791398667e-05
2.7720447178580798e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.27303257712702844; val_accuracy: 0.9368033439490446 

The current subspace-distance is: 2.7720447178580798e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.98
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.98
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

5.8754008932737634e-05
2.5921630367520265e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.27262864328303915; val_accuracy: 0.9361066878980892 

The current subspace-distance is: 2.5921630367520265e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.84
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.12; acc: 1.0
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

5.9574347687885165e-05
2.6484363843337633e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.26654205295690303; val_accuracy: 0.9375 

The current subspace-distance is: 2.6484363843337633e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.98
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

5.996210529701784e-05
2.651494651217945e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2632295876551586; val_accuracy: 0.9371019108280255 

The current subspace-distance is: 2.651494651217945e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.98
Batch: 580; loss: 0.19; acc: 1.0
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

5.981019057799131e-05
2.6672832973417826e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2616369648344198; val_accuracy: 0.9370023885350318 

The current subspace-distance is: 2.6672832973417826e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.98
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.98
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.18; acc: 0.98
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.079790182411671e-05
2.7934802346862853e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.26769098459155694; val_accuracy: 0.9380971337579618 

The current subspace-distance is: 2.7934802346862853e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.98
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.83
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.19; acc: 1.0
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.83
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.028591451467946e-05
2.7849009711644612e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2628828634027463; val_accuracy: 0.93859474522293 

The current subspace-distance is: 2.7849009711644612e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.060105442884378e-05
2.7785230486188084e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.2651671498159694; val_accuracy: 0.9386942675159236 

The current subspace-distance is: 2.7785230486188084e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.15; acc: 1.0
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.081562241888605e-05
2.6668407372199e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2640801969512253; val_accuracy: 0.9371019108280255 

The current subspace-distance is: 2.6668407372199e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.19; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.108958041295409e-05
2.771965228021145e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.25962136771268907; val_accuracy: 0.9378980891719745 

The current subspace-distance is: 2.771965228021145e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.98
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.048875366104767e-05
2.6623683879734017e-05
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2572154434053761; val_accuracy: 0.9387937898089171 

The current subspace-distance is: 2.6623683879734017e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.98
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.98
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.130854308139533e-05
2.705535189306829e-05
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.98
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.25833743374059154; val_accuracy: 0.9380971337579618 

The current subspace-distance is: 2.705535189306829e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.98
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.183967343531549e-05
2.9139257094357163e-05
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2550753113001015; val_accuracy: 0.9386942675159236 

The current subspace-distance is: 2.9139257094357163e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.81
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.2; acc: 0.98
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.215322355274111e-05
2.8813647077186033e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.98
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2570045055097835; val_accuracy: 0.9392914012738853 

The current subspace-distance is: 2.8813647077186033e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_11_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_11_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
