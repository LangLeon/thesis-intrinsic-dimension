model : table13slim
N : 12
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:52

Channel scaling factor: 1.17

The number of parameters is: 270144

The number of individual parameters is:

10
180
10
10
15
33600
15
15
29
97440
29
29
64
133632
64
64
4096
64
640
10
64
64

nonzero elements in E: 13507198
elements in E: 13507200
fraction nonzero: 0.9999998519308221
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.54; acc: 0.0
Batch: 20; loss: 2.38; acc: 0.14
Batch: 40; loss: 2.38; acc: 0.11
Batch: 60; loss: 2.37; acc: 0.14
Batch: 80; loss: 2.33; acc: 0.2
Batch: 100; loss: 2.22; acc: 0.22
Batch: 120; loss: 2.19; acc: 0.28
Batch: 140; loss: 2.2; acc: 0.25
Batch: 160; loss: 2.21; acc: 0.22
Batch: 180; loss: 2.05; acc: 0.31
Batch: 200; loss: 2.11; acc: 0.28
Batch: 220; loss: 2.14; acc: 0.27
Batch: 240; loss: 2.08; acc: 0.27
Batch: 260; loss: 2.07; acc: 0.39
Batch: 280; loss: 2.12; acc: 0.34
Batch: 300; loss: 2.14; acc: 0.28
Batch: 320; loss: 2.04; acc: 0.27
Batch: 340; loss: 2.04; acc: 0.38
Batch: 360; loss: 2.03; acc: 0.38
Batch: 380; loss: 1.91; acc: 0.41
Batch: 400; loss: 2.21; acc: 0.22
Batch: 420; loss: 2.09; acc: 0.34
Batch: 440; loss: 2.02; acc: 0.39
Batch: 460; loss: 2.02; acc: 0.36
Batch: 480; loss: 1.99; acc: 0.39
Batch: 500; loss: 2.05; acc: 0.34
Batch: 520; loss: 1.98; acc: 0.33
Batch: 540; loss: 2.11; acc: 0.33
Batch: 560; loss: 1.91; acc: 0.39
Batch: 580; loss: 1.92; acc: 0.33
Batch: 600; loss: 2.0; acc: 0.3
Batch: 620; loss: 2.06; acc: 0.34
Batch: 640; loss: 1.9; acc: 0.42
Batch: 660; loss: 1.96; acc: 0.31
Batch: 680; loss: 1.93; acc: 0.36
Batch: 700; loss: 2.01; acc: 0.31
Batch: 720; loss: 1.92; acc: 0.42
Batch: 740; loss: 1.91; acc: 0.36
Batch: 760; loss: 1.92; acc: 0.38
Batch: 780; loss: 1.79; acc: 0.45
Train Epoch over. train_loss: 2.07; train_accuracy: 0.3 

2.3815046006347984e-05
4.789659669768298e-06
Batch: 0; loss: 1.94; acc: 0.36
Batch: 20; loss: 1.99; acc: 0.33
Batch: 40; loss: 1.77; acc: 0.47
Batch: 60; loss: 1.84; acc: 0.48
Batch: 80; loss: 1.96; acc: 0.38
Batch: 100; loss: 1.81; acc: 0.48
Batch: 120; loss: 1.92; acc: 0.36
Batch: 140; loss: 1.89; acc: 0.45
Val Epoch over. val_loss: 1.9036293363874885; val_accuracy: 0.3953025477707006 

The current subspace-distance is: 4.789659669768298e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.96; acc: 0.31
Batch: 20; loss: 1.94; acc: 0.38
Batch: 40; loss: 1.9; acc: 0.34
Batch: 60; loss: 2.04; acc: 0.27
Batch: 80; loss: 1.85; acc: 0.42
Batch: 100; loss: 1.95; acc: 0.33
Batch: 120; loss: 1.99; acc: 0.33
Batch: 140; loss: 1.85; acc: 0.44
Batch: 160; loss: 1.92; acc: 0.36
Batch: 180; loss: 1.93; acc: 0.34
Batch: 200; loss: 1.92; acc: 0.36
Batch: 220; loss: 1.93; acc: 0.36
Batch: 240; loss: 1.89; acc: 0.38
Batch: 260; loss: 1.86; acc: 0.41
Batch: 280; loss: 1.84; acc: 0.41
Batch: 300; loss: 1.86; acc: 0.39
Batch: 320; loss: 1.88; acc: 0.36
Batch: 340; loss: 1.76; acc: 0.44
Batch: 360; loss: 1.86; acc: 0.41
Batch: 380; loss: 1.93; acc: 0.23
Batch: 400; loss: 1.88; acc: 0.44
Batch: 420; loss: 1.91; acc: 0.36
Batch: 440; loss: 1.76; acc: 0.52
Batch: 460; loss: 1.73; acc: 0.53
Batch: 480; loss: 1.85; acc: 0.39
Batch: 500; loss: 1.84; acc: 0.45
Batch: 520; loss: 1.91; acc: 0.38
Batch: 540; loss: 1.77; acc: 0.42
Batch: 560; loss: 1.8; acc: 0.41
Batch: 580; loss: 1.91; acc: 0.38
Batch: 600; loss: 1.96; acc: 0.33
Batch: 620; loss: 1.73; acc: 0.5
Batch: 640; loss: 1.89; acc: 0.36
Batch: 660; loss: 1.87; acc: 0.33
Batch: 680; loss: 1.79; acc: 0.38
Batch: 700; loss: 1.74; acc: 0.44
Batch: 720; loss: 1.78; acc: 0.47
Batch: 740; loss: 1.74; acc: 0.39
Batch: 760; loss: 1.75; acc: 0.52
Batch: 780; loss: 1.76; acc: 0.48
Train Epoch over. train_loss: 1.86; train_accuracy: 0.4 

2.829490404110402e-05
8.34521051729098e-06
Batch: 0; loss: 1.83; acc: 0.39
Batch: 20; loss: 1.83; acc: 0.42
Batch: 40; loss: 1.62; acc: 0.55
Batch: 60; loss: 1.72; acc: 0.55
Batch: 80; loss: 1.82; acc: 0.41
Batch: 100; loss: 1.68; acc: 0.48
Batch: 120; loss: 1.85; acc: 0.38
Batch: 140; loss: 1.79; acc: 0.44
Val Epoch over. val_loss: 1.778824899606644; val_accuracy: 0.4419785031847134 

The current subspace-distance is: 8.34521051729098e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.48
Batch: 20; loss: 1.72; acc: 0.5
Batch: 40; loss: 1.94; acc: 0.36
Batch: 60; loss: 1.76; acc: 0.45
Batch: 80; loss: 1.69; acc: 0.5
Batch: 100; loss: 1.95; acc: 0.38
Batch: 120; loss: 1.7; acc: 0.52
Batch: 140; loss: 1.84; acc: 0.34
Batch: 160; loss: 1.88; acc: 0.38
Batch: 180; loss: 1.68; acc: 0.45
Batch: 200; loss: 1.82; acc: 0.36
Batch: 220; loss: 1.82; acc: 0.39
Batch: 240; loss: 1.78; acc: 0.42
Batch: 260; loss: 1.73; acc: 0.45
Batch: 280; loss: 1.67; acc: 0.48
Batch: 300; loss: 1.81; acc: 0.42
Batch: 320; loss: 1.86; acc: 0.44
Batch: 340; loss: 1.73; acc: 0.45
Batch: 360; loss: 1.67; acc: 0.5
Batch: 380; loss: 1.72; acc: 0.45
Batch: 400; loss: 1.75; acc: 0.41
Batch: 420; loss: 1.78; acc: 0.41
Batch: 440; loss: 1.74; acc: 0.42
Batch: 460; loss: 1.77; acc: 0.42
Batch: 480; loss: 1.56; acc: 0.55
Batch: 500; loss: 1.74; acc: 0.5
Batch: 520; loss: 1.59; acc: 0.48
Batch: 540; loss: 1.79; acc: 0.42
Batch: 560; loss: 1.86; acc: 0.31
Batch: 580; loss: 1.66; acc: 0.48
Batch: 600; loss: 1.74; acc: 0.44
Batch: 620; loss: 1.65; acc: 0.53
Batch: 640; loss: 1.75; acc: 0.48
Batch: 660; loss: 1.76; acc: 0.42
Batch: 680; loss: 1.7; acc: 0.42
Batch: 700; loss: 1.65; acc: 0.48
Batch: 720; loss: 1.67; acc: 0.5
Batch: 740; loss: 1.64; acc: 0.52
Batch: 760; loss: 1.63; acc: 0.53
Batch: 780; loss: 1.79; acc: 0.47
Train Epoch over. train_loss: 1.75; train_accuracy: 0.44 

3.122732960036956e-05
9.276322998630349e-06
Batch: 0; loss: 1.74; acc: 0.39
Batch: 20; loss: 1.79; acc: 0.38
Batch: 40; loss: 1.46; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.58
Batch: 80; loss: 1.68; acc: 0.45
Batch: 100; loss: 1.59; acc: 0.52
Batch: 120; loss: 1.81; acc: 0.38
Batch: 140; loss: 1.72; acc: 0.42
Val Epoch over. val_loss: 1.6839716882462714; val_accuracy: 0.46884952229299365 

The current subspace-distance is: 9.276322998630349e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.55
Batch: 20; loss: 1.74; acc: 0.44
Batch: 40; loss: 1.69; acc: 0.5
Batch: 60; loss: 1.66; acc: 0.52
Batch: 80; loss: 1.71; acc: 0.42
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.59; acc: 0.48
Batch: 140; loss: 1.67; acc: 0.48
Batch: 160; loss: 1.7; acc: 0.41
Batch: 180; loss: 1.71; acc: 0.45
Batch: 200; loss: 1.67; acc: 0.53
Batch: 220; loss: 1.73; acc: 0.41
Batch: 240; loss: 1.79; acc: 0.39
Batch: 260; loss: 1.66; acc: 0.52
Batch: 280; loss: 1.73; acc: 0.42
Batch: 300; loss: 1.65; acc: 0.5
Batch: 320; loss: 1.69; acc: 0.47
Batch: 340; loss: 1.71; acc: 0.44
Batch: 360; loss: 1.62; acc: 0.5
Batch: 380; loss: 1.87; acc: 0.34
Batch: 400; loss: 1.66; acc: 0.55
Batch: 420; loss: 1.69; acc: 0.5
Batch: 440; loss: 1.74; acc: 0.45
Batch: 460; loss: 1.59; acc: 0.52
Batch: 480; loss: 1.75; acc: 0.44
Batch: 500; loss: 1.64; acc: 0.47
Batch: 520; loss: 1.79; acc: 0.41
Batch: 540; loss: 1.7; acc: 0.52
Batch: 560; loss: 1.64; acc: 0.5
Batch: 580; loss: 1.71; acc: 0.45
Batch: 600; loss: 1.78; acc: 0.42
Batch: 620; loss: 1.66; acc: 0.5
Batch: 640; loss: 1.7; acc: 0.47
Batch: 660; loss: 1.6; acc: 0.53
Batch: 680; loss: 1.64; acc: 0.47
Batch: 700; loss: 1.78; acc: 0.38
Batch: 720; loss: 1.58; acc: 0.5
Batch: 740; loss: 1.65; acc: 0.48
Batch: 760; loss: 1.77; acc: 0.39
Batch: 780; loss: 1.71; acc: 0.53
Train Epoch over. train_loss: 1.68; train_accuracy: 0.47 

3.353007195983082e-05
1.1319685654598288e-05
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.4; acc: 0.61
Batch: 60; loss: 1.5; acc: 0.61
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.52; acc: 0.53
Batch: 120; loss: 1.77; acc: 0.39
Batch: 140; loss: 1.69; acc: 0.45
Val Epoch over. val_loss: 1.6328396789587227; val_accuracy: 0.48039410828025475 

The current subspace-distance is: 1.1319685654598288e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.39
Batch: 20; loss: 1.6; acc: 0.47
Batch: 40; loss: 1.52; acc: 0.59
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.56; acc: 0.53
Batch: 100; loss: 1.71; acc: 0.47
Batch: 120; loss: 1.67; acc: 0.47
Batch: 140; loss: 1.65; acc: 0.48
Batch: 160; loss: 1.76; acc: 0.39
Batch: 180; loss: 1.58; acc: 0.52
Batch: 200; loss: 1.64; acc: 0.5
Batch: 220; loss: 1.58; acc: 0.53
Batch: 240; loss: 1.68; acc: 0.42
Batch: 260; loss: 1.51; acc: 0.62
Batch: 280; loss: 1.61; acc: 0.42
Batch: 300; loss: 1.54; acc: 0.59
Batch: 320; loss: 1.75; acc: 0.41
Batch: 340; loss: 1.51; acc: 0.58
Batch: 360; loss: 1.55; acc: 0.55
Batch: 380; loss: 1.59; acc: 0.42
Batch: 400; loss: 1.6; acc: 0.48
Batch: 420; loss: 1.65; acc: 0.52
Batch: 440; loss: 1.73; acc: 0.5
Batch: 460; loss: 1.67; acc: 0.48
Batch: 480; loss: 1.54; acc: 0.5
Batch: 500; loss: 1.61; acc: 0.5
Batch: 520; loss: 1.7; acc: 0.52
Batch: 540; loss: 1.64; acc: 0.53
Batch: 560; loss: 1.65; acc: 0.45
Batch: 580; loss: 1.61; acc: 0.5
Batch: 600; loss: 1.53; acc: 0.56
Batch: 620; loss: 1.6; acc: 0.52
Batch: 640; loss: 1.59; acc: 0.5
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.55; acc: 0.55
Batch: 700; loss: 1.49; acc: 0.67
Batch: 720; loss: 1.72; acc: 0.44
Batch: 740; loss: 1.58; acc: 0.52
Batch: 760; loss: 1.57; acc: 0.52
Batch: 780; loss: 1.65; acc: 0.41
Train Epoch over. train_loss: 1.63; train_accuracy: 0.49 

3.6450281186262146e-05
1.1212851859454531e-05
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.47
Batch: 100; loss: 1.49; acc: 0.56
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.68; acc: 0.48
Val Epoch over. val_loss: 1.6033838106568452; val_accuracy: 0.482484076433121 

The current subspace-distance is: 1.1212851859454531e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.47
Batch: 20; loss: 1.54; acc: 0.55
Batch: 40; loss: 1.6; acc: 0.48
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.64; acc: 0.39
Batch: 140; loss: 1.55; acc: 0.52
Batch: 160; loss: 1.62; acc: 0.44
Batch: 180; loss: 1.53; acc: 0.62
Batch: 200; loss: 1.46; acc: 0.59
Batch: 220; loss: 1.8; acc: 0.38
Batch: 240; loss: 1.56; acc: 0.56
Batch: 260; loss: 1.63; acc: 0.52
Batch: 280; loss: 1.66; acc: 0.38
Batch: 300; loss: 1.56; acc: 0.48
Batch: 320; loss: 1.48; acc: 0.53
Batch: 340; loss: 1.65; acc: 0.48
Batch: 360; loss: 1.62; acc: 0.44
Batch: 380; loss: 1.55; acc: 0.52
Batch: 400; loss: 1.69; acc: 0.42
Batch: 420; loss: 1.45; acc: 0.59
Batch: 440; loss: 1.62; acc: 0.34
Batch: 460; loss: 1.63; acc: 0.45
Batch: 480; loss: 1.54; acc: 0.52
Batch: 500; loss: 1.65; acc: 0.42
Batch: 520; loss: 1.61; acc: 0.42
Batch: 540; loss: 1.62; acc: 0.41
Batch: 560; loss: 1.52; acc: 0.55
Batch: 580; loss: 1.59; acc: 0.52
Batch: 600; loss: 1.54; acc: 0.48
Batch: 620; loss: 1.57; acc: 0.5
Batch: 640; loss: 1.66; acc: 0.39
Batch: 660; loss: 1.64; acc: 0.45
Batch: 680; loss: 1.52; acc: 0.55
Batch: 700; loss: 1.72; acc: 0.39
Batch: 720; loss: 1.65; acc: 0.42
Batch: 740; loss: 1.64; acc: 0.45
Batch: 760; loss: 1.61; acc: 0.47
Batch: 780; loss: 1.53; acc: 0.47
Train Epoch over. train_loss: 1.59; train_accuracy: 0.49 

3.967099473811686e-05
1.4844226825516671e-05
Batch: 0; loss: 1.64; acc: 0.44
Batch: 20; loss: 1.78; acc: 0.42
Batch: 40; loss: 1.3; acc: 0.62
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.44; acc: 0.58
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.67; acc: 0.47
Val Epoch over. val_loss: 1.5599863992375174; val_accuracy: 0.4903463375796178 

The current subspace-distance is: 1.4844226825516671e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.47
Batch: 20; loss: 1.56; acc: 0.56
Batch: 40; loss: 1.53; acc: 0.47
Batch: 60; loss: 1.74; acc: 0.36
Batch: 80; loss: 1.63; acc: 0.5
Batch: 100; loss: 1.52; acc: 0.53
Batch: 120; loss: 1.66; acc: 0.44
Batch: 140; loss: 1.53; acc: 0.5
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.63; acc: 0.38
Batch: 200; loss: 1.63; acc: 0.45
Batch: 220; loss: 1.58; acc: 0.53
Batch: 240; loss: 1.6; acc: 0.52
Batch: 260; loss: 1.42; acc: 0.62
Batch: 280; loss: 1.49; acc: 0.55
Batch: 300; loss: 1.5; acc: 0.66
Batch: 320; loss: 1.43; acc: 0.58
Batch: 340; loss: 1.71; acc: 0.45
Batch: 360; loss: 1.59; acc: 0.41
Batch: 380; loss: 1.57; acc: 0.45
Batch: 400; loss: 1.53; acc: 0.45
Batch: 420; loss: 1.58; acc: 0.5
Batch: 440; loss: 1.74; acc: 0.34
Batch: 460; loss: 1.46; acc: 0.61
Batch: 480; loss: 1.48; acc: 0.52
Batch: 500; loss: 1.61; acc: 0.48
Batch: 520; loss: 1.79; acc: 0.33
Batch: 540; loss: 1.59; acc: 0.53
Batch: 560; loss: 1.39; acc: 0.66
Batch: 580; loss: 1.62; acc: 0.45
Batch: 600; loss: 1.51; acc: 0.44
Batch: 620; loss: 1.56; acc: 0.5
Batch: 640; loss: 1.54; acc: 0.5
Batch: 660; loss: 1.62; acc: 0.5
Batch: 680; loss: 1.44; acc: 0.48
Batch: 700; loss: 1.59; acc: 0.53
Batch: 720; loss: 1.64; acc: 0.38
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.49; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.52
Train Epoch over. train_loss: 1.57; train_accuracy: 0.49 

4.126290514250286e-05
1.475187309551984e-05
Batch: 0; loss: 1.63; acc: 0.44
Batch: 20; loss: 1.76; acc: 0.44
Batch: 40; loss: 1.27; acc: 0.66
Batch: 60; loss: 1.39; acc: 0.58
Batch: 80; loss: 1.43; acc: 0.53
Batch: 100; loss: 1.43; acc: 0.59
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.66; acc: 0.47
Val Epoch over. val_loss: 1.5486721726739483; val_accuracy: 0.48706210191082805 

The current subspace-distance is: 1.475187309551984e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.59; acc: 0.44
Batch: 40; loss: 1.51; acc: 0.53
Batch: 60; loss: 1.69; acc: 0.42
Batch: 80; loss: 1.47; acc: 0.53
Batch: 100; loss: 1.47; acc: 0.5
Batch: 120; loss: 1.47; acc: 0.47
Batch: 140; loss: 1.5; acc: 0.5
Batch: 160; loss: 1.55; acc: 0.5
Batch: 180; loss: 1.51; acc: 0.48
Batch: 200; loss: 1.59; acc: 0.45
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.61; acc: 0.44
Batch: 260; loss: 1.49; acc: 0.45
Batch: 280; loss: 1.41; acc: 0.55
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 1.48; acc: 0.53
Batch: 340; loss: 1.46; acc: 0.53
Batch: 360; loss: 1.8; acc: 0.34
Batch: 380; loss: 1.62; acc: 0.48
Batch: 400; loss: 1.49; acc: 0.55
Batch: 420; loss: 1.54; acc: 0.5
Batch: 440; loss: 1.45; acc: 0.53
Batch: 460; loss: 1.54; acc: 0.52
Batch: 480; loss: 1.57; acc: 0.47
Batch: 500; loss: 1.55; acc: 0.45
Batch: 520; loss: 1.73; acc: 0.42
Batch: 540; loss: 1.55; acc: 0.45
Batch: 560; loss: 1.58; acc: 0.45
Batch: 580; loss: 1.47; acc: 0.55
Batch: 600; loss: 1.56; acc: 0.47
Batch: 620; loss: 1.8; acc: 0.41
Batch: 640; loss: 1.59; acc: 0.5
Batch: 660; loss: 1.58; acc: 0.47
Batch: 680; loss: 1.4; acc: 0.58
Batch: 700; loss: 1.5; acc: 0.5
Batch: 720; loss: 1.65; acc: 0.44
Batch: 740; loss: 1.58; acc: 0.41
Batch: 760; loss: 1.54; acc: 0.47
Batch: 780; loss: 1.51; acc: 0.58
Train Epoch over. train_loss: 1.55; train_accuracy: 0.49 

4.243045987095684e-05
1.4220914636098314e-05
Batch: 0; loss: 1.6; acc: 0.45
Batch: 20; loss: 1.78; acc: 0.41
Batch: 40; loss: 1.27; acc: 0.66
Batch: 60; loss: 1.38; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.55
Batch: 100; loss: 1.43; acc: 0.55
Batch: 120; loss: 1.76; acc: 0.38
Batch: 140; loss: 1.65; acc: 0.42
Val Epoch over. val_loss: 1.5376993189951418; val_accuracy: 0.48477308917197454 

The current subspace-distance is: 1.4220914636098314e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.42
Batch: 20; loss: 1.49; acc: 0.56
Batch: 40; loss: 1.53; acc: 0.45
Batch: 60; loss: 1.51; acc: 0.58
Batch: 80; loss: 1.59; acc: 0.42
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 1.69; acc: 0.39
Batch: 160; loss: 1.66; acc: 0.44
Batch: 180; loss: 1.52; acc: 0.52
Batch: 200; loss: 1.4; acc: 0.55
Batch: 220; loss: 1.72; acc: 0.45
Batch: 240; loss: 1.49; acc: 0.47
Batch: 260; loss: 1.44; acc: 0.56
Batch: 280; loss: 1.5; acc: 0.47
Batch: 300; loss: 1.57; acc: 0.39
Batch: 320; loss: 1.45; acc: 0.5
Batch: 340; loss: 1.67; acc: 0.44
Batch: 360; loss: 1.46; acc: 0.52
Batch: 380; loss: 1.52; acc: 0.48
Batch: 400; loss: 1.57; acc: 0.42
Batch: 420; loss: 1.54; acc: 0.48
Batch: 440; loss: 1.6; acc: 0.5
Batch: 460; loss: 1.45; acc: 0.56
Batch: 480; loss: 1.63; acc: 0.47
Batch: 500; loss: 1.42; acc: 0.58
Batch: 520; loss: 1.63; acc: 0.42
Batch: 540; loss: 1.53; acc: 0.47
Batch: 560; loss: 1.55; acc: 0.53
Batch: 580; loss: 1.47; acc: 0.52
Batch: 600; loss: 1.61; acc: 0.44
Batch: 620; loss: 1.58; acc: 0.39
Batch: 640; loss: 1.57; acc: 0.41
Batch: 660; loss: 1.45; acc: 0.48
Batch: 680; loss: 1.43; acc: 0.55
Batch: 700; loss: 1.55; acc: 0.42
Batch: 720; loss: 1.59; acc: 0.45
Batch: 740; loss: 1.41; acc: 0.55
Batch: 760; loss: 1.51; acc: 0.48
Batch: 780; loss: 1.3; acc: 0.69
Train Epoch over. train_loss: 1.54; train_accuracy: 0.49 

4.3374831875553355e-05
1.3942489204055164e-05
Batch: 0; loss: 1.58; acc: 0.47
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.24; acc: 0.7
Batch: 60; loss: 1.37; acc: 0.56
Batch: 80; loss: 1.38; acc: 0.52
Batch: 100; loss: 1.39; acc: 0.58
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.61; acc: 0.45
Val Epoch over. val_loss: 1.5174163959588214; val_accuracy: 0.488953025477707 

The current subspace-distance is: 1.3942489204055164e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.48
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 1.45; acc: 0.55
Batch: 60; loss: 1.66; acc: 0.5
Batch: 80; loss: 1.49; acc: 0.48
Batch: 100; loss: 1.6; acc: 0.42
Batch: 120; loss: 1.74; acc: 0.38
Batch: 140; loss: 1.51; acc: 0.42
Batch: 160; loss: 1.32; acc: 0.64
Batch: 180; loss: 1.65; acc: 0.45
Batch: 200; loss: 1.49; acc: 0.55
Batch: 220; loss: 1.58; acc: 0.52
Batch: 240; loss: 1.59; acc: 0.45
Batch: 260; loss: 1.42; acc: 0.59
Batch: 280; loss: 1.55; acc: 0.45
Batch: 300; loss: 1.61; acc: 0.42
Batch: 320; loss: 1.45; acc: 0.48
Batch: 340; loss: 1.66; acc: 0.41
Batch: 360; loss: 1.58; acc: 0.45
Batch: 380; loss: 1.76; acc: 0.33
Batch: 400; loss: 1.52; acc: 0.5
Batch: 420; loss: 1.6; acc: 0.42
Batch: 440; loss: 1.53; acc: 0.45
Batch: 460; loss: 1.48; acc: 0.47
Batch: 480; loss: 1.56; acc: 0.52
Batch: 500; loss: 1.49; acc: 0.48
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.45; acc: 0.56
Batch: 560; loss: 1.61; acc: 0.41
Batch: 580; loss: 1.47; acc: 0.48
Batch: 600; loss: 1.38; acc: 0.58
Batch: 620; loss: 1.56; acc: 0.5
Batch: 640; loss: 1.57; acc: 0.5
Batch: 660; loss: 1.5; acc: 0.44
Batch: 680; loss: 1.47; acc: 0.52
Batch: 700; loss: 1.56; acc: 0.45
Batch: 720; loss: 1.46; acc: 0.52
Batch: 740; loss: 1.46; acc: 0.53
Batch: 760; loss: 1.4; acc: 0.5
Batch: 780; loss: 1.67; acc: 0.39
Train Epoch over. train_loss: 1.53; train_accuracy: 0.5 

4.524813266471028e-05
1.3910897905589081e-05
Batch: 0; loss: 1.56; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.41
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 1.36; acc: 0.52
Batch: 80; loss: 1.36; acc: 0.58
Batch: 100; loss: 1.37; acc: 0.62
Batch: 120; loss: 1.73; acc: 0.41
Batch: 140; loss: 1.59; acc: 0.48
Val Epoch over. val_loss: 1.5054833676405013; val_accuracy: 0.4895501592356688 

The current subspace-distance is: 1.3910897905589081e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.48
Batch: 20; loss: 1.39; acc: 0.55
Batch: 40; loss: 1.53; acc: 0.48
Batch: 60; loss: 1.48; acc: 0.53
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.47; acc: 0.55
Batch: 140; loss: 1.42; acc: 0.52
Batch: 160; loss: 1.5; acc: 0.55
Batch: 180; loss: 1.39; acc: 0.58
Batch: 200; loss: 1.5; acc: 0.53
Batch: 220; loss: 1.57; acc: 0.52
Batch: 240; loss: 1.58; acc: 0.44
Batch: 260; loss: 1.52; acc: 0.58
Batch: 280; loss: 1.65; acc: 0.42
Batch: 300; loss: 1.59; acc: 0.42
Batch: 320; loss: 1.74; acc: 0.36
Batch: 340; loss: 1.63; acc: 0.45
Batch: 360; loss: 1.5; acc: 0.55
Batch: 380; loss: 1.52; acc: 0.53
Batch: 400; loss: 1.54; acc: 0.41
Batch: 420; loss: 1.76; acc: 0.41
Batch: 440; loss: 1.42; acc: 0.59
Batch: 460; loss: 1.4; acc: 0.59
Batch: 480; loss: 1.49; acc: 0.45
Batch: 500; loss: 1.43; acc: 0.53
Batch: 520; loss: 1.42; acc: 0.47
Batch: 540; loss: 1.56; acc: 0.48
Batch: 560; loss: 1.7; acc: 0.48
Batch: 580; loss: 1.58; acc: 0.44
Batch: 600; loss: 1.44; acc: 0.53
Batch: 620; loss: 1.46; acc: 0.53
Batch: 640; loss: 1.44; acc: 0.5
Batch: 660; loss: 1.53; acc: 0.45
Batch: 680; loss: 1.67; acc: 0.42
Batch: 700; loss: 1.51; acc: 0.55
Batch: 720; loss: 1.43; acc: 0.59
Batch: 740; loss: 1.59; acc: 0.47
Batch: 760; loss: 1.4; acc: 0.52
Batch: 780; loss: 1.51; acc: 0.42
Train Epoch over. train_loss: 1.52; train_accuracy: 0.5 

4.6234403271228075e-05
1.2205196981085464e-05
Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.22; acc: 0.69
Batch: 60; loss: 1.36; acc: 0.53
Batch: 80; loss: 1.36; acc: 0.55
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.72; acc: 0.42
Batch: 140; loss: 1.58; acc: 0.47
Val Epoch over. val_loss: 1.4964994328796484; val_accuracy: 0.49392914012738853 

The current subspace-distance is: 1.2205196981085464e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.47
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.43; acc: 0.47
Batch: 80; loss: 1.48; acc: 0.55
Batch: 100; loss: 1.56; acc: 0.52
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 1.51; acc: 0.56
Batch: 160; loss: 1.53; acc: 0.5
Batch: 180; loss: 1.54; acc: 0.47
Batch: 200; loss: 1.52; acc: 0.5
Batch: 220; loss: 1.62; acc: 0.47
Batch: 240; loss: 1.41; acc: 0.55
Batch: 260; loss: 1.49; acc: 0.5
Batch: 280; loss: 1.38; acc: 0.64
Batch: 300; loss: 1.48; acc: 0.55
Batch: 320; loss: 1.62; acc: 0.52
Batch: 340; loss: 1.7; acc: 0.42
Batch: 360; loss: 1.6; acc: 0.44
Batch: 380; loss: 1.51; acc: 0.47
Batch: 400; loss: 1.51; acc: 0.45
Batch: 420; loss: 1.52; acc: 0.52
Batch: 440; loss: 1.46; acc: 0.56
Batch: 460; loss: 1.38; acc: 0.59
Batch: 480; loss: 1.5; acc: 0.48
Batch: 500; loss: 1.53; acc: 0.52
Batch: 520; loss: 1.51; acc: 0.5
Batch: 540; loss: 1.66; acc: 0.42
Batch: 560; loss: 1.48; acc: 0.53
Batch: 580; loss: 1.4; acc: 0.58
Batch: 600; loss: 1.57; acc: 0.48
Batch: 620; loss: 1.49; acc: 0.52
Batch: 640; loss: 1.48; acc: 0.42
Batch: 660; loss: 1.58; acc: 0.38
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.54; acc: 0.47
Batch: 720; loss: 1.45; acc: 0.53
Batch: 740; loss: 1.42; acc: 0.59
Batch: 760; loss: 1.38; acc: 0.53
Batch: 780; loss: 1.52; acc: 0.45
Train Epoch over. train_loss: 1.51; train_accuracy: 0.5 

4.6964003558969125e-05
1.531971611257177e-05
Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.76; acc: 0.42
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.36; acc: 0.52
Batch: 80; loss: 1.34; acc: 0.56
Batch: 100; loss: 1.37; acc: 0.59
Batch: 120; loss: 1.71; acc: 0.41
Batch: 140; loss: 1.57; acc: 0.52
Val Epoch over. val_loss: 1.4979349799976227; val_accuracy: 0.4928343949044586 

The current subspace-distance is: 1.531971611257177e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.5
Batch: 20; loss: 1.52; acc: 0.53
Batch: 40; loss: 1.7; acc: 0.36
Batch: 60; loss: 1.48; acc: 0.53
Batch: 80; loss: 1.41; acc: 0.59
Batch: 100; loss: 1.5; acc: 0.48
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 1.67; acc: 0.36
Batch: 160; loss: 1.4; acc: 0.59
Batch: 180; loss: 1.49; acc: 0.53
Batch: 200; loss: 1.49; acc: 0.53
Batch: 220; loss: 1.57; acc: 0.44
Batch: 240; loss: 1.44; acc: 0.55
Batch: 260; loss: 1.36; acc: 0.56
Batch: 280; loss: 1.51; acc: 0.59
Batch: 300; loss: 1.41; acc: 0.58
Batch: 320; loss: 1.4; acc: 0.62
Batch: 340; loss: 1.59; acc: 0.44
Batch: 360; loss: 1.4; acc: 0.45
Batch: 380; loss: 1.38; acc: 0.66
Batch: 400; loss: 1.46; acc: 0.45
Batch: 420; loss: 1.48; acc: 0.52
Batch: 440; loss: 1.54; acc: 0.47
Batch: 460; loss: 1.52; acc: 0.5
Batch: 480; loss: 1.55; acc: 0.48
Batch: 500; loss: 1.3; acc: 0.69
Batch: 520; loss: 1.64; acc: 0.44
Batch: 540; loss: 1.54; acc: 0.52
Batch: 560; loss: 1.52; acc: 0.42
Batch: 580; loss: 1.43; acc: 0.58
Batch: 600; loss: 1.62; acc: 0.52
Batch: 620; loss: 1.6; acc: 0.45
Batch: 640; loss: 1.57; acc: 0.47
Batch: 660; loss: 1.67; acc: 0.44
Batch: 680; loss: 1.61; acc: 0.44
Batch: 700; loss: 1.51; acc: 0.39
Batch: 720; loss: 1.6; acc: 0.39
Batch: 740; loss: 1.62; acc: 0.47
Batch: 760; loss: 1.38; acc: 0.61
Batch: 780; loss: 1.61; acc: 0.41
Train Epoch over. train_loss: 1.51; train_accuracy: 0.5 

4.7475634346483275e-05
1.4677529179607518e-05
Batch: 0; loss: 1.52; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.35; acc: 0.52
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.34; acc: 0.62
Batch: 120; loss: 1.71; acc: 0.41
Batch: 140; loss: 1.56; acc: 0.52
Val Epoch over. val_loss: 1.4877590586425393; val_accuracy: 0.49980095541401276 

The current subspace-distance is: 1.4677529179607518e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.48; acc: 0.5
Batch: 20; loss: 1.35; acc: 0.59
Batch: 40; loss: 1.57; acc: 0.44
Batch: 60; loss: 1.62; acc: 0.45
Batch: 80; loss: 1.59; acc: 0.44
Batch: 100; loss: 1.55; acc: 0.45
Batch: 120; loss: 1.41; acc: 0.53
Batch: 140; loss: 1.57; acc: 0.5
Batch: 160; loss: 1.56; acc: 0.48
Batch: 180; loss: 1.49; acc: 0.45
Batch: 200; loss: 1.52; acc: 0.53
Batch: 220; loss: 1.46; acc: 0.53
Batch: 240; loss: 1.44; acc: 0.53
Batch: 260; loss: 1.58; acc: 0.47
Batch: 280; loss: 1.58; acc: 0.47
Batch: 300; loss: 1.44; acc: 0.53
Batch: 320; loss: 1.72; acc: 0.41
Batch: 340; loss: 1.47; acc: 0.47
Batch: 360; loss: 1.54; acc: 0.44
Batch: 380; loss: 1.4; acc: 0.55
Batch: 400; loss: 1.42; acc: 0.53
Batch: 420; loss: 1.4; acc: 0.58
Batch: 440; loss: 1.61; acc: 0.47
Batch: 460; loss: 1.55; acc: 0.44
Batch: 480; loss: 1.65; acc: 0.41
Batch: 500; loss: 1.63; acc: 0.42
Batch: 520; loss: 1.51; acc: 0.5
Batch: 540; loss: 1.62; acc: 0.39
Batch: 560; loss: 1.34; acc: 0.59
Batch: 580; loss: 1.45; acc: 0.5
Batch: 600; loss: 1.41; acc: 0.52
Batch: 620; loss: 1.55; acc: 0.5
Batch: 640; loss: 1.48; acc: 0.52
Batch: 660; loss: 1.43; acc: 0.58
Batch: 680; loss: 1.63; acc: 0.5
Batch: 700; loss: 1.47; acc: 0.5
Batch: 720; loss: 1.41; acc: 0.52
Batch: 740; loss: 1.45; acc: 0.48
Batch: 760; loss: 1.41; acc: 0.61
Batch: 780; loss: 1.7; acc: 0.42
Train Epoch over. train_loss: 1.51; train_accuracy: 0.5 

4.74820080853533e-05
1.3609107554657385e-05
Batch: 0; loss: 1.54; acc: 0.48
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.21; acc: 0.67
Batch: 60; loss: 1.36; acc: 0.5
Batch: 80; loss: 1.36; acc: 0.52
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.71; acc: 0.41
Batch: 140; loss: 1.56; acc: 0.52
Val Epoch over. val_loss: 1.4897391978342822; val_accuracy: 0.49562101910828027 

The current subspace-distance is: 1.3609107554657385e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.5
Batch: 20; loss: 1.47; acc: 0.56
Batch: 40; loss: 1.36; acc: 0.59
Batch: 60; loss: 1.53; acc: 0.53
Batch: 80; loss: 1.6; acc: 0.42
Batch: 100; loss: 1.5; acc: 0.47
Batch: 120; loss: 1.4; acc: 0.52
Batch: 140; loss: 1.59; acc: 0.38
Batch: 160; loss: 1.45; acc: 0.48
Batch: 180; loss: 1.75; acc: 0.39
Batch: 200; loss: 1.43; acc: 0.55
Batch: 220; loss: 1.35; acc: 0.53
Batch: 240; loss: 1.59; acc: 0.47
Batch: 260; loss: 1.44; acc: 0.55
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.35; acc: 0.59
Batch: 320; loss: 1.41; acc: 0.64
Batch: 340; loss: 1.39; acc: 0.53
Batch: 360; loss: 1.49; acc: 0.53
Batch: 380; loss: 1.5; acc: 0.45
Batch: 400; loss: 1.43; acc: 0.53
Batch: 420; loss: 1.43; acc: 0.53
Batch: 440; loss: 1.52; acc: 0.48
Batch: 460; loss: 1.6; acc: 0.47
Batch: 480; loss: 1.64; acc: 0.44
Batch: 500; loss: 1.47; acc: 0.55
Batch: 520; loss: 1.47; acc: 0.56
Batch: 540; loss: 1.43; acc: 0.53
Batch: 560; loss: 1.64; acc: 0.44
Batch: 580; loss: 1.49; acc: 0.48
Batch: 600; loss: 1.49; acc: 0.55
Batch: 620; loss: 1.52; acc: 0.48
Batch: 640; loss: 1.56; acc: 0.44
Batch: 660; loss: 1.69; acc: 0.44
Batch: 680; loss: 1.57; acc: 0.53
Batch: 700; loss: 1.43; acc: 0.56
Batch: 720; loss: 1.5; acc: 0.56
Batch: 740; loss: 1.42; acc: 0.48
Batch: 760; loss: 1.59; acc: 0.38
Batch: 780; loss: 1.51; acc: 0.52
Train Epoch over. train_loss: 1.5; train_accuracy: 0.5 

4.816154614672996e-05
1.5853756849537604e-05
Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.78; acc: 0.42
Batch: 40; loss: 1.22; acc: 0.72
Batch: 60; loss: 1.36; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.56
Batch: 100; loss: 1.35; acc: 0.59
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.56; acc: 0.48
Val Epoch over. val_loss: 1.4887052652942148; val_accuracy: 0.4961186305732484 

The current subspace-distance is: 1.5853756849537604e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.49; acc: 0.53
Batch: 20; loss: 1.57; acc: 0.42
Batch: 40; loss: 1.46; acc: 0.48
Batch: 60; loss: 1.68; acc: 0.39
Batch: 80; loss: 1.46; acc: 0.53
Batch: 100; loss: 1.5; acc: 0.56
Batch: 120; loss: 1.4; acc: 0.56
Batch: 140; loss: 1.48; acc: 0.5
Batch: 160; loss: 1.51; acc: 0.52
Batch: 180; loss: 1.43; acc: 0.53
Batch: 200; loss: 1.42; acc: 0.62
Batch: 220; loss: 1.57; acc: 0.45
Batch: 240; loss: 1.35; acc: 0.59
Batch: 260; loss: 1.35; acc: 0.55
Batch: 280; loss: 1.47; acc: 0.48
Batch: 300; loss: 1.51; acc: 0.52
Batch: 320; loss: 1.49; acc: 0.52
Batch: 340; loss: 1.67; acc: 0.41
Batch: 360; loss: 1.64; acc: 0.41
Batch: 380; loss: 1.35; acc: 0.55
Batch: 400; loss: 1.48; acc: 0.5
Batch: 420; loss: 1.52; acc: 0.45
Batch: 440; loss: 1.44; acc: 0.5
Batch: 460; loss: 1.54; acc: 0.47
Batch: 480; loss: 1.63; acc: 0.52
Batch: 500; loss: 1.61; acc: 0.47
Batch: 520; loss: 1.57; acc: 0.5
Batch: 540; loss: 1.44; acc: 0.58
Batch: 560; loss: 1.47; acc: 0.48
Batch: 580; loss: 1.53; acc: 0.55
Batch: 600; loss: 1.57; acc: 0.41
Batch: 620; loss: 1.57; acc: 0.44
Batch: 640; loss: 1.6; acc: 0.39
Batch: 660; loss: 1.53; acc: 0.58
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.58; acc: 0.42
Batch: 720; loss: 1.67; acc: 0.38
Batch: 740; loss: 1.57; acc: 0.47
Batch: 760; loss: 1.5; acc: 0.5
Batch: 780; loss: 1.61; acc: 0.36
Train Epoch over. train_loss: 1.5; train_accuracy: 0.5 

4.826525764656253e-05
1.5896896002232097e-05
Batch: 0; loss: 1.52; acc: 0.52
Batch: 20; loss: 1.79; acc: 0.41
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.36; acc: 0.5
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.35; acc: 0.61
Batch: 120; loss: 1.71; acc: 0.41
Batch: 140; loss: 1.54; acc: 0.53
Val Epoch over. val_loss: 1.48472207594829; val_accuracy: 0.49701433121019106 

The current subspace-distance is: 1.5896896002232097e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.44
Batch: 20; loss: 1.48; acc: 0.5
Batch: 40; loss: 1.5; acc: 0.47
Batch: 60; loss: 1.51; acc: 0.48
Batch: 80; loss: 1.64; acc: 0.48
Batch: 100; loss: 1.51; acc: 0.52
Batch: 120; loss: 1.54; acc: 0.45
Batch: 140; loss: 1.55; acc: 0.47
Batch: 160; loss: 1.57; acc: 0.42
Batch: 180; loss: 1.5; acc: 0.47
Batch: 200; loss: 1.51; acc: 0.48
Batch: 220; loss: 1.45; acc: 0.5
Batch: 240; loss: 1.5; acc: 0.5
Batch: 260; loss: 1.53; acc: 0.53
Batch: 280; loss: 1.58; acc: 0.47
Batch: 300; loss: 1.54; acc: 0.42
Batch: 320; loss: 1.49; acc: 0.5
Batch: 340; loss: 1.44; acc: 0.55
Batch: 360; loss: 1.41; acc: 0.56
Batch: 380; loss: 1.63; acc: 0.47
Batch: 400; loss: 1.5; acc: 0.52
Batch: 420; loss: 1.49; acc: 0.53
Batch: 440; loss: 1.47; acc: 0.58
Batch: 460; loss: 1.56; acc: 0.42
Batch: 480; loss: 1.37; acc: 0.56
Batch: 500; loss: 1.59; acc: 0.45
Batch: 520; loss: 1.44; acc: 0.53
Batch: 540; loss: 1.48; acc: 0.44
Batch: 560; loss: 1.41; acc: 0.55
Batch: 580; loss: 1.41; acc: 0.52
Batch: 600; loss: 1.51; acc: 0.55
Batch: 620; loss: 1.37; acc: 0.48
Batch: 640; loss: 1.56; acc: 0.45
Batch: 660; loss: 1.45; acc: 0.59
Batch: 680; loss: 1.51; acc: 0.48
Batch: 700; loss: 1.45; acc: 0.52
Batch: 720; loss: 1.54; acc: 0.52
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.44; acc: 0.56
Batch: 780; loss: 1.59; acc: 0.47
Train Epoch over. train_loss: 1.5; train_accuracy: 0.5 

5.021961987949908e-05
1.9161912859999575e-05
Batch: 0; loss: 1.51; acc: 0.53
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.35; acc: 0.53
Batch: 80; loss: 1.34; acc: 0.56
Batch: 100; loss: 1.33; acc: 0.64
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.53; acc: 0.52
Val Epoch over. val_loss: 1.4754564329317421; val_accuracy: 0.5016918789808917 

The current subspace-distance is: 1.9161912859999575e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.42
Batch: 20; loss: 1.59; acc: 0.48
Batch: 40; loss: 1.54; acc: 0.5
Batch: 60; loss: 1.46; acc: 0.48
Batch: 80; loss: 1.4; acc: 0.55
Batch: 100; loss: 1.48; acc: 0.56
Batch: 120; loss: 1.49; acc: 0.45
Batch: 140; loss: 1.43; acc: 0.47
Batch: 160; loss: 1.32; acc: 0.59
Batch: 180; loss: 1.55; acc: 0.48
Batch: 200; loss: 1.41; acc: 0.59
Batch: 220; loss: 1.47; acc: 0.56
Batch: 240; loss: 1.52; acc: 0.53
Batch: 260; loss: 1.42; acc: 0.56
Batch: 280; loss: 1.56; acc: 0.5
Batch: 300; loss: 1.77; acc: 0.45
Batch: 320; loss: 1.52; acc: 0.52
Batch: 340; loss: 1.57; acc: 0.52
Batch: 360; loss: 1.58; acc: 0.5
Batch: 380; loss: 1.6; acc: 0.47
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.49; acc: 0.5
Batch: 440; loss: 1.23; acc: 0.66
Batch: 460; loss: 1.32; acc: 0.58
Batch: 480; loss: 1.46; acc: 0.53
Batch: 500; loss: 1.46; acc: 0.48
Batch: 520; loss: 1.47; acc: 0.48
Batch: 540; loss: 1.56; acc: 0.58
Batch: 560; loss: 1.41; acc: 0.48
Batch: 580; loss: 1.53; acc: 0.5
Batch: 600; loss: 1.54; acc: 0.5
Batch: 620; loss: 1.5; acc: 0.42
Batch: 640; loss: 1.41; acc: 0.58
Batch: 660; loss: 1.32; acc: 0.59
Batch: 680; loss: 1.41; acc: 0.45
Batch: 700; loss: 1.63; acc: 0.42
Batch: 720; loss: 1.37; acc: 0.58
Batch: 740; loss: 1.4; acc: 0.55
Batch: 760; loss: 1.67; acc: 0.36
Batch: 780; loss: 1.33; acc: 0.58
Train Epoch over. train_loss: 1.5; train_accuracy: 0.51 

4.911983705824241e-05
1.4931720215827227e-05
Batch: 0; loss: 1.51; acc: 0.52
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.35; acc: 0.52
Batch: 80; loss: 1.34; acc: 0.55
Batch: 100; loss: 1.34; acc: 0.66
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.51; acc: 0.52
Val Epoch over. val_loss: 1.4727740333338453; val_accuracy: 0.5006966560509554 

The current subspace-distance is: 1.4931720215827227e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.52; acc: 0.45
Batch: 20; loss: 1.52; acc: 0.47
Batch: 40; loss: 1.45; acc: 0.45
Batch: 60; loss: 1.37; acc: 0.58
Batch: 80; loss: 1.55; acc: 0.45
Batch: 100; loss: 1.55; acc: 0.45
Batch: 120; loss: 1.44; acc: 0.5
Batch: 140; loss: 1.38; acc: 0.62
Batch: 160; loss: 1.41; acc: 0.48
Batch: 180; loss: 1.42; acc: 0.52
Batch: 200; loss: 1.41; acc: 0.62
Batch: 220; loss: 1.53; acc: 0.47
Batch: 240; loss: 1.28; acc: 0.66
Batch: 260; loss: 1.49; acc: 0.58
Batch: 280; loss: 1.46; acc: 0.52
Batch: 300; loss: 1.48; acc: 0.56
Batch: 320; loss: 1.54; acc: 0.52
Batch: 340; loss: 1.37; acc: 0.53
Batch: 360; loss: 1.6; acc: 0.48
Batch: 380; loss: 1.37; acc: 0.48
Batch: 400; loss: 1.46; acc: 0.41
Batch: 420; loss: 1.47; acc: 0.58
Batch: 440; loss: 1.55; acc: 0.48
Batch: 460; loss: 1.49; acc: 0.47
Batch: 480; loss: 1.71; acc: 0.38
Batch: 500; loss: 1.48; acc: 0.5
Batch: 520; loss: 1.48; acc: 0.47
Batch: 540; loss: 1.55; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.47
Batch: 580; loss: 1.46; acc: 0.48
Batch: 600; loss: 1.51; acc: 0.56
Batch: 620; loss: 1.47; acc: 0.55
Batch: 640; loss: 1.57; acc: 0.41
Batch: 660; loss: 1.3; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.53
Batch: 700; loss: 1.49; acc: 0.5
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.42; acc: 0.56
Batch: 760; loss: 1.48; acc: 0.45
Batch: 780; loss: 1.64; acc: 0.38
Train Epoch over. train_loss: 1.49; train_accuracy: 0.51 

4.9731032049749047e-05
1.6977592167677358e-05
Batch: 0; loss: 1.5; acc: 0.48
Batch: 20; loss: 1.78; acc: 0.42
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.34; acc: 0.53
Batch: 80; loss: 1.35; acc: 0.53
Batch: 100; loss: 1.33; acc: 0.67
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.52; acc: 0.52
Val Epoch over. val_loss: 1.469463666533209; val_accuracy: 0.504578025477707 

The current subspace-distance is: 1.6977592167677358e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.48
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.55; acc: 0.48
Batch: 60; loss: 1.49; acc: 0.47
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.42; acc: 0.53
Batch: 140; loss: 1.29; acc: 0.59
Batch: 160; loss: 1.46; acc: 0.5
Batch: 180; loss: 1.58; acc: 0.5
Batch: 200; loss: 1.38; acc: 0.52
Batch: 220; loss: 1.43; acc: 0.5
Batch: 240; loss: 1.5; acc: 0.52
Batch: 260; loss: 1.52; acc: 0.45
Batch: 280; loss: 1.39; acc: 0.58
Batch: 300; loss: 1.63; acc: 0.48
Batch: 320; loss: 1.49; acc: 0.52
Batch: 340; loss: 1.45; acc: 0.58
Batch: 360; loss: 1.63; acc: 0.38
Batch: 380; loss: 1.67; acc: 0.38
Batch: 400; loss: 1.57; acc: 0.48
Batch: 420; loss: 1.45; acc: 0.55
Batch: 440; loss: 1.63; acc: 0.41
Batch: 460; loss: 1.59; acc: 0.41
Batch: 480; loss: 1.39; acc: 0.56
Batch: 500; loss: 1.43; acc: 0.52
Batch: 520; loss: 1.4; acc: 0.53
Batch: 540; loss: 1.55; acc: 0.52
Batch: 560; loss: 1.41; acc: 0.52
Batch: 580; loss: 1.55; acc: 0.44
Batch: 600; loss: 1.61; acc: 0.39
Batch: 620; loss: 1.36; acc: 0.66
Batch: 640; loss: 1.47; acc: 0.53
Batch: 660; loss: 1.64; acc: 0.39
Batch: 680; loss: 1.58; acc: 0.47
Batch: 700; loss: 1.39; acc: 0.55
Batch: 720; loss: 1.43; acc: 0.56
Batch: 740; loss: 1.55; acc: 0.5
Batch: 760; loss: 1.5; acc: 0.39
Batch: 780; loss: 1.72; acc: 0.34
Train Epoch over. train_loss: 1.49; train_accuracy: 0.51 

4.92659819428809e-05
1.599795177753549e-05
Batch: 0; loss: 1.51; acc: 0.5
Batch: 20; loss: 1.79; acc: 0.41
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.36; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.53
Batch: 100; loss: 1.34; acc: 0.61
Batch: 120; loss: 1.69; acc: 0.45
Batch: 140; loss: 1.51; acc: 0.48
Val Epoch over. val_loss: 1.4719772786851142; val_accuracy: 0.5029856687898089 

The current subspace-distance is: 1.599795177753549e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.7; acc: 0.41
Batch: 20; loss: 1.44; acc: 0.56
Batch: 40; loss: 1.45; acc: 0.48
Batch: 60; loss: 1.33; acc: 0.61
Batch: 80; loss: 1.55; acc: 0.53
Batch: 100; loss: 1.44; acc: 0.56
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.5
Batch: 160; loss: 1.53; acc: 0.52
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.48; acc: 0.47
Batch: 220; loss: 1.5; acc: 0.53
Batch: 240; loss: 1.68; acc: 0.42
Batch: 260; loss: 1.38; acc: 0.56
Batch: 280; loss: 1.5; acc: 0.53
Batch: 300; loss: 1.5; acc: 0.48
Batch: 320; loss: 1.48; acc: 0.58
Batch: 340; loss: 1.44; acc: 0.58
Batch: 360; loss: 1.51; acc: 0.52
Batch: 380; loss: 1.4; acc: 0.52
Batch: 400; loss: 1.71; acc: 0.41
Batch: 420; loss: 1.41; acc: 0.55
Batch: 440; loss: 1.29; acc: 0.62
Batch: 460; loss: 1.47; acc: 0.47
Batch: 480; loss: 1.44; acc: 0.52
Batch: 500; loss: 1.58; acc: 0.45
Batch: 520; loss: 1.38; acc: 0.48
Batch: 540; loss: 1.53; acc: 0.47
Batch: 560; loss: 1.65; acc: 0.47
Batch: 580; loss: 1.44; acc: 0.58
Batch: 600; loss: 1.35; acc: 0.61
Batch: 620; loss: 1.46; acc: 0.5
Batch: 640; loss: 1.35; acc: 0.59
Batch: 660; loss: 1.66; acc: 0.39
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.41; acc: 0.48
Batch: 720; loss: 1.38; acc: 0.52
Batch: 740; loss: 1.48; acc: 0.52
Batch: 760; loss: 1.55; acc: 0.47
Batch: 780; loss: 1.55; acc: 0.53
Train Epoch over. train_loss: 1.49; train_accuracy: 0.51 

5.001123281545006e-05
1.6466836314066313e-05
Batch: 0; loss: 1.49; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.34; acc: 0.55
Batch: 80; loss: 1.35; acc: 0.55
Batch: 100; loss: 1.32; acc: 0.67
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.5; acc: 0.5
Val Epoch over. val_loss: 1.466826807161805; val_accuracy: 0.5062699044585988 

The current subspace-distance is: 1.6466836314066313e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 1.43; acc: 0.53
Batch: 60; loss: 1.54; acc: 0.53
Batch: 80; loss: 1.49; acc: 0.52
Batch: 100; loss: 1.29; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.5
Batch: 160; loss: 1.37; acc: 0.52
Batch: 180; loss: 1.5; acc: 0.52
Batch: 200; loss: 1.46; acc: 0.44
Batch: 220; loss: 1.41; acc: 0.55
Batch: 240; loss: 1.41; acc: 0.64
Batch: 260; loss: 1.5; acc: 0.5
Batch: 280; loss: 1.48; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.64
Batch: 320; loss: 1.45; acc: 0.5
Batch: 340; loss: 1.51; acc: 0.5
Batch: 360; loss: 1.52; acc: 0.5
Batch: 380; loss: 1.53; acc: 0.44
Batch: 400; loss: 1.62; acc: 0.45
Batch: 420; loss: 1.53; acc: 0.52
Batch: 440; loss: 1.53; acc: 0.5
Batch: 460; loss: 1.53; acc: 0.5
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.37; acc: 0.59
Batch: 520; loss: 1.37; acc: 0.55
Batch: 540; loss: 1.56; acc: 0.45
Batch: 560; loss: 1.46; acc: 0.47
Batch: 580; loss: 1.56; acc: 0.41
Batch: 600; loss: 1.55; acc: 0.53
Batch: 620; loss: 1.47; acc: 0.62
Batch: 640; loss: 1.5; acc: 0.56
Batch: 660; loss: 1.29; acc: 0.62
Batch: 680; loss: 1.5; acc: 0.5
Batch: 700; loss: 1.46; acc: 0.53
Batch: 720; loss: 1.36; acc: 0.62
Batch: 740; loss: 1.48; acc: 0.58
Batch: 760; loss: 1.42; acc: 0.52
Batch: 780; loss: 1.43; acc: 0.53
Train Epoch over. train_loss: 1.49; train_accuracy: 0.51 

5.06959440826904e-05
1.757256359269377e-05
Batch: 0; loss: 1.49; acc: 0.55
Batch: 20; loss: 1.79; acc: 0.39
Batch: 40; loss: 1.18; acc: 0.72
Batch: 60; loss: 1.34; acc: 0.55
Batch: 80; loss: 1.36; acc: 0.5
Batch: 100; loss: 1.31; acc: 0.62
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.49; acc: 0.53
Val Epoch over. val_loss: 1.4650248775056973; val_accuracy: 0.5068670382165605 

The current subspace-distance is: 1.757256359269377e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.55
Batch: 20; loss: 1.58; acc: 0.45
Batch: 40; loss: 1.45; acc: 0.47
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.63; acc: 0.45
Batch: 100; loss: 1.47; acc: 0.45
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 1.6; acc: 0.52
Batch: 160; loss: 1.56; acc: 0.44
Batch: 180; loss: 1.39; acc: 0.52
Batch: 200; loss: 1.62; acc: 0.45
Batch: 220; loss: 1.51; acc: 0.58
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 1.32; acc: 0.59
Batch: 280; loss: 1.57; acc: 0.44
Batch: 300; loss: 1.58; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.55
Batch: 340; loss: 1.45; acc: 0.56
Batch: 360; loss: 1.37; acc: 0.55
Batch: 380; loss: 1.63; acc: 0.44
Batch: 400; loss: 1.39; acc: 0.56
Batch: 420; loss: 1.38; acc: 0.59
Batch: 440; loss: 1.41; acc: 0.55
Batch: 460; loss: 1.5; acc: 0.45
Batch: 480; loss: 1.55; acc: 0.45
Batch: 500; loss: 1.24; acc: 0.67
Batch: 520; loss: 1.41; acc: 0.52
Batch: 540; loss: 1.38; acc: 0.56
Batch: 560; loss: 1.63; acc: 0.5
Batch: 580; loss: 1.45; acc: 0.5
Batch: 600; loss: 1.47; acc: 0.52
Batch: 620; loss: 1.56; acc: 0.47
Batch: 640; loss: 1.54; acc: 0.42
Batch: 660; loss: 1.39; acc: 0.52
Batch: 680; loss: 1.38; acc: 0.53
Batch: 700; loss: 1.19; acc: 0.62
Batch: 720; loss: 1.53; acc: 0.47
Batch: 740; loss: 1.42; acc: 0.55
Batch: 760; loss: 1.55; acc: 0.47
Batch: 780; loss: 1.49; acc: 0.41
Train Epoch over. train_loss: 1.48; train_accuracy: 0.51 

5.072589556220919e-05
1.7315102013526484e-05
Batch: 0; loss: 1.5; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.34; acc: 0.55
Batch: 80; loss: 1.35; acc: 0.55
Batch: 100; loss: 1.32; acc: 0.62
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.5; acc: 0.52
Val Epoch over. val_loss: 1.467053831762569; val_accuracy: 0.5049761146496815 

The current subspace-distance is: 1.7315102013526484e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.42; acc: 0.5
Batch: 20; loss: 1.44; acc: 0.52
Batch: 40; loss: 1.65; acc: 0.47
Batch: 60; loss: 1.52; acc: 0.53
Batch: 80; loss: 1.39; acc: 0.52
Batch: 100; loss: 1.4; acc: 0.59
Batch: 120; loss: 1.36; acc: 0.53
Batch: 140; loss: 1.6; acc: 0.41
Batch: 160; loss: 1.56; acc: 0.47
Batch: 180; loss: 1.44; acc: 0.53
Batch: 200; loss: 1.67; acc: 0.42
Batch: 220; loss: 1.66; acc: 0.38
Batch: 240; loss: 1.34; acc: 0.64
Batch: 260; loss: 1.54; acc: 0.47
Batch: 280; loss: 1.49; acc: 0.56
Batch: 300; loss: 1.32; acc: 0.55
Batch: 320; loss: 1.46; acc: 0.56
Batch: 340; loss: 1.49; acc: 0.59
Batch: 360; loss: 1.55; acc: 0.53
Batch: 380; loss: 1.39; acc: 0.62
Batch: 400; loss: 1.48; acc: 0.56
Batch: 420; loss: 1.37; acc: 0.58
Batch: 440; loss: 1.41; acc: 0.56
Batch: 460; loss: 1.6; acc: 0.45
Batch: 480; loss: 1.46; acc: 0.56
Batch: 500; loss: 1.49; acc: 0.47
Batch: 520; loss: 1.45; acc: 0.55
Batch: 540; loss: 1.47; acc: 0.53
Batch: 560; loss: 1.65; acc: 0.44
Batch: 580; loss: 1.43; acc: 0.58
Batch: 600; loss: 1.38; acc: 0.61
Batch: 620; loss: 1.5; acc: 0.5
Batch: 640; loss: 1.63; acc: 0.47
Batch: 660; loss: 1.31; acc: 0.59
Batch: 680; loss: 1.55; acc: 0.44
Batch: 700; loss: 1.68; acc: 0.44
Batch: 720; loss: 1.51; acc: 0.47
Batch: 740; loss: 1.57; acc: 0.52
Batch: 760; loss: 1.56; acc: 0.44
Batch: 780; loss: 1.53; acc: 0.52
Train Epoch over. train_loss: 1.48; train_accuracy: 0.51 

5.064511060481891e-05
1.8147986338590272e-05
Batch: 0; loss: 1.48; acc: 0.53
Batch: 20; loss: 1.78; acc: 0.41
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.33; acc: 0.53
Batch: 80; loss: 1.34; acc: 0.55
Batch: 100; loss: 1.3; acc: 0.62
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.47; acc: 0.52
Val Epoch over. val_loss: 1.4563486925355948; val_accuracy: 0.5096536624203821 

The current subspace-distance is: 1.8147986338590272e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.5
Batch: 20; loss: 1.5; acc: 0.47
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.57; acc: 0.47
Batch: 80; loss: 1.54; acc: 0.5
Batch: 100; loss: 1.65; acc: 0.41
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.55
Batch: 160; loss: 1.47; acc: 0.56
Batch: 180; loss: 1.34; acc: 0.59
Batch: 200; loss: 1.32; acc: 0.58
Batch: 220; loss: 1.52; acc: 0.45
Batch: 240; loss: 1.54; acc: 0.44
Batch: 260; loss: 1.31; acc: 0.58
Batch: 280; loss: 1.31; acc: 0.62
Batch: 300; loss: 1.34; acc: 0.59
Batch: 320; loss: 1.38; acc: 0.53
Batch: 340; loss: 1.47; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.5
Batch: 380; loss: 1.55; acc: 0.55
Batch: 400; loss: 1.46; acc: 0.5
Batch: 420; loss: 1.42; acc: 0.48
Batch: 440; loss: 1.39; acc: 0.53
Batch: 460; loss: 1.49; acc: 0.58
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.46; acc: 0.55
Batch: 520; loss: 1.53; acc: 0.45
Batch: 540; loss: 1.56; acc: 0.45
Batch: 560; loss: 1.6; acc: 0.52
Batch: 580; loss: 1.67; acc: 0.44
Batch: 600; loss: 1.58; acc: 0.39
Batch: 620; loss: 1.46; acc: 0.48
Batch: 640; loss: 1.58; acc: 0.45
Batch: 660; loss: 1.49; acc: 0.47
Batch: 680; loss: 1.56; acc: 0.39
Batch: 700; loss: 1.35; acc: 0.56
Batch: 720; loss: 1.42; acc: 0.53
Batch: 740; loss: 1.62; acc: 0.42
Batch: 760; loss: 1.45; acc: 0.52
Batch: 780; loss: 1.5; acc: 0.56
Train Epoch over. train_loss: 1.48; train_accuracy: 0.51 

5.059344039182179e-05
1.8664200979401357e-05
Batch: 0; loss: 1.49; acc: 0.52
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.34; acc: 0.56
Batch: 80; loss: 1.35; acc: 0.52
Batch: 100; loss: 1.32; acc: 0.64
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.49; acc: 0.52
Val Epoch over. val_loss: 1.4618732116784259; val_accuracy: 0.5073646496815286 

The current subspace-distance is: 1.8664200979401357e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.53
Batch: 20; loss: 1.39; acc: 0.61
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.47; acc: 0.58
Batch: 100; loss: 1.49; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.47
Batch: 140; loss: 1.58; acc: 0.45
Batch: 160; loss: 1.4; acc: 0.53
Batch: 180; loss: 1.45; acc: 0.52
Batch: 200; loss: 1.43; acc: 0.55
Batch: 220; loss: 1.64; acc: 0.44
Batch: 240; loss: 1.58; acc: 0.42
Batch: 260; loss: 1.44; acc: 0.56
Batch: 280; loss: 1.38; acc: 0.58
Batch: 300; loss: 1.52; acc: 0.47
Batch: 320; loss: 1.42; acc: 0.53
Batch: 340; loss: 1.38; acc: 0.5
Batch: 360; loss: 1.44; acc: 0.48
Batch: 380; loss: 1.58; acc: 0.41
Batch: 400; loss: 1.55; acc: 0.52
Batch: 420; loss: 1.31; acc: 0.55
Batch: 440; loss: 1.63; acc: 0.38
Batch: 460; loss: 1.34; acc: 0.53
Batch: 480; loss: 1.53; acc: 0.45
Batch: 500; loss: 1.53; acc: 0.5
Batch: 520; loss: 1.68; acc: 0.38
Batch: 540; loss: 1.54; acc: 0.45
Batch: 560; loss: 1.42; acc: 0.55
Batch: 580; loss: 1.49; acc: 0.5
Batch: 600; loss: 1.57; acc: 0.48
Batch: 620; loss: 1.5; acc: 0.44
Batch: 640; loss: 1.48; acc: 0.52
Batch: 660; loss: 1.62; acc: 0.45
Batch: 680; loss: 1.53; acc: 0.56
Batch: 700; loss: 1.47; acc: 0.56
Batch: 720; loss: 1.36; acc: 0.58
Batch: 740; loss: 1.44; acc: 0.56
Batch: 760; loss: 1.51; acc: 0.53
Batch: 780; loss: 1.24; acc: 0.62
Train Epoch over. train_loss: 1.48; train_accuracy: 0.51 

5.03052506246604e-05
1.564260310260579e-05
Batch: 0; loss: 1.48; acc: 0.53
Batch: 20; loss: 1.78; acc: 0.44
Batch: 40; loss: 1.18; acc: 0.7
Batch: 60; loss: 1.32; acc: 0.53
Batch: 80; loss: 1.35; acc: 0.48
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.69; acc: 0.41
Batch: 140; loss: 1.46; acc: 0.55
Val Epoch over. val_loss: 1.451174109604708; val_accuracy: 0.5141321656050956 

The current subspace-distance is: 1.564260310260579e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.46; acc: 0.52
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 1.52; acc: 0.5
Batch: 60; loss: 1.48; acc: 0.5
Batch: 80; loss: 1.5; acc: 0.5
Batch: 100; loss: 1.51; acc: 0.52
Batch: 120; loss: 1.53; acc: 0.55
Batch: 140; loss: 1.42; acc: 0.52
Batch: 160; loss: 1.42; acc: 0.58
Batch: 180; loss: 1.68; acc: 0.36
Batch: 200; loss: 1.51; acc: 0.53
Batch: 220; loss: 1.59; acc: 0.47
Batch: 240; loss: 1.44; acc: 0.58
Batch: 260; loss: 1.78; acc: 0.38
Batch: 280; loss: 1.47; acc: 0.53
Batch: 300; loss: 1.47; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.42
Batch: 340; loss: 1.38; acc: 0.56
Batch: 360; loss: 1.51; acc: 0.53
Batch: 380; loss: 1.46; acc: 0.48
Batch: 400; loss: 1.46; acc: 0.52
Batch: 420; loss: 1.38; acc: 0.53
Batch: 440; loss: 1.28; acc: 0.67
Batch: 460; loss: 1.46; acc: 0.5
Batch: 480; loss: 1.65; acc: 0.39
Batch: 500; loss: 1.45; acc: 0.5
Batch: 520; loss: 1.53; acc: 0.5
Batch: 540; loss: 1.57; acc: 0.47
Batch: 560; loss: 1.42; acc: 0.52
Batch: 580; loss: 1.46; acc: 0.59
Batch: 600; loss: 1.45; acc: 0.58
Batch: 620; loss: 1.65; acc: 0.42
Batch: 640; loss: 1.32; acc: 0.55
Batch: 660; loss: 1.4; acc: 0.55
Batch: 680; loss: 1.32; acc: 0.58
Batch: 700; loss: 1.59; acc: 0.47
Batch: 720; loss: 1.49; acc: 0.47
Batch: 740; loss: 1.53; acc: 0.45
Batch: 760; loss: 1.4; acc: 0.5
Batch: 780; loss: 1.47; acc: 0.55
Train Epoch over. train_loss: 1.48; train_accuracy: 0.51 

5.047896411269903e-05
1.5142328265937977e-05
Batch: 0; loss: 1.47; acc: 0.53
Batch: 20; loss: 1.78; acc: 0.39
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.33; acc: 0.55
Batch: 80; loss: 1.35; acc: 0.53
Batch: 100; loss: 1.3; acc: 0.64
Batch: 120; loss: 1.66; acc: 0.44
Batch: 140; loss: 1.48; acc: 0.52
Val Epoch over. val_loss: 1.4570370206407681; val_accuracy: 0.5108479299363057 

The current subspace-distance is: 1.5142328265937977e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.5
Batch: 20; loss: 1.61; acc: 0.48
Batch: 40; loss: 1.61; acc: 0.44
Batch: 60; loss: 1.55; acc: 0.5
Batch: 80; loss: 1.41; acc: 0.53
Batch: 100; loss: 1.37; acc: 0.58
Batch: 120; loss: 1.45; acc: 0.58
Batch: 140; loss: 1.46; acc: 0.55
Batch: 160; loss: 1.6; acc: 0.53
Batch: 180; loss: 1.45; acc: 0.58
Batch: 200; loss: 1.43; acc: 0.47
Batch: 220; loss: 1.34; acc: 0.58
Batch: 240; loss: 1.49; acc: 0.56
Batch: 260; loss: 1.59; acc: 0.56
Batch: 280; loss: 1.38; acc: 0.53
Batch: 300; loss: 1.61; acc: 0.41
Batch: 320; loss: 1.53; acc: 0.5
Batch: 340; loss: 1.5; acc: 0.52
Batch: 360; loss: 1.41; acc: 0.61
Batch: 380; loss: 1.41; acc: 0.47
Batch: 400; loss: 1.65; acc: 0.39
Batch: 420; loss: 1.65; acc: 0.42
Batch: 440; loss: 1.54; acc: 0.45
Batch: 460; loss: 1.46; acc: 0.59
Batch: 480; loss: 1.52; acc: 0.45
Batch: 500; loss: 1.64; acc: 0.44
Batch: 520; loss: 1.45; acc: 0.55
Batch: 540; loss: 1.31; acc: 0.58
Batch: 560; loss: 1.44; acc: 0.58
Batch: 580; loss: 1.46; acc: 0.53
Batch: 600; loss: 1.35; acc: 0.58
Batch: 620; loss: 1.45; acc: 0.5
Batch: 640; loss: 1.57; acc: 0.52
Batch: 660; loss: 1.47; acc: 0.48
Batch: 680; loss: 1.44; acc: 0.52
Batch: 700; loss: 1.42; acc: 0.55
Batch: 720; loss: 1.41; acc: 0.53
Batch: 740; loss: 1.51; acc: 0.44
Batch: 760; loss: 1.46; acc: 0.56
Batch: 780; loss: 1.49; acc: 0.5
Train Epoch over. train_loss: 1.48; train_accuracy: 0.51 

5.1055623771389946e-05
1.8983941117767245e-05
Batch: 0; loss: 1.48; acc: 0.52
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.33; acc: 0.56
Batch: 80; loss: 1.34; acc: 0.52
Batch: 100; loss: 1.32; acc: 0.62
Batch: 120; loss: 1.67; acc: 0.45
Batch: 140; loss: 1.47; acc: 0.52
Val Epoch over. val_loss: 1.4543418489444029; val_accuracy: 0.5121417197452229 

The current subspace-distance is: 1.8983941117767245e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.41; acc: 0.62
Batch: 20; loss: 1.49; acc: 0.52
Batch: 40; loss: 1.6; acc: 0.45
Batch: 60; loss: 1.58; acc: 0.5
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.53; acc: 0.41
Batch: 120; loss: 1.66; acc: 0.39
Batch: 140; loss: 1.57; acc: 0.55
Batch: 160; loss: 1.48; acc: 0.5
Batch: 180; loss: 1.38; acc: 0.62
Batch: 200; loss: 1.34; acc: 0.59
Batch: 220; loss: 1.3; acc: 0.53
Batch: 240; loss: 1.52; acc: 0.48
Batch: 260; loss: 1.52; acc: 0.48
Batch: 280; loss: 1.49; acc: 0.48
Batch: 300; loss: 1.58; acc: 0.48
Batch: 320; loss: 1.39; acc: 0.55
Batch: 340; loss: 1.44; acc: 0.5
Batch: 360; loss: 1.52; acc: 0.48
Batch: 380; loss: 1.72; acc: 0.36
Batch: 400; loss: 1.46; acc: 0.55
Batch: 420; loss: 1.48; acc: 0.44
Batch: 440; loss: 1.44; acc: 0.5
Batch: 460; loss: 1.48; acc: 0.53
Batch: 480; loss: 1.44; acc: 0.55
Batch: 500; loss: 1.59; acc: 0.38
Batch: 520; loss: 1.5; acc: 0.53
Batch: 540; loss: 1.49; acc: 0.52
Batch: 560; loss: 1.51; acc: 0.5
Batch: 580; loss: 1.68; acc: 0.39
Batch: 600; loss: 1.5; acc: 0.52
Batch: 620; loss: 1.3; acc: 0.59
Batch: 640; loss: 1.59; acc: 0.48
Batch: 660; loss: 1.57; acc: 0.5
Batch: 680; loss: 1.49; acc: 0.5
Batch: 700; loss: 1.31; acc: 0.58
Batch: 720; loss: 1.4; acc: 0.64
Batch: 740; loss: 1.42; acc: 0.45
Batch: 760; loss: 1.42; acc: 0.5
Batch: 780; loss: 1.67; acc: 0.45
Train Epoch over. train_loss: 1.48; train_accuracy: 0.51 

5.087658792035654e-05
1.7822716472437605e-05
Batch: 0; loss: 1.48; acc: 0.5
Batch: 20; loss: 1.78; acc: 0.42
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.32; acc: 0.55
Batch: 80; loss: 1.34; acc: 0.52
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 1.7; acc: 0.44
Batch: 140; loss: 1.45; acc: 0.52
Val Epoch over. val_loss: 1.4487152433699104; val_accuracy: 0.5092555732484076 

The current subspace-distance is: 1.7822716472437605e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.42; acc: 0.5
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 1.45; acc: 0.53
Batch: 60; loss: 1.53; acc: 0.45
Batch: 80; loss: 1.47; acc: 0.53
Batch: 100; loss: 1.4; acc: 0.52
Batch: 120; loss: 1.52; acc: 0.48
Batch: 140; loss: 1.61; acc: 0.48
Batch: 160; loss: 1.4; acc: 0.53
Batch: 180; loss: 1.41; acc: 0.62
Batch: 200; loss: 1.64; acc: 0.45
Batch: 220; loss: 1.51; acc: 0.48
Batch: 240; loss: 1.53; acc: 0.5
Batch: 260; loss: 1.38; acc: 0.53
Batch: 280; loss: 1.42; acc: 0.55
Batch: 300; loss: 1.38; acc: 0.61
Batch: 320; loss: 1.61; acc: 0.47
Batch: 340; loss: 1.43; acc: 0.58
Batch: 360; loss: 1.44; acc: 0.5
Batch: 380; loss: 1.42; acc: 0.62
Batch: 400; loss: 1.38; acc: 0.58
Batch: 420; loss: 1.42; acc: 0.53
Batch: 440; loss: 1.34; acc: 0.59
Batch: 460; loss: 1.42; acc: 0.56
Batch: 480; loss: 1.48; acc: 0.48
Batch: 500; loss: 1.74; acc: 0.38
Batch: 520; loss: 1.56; acc: 0.42
Batch: 540; loss: 1.43; acc: 0.53
Batch: 560; loss: 1.64; acc: 0.41
Batch: 580; loss: 1.41; acc: 0.5
Batch: 600; loss: 1.47; acc: 0.53
Batch: 620; loss: 1.38; acc: 0.62
Batch: 640; loss: 1.61; acc: 0.44
Batch: 660; loss: 1.49; acc: 0.48
Batch: 680; loss: 1.36; acc: 0.48
Batch: 700; loss: 1.58; acc: 0.53
Batch: 720; loss: 1.47; acc: 0.5
Batch: 740; loss: 1.63; acc: 0.42
Batch: 760; loss: 1.48; acc: 0.53
Batch: 780; loss: 1.4; acc: 0.55
Train Epoch over. train_loss: 1.48; train_accuracy: 0.51 

5.130148565513082e-05
2.0734514691866934e-05
Batch: 0; loss: 1.49; acc: 0.47
Batch: 20; loss: 1.78; acc: 0.41
Batch: 40; loss: 1.19; acc: 0.69
Batch: 60; loss: 1.34; acc: 0.56
Batch: 80; loss: 1.34; acc: 0.53
Batch: 100; loss: 1.33; acc: 0.62
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.46; acc: 0.53
Val Epoch over. val_loss: 1.4586316749548456; val_accuracy: 0.5103503184713376 

The current subspace-distance is: 2.0734514691866934e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_12_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.17

The number of parameters is: 270144

The number of individual parameters is:

10
180
10
10
15
33600
15
15
29
97440
29
29
64
133632
64
64
4096
64
640
10
64
64

nonzero elements in E: 27014397
elements in E: 27014400
fraction nonzero: 0.9999998889481165
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.5; acc: 0.06
Batch: 20; loss: 2.41; acc: 0.12
Batch: 40; loss: 2.44; acc: 0.08
Batch: 60; loss: 2.2; acc: 0.2
Batch: 80; loss: 2.13; acc: 0.27
Batch: 100; loss: 2.16; acc: 0.23
Batch: 120; loss: 2.04; acc: 0.28
Batch: 140; loss: 1.97; acc: 0.36
Batch: 160; loss: 1.95; acc: 0.45
Batch: 180; loss: 1.97; acc: 0.38
Batch: 200; loss: 1.9; acc: 0.39
Batch: 220; loss: 1.87; acc: 0.39
Batch: 240; loss: 1.84; acc: 0.39
Batch: 260; loss: 1.96; acc: 0.33
Batch: 280; loss: 1.92; acc: 0.38
Batch: 300; loss: 1.93; acc: 0.33
Batch: 320; loss: 1.83; acc: 0.44
Batch: 340; loss: 1.87; acc: 0.45
Batch: 360; loss: 1.9; acc: 0.38
Batch: 380; loss: 1.72; acc: 0.52
Batch: 400; loss: 1.77; acc: 0.42
Batch: 420; loss: 1.81; acc: 0.53
Batch: 440; loss: 1.85; acc: 0.41
Batch: 460; loss: 1.78; acc: 0.48
Batch: 480; loss: 1.76; acc: 0.53
Batch: 500; loss: 1.74; acc: 0.55
Batch: 520; loss: 1.71; acc: 0.52
Batch: 540; loss: 1.78; acc: 0.48
Batch: 560; loss: 1.83; acc: 0.45
Batch: 580; loss: 1.71; acc: 0.48
Batch: 600; loss: 1.72; acc: 0.5
Batch: 620; loss: 1.88; acc: 0.34
Batch: 640; loss: 1.74; acc: 0.44
Batch: 660; loss: 1.69; acc: 0.47
Batch: 680; loss: 1.58; acc: 0.58
Batch: 700; loss: 1.66; acc: 0.59
Batch: 720; loss: 1.73; acc: 0.48
Batch: 740; loss: 1.73; acc: 0.47
Batch: 760; loss: 1.71; acc: 0.5
Batch: 780; loss: 1.66; acc: 0.53
Train Epoch over. train_loss: 1.88; train_accuracy: 0.42 

5.85618763579987e-05
5.300833799992688e-05
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.84; acc: 0.41
Batch: 40; loss: 1.48; acc: 0.62
Batch: 60; loss: 1.56; acc: 0.58
Batch: 80; loss: 1.57; acc: 0.56
Batch: 100; loss: 1.62; acc: 0.58
Batch: 120; loss: 1.76; acc: 0.48
Batch: 140; loss: 1.66; acc: 0.59
Val Epoch over. val_loss: 1.6710263657721744; val_accuracy: 0.5257762738853503 

The current subspace-distance is: 5.300833799992688e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.61
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.82; acc: 0.42
Batch: 60; loss: 1.73; acc: 0.47
Batch: 80; loss: 1.69; acc: 0.48
Batch: 100; loss: 1.76; acc: 0.48
Batch: 120; loss: 1.63; acc: 0.53
Batch: 140; loss: 1.45; acc: 0.66
Batch: 160; loss: 1.66; acc: 0.55
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.63; acc: 0.53
Batch: 220; loss: 1.67; acc: 0.55
Batch: 240; loss: 1.66; acc: 0.52
Batch: 260; loss: 1.61; acc: 0.61
Batch: 280; loss: 1.64; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.5
Batch: 320; loss: 1.6; acc: 0.56
Batch: 340; loss: 1.77; acc: 0.39
Batch: 360; loss: 1.64; acc: 0.52
Batch: 380; loss: 1.64; acc: 0.53
Batch: 400; loss: 1.54; acc: 0.56
Batch: 420; loss: 1.69; acc: 0.44
Batch: 440; loss: 1.7; acc: 0.5
Batch: 460; loss: 1.68; acc: 0.52
Batch: 480; loss: 1.45; acc: 0.66
Batch: 500; loss: 1.71; acc: 0.59
Batch: 520; loss: 1.62; acc: 0.58
Batch: 540; loss: 1.71; acc: 0.53
Batch: 560; loss: 1.42; acc: 0.67
Batch: 580; loss: 1.48; acc: 0.62
Batch: 600; loss: 1.5; acc: 0.56
Batch: 620; loss: 1.8; acc: 0.41
Batch: 640; loss: 1.46; acc: 0.62
Batch: 660; loss: 1.4; acc: 0.7
Batch: 680; loss: 1.63; acc: 0.5
Batch: 700; loss: 1.42; acc: 0.64
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.54; acc: 0.59
Batch: 760; loss: 1.55; acc: 0.55
Batch: 780; loss: 1.61; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.55 

7.854487921576947e-05
7.305660255951807e-05
Batch: 0; loss: 1.51; acc: 0.55
Batch: 20; loss: 1.7; acc: 0.53
Batch: 40; loss: 1.3; acc: 0.78
Batch: 60; loss: 1.44; acc: 0.66
Batch: 80; loss: 1.38; acc: 0.66
Batch: 100; loss: 1.51; acc: 0.66
Batch: 120; loss: 1.55; acc: 0.58
Batch: 140; loss: 1.51; acc: 0.62
Val Epoch over. val_loss: 1.4981759444923157; val_accuracy: 0.6033041401273885 

The current subspace-distance is: 7.305660255951807e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.52; acc: 0.56
Batch: 40; loss: 1.6; acc: 0.45
Batch: 60; loss: 1.43; acc: 0.69
Batch: 80; loss: 1.49; acc: 0.64
Batch: 100; loss: 1.41; acc: 0.66
Batch: 120; loss: 1.55; acc: 0.61
Batch: 140; loss: 1.55; acc: 0.58
Batch: 160; loss: 1.56; acc: 0.55
Batch: 180; loss: 1.48; acc: 0.59
Batch: 200; loss: 1.38; acc: 0.67
Batch: 220; loss: 1.48; acc: 0.55
Batch: 240; loss: 1.51; acc: 0.62
Batch: 260; loss: 1.41; acc: 0.58
Batch: 280; loss: 1.28; acc: 0.78
Batch: 300; loss: 1.49; acc: 0.56
Batch: 320; loss: 1.53; acc: 0.58
Batch: 340; loss: 1.41; acc: 0.62
Batch: 360; loss: 1.35; acc: 0.69
Batch: 380; loss: 1.36; acc: 0.66
Batch: 400; loss: 1.63; acc: 0.52
Batch: 420; loss: 1.49; acc: 0.69
Batch: 440; loss: 1.48; acc: 0.62
Batch: 460; loss: 1.51; acc: 0.58
Batch: 480; loss: 1.45; acc: 0.62
Batch: 500; loss: 1.33; acc: 0.69
Batch: 520; loss: 1.44; acc: 0.53
Batch: 540; loss: 1.33; acc: 0.67
Batch: 560; loss: 1.41; acc: 0.56
Batch: 580; loss: 1.42; acc: 0.66
Batch: 600; loss: 1.58; acc: 0.53
Batch: 620; loss: 1.41; acc: 0.61
Batch: 640; loss: 1.38; acc: 0.66
Batch: 660; loss: 1.48; acc: 0.58
Batch: 680; loss: 1.45; acc: 0.59
Batch: 700; loss: 1.44; acc: 0.69
Batch: 720; loss: 1.42; acc: 0.66
Batch: 740; loss: 1.32; acc: 0.67
Batch: 760; loss: 1.27; acc: 0.75
Batch: 780; loss: 1.36; acc: 0.67
Train Epoch over. train_loss: 1.48; train_accuracy: 0.6 

9.471941302763298e-05
8.906658331397921e-05
Batch: 0; loss: 1.42; acc: 0.61
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.17; acc: 0.75
Batch: 60; loss: 1.36; acc: 0.64
Batch: 80; loss: 1.26; acc: 0.67
Batch: 100; loss: 1.43; acc: 0.56
Batch: 120; loss: 1.48; acc: 0.66
Batch: 140; loss: 1.36; acc: 0.59
Val Epoch over. val_loss: 1.3864474607880708; val_accuracy: 0.6584394904458599 

The current subspace-distance is: 8.906658331397921e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.56
Batch: 20; loss: 1.48; acc: 0.61
Batch: 40; loss: 1.31; acc: 0.62
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.43; acc: 0.58
Batch: 100; loss: 1.41; acc: 0.62
Batch: 120; loss: 1.42; acc: 0.62
Batch: 140; loss: 1.45; acc: 0.62
Batch: 160; loss: 1.54; acc: 0.59
Batch: 180; loss: 1.43; acc: 0.64
Batch: 200; loss: 1.24; acc: 0.69
Batch: 220; loss: 1.46; acc: 0.58
Batch: 240; loss: 1.32; acc: 0.67
Batch: 260; loss: 1.3; acc: 0.75
Batch: 280; loss: 1.33; acc: 0.66
Batch: 300; loss: 1.5; acc: 0.59
Batch: 320; loss: 1.42; acc: 0.69
Batch: 340; loss: 1.38; acc: 0.59
Batch: 360; loss: 1.54; acc: 0.56
Batch: 380; loss: 1.32; acc: 0.67
Batch: 400; loss: 1.27; acc: 0.77
Batch: 420; loss: 1.42; acc: 0.66
Batch: 440; loss: 1.39; acc: 0.69
Batch: 460; loss: 1.56; acc: 0.61
Batch: 480; loss: 1.41; acc: 0.64
Batch: 500; loss: 1.29; acc: 0.7
Batch: 520; loss: 1.41; acc: 0.64
Batch: 540; loss: 1.48; acc: 0.66
Batch: 560; loss: 1.29; acc: 0.67
Batch: 580; loss: 1.44; acc: 0.69
Batch: 600; loss: 1.28; acc: 0.78
Batch: 620; loss: 1.25; acc: 0.75
Batch: 640; loss: 1.55; acc: 0.58
Batch: 660; loss: 1.32; acc: 0.67
Batch: 680; loss: 1.49; acc: 0.58
Batch: 700; loss: 1.21; acc: 0.69
Batch: 720; loss: 1.38; acc: 0.59
Batch: 740; loss: 1.4; acc: 0.58
Batch: 760; loss: 1.34; acc: 0.69
Batch: 780; loss: 1.36; acc: 0.66
Train Epoch over. train_loss: 1.39; train_accuracy: 0.64 

0.00010970137373078614
0.00010470872075529769
Batch: 0; loss: 1.38; acc: 0.69
Batch: 20; loss: 1.53; acc: 0.53
Batch: 40; loss: 1.08; acc: 0.8
Batch: 60; loss: 1.3; acc: 0.7
Batch: 80; loss: 1.22; acc: 0.69
Batch: 100; loss: 1.33; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.64
Batch: 140; loss: 1.27; acc: 0.66
Val Epoch over. val_loss: 1.3070260069932147; val_accuracy: 0.6899880573248408 

The current subspace-distance is: 0.00010470872075529769 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.48; acc: 0.61
Batch: 20; loss: 1.33; acc: 0.7
Batch: 40; loss: 1.31; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.4; acc: 0.62
Batch: 120; loss: 1.26; acc: 0.75
Batch: 140; loss: 1.27; acc: 0.67
Batch: 160; loss: 1.49; acc: 0.53
Batch: 180; loss: 1.44; acc: 0.56
Batch: 200; loss: 1.28; acc: 0.64
Batch: 220; loss: 1.28; acc: 0.7
Batch: 240; loss: 1.39; acc: 0.62
Batch: 260; loss: 1.54; acc: 0.56
Batch: 280; loss: 1.45; acc: 0.61
Batch: 300; loss: 1.26; acc: 0.72
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 1.23; acc: 0.72
Batch: 360; loss: 1.33; acc: 0.66
Batch: 380; loss: 1.3; acc: 0.72
Batch: 400; loss: 1.28; acc: 0.77
Batch: 420; loss: 1.35; acc: 0.75
Batch: 440; loss: 1.34; acc: 0.64
Batch: 460; loss: 1.5; acc: 0.55
Batch: 480; loss: 1.49; acc: 0.55
Batch: 500; loss: 1.45; acc: 0.55
Batch: 520; loss: 1.14; acc: 0.73
Batch: 540; loss: 1.3; acc: 0.72
Batch: 560; loss: 1.24; acc: 0.72
Batch: 580; loss: 1.22; acc: 0.72
Batch: 600; loss: 1.35; acc: 0.58
Batch: 620; loss: 1.29; acc: 0.72
Batch: 640; loss: 1.25; acc: 0.67
Batch: 660; loss: 1.28; acc: 0.61
Batch: 680; loss: 1.37; acc: 0.55
Batch: 700; loss: 1.16; acc: 0.69
Batch: 720; loss: 1.29; acc: 0.69
Batch: 740; loss: 1.47; acc: 0.56
Batch: 760; loss: 1.29; acc: 0.66
Batch: 780; loss: 1.26; acc: 0.62
Train Epoch over. train_loss: 1.32; train_accuracy: 0.66 

0.00012250567669980228
0.00011765478120651096
Batch: 0; loss: 1.34; acc: 0.64
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 0.99; acc: 0.81
Batch: 60; loss: 1.22; acc: 0.73
Batch: 80; loss: 1.14; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.67
Batch: 120; loss: 1.35; acc: 0.66
Batch: 140; loss: 1.2; acc: 0.64
Val Epoch over. val_loss: 1.2334865927696228; val_accuracy: 0.7046178343949044 

The current subspace-distance is: 0.00011765478120651096 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.7
Batch: 20; loss: 1.39; acc: 0.58
Batch: 40; loss: 1.25; acc: 0.64
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.33; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.64
Batch: 120; loss: 1.35; acc: 0.69
Batch: 140; loss: 1.28; acc: 0.67
Batch: 160; loss: 1.22; acc: 0.7
Batch: 180; loss: 1.45; acc: 0.59
Batch: 200; loss: 1.28; acc: 0.67
Batch: 220; loss: 1.27; acc: 0.59
Batch: 240; loss: 1.28; acc: 0.62
Batch: 260; loss: 1.19; acc: 0.7
Batch: 280; loss: 1.27; acc: 0.67
Batch: 300; loss: 1.38; acc: 0.64
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.4; acc: 0.64
Batch: 360; loss: 1.24; acc: 0.64
Batch: 380; loss: 1.24; acc: 0.73
Batch: 400; loss: 1.32; acc: 0.61
Batch: 420; loss: 1.31; acc: 0.67
Batch: 440; loss: 1.19; acc: 0.72
Batch: 460; loss: 1.32; acc: 0.58
Batch: 480; loss: 1.19; acc: 0.73
Batch: 500; loss: 1.47; acc: 0.58
Batch: 520; loss: 1.34; acc: 0.56
Batch: 540; loss: 1.24; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 1.33; acc: 0.58
Batch: 600; loss: 1.26; acc: 0.69
Batch: 620; loss: 1.3; acc: 0.66
Batch: 640; loss: 1.19; acc: 0.69
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.13; acc: 0.77
Batch: 700; loss: 1.43; acc: 0.58
Batch: 720; loss: 1.31; acc: 0.56
Batch: 740; loss: 1.2; acc: 0.78
Batch: 760; loss: 1.21; acc: 0.7
Batch: 780; loss: 1.27; acc: 0.7
Train Epoch over. train_loss: 1.26; train_accuracy: 0.67 

0.00013558156206272542
0.0001285473263124004
Batch: 0; loss: 1.3; acc: 0.7
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 0.93; acc: 0.88
Batch: 60; loss: 1.14; acc: 0.73
Batch: 80; loss: 1.09; acc: 0.7
Batch: 100; loss: 1.16; acc: 0.69
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 1.16; acc: 0.66
Val Epoch over. val_loss: 1.1784257915369265; val_accuracy: 0.7112858280254777 

The current subspace-distance is: 0.0001285473263124004 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 1.32; acc: 0.62
Batch: 40; loss: 1.2; acc: 0.66
Batch: 60; loss: 1.21; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.24; acc: 0.67
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 1.26; acc: 0.64
Batch: 160; loss: 1.24; acc: 0.67
Batch: 180; loss: 1.14; acc: 0.7
Batch: 200; loss: 1.23; acc: 0.69
Batch: 220; loss: 1.21; acc: 0.66
Batch: 240; loss: 1.27; acc: 0.61
Batch: 260; loss: 1.27; acc: 0.61
Batch: 280; loss: 1.27; acc: 0.66
Batch: 300; loss: 1.41; acc: 0.58
Batch: 320; loss: 1.21; acc: 0.66
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 1.2; acc: 0.64
Batch: 380; loss: 1.2; acc: 0.67
Batch: 400; loss: 1.07; acc: 0.78
Batch: 420; loss: 1.08; acc: 0.78
Batch: 440; loss: 1.41; acc: 0.55
Batch: 460; loss: 1.09; acc: 0.72
Batch: 480; loss: 1.21; acc: 0.66
Batch: 500; loss: 1.28; acc: 0.61
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.25; acc: 0.64
Batch: 560; loss: 1.42; acc: 0.52
Batch: 580; loss: 1.16; acc: 0.73
Batch: 600; loss: 1.36; acc: 0.62
Batch: 620; loss: 1.25; acc: 0.64
Batch: 640; loss: 1.29; acc: 0.69
Batch: 660; loss: 1.17; acc: 0.7
Batch: 680; loss: 1.23; acc: 0.58
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 1.11; acc: 0.72
Batch: 740; loss: 1.27; acc: 0.7
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.3; acc: 0.7
Train Epoch over. train_loss: 1.22; train_accuracy: 0.67 

0.00014670960081275553
0.00014190829824656248
Batch: 0; loss: 1.27; acc: 0.7
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.88; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.77
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 1.14; acc: 0.64
Val Epoch over. val_loss: 1.1414298779645544; val_accuracy: 0.7125796178343949 

The current subspace-distance is: 0.00014190829824656248 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.17; acc: 0.73
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.03; acc: 0.86
Batch: 80; loss: 1.18; acc: 0.75
Batch: 100; loss: 1.05; acc: 0.78
Batch: 120; loss: 1.16; acc: 0.62
Batch: 140; loss: 1.2; acc: 0.7
Batch: 160; loss: 1.16; acc: 0.7
Batch: 180; loss: 1.45; acc: 0.58
Batch: 200; loss: 1.16; acc: 0.7
Batch: 220; loss: 1.29; acc: 0.64
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.19; acc: 0.66
Batch: 280; loss: 1.15; acc: 0.7
Batch: 300; loss: 1.16; acc: 0.67
Batch: 320; loss: 1.06; acc: 0.72
Batch: 340; loss: 1.19; acc: 0.67
Batch: 360; loss: 1.13; acc: 0.75
Batch: 380; loss: 1.19; acc: 0.72
Batch: 400; loss: 1.01; acc: 0.75
Batch: 420; loss: 1.17; acc: 0.66
Batch: 440; loss: 1.06; acc: 0.7
Batch: 460; loss: 1.09; acc: 0.67
Batch: 480; loss: 1.26; acc: 0.62
Batch: 500; loss: 1.25; acc: 0.66
Batch: 520; loss: 1.04; acc: 0.73
Batch: 540; loss: 1.1; acc: 0.72
Batch: 560; loss: 1.21; acc: 0.69
Batch: 580; loss: 1.15; acc: 0.66
Batch: 600; loss: 1.22; acc: 0.59
Batch: 620; loss: 1.18; acc: 0.72
Batch: 640; loss: 1.17; acc: 0.69
Batch: 660; loss: 1.08; acc: 0.77
Batch: 680; loss: 1.21; acc: 0.69
Batch: 700; loss: 1.17; acc: 0.72
Batch: 720; loss: 1.06; acc: 0.73
Batch: 740; loss: 1.21; acc: 0.66
Batch: 760; loss: 1.25; acc: 0.66
Batch: 780; loss: 1.16; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00016113609308376908
0.00015416547830682248
Batch: 0; loss: 1.24; acc: 0.69
Batch: 20; loss: 1.36; acc: 0.5
Batch: 40; loss: 0.85; acc: 0.86
Batch: 60; loss: 1.03; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.77
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 1.1; acc: 0.69
Val Epoch over. val_loss: 1.1054222971011118; val_accuracy: 0.71984474522293 

The current subspace-distance is: 0.00015416547830682248 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 1.21; acc: 0.62
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.78
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.29; acc: 0.67
Batch: 140; loss: 1.2; acc: 0.66
Batch: 160; loss: 1.16; acc: 0.69
Batch: 180; loss: 1.13; acc: 0.73
Batch: 200; loss: 1.36; acc: 0.59
Batch: 220; loss: 1.12; acc: 0.7
Batch: 240; loss: 0.97; acc: 0.81
Batch: 260; loss: 1.15; acc: 0.64
Batch: 280; loss: 1.09; acc: 0.75
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 1.2; acc: 0.62
Batch: 340; loss: 1.29; acc: 0.62
Batch: 360; loss: 1.21; acc: 0.66
Batch: 380; loss: 1.13; acc: 0.7
Batch: 400; loss: 1.01; acc: 0.8
Batch: 420; loss: 1.22; acc: 0.7
Batch: 440; loss: 1.05; acc: 0.75
Batch: 460; loss: 1.28; acc: 0.58
Batch: 480; loss: 1.14; acc: 0.69
Batch: 500; loss: 0.92; acc: 0.86
Batch: 520; loss: 1.02; acc: 0.73
Batch: 540; loss: 1.16; acc: 0.73
Batch: 560; loss: 1.25; acc: 0.66
Batch: 580; loss: 1.2; acc: 0.69
Batch: 600; loss: 1.12; acc: 0.73
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 1.14; acc: 0.72
Batch: 660; loss: 1.16; acc: 0.67
Batch: 680; loss: 1.18; acc: 0.64
Batch: 700; loss: 0.98; acc: 0.78
Batch: 720; loss: 1.18; acc: 0.62
Batch: 740; loss: 0.99; acc: 0.73
Batch: 760; loss: 1.29; acc: 0.67
Batch: 780; loss: 1.24; acc: 0.7
Train Epoch over. train_loss: 1.15; train_accuracy: 0.69 

0.0001707444025669247
0.00016452667478006333
Batch: 0; loss: 1.21; acc: 0.67
Batch: 20; loss: 1.31; acc: 0.52
Batch: 40; loss: 0.83; acc: 0.89
Batch: 60; loss: 1.01; acc: 0.77
Batch: 80; loss: 1.01; acc: 0.73
Batch: 100; loss: 1.02; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.73
Val Epoch over. val_loss: 1.0851161244568552; val_accuracy: 0.7205414012738853 

The current subspace-distance is: 0.00016452667478006333 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.72
Batch: 20; loss: 1.12; acc: 0.69
Batch: 40; loss: 1.28; acc: 0.62
Batch: 60; loss: 1.03; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.77
Batch: 100; loss: 1.29; acc: 0.62
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 1.07; acc: 0.77
Batch: 160; loss: 1.22; acc: 0.72
Batch: 180; loss: 1.1; acc: 0.7
Batch: 200; loss: 1.0; acc: 0.77
Batch: 220; loss: 1.17; acc: 0.72
Batch: 240; loss: 1.33; acc: 0.61
Batch: 260; loss: 1.19; acc: 0.7
Batch: 280; loss: 1.03; acc: 0.73
Batch: 300; loss: 1.05; acc: 0.77
Batch: 320; loss: 1.03; acc: 0.78
Batch: 340; loss: 1.04; acc: 0.73
Batch: 360; loss: 1.35; acc: 0.55
Batch: 380; loss: 1.21; acc: 0.64
Batch: 400; loss: 1.19; acc: 0.66
Batch: 420; loss: 1.17; acc: 0.61
Batch: 440; loss: 1.04; acc: 0.73
Batch: 460; loss: 1.06; acc: 0.75
Batch: 480; loss: 0.96; acc: 0.7
Batch: 500; loss: 1.15; acc: 0.62
Batch: 520; loss: 1.15; acc: 0.64
Batch: 540; loss: 1.05; acc: 0.72
Batch: 560; loss: 0.99; acc: 0.73
Batch: 580; loss: 0.97; acc: 0.77
Batch: 600; loss: 0.96; acc: 0.75
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.2; acc: 0.62
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.16; acc: 0.69
Batch: 700; loss: 1.13; acc: 0.72
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 1.32; acc: 0.62
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 1.18; acc: 0.66
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.00018287409329786897
0.00017650813970249146
Batch: 0; loss: 1.18; acc: 0.69
Batch: 20; loss: 1.26; acc: 0.56
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.81
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.02; acc: 0.77
Val Epoch over. val_loss: 1.0507963618655114; val_accuracy: 0.7344745222929936 

The current subspace-distance is: 0.00017650813970249146 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.98; acc: 0.81
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 1.09; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 1.18; acc: 0.67
Batch: 160; loss: 1.18; acc: 0.69
Batch: 180; loss: 0.97; acc: 0.73
Batch: 200; loss: 1.21; acc: 0.66
Batch: 220; loss: 1.29; acc: 0.61
Batch: 240; loss: 1.02; acc: 0.72
Batch: 260; loss: 1.17; acc: 0.67
Batch: 280; loss: 1.12; acc: 0.69
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.14; acc: 0.69
Batch: 340; loss: 1.18; acc: 0.66
Batch: 360; loss: 1.02; acc: 0.78
Batch: 380; loss: 1.12; acc: 0.72
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 1.23; acc: 0.7
Batch: 440; loss: 1.06; acc: 0.7
Batch: 460; loss: 1.07; acc: 0.73
Batch: 480; loss: 0.96; acc: 0.81
Batch: 500; loss: 1.16; acc: 0.7
Batch: 520; loss: 1.27; acc: 0.69
Batch: 540; loss: 0.97; acc: 0.72
Batch: 560; loss: 0.99; acc: 0.72
Batch: 580; loss: 1.21; acc: 0.64
Batch: 600; loss: 1.06; acc: 0.66
Batch: 620; loss: 0.98; acc: 0.83
Batch: 640; loss: 1.05; acc: 0.78
Batch: 660; loss: 1.15; acc: 0.67
Batch: 680; loss: 1.01; acc: 0.72
Batch: 700; loss: 1.3; acc: 0.61
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 0.99; acc: 0.8
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.1; train_accuracy: 0.7 

0.00018035955145023763
0.0001758275757310912
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.25; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 0.98; acc: 0.8
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 0.99; acc: 0.8
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.01; acc: 0.77
Val Epoch over. val_loss: 1.0486287402499253; val_accuracy: 0.7330812101910829 

The current subspace-distance is: 0.0001758275757310912 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.29; acc: 0.59
Batch: 20; loss: 1.09; acc: 0.64
Batch: 40; loss: 1.18; acc: 0.67
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 1.17; acc: 0.64
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.25; acc: 0.58
Batch: 140; loss: 1.04; acc: 0.78
Batch: 160; loss: 1.08; acc: 0.73
Batch: 180; loss: 1.07; acc: 0.67
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 0.93; acc: 0.75
Batch: 240; loss: 1.01; acc: 0.75
Batch: 260; loss: 1.16; acc: 0.64
Batch: 280; loss: 1.09; acc: 0.67
Batch: 300; loss: 1.26; acc: 0.59
Batch: 320; loss: 0.91; acc: 0.8
Batch: 340; loss: 1.1; acc: 0.62
Batch: 360; loss: 1.12; acc: 0.73
Batch: 380; loss: 1.06; acc: 0.69
Batch: 400; loss: 0.99; acc: 0.8
Batch: 420; loss: 1.19; acc: 0.72
Batch: 440; loss: 1.35; acc: 0.58
Batch: 460; loss: 1.11; acc: 0.69
Batch: 480; loss: 1.11; acc: 0.72
Batch: 500; loss: 0.98; acc: 0.75
Batch: 520; loss: 1.07; acc: 0.69
Batch: 540; loss: 1.0; acc: 0.72
Batch: 560; loss: 1.12; acc: 0.67
Batch: 580; loss: 0.99; acc: 0.77
Batch: 600; loss: 1.0; acc: 0.7
Batch: 620; loss: 1.1; acc: 0.73
Batch: 640; loss: 0.93; acc: 0.78
Batch: 660; loss: 1.14; acc: 0.62
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 0.99; acc: 0.78
Batch: 740; loss: 1.14; acc: 0.69
Batch: 760; loss: 0.94; acc: 0.77
Batch: 780; loss: 1.19; acc: 0.59
Train Epoch over. train_loss: 1.09; train_accuracy: 0.7 

0.00018331751925870776
0.0001775045384420082
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.8; acc: 0.8
Batch: 60; loss: 0.96; acc: 0.8
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 0.98; acc: 0.8
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.0; acc: 0.78
Val Epoch over. val_loss: 1.0386918221309687; val_accuracy: 0.7349721337579618 

The current subspace-distance is: 0.0001775045384420082 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.61
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 1.08; acc: 0.69
Batch: 100; loss: 1.15; acc: 0.64
Batch: 120; loss: 1.27; acc: 0.69
Batch: 140; loss: 1.18; acc: 0.67
Batch: 160; loss: 1.16; acc: 0.69
Batch: 180; loss: 1.12; acc: 0.66
Batch: 200; loss: 1.2; acc: 0.62
Batch: 220; loss: 1.2; acc: 0.66
Batch: 240; loss: 1.13; acc: 0.72
Batch: 260; loss: 1.08; acc: 0.73
Batch: 280; loss: 1.24; acc: 0.61
Batch: 300; loss: 1.17; acc: 0.73
Batch: 320; loss: 0.82; acc: 0.89
Batch: 340; loss: 1.1; acc: 0.69
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.07; acc: 0.73
Batch: 400; loss: 1.01; acc: 0.77
Batch: 420; loss: 1.1; acc: 0.72
Batch: 440; loss: 1.09; acc: 0.69
Batch: 460; loss: 1.03; acc: 0.75
Batch: 480; loss: 1.24; acc: 0.7
Batch: 500; loss: 0.97; acc: 0.8
Batch: 520; loss: 1.11; acc: 0.7
Batch: 540; loss: 1.27; acc: 0.61
Batch: 560; loss: 1.13; acc: 0.7
Batch: 580; loss: 0.97; acc: 0.7
Batch: 600; loss: 1.03; acc: 0.75
Batch: 620; loss: 1.15; acc: 0.72
Batch: 640; loss: 1.05; acc: 0.72
Batch: 660; loss: 1.26; acc: 0.67
Batch: 680; loss: 1.08; acc: 0.67
Batch: 700; loss: 1.08; acc: 0.7
Batch: 720; loss: 1.19; acc: 0.67
Batch: 740; loss: 1.22; acc: 0.67
Batch: 760; loss: 0.93; acc: 0.73
Batch: 780; loss: 1.05; acc: 0.67
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.00018982308392878622
0.00018367648590356112
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.24; acc: 0.58
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.96; acc: 0.8
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.0282320938292582; val_accuracy: 0.7379578025477707 

The current subspace-distance is: 0.00018367648590356112 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.04; acc: 0.73
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.64
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 0.99; acc: 0.77
Batch: 160; loss: 1.02; acc: 0.75
Batch: 180; loss: 1.15; acc: 0.62
Batch: 200; loss: 1.07; acc: 0.75
Batch: 220; loss: 1.11; acc: 0.73
Batch: 240; loss: 1.13; acc: 0.64
Batch: 260; loss: 1.12; acc: 0.67
Batch: 280; loss: 1.08; acc: 0.72
Batch: 300; loss: 1.11; acc: 0.66
Batch: 320; loss: 1.07; acc: 0.77
Batch: 340; loss: 1.11; acc: 0.73
Batch: 360; loss: 1.05; acc: 0.73
Batch: 380; loss: 1.19; acc: 0.66
Batch: 400; loss: 1.15; acc: 0.69
Batch: 420; loss: 1.06; acc: 0.77
Batch: 440; loss: 1.07; acc: 0.64
Batch: 460; loss: 1.24; acc: 0.64
Batch: 480; loss: 1.17; acc: 0.64
Batch: 500; loss: 0.94; acc: 0.77
Batch: 520; loss: 0.86; acc: 0.83
Batch: 540; loss: 0.94; acc: 0.86
Batch: 560; loss: 1.03; acc: 0.78
Batch: 580; loss: 1.04; acc: 0.75
Batch: 600; loss: 1.19; acc: 0.69
Batch: 620; loss: 0.9; acc: 0.84
Batch: 640; loss: 1.05; acc: 0.67
Batch: 660; loss: 1.04; acc: 0.75
Batch: 680; loss: 1.0; acc: 0.78
Batch: 700; loss: 1.0; acc: 0.75
Batch: 720; loss: 0.95; acc: 0.77
Batch: 740; loss: 0.97; acc: 0.78
Batch: 760; loss: 0.98; acc: 0.78
Batch: 780; loss: 1.02; acc: 0.73
Train Epoch over. train_loss: 1.08; train_accuracy: 0.71 

0.00019212489132769406
0.00018616477609612048
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.61
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 0.95; acc: 0.78
Val Epoch over. val_loss: 1.0183548335057155; val_accuracy: 0.7398487261146497 

The current subspace-distance is: 0.00018616477609612048 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.04; acc: 0.72
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 1.03; acc: 0.7
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.01; acc: 0.73
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.99; acc: 0.72
Batch: 160; loss: 1.01; acc: 0.73
Batch: 180; loss: 1.04; acc: 0.72
Batch: 200; loss: 1.3; acc: 0.59
Batch: 220; loss: 0.99; acc: 0.78
Batch: 240; loss: 1.1; acc: 0.7
Batch: 260; loss: 1.1; acc: 0.69
Batch: 280; loss: 1.04; acc: 0.72
Batch: 300; loss: 1.14; acc: 0.75
Batch: 320; loss: 1.06; acc: 0.72
Batch: 340; loss: 1.05; acc: 0.77
Batch: 360; loss: 1.02; acc: 0.78
Batch: 380; loss: 1.03; acc: 0.69
Batch: 400; loss: 1.26; acc: 0.61
Batch: 420; loss: 1.27; acc: 0.61
Batch: 440; loss: 1.1; acc: 0.67
Batch: 460; loss: 1.15; acc: 0.56
Batch: 480; loss: 1.04; acc: 0.75
Batch: 500; loss: 1.27; acc: 0.64
Batch: 520; loss: 0.96; acc: 0.78
Batch: 540; loss: 1.11; acc: 0.72
Batch: 560; loss: 0.98; acc: 0.73
Batch: 580; loss: 1.04; acc: 0.67
Batch: 600; loss: 1.17; acc: 0.69
Batch: 620; loss: 1.17; acc: 0.64
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 1.02; acc: 0.77
Batch: 680; loss: 0.97; acc: 0.78
Batch: 700; loss: 1.28; acc: 0.66
Batch: 720; loss: 1.19; acc: 0.69
Batch: 740; loss: 1.11; acc: 0.7
Batch: 760; loss: 1.11; acc: 0.7
Batch: 780; loss: 1.22; acc: 0.66
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.00019514869200065732
0.00018835384980775416
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 0.77; acc: 0.78
Batch: 60; loss: 0.94; acc: 0.8
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.94; acc: 0.81
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 0.96; acc: 0.78
Val Epoch over. val_loss: 1.014242300182391; val_accuracy: 0.738953025477707 

The current subspace-distance is: 0.00018835384980775416 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 1.14; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 1.15; acc: 0.64
Batch: 100; loss: 0.99; acc: 0.7
Batch: 120; loss: 0.83; acc: 0.86
Batch: 140; loss: 1.14; acc: 0.64
Batch: 160; loss: 1.13; acc: 0.66
Batch: 180; loss: 1.25; acc: 0.66
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 1.06; acc: 0.7
Batch: 240; loss: 1.23; acc: 0.59
Batch: 260; loss: 1.02; acc: 0.72
Batch: 280; loss: 0.94; acc: 0.78
Batch: 300; loss: 1.02; acc: 0.7
Batch: 320; loss: 1.08; acc: 0.7
Batch: 340; loss: 0.92; acc: 0.8
Batch: 360; loss: 1.05; acc: 0.66
Batch: 380; loss: 1.05; acc: 0.77
Batch: 400; loss: 1.07; acc: 0.64
Batch: 420; loss: 1.21; acc: 0.69
Batch: 440; loss: 1.08; acc: 0.69
Batch: 460; loss: 1.01; acc: 0.77
Batch: 480; loss: 1.14; acc: 0.67
Batch: 500; loss: 0.95; acc: 0.77
Batch: 520; loss: 1.04; acc: 0.69
Batch: 540; loss: 1.16; acc: 0.67
Batch: 560; loss: 1.05; acc: 0.7
Batch: 580; loss: 1.12; acc: 0.73
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 1.02; acc: 0.7
Batch: 640; loss: 0.99; acc: 0.7
Batch: 660; loss: 1.23; acc: 0.66
Batch: 680; loss: 1.08; acc: 0.75
Batch: 700; loss: 1.15; acc: 0.67
Batch: 720; loss: 0.96; acc: 0.83
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 0.9; acc: 0.84
Batch: 780; loss: 1.03; acc: 0.73
Train Epoch over. train_loss: 1.07; train_accuracy: 0.71 

0.0001952876045834273
0.00018915404507424682
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 0.95; acc: 0.8
Batch: 80; loss: 0.94; acc: 0.77
Batch: 100; loss: 0.94; acc: 0.81
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.0120958249280407; val_accuracy: 0.7409434713375797 

The current subspace-distance is: 0.00018915404507424682 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.97; acc: 0.77
Batch: 60; loss: 1.09; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.61
Batch: 100; loss: 1.3; acc: 0.64
Batch: 120; loss: 1.01; acc: 0.7
Batch: 140; loss: 1.11; acc: 0.67
Batch: 160; loss: 0.98; acc: 0.72
Batch: 180; loss: 0.95; acc: 0.75
Batch: 200; loss: 0.92; acc: 0.75
Batch: 220; loss: 1.26; acc: 0.61
Batch: 240; loss: 1.13; acc: 0.7
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 1.01; acc: 0.69
Batch: 300; loss: 1.07; acc: 0.67
Batch: 320; loss: 1.11; acc: 0.69
Batch: 340; loss: 0.99; acc: 0.67
Batch: 360; loss: 1.07; acc: 0.73
Batch: 380; loss: 1.07; acc: 0.66
Batch: 400; loss: 1.1; acc: 0.66
Batch: 420; loss: 0.99; acc: 0.77
Batch: 440; loss: 0.98; acc: 0.75
Batch: 460; loss: 1.03; acc: 0.67
Batch: 480; loss: 1.02; acc: 0.75
Batch: 500; loss: 1.09; acc: 0.72
Batch: 520; loss: 1.24; acc: 0.61
Batch: 540; loss: 1.16; acc: 0.7
Batch: 560; loss: 0.93; acc: 0.8
Batch: 580; loss: 1.03; acc: 0.77
Batch: 600; loss: 1.11; acc: 0.62
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 1.02; acc: 0.78
Batch: 660; loss: 1.02; acc: 0.73
Batch: 680; loss: 0.93; acc: 0.78
Batch: 700; loss: 0.97; acc: 0.75
Batch: 720; loss: 1.28; acc: 0.61
Batch: 740; loss: 1.33; acc: 0.56
Batch: 760; loss: 1.0; acc: 0.78
Batch: 780; loss: 1.05; acc: 0.73
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00019884055654983968
0.00019253988284617662
Batch: 0; loss: 1.17; acc: 0.61
Batch: 20; loss: 1.2; acc: 0.62
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.95; acc: 0.81
Batch: 120; loss: 1.17; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.78
Val Epoch over. val_loss: 1.0142977799579596; val_accuracy: 0.7402468152866242 

The current subspace-distance is: 0.00019253988284617662 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.09; acc: 0.7
Batch: 20; loss: 1.0; acc: 0.78
Batch: 40; loss: 1.28; acc: 0.59
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.72
Batch: 100; loss: 1.03; acc: 0.72
Batch: 120; loss: 1.13; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.77
Batch: 160; loss: 0.98; acc: 0.75
Batch: 180; loss: 1.07; acc: 0.7
Batch: 200; loss: 1.01; acc: 0.72
Batch: 220; loss: 1.16; acc: 0.73
Batch: 240; loss: 1.03; acc: 0.72
Batch: 260; loss: 1.13; acc: 0.67
Batch: 280; loss: 1.2; acc: 0.66
Batch: 300; loss: 0.98; acc: 0.72
Batch: 320; loss: 0.83; acc: 0.81
Batch: 340; loss: 1.18; acc: 0.67
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.1; acc: 0.62
Batch: 400; loss: 1.14; acc: 0.64
Batch: 420; loss: 1.12; acc: 0.67
Batch: 440; loss: 1.0; acc: 0.72
Batch: 460; loss: 1.12; acc: 0.69
Batch: 480; loss: 1.1; acc: 0.67
Batch: 500; loss: 1.02; acc: 0.78
Batch: 520; loss: 1.11; acc: 0.67
Batch: 540; loss: 1.12; acc: 0.73
Batch: 560; loss: 1.13; acc: 0.69
Batch: 580; loss: 1.0; acc: 0.78
Batch: 600; loss: 1.03; acc: 0.73
Batch: 620; loss: 0.98; acc: 0.7
Batch: 640; loss: 1.09; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.67
Batch: 680; loss: 1.04; acc: 0.64
Batch: 700; loss: 1.13; acc: 0.7
Batch: 720; loss: 1.01; acc: 0.77
Batch: 740; loss: 0.97; acc: 0.69
Batch: 760; loss: 1.05; acc: 0.64
Batch: 780; loss: 1.08; acc: 0.66
Train Epoch over. train_loss: 1.06; train_accuracy: 0.71 

0.00020029830920975655
0.00019593708566389978
Batch: 0; loss: 1.15; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 0.95; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.83
Batch: 120; loss: 1.18; acc: 0.62
Batch: 140; loss: 0.9; acc: 0.77
Val Epoch over. val_loss: 0.9935589094830167; val_accuracy: 0.743531050955414 

The current subspace-distance is: 0.00019593708566389978 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.64
Batch: 40; loss: 1.02; acc: 0.69
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 0.9; acc: 0.81
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.06; acc: 0.67
Batch: 140; loss: 1.09; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.77
Batch: 180; loss: 0.98; acc: 0.7
Batch: 200; loss: 1.07; acc: 0.72
Batch: 220; loss: 1.22; acc: 0.64
Batch: 240; loss: 0.97; acc: 0.73
Batch: 260; loss: 0.81; acc: 0.84
Batch: 280; loss: 1.17; acc: 0.66
Batch: 300; loss: 0.93; acc: 0.8
Batch: 320; loss: 1.12; acc: 0.72
Batch: 340; loss: 1.04; acc: 0.66
Batch: 360; loss: 1.26; acc: 0.59
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 0.99; acc: 0.77
Batch: 420; loss: 1.35; acc: 0.58
Batch: 440; loss: 1.15; acc: 0.73
Batch: 460; loss: 1.1; acc: 0.69
Batch: 480; loss: 0.97; acc: 0.75
Batch: 500; loss: 1.11; acc: 0.72
Batch: 520; loss: 0.89; acc: 0.8
Batch: 540; loss: 1.05; acc: 0.75
Batch: 560; loss: 1.07; acc: 0.72
Batch: 580; loss: 1.19; acc: 0.67
Batch: 600; loss: 1.03; acc: 0.7
Batch: 620; loss: 0.98; acc: 0.8
Batch: 640; loss: 0.98; acc: 0.67
Batch: 660; loss: 1.07; acc: 0.73
Batch: 680; loss: 0.98; acc: 0.72
Batch: 700; loss: 0.98; acc: 0.77
Batch: 720; loss: 0.9; acc: 0.73
Batch: 740; loss: 1.11; acc: 0.72
Batch: 760; loss: 0.94; acc: 0.75
Batch: 780; loss: 1.24; acc: 0.56
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.00020480573584791273
0.0001979449880309403
Batch: 0; loss: 1.17; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.94; acc: 0.78
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.93; acc: 0.81
Batch: 120; loss: 1.18; acc: 0.61
Batch: 140; loss: 0.9; acc: 0.8
Val Epoch over. val_loss: 1.0030258240973113; val_accuracy: 0.7411425159235668 

The current subspace-distance is: 0.0001979449880309403 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 1.1; acc: 0.72
Batch: 40; loss: 1.01; acc: 0.81
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.78
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.96; acc: 0.77
Batch: 160; loss: 1.03; acc: 0.77
Batch: 180; loss: 0.97; acc: 0.75
Batch: 200; loss: 1.28; acc: 0.64
Batch: 220; loss: 1.01; acc: 0.73
Batch: 240; loss: 1.12; acc: 0.69
Batch: 260; loss: 1.04; acc: 0.69
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 1.05; acc: 0.7
Batch: 320; loss: 1.02; acc: 0.69
Batch: 340; loss: 1.11; acc: 0.72
Batch: 360; loss: 0.93; acc: 0.75
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 1.02; acc: 0.75
Batch: 420; loss: 0.9; acc: 0.81
Batch: 440; loss: 1.16; acc: 0.69
Batch: 460; loss: 1.17; acc: 0.64
Batch: 480; loss: 1.08; acc: 0.72
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 1.09; acc: 0.64
Batch: 540; loss: 0.96; acc: 0.83
Batch: 560; loss: 1.02; acc: 0.67
Batch: 580; loss: 1.08; acc: 0.72
Batch: 600; loss: 1.08; acc: 0.75
Batch: 620; loss: 0.92; acc: 0.78
Batch: 640; loss: 1.11; acc: 0.62
Batch: 660; loss: 1.09; acc: 0.58
Batch: 680; loss: 0.95; acc: 0.8
Batch: 700; loss: 0.94; acc: 0.75
Batch: 720; loss: 0.98; acc: 0.77
Batch: 740; loss: 1.06; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.8
Batch: 780; loss: 0.99; acc: 0.66
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.00020546923042275012
0.00019873635028488934
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 0.87; acc: 0.8
Val Epoch over. val_loss: 0.9914909069705161; val_accuracy: 0.7424363057324841 

The current subspace-distance is: 0.00019873635028488934 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.69
Batch: 20; loss: 1.01; acc: 0.72
Batch: 40; loss: 1.05; acc: 0.7
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.72
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 0.94; acc: 0.78
Batch: 140; loss: 1.26; acc: 0.56
Batch: 160; loss: 1.2; acc: 0.61
Batch: 180; loss: 1.09; acc: 0.73
Batch: 200; loss: 1.19; acc: 0.64
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 1.05; acc: 0.7
Batch: 260; loss: 1.13; acc: 0.69
Batch: 280; loss: 1.07; acc: 0.64
Batch: 300; loss: 1.05; acc: 0.73
Batch: 320; loss: 1.08; acc: 0.75
Batch: 340; loss: 1.22; acc: 0.64
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 0.92; acc: 0.77
Batch: 400; loss: 0.96; acc: 0.77
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 1.09; acc: 0.69
Batch: 460; loss: 0.98; acc: 0.75
Batch: 480; loss: 0.89; acc: 0.8
Batch: 500; loss: 1.15; acc: 0.67
Batch: 520; loss: 0.9; acc: 0.77
Batch: 540; loss: 1.17; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.75
Batch: 580; loss: 0.98; acc: 0.77
Batch: 600; loss: 1.0; acc: 0.75
Batch: 620; loss: 0.9; acc: 0.78
Batch: 640; loss: 0.99; acc: 0.81
Batch: 660; loss: 1.01; acc: 0.64
Batch: 680; loss: 0.98; acc: 0.77
Batch: 700; loss: 0.9; acc: 0.77
Batch: 720; loss: 0.96; acc: 0.73
Batch: 740; loss: 0.98; acc: 0.75
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.08; acc: 0.64
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

0.00020737007434945554
0.00019968055130448192
Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.81
Batch: 120; loss: 1.18; acc: 0.61
Batch: 140; loss: 0.87; acc: 0.8
Val Epoch over. val_loss: 0.9814580322071246; val_accuracy: 0.7478105095541401 

The current subspace-distance is: 0.00019968055130448192 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.06; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 1.21; acc: 0.66
Batch: 100; loss: 0.96; acc: 0.7
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 1.17; acc: 0.67
Batch: 160; loss: 1.0; acc: 0.69
Batch: 180; loss: 1.06; acc: 0.67
Batch: 200; loss: 1.1; acc: 0.73
Batch: 220; loss: 1.03; acc: 0.75
Batch: 240; loss: 1.08; acc: 0.69
Batch: 260; loss: 1.14; acc: 0.67
Batch: 280; loss: 1.03; acc: 0.75
Batch: 300; loss: 1.02; acc: 0.69
Batch: 320; loss: 1.03; acc: 0.72
Batch: 340; loss: 1.04; acc: 0.66
Batch: 360; loss: 0.99; acc: 0.75
Batch: 380; loss: 1.08; acc: 0.67
Batch: 400; loss: 1.23; acc: 0.61
Batch: 420; loss: 1.04; acc: 0.62
Batch: 440; loss: 0.96; acc: 0.73
Batch: 460; loss: 1.19; acc: 0.69
Batch: 480; loss: 1.0; acc: 0.8
Batch: 500; loss: 0.93; acc: 0.81
Batch: 520; loss: 1.01; acc: 0.77
Batch: 540; loss: 0.96; acc: 0.73
Batch: 560; loss: 1.01; acc: 0.73
Batch: 580; loss: 1.0; acc: 0.7
Batch: 600; loss: 0.87; acc: 0.8
Batch: 620; loss: 1.29; acc: 0.56
Batch: 640; loss: 0.86; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.73
Batch: 680; loss: 1.01; acc: 0.69
Batch: 700; loss: 0.93; acc: 0.73
Batch: 720; loss: 0.98; acc: 0.73
Batch: 740; loss: 1.06; acc: 0.72
Batch: 760; loss: 0.97; acc: 0.72
Batch: 780; loss: 0.98; acc: 0.75
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

0.0002067719033220783
0.00019980309298262
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 1.17; acc: 0.61
Batch: 140; loss: 0.86; acc: 0.8
Val Epoch over. val_loss: 0.977646520562992; val_accuracy: 0.7476114649681529 

The current subspace-distance is: 0.00019980309298262 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.06; acc: 0.7
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 1.1; acc: 0.7
Batch: 60; loss: 1.12; acc: 0.73
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 1.0; acc: 0.67
Batch: 160; loss: 0.94; acc: 0.77
Batch: 180; loss: 1.01; acc: 0.77
Batch: 200; loss: 1.09; acc: 0.67
Batch: 220; loss: 1.09; acc: 0.75
Batch: 240; loss: 1.15; acc: 0.69
Batch: 260; loss: 0.91; acc: 0.88
Batch: 280; loss: 0.96; acc: 0.77
Batch: 300; loss: 1.12; acc: 0.64
Batch: 320; loss: 1.08; acc: 0.7
Batch: 340; loss: 0.92; acc: 0.72
Batch: 360; loss: 0.9; acc: 0.73
Batch: 380; loss: 1.28; acc: 0.56
Batch: 400; loss: 1.1; acc: 0.75
Batch: 420; loss: 1.06; acc: 0.7
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.09; acc: 0.73
Batch: 480; loss: 1.04; acc: 0.75
Batch: 500; loss: 1.07; acc: 0.69
Batch: 520; loss: 1.05; acc: 0.73
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 0.96; acc: 0.73
Batch: 580; loss: 1.27; acc: 0.58
Batch: 600; loss: 1.13; acc: 0.62
Batch: 620; loss: 1.15; acc: 0.69
Batch: 640; loss: 1.02; acc: 0.73
Batch: 660; loss: 1.12; acc: 0.69
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.1; acc: 0.72
Batch: 720; loss: 0.99; acc: 0.72
Batch: 740; loss: 1.01; acc: 0.78
Batch: 760; loss: 1.24; acc: 0.59
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 1.04; train_accuracy: 0.72 

0.0002114350936608389
0.00020435158512555063
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.92; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.8
Batch: 120; loss: 1.18; acc: 0.59
Batch: 140; loss: 0.87; acc: 0.8
Val Epoch over. val_loss: 0.9860576748088666; val_accuracy: 0.7441281847133758 

The current subspace-distance is: 0.00020435158512555063 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 0.99; acc: 0.77
Batch: 40; loss: 0.9; acc: 0.8
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.98; acc: 0.73
Batch: 100; loss: 0.98; acc: 0.78
Batch: 120; loss: 1.02; acc: 0.77
Batch: 140; loss: 0.88; acc: 0.84
Batch: 160; loss: 1.11; acc: 0.72
Batch: 180; loss: 1.09; acc: 0.73
Batch: 200; loss: 1.01; acc: 0.77
Batch: 220; loss: 0.97; acc: 0.81
Batch: 240; loss: 1.04; acc: 0.7
Batch: 260; loss: 1.03; acc: 0.64
Batch: 280; loss: 1.08; acc: 0.72
Batch: 300; loss: 1.06; acc: 0.67
Batch: 320; loss: 0.96; acc: 0.78
Batch: 340; loss: 0.95; acc: 0.73
Batch: 360; loss: 1.13; acc: 0.66
Batch: 380; loss: 1.07; acc: 0.75
Batch: 400; loss: 0.9; acc: 0.78
Batch: 420; loss: 1.09; acc: 0.7
Batch: 440; loss: 0.96; acc: 0.72
Batch: 460; loss: 1.25; acc: 0.62
Batch: 480; loss: 1.09; acc: 0.69
Batch: 500; loss: 1.04; acc: 0.73
Batch: 520; loss: 1.28; acc: 0.64
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 1.12; acc: 0.67
Batch: 580; loss: 1.16; acc: 0.64
Batch: 600; loss: 0.85; acc: 0.77
Batch: 620; loss: 0.91; acc: 0.77
Batch: 640; loss: 1.06; acc: 0.66
Batch: 660; loss: 0.88; acc: 0.81
Batch: 680; loss: 1.08; acc: 0.69
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 0.95; acc: 0.73
Batch: 740; loss: 1.22; acc: 0.59
Batch: 760; loss: 1.07; acc: 0.72
Batch: 780; loss: 1.07; acc: 0.73
Train Epoch over. train_loss: 1.04; train_accuracy: 0.72 

0.00020743976347148418
0.0002014119818340987
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 0.92; acc: 0.81
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 0.87; acc: 0.81
Val Epoch over. val_loss: 0.9869010968572774; val_accuracy: 0.7456210191082803 

The current subspace-distance is: 0.0002014119818340987 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 0.92; acc: 0.81
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 0.98; acc: 0.78
Batch: 100; loss: 0.87; acc: 0.86
Batch: 120; loss: 1.08; acc: 0.67
Batch: 140; loss: 0.98; acc: 0.78
Batch: 160; loss: 1.1; acc: 0.67
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 1.04; acc: 0.75
Batch: 240; loss: 1.27; acc: 0.64
Batch: 260; loss: 1.16; acc: 0.56
Batch: 280; loss: 1.0; acc: 0.75
Batch: 300; loss: 0.84; acc: 0.78
Batch: 320; loss: 0.87; acc: 0.77
Batch: 340; loss: 1.25; acc: 0.67
Batch: 360; loss: 1.24; acc: 0.61
Batch: 380; loss: 1.02; acc: 0.67
Batch: 400; loss: 1.0; acc: 0.75
Batch: 420; loss: 0.97; acc: 0.75
Batch: 440; loss: 1.01; acc: 0.75
Batch: 460; loss: 1.15; acc: 0.69
Batch: 480; loss: 1.01; acc: 0.69
Batch: 500; loss: 0.97; acc: 0.73
Batch: 520; loss: 1.01; acc: 0.7
Batch: 540; loss: 1.13; acc: 0.7
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 0.96; acc: 0.7
Batch: 600; loss: 0.98; acc: 0.72
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 1.0; acc: 0.72
Batch: 660; loss: 0.88; acc: 0.8
Batch: 680; loss: 1.0; acc: 0.7
Batch: 700; loss: 0.97; acc: 0.75
Batch: 720; loss: 0.88; acc: 0.86
Batch: 740; loss: 0.94; acc: 0.77
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 0.89; acc: 0.77
Train Epoch over. train_loss: 1.04; train_accuracy: 0.72 

0.000209163932595402
0.0002032627526205033
Batch: 0; loss: 1.14; acc: 0.61
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 1.17; acc: 0.59
Batch: 140; loss: 0.86; acc: 0.8
Val Epoch over. val_loss: 0.9801290908436866; val_accuracy: 0.7461186305732485 

The current subspace-distance is: 0.0002032627526205033 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.94; acc: 0.77
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 1.04; acc: 0.72
Batch: 160; loss: 1.02; acc: 0.73
Batch: 180; loss: 1.05; acc: 0.75
Batch: 200; loss: 0.98; acc: 0.73
Batch: 220; loss: 1.13; acc: 0.64
Batch: 240; loss: 1.0; acc: 0.77
Batch: 260; loss: 1.03; acc: 0.69
Batch: 280; loss: 1.17; acc: 0.64
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.2; acc: 0.59
Batch: 340; loss: 0.99; acc: 0.75
Batch: 360; loss: 1.0; acc: 0.66
Batch: 380; loss: 1.06; acc: 0.67
Batch: 400; loss: 1.08; acc: 0.73
Batch: 420; loss: 1.26; acc: 0.62
Batch: 440; loss: 0.9; acc: 0.8
Batch: 460; loss: 0.76; acc: 0.88
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 1.18; acc: 0.66
Batch: 520; loss: 0.97; acc: 0.72
Batch: 540; loss: 0.99; acc: 0.77
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 1.19; acc: 0.67
Batch: 600; loss: 1.02; acc: 0.7
Batch: 620; loss: 1.07; acc: 0.69
Batch: 640; loss: 1.04; acc: 0.75
Batch: 660; loss: 0.87; acc: 0.69
Batch: 680; loss: 1.15; acc: 0.58
Batch: 700; loss: 1.11; acc: 0.7
Batch: 720; loss: 1.12; acc: 0.66
Batch: 740; loss: 1.09; acc: 0.69
Batch: 760; loss: 1.11; acc: 0.75
Batch: 780; loss: 0.96; acc: 0.75
Train Epoch over. train_loss: 1.04; train_accuracy: 0.72 

0.00021003381698392332
0.0002032101183431223
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.64
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 1.18; acc: 0.59
Batch: 140; loss: 0.88; acc: 0.8
Val Epoch over. val_loss: 0.9900499160882015; val_accuracy: 0.741640127388535 

The current subspace-distance is: 0.0002032101183431223 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 1.04; acc: 0.72
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 1.18; acc: 0.67
Batch: 100; loss: 1.36; acc: 0.59
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 1.07; acc: 0.69
Batch: 160; loss: 1.01; acc: 0.72
Batch: 180; loss: 1.13; acc: 0.7
Batch: 200; loss: 0.93; acc: 0.8
Batch: 220; loss: 0.85; acc: 0.81
Batch: 240; loss: 1.01; acc: 0.66
Batch: 260; loss: 1.05; acc: 0.78
Batch: 280; loss: 1.0; acc: 0.77
Batch: 300; loss: 0.97; acc: 0.77
Batch: 320; loss: 1.02; acc: 0.72
Batch: 340; loss: 1.16; acc: 0.62
Batch: 360; loss: 0.92; acc: 0.77
Batch: 380; loss: 1.09; acc: 0.67
Batch: 400; loss: 1.07; acc: 0.64
Batch: 420; loss: 1.05; acc: 0.67
Batch: 440; loss: 1.09; acc: 0.67
Batch: 460; loss: 0.98; acc: 0.77
Batch: 480; loss: 1.07; acc: 0.73
Batch: 500; loss: 1.12; acc: 0.66
Batch: 520; loss: 1.36; acc: 0.66
Batch: 540; loss: 0.89; acc: 0.75
Batch: 560; loss: 1.08; acc: 0.66
Batch: 580; loss: 0.93; acc: 0.77
Batch: 600; loss: 1.02; acc: 0.77
Batch: 620; loss: 0.96; acc: 0.77
Batch: 640; loss: 0.93; acc: 0.73
Batch: 660; loss: 0.92; acc: 0.8
Batch: 680; loss: 0.97; acc: 0.75
Batch: 700; loss: 1.01; acc: 0.78
Batch: 720; loss: 0.98; acc: 0.73
Batch: 740; loss: 1.02; acc: 0.7
Batch: 760; loss: 1.15; acc: 0.61
Batch: 780; loss: 1.16; acc: 0.69
Train Epoch over. train_loss: 1.04; train_accuracy: 0.72 

0.00021152790577616543
0.00020177208352833986
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.66
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.94; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.81
Batch: 100; loss: 0.91; acc: 0.81
Batch: 120; loss: 1.18; acc: 0.59
Batch: 140; loss: 0.86; acc: 0.8
Val Epoch over. val_loss: 0.9855922961690623; val_accuracy: 0.742734872611465 

The current subspace-distance is: 0.00020177208352833986 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.06; acc: 0.67
Batch: 20; loss: 1.07; acc: 0.73
Batch: 40; loss: 0.79; acc: 0.86
Batch: 60; loss: 0.91; acc: 0.8
Batch: 80; loss: 0.95; acc: 0.75
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 1.0; acc: 0.69
Batch: 180; loss: 1.2; acc: 0.64
Batch: 200; loss: 0.93; acc: 0.77
Batch: 220; loss: 1.1; acc: 0.7
Batch: 240; loss: 1.17; acc: 0.59
Batch: 260; loss: 1.05; acc: 0.69
Batch: 280; loss: 1.09; acc: 0.69
Batch: 300; loss: 1.07; acc: 0.69
Batch: 320; loss: 1.07; acc: 0.7
Batch: 340; loss: 1.24; acc: 0.67
Batch: 360; loss: 1.17; acc: 0.69
Batch: 380; loss: 1.07; acc: 0.69
Batch: 400; loss: 0.87; acc: 0.8
Batch: 420; loss: 0.99; acc: 0.72
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 0.98; acc: 0.78
Batch: 480; loss: 1.02; acc: 0.66
Batch: 500; loss: 1.11; acc: 0.69
Batch: 520; loss: 1.01; acc: 0.73
Batch: 540; loss: 1.11; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.61
Batch: 580; loss: 1.15; acc: 0.66
Batch: 600; loss: 0.98; acc: 0.73
Batch: 620; loss: 1.09; acc: 0.69
Batch: 640; loss: 1.13; acc: 0.7
Batch: 660; loss: 0.89; acc: 0.78
Batch: 680; loss: 1.01; acc: 0.75
Batch: 700; loss: 0.97; acc: 0.7
Batch: 720; loss: 0.95; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.8
Batch: 760; loss: 0.94; acc: 0.78
Batch: 780; loss: 1.04; acc: 0.67
Train Epoch over. train_loss: 1.03; train_accuracy: 0.72 

0.00021346137509681284
0.0002053664647974074
Batch: 0; loss: 1.13; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 1.18; acc: 0.56
Batch: 140; loss: 0.85; acc: 0.8
Val Epoch over. val_loss: 0.9758010563577056; val_accuracy: 0.746218152866242 

The current subspace-distance is: 0.0002053664647974074 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.97; acc: 0.7
Batch: 20; loss: 0.92; acc: 0.81
Batch: 40; loss: 1.16; acc: 0.59
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 1.18; acc: 0.58
Batch: 140; loss: 1.06; acc: 0.75
Batch: 160; loss: 0.93; acc: 0.77
Batch: 180; loss: 1.13; acc: 0.67
Batch: 200; loss: 0.88; acc: 0.84
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 1.1; acc: 0.66
Batch: 260; loss: 1.1; acc: 0.67
Batch: 280; loss: 1.16; acc: 0.69
Batch: 300; loss: 0.97; acc: 0.75
Batch: 320; loss: 1.26; acc: 0.62
Batch: 340; loss: 1.44; acc: 0.53
Batch: 360; loss: 0.97; acc: 0.73
Batch: 380; loss: 1.08; acc: 0.64
Batch: 400; loss: 1.08; acc: 0.67
Batch: 420; loss: 1.03; acc: 0.78
Batch: 440; loss: 1.11; acc: 0.7
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.01; acc: 0.72
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 1.02; acc: 0.69
Batch: 540; loss: 0.95; acc: 0.78
Batch: 560; loss: 1.18; acc: 0.62
Batch: 580; loss: 1.02; acc: 0.66
Batch: 600; loss: 0.97; acc: 0.81
Batch: 620; loss: 0.97; acc: 0.77
Batch: 640; loss: 1.35; acc: 0.59
Batch: 660; loss: 1.15; acc: 0.72
Batch: 680; loss: 1.09; acc: 0.67
Batch: 700; loss: 0.93; acc: 0.81
Batch: 720; loss: 1.32; acc: 0.58
Batch: 740; loss: 0.96; acc: 0.81
Batch: 760; loss: 1.16; acc: 0.67
Batch: 780; loss: 1.15; acc: 0.72
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

0.0002163268072763458
0.00020695147395599633
Batch: 0; loss: 1.14; acc: 0.59
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.8
Batch: 120; loss: 1.19; acc: 0.56
Batch: 140; loss: 0.85; acc: 0.8
Val Epoch over. val_loss: 0.9777265642858615; val_accuracy: 0.7445262738853503 

The current subspace-distance is: 0.00020695147395599633 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.73
Batch: 20; loss: 1.01; acc: 0.8
Batch: 40; loss: 0.96; acc: 0.77
Batch: 60; loss: 1.04; acc: 0.67
Batch: 80; loss: 1.01; acc: 0.73
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 1.03; acc: 0.72
Batch: 160; loss: 1.17; acc: 0.72
Batch: 180; loss: 1.05; acc: 0.75
Batch: 200; loss: 1.01; acc: 0.73
Batch: 220; loss: 0.84; acc: 0.77
Batch: 240; loss: 0.9; acc: 0.8
Batch: 260; loss: 1.05; acc: 0.7
Batch: 280; loss: 0.96; acc: 0.77
Batch: 300; loss: 1.08; acc: 0.77
Batch: 320; loss: 1.22; acc: 0.62
Batch: 340; loss: 0.96; acc: 0.77
Batch: 360; loss: 1.27; acc: 0.59
Batch: 380; loss: 1.06; acc: 0.7
Batch: 400; loss: 0.89; acc: 0.81
Batch: 420; loss: 0.95; acc: 0.75
Batch: 440; loss: 0.82; acc: 0.78
Batch: 460; loss: 0.9; acc: 0.8
Batch: 480; loss: 0.89; acc: 0.86
Batch: 500; loss: 0.99; acc: 0.67
Batch: 520; loss: 1.11; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.66
Batch: 560; loss: 1.11; acc: 0.66
Batch: 580; loss: 0.93; acc: 0.78
Batch: 600; loss: 1.1; acc: 0.67
Batch: 620; loss: 0.93; acc: 0.75
Batch: 640; loss: 1.06; acc: 0.69
Batch: 660; loss: 1.12; acc: 0.64
Batch: 680; loss: 0.97; acc: 0.78
Batch: 700; loss: 1.05; acc: 0.66
Batch: 720; loss: 1.08; acc: 0.7
Batch: 740; loss: 1.17; acc: 0.62
Batch: 760; loss: 1.05; acc: 0.73
Batch: 780; loss: 0.86; acc: 0.75
Train Epoch over. train_loss: 1.03; train_accuracy: 0.72 

0.00021435115195345134
0.00020819036581087857
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 0.86; acc: 0.81
Val Epoch over. val_loss: 0.9836010511513729; val_accuracy: 0.7407444267515924 

The current subspace-distance is: 0.00020819036581087857 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_12_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.17

The number of parameters is: 270144

The number of individual parameters is:

10
180
10
10
15
33600
15
15
29
97440
29
29
64
133632
64
64
4096
64
640
10
64
64

nonzero elements in E: 54028795
elements in E: 54028800
fraction nonzero: 0.9999999074567638
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.11; acc: 0.28
Batch: 60; loss: 1.98; acc: 0.34
Batch: 80; loss: 1.96; acc: 0.31
Batch: 100; loss: 2.06; acc: 0.27
Batch: 120; loss: 2.02; acc: 0.36
Batch: 140; loss: 1.8; acc: 0.44
Batch: 160; loss: 1.73; acc: 0.52
Batch: 180; loss: 1.66; acc: 0.56
Batch: 200; loss: 1.68; acc: 0.58
Batch: 220; loss: 1.59; acc: 0.61
Batch: 240; loss: 1.61; acc: 0.52
Batch: 260; loss: 1.69; acc: 0.48
Batch: 280; loss: 1.63; acc: 0.58
Batch: 300; loss: 1.62; acc: 0.61
Batch: 320; loss: 1.63; acc: 0.5
Batch: 340; loss: 1.47; acc: 0.66
Batch: 360; loss: 1.61; acc: 0.56
Batch: 380; loss: 1.56; acc: 0.58
Batch: 400; loss: 1.51; acc: 0.64
Batch: 420; loss: 1.47; acc: 0.64
Batch: 440; loss: 1.57; acc: 0.56
Batch: 460; loss: 1.38; acc: 0.67
Batch: 480; loss: 1.48; acc: 0.62
Batch: 500; loss: 1.5; acc: 0.67
Batch: 520; loss: 1.5; acc: 0.58
Batch: 540; loss: 1.44; acc: 0.69
Batch: 560; loss: 1.49; acc: 0.58
Batch: 580; loss: 1.34; acc: 0.72
Batch: 600; loss: 1.33; acc: 0.7
Batch: 620; loss: 1.46; acc: 0.61
Batch: 640; loss: 1.36; acc: 0.66
Batch: 660; loss: 1.38; acc: 0.72
Batch: 680; loss: 1.38; acc: 0.7
Batch: 700; loss: 1.37; acc: 0.7
Batch: 720; loss: 1.31; acc: 0.67
Batch: 740; loss: 1.37; acc: 0.66
Batch: 760; loss: 1.37; acc: 0.62
Batch: 780; loss: 1.27; acc: 0.72
Train Epoch over. train_loss: 1.61; train_accuracy: 0.55 

6.80419325362891e-05
6.326002767309546e-05
Batch: 0; loss: 1.39; acc: 0.66
Batch: 20; loss: 1.41; acc: 0.62
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.19; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.77
Batch: 100; loss: 1.31; acc: 0.72
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 1.15; acc: 0.7
Val Epoch over. val_loss: 1.2772974914805904; val_accuracy: 0.7033240445859873 

The current subspace-distance is: 6.326002767309546e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.67
Batch: 20; loss: 1.44; acc: 0.61
Batch: 40; loss: 1.32; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.73
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.2; acc: 0.84
Batch: 140; loss: 1.21; acc: 0.77
Batch: 160; loss: 1.38; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.73
Batch: 200; loss: 1.26; acc: 0.72
Batch: 220; loss: 1.37; acc: 0.7
Batch: 240; loss: 1.22; acc: 0.73
Batch: 260; loss: 1.38; acc: 0.7
Batch: 280; loss: 1.21; acc: 0.73
Batch: 300; loss: 1.21; acc: 0.72
Batch: 320; loss: 1.35; acc: 0.58
Batch: 340; loss: 1.27; acc: 0.72
Batch: 360; loss: 1.28; acc: 0.64
Batch: 380; loss: 1.14; acc: 0.75
Batch: 400; loss: 1.25; acc: 0.7
Batch: 420; loss: 1.31; acc: 0.75
Batch: 440; loss: 1.12; acc: 0.81
Batch: 460; loss: 1.24; acc: 0.7
Batch: 480; loss: 1.13; acc: 0.73
Batch: 500; loss: 1.32; acc: 0.67
Batch: 520; loss: 1.25; acc: 0.69
Batch: 540; loss: 1.28; acc: 0.61
Batch: 560; loss: 1.13; acc: 0.75
Batch: 580; loss: 1.29; acc: 0.69
Batch: 600; loss: 1.18; acc: 0.72
Batch: 620; loss: 1.18; acc: 0.75
Batch: 640; loss: 1.12; acc: 0.83
Batch: 660; loss: 1.23; acc: 0.7
Batch: 680; loss: 1.14; acc: 0.77
Batch: 700; loss: 1.16; acc: 0.81
Batch: 720; loss: 1.2; acc: 0.78
Batch: 740; loss: 1.09; acc: 0.77
Batch: 760; loss: 1.22; acc: 0.62
Batch: 780; loss: 1.0; acc: 0.84
Train Epoch over. train_loss: 1.25; train_accuracy: 0.71 

9.195823804475367e-05
8.69254072313197e-05
Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.26; acc: 0.67
Batch: 40; loss: 0.9; acc: 0.88
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 1.16; acc: 0.77
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 0.99; acc: 0.78
Val Epoch over. val_loss: 1.1186784752614938; val_accuracy: 0.754578025477707 

The current subspace-distance is: 8.69254072313197e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.77
Batch: 20; loss: 1.18; acc: 0.77
Batch: 40; loss: 1.11; acc: 0.75
Batch: 60; loss: 1.21; acc: 0.7
Batch: 80; loss: 1.22; acc: 0.72
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 1.16; acc: 0.78
Batch: 160; loss: 1.12; acc: 0.77
Batch: 180; loss: 1.44; acc: 0.55
Batch: 200; loss: 1.08; acc: 0.73
Batch: 220; loss: 1.22; acc: 0.67
Batch: 240; loss: 1.05; acc: 0.81
Batch: 260; loss: 1.2; acc: 0.72
Batch: 280; loss: 1.13; acc: 0.75
Batch: 300; loss: 1.19; acc: 0.73
Batch: 320; loss: 1.19; acc: 0.66
Batch: 340; loss: 1.18; acc: 0.7
Batch: 360; loss: 1.16; acc: 0.69
Batch: 380; loss: 1.07; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.75
Batch: 420; loss: 1.21; acc: 0.72
Batch: 440; loss: 1.04; acc: 0.78
Batch: 460; loss: 1.13; acc: 0.72
Batch: 480; loss: 1.15; acc: 0.7
Batch: 500; loss: 1.16; acc: 0.81
Batch: 520; loss: 1.12; acc: 0.72
Batch: 540; loss: 1.21; acc: 0.72
Batch: 560; loss: 1.08; acc: 0.77
Batch: 580; loss: 1.09; acc: 0.77
Batch: 600; loss: 0.92; acc: 0.86
Batch: 620; loss: 1.07; acc: 0.78
Batch: 640; loss: 1.07; acc: 0.73
Batch: 660; loss: 1.0; acc: 0.8
Batch: 680; loss: 1.15; acc: 0.73
Batch: 700; loss: 1.06; acc: 0.78
Batch: 720; loss: 1.08; acc: 0.75
Batch: 740; loss: 1.22; acc: 0.67
Batch: 760; loss: 1.05; acc: 0.8
Batch: 780; loss: 1.04; acc: 0.7
Train Epoch over. train_loss: 1.12; train_accuracy: 0.75 

0.00011113799700979143
0.00010503693192731589
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 0.78; acc: 0.91
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 0.98; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.84
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 0.91; acc: 0.81
Val Epoch over. val_loss: 1.0057254691792141; val_accuracy: 0.7923964968152867 

The current subspace-distance is: 0.00010503693192731589 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.72
Batch: 20; loss: 0.94; acc: 0.86
Batch: 40; loss: 1.07; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.8
Batch: 80; loss: 1.05; acc: 0.78
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 0.97; acc: 0.81
Batch: 160; loss: 0.95; acc: 0.77
Batch: 180; loss: 1.13; acc: 0.73
Batch: 200; loss: 1.09; acc: 0.69
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 1.06; acc: 0.75
Batch: 260; loss: 1.08; acc: 0.75
Batch: 280; loss: 0.97; acc: 0.78
Batch: 300; loss: 0.97; acc: 0.77
Batch: 320; loss: 1.1; acc: 0.72
Batch: 340; loss: 1.1; acc: 0.73
Batch: 360; loss: 1.03; acc: 0.73
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.08; acc: 0.8
Batch: 420; loss: 0.97; acc: 0.78
Batch: 440; loss: 0.99; acc: 0.8
Batch: 460; loss: 0.96; acc: 0.81
Batch: 480; loss: 1.1; acc: 0.73
Batch: 500; loss: 1.06; acc: 0.78
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 0.92; acc: 0.81
Batch: 560; loss: 1.06; acc: 0.77
Batch: 580; loss: 1.01; acc: 0.72
Batch: 600; loss: 0.94; acc: 0.77
Batch: 620; loss: 0.83; acc: 0.84
Batch: 640; loss: 0.89; acc: 0.78
Batch: 660; loss: 1.06; acc: 0.73
Batch: 680; loss: 0.91; acc: 0.78
Batch: 700; loss: 0.97; acc: 0.84
Batch: 720; loss: 1.02; acc: 0.73
Batch: 740; loss: 0.92; acc: 0.84
Batch: 760; loss: 1.01; acc: 0.81
Batch: 780; loss: 0.93; acc: 0.8
Train Epoch over. train_loss: 1.02; train_accuracy: 0.77 

0.00012311282625887543
0.00011858822836074978
Batch: 0; loss: 1.0; acc: 0.81
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.7; acc: 0.94
Batch: 60; loss: 0.95; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.84
Batch: 120; loss: 1.06; acc: 0.81
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 0.9134435665075946; val_accuracy: 0.8102109872611465 

The current subspace-distance is: 0.00011858822836074978 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.83
Batch: 20; loss: 0.92; acc: 0.83
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 0.88; acc: 0.81
Batch: 80; loss: 0.97; acc: 0.8
Batch: 100; loss: 0.99; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.81
Batch: 140; loss: 0.95; acc: 0.75
Batch: 160; loss: 0.89; acc: 0.84
Batch: 180; loss: 1.03; acc: 0.78
Batch: 200; loss: 1.07; acc: 0.7
Batch: 220; loss: 1.07; acc: 0.77
Batch: 240; loss: 0.86; acc: 0.88
Batch: 260; loss: 0.85; acc: 0.86
Batch: 280; loss: 0.78; acc: 0.84
Batch: 300; loss: 0.9; acc: 0.86
Batch: 320; loss: 0.93; acc: 0.83
Batch: 340; loss: 0.93; acc: 0.77
Batch: 360; loss: 0.97; acc: 0.84
Batch: 380; loss: 0.99; acc: 0.77
Batch: 400; loss: 0.91; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.91
Batch: 440; loss: 0.82; acc: 0.84
Batch: 460; loss: 1.04; acc: 0.72
Batch: 480; loss: 0.87; acc: 0.8
Batch: 500; loss: 0.88; acc: 0.78
Batch: 520; loss: 1.06; acc: 0.73
Batch: 540; loss: 1.0; acc: 0.81
Batch: 560; loss: 0.96; acc: 0.73
Batch: 580; loss: 0.73; acc: 0.88
Batch: 600; loss: 1.0; acc: 0.78
Batch: 620; loss: 1.05; acc: 0.77
Batch: 640; loss: 0.86; acc: 0.81
Batch: 660; loss: 0.87; acc: 0.84
Batch: 680; loss: 0.88; acc: 0.86
Batch: 700; loss: 0.82; acc: 0.83
Batch: 720; loss: 0.96; acc: 0.73
Batch: 740; loss: 0.86; acc: 0.88
Batch: 760; loss: 0.95; acc: 0.78
Batch: 780; loss: 0.84; acc: 0.88
Train Epoch over. train_loss: 0.94; train_accuracy: 0.79 

0.00013508653501048684
0.00013163971016183496
Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 0.63; acc: 0.92
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.89
Batch: 120; loss: 0.98; acc: 0.81
Batch: 140; loss: 0.74; acc: 0.81
Val Epoch over. val_loss: 0.8458064855283992; val_accuracy: 0.8288216560509554 

The current subspace-distance is: 0.00013163971016183496 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.89
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.88; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.96; acc: 0.73
Batch: 100; loss: 0.96; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.86
Batch: 140; loss: 0.81; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.83
Batch: 180; loss: 0.94; acc: 0.75
Batch: 200; loss: 0.83; acc: 0.81
Batch: 220; loss: 0.89; acc: 0.77
Batch: 240; loss: 0.87; acc: 0.84
Batch: 260; loss: 0.96; acc: 0.75
Batch: 280; loss: 0.78; acc: 0.84
Batch: 300; loss: 0.86; acc: 0.8
Batch: 320; loss: 1.01; acc: 0.77
Batch: 340; loss: 0.9; acc: 0.77
Batch: 360; loss: 0.97; acc: 0.72
Batch: 380; loss: 0.94; acc: 0.75
Batch: 400; loss: 0.91; acc: 0.81
Batch: 420; loss: 0.86; acc: 0.86
Batch: 440; loss: 0.92; acc: 0.81
Batch: 460; loss: 0.92; acc: 0.77
Batch: 480; loss: 0.83; acc: 0.81
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 0.93; acc: 0.75
Batch: 540; loss: 0.8; acc: 0.86
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.82; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.84
Batch: 620; loss: 0.89; acc: 0.83
Batch: 640; loss: 0.89; acc: 0.77
Batch: 660; loss: 0.93; acc: 0.77
Batch: 680; loss: 0.79; acc: 0.81
Batch: 700; loss: 0.91; acc: 0.81
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.88; acc: 0.8
Batch: 760; loss: 0.79; acc: 0.8
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.87; train_accuracy: 0.81 

0.0001468281407142058
0.00014213183021638542
Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.83
Batch: 140; loss: 0.64; acc: 0.91
Val Epoch over. val_loss: 0.7755133782982067; val_accuracy: 0.8457404458598726 

The current subspace-distance is: 0.00014213183021638542 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.89
Batch: 20; loss: 0.81; acc: 0.83
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 0.75; acc: 0.88
Batch: 80; loss: 0.89; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.74; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.86
Batch: 200; loss: 0.81; acc: 0.84
Batch: 220; loss: 0.81; acc: 0.81
Batch: 240; loss: 0.89; acc: 0.77
Batch: 260; loss: 0.78; acc: 0.88
Batch: 280; loss: 0.78; acc: 0.81
Batch: 300; loss: 1.15; acc: 0.67
Batch: 320; loss: 0.79; acc: 0.88
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.79; acc: 0.78
Batch: 380; loss: 0.66; acc: 0.91
Batch: 400; loss: 0.82; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.88; acc: 0.81
Batch: 480; loss: 0.89; acc: 0.83
Batch: 500; loss: 0.59; acc: 0.94
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.85; acc: 0.81
Batch: 560; loss: 0.79; acc: 0.81
Batch: 580; loss: 0.81; acc: 0.83
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.8; acc: 0.83
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.89; acc: 0.83
Batch: 720; loss: 0.85; acc: 0.83
Batch: 740; loss: 0.79; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.83
Batch: 780; loss: 0.82; acc: 0.75
Train Epoch over. train_loss: 0.81; train_accuracy: 0.82 

0.00015860442363191396
0.0001516890770290047
Batch: 0; loss: 0.8; acc: 0.86
Batch: 20; loss: 0.93; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.94
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.7210532658419032; val_accuracy: 0.8564888535031847 

The current subspace-distance is: 0.0001516890770290047 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 0.76; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.88
Batch: 140; loss: 0.96; acc: 0.72
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.84
Batch: 200; loss: 0.74; acc: 0.88
Batch: 220; loss: 0.83; acc: 0.81
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 1.0; acc: 0.73
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.64; acc: 0.89
Batch: 400; loss: 0.77; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.88
Batch: 440; loss: 0.74; acc: 0.81
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.79; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.91
Batch: 520; loss: 0.75; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.91
Batch: 580; loss: 0.71; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.91
Batch: 620; loss: 0.73; acc: 0.88
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.88
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.9; acc: 0.8
Batch: 740; loss: 0.62; acc: 0.91
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.83 

0.00016912620048969984
0.00016244348080363125
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.94
Val Epoch over. val_loss: 0.6823640623290068; val_accuracy: 0.8610668789808917 

The current subspace-distance is: 0.00016244348080363125 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.81; acc: 0.83
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.93; acc: 0.75
Batch: 200; loss: 0.65; acc: 0.86
Batch: 220; loss: 0.77; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.74; acc: 0.8
Batch: 280; loss: 0.75; acc: 0.86
Batch: 300; loss: 0.82; acc: 0.8
Batch: 320; loss: 0.8; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.94
Batch: 360; loss: 0.73; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.86
Batch: 400; loss: 0.82; acc: 0.86
Batch: 420; loss: 0.88; acc: 0.75
Batch: 440; loss: 0.74; acc: 0.81
Batch: 460; loss: 0.77; acc: 0.84
Batch: 480; loss: 0.86; acc: 0.83
Batch: 500; loss: 0.75; acc: 0.91
Batch: 520; loss: 0.77; acc: 0.8
Batch: 540; loss: 0.69; acc: 0.88
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.85; acc: 0.77
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.8; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.75; acc: 0.78
Batch: 700; loss: 0.71; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.92
Batch: 740; loss: 0.85; acc: 0.77
Batch: 760; loss: 0.83; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.73; train_accuracy: 0.84 

0.00017800657951738685
0.00016980325744953007
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.92
Val Epoch over. val_loss: 0.6561403253655524; val_accuracy: 0.8640525477707006 

The current subspace-distance is: 0.00016980325744953007 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.67; acc: 0.91
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.72; acc: 0.84
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.89
Batch: 260; loss: 0.74; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.91
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.83; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.8
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.76; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.66; acc: 0.88
Batch: 500; loss: 0.75; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.73; acc: 0.86
Batch: 620; loss: 0.74; acc: 0.78
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.92
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.85; acc: 0.81
Batch: 780; loss: 0.68; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00018653002916835248
0.00017811707220971584
Batch: 0; loss: 0.71; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.627524195772827; val_accuracy: 0.8642515923566879 

The current subspace-distance is: 0.00017811707220971584 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 0.74; acc: 0.78
Batch: 180; loss: 0.79; acc: 0.81
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.64; acc: 0.89
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.88
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.89
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.8; acc: 0.77
Batch: 480; loss: 0.59; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.84; acc: 0.8
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.85; acc: 0.78
Batch: 680; loss: 0.67; acc: 0.86
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.89; acc: 0.77
Batch: 760; loss: 0.85; acc: 0.77
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.0001879417395684868
0.00017945305444300175
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.91
Val Epoch over. val_loss: 0.6203361734463151; val_accuracy: 0.8647492038216561 

The current subspace-distance is: 0.00017945305444300175 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.84
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.69; acc: 0.89
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.89
Batch: 420; loss: 0.61; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.74; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.78; acc: 0.75
Batch: 580; loss: 0.76; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.71; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.78
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.71; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00018983449263032526
0.00018019176786765456
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6113678032805205; val_accuracy: 0.8670382165605095 

The current subspace-distance is: 0.00018019176786765456 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.94
Batch: 140; loss: 0.75; acc: 0.77
Batch: 160; loss: 0.62; acc: 0.89
Batch: 180; loss: 0.76; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.91
Batch: 360; loss: 0.83; acc: 0.73
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.93; acc: 0.75
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.72; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.81
Batch: 480; loss: 0.82; acc: 0.73
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.84
Batch: 580; loss: 0.87; acc: 0.77
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.86
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.83; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.89
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00019269250333309174
0.00018422400171402842
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6062968050598339; val_accuracy: 0.8662420382165605 

The current subspace-distance is: 0.00018422400171402842 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.67; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.7; acc: 0.88
Batch: 340; loss: 0.85; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.82; acc: 0.77
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.61; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.91
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.87; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.94
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.67; train_accuracy: 0.85 

0.00019413966219872236
0.00018587747763376683
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.6079330598093142; val_accuracy: 0.8635549363057324 

The current subspace-distance is: 0.00018587747763376683 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.92
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.91
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.91
Batch: 240; loss: 0.56; acc: 0.91
Batch: 260; loss: 0.65; acc: 0.89
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.8
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.78; acc: 0.78
Batch: 460; loss: 0.55; acc: 0.94
Batch: 480; loss: 0.81; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.68; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.88
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.81; acc: 0.8
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.81
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.66; train_accuracy: 0.85 

0.00019759678980335593
0.0001893907756311819
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.5976179955871241; val_accuracy: 0.8662420382165605 

The current subspace-distance is: 0.0001893907756311819 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.72; acc: 0.84
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.74; acc: 0.84
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.78; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.71; acc: 0.81
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.92
Batch: 560; loss: 0.91; acc: 0.78
Batch: 580; loss: 0.67; acc: 0.84
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.86
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.84; acc: 0.72
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.56; acc: 0.94
Train Epoch over. train_loss: 0.66; train_accuracy: 0.85 

0.00019832472025882453
0.0001897719339467585
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.89
Val Epoch over. val_loss: 0.5916023808679763; val_accuracy: 0.8674363057324841 

The current subspace-distance is: 0.0001897719339467585 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.91
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.91
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.97
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.94
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.74; acc: 0.75
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.77; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.86
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.65; train_accuracy: 0.85 

0.0001999207161134109
0.000191858212929219
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.5907452275418932; val_accuracy: 0.8680334394904459 

The current subspace-distance is: 0.000191858212929219 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.81; acc: 0.73
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.61; acc: 0.95
Batch: 160; loss: 0.63; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.87; acc: 0.78
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.7; acc: 0.8
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.77
Batch: 360; loss: 0.73; acc: 0.84
Batch: 380; loss: 0.8; acc: 0.8
Batch: 400; loss: 0.64; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.92
Batch: 460; loss: 0.79; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.91
Batch: 500; loss: 0.66; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.92
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.56; acc: 0.89
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.69; acc: 0.78
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.71; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.89
Batch: 780; loss: 0.72; acc: 0.78
Train Epoch over. train_loss: 0.65; train_accuracy: 0.85 

0.00019916410383302718
0.0001921358343679458
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.89
Val Epoch over. val_loss: 0.5819864705869346; val_accuracy: 0.8674363057324841 

The current subspace-distance is: 0.0001921358343679458 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.61; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.92
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.81; acc: 0.8
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.6; acc: 0.89
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.77
Batch: 400; loss: 0.79; acc: 0.75
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.93; acc: 0.75
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.66; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.95
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.88
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.68; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00020185082394164056
0.00019456334121059626
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.5831551436026385; val_accuracy: 0.8663415605095541 

The current subspace-distance is: 0.00019456334121059626 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.95
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.72; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.89
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.52; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.67; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.84; acc: 0.81
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.0002061313425656408
0.00019690163026098162
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5785369003654286; val_accuracy: 0.8678343949044586 

The current subspace-distance is: 0.00019690163026098162 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.74; acc: 0.81
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.93; acc: 0.75
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 0.61; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.95
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.78; acc: 0.78
Batch: 520; loss: 0.57; acc: 0.94
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.89
Batch: 600; loss: 0.62; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.8; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.83; acc: 0.75
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00020592744112946093
0.00019793420506175607
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.5768896411558625; val_accuracy: 0.8688296178343949 

The current subspace-distance is: 0.00019793420506175607 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.53; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.94
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.68; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.91
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.74; acc: 0.81
Batch: 480; loss: 0.78; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.7; acc: 0.73
Batch: 540; loss: 0.81; acc: 0.72
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.56; acc: 0.92
Batch: 740; loss: 0.77; acc: 0.72
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.54; acc: 0.94
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00020830804714933038
0.00020023649267386645
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.575408044324559; val_accuracy: 0.8695262738853503 

The current subspace-distance is: 0.00020023649267386645 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.84; acc: 0.72
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.94
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.8
Batch: 360; loss: 0.62; acc: 0.77
Batch: 380; loss: 0.63; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.94
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.91
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.97
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.89
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.69; acc: 0.73
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.0002109743218170479
0.00020111617050133646
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.91
Val Epoch over. val_loss: 0.5720515319496203; val_accuracy: 0.8675358280254777 

The current subspace-distance is: 0.00020111617050133646 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 0.54; acc: 0.92
Batch: 140; loss: 0.74; acc: 0.83
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.88; acc: 0.73
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.72; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.72; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.92
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 0.71; acc: 0.77
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.63; acc: 0.84
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.8; acc: 0.75
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00020759995095431805
0.000197231478523463
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.5727224397431513; val_accuracy: 0.8711186305732485 

The current subspace-distance is: 0.000197231478523463 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.72; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.66; acc: 0.84
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.66; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.94
Batch: 460; loss: 0.8; acc: 0.75
Batch: 480; loss: 0.66; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.83
Batch: 520; loss: 0.76; acc: 0.78
Batch: 540; loss: 0.71; acc: 0.78
Batch: 560; loss: 0.46; acc: 0.95
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 0.67; acc: 0.77
Batch: 640; loss: 0.8; acc: 0.73
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.92
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00020939037494827062
0.0002005857677431777
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.570567910838279; val_accuracy: 0.8686305732484076 

The current subspace-distance is: 0.0002005857677431777 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.62; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.88
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.91
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.83; acc: 0.77
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.82; acc: 0.73
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.79; acc: 0.78
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.74; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.92
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.0002102118742186576
0.00020146706083323807
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.92
Val Epoch over. val_loss: 0.5749380123463406; val_accuracy: 0.8651472929936306 

The current subspace-distance is: 0.00020146706083323807 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.79; acc: 0.77
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.64; acc: 0.77
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.78
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.94
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.94
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.75; acc: 0.86
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.89; acc: 0.73
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.54; acc: 0.91
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.92
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00020958558889105916
0.00020005504484288394
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.5666157701972184; val_accuracy: 0.8715167197452229 

The current subspace-distance is: 0.00020005504484288394 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.64; acc: 0.88
Batch: 220; loss: 0.77; acc: 0.78
Batch: 240; loss: 0.78; acc: 0.77
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.69; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.65; acc: 0.8
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.86
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.0002121573343174532
0.0002022840635618195
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.91
Val Epoch over. val_loss: 0.5678560779352856; val_accuracy: 0.8717157643312102 

The current subspace-distance is: 0.0002022840635618195 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.75; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.71; acc: 0.75
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.82; acc: 0.77
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.89
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.68; acc: 0.81
Batch: 580; loss: 0.72; acc: 0.78
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.63; train_accuracy: 0.85 

0.00020879744261037558
0.0002010213938774541
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.89
Val Epoch over. val_loss: 0.5729954349007577; val_accuracy: 0.8676353503184714 

The current subspace-distance is: 0.0002010213938774541 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.84; acc: 0.78
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.63; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.89
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.81
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.61; acc: 0.91
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.92
Batch: 720; loss: 0.67; acc: 0.8
Batch: 740; loss: 0.6; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.94
Train Epoch over. train_loss: 0.63; train_accuracy: 0.84 

0.00021149760868865997
0.00020446698181331158
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.91
Val Epoch over. val_loss: 0.565402598897363; val_accuracy: 0.8687300955414012 

The current subspace-distance is: 0.00020446698181331158 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_12_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.17

The number of parameters is: 270144

The number of individual parameters is:

10
180
10
10
15
33600
15
15
29
97440
29
29
64
133632
64
64
4096
64
640
10
64
64

nonzero elements in E: 81043191
elements in E: 81043200
fraction nonzero: 0.9999998889481165
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.48; acc: 0.08
Batch: 20; loss: 2.16; acc: 0.3
Batch: 40; loss: 1.91; acc: 0.38
Batch: 60; loss: 1.87; acc: 0.39
Batch: 80; loss: 1.79; acc: 0.41
Batch: 100; loss: 1.73; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.63; acc: 0.56
Batch: 160; loss: 1.7; acc: 0.47
Batch: 180; loss: 1.56; acc: 0.53
Batch: 200; loss: 1.52; acc: 0.62
Batch: 220; loss: 1.43; acc: 0.7
Batch: 240; loss: 1.4; acc: 0.73
Batch: 260; loss: 1.54; acc: 0.61
Batch: 280; loss: 1.61; acc: 0.56
Batch: 300; loss: 1.5; acc: 0.69
Batch: 320; loss: 1.46; acc: 0.69
Batch: 340; loss: 1.36; acc: 0.7
Batch: 360; loss: 1.29; acc: 0.77
Batch: 380; loss: 1.5; acc: 0.58
Batch: 400; loss: 1.43; acc: 0.69
Batch: 420; loss: 1.61; acc: 0.56
Batch: 440; loss: 1.39; acc: 0.66
Batch: 460; loss: 1.15; acc: 0.83
Batch: 480; loss: 1.39; acc: 0.7
Batch: 500; loss: 1.28; acc: 0.83
Batch: 520; loss: 1.25; acc: 0.78
Batch: 540; loss: 1.33; acc: 0.7
Batch: 560; loss: 1.38; acc: 0.67
Batch: 580; loss: 1.32; acc: 0.8
Batch: 600; loss: 1.32; acc: 0.78
Batch: 620; loss: 1.31; acc: 0.75
Batch: 640; loss: 1.25; acc: 0.75
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.22; acc: 0.78
Batch: 700; loss: 1.22; acc: 0.81
Batch: 720; loss: 1.27; acc: 0.77
Batch: 740; loss: 1.35; acc: 0.69
Batch: 760; loss: 1.18; acc: 0.8
Batch: 780; loss: 1.22; acc: 0.75
Train Epoch over. train_loss: 1.46; train_accuracy: 0.65 

6.212653534021229e-05
5.718780812458135e-05
Batch: 0; loss: 1.25; acc: 0.77
Batch: 20; loss: 1.38; acc: 0.69
Batch: 40; loss: 0.89; acc: 0.94
Batch: 60; loss: 1.17; acc: 0.78
Batch: 80; loss: 1.03; acc: 0.84
Batch: 100; loss: 1.19; acc: 0.81
Batch: 120; loss: 1.31; acc: 0.69
Batch: 140; loss: 1.14; acc: 0.84
Val Epoch over. val_loss: 1.1752107769820341; val_accuracy: 0.7913017515923567 

The current subspace-distance is: 5.718780812458135e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.77
Batch: 20; loss: 1.26; acc: 0.78
Batch: 40; loss: 1.19; acc: 0.8
Batch: 60; loss: 1.09; acc: 0.78
Batch: 80; loss: 1.29; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.78
Batch: 120; loss: 1.12; acc: 0.83
Batch: 140; loss: 1.14; acc: 0.77
Batch: 160; loss: 1.08; acc: 0.86
Batch: 180; loss: 1.16; acc: 0.84
Batch: 200; loss: 1.22; acc: 0.73
Batch: 220; loss: 1.16; acc: 0.77
Batch: 240; loss: 1.2; acc: 0.75
Batch: 260; loss: 1.22; acc: 0.78
Batch: 280; loss: 1.11; acc: 0.77
Batch: 300; loss: 1.14; acc: 0.78
Batch: 320; loss: 1.24; acc: 0.75
Batch: 340; loss: 1.13; acc: 0.8
Batch: 360; loss: 1.12; acc: 0.78
Batch: 380; loss: 1.07; acc: 0.81
Batch: 400; loss: 1.15; acc: 0.78
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 1.18; acc: 0.78
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.19; acc: 0.77
Batch: 500; loss: 1.05; acc: 0.86
Batch: 520; loss: 1.04; acc: 0.81
Batch: 540; loss: 1.13; acc: 0.72
Batch: 560; loss: 1.11; acc: 0.78
Batch: 580; loss: 0.92; acc: 0.89
Batch: 600; loss: 1.04; acc: 0.77
Batch: 620; loss: 1.19; acc: 0.69
Batch: 640; loss: 1.02; acc: 0.78
Batch: 660; loss: 1.1; acc: 0.73
Batch: 680; loss: 1.13; acc: 0.78
Batch: 700; loss: 1.08; acc: 0.8
Batch: 720; loss: 1.05; acc: 0.75
Batch: 740; loss: 1.08; acc: 0.83
Batch: 760; loss: 1.01; acc: 0.86
Batch: 780; loss: 1.08; acc: 0.75
Train Epoch over. train_loss: 1.12; train_accuracy: 0.78 

8.448286826023832e-05
7.902448123786598e-05
Batch: 0; loss: 1.06; acc: 0.8
Batch: 20; loss: 1.23; acc: 0.73
Batch: 40; loss: 0.71; acc: 0.95
Batch: 60; loss: 1.0; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.86
Batch: 100; loss: 0.99; acc: 0.84
Batch: 120; loss: 1.12; acc: 0.75
Batch: 140; loss: 0.92; acc: 0.89
Val Epoch over. val_loss: 0.9889441379316294; val_accuracy: 0.8271297770700637 

The current subspace-distance is: 7.902448123786598e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 0.91; acc: 0.89
Batch: 40; loss: 1.0; acc: 0.81
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 0.98; acc: 0.8
Batch: 100; loss: 1.06; acc: 0.81
Batch: 120; loss: 1.05; acc: 0.8
Batch: 140; loss: 0.96; acc: 0.8
Batch: 160; loss: 0.98; acc: 0.81
Batch: 180; loss: 1.0; acc: 0.83
Batch: 200; loss: 1.01; acc: 0.81
Batch: 220; loss: 1.03; acc: 0.78
Batch: 240; loss: 1.1; acc: 0.8
Batch: 260; loss: 0.86; acc: 0.92
Batch: 280; loss: 1.06; acc: 0.8
Batch: 300; loss: 1.03; acc: 0.75
Batch: 320; loss: 0.97; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.86
Batch: 360; loss: 1.07; acc: 0.75
Batch: 380; loss: 0.99; acc: 0.84
Batch: 400; loss: 0.96; acc: 0.83
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 0.97; acc: 0.8
Batch: 460; loss: 0.86; acc: 0.88
Batch: 480; loss: 0.95; acc: 0.83
Batch: 500; loss: 0.99; acc: 0.81
Batch: 520; loss: 0.95; acc: 0.77
Batch: 540; loss: 0.87; acc: 0.83
Batch: 560; loss: 1.02; acc: 0.83
Batch: 580; loss: 0.9; acc: 0.83
Batch: 600; loss: 0.93; acc: 0.88
Batch: 620; loss: 0.93; acc: 0.83
Batch: 640; loss: 0.93; acc: 0.83
Batch: 660; loss: 0.98; acc: 0.8
Batch: 680; loss: 0.86; acc: 0.83
Batch: 700; loss: 1.08; acc: 0.73
Batch: 720; loss: 0.96; acc: 0.83
Batch: 740; loss: 0.93; acc: 0.81
Batch: 760; loss: 0.94; acc: 0.8
Batch: 780; loss: 0.9; acc: 0.83
Train Epoch over. train_loss: 0.98; train_accuracy: 0.81 

0.00010163065599044785
9.681137453299016e-05
Batch: 0; loss: 1.01; acc: 0.78
Batch: 20; loss: 1.13; acc: 0.75
Batch: 40; loss: 0.6; acc: 0.91
Batch: 60; loss: 0.92; acc: 0.83
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.84; acc: 0.89
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 0.78; acc: 0.91
Val Epoch over. val_loss: 0.86520153853544; val_accuracy: 0.8486265923566879 

The current subspace-distance is: 9.681137453299016e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.81
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.89; acc: 0.83
Batch: 60; loss: 0.93; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.97
Batch: 100; loss: 0.79; acc: 0.91
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.88; acc: 0.89
Batch: 160; loss: 0.97; acc: 0.75
Batch: 180; loss: 0.85; acc: 0.83
Batch: 200; loss: 1.0; acc: 0.78
Batch: 220; loss: 0.81; acc: 0.89
Batch: 240; loss: 0.91; acc: 0.86
Batch: 260; loss: 0.92; acc: 0.8
Batch: 280; loss: 0.86; acc: 0.81
Batch: 300; loss: 0.9; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.78
Batch: 340; loss: 1.0; acc: 0.73
Batch: 360; loss: 0.82; acc: 0.89
Batch: 380; loss: 0.85; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.84
Batch: 420; loss: 0.99; acc: 0.7
Batch: 440; loss: 0.74; acc: 0.91
Batch: 460; loss: 0.89; acc: 0.86
Batch: 480; loss: 0.79; acc: 0.86
Batch: 500; loss: 0.88; acc: 0.83
Batch: 520; loss: 0.8; acc: 0.92
Batch: 540; loss: 0.91; acc: 0.81
Batch: 560; loss: 0.87; acc: 0.81
Batch: 580; loss: 0.85; acc: 0.86
Batch: 600; loss: 0.71; acc: 0.94
Batch: 620; loss: 0.73; acc: 0.84
Batch: 640; loss: 0.87; acc: 0.89
Batch: 660; loss: 0.94; acc: 0.81
Batch: 680; loss: 0.73; acc: 0.91
Batch: 700; loss: 0.93; acc: 0.81
Batch: 720; loss: 0.91; acc: 0.86
Batch: 740; loss: 0.85; acc: 0.84
Batch: 760; loss: 0.79; acc: 0.88
Batch: 780; loss: 0.74; acc: 0.91
Train Epoch over. train_loss: 0.87; train_accuracy: 0.83 

0.00011941283446503803
0.00011390542204026133
Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.91
Batch: 120; loss: 0.94; acc: 0.73
Batch: 140; loss: 0.67; acc: 0.94
Val Epoch over. val_loss: 0.7705644815211083; val_accuracy: 0.863953025477707 

The current subspace-distance is: 0.00011390542204026133 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 0.89; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.88
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.88
Batch: 140; loss: 0.75; acc: 0.91
Batch: 160; loss: 0.72; acc: 0.91
Batch: 180; loss: 0.87; acc: 0.83
Batch: 200; loss: 0.88; acc: 0.81
Batch: 220; loss: 0.8; acc: 0.84
Batch: 240; loss: 0.76; acc: 0.83
Batch: 260; loss: 0.76; acc: 0.88
Batch: 280; loss: 0.76; acc: 0.86
Batch: 300; loss: 0.87; acc: 0.84
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.77; acc: 0.91
Batch: 360; loss: 0.65; acc: 0.89
Batch: 380; loss: 0.81; acc: 0.8
Batch: 400; loss: 0.74; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.86
Batch: 440; loss: 0.73; acc: 0.84
Batch: 460; loss: 0.74; acc: 0.89
Batch: 480; loss: 0.77; acc: 0.84
Batch: 500; loss: 0.8; acc: 0.84
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.93; acc: 0.77
Batch: 580; loss: 0.71; acc: 0.86
Batch: 600; loss: 0.83; acc: 0.81
Batch: 620; loss: 0.71; acc: 0.91
Batch: 640; loss: 0.69; acc: 0.88
Batch: 660; loss: 0.76; acc: 0.89
Batch: 680; loss: 0.77; acc: 0.84
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.7; acc: 0.92
Batch: 740; loss: 0.79; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.94
Batch: 780; loss: 0.81; acc: 0.81
Train Epoch over. train_loss: 0.8; train_accuracy: 0.85 

0.00013282524014357477
0.00012811945634894073
Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.57; acc: 0.94
Val Epoch over. val_loss: 0.7013640022201902; val_accuracy: 0.8714171974522293 

The current subspace-distance is: 0.00012811945634894073 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.86
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.86
Batch: 80; loss: 0.81; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.92
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.78; acc: 0.86
Batch: 200; loss: 0.71; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.91
Batch: 240; loss: 0.81; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.78; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.68; acc: 0.89
Batch: 380; loss: 0.7; acc: 0.88
Batch: 400; loss: 0.72; acc: 0.89
Batch: 420; loss: 0.69; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.94
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.66; acc: 0.97
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.71; acc: 0.86
Batch: 560; loss: 0.89; acc: 0.78
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.86; acc: 0.89
Batch: 620; loss: 0.74; acc: 0.83
Batch: 640; loss: 0.63; acc: 0.91
Batch: 660; loss: 0.69; acc: 0.89
Batch: 680; loss: 0.79; acc: 0.81
Batch: 700; loss: 0.53; acc: 0.95
Batch: 720; loss: 0.69; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.92
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.85 

0.0001451934513170272
0.00013882583880331367
Batch: 0; loss: 0.79; acc: 0.8
Batch: 20; loss: 0.9; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6555190530552226; val_accuracy: 0.8807722929936306 

The current subspace-distance is: 0.00013882583880331367 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.86
Batch: 140; loss: 0.81; acc: 0.88
Batch: 160; loss: 0.88; acc: 0.77
Batch: 180; loss: 0.69; acc: 0.86
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.69; acc: 0.91
Batch: 300; loss: 0.66; acc: 0.89
Batch: 320; loss: 0.65; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.92
Batch: 360; loss: 0.68; acc: 0.91
Batch: 380; loss: 0.71; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.77
Batch: 420; loss: 0.7; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.92
Batch: 460; loss: 0.66; acc: 0.86
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.78; acc: 0.83
Batch: 520; loss: 0.58; acc: 0.94
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 0.71; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.92
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.74; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.65; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.81
Batch: 760; loss: 0.59; acc: 0.92
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.69; train_accuracy: 0.86 

0.00015747977886348963
0.00015281140804290771
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.94
Val Epoch over. val_loss: 0.617629211989178; val_accuracy: 0.8818670382165605 

The current subspace-distance is: 0.00015281140804290771 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.91
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.91
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.94
Batch: 180; loss: 0.78; acc: 0.78
Batch: 200; loss: 0.58; acc: 0.91
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.67; acc: 0.83
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.92
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.69; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.95
Batch: 580; loss: 0.76; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.73; acc: 0.81
Batch: 640; loss: 0.89; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.92
Batch: 680; loss: 0.63; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.77; acc: 0.81
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.86 

0.0001656685199122876
0.000159044488100335
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.5805960603200706; val_accuracy: 0.887937898089172 

The current subspace-distance is: 0.000159044488100335 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.55; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.9; acc: 0.7
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.94
Batch: 240; loss: 0.66; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.73; acc: 0.78
Batch: 300; loss: 0.49; acc: 0.97
Batch: 320; loss: 0.57; acc: 0.91
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.65; acc: 0.88
Batch: 600; loss: 0.79; acc: 0.75
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.71; acc: 0.89
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.86 

0.00017345060769002885
0.00016749098722357303
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.5602709723126357; val_accuracy: 0.8852507961783439 

The current subspace-distance is: 0.00016749098722357303 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.95
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.79; acc: 0.75
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.66; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.56; acc: 0.92
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.73; acc: 0.78
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.88; acc: 0.75
Batch: 600; loss: 0.68; acc: 0.91
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.42; acc: 0.98
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.47; acc: 0.95
Batch: 700; loss: 0.65; acc: 0.81
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.73; acc: 0.78
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.87 

0.0001811797556001693
0.00017453328473493457
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.76; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5319706997863806; val_accuracy: 0.8911226114649682 

The current subspace-distance is: 0.00017453328473493457 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.95
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.61; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.94
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.48; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.95
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.95
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.8
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.00018286885460838675
0.00017577917606104165
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5220108011345954; val_accuracy: 0.8913216560509554 

The current subspace-distance is: 0.00017577917606104165 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.74; acc: 0.8
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.66; acc: 0.78
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.68; acc: 0.8
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.94
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.00018792749324347824
0.00018003412696998566
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5103451549817043; val_accuracy: 0.8948049363057324 

The current subspace-distance is: 0.00018003412696998566 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.92
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.67; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.97
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.94
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.95
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.76; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.92
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00018881898722611368
0.00018146137881558388
Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5153631007975075; val_accuracy: 0.89171974522293 

The current subspace-distance is: 0.00018146137881558388 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.78
Batch: 120; loss: 0.55; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.97
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.94
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.97
Batch: 560; loss: 0.47; acc: 0.95
Batch: 580; loss: 0.53; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.65; acc: 0.83
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00019192261970601976
0.0001817958545871079
Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.5013211126540117; val_accuracy: 0.894406847133758 

The current subspace-distance is: 0.0001817958545871079 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.42; acc: 0.95
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.78
Batch: 200; loss: 0.6; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.62; acc: 0.81
Batch: 400; loss: 0.84; acc: 0.78
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.66; acc: 0.81
Batch: 460; loss: 0.67; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.89
Batch: 700; loss: 0.7; acc: 0.75
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.91
Train Epoch over. train_loss: 0.56; train_accuracy: 0.88 

0.00019384777988307178
0.0001854991860454902
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.4923989269763801; val_accuracy: 0.8971934713375797 

The current subspace-distance is: 0.0001854991860454902 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.75; acc: 0.77
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.91
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.92
Batch: 300; loss: 0.59; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.78; acc: 0.78
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.97
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.0001943677052622661
0.00018769045709632337
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.4915643213850677; val_accuracy: 0.8953025477707006 

The current subspace-distance is: 0.00018769045709632337 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.94
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.69; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.94
Batch: 440; loss: 0.56; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.97
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.00019587778660934418
0.00018618781177792698
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.48106145137434553; val_accuracy: 0.8988853503184714 

The current subspace-distance is: 0.00018618781177792698 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.61; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.97
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.76; acc: 0.73
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.00019850408716592938
0.00019193338812328875
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.47864610061144375; val_accuracy: 0.8983877388535032 

The current subspace-distance is: 0.00019193338812328875 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.33; acc: 0.97
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.73
Batch: 680; loss: 0.51; acc: 0.92
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.00019962168880738318
0.00019067783432547003
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.47145402393523295; val_accuracy: 0.8981886942675159 

The current subspace-distance is: 0.00019067783432547003 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.95
Batch: 160; loss: 0.45; acc: 0.95
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.95
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.94
Batch: 680; loss: 0.54; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00020167460024822503
0.00019428766972851008
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.46934098309012734; val_accuracy: 0.8984872611464968 

The current subspace-distance is: 0.00019428766972851008 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.66; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.7; acc: 0.78
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.8
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0002012692129937932
0.00019396429706830531
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4720623352725035; val_accuracy: 0.8965963375796179 

The current subspace-distance is: 0.00019396429706830531 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.92
Batch: 340; loss: 0.61; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.68; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.57; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.74; acc: 0.81
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00020226849301252514
0.00019371035159565508
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.46612793216659765; val_accuracy: 0.897093949044586 

The current subspace-distance is: 0.00019371035159565508 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.79; acc: 0.8
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020262184261810035
0.00019524549134075642
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.24; acc: 1.0
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.4659460409052053; val_accuracy: 0.8992834394904459 

The current subspace-distance is: 0.00019524549134075642 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.95
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.92
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.94
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.94
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.48; acc: 0.95
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0002050972980214283
0.00019626787980087101
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.46021973669149313; val_accuracy: 0.9011743630573248 

The current subspace-distance is: 0.00019626787980087101 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.98
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.95
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.54; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.78
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.95
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.94
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020429336291272193
0.0001968629949260503
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.24; acc: 1.0
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.4656075056001639; val_accuracy: 0.9002786624203821 

The current subspace-distance is: 0.0001968629949260503 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.95
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.5; acc: 0.94
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020241599122527987
0.00019566903938539326
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.45706105535956704; val_accuracy: 0.9007762738853503 

The current subspace-distance is: 0.00019566903938539326 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.94
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.77; acc: 0.72
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.95
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.92
Batch: 780; loss: 0.64; acc: 0.8
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0002051698975265026
0.0001991364551940933
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.4625677168369293; val_accuracy: 0.9000796178343949 

The current subspace-distance is: 0.0001991364551940933 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.67; acc: 0.88
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.8
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.89
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.95
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.95
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.63; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020705147471744567
0.0001992490579141304
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4640487537832017; val_accuracy: 0.8982882165605095 

The current subspace-distance is: 0.0001992490579141304 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.37; acc: 0.95
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.94
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020769813272636384
0.00019919979968108237
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.46362977014605405; val_accuracy: 0.901671974522293 

The current subspace-distance is: 0.00019919979968108237 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.65; acc: 0.78
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020784710068255663
0.00019878933380823582
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.45148083501181024; val_accuracy: 0.9008757961783439 

The current subspace-distance is: 0.00019878933380823582 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_12_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.17

The number of parameters is: 270144

The number of individual parameters is:

10
180
10
10
15
33600
15
15
29
97440
29
29
64
133632
64
64
4096
64
640
10
64
64

nonzero elements in E: 108057589
elements in E: 108057600
fraction nonzero: 0.9999998982024402
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.4; acc: 0.06
Batch: 20; loss: 2.1; acc: 0.28
Batch: 40; loss: 1.94; acc: 0.3
Batch: 60; loss: 1.81; acc: 0.45
Batch: 80; loss: 1.64; acc: 0.58
Batch: 100; loss: 1.57; acc: 0.66
Batch: 120; loss: 1.74; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.64
Batch: 160; loss: 1.43; acc: 0.75
Batch: 180; loss: 1.48; acc: 0.59
Batch: 200; loss: 1.38; acc: 0.75
Batch: 220; loss: 1.28; acc: 0.78
Batch: 240; loss: 1.33; acc: 0.72
Batch: 260; loss: 1.42; acc: 0.67
Batch: 280; loss: 1.3; acc: 0.7
Batch: 300; loss: 1.2; acc: 0.77
Batch: 320; loss: 1.34; acc: 0.66
Batch: 340; loss: 1.24; acc: 0.73
Batch: 360; loss: 1.13; acc: 0.84
Batch: 380; loss: 1.08; acc: 0.78
Batch: 400; loss: 1.1; acc: 0.81
Batch: 420; loss: 1.12; acc: 0.78
Batch: 440; loss: 1.05; acc: 0.84
Batch: 460; loss: 1.19; acc: 0.7
Batch: 480; loss: 1.12; acc: 0.81
Batch: 500; loss: 1.07; acc: 0.83
Batch: 520; loss: 1.04; acc: 0.8
Batch: 540; loss: 0.94; acc: 0.88
Batch: 560; loss: 0.98; acc: 0.86
Batch: 580; loss: 0.94; acc: 0.88
Batch: 600; loss: 1.01; acc: 0.86
Batch: 620; loss: 0.94; acc: 0.86
Batch: 640; loss: 1.07; acc: 0.77
Batch: 660; loss: 0.95; acc: 0.83
Batch: 680; loss: 0.8; acc: 0.94
Batch: 700; loss: 1.0; acc: 0.78
Batch: 720; loss: 1.0; acc: 0.77
Batch: 740; loss: 1.05; acc: 0.7
Batch: 760; loss: 0.96; acc: 0.81
Batch: 780; loss: 1.05; acc: 0.75
Train Epoch over. train_loss: 1.26; train_accuracy: 0.73 

2.677612064871937e-05
1.0151552487513982e-05
Batch: 0; loss: 0.93; acc: 0.86
Batch: 20; loss: 1.1; acc: 0.67
Batch: 40; loss: 0.62; acc: 0.94
Batch: 60; loss: 0.87; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.95
Batch: 100; loss: 0.84; acc: 0.91
Batch: 120; loss: 1.04; acc: 0.75
Batch: 140; loss: 0.71; acc: 0.92
Val Epoch over. val_loss: 0.874985195648898; val_accuracy: 0.852109872611465 

The current subspace-distance is: 1.0151552487513982e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.84
Batch: 20; loss: 0.94; acc: 0.8
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 0.9; acc: 0.88
Batch: 80; loss: 0.97; acc: 0.84
Batch: 100; loss: 0.8; acc: 0.89
Batch: 120; loss: 0.94; acc: 0.8
Batch: 140; loss: 0.87; acc: 0.81
Batch: 160; loss: 0.92; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.94
Batch: 200; loss: 0.85; acc: 0.84
Batch: 220; loss: 0.84; acc: 0.86
Batch: 240; loss: 0.97; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.89
Batch: 280; loss: 0.93; acc: 0.83
Batch: 300; loss: 0.97; acc: 0.73
Batch: 320; loss: 0.9; acc: 0.83
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.83; acc: 0.89
Batch: 380; loss: 0.82; acc: 0.8
Batch: 400; loss: 0.75; acc: 0.91
Batch: 420; loss: 0.96; acc: 0.77
Batch: 440; loss: 0.92; acc: 0.83
Batch: 460; loss: 0.72; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.94
Batch: 500; loss: 0.85; acc: 0.84
Batch: 520; loss: 0.77; acc: 0.91
Batch: 540; loss: 0.76; acc: 0.84
Batch: 560; loss: 0.87; acc: 0.84
Batch: 580; loss: 0.73; acc: 0.91
Batch: 600; loss: 0.79; acc: 0.8
Batch: 620; loss: 0.85; acc: 0.8
Batch: 640; loss: 0.74; acc: 0.89
Batch: 660; loss: 0.74; acc: 0.88
Batch: 680; loss: 0.76; acc: 0.83
Batch: 700; loss: 0.75; acc: 0.86
Batch: 720; loss: 0.84; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.73; acc: 0.88
Batch: 780; loss: 0.72; acc: 0.91
Train Epoch over. train_loss: 0.82; train_accuracy: 0.85 

3.2909752917476e-05
1.266558774659643e-05
Batch: 0; loss: 0.75; acc: 0.88
Batch: 20; loss: 0.92; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.98
Batch: 60; loss: 0.7; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.97
Batch: 100; loss: 0.63; acc: 0.94
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.95
Val Epoch over. val_loss: 0.6842289024097904; val_accuracy: 0.8791799363057324 

The current subspace-distance is: 1.266558774659643e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.57; acc: 0.97
Batch: 60; loss: 0.72; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.84; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.85; acc: 0.78
Batch: 160; loss: 0.61; acc: 0.91
Batch: 180; loss: 0.57; acc: 0.94
Batch: 200; loss: 0.73; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.83
Batch: 240; loss: 0.58; acc: 0.91
Batch: 260; loss: 0.78; acc: 0.8
Batch: 280; loss: 0.66; acc: 0.88
Batch: 300; loss: 0.78; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.89
Batch: 340; loss: 0.75; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.95
Batch: 380; loss: 0.73; acc: 0.84
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.86
Batch: 440; loss: 0.79; acc: 0.8
Batch: 460; loss: 0.66; acc: 0.89
Batch: 480; loss: 0.65; acc: 0.95
Batch: 500; loss: 0.66; acc: 0.92
Batch: 520; loss: 0.69; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.89
Batch: 560; loss: 0.68; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.63; acc: 0.88
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.71; acc: 0.91
Batch: 660; loss: 0.72; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.94
Batch: 700; loss: 0.78; acc: 0.77
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.88
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.87 

3.833633672911674e-05
1.608523234608583e-05
Batch: 0; loss: 0.68; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.97
Batch: 60; loss: 0.66; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.98
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.98
Val Epoch over. val_loss: 0.6153542389915247; val_accuracy: 0.8861464968152867 

The current subspace-distance is: 1.608523234608583e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.91
Batch: 40; loss: 0.66; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.94
Batch: 100; loss: 0.65; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.94
Batch: 160; loss: 0.75; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.92
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.94
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.98
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.8; acc: 0.81
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.6; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.89
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.75; acc: 0.81
Batch: 740; loss: 0.62; acc: 0.91
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.6; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.87 

4.081982115167193e-05
1.7241882233065553e-05
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.98
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.88; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.98
Val Epoch over. val_loss: 0.5776083391563148; val_accuracy: 0.8924164012738853 

The current subspace-distance is: 1.7241882233065553e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.64; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.7; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.95
Batch: 240; loss: 0.57; acc: 0.92
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.95
Batch: 380; loss: 0.61; acc: 0.89
Batch: 400; loss: 0.7; acc: 0.78
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.95
Batch: 480; loss: 0.66; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.66; acc: 0.89
Batch: 540; loss: 0.69; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.89
Batch: 780; loss: 0.78; acc: 0.83
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

4.3633128370856866e-05
1.8811946574714966e-05
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.98
Batch: 60; loss: 0.57; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.98
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.82; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.98
Val Epoch over. val_loss: 0.5410328359360908; val_accuracy: 0.8983877388535032 

The current subspace-distance is: 1.8811946574714966e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.94
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.97
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.94
Batch: 660; loss: 0.52; acc: 0.95
Batch: 680; loss: 0.57; acc: 0.89
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.92
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

4.629015893442556e-05
1.983023321372457e-05
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.98
Val Epoch over. val_loss: 0.5075367482224847; val_accuracy: 0.8999800955414012 

The current subspace-distance is: 1.983023321372457e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.56; acc: 0.92
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.48; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.97
Batch: 640; loss: 0.57; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.898076440440491e-05
2.1825399016961455e-05
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.98
Val Epoch over. val_loss: 0.48809584956260244; val_accuracy: 0.9003781847133758 

The current subspace-distance is: 2.1825399016961455e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.98
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.95
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.97
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.94
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.54; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

5.050481922808103e-05
2.231393227702938e-05
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.98
Val Epoch over. val_loss: 0.4619670529274424; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 2.231393227702938e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.97
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.7; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

5.2569452236639336e-05
2.375931217102334e-05
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.44251334705170553; val_accuracy: 0.9066480891719745 

The current subspace-distance is: 2.375931217102334e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.94
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

5.514935764949769e-05
2.4409608158748597e-05
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4279430534239787; val_accuracy: 0.908140923566879 

The current subspace-distance is: 2.4409608158748597e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.97
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.97
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

5.587051418842748e-05
2.3825599782867357e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.419404846183054; val_accuracy: 0.9078423566878981 

The current subspace-distance is: 2.3825599782867357e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.95
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.6724671594565734e-05
2.441383730911184e-05
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.42012138133216054; val_accuracy: 0.9082404458598726 

The current subspace-distance is: 2.441383730911184e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.6947006669361144e-05
2.5486820959486067e-05
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.41372056960300274; val_accuracy: 0.9093351910828026 

The current subspace-distance is: 2.5486820959486067e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.820715887239203e-05
2.599186700535938e-05
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.40581651857704115; val_accuracy: 0.9118232484076433 

The current subspace-distance is: 2.599186700535938e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.905633952352218e-05
2.8065882361261174e-05
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.40202076438885587; val_accuracy: 0.912718949044586 

The current subspace-distance is: 2.8065882361261174e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.63; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.97
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.95
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.95
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.869469896424562e-05
2.7080037398263812e-05
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.40427372448003973; val_accuracy: 0.9102308917197452 

The current subspace-distance is: 2.7080037398263812e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.54; acc: 0.8
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.95
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.862594480277039e-05
2.5897141313180327e-05
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.3983225140032495; val_accuracy: 0.9122213375796179 

The current subspace-distance is: 2.5897141313180327e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.92
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.95
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.9900743508478627e-05
2.6122879717149772e-05
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3932774544331678; val_accuracy: 0.910828025477707 

The current subspace-distance is: 2.6122879717149772e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.95
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.026065966580063e-05
2.605202462291345e-05
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.39007900902040443; val_accuracy: 0.9122213375796179 

The current subspace-distance is: 2.605202462291345e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.94
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.050736192264594e-05
2.6775560399983078e-05
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.37944951283324296; val_accuracy: 0.9136146496815286 

The current subspace-distance is: 2.6775560399983078e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.97
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.095682692830451e-05
2.730567030084785e-05
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3819968132836044; val_accuracy: 0.9133160828025477 

The current subspace-distance is: 2.730567030084785e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.97
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.78
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.130531983217224e-05
2.6571258786134422e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.37917544621570853; val_accuracy: 0.9128184713375797 

The current subspace-distance is: 2.6571258786134422e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.29; acc: 0.97
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.0948612372158095e-05
2.4934088287409395e-05
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3867003717430078; val_accuracy: 0.9120222929936306 

The current subspace-distance is: 2.4934088287409395e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.25; acc: 1.0
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.08285736234393e-05
2.5748224288690835e-05
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3839835507474887; val_accuracy: 0.9114251592356688 

The current subspace-distance is: 2.5748224288690835e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.98
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.124042556621134e-05
2.661914550117217e-05
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3744148107091333; val_accuracy: 0.9153065286624203 

The current subspace-distance is: 2.661914550117217e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.97
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.200781353982165e-05
2.7249381673755124e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.37740493114966495; val_accuracy: 0.9155055732484076 

The current subspace-distance is: 2.7249381673755124e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.98
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.97
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.176668102853e-05
2.76363170996774e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3803431967830962; val_accuracy: 0.914609872611465 

The current subspace-distance is: 2.76363170996774e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.97
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.214343738975003e-05
2.7786451028077863e-05
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3706376184323791; val_accuracy: 0.9142117834394905 

The current subspace-distance is: 2.7786451028077863e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.19986021774821e-05
2.7187987143406644e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.37310888660941155; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.7187987143406644e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.57; acc: 0.78
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.284181290538982e-05
2.7864343792316504e-05
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.37311828060514607; val_accuracy: 0.9140127388535032 

The current subspace-distance is: 2.7864343792316504e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_12_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.17

The number of parameters is: 270144

The number of individual parameters is:

10
180
10
10
15
33600
15
15
29
97440
29
29
64
133632
64
64
4096
64
640
10
64
64

nonzero elements in E: 135071989
elements in E: 135072000
fraction nonzero: 0.9999999185619521
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.42; acc: 0.06
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.78; acc: 0.52
Batch: 60; loss: 1.76; acc: 0.47
Batch: 80; loss: 1.62; acc: 0.5
Batch: 100; loss: 1.47; acc: 0.69
Batch: 120; loss: 1.56; acc: 0.56
Batch: 140; loss: 1.53; acc: 0.62
Batch: 160; loss: 1.46; acc: 0.67
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.4; acc: 0.72
Batch: 220; loss: 1.29; acc: 0.69
Batch: 240; loss: 1.23; acc: 0.73
Batch: 260; loss: 1.29; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.84
Batch: 300; loss: 1.28; acc: 0.69
Batch: 320; loss: 1.11; acc: 0.72
Batch: 340; loss: 1.02; acc: 0.86
Batch: 360; loss: 1.0; acc: 0.92
Batch: 380; loss: 1.12; acc: 0.83
Batch: 400; loss: 1.08; acc: 0.8
Batch: 420; loss: 1.13; acc: 0.72
Batch: 440; loss: 1.08; acc: 0.77
Batch: 460; loss: 0.97; acc: 0.84
Batch: 480; loss: 0.91; acc: 0.83
Batch: 500; loss: 0.91; acc: 0.83
Batch: 520; loss: 0.96; acc: 0.83
Batch: 540; loss: 0.92; acc: 0.88
Batch: 560; loss: 0.89; acc: 0.86
Batch: 580; loss: 0.98; acc: 0.83
Batch: 600; loss: 0.91; acc: 0.77
Batch: 620; loss: 0.96; acc: 0.81
Batch: 640; loss: 0.86; acc: 0.84
Batch: 660; loss: 0.88; acc: 0.84
Batch: 680; loss: 0.91; acc: 0.78
Batch: 700; loss: 0.8; acc: 0.88
Batch: 720; loss: 0.81; acc: 0.88
Batch: 740; loss: 0.89; acc: 0.84
Batch: 760; loss: 0.99; acc: 0.72
Batch: 780; loss: 0.94; acc: 0.8
Train Epoch over. train_loss: 1.17; train_accuracy: 0.75 

2.6833260562852956e-05
9.495766789768822e-06
Batch: 0; loss: 0.79; acc: 0.91
Batch: 20; loss: 1.03; acc: 0.75
Batch: 40; loss: 0.52; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.94
Batch: 100; loss: 0.76; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 0.67; acc: 0.92
Val Epoch over. val_loss: 0.7860210131687723; val_accuracy: 0.8747014331210191 

The current subspace-distance is: 9.495766789768822e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.86
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.85; acc: 0.81
Batch: 120; loss: 0.81; acc: 0.89
Batch: 140; loss: 0.87; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.92
Batch: 180; loss: 0.86; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.89
Batch: 220; loss: 0.7; acc: 0.89
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.71; acc: 0.88
Batch: 280; loss: 0.7; acc: 0.89
Batch: 300; loss: 0.75; acc: 0.89
Batch: 320; loss: 0.75; acc: 0.84
Batch: 340; loss: 0.6; acc: 0.95
Batch: 360; loss: 0.74; acc: 0.84
Batch: 380; loss: 0.79; acc: 0.84
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.81; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.84
Batch: 480; loss: 0.72; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.92
Batch: 520; loss: 0.77; acc: 0.86
Batch: 540; loss: 0.7; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.92
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.92
Batch: 640; loss: 0.77; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.89
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.53; acc: 0.97
Batch: 720; loss: 0.73; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.89
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.7; acc: 0.88
Train Epoch over. train_loss: 0.74; train_accuracy: 0.86 

3.3451273338869214e-05
1.372424594592303e-05
Batch: 0; loss: 0.56; acc: 0.95
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.599264986385965; val_accuracy: 0.8935111464968153 

The current subspace-distance is: 1.372424594592303e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.88
Batch: 40; loss: 0.71; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.92
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.54; acc: 0.95
Batch: 140; loss: 0.57; acc: 0.92
Batch: 160; loss: 0.58; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.94
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.94
Batch: 280; loss: 0.58; acc: 0.89
Batch: 300; loss: 0.65; acc: 0.88
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.95
Batch: 400; loss: 0.55; acc: 0.95
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.89
Batch: 560; loss: 0.57; acc: 0.91
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.95
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.88 

3.812211434706114e-05
1.524070103187114e-05
Batch: 0; loss: 0.45; acc: 0.97
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.5067664885027393; val_accuracy: 0.9050557324840764 

The current subspace-distance is: 1.524070103187114e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.94
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.44; acc: 0.94
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.92
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.97
Batch: 480; loss: 0.53; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.94
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.95
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

4.245680247549899e-05
1.7644308172748424e-05
Batch: 0; loss: 0.38; acc: 1.0
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.4480600625656213; val_accuracy: 0.9125199044585988 

The current subspace-distance is: 1.7644308172748424e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.97
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.95
Batch: 140; loss: 0.49; acc: 0.94
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.94
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.44; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.97
Batch: 500; loss: 0.54; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.6; acc: 0.91
Batch: 560; loss: 0.67; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.97
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

4.519088543020189e-05
1.9325301764183678e-05
Batch: 0; loss: 0.34; acc: 0.98
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4120288263460633; val_accuracy: 0.915406050955414 

The current subspace-distance is: 1.9325301764183678e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.98
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.97
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.59; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.98
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.95
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.97
Batch: 620; loss: 0.4; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.91 

4.8246703954646364e-05
2.1912459487793967e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.37856082570780614; val_accuracy: 0.9209792993630573 

The current subspace-distance is: 2.1912459487793967e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.072918065707199e-05
2.273339850944467e-05
Batch: 0; loss: 0.28; acc: 0.98
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3557490680343027; val_accuracy: 0.9221735668789809 

The current subspace-distance is: 2.273339850944467e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.306684397510253e-05
2.448117629683111e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.33865832974007176; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 2.448117629683111e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.534001320484094e-05
2.5035998987732455e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.31984441361989185; val_accuracy: 0.9280453821656051 

The current subspace-distance is: 2.5035998987732455e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.706488445866853e-05
2.5949151677195914e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3092428326701662; val_accuracy: 0.9301353503184714 

The current subspace-distance is: 2.5949151677195914e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.98
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.98
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.720217450289056e-05
2.6016949050244875e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.30195557444718235; val_accuracy: 0.9316281847133758 

The current subspace-distance is: 2.6016949050244875e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.98
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.805048203910701e-05
2.6508369046496227e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3019033985058214; val_accuracy: 0.9322253184713376 

The current subspace-distance is: 2.6508369046496227e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.97
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.8741148677654564e-05
2.6723837436293252e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.29561246494958354; val_accuracy: 0.933718152866242 

The current subspace-distance is: 2.6723837436293252e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.9214075008640066e-05
2.672880691534374e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.29582953884912905; val_accuracy: 0.9341162420382165 

The current subspace-distance is: 2.672880691534374e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.9527650591917336e-05
2.75217662419891e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2939786407503353; val_accuracy: 0.9330214968152867 

The current subspace-distance is: 2.75217662419891e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.22; acc: 1.0
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.98
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.0274291172390804e-05
2.7531938030733727e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.28653342058514336; val_accuracy: 0.9351114649681529 

The current subspace-distance is: 2.7531938030733727e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.98
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.98
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.0651098465314135e-05
2.8320044293650426e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.28840674080286816; val_accuracy: 0.9343152866242038 

The current subspace-distance is: 2.8320044293650426e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.046790804248303e-05
2.7039239284931682e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.282934458344985; val_accuracy: 0.9363057324840764 

The current subspace-distance is: 2.7039239284931682e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.22; acc: 1.0
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.15; acc: 1.0
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.98
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.191461579874158e-05
2.9434500902425498e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.27882556413199494; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.9434500902425498e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.81
Batch: 200; loss: 0.23; acc: 0.98
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.22; acc: 1.0
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.150627450551838e-05
2.58918425970478e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.28140850194320555; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.58918425970478e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.98
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.98
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.17; acc: 0.98
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.311416655080393e-05
3.0242526918300427e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2790450993332134; val_accuracy: 0.9365047770700637 

The current subspace-distance is: 3.0242526918300427e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.97
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.98
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.98
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.233731983229518e-05
2.8215230486239307e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2773763112201812; val_accuracy: 0.9372014331210191 

The current subspace-distance is: 2.8215230486239307e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.98
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.200605275807902e-05
2.7349393349140882e-05
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.27582925700457994; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.7349393349140882e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.98
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.98
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.98
Batch: 740; loss: 0.4; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.280314846662804e-05
2.855787352018524e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.27177442477387226; val_accuracy: 0.9376990445859873 

The current subspace-distance is: 2.855787352018524e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.22; acc: 1.0
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.98
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.3; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.220185605343431e-05
2.7005307856597938e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.27357324331429356; val_accuracy: 0.9373009554140127 

The current subspace-distance is: 2.7005307856597938e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.98
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.344937719404697e-05
2.8532052965601906e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.275060039796647; val_accuracy: 0.9351114649681529 

The current subspace-distance is: 2.8532052965601906e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.21; acc: 1.0
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.274416227824986e-05
2.8237909646122716e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.269232505469755; val_accuracy: 0.9384952229299363 

The current subspace-distance is: 2.8237909646122716e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.98
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.81
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.22; acc: 1.0
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.29522037343122e-05
2.875081008824054e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.27059532336558506; val_accuracy: 0.9374004777070064 

The current subspace-distance is: 2.875081008824054e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.98
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.98
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.295427738223225e-05
2.7213001885684207e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.27280328161777206; val_accuracy: 0.9380971337579618 

The current subspace-distance is: 2.7213001885684207e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.52; acc: 0.8
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.395034870365635e-05
2.95713161904132e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2711264514809201; val_accuracy: 0.9375 

The current subspace-distance is: 2.95713161904132e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:52/N_12_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:52/N_12_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
