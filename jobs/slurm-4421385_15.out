model : table13slim
N : 15
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:47

Channel scaling factor: 1.5620723414746194

The number of parameters is: 263271

The number of individual parameters is:

13
234
13
13
19
37791
19
19
38
110466
38
38
64
109440
64
64
4096
64
640
10
64
64

nonzero elements in E: 13163549
elements in E: 13163550
fraction nonzero: 0.9999999240326508
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.39; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.09
Batch: 40; loss: 2.28; acc: 0.17
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.11; acc: 0.27
Batch: 100; loss: 2.09; acc: 0.22
Batch: 120; loss: 2.15; acc: 0.28
Batch: 140; loss: 2.12; acc: 0.27
Batch: 160; loss: 2.12; acc: 0.3
Batch: 180; loss: 1.99; acc: 0.33
Batch: 200; loss: 2.08; acc: 0.28
Batch: 220; loss: 2.06; acc: 0.31
Batch: 240; loss: 1.98; acc: 0.36
Batch: 260; loss: 2.09; acc: 0.33
Batch: 280; loss: 2.01; acc: 0.31
Batch: 300; loss: 2.01; acc: 0.31
Batch: 320; loss: 2.02; acc: 0.34
Batch: 340; loss: 2.05; acc: 0.31
Batch: 360; loss: 1.82; acc: 0.52
Batch: 380; loss: 1.96; acc: 0.38
Batch: 400; loss: 2.06; acc: 0.31
Batch: 420; loss: 2.04; acc: 0.36
Batch: 440; loss: 1.99; acc: 0.36
Batch: 460; loss: 2.03; acc: 0.36
Batch: 480; loss: 1.97; acc: 0.36
Batch: 500; loss: 1.99; acc: 0.38
Batch: 520; loss: 2.0; acc: 0.33
Batch: 540; loss: 2.05; acc: 0.3
Batch: 560; loss: 1.94; acc: 0.39
Batch: 580; loss: 1.95; acc: 0.38
Batch: 600; loss: 1.97; acc: 0.36
Batch: 620; loss: 1.93; acc: 0.41
Batch: 640; loss: 2.01; acc: 0.38
Batch: 660; loss: 1.9; acc: 0.48
Batch: 680; loss: 1.94; acc: 0.42
Batch: 700; loss: 1.91; acc: 0.47
Batch: 720; loss: 2.01; acc: 0.3
Batch: 740; loss: 1.97; acc: 0.34
Batch: 760; loss: 2.05; acc: 0.28
Batch: 780; loss: 1.98; acc: 0.33
Train Epoch over. train_loss: 2.05; train_accuracy: 0.3 

2.159966061299201e-05
3.3211199479410425e-06
Batch: 0; loss: 1.92; acc: 0.36
Batch: 20; loss: 2.05; acc: 0.3
Batch: 40; loss: 1.83; acc: 0.5
Batch: 60; loss: 1.95; acc: 0.42
Batch: 80; loss: 1.82; acc: 0.5
Batch: 100; loss: 1.93; acc: 0.34
Batch: 120; loss: 1.92; acc: 0.44
Batch: 140; loss: 1.92; acc: 0.41
Val Epoch over. val_loss: 1.9395964024173227; val_accuracy: 0.3762937898089172 

The current subspace-distance is: 3.3211199479410425e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.92; acc: 0.47
Batch: 20; loss: 1.96; acc: 0.45
Batch: 40; loss: 2.0; acc: 0.3
Batch: 60; loss: 2.06; acc: 0.33
Batch: 80; loss: 1.88; acc: 0.42
Batch: 100; loss: 1.97; acc: 0.38
Batch: 120; loss: 2.0; acc: 0.31
Batch: 140; loss: 1.95; acc: 0.36
Batch: 160; loss: 2.0; acc: 0.33
Batch: 180; loss: 1.87; acc: 0.48
Batch: 200; loss: 1.86; acc: 0.44
Batch: 220; loss: 1.94; acc: 0.38
Batch: 240; loss: 1.96; acc: 0.38
Batch: 260; loss: 1.88; acc: 0.39
Batch: 280; loss: 1.91; acc: 0.45
Batch: 300; loss: 1.92; acc: 0.39
Batch: 320; loss: 1.93; acc: 0.41
Batch: 340; loss: 1.96; acc: 0.38
Batch: 360; loss: 1.84; acc: 0.38
Batch: 380; loss: 1.91; acc: 0.53
Batch: 400; loss: 1.83; acc: 0.52
Batch: 420; loss: 1.98; acc: 0.36
Batch: 440; loss: 1.83; acc: 0.45
Batch: 460; loss: 1.86; acc: 0.47
Batch: 480; loss: 1.84; acc: 0.44
Batch: 500; loss: 1.99; acc: 0.3
Batch: 520; loss: 1.84; acc: 0.38
Batch: 540; loss: 1.94; acc: 0.34
Batch: 560; loss: 1.92; acc: 0.34
Batch: 580; loss: 1.85; acc: 0.44
Batch: 600; loss: 1.88; acc: 0.44
Batch: 620; loss: 1.96; acc: 0.38
Batch: 640; loss: 1.83; acc: 0.44
Batch: 660; loss: 1.87; acc: 0.44
Batch: 680; loss: 1.94; acc: 0.31
Batch: 700; loss: 1.98; acc: 0.36
Batch: 720; loss: 1.96; acc: 0.3
Batch: 740; loss: 1.87; acc: 0.45
Batch: 760; loss: 1.84; acc: 0.42
Batch: 780; loss: 1.88; acc: 0.39
Train Epoch over. train_loss: 1.91; train_accuracy: 0.39 

2.3020118533167988e-05
3.794444864979596e-06
Batch: 0; loss: 1.83; acc: 0.47
Batch: 20; loss: 2.0; acc: 0.33
Batch: 40; loss: 1.73; acc: 0.58
Batch: 60; loss: 1.8; acc: 0.48
Batch: 80; loss: 1.76; acc: 0.5
Batch: 100; loss: 1.92; acc: 0.39
Batch: 120; loss: 1.88; acc: 0.41
Batch: 140; loss: 1.85; acc: 0.47
Val Epoch over. val_loss: 1.860489596986467; val_accuracy: 0.4303343949044586 

The current subspace-distance is: 3.794444864979596e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.88; acc: 0.39
Batch: 20; loss: 1.91; acc: 0.39
Batch: 40; loss: 1.87; acc: 0.45
Batch: 60; loss: 1.89; acc: 0.39
Batch: 80; loss: 1.8; acc: 0.42
Batch: 100; loss: 1.9; acc: 0.41
Batch: 120; loss: 1.83; acc: 0.45
Batch: 140; loss: 1.87; acc: 0.41
Batch: 160; loss: 1.77; acc: 0.45
Batch: 180; loss: 1.88; acc: 0.44
Batch: 200; loss: 1.9; acc: 0.34
Batch: 220; loss: 1.84; acc: 0.44
Batch: 240; loss: 2.01; acc: 0.34
Batch: 260; loss: 1.98; acc: 0.28
Batch: 280; loss: 1.9; acc: 0.42
Batch: 300; loss: 1.79; acc: 0.44
Batch: 320; loss: 1.81; acc: 0.48
Batch: 340; loss: 2.0; acc: 0.3
Batch: 360; loss: 1.73; acc: 0.52
Batch: 380; loss: 1.95; acc: 0.34
Batch: 400; loss: 1.84; acc: 0.45
Batch: 420; loss: 1.83; acc: 0.5
Batch: 440; loss: 1.94; acc: 0.39
Batch: 460; loss: 1.81; acc: 0.47
Batch: 480; loss: 1.9; acc: 0.41
Batch: 500; loss: 1.9; acc: 0.41
Batch: 520; loss: 1.83; acc: 0.33
Batch: 540; loss: 1.84; acc: 0.36
Batch: 560; loss: 1.89; acc: 0.34
Batch: 580; loss: 1.86; acc: 0.38
Batch: 600; loss: 1.78; acc: 0.48
Batch: 620; loss: 1.88; acc: 0.41
Batch: 640; loss: 1.8; acc: 0.45
Batch: 660; loss: 1.83; acc: 0.47
Batch: 680; loss: 1.85; acc: 0.48
Batch: 700; loss: 1.84; acc: 0.38
Batch: 720; loss: 1.72; acc: 0.53
Batch: 740; loss: 1.84; acc: 0.38
Batch: 760; loss: 1.85; acc: 0.45
Batch: 780; loss: 1.81; acc: 0.44
Train Epoch over. train_loss: 1.86; train_accuracy: 0.42 

2.5093431759160012e-05
6.1021200963296e-06
Batch: 0; loss: 1.79; acc: 0.53
Batch: 20; loss: 2.01; acc: 0.33
Batch: 40; loss: 1.7; acc: 0.52
Batch: 60; loss: 1.74; acc: 0.45
Batch: 80; loss: 1.73; acc: 0.52
Batch: 100; loss: 1.91; acc: 0.39
Batch: 120; loss: 1.86; acc: 0.44
Batch: 140; loss: 1.83; acc: 0.47
Val Epoch over. val_loss: 1.8228855732899563; val_accuracy: 0.4408837579617834 

The current subspace-distance is: 6.1021200963296e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.84; acc: 0.45
Batch: 20; loss: 1.93; acc: 0.41
Batch: 40; loss: 1.79; acc: 0.55
Batch: 60; loss: 1.83; acc: 0.39
Batch: 80; loss: 1.94; acc: 0.42
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.78; acc: 0.5
Batch: 140; loss: 1.82; acc: 0.39
Batch: 160; loss: 1.87; acc: 0.34
Batch: 180; loss: 1.77; acc: 0.39
Batch: 200; loss: 1.75; acc: 0.48
Batch: 220; loss: 1.77; acc: 0.48
Batch: 240; loss: 1.85; acc: 0.41
Batch: 260; loss: 1.78; acc: 0.44
Batch: 280; loss: 1.92; acc: 0.36
Batch: 300; loss: 1.85; acc: 0.44
Batch: 320; loss: 1.84; acc: 0.39
Batch: 340; loss: 1.85; acc: 0.44
Batch: 360; loss: 1.92; acc: 0.42
Batch: 380; loss: 1.88; acc: 0.33
Batch: 400; loss: 1.84; acc: 0.39
Batch: 420; loss: 1.73; acc: 0.56
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.87; acc: 0.45
Batch: 480; loss: 1.79; acc: 0.52
Batch: 500; loss: 1.84; acc: 0.42
Batch: 520; loss: 1.89; acc: 0.36
Batch: 540; loss: 1.76; acc: 0.5
Batch: 560; loss: 1.91; acc: 0.42
Batch: 580; loss: 1.85; acc: 0.31
Batch: 600; loss: 1.69; acc: 0.58
Batch: 620; loss: 1.79; acc: 0.47
Batch: 640; loss: 1.89; acc: 0.33
Batch: 660; loss: 1.84; acc: 0.48
Batch: 680; loss: 1.82; acc: 0.42
Batch: 700; loss: 1.78; acc: 0.45
Batch: 720; loss: 1.9; acc: 0.38
Batch: 740; loss: 1.78; acc: 0.48
Batch: 760; loss: 1.81; acc: 0.42
Batch: 780; loss: 1.81; acc: 0.45
Train Epoch over. train_loss: 1.82; train_accuracy: 0.44 

2.645314270921517e-05
5.87218801229028e-06
Batch: 0; loss: 1.77; acc: 0.59
Batch: 20; loss: 1.97; acc: 0.28
Batch: 40; loss: 1.63; acc: 0.56
Batch: 60; loss: 1.69; acc: 0.42
Batch: 80; loss: 1.69; acc: 0.58
Batch: 100; loss: 1.84; acc: 0.41
Batch: 120; loss: 1.78; acc: 0.42
Batch: 140; loss: 1.76; acc: 0.48
Val Epoch over. val_loss: 1.7685521689190227; val_accuracy: 0.482484076433121 

The current subspace-distance is: 5.87218801229028e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.47
Batch: 20; loss: 1.93; acc: 0.36
Batch: 40; loss: 1.81; acc: 0.38
Batch: 60; loss: 1.77; acc: 0.47
Batch: 80; loss: 1.73; acc: 0.5
Batch: 100; loss: 1.83; acc: 0.56
Batch: 120; loss: 1.79; acc: 0.47
Batch: 140; loss: 1.75; acc: 0.5
Batch: 160; loss: 1.7; acc: 0.5
Batch: 180; loss: 1.92; acc: 0.34
Batch: 200; loss: 1.67; acc: 0.5
Batch: 220; loss: 1.71; acc: 0.47
Batch: 240; loss: 1.79; acc: 0.44
Batch: 260; loss: 1.78; acc: 0.44
Batch: 280; loss: 1.76; acc: 0.52
Batch: 300; loss: 1.85; acc: 0.42
Batch: 320; loss: 1.78; acc: 0.42
Batch: 340; loss: 1.81; acc: 0.39
Batch: 360; loss: 1.73; acc: 0.55
Batch: 380; loss: 1.85; acc: 0.42
Batch: 400; loss: 1.8; acc: 0.47
Batch: 420; loss: 1.74; acc: 0.55
Batch: 440; loss: 1.73; acc: 0.53
Batch: 460; loss: 1.82; acc: 0.47
Batch: 480; loss: 1.63; acc: 0.62
Batch: 500; loss: 1.7; acc: 0.52
Batch: 520; loss: 1.8; acc: 0.42
Batch: 540; loss: 1.67; acc: 0.58
Batch: 560; loss: 1.82; acc: 0.42
Batch: 580; loss: 1.94; acc: 0.34
Batch: 600; loss: 1.7; acc: 0.48
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.61; acc: 0.61
Batch: 660; loss: 1.85; acc: 0.45
Batch: 680; loss: 1.6; acc: 0.66
Batch: 700; loss: 1.7; acc: 0.52
Batch: 720; loss: 1.8; acc: 0.41
Batch: 740; loss: 1.86; acc: 0.39
Batch: 760; loss: 1.68; acc: 0.5
Batch: 780; loss: 1.64; acc: 0.58
Train Epoch over. train_loss: 1.76; train_accuracy: 0.48 

2.9423676096484996e-05
1.0733134331530891e-05
Batch: 0; loss: 1.73; acc: 0.55
Batch: 20; loss: 1.92; acc: 0.39
Batch: 40; loss: 1.51; acc: 0.66
Batch: 60; loss: 1.61; acc: 0.48
Batch: 80; loss: 1.64; acc: 0.58
Batch: 100; loss: 1.74; acc: 0.47
Batch: 120; loss: 1.7; acc: 0.55
Batch: 140; loss: 1.65; acc: 0.58
Val Epoch over. val_loss: 1.6971331045126459; val_accuracy: 0.5166202229299363 

The current subspace-distance is: 1.0733134331530891e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.53
Batch: 20; loss: 1.68; acc: 0.52
Batch: 40; loss: 1.76; acc: 0.55
Batch: 60; loss: 1.59; acc: 0.59
Batch: 80; loss: 1.79; acc: 0.58
Batch: 100; loss: 1.66; acc: 0.5
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.64; acc: 0.48
Batch: 160; loss: 1.69; acc: 0.52
Batch: 180; loss: 1.81; acc: 0.41
Batch: 200; loss: 1.68; acc: 0.48
Batch: 220; loss: 1.76; acc: 0.44
Batch: 240; loss: 1.73; acc: 0.48
Batch: 260; loss: 1.71; acc: 0.47
Batch: 280; loss: 1.73; acc: 0.42
Batch: 300; loss: 1.61; acc: 0.58
Batch: 320; loss: 1.73; acc: 0.48
Batch: 340; loss: 1.74; acc: 0.41
Batch: 360; loss: 1.69; acc: 0.52
Batch: 380; loss: 1.64; acc: 0.47
Batch: 400; loss: 1.59; acc: 0.58
Batch: 420; loss: 1.67; acc: 0.55
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.69; acc: 0.52
Batch: 480; loss: 1.73; acc: 0.44
Batch: 500; loss: 1.72; acc: 0.44
Batch: 520; loss: 1.61; acc: 0.5
Batch: 540; loss: 1.52; acc: 0.64
Batch: 560; loss: 1.57; acc: 0.59
Batch: 580; loss: 1.69; acc: 0.44
Batch: 600; loss: 1.69; acc: 0.47
Batch: 620; loss: 1.6; acc: 0.5
Batch: 640; loss: 1.68; acc: 0.44
Batch: 660; loss: 1.66; acc: 0.5
Batch: 680; loss: 1.69; acc: 0.55
Batch: 700; loss: 1.65; acc: 0.56
Batch: 720; loss: 1.7; acc: 0.47
Batch: 740; loss: 1.77; acc: 0.39
Batch: 760; loss: 1.72; acc: 0.52
Batch: 780; loss: 1.69; acc: 0.47
Train Epoch over. train_loss: 1.69; train_accuracy: 0.49 

3.2658299460308626e-05
1.0775903319881763e-05
Batch: 0; loss: 1.73; acc: 0.5
Batch: 20; loss: 1.86; acc: 0.42
Batch: 40; loss: 1.43; acc: 0.67
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.6; acc: 0.55
Batch: 100; loss: 1.64; acc: 0.56
Batch: 120; loss: 1.62; acc: 0.56
Batch: 140; loss: 1.54; acc: 0.55
Val Epoch over. val_loss: 1.6438649976329438; val_accuracy: 0.5246815286624203 

The current subspace-distance is: 1.0775903319881763e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.42
Batch: 20; loss: 1.58; acc: 0.61
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.67; acc: 0.55
Batch: 80; loss: 1.74; acc: 0.38
Batch: 100; loss: 1.68; acc: 0.48
Batch: 120; loss: 1.62; acc: 0.55
Batch: 140; loss: 1.61; acc: 0.52
Batch: 160; loss: 1.64; acc: 0.52
Batch: 180; loss: 1.74; acc: 0.45
Batch: 200; loss: 1.63; acc: 0.44
Batch: 220; loss: 1.66; acc: 0.5
Batch: 240; loss: 1.65; acc: 0.52
Batch: 260; loss: 1.7; acc: 0.48
Batch: 280; loss: 1.58; acc: 0.53
Batch: 300; loss: 1.58; acc: 0.52
Batch: 320; loss: 1.71; acc: 0.47
Batch: 340; loss: 1.61; acc: 0.52
Batch: 360; loss: 1.61; acc: 0.55
Batch: 380; loss: 1.67; acc: 0.52
Batch: 400; loss: 1.67; acc: 0.45
Batch: 420; loss: 1.71; acc: 0.47
Batch: 440; loss: 1.61; acc: 0.47
Batch: 460; loss: 1.61; acc: 0.5
Batch: 480; loss: 1.66; acc: 0.47
Batch: 500; loss: 1.77; acc: 0.48
Batch: 520; loss: 1.53; acc: 0.59
Batch: 540; loss: 1.88; acc: 0.33
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.77; acc: 0.36
Batch: 600; loss: 1.6; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.55
Batch: 640; loss: 1.56; acc: 0.62
Batch: 660; loss: 1.74; acc: 0.47
Batch: 680; loss: 1.55; acc: 0.55
Batch: 700; loss: 1.61; acc: 0.56
Batch: 720; loss: 1.62; acc: 0.56
Batch: 740; loss: 1.52; acc: 0.55
Batch: 760; loss: 1.53; acc: 0.56
Batch: 780; loss: 1.62; acc: 0.53
Train Epoch over. train_loss: 1.65; train_accuracy: 0.5 

3.405169991310686e-05
1.0351209311920684e-05
Batch: 0; loss: 1.74; acc: 0.48
Batch: 20; loss: 1.82; acc: 0.41
Batch: 40; loss: 1.38; acc: 0.69
Batch: 60; loss: 1.51; acc: 0.58
Batch: 80; loss: 1.57; acc: 0.56
Batch: 100; loss: 1.59; acc: 0.56
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.47; acc: 0.59
Val Epoch over. val_loss: 1.6093584306680473; val_accuracy: 0.5333399681528662 

The current subspace-distance is: 1.0351209311920684e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.44
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.54; acc: 0.55
Batch: 60; loss: 1.61; acc: 0.56
Batch: 80; loss: 1.71; acc: 0.44
Batch: 100; loss: 1.61; acc: 0.58
Batch: 120; loss: 1.58; acc: 0.53
Batch: 140; loss: 1.53; acc: 0.53
Batch: 160; loss: 1.69; acc: 0.48
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.66; acc: 0.5
Batch: 220; loss: 1.56; acc: 0.52
Batch: 240; loss: 1.59; acc: 0.53
Batch: 260; loss: 1.6; acc: 0.55
Batch: 280; loss: 1.64; acc: 0.48
Batch: 300; loss: 1.62; acc: 0.48
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.54; acc: 0.53
Batch: 360; loss: 1.7; acc: 0.39
Batch: 380; loss: 1.62; acc: 0.48
Batch: 400; loss: 1.73; acc: 0.41
Batch: 420; loss: 1.59; acc: 0.56
Batch: 440; loss: 1.55; acc: 0.69
Batch: 460; loss: 1.78; acc: 0.47
Batch: 480; loss: 1.64; acc: 0.55
Batch: 500; loss: 1.8; acc: 0.33
Batch: 520; loss: 1.7; acc: 0.45
Batch: 540; loss: 1.71; acc: 0.42
Batch: 560; loss: 1.73; acc: 0.38
Batch: 580; loss: 1.42; acc: 0.64
Batch: 600; loss: 1.58; acc: 0.56
Batch: 620; loss: 1.61; acc: 0.52
Batch: 640; loss: 1.65; acc: 0.53
Batch: 660; loss: 1.57; acc: 0.56
Batch: 680; loss: 1.62; acc: 0.53
Batch: 700; loss: 1.58; acc: 0.52
Batch: 720; loss: 1.72; acc: 0.5
Batch: 740; loss: 1.57; acc: 0.5
Batch: 760; loss: 1.69; acc: 0.41
Batch: 780; loss: 1.7; acc: 0.47
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

3.670140722533688e-05
1.154485835286323e-05
Batch: 0; loss: 1.72; acc: 0.47
Batch: 20; loss: 1.8; acc: 0.41
Batch: 40; loss: 1.36; acc: 0.69
Batch: 60; loss: 1.5; acc: 0.55
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.56; acc: 0.58
Batch: 120; loss: 1.59; acc: 0.59
Batch: 140; loss: 1.43; acc: 0.69
Val Epoch over. val_loss: 1.591903239298778; val_accuracy: 0.5373208598726115 

The current subspace-distance is: 1.154485835286323e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.5
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.6; acc: 0.55
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.42
Batch: 100; loss: 1.66; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.5
Batch: 140; loss: 1.6; acc: 0.55
Batch: 160; loss: 1.64; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.52
Batch: 200; loss: 1.47; acc: 0.58
Batch: 220; loss: 1.72; acc: 0.41
Batch: 240; loss: 1.74; acc: 0.45
Batch: 260; loss: 1.74; acc: 0.38
Batch: 280; loss: 1.64; acc: 0.48
Batch: 300; loss: 1.63; acc: 0.5
Batch: 320; loss: 1.68; acc: 0.55
Batch: 340; loss: 1.53; acc: 0.58
Batch: 360; loss: 1.59; acc: 0.56
Batch: 380; loss: 1.59; acc: 0.55
Batch: 400; loss: 1.61; acc: 0.48
Batch: 420; loss: 1.66; acc: 0.47
Batch: 440; loss: 1.47; acc: 0.59
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.56; acc: 0.55
Batch: 500; loss: 1.73; acc: 0.44
Batch: 520; loss: 1.62; acc: 0.5
Batch: 540; loss: 1.56; acc: 0.5
Batch: 560; loss: 1.68; acc: 0.47
Batch: 580; loss: 1.52; acc: 0.55
Batch: 600; loss: 1.56; acc: 0.55
Batch: 620; loss: 1.65; acc: 0.45
Batch: 640; loss: 1.57; acc: 0.56
Batch: 660; loss: 1.59; acc: 0.56
Batch: 680; loss: 1.67; acc: 0.52
Batch: 700; loss: 1.73; acc: 0.47
Batch: 720; loss: 1.55; acc: 0.47
Batch: 740; loss: 1.7; acc: 0.41
Batch: 760; loss: 1.56; acc: 0.48
Batch: 780; loss: 1.67; acc: 0.48
Train Epoch over. train_loss: 1.62; train_accuracy: 0.5 

3.8841568311909214e-05
1.5092943613126408e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.34; acc: 0.67
Batch: 60; loss: 1.48; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.56; acc: 0.55
Batch: 140; loss: 1.4; acc: 0.69
Val Epoch over. val_loss: 1.5761356156343107; val_accuracy: 0.5343351910828026 

The current subspace-distance is: 1.5092943613126408e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.57; acc: 0.58
Batch: 20; loss: 1.58; acc: 0.5
Batch: 40; loss: 1.84; acc: 0.44
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.69; acc: 0.48
Batch: 120; loss: 1.63; acc: 0.5
Batch: 140; loss: 1.74; acc: 0.42
Batch: 160; loss: 1.63; acc: 0.41
Batch: 180; loss: 1.65; acc: 0.45
Batch: 200; loss: 1.53; acc: 0.55
Batch: 220; loss: 1.71; acc: 0.42
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.57; acc: 0.52
Batch: 280; loss: 1.57; acc: 0.52
Batch: 300; loss: 1.8; acc: 0.42
Batch: 320; loss: 1.52; acc: 0.56
Batch: 340; loss: 1.5; acc: 0.56
Batch: 360; loss: 1.65; acc: 0.48
Batch: 380; loss: 1.63; acc: 0.48
Batch: 400; loss: 1.64; acc: 0.52
Batch: 420; loss: 1.78; acc: 0.44
Batch: 440; loss: 1.53; acc: 0.55
Batch: 460; loss: 1.65; acc: 0.47
Batch: 480; loss: 1.7; acc: 0.39
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.55; acc: 0.59
Batch: 540; loss: 1.66; acc: 0.47
Batch: 560; loss: 1.73; acc: 0.53
Batch: 580; loss: 1.57; acc: 0.53
Batch: 600; loss: 1.65; acc: 0.45
Batch: 620; loss: 1.52; acc: 0.48
Batch: 640; loss: 1.64; acc: 0.47
Batch: 660; loss: 1.55; acc: 0.47
Batch: 680; loss: 1.69; acc: 0.42
Batch: 700; loss: 1.46; acc: 0.53
Batch: 720; loss: 1.72; acc: 0.42
Batch: 740; loss: 1.75; acc: 0.42
Batch: 760; loss: 1.56; acc: 0.62
Batch: 780; loss: 1.73; acc: 0.42
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

4.007408278994262e-05
1.5345718566095456e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.75; acc: 0.45
Batch: 40; loss: 1.32; acc: 0.73
Batch: 60; loss: 1.47; acc: 0.58
Batch: 80; loss: 1.57; acc: 0.55
Batch: 100; loss: 1.49; acc: 0.62
Batch: 120; loss: 1.55; acc: 0.56
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5667478840821867; val_accuracy: 0.5382165605095541 

The current subspace-distance is: 1.5345718566095456e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.57; acc: 0.53
Batch: 60; loss: 1.68; acc: 0.42
Batch: 80; loss: 1.72; acc: 0.34
Batch: 100; loss: 1.43; acc: 0.56
Batch: 120; loss: 1.69; acc: 0.47
Batch: 140; loss: 1.6; acc: 0.52
Batch: 160; loss: 1.5; acc: 0.56
Batch: 180; loss: 1.73; acc: 0.42
Batch: 200; loss: 1.49; acc: 0.53
Batch: 220; loss: 1.43; acc: 0.67
Batch: 240; loss: 1.64; acc: 0.42
Batch: 260; loss: 1.6; acc: 0.53
Batch: 280; loss: 1.69; acc: 0.44
Batch: 300; loss: 1.58; acc: 0.59
Batch: 320; loss: 1.53; acc: 0.59
Batch: 340; loss: 1.59; acc: 0.59
Batch: 360; loss: 1.73; acc: 0.44
Batch: 380; loss: 1.55; acc: 0.58
Batch: 400; loss: 1.59; acc: 0.5
Batch: 420; loss: 1.63; acc: 0.44
Batch: 440; loss: 1.71; acc: 0.34
Batch: 460; loss: 1.63; acc: 0.52
Batch: 480; loss: 1.59; acc: 0.45
Batch: 500; loss: 1.74; acc: 0.39
Batch: 520; loss: 1.7; acc: 0.45
Batch: 540; loss: 1.6; acc: 0.52
Batch: 560; loss: 1.69; acc: 0.52
Batch: 580; loss: 1.52; acc: 0.58
Batch: 600; loss: 1.53; acc: 0.56
Batch: 620; loss: 1.57; acc: 0.5
Batch: 640; loss: 1.57; acc: 0.53
Batch: 660; loss: 1.56; acc: 0.61
Batch: 680; loss: 1.7; acc: 0.5
Batch: 700; loss: 1.77; acc: 0.39
Batch: 720; loss: 1.55; acc: 0.53
Batch: 740; loss: 1.53; acc: 0.61
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.54; acc: 0.53
Train Epoch over. train_loss: 1.6; train_accuracy: 0.5 

4.0405142499366775e-05
1.1584729691094253e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.75; acc: 0.44
Batch: 40; loss: 1.32; acc: 0.72
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.56; acc: 0.53
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.56; acc: 0.58
Batch: 140; loss: 1.38; acc: 0.7
Val Epoch over. val_loss: 1.558864317881833; val_accuracy: 0.535828025477707 

The current subspace-distance is: 1.1584729691094253e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.52
Batch: 20; loss: 1.53; acc: 0.58
Batch: 40; loss: 1.79; acc: 0.36
Batch: 60; loss: 1.41; acc: 0.64
Batch: 80; loss: 1.64; acc: 0.47
Batch: 100; loss: 1.48; acc: 0.56
Batch: 120; loss: 1.63; acc: 0.55
Batch: 140; loss: 1.57; acc: 0.56
Batch: 160; loss: 1.69; acc: 0.44
Batch: 180; loss: 1.51; acc: 0.55
Batch: 200; loss: 1.49; acc: 0.62
Batch: 220; loss: 1.65; acc: 0.48
Batch: 240; loss: 1.53; acc: 0.58
Batch: 260; loss: 1.66; acc: 0.47
Batch: 280; loss: 1.55; acc: 0.5
Batch: 300; loss: 1.63; acc: 0.48
Batch: 320; loss: 1.58; acc: 0.53
Batch: 340; loss: 1.52; acc: 0.55
Batch: 360; loss: 1.57; acc: 0.52
Batch: 380; loss: 1.67; acc: 0.48
Batch: 400; loss: 1.6; acc: 0.45
Batch: 420; loss: 1.77; acc: 0.41
Batch: 440; loss: 1.67; acc: 0.45
Batch: 460; loss: 1.63; acc: 0.47
Batch: 480; loss: 1.5; acc: 0.64
Batch: 500; loss: 1.7; acc: 0.5
Batch: 520; loss: 1.66; acc: 0.42
Batch: 540; loss: 1.66; acc: 0.44
Batch: 560; loss: 1.65; acc: 0.52
Batch: 580; loss: 1.68; acc: 0.47
Batch: 600; loss: 1.72; acc: 0.45
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.57; acc: 0.55
Batch: 660; loss: 1.65; acc: 0.52
Batch: 680; loss: 1.67; acc: 0.44
Batch: 700; loss: 1.55; acc: 0.5
Batch: 720; loss: 1.86; acc: 0.39
Batch: 740; loss: 1.61; acc: 0.5
Batch: 760; loss: 1.49; acc: 0.56
Batch: 780; loss: 1.51; acc: 0.5
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.049327253596857e-05
1.1923211786779575e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.75; acc: 0.45
Batch: 40; loss: 1.31; acc: 0.72
Batch: 60; loss: 1.46; acc: 0.58
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.47; acc: 0.62
Batch: 120; loss: 1.54; acc: 0.56
Batch: 140; loss: 1.36; acc: 0.69
Val Epoch over. val_loss: 1.5546792722811364; val_accuracy: 0.5344347133757962 

The current subspace-distance is: 1.1923211786779575e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.45; acc: 0.61
Batch: 20; loss: 1.67; acc: 0.52
Batch: 40; loss: 1.74; acc: 0.41
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.55; acc: 0.45
Batch: 100; loss: 1.56; acc: 0.52
Batch: 120; loss: 1.6; acc: 0.53
Batch: 140; loss: 1.59; acc: 0.52
Batch: 160; loss: 1.63; acc: 0.47
Batch: 180; loss: 1.59; acc: 0.56
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.53; acc: 0.48
Batch: 240; loss: 1.54; acc: 0.53
Batch: 260; loss: 1.54; acc: 0.52
Batch: 280; loss: 1.7; acc: 0.42
Batch: 300; loss: 1.6; acc: 0.58
Batch: 320; loss: 1.53; acc: 0.48
Batch: 340; loss: 1.71; acc: 0.47
Batch: 360; loss: 1.45; acc: 0.53
Batch: 380; loss: 1.54; acc: 0.52
Batch: 400; loss: 1.55; acc: 0.59
Batch: 420; loss: 1.51; acc: 0.62
Batch: 440; loss: 1.66; acc: 0.48
Batch: 460; loss: 1.8; acc: 0.45
Batch: 480; loss: 1.73; acc: 0.47
Batch: 500; loss: 1.54; acc: 0.52
Batch: 520; loss: 1.59; acc: 0.55
Batch: 540; loss: 1.71; acc: 0.48
Batch: 560; loss: 1.74; acc: 0.41
Batch: 580; loss: 1.46; acc: 0.59
Batch: 600; loss: 1.55; acc: 0.53
Batch: 620; loss: 1.5; acc: 0.53
Batch: 640; loss: 1.5; acc: 0.56
Batch: 660; loss: 1.42; acc: 0.56
Batch: 680; loss: 1.67; acc: 0.45
Batch: 700; loss: 1.55; acc: 0.52
Batch: 720; loss: 1.55; acc: 0.48
Batch: 740; loss: 1.63; acc: 0.42
Batch: 760; loss: 1.6; acc: 0.52
Batch: 780; loss: 1.6; acc: 0.42
Train Epoch over. train_loss: 1.59; train_accuracy: 0.51 

4.179575262241997e-05
1.4551732419931795e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.73; acc: 0.45
Batch: 40; loss: 1.3; acc: 0.7
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.46; acc: 0.61
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.36; acc: 0.69
Val Epoch over. val_loss: 1.5477372795153574; val_accuracy: 0.5378184713375797 

The current subspace-distance is: 1.4551732419931795e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.51; acc: 0.58
Batch: 20; loss: 1.63; acc: 0.47
Batch: 40; loss: 1.55; acc: 0.52
Batch: 60; loss: 1.61; acc: 0.5
Batch: 80; loss: 1.56; acc: 0.52
Batch: 100; loss: 1.62; acc: 0.42
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.59; acc: 0.45
Batch: 160; loss: 1.64; acc: 0.47
Batch: 180; loss: 1.54; acc: 0.5
Batch: 200; loss: 1.67; acc: 0.47
Batch: 220; loss: 1.69; acc: 0.42
Batch: 240; loss: 1.71; acc: 0.45
Batch: 260; loss: 1.51; acc: 0.55
Batch: 280; loss: 1.66; acc: 0.52
Batch: 300; loss: 1.69; acc: 0.41
Batch: 320; loss: 1.56; acc: 0.56
Batch: 340; loss: 1.6; acc: 0.5
Batch: 360; loss: 1.62; acc: 0.5
Batch: 380; loss: 1.54; acc: 0.48
Batch: 400; loss: 1.62; acc: 0.59
Batch: 420; loss: 1.45; acc: 0.56
Batch: 440; loss: 1.48; acc: 0.58
Batch: 460; loss: 1.62; acc: 0.48
Batch: 480; loss: 1.56; acc: 0.52
Batch: 500; loss: 1.51; acc: 0.48
Batch: 520; loss: 1.54; acc: 0.52
Batch: 540; loss: 1.71; acc: 0.47
Batch: 560; loss: 1.51; acc: 0.55
Batch: 580; loss: 1.69; acc: 0.45
Batch: 600; loss: 1.49; acc: 0.5
Batch: 620; loss: 1.53; acc: 0.53
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.46; acc: 0.52
Batch: 680; loss: 1.65; acc: 0.52
Batch: 700; loss: 1.72; acc: 0.44
Batch: 720; loss: 1.47; acc: 0.59
Batch: 740; loss: 1.71; acc: 0.45
Batch: 760; loss: 1.52; acc: 0.44
Batch: 780; loss: 1.62; acc: 0.48
Train Epoch over. train_loss: 1.59; train_accuracy: 0.51 

4.0671973692951724e-05
1.0568661309662275e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.31; acc: 0.67
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.57; acc: 0.58
Batch: 100; loss: 1.45; acc: 0.62
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.35; acc: 0.69
Val Epoch over. val_loss: 1.552102255973087; val_accuracy: 0.5318471337579618 

The current subspace-distance is: 1.0568661309662275e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.62; acc: 0.45
Batch: 60; loss: 1.77; acc: 0.39
Batch: 80; loss: 1.53; acc: 0.5
Batch: 100; loss: 1.65; acc: 0.47
Batch: 120; loss: 1.51; acc: 0.56
Batch: 140; loss: 1.56; acc: 0.47
Batch: 160; loss: 1.54; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.47
Batch: 200; loss: 1.75; acc: 0.39
Batch: 220; loss: 1.52; acc: 0.5
Batch: 240; loss: 1.46; acc: 0.62
Batch: 260; loss: 1.48; acc: 0.58
Batch: 280; loss: 1.67; acc: 0.44
Batch: 300; loss: 1.71; acc: 0.45
Batch: 320; loss: 1.52; acc: 0.53
Batch: 340; loss: 1.71; acc: 0.48
Batch: 360; loss: 1.77; acc: 0.42
Batch: 380; loss: 1.51; acc: 0.53
Batch: 400; loss: 1.56; acc: 0.55
Batch: 420; loss: 1.47; acc: 0.59
Batch: 440; loss: 1.49; acc: 0.5
Batch: 460; loss: 1.59; acc: 0.5
Batch: 480; loss: 1.63; acc: 0.45
Batch: 500; loss: 1.63; acc: 0.39
Batch: 520; loss: 1.66; acc: 0.55
Batch: 540; loss: 1.61; acc: 0.58
Batch: 560; loss: 1.4; acc: 0.66
Batch: 580; loss: 1.59; acc: 0.5
Batch: 600; loss: 1.6; acc: 0.42
Batch: 620; loss: 1.55; acc: 0.52
Batch: 640; loss: 1.67; acc: 0.47
Batch: 660; loss: 1.62; acc: 0.52
Batch: 680; loss: 1.43; acc: 0.58
Batch: 700; loss: 1.53; acc: 0.53
Batch: 720; loss: 1.54; acc: 0.53
Batch: 740; loss: 1.69; acc: 0.44
Batch: 760; loss: 1.63; acc: 0.48
Batch: 780; loss: 1.51; acc: 0.53
Train Epoch over. train_loss: 1.58; train_accuracy: 0.51 

4.394152347231284e-05
1.7961880075745285e-05
Batch: 0; loss: 1.67; acc: 0.45
Batch: 20; loss: 1.73; acc: 0.47
Batch: 40; loss: 1.3; acc: 0.66
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.45; acc: 0.61
Batch: 120; loss: 1.52; acc: 0.53
Batch: 140; loss: 1.35; acc: 0.72
Val Epoch over. val_loss: 1.5427026239929684; val_accuracy: 0.5402070063694268 

The current subspace-distance is: 1.7961880075745285e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.52
Batch: 20; loss: 1.59; acc: 0.48
Batch: 40; loss: 1.52; acc: 0.58
Batch: 60; loss: 1.64; acc: 0.42
Batch: 80; loss: 1.48; acc: 0.61
Batch: 100; loss: 1.72; acc: 0.34
Batch: 120; loss: 1.6; acc: 0.44
Batch: 140; loss: 1.48; acc: 0.56
Batch: 160; loss: 1.52; acc: 0.55
Batch: 180; loss: 1.56; acc: 0.56
Batch: 200; loss: 1.49; acc: 0.56
Batch: 220; loss: 1.48; acc: 0.59
Batch: 240; loss: 1.6; acc: 0.5
Batch: 260; loss: 1.53; acc: 0.62
Batch: 280; loss: 1.5; acc: 0.58
Batch: 300; loss: 1.49; acc: 0.58
Batch: 320; loss: 1.54; acc: 0.53
Batch: 340; loss: 1.53; acc: 0.53
Batch: 360; loss: 1.65; acc: 0.45
Batch: 380; loss: 1.41; acc: 0.64
Batch: 400; loss: 1.44; acc: 0.58
Batch: 420; loss: 1.51; acc: 0.52
Batch: 440; loss: 1.72; acc: 0.39
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.7; acc: 0.44
Batch: 500; loss: 1.65; acc: 0.45
Batch: 520; loss: 1.68; acc: 0.44
Batch: 540; loss: 1.54; acc: 0.53
Batch: 560; loss: 1.54; acc: 0.48
Batch: 580; loss: 1.41; acc: 0.64
Batch: 600; loss: 1.57; acc: 0.58
Batch: 620; loss: 1.63; acc: 0.53
Batch: 640; loss: 1.63; acc: 0.48
Batch: 660; loss: 1.45; acc: 0.61
Batch: 680; loss: 1.55; acc: 0.42
Batch: 700; loss: 1.51; acc: 0.55
Batch: 720; loss: 1.48; acc: 0.61
Batch: 740; loss: 1.51; acc: 0.52
Batch: 760; loss: 1.69; acc: 0.52
Batch: 780; loss: 1.45; acc: 0.59
Train Epoch over. train_loss: 1.58; train_accuracy: 0.51 

4.2170402593910694e-05
1.216281179949874e-05
Batch: 0; loss: 1.65; acc: 0.45
Batch: 20; loss: 1.72; acc: 0.52
Batch: 40; loss: 1.29; acc: 0.66
Batch: 60; loss: 1.45; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.56
Batch: 100; loss: 1.44; acc: 0.62
Batch: 120; loss: 1.51; acc: 0.53
Batch: 140; loss: 1.34; acc: 0.69
Val Epoch over. val_loss: 1.5305147991058932; val_accuracy: 0.544984076433121 

The current subspace-distance is: 1.216281179949874e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.53
Batch: 20; loss: 1.54; acc: 0.55
Batch: 40; loss: 1.64; acc: 0.52
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.57; acc: 0.56
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.7; acc: 0.44
Batch: 160; loss: 1.52; acc: 0.62
Batch: 180; loss: 1.54; acc: 0.52
Batch: 200; loss: 1.57; acc: 0.52
Batch: 220; loss: 1.31; acc: 0.72
Batch: 240; loss: 1.6; acc: 0.48
Batch: 260; loss: 1.64; acc: 0.5
Batch: 280; loss: 1.59; acc: 0.45
Batch: 300; loss: 1.56; acc: 0.53
Batch: 320; loss: 1.47; acc: 0.59
Batch: 340; loss: 1.54; acc: 0.52
Batch: 360; loss: 1.61; acc: 0.5
Batch: 380; loss: 1.54; acc: 0.48
Batch: 400; loss: 1.79; acc: 0.42
Batch: 420; loss: 1.54; acc: 0.56
Batch: 440; loss: 1.47; acc: 0.64
Batch: 460; loss: 1.55; acc: 0.52
Batch: 480; loss: 1.62; acc: 0.5
Batch: 500; loss: 1.71; acc: 0.34
Batch: 520; loss: 1.68; acc: 0.45
Batch: 540; loss: 1.49; acc: 0.56
Batch: 560; loss: 1.47; acc: 0.55
Batch: 580; loss: 1.44; acc: 0.5
Batch: 600; loss: 1.62; acc: 0.42
Batch: 620; loss: 1.46; acc: 0.59
Batch: 640; loss: 1.51; acc: 0.58
Batch: 660; loss: 1.48; acc: 0.58
Batch: 680; loss: 1.51; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.48
Batch: 720; loss: 1.63; acc: 0.5
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.48; acc: 0.58
Batch: 780; loss: 1.54; acc: 0.48
Train Epoch over. train_loss: 1.58; train_accuracy: 0.51 

4.231325874570757e-05
1.1901175639650319e-05
Batch: 0; loss: 1.64; acc: 0.44
Batch: 20; loss: 1.72; acc: 0.48
Batch: 40; loss: 1.29; acc: 0.66
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.52; acc: 0.52
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.5298258628055548; val_accuracy: 0.5447850318471338 

The current subspace-distance is: 1.1901175639650319e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.48; acc: 0.62
Batch: 20; loss: 1.78; acc: 0.38
Batch: 40; loss: 1.58; acc: 0.5
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.54; acc: 0.53
Batch: 100; loss: 1.72; acc: 0.42
Batch: 120; loss: 1.51; acc: 0.5
Batch: 140; loss: 1.45; acc: 0.59
Batch: 160; loss: 1.65; acc: 0.38
Batch: 180; loss: 1.34; acc: 0.61
Batch: 200; loss: 1.67; acc: 0.52
Batch: 220; loss: 1.61; acc: 0.42
Batch: 240; loss: 1.59; acc: 0.53
Batch: 260; loss: 1.7; acc: 0.52
Batch: 280; loss: 1.5; acc: 0.53
Batch: 300; loss: 1.4; acc: 0.58
Batch: 320; loss: 1.56; acc: 0.52
Batch: 340; loss: 1.64; acc: 0.53
Batch: 360; loss: 1.57; acc: 0.5
Batch: 380; loss: 1.46; acc: 0.58
Batch: 400; loss: 1.52; acc: 0.61
Batch: 420; loss: 1.6; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.66
Batch: 460; loss: 1.63; acc: 0.45
Batch: 480; loss: 1.53; acc: 0.52
Batch: 500; loss: 1.6; acc: 0.52
Batch: 520; loss: 1.61; acc: 0.45
Batch: 540; loss: 1.58; acc: 0.44
Batch: 560; loss: 1.51; acc: 0.5
Batch: 580; loss: 1.68; acc: 0.42
Batch: 600; loss: 1.45; acc: 0.5
Batch: 620; loss: 1.66; acc: 0.45
Batch: 640; loss: 1.6; acc: 0.5
Batch: 660; loss: 1.57; acc: 0.56
Batch: 680; loss: 1.63; acc: 0.45
Batch: 700; loss: 1.62; acc: 0.47
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.79; acc: 0.38
Batch: 760; loss: 1.62; acc: 0.56
Batch: 780; loss: 1.46; acc: 0.55
Train Epoch over. train_loss: 1.57; train_accuracy: 0.51 

4.266232281224802e-05
1.156934831669787e-05
Batch: 0; loss: 1.64; acc: 0.45
Batch: 20; loss: 1.71; acc: 0.5
Batch: 40; loss: 1.29; acc: 0.66
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.51; acc: 0.52
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.5294841095140785; val_accuracy: 0.5394108280254777 

The current subspace-distance is: 1.156934831669787e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.47
Batch: 20; loss: 1.41; acc: 0.58
Batch: 40; loss: 1.53; acc: 0.55
Batch: 60; loss: 1.46; acc: 0.64
Batch: 80; loss: 1.46; acc: 0.55
Batch: 100; loss: 1.55; acc: 0.56
Batch: 120; loss: 1.56; acc: 0.45
Batch: 140; loss: 1.63; acc: 0.47
Batch: 160; loss: 1.64; acc: 0.52
Batch: 180; loss: 1.52; acc: 0.52
Batch: 200; loss: 1.51; acc: 0.58
Batch: 220; loss: 1.51; acc: 0.52
Batch: 240; loss: 1.62; acc: 0.48
Batch: 260; loss: 1.66; acc: 0.52
Batch: 280; loss: 1.53; acc: 0.56
Batch: 300; loss: 1.52; acc: 0.5
Batch: 320; loss: 1.52; acc: 0.48
Batch: 340; loss: 1.48; acc: 0.55
Batch: 360; loss: 1.48; acc: 0.59
Batch: 380; loss: 1.61; acc: 0.5
Batch: 400; loss: 1.59; acc: 0.52
Batch: 420; loss: 1.82; acc: 0.41
Batch: 440; loss: 1.53; acc: 0.55
Batch: 460; loss: 1.68; acc: 0.44
Batch: 480; loss: 1.58; acc: 0.44
Batch: 500; loss: 1.48; acc: 0.56
Batch: 520; loss: 1.61; acc: 0.47
Batch: 540; loss: 1.54; acc: 0.58
Batch: 560; loss: 1.6; acc: 0.52
Batch: 580; loss: 1.56; acc: 0.53
Batch: 600; loss: 1.53; acc: 0.53
Batch: 620; loss: 1.62; acc: 0.5
Batch: 640; loss: 1.33; acc: 0.64
Batch: 660; loss: 1.49; acc: 0.62
Batch: 680; loss: 1.41; acc: 0.59
Batch: 700; loss: 1.81; acc: 0.39
Batch: 720; loss: 1.47; acc: 0.64
Batch: 740; loss: 1.54; acc: 0.53
Batch: 760; loss: 1.7; acc: 0.41
Batch: 780; loss: 1.41; acc: 0.53
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

4.320412335800938e-05
1.2028934179397766e-05
Batch: 0; loss: 1.65; acc: 0.45
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.29; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.51; acc: 0.52
Batch: 140; loss: 1.32; acc: 0.69
Val Epoch over. val_loss: 1.5321476140599342; val_accuracy: 0.5363256369426752 

The current subspace-distance is: 1.2028934179397766e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.4; acc: 0.58
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.48; acc: 0.62
Batch: 100; loss: 1.52; acc: 0.55
Batch: 120; loss: 1.45; acc: 0.61
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.46; acc: 0.56
Batch: 180; loss: 1.57; acc: 0.53
Batch: 200; loss: 1.49; acc: 0.56
Batch: 220; loss: 1.82; acc: 0.47
Batch: 240; loss: 1.58; acc: 0.53
Batch: 260; loss: 1.66; acc: 0.45
Batch: 280; loss: 1.67; acc: 0.53
Batch: 300; loss: 1.52; acc: 0.53
Batch: 320; loss: 1.5; acc: 0.62
Batch: 340; loss: 1.81; acc: 0.44
Batch: 360; loss: 1.31; acc: 0.62
Batch: 380; loss: 1.56; acc: 0.53
Batch: 400; loss: 1.59; acc: 0.47
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.54; acc: 0.58
Batch: 460; loss: 1.53; acc: 0.58
Batch: 480; loss: 1.56; acc: 0.5
Batch: 500; loss: 1.48; acc: 0.61
Batch: 520; loss: 1.5; acc: 0.55
Batch: 540; loss: 1.59; acc: 0.52
Batch: 560; loss: 1.62; acc: 0.45
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.59; acc: 0.41
Batch: 620; loss: 1.61; acc: 0.53
Batch: 640; loss: 1.54; acc: 0.5
Batch: 660; loss: 1.44; acc: 0.56
Batch: 680; loss: 1.56; acc: 0.56
Batch: 700; loss: 1.76; acc: 0.39
Batch: 720; loss: 1.59; acc: 0.48
Batch: 740; loss: 1.52; acc: 0.48
Batch: 760; loss: 1.61; acc: 0.48
Batch: 780; loss: 1.45; acc: 0.59
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.362373147159815e-05
1.4974023542890791e-05
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.28; acc: 0.66
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.42; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.5
Batch: 140; loss: 1.31; acc: 0.69
Val Epoch over. val_loss: 1.5220766819206772; val_accuracy: 0.5431926751592356 

The current subspace-distance is: 1.4974023542890791e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.44
Batch: 20; loss: 1.55; acc: 0.58
Batch: 40; loss: 1.47; acc: 0.59
Batch: 60; loss: 1.73; acc: 0.39
Batch: 80; loss: 1.58; acc: 0.55
Batch: 100; loss: 1.53; acc: 0.52
Batch: 120; loss: 1.55; acc: 0.47
Batch: 140; loss: 1.77; acc: 0.44
Batch: 160; loss: 1.67; acc: 0.42
Batch: 180; loss: 1.62; acc: 0.44
Batch: 200; loss: 1.37; acc: 0.66
Batch: 220; loss: 1.55; acc: 0.52
Batch: 240; loss: 1.42; acc: 0.62
Batch: 260; loss: 1.45; acc: 0.55
Batch: 280; loss: 1.54; acc: 0.5
Batch: 300; loss: 1.54; acc: 0.61
Batch: 320; loss: 1.5; acc: 0.55
Batch: 340; loss: 1.59; acc: 0.48
Batch: 360; loss: 1.59; acc: 0.48
Batch: 380; loss: 1.75; acc: 0.48
Batch: 400; loss: 1.5; acc: 0.5
Batch: 420; loss: 1.49; acc: 0.53
Batch: 440; loss: 1.42; acc: 0.69
Batch: 460; loss: 1.7; acc: 0.45
Batch: 480; loss: 1.66; acc: 0.53
Batch: 500; loss: 1.66; acc: 0.41
Batch: 520; loss: 1.49; acc: 0.58
Batch: 540; loss: 1.72; acc: 0.41
Batch: 560; loss: 1.56; acc: 0.53
Batch: 580; loss: 1.72; acc: 0.45
Batch: 600; loss: 1.59; acc: 0.52
Batch: 620; loss: 1.71; acc: 0.45
Batch: 640; loss: 1.45; acc: 0.58
Batch: 660; loss: 1.59; acc: 0.5
Batch: 680; loss: 1.69; acc: 0.45
Batch: 700; loss: 1.56; acc: 0.52
Batch: 720; loss: 1.59; acc: 0.45
Batch: 740; loss: 1.56; acc: 0.53
Batch: 760; loss: 1.52; acc: 0.56
Batch: 780; loss: 1.35; acc: 0.64
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.3558724428294227e-05
1.433546367479721e-05
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.42; acc: 0.61
Batch: 120; loss: 1.49; acc: 0.5
Batch: 140; loss: 1.31; acc: 0.69
Val Epoch over. val_loss: 1.5224891756750216; val_accuracy: 0.5450835987261147 

The current subspace-distance is: 1.433546367479721e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.45; acc: 0.62
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.51; acc: 0.53
Batch: 80; loss: 1.62; acc: 0.42
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 1.47; acc: 0.56
Batch: 160; loss: 1.51; acc: 0.55
Batch: 180; loss: 1.58; acc: 0.45
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.55; acc: 0.48
Batch: 240; loss: 1.64; acc: 0.44
Batch: 260; loss: 1.63; acc: 0.52
Batch: 280; loss: 1.52; acc: 0.41
Batch: 300; loss: 1.73; acc: 0.47
Batch: 320; loss: 1.45; acc: 0.56
Batch: 340; loss: 1.59; acc: 0.47
Batch: 360; loss: 1.53; acc: 0.5
Batch: 380; loss: 1.66; acc: 0.42
Batch: 400; loss: 1.76; acc: 0.38
Batch: 420; loss: 1.5; acc: 0.56
Batch: 440; loss: 1.56; acc: 0.47
Batch: 460; loss: 1.49; acc: 0.53
Batch: 480; loss: 1.57; acc: 0.52
Batch: 500; loss: 1.45; acc: 0.61
Batch: 520; loss: 1.67; acc: 0.47
Batch: 540; loss: 1.55; acc: 0.56
Batch: 560; loss: 1.51; acc: 0.52
Batch: 580; loss: 1.58; acc: 0.47
Batch: 600; loss: 1.51; acc: 0.59
Batch: 620; loss: 1.42; acc: 0.59
Batch: 640; loss: 1.44; acc: 0.58
Batch: 660; loss: 1.68; acc: 0.45
Batch: 680; loss: 1.72; acc: 0.48
Batch: 700; loss: 1.7; acc: 0.45
Batch: 720; loss: 1.56; acc: 0.53
Batch: 740; loss: 1.45; acc: 0.66
Batch: 760; loss: 1.47; acc: 0.47
Batch: 780; loss: 1.59; acc: 0.53
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.388445086078718e-05
1.6305973986163735e-05
Batch: 0; loss: 1.64; acc: 0.45
Batch: 20; loss: 1.71; acc: 0.45
Batch: 40; loss: 1.29; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.54; acc: 0.53
Batch: 100; loss: 1.43; acc: 0.59
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.33; acc: 0.69
Val Epoch over. val_loss: 1.5259037830267743; val_accuracy: 0.5391122611464968 

The current subspace-distance is: 1.6305973986163735e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.61; acc: 0.52
Batch: 60; loss: 1.46; acc: 0.62
Batch: 80; loss: 1.68; acc: 0.38
Batch: 100; loss: 1.51; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.45
Batch: 140; loss: 1.41; acc: 0.58
Batch: 160; loss: 1.43; acc: 0.64
Batch: 180; loss: 1.52; acc: 0.52
Batch: 200; loss: 1.55; acc: 0.44
Batch: 220; loss: 1.62; acc: 0.5
Batch: 240; loss: 1.53; acc: 0.5
Batch: 260; loss: 1.44; acc: 0.59
Batch: 280; loss: 1.66; acc: 0.44
Batch: 300; loss: 1.48; acc: 0.58
Batch: 320; loss: 1.52; acc: 0.55
Batch: 340; loss: 1.65; acc: 0.52
Batch: 360; loss: 1.46; acc: 0.56
Batch: 380; loss: 1.48; acc: 0.53
Batch: 400; loss: 1.5; acc: 0.58
Batch: 420; loss: 1.55; acc: 0.53
Batch: 440; loss: 1.51; acc: 0.52
Batch: 460; loss: 1.44; acc: 0.58
Batch: 480; loss: 1.59; acc: 0.52
Batch: 500; loss: 1.6; acc: 0.48
Batch: 520; loss: 1.42; acc: 0.59
Batch: 540; loss: 1.6; acc: 0.47
Batch: 560; loss: 1.54; acc: 0.55
Batch: 580; loss: 1.47; acc: 0.59
Batch: 600; loss: 1.69; acc: 0.52
Batch: 620; loss: 1.64; acc: 0.5
Batch: 640; loss: 1.68; acc: 0.42
Batch: 660; loss: 1.63; acc: 0.44
Batch: 680; loss: 1.57; acc: 0.55
Batch: 700; loss: 1.66; acc: 0.47
Batch: 720; loss: 1.52; acc: 0.61
Batch: 740; loss: 1.49; acc: 0.5
Batch: 760; loss: 1.49; acc: 0.53
Batch: 780; loss: 1.45; acc: 0.66
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.4069580326322466e-05
1.1891386748175137e-05
Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.7; acc: 0.47
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.56; acc: 0.53
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.3; acc: 0.7
Val Epoch over. val_loss: 1.5188230093877026; val_accuracy: 0.5431926751592356 

The current subspace-distance is: 1.1891386748175137e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.49; acc: 0.61
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.45; acc: 0.58
Batch: 100; loss: 1.53; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.38
Batch: 140; loss: 1.45; acc: 0.53
Batch: 160; loss: 1.6; acc: 0.52
Batch: 180; loss: 1.55; acc: 0.61
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 1.51; acc: 0.52
Batch: 240; loss: 1.62; acc: 0.44
Batch: 260; loss: 1.58; acc: 0.53
Batch: 280; loss: 1.59; acc: 0.45
Batch: 300; loss: 1.7; acc: 0.47
Batch: 320; loss: 1.61; acc: 0.5
Batch: 340; loss: 1.53; acc: 0.59
Batch: 360; loss: 1.57; acc: 0.55
Batch: 380; loss: 1.53; acc: 0.48
Batch: 400; loss: 1.5; acc: 0.55
Batch: 420; loss: 1.57; acc: 0.53
Batch: 440; loss: 1.51; acc: 0.59
Batch: 460; loss: 1.63; acc: 0.52
Batch: 480; loss: 1.66; acc: 0.45
Batch: 500; loss: 1.58; acc: 0.5
Batch: 520; loss: 1.55; acc: 0.5
Batch: 540; loss: 1.55; acc: 0.52
Batch: 560; loss: 1.6; acc: 0.47
Batch: 580; loss: 1.59; acc: 0.56
Batch: 600; loss: 1.51; acc: 0.5
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.74; acc: 0.42
Batch: 660; loss: 1.35; acc: 0.55
Batch: 680; loss: 1.5; acc: 0.52
Batch: 700; loss: 1.56; acc: 0.48
Batch: 720; loss: 1.46; acc: 0.58
Batch: 740; loss: 1.66; acc: 0.48
Batch: 760; loss: 1.72; acc: 0.41
Batch: 780; loss: 1.56; acc: 0.53
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.392184564494528e-05
1.4179895515553653e-05
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.27; acc: 0.66
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.54; acc: 0.55
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.48; acc: 0.5
Batch: 140; loss: 1.3; acc: 0.67
Val Epoch over. val_loss: 1.5141252377989944; val_accuracy: 0.5425955414012739 

The current subspace-distance is: 1.4179895515553653e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.52
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.4; acc: 0.66
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.64; acc: 0.47
Batch: 100; loss: 1.34; acc: 0.62
Batch: 120; loss: 1.48; acc: 0.53
Batch: 140; loss: 1.59; acc: 0.47
Batch: 160; loss: 1.66; acc: 0.48
Batch: 180; loss: 1.5; acc: 0.53
Batch: 200; loss: 1.56; acc: 0.47
Batch: 220; loss: 1.51; acc: 0.55
Batch: 240; loss: 1.45; acc: 0.5
Batch: 260; loss: 1.57; acc: 0.48
Batch: 280; loss: 1.57; acc: 0.48
Batch: 300; loss: 1.56; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.53
Batch: 340; loss: 1.52; acc: 0.56
Batch: 360; loss: 1.71; acc: 0.36
Batch: 380; loss: 1.66; acc: 0.48
Batch: 400; loss: 1.62; acc: 0.45
Batch: 420; loss: 1.55; acc: 0.52
Batch: 440; loss: 1.55; acc: 0.55
Batch: 460; loss: 1.73; acc: 0.45
Batch: 480; loss: 1.39; acc: 0.61
Batch: 500; loss: 1.52; acc: 0.53
Batch: 520; loss: 1.51; acc: 0.55
Batch: 540; loss: 1.54; acc: 0.59
Batch: 560; loss: 1.48; acc: 0.62
Batch: 580; loss: 1.53; acc: 0.58
Batch: 600; loss: 1.63; acc: 0.53
Batch: 620; loss: 1.78; acc: 0.36
Batch: 640; loss: 1.62; acc: 0.52
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.51; acc: 0.48
Batch: 700; loss: 1.55; acc: 0.58
Batch: 720; loss: 1.51; acc: 0.47
Batch: 740; loss: 1.56; acc: 0.53
Batch: 760; loss: 1.5; acc: 0.5
Batch: 780; loss: 1.58; acc: 0.53
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.4001033529639244e-05
1.2927974239573814e-05
Batch: 0; loss: 1.62; acc: 0.47
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.27; acc: 0.66
Batch: 60; loss: 1.43; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.56
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.48; acc: 0.5
Batch: 140; loss: 1.3; acc: 0.7
Val Epoch over. val_loss: 1.5126663476798186; val_accuracy: 0.5429936305732485 

The current subspace-distance is: 1.2927974239573814e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.52
Batch: 20; loss: 1.74; acc: 0.45
Batch: 40; loss: 1.52; acc: 0.52
Batch: 60; loss: 1.56; acc: 0.47
Batch: 80; loss: 1.64; acc: 0.48
Batch: 100; loss: 1.62; acc: 0.44
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.67; acc: 0.47
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.68; acc: 0.47
Batch: 200; loss: 1.53; acc: 0.53
Batch: 220; loss: 1.69; acc: 0.42
Batch: 240; loss: 1.62; acc: 0.5
Batch: 260; loss: 1.62; acc: 0.55
Batch: 280; loss: 1.48; acc: 0.55
Batch: 300; loss: 1.44; acc: 0.61
Batch: 320; loss: 1.49; acc: 0.59
Batch: 340; loss: 1.45; acc: 0.59
Batch: 360; loss: 1.57; acc: 0.45
Batch: 380; loss: 1.56; acc: 0.47
Batch: 400; loss: 1.51; acc: 0.55
Batch: 420; loss: 1.59; acc: 0.52
Batch: 440; loss: 1.59; acc: 0.52
Batch: 460; loss: 1.54; acc: 0.56
Batch: 480; loss: 1.64; acc: 0.5
Batch: 500; loss: 1.5; acc: 0.58
Batch: 520; loss: 1.59; acc: 0.44
Batch: 540; loss: 1.46; acc: 0.59
Batch: 560; loss: 1.6; acc: 0.45
Batch: 580; loss: 1.57; acc: 0.53
Batch: 600; loss: 1.72; acc: 0.48
Batch: 620; loss: 1.51; acc: 0.48
Batch: 640; loss: 1.6; acc: 0.5
Batch: 660; loss: 1.63; acc: 0.47
Batch: 680; loss: 1.43; acc: 0.61
Batch: 700; loss: 1.74; acc: 0.41
Batch: 720; loss: 1.56; acc: 0.52
Batch: 740; loss: 1.54; acc: 0.55
Batch: 760; loss: 1.61; acc: 0.47
Batch: 780; loss: 1.64; acc: 0.44
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.423745485837571e-05
1.5240128050209023e-05
Batch: 0; loss: 1.63; acc: 0.47
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.29; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.42; acc: 0.61
Batch: 120; loss: 1.49; acc: 0.5
Batch: 140; loss: 1.31; acc: 0.69
Val Epoch over. val_loss: 1.5243872145938266; val_accuracy: 0.5401074840764332 

The current subspace-distance is: 1.5240128050209023e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.51; acc: 0.58
Batch: 20; loss: 1.58; acc: 0.5
Batch: 40; loss: 1.78; acc: 0.39
Batch: 60; loss: 1.55; acc: 0.5
Batch: 80; loss: 1.48; acc: 0.55
Batch: 100; loss: 1.55; acc: 0.47
Batch: 120; loss: 1.67; acc: 0.45
Batch: 140; loss: 1.68; acc: 0.48
Batch: 160; loss: 1.54; acc: 0.44
Batch: 180; loss: 1.41; acc: 0.61
Batch: 200; loss: 1.58; acc: 0.52
Batch: 220; loss: 1.5; acc: 0.55
Batch: 240; loss: 1.61; acc: 0.47
Batch: 260; loss: 1.59; acc: 0.5
Batch: 280; loss: 1.6; acc: 0.47
Batch: 300; loss: 1.7; acc: 0.45
Batch: 320; loss: 1.73; acc: 0.45
Batch: 340; loss: 1.53; acc: 0.55
Batch: 360; loss: 1.55; acc: 0.53
Batch: 380; loss: 1.59; acc: 0.45
Batch: 400; loss: 1.42; acc: 0.59
Batch: 420; loss: 1.55; acc: 0.5
Batch: 440; loss: 1.61; acc: 0.42
Batch: 460; loss: 1.58; acc: 0.48
Batch: 480; loss: 1.57; acc: 0.5
Batch: 500; loss: 1.44; acc: 0.56
Batch: 520; loss: 1.68; acc: 0.44
Batch: 540; loss: 1.48; acc: 0.56
Batch: 560; loss: 1.5; acc: 0.5
Batch: 580; loss: 1.42; acc: 0.59
Batch: 600; loss: 1.6; acc: 0.52
Batch: 620; loss: 1.48; acc: 0.47
Batch: 640; loss: 1.48; acc: 0.55
Batch: 660; loss: 1.51; acc: 0.5
Batch: 680; loss: 1.6; acc: 0.5
Batch: 700; loss: 1.56; acc: 0.53
Batch: 720; loss: 1.6; acc: 0.45
Batch: 740; loss: 1.52; acc: 0.56
Batch: 760; loss: 1.56; acc: 0.48
Batch: 780; loss: 1.6; acc: 0.58
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.4793127017328516e-05
1.405548391630873e-05
Batch: 0; loss: 1.62; acc: 0.5
Batch: 20; loss: 1.69; acc: 0.5
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.48; acc: 0.52
Batch: 140; loss: 1.3; acc: 0.7
Val Epoch over. val_loss: 1.5157093212103387; val_accuracy: 0.5450835987261147 

The current subspace-distance is: 1.405548391630873e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.54; acc: 0.56
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 1.62; acc: 0.45
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.62; acc: 0.5
Batch: 160; loss: 1.68; acc: 0.53
Batch: 180; loss: 1.52; acc: 0.58
Batch: 200; loss: 1.47; acc: 0.56
Batch: 220; loss: 1.55; acc: 0.59
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.6; acc: 0.45
Batch: 280; loss: 1.52; acc: 0.52
Batch: 300; loss: 1.54; acc: 0.5
Batch: 320; loss: 1.58; acc: 0.42
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.6; acc: 0.47
Batch: 380; loss: 1.5; acc: 0.64
Batch: 400; loss: 1.5; acc: 0.58
Batch: 420; loss: 1.52; acc: 0.59
Batch: 440; loss: 1.57; acc: 0.45
Batch: 460; loss: 1.31; acc: 0.67
Batch: 480; loss: 1.46; acc: 0.62
Batch: 500; loss: 1.48; acc: 0.56
Batch: 520; loss: 1.46; acc: 0.59
Batch: 540; loss: 1.6; acc: 0.47
Batch: 560; loss: 1.63; acc: 0.42
Batch: 580; loss: 1.56; acc: 0.52
Batch: 600; loss: 1.48; acc: 0.58
Batch: 620; loss: 1.58; acc: 0.53
Batch: 640; loss: 1.55; acc: 0.48
Batch: 660; loss: 1.65; acc: 0.42
Batch: 680; loss: 1.45; acc: 0.55
Batch: 700; loss: 1.51; acc: 0.52
Batch: 720; loss: 1.56; acc: 0.53
Batch: 740; loss: 1.68; acc: 0.44
Batch: 760; loss: 1.58; acc: 0.53
Batch: 780; loss: 1.66; acc: 0.48
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.3908890802413225e-05
1.3001351362618152e-05
Batch: 0; loss: 1.62; acc: 0.5
Batch: 20; loss: 1.69; acc: 0.48
Batch: 40; loss: 1.27; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.4; acc: 0.61
Batch: 120; loss: 1.49; acc: 0.53
Batch: 140; loss: 1.28; acc: 0.7
Val Epoch over. val_loss: 1.5098213276286034; val_accuracy: 0.5429936305732485 

The current subspace-distance is: 1.3001351362618152e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.52
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.51; acc: 0.48
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.52; acc: 0.53
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.37; acc: 0.62
Batch: 140; loss: 1.55; acc: 0.5
Batch: 160; loss: 1.62; acc: 0.47
Batch: 180; loss: 1.54; acc: 0.55
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.56; acc: 0.55
Batch: 240; loss: 1.42; acc: 0.62
Batch: 260; loss: 1.62; acc: 0.5
Batch: 280; loss: 1.48; acc: 0.53
Batch: 300; loss: 1.61; acc: 0.48
Batch: 320; loss: 1.45; acc: 0.52
Batch: 340; loss: 1.38; acc: 0.64
Batch: 360; loss: 1.57; acc: 0.52
Batch: 380; loss: 1.55; acc: 0.56
Batch: 400; loss: 1.5; acc: 0.47
Batch: 420; loss: 1.52; acc: 0.52
Batch: 440; loss: 1.53; acc: 0.55
Batch: 460; loss: 1.52; acc: 0.53
Batch: 480; loss: 1.68; acc: 0.47
Batch: 500; loss: 1.55; acc: 0.58
Batch: 520; loss: 1.56; acc: 0.52
Batch: 540; loss: 1.51; acc: 0.47
Batch: 560; loss: 1.67; acc: 0.42
Batch: 580; loss: 1.5; acc: 0.59
Batch: 600; loss: 1.54; acc: 0.55
Batch: 620; loss: 1.55; acc: 0.52
Batch: 640; loss: 1.52; acc: 0.58
Batch: 660; loss: 1.58; acc: 0.42
Batch: 680; loss: 1.6; acc: 0.48
Batch: 700; loss: 1.5; acc: 0.56
Batch: 720; loss: 1.58; acc: 0.56
Batch: 740; loss: 1.61; acc: 0.45
Batch: 760; loss: 1.58; acc: 0.52
Batch: 780; loss: 1.62; acc: 0.48
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.4363154302118346e-05
1.3873777788830921e-05
Batch: 0; loss: 1.63; acc: 0.47
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.42; acc: 0.61
Batch: 120; loss: 1.48; acc: 0.52
Batch: 140; loss: 1.3; acc: 0.69
Val Epoch over. val_loss: 1.522633671760559; val_accuracy: 0.5366242038216561 

The current subspace-distance is: 1.3873777788830921e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.55
Batch: 20; loss: 1.59; acc: 0.44
Batch: 40; loss: 1.67; acc: 0.44
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.53; acc: 0.5
Batch: 100; loss: 1.75; acc: 0.42
Batch: 120; loss: 1.53; acc: 0.45
Batch: 140; loss: 1.75; acc: 0.42
Batch: 160; loss: 1.54; acc: 0.48
Batch: 180; loss: 1.55; acc: 0.56
Batch: 200; loss: 1.51; acc: 0.56
Batch: 220; loss: 1.54; acc: 0.47
Batch: 240; loss: 1.58; acc: 0.48
Batch: 260; loss: 1.47; acc: 0.53
Batch: 280; loss: 1.49; acc: 0.52
Batch: 300; loss: 1.58; acc: 0.45
Batch: 320; loss: 1.52; acc: 0.56
Batch: 340; loss: 1.67; acc: 0.5
Batch: 360; loss: 1.59; acc: 0.53
Batch: 380; loss: 1.64; acc: 0.45
Batch: 400; loss: 1.59; acc: 0.56
Batch: 420; loss: 1.56; acc: 0.47
Batch: 440; loss: 1.53; acc: 0.48
Batch: 460; loss: 1.52; acc: 0.48
Batch: 480; loss: 1.54; acc: 0.48
Batch: 500; loss: 1.51; acc: 0.55
Batch: 520; loss: 1.5; acc: 0.53
Batch: 540; loss: 1.6; acc: 0.53
Batch: 560; loss: 1.58; acc: 0.45
Batch: 580; loss: 1.48; acc: 0.52
Batch: 600; loss: 1.66; acc: 0.45
Batch: 620; loss: 1.67; acc: 0.44
Batch: 640; loss: 1.56; acc: 0.5
Batch: 660; loss: 1.69; acc: 0.48
Batch: 680; loss: 1.41; acc: 0.58
Batch: 700; loss: 1.6; acc: 0.42
Batch: 720; loss: 1.6; acc: 0.52
Batch: 740; loss: 1.7; acc: 0.47
Batch: 760; loss: 1.43; acc: 0.66
Batch: 780; loss: 1.42; acc: 0.55
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.5578006393043324e-05
1.7316086086793803e-05
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.27; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.4; acc: 0.61
Batch: 120; loss: 1.48; acc: 0.52
Batch: 140; loss: 1.28; acc: 0.72
Val Epoch over. val_loss: 1.5099938196741092; val_accuracy: 0.5455812101910829 

The current subspace-distance is: 1.7316086086793803e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_15_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.5620723414746194

The number of parameters is: 263271

The number of individual parameters is:

13
234
13
13
19
37791
19
19
38
110466
38
38
64
109440
64
64
4096
64
640
10
64
64

nonzero elements in E: 26327097
elements in E: 26327100
fraction nonzero: 0.9999998860489762
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.47; acc: 0.09
Batch: 20; loss: 2.35; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.16
Batch: 60; loss: 2.2; acc: 0.11
Batch: 80; loss: 2.13; acc: 0.17
Batch: 100; loss: 1.99; acc: 0.33
Batch: 120; loss: 2.02; acc: 0.28
Batch: 140; loss: 2.04; acc: 0.27
Batch: 160; loss: 1.97; acc: 0.36
Batch: 180; loss: 1.95; acc: 0.38
Batch: 200; loss: 2.0; acc: 0.23
Batch: 220; loss: 1.89; acc: 0.47
Batch: 240; loss: 1.85; acc: 0.45
Batch: 260; loss: 1.83; acc: 0.56
Batch: 280; loss: 1.87; acc: 0.45
Batch: 300; loss: 1.74; acc: 0.62
Batch: 320; loss: 1.82; acc: 0.53
Batch: 340; loss: 1.83; acc: 0.45
Batch: 360; loss: 1.84; acc: 0.47
Batch: 380; loss: 1.79; acc: 0.5
Batch: 400; loss: 1.88; acc: 0.5
Batch: 420; loss: 1.78; acc: 0.45
Batch: 440; loss: 1.75; acc: 0.59
Batch: 460; loss: 1.74; acc: 0.62
Batch: 480; loss: 1.81; acc: 0.47
Batch: 500; loss: 1.78; acc: 0.56
Batch: 520; loss: 1.72; acc: 0.61
Batch: 540; loss: 1.74; acc: 0.58
Batch: 560; loss: 1.67; acc: 0.55
Batch: 580; loss: 1.72; acc: 0.55
Batch: 600; loss: 1.71; acc: 0.56
Batch: 620; loss: 1.59; acc: 0.61
Batch: 640; loss: 1.6; acc: 0.64
Batch: 660; loss: 1.63; acc: 0.59
Batch: 680; loss: 1.66; acc: 0.58
Batch: 700; loss: 1.7; acc: 0.58
Batch: 720; loss: 1.68; acc: 0.5
Batch: 740; loss: 1.57; acc: 0.66
Batch: 760; loss: 1.54; acc: 0.69
Batch: 780; loss: 1.59; acc: 0.53
Train Epoch over. train_loss: 1.84; train_accuracy: 0.46 

5.448707815958187e-05
5.0301707233302295e-05
Batch: 0; loss: 1.64; acc: 0.56
Batch: 20; loss: 1.74; acc: 0.56
Batch: 40; loss: 1.44; acc: 0.62
Batch: 60; loss: 1.49; acc: 0.67
Batch: 80; loss: 1.45; acc: 0.7
Batch: 100; loss: 1.62; acc: 0.67
Batch: 120; loss: 1.76; acc: 0.52
Batch: 140; loss: 1.46; acc: 0.7
Val Epoch over. val_loss: 1.5907530860536416; val_accuracy: 0.6244028662420382 

The current subspace-distance is: 5.0301707233302295e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.55
Batch: 40; loss: 1.66; acc: 0.61
Batch: 60; loss: 1.6; acc: 0.64
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.42; acc: 0.75
Batch: 120; loss: 1.63; acc: 0.59
Batch: 140; loss: 1.56; acc: 0.66
Batch: 160; loss: 1.57; acc: 0.62
Batch: 180; loss: 1.57; acc: 0.56
Batch: 200; loss: 1.67; acc: 0.58
Batch: 220; loss: 1.64; acc: 0.52
Batch: 240; loss: 1.61; acc: 0.59
Batch: 260; loss: 1.58; acc: 0.56
Batch: 280; loss: 1.52; acc: 0.66
Batch: 300; loss: 1.49; acc: 0.64
Batch: 320; loss: 1.61; acc: 0.61
Batch: 340; loss: 1.56; acc: 0.61
Batch: 360; loss: 1.57; acc: 0.61
Batch: 380; loss: 1.56; acc: 0.59
Batch: 400; loss: 1.52; acc: 0.7
Batch: 420; loss: 1.56; acc: 0.67
Batch: 440; loss: 1.62; acc: 0.58
Batch: 460; loss: 1.67; acc: 0.42
Batch: 480; loss: 1.58; acc: 0.59
Batch: 500; loss: 1.47; acc: 0.7
Batch: 520; loss: 1.7; acc: 0.48
Batch: 540; loss: 1.5; acc: 0.67
Batch: 560; loss: 1.45; acc: 0.62
Batch: 580; loss: 1.46; acc: 0.69
Batch: 600; loss: 1.55; acc: 0.59
Batch: 620; loss: 1.56; acc: 0.59
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.56; acc: 0.55
Batch: 680; loss: 1.64; acc: 0.59
Batch: 700; loss: 1.58; acc: 0.61
Batch: 720; loss: 1.51; acc: 0.58
Batch: 740; loss: 1.5; acc: 0.7
Batch: 760; loss: 1.63; acc: 0.48
Batch: 780; loss: 1.54; acc: 0.61
Train Epoch over. train_loss: 1.57; train_accuracy: 0.6 

6.937051512068138e-05
6.340482650557533e-05
Batch: 0; loss: 1.5; acc: 0.61
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.3; acc: 0.69
Batch: 60; loss: 1.35; acc: 0.78
Batch: 80; loss: 1.37; acc: 0.7
Batch: 100; loss: 1.5; acc: 0.66
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.34; acc: 0.75
Val Epoch over. val_loss: 1.4876575356076478; val_accuracy: 0.6469944267515924 

The current subspace-distance is: 6.340482650557533e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.72
Batch: 20; loss: 1.49; acc: 0.75
Batch: 40; loss: 1.53; acc: 0.66
Batch: 60; loss: 1.73; acc: 0.53
Batch: 80; loss: 1.68; acc: 0.58
Batch: 100; loss: 1.62; acc: 0.55
Batch: 120; loss: 1.54; acc: 0.62
Batch: 140; loss: 1.62; acc: 0.53
Batch: 160; loss: 1.6; acc: 0.55
Batch: 180; loss: 1.51; acc: 0.64
Batch: 200; loss: 1.42; acc: 0.66
Batch: 220; loss: 1.52; acc: 0.64
Batch: 240; loss: 1.53; acc: 0.64
Batch: 260; loss: 1.49; acc: 0.64
Batch: 280; loss: 1.33; acc: 0.73
Batch: 300; loss: 1.64; acc: 0.53
Batch: 320; loss: 1.58; acc: 0.55
Batch: 340; loss: 1.4; acc: 0.72
Batch: 360; loss: 1.56; acc: 0.61
Batch: 380; loss: 1.47; acc: 0.62
Batch: 400; loss: 1.6; acc: 0.59
Batch: 420; loss: 1.48; acc: 0.61
Batch: 440; loss: 1.52; acc: 0.61
Batch: 460; loss: 1.6; acc: 0.5
Batch: 480; loss: 1.48; acc: 0.58
Batch: 500; loss: 1.55; acc: 0.61
Batch: 520; loss: 1.63; acc: 0.58
Batch: 540; loss: 1.59; acc: 0.59
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.53; acc: 0.64
Batch: 600; loss: 1.56; acc: 0.59
Batch: 620; loss: 1.39; acc: 0.67
Batch: 640; loss: 1.49; acc: 0.59
Batch: 660; loss: 1.44; acc: 0.69
Batch: 680; loss: 1.52; acc: 0.61
Batch: 700; loss: 1.62; acc: 0.5
Batch: 720; loss: 1.33; acc: 0.73
Batch: 740; loss: 1.54; acc: 0.61
Batch: 760; loss: 1.56; acc: 0.64
Batch: 780; loss: 1.42; acc: 0.67
Train Epoch over. train_loss: 1.5; train_accuracy: 0.62 

7.961590745253488e-05
7.413452112814412e-05
Batch: 0; loss: 1.47; acc: 0.62
Batch: 20; loss: 1.57; acc: 0.55
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.29; acc: 0.75
Batch: 80; loss: 1.32; acc: 0.72
Batch: 100; loss: 1.46; acc: 0.72
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.27; acc: 0.75
Val Epoch over. val_loss: 1.43913637908401; val_accuracy: 0.6546576433121019 

The current subspace-distance is: 7.413452112814412e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.4; acc: 0.72
Batch: 40; loss: 1.49; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.64
Batch: 80; loss: 1.66; acc: 0.53
Batch: 100; loss: 1.4; acc: 0.7
Batch: 120; loss: 1.5; acc: 0.59
Batch: 140; loss: 1.42; acc: 0.64
Batch: 160; loss: 1.42; acc: 0.69
Batch: 180; loss: 1.45; acc: 0.62
Batch: 200; loss: 1.4; acc: 0.67
Batch: 220; loss: 1.47; acc: 0.72
Batch: 240; loss: 1.53; acc: 0.53
Batch: 260; loss: 1.48; acc: 0.59
Batch: 280; loss: 1.58; acc: 0.55
Batch: 300; loss: 1.48; acc: 0.64
Batch: 320; loss: 1.34; acc: 0.73
Batch: 340; loss: 1.49; acc: 0.66
Batch: 360; loss: 1.39; acc: 0.69
Batch: 380; loss: 1.43; acc: 0.69
Batch: 400; loss: 1.51; acc: 0.55
Batch: 420; loss: 1.33; acc: 0.78
Batch: 440; loss: 1.51; acc: 0.55
Batch: 460; loss: 1.35; acc: 0.72
Batch: 480; loss: 1.38; acc: 0.69
Batch: 500; loss: 1.41; acc: 0.69
Batch: 520; loss: 1.53; acc: 0.67
Batch: 540; loss: 1.44; acc: 0.59
Batch: 560; loss: 1.34; acc: 0.72
Batch: 580; loss: 1.35; acc: 0.72
Batch: 600; loss: 1.45; acc: 0.62
Batch: 620; loss: 1.41; acc: 0.62
Batch: 640; loss: 1.5; acc: 0.58
Batch: 660; loss: 1.43; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.7
Batch: 700; loss: 1.4; acc: 0.66
Batch: 720; loss: 1.51; acc: 0.62
Batch: 740; loss: 1.19; acc: 0.78
Batch: 760; loss: 1.5; acc: 0.56
Batch: 780; loss: 1.5; acc: 0.64
Train Epoch over. train_loss: 1.45; train_accuracy: 0.64 

9.334861533716321e-05
8.848039578879252e-05
Batch: 0; loss: 1.43; acc: 0.62
Batch: 20; loss: 1.53; acc: 0.66
Batch: 40; loss: 1.14; acc: 0.78
Batch: 60; loss: 1.26; acc: 0.72
Batch: 80; loss: 1.28; acc: 0.73
Batch: 100; loss: 1.41; acc: 0.78
Batch: 120; loss: 1.61; acc: 0.5
Batch: 140; loss: 1.22; acc: 0.81
Val Epoch over. val_loss: 1.3927475791068593; val_accuracy: 0.6725716560509554 

The current subspace-distance is: 8.848039578879252e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.66
Batch: 20; loss: 1.45; acc: 0.64
Batch: 40; loss: 1.38; acc: 0.66
Batch: 60; loss: 1.42; acc: 0.64
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.39; acc: 0.62
Batch: 120; loss: 1.43; acc: 0.66
Batch: 140; loss: 1.27; acc: 0.75
Batch: 160; loss: 1.38; acc: 0.69
Batch: 180; loss: 1.46; acc: 0.61
Batch: 200; loss: 1.47; acc: 0.58
Batch: 220; loss: 1.35; acc: 0.67
Batch: 240; loss: 1.47; acc: 0.62
Batch: 260; loss: 1.38; acc: 0.59
Batch: 280; loss: 1.27; acc: 0.77
Batch: 300; loss: 1.42; acc: 0.61
Batch: 320; loss: 1.49; acc: 0.58
Batch: 340; loss: 1.46; acc: 0.59
Batch: 360; loss: 1.25; acc: 0.77
Batch: 380; loss: 1.38; acc: 0.62
Batch: 400; loss: 1.42; acc: 0.62
Batch: 420; loss: 1.41; acc: 0.61
Batch: 440; loss: 1.39; acc: 0.67
Batch: 460; loss: 1.39; acc: 0.64
Batch: 480; loss: 1.34; acc: 0.66
Batch: 500; loss: 1.41; acc: 0.59
Batch: 520; loss: 1.32; acc: 0.69
Batch: 540; loss: 1.32; acc: 0.67
Batch: 560; loss: 1.48; acc: 0.61
Batch: 580; loss: 1.39; acc: 0.64
Batch: 600; loss: 1.34; acc: 0.66
Batch: 620; loss: 1.23; acc: 0.73
Batch: 640; loss: 1.43; acc: 0.62
Batch: 660; loss: 1.26; acc: 0.73
Batch: 680; loss: 1.56; acc: 0.52
Batch: 700; loss: 1.25; acc: 0.69
Batch: 720; loss: 1.4; acc: 0.61
Batch: 740; loss: 1.4; acc: 0.62
Batch: 760; loss: 1.36; acc: 0.61
Batch: 780; loss: 1.24; acc: 0.73
Train Epoch over. train_loss: 1.39; train_accuracy: 0.65 

0.0001030412022373639
9.76853771135211e-05
Batch: 0; loss: 1.34; acc: 0.7
Batch: 20; loss: 1.43; acc: 0.66
Batch: 40; loss: 1.06; acc: 0.78
Batch: 60; loss: 1.22; acc: 0.72
Batch: 80; loss: 1.2; acc: 0.81
Batch: 100; loss: 1.33; acc: 0.77
Batch: 120; loss: 1.57; acc: 0.5
Batch: 140; loss: 1.17; acc: 0.8
Val Epoch over. val_loss: 1.3207640093602953; val_accuracy: 0.6868033439490446 

The current subspace-distance is: 9.76853771135211e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.31; acc: 0.69
Batch: 40; loss: 1.37; acc: 0.61
Batch: 60; loss: 1.4; acc: 0.69
Batch: 80; loss: 1.24; acc: 0.75
Batch: 100; loss: 1.39; acc: 0.66
Batch: 120; loss: 1.25; acc: 0.77
Batch: 140; loss: 1.37; acc: 0.62
Batch: 160; loss: 1.49; acc: 0.59
Batch: 180; loss: 1.38; acc: 0.62
Batch: 200; loss: 1.47; acc: 0.58
Batch: 220; loss: 1.3; acc: 0.7
Batch: 240; loss: 1.3; acc: 0.7
Batch: 260; loss: 1.23; acc: 0.75
Batch: 280; loss: 1.44; acc: 0.58
Batch: 300; loss: 1.29; acc: 0.62
Batch: 320; loss: 1.26; acc: 0.7
Batch: 340; loss: 1.31; acc: 0.66
Batch: 360; loss: 1.42; acc: 0.58
Batch: 380; loss: 1.3; acc: 0.7
Batch: 400; loss: 1.25; acc: 0.7
Batch: 420; loss: 1.34; acc: 0.66
Batch: 440; loss: 1.45; acc: 0.56
Batch: 460; loss: 1.27; acc: 0.7
Batch: 480; loss: 1.33; acc: 0.69
Batch: 500; loss: 1.21; acc: 0.69
Batch: 520; loss: 1.28; acc: 0.66
Batch: 540; loss: 1.36; acc: 0.67
Batch: 560; loss: 1.37; acc: 0.66
Batch: 580; loss: 1.26; acc: 0.62
Batch: 600; loss: 1.14; acc: 0.78
Batch: 620; loss: 1.41; acc: 0.59
Batch: 640; loss: 1.27; acc: 0.72
Batch: 660; loss: 1.33; acc: 0.69
Batch: 680; loss: 1.31; acc: 0.7
Batch: 700; loss: 1.23; acc: 0.7
Batch: 720; loss: 1.27; acc: 0.67
Batch: 740; loss: 1.23; acc: 0.75
Batch: 760; loss: 1.3; acc: 0.72
Batch: 780; loss: 1.37; acc: 0.58
Train Epoch over. train_loss: 1.35; train_accuracy: 0.66 

0.00011534763325471431
0.00011104626901214942
Batch: 0; loss: 1.31; acc: 0.67
Batch: 20; loss: 1.37; acc: 0.67
Batch: 40; loss: 1.01; acc: 0.75
Batch: 60; loss: 1.21; acc: 0.73
Batch: 80; loss: 1.18; acc: 0.75
Batch: 100; loss: 1.26; acc: 0.8
Batch: 120; loss: 1.52; acc: 0.52
Batch: 140; loss: 1.13; acc: 0.8
Val Epoch over. val_loss: 1.273585904935363; val_accuracy: 0.6944665605095541 

The current subspace-distance is: 0.00011104626901214942 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.73
Batch: 20; loss: 1.4; acc: 0.56
Batch: 40; loss: 1.3; acc: 0.72
Batch: 60; loss: 1.27; acc: 0.66
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.26; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 1.32; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.72
Batch: 180; loss: 1.23; acc: 0.72
Batch: 200; loss: 1.33; acc: 0.64
Batch: 220; loss: 1.31; acc: 0.62
Batch: 240; loss: 1.26; acc: 0.67
Batch: 260; loss: 1.4; acc: 0.61
Batch: 280; loss: 1.2; acc: 0.75
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 1.18; acc: 0.7
Batch: 340; loss: 1.27; acc: 0.72
Batch: 360; loss: 1.2; acc: 0.73
Batch: 380; loss: 1.26; acc: 0.73
Batch: 400; loss: 1.35; acc: 0.66
Batch: 420; loss: 1.35; acc: 0.61
Batch: 440; loss: 1.18; acc: 0.73
Batch: 460; loss: 1.38; acc: 0.61
Batch: 480; loss: 1.41; acc: 0.64
Batch: 500; loss: 1.25; acc: 0.64
Batch: 520; loss: 1.37; acc: 0.64
Batch: 540; loss: 1.29; acc: 0.64
Batch: 560; loss: 1.24; acc: 0.69
Batch: 580; loss: 1.3; acc: 0.62
Batch: 600; loss: 1.36; acc: 0.67
Batch: 620; loss: 1.28; acc: 0.61
Batch: 640; loss: 1.33; acc: 0.64
Batch: 660; loss: 1.25; acc: 0.69
Batch: 680; loss: 1.22; acc: 0.7
Batch: 700; loss: 1.26; acc: 0.69
Batch: 720; loss: 1.25; acc: 0.77
Batch: 740; loss: 1.3; acc: 0.7
Batch: 760; loss: 1.44; acc: 0.58
Batch: 780; loss: 1.31; acc: 0.61
Train Epoch over. train_loss: 1.31; train_accuracy: 0.66 

0.00012630975106731057
0.00011959068069700152
Batch: 0; loss: 1.3; acc: 0.69
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 1.22; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.23; acc: 0.78
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.1; acc: 0.78
Val Epoch over. val_loss: 1.2501812429185126; val_accuracy: 0.6965565286624203 

The current subspace-distance is: 0.00011959068069700152 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.21; acc: 0.69
Batch: 40; loss: 1.39; acc: 0.53
Batch: 60; loss: 1.3; acc: 0.64
Batch: 80; loss: 1.34; acc: 0.62
Batch: 100; loss: 1.3; acc: 0.72
Batch: 120; loss: 1.36; acc: 0.62
Batch: 140; loss: 1.25; acc: 0.72
Batch: 160; loss: 1.41; acc: 0.62
Batch: 180; loss: 1.37; acc: 0.72
Batch: 200; loss: 1.31; acc: 0.64
Batch: 220; loss: 1.34; acc: 0.62
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.25; acc: 0.7
Batch: 280; loss: 1.3; acc: 0.66
Batch: 300; loss: 1.18; acc: 0.72
Batch: 320; loss: 1.08; acc: 0.77
Batch: 340; loss: 1.21; acc: 0.75
Batch: 360; loss: 1.42; acc: 0.61
Batch: 380; loss: 1.33; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.7
Batch: 420; loss: 1.26; acc: 0.64
Batch: 440; loss: 1.16; acc: 0.72
Batch: 460; loss: 1.15; acc: 0.7
Batch: 480; loss: 1.26; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 1.31; acc: 0.64
Batch: 540; loss: 1.25; acc: 0.64
Batch: 560; loss: 1.28; acc: 0.77
Batch: 580; loss: 1.31; acc: 0.67
Batch: 600; loss: 1.29; acc: 0.64
Batch: 620; loss: 1.28; acc: 0.64
Batch: 640; loss: 1.28; acc: 0.75
Batch: 660; loss: 1.29; acc: 0.66
Batch: 680; loss: 1.22; acc: 0.7
Batch: 700; loss: 1.16; acc: 0.73
Batch: 720; loss: 1.31; acc: 0.67
Batch: 740; loss: 1.41; acc: 0.55
Batch: 760; loss: 1.21; acc: 0.69
Batch: 780; loss: 1.15; acc: 0.73
Train Epoch over. train_loss: 1.28; train_accuracy: 0.66 

0.0001329309307038784
0.0001261215948034078
Batch: 0; loss: 1.27; acc: 0.67
Batch: 20; loss: 1.34; acc: 0.66
Batch: 40; loss: 0.96; acc: 0.8
Batch: 60; loss: 1.22; acc: 0.7
Batch: 80; loss: 1.14; acc: 0.73
Batch: 100; loss: 1.2; acc: 0.77
Batch: 120; loss: 1.45; acc: 0.52
Batch: 140; loss: 1.07; acc: 0.78
Val Epoch over. val_loss: 1.2284540158168527; val_accuracy: 0.6922770700636943 

The current subspace-distance is: 0.0001261215948034078 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.37; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.72
Batch: 40; loss: 1.17; acc: 0.7
Batch: 60; loss: 1.45; acc: 0.61
Batch: 80; loss: 1.39; acc: 0.59
Batch: 100; loss: 1.21; acc: 0.67
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 1.41; acc: 0.55
Batch: 160; loss: 1.33; acc: 0.58
Batch: 180; loss: 1.32; acc: 0.66
Batch: 200; loss: 1.29; acc: 0.61
Batch: 220; loss: 1.18; acc: 0.7
Batch: 240; loss: 1.24; acc: 0.67
Batch: 260; loss: 1.27; acc: 0.64
Batch: 280; loss: 1.35; acc: 0.64
Batch: 300; loss: 1.25; acc: 0.69
Batch: 320; loss: 1.15; acc: 0.83
Batch: 340; loss: 1.22; acc: 0.7
Batch: 360; loss: 1.17; acc: 0.66
Batch: 380; loss: 1.23; acc: 0.69
Batch: 400; loss: 1.26; acc: 0.61
Batch: 420; loss: 1.32; acc: 0.62
Batch: 440; loss: 1.21; acc: 0.72
Batch: 460; loss: 1.25; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.66
Batch: 500; loss: 1.31; acc: 0.62
Batch: 520; loss: 1.17; acc: 0.75
Batch: 540; loss: 1.2; acc: 0.75
Batch: 560; loss: 1.36; acc: 0.64
Batch: 580; loss: 1.4; acc: 0.62
Batch: 600; loss: 1.23; acc: 0.66
Batch: 620; loss: 1.34; acc: 0.62
Batch: 640; loss: 1.25; acc: 0.67
Batch: 660; loss: 1.21; acc: 0.64
Batch: 680; loss: 1.34; acc: 0.56
Batch: 700; loss: 1.25; acc: 0.64
Batch: 720; loss: 1.39; acc: 0.59
Batch: 740; loss: 1.29; acc: 0.58
Batch: 760; loss: 1.45; acc: 0.59
Batch: 780; loss: 1.15; acc: 0.7
Train Epoch over. train_loss: 1.26; train_accuracy: 0.67 

0.00014188734348863363
0.00013518863124772906
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.31; acc: 0.67
Batch: 40; loss: 0.92; acc: 0.8
Batch: 60; loss: 1.19; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.77
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 1.03; acc: 0.8
Val Epoch over. val_loss: 1.1918814865646847; val_accuracy: 0.699343152866242 

The current subspace-distance is: 0.00013518863124772906 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 1.32; acc: 0.66
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.2; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.66
Batch: 160; loss: 1.25; acc: 0.7
Batch: 180; loss: 1.26; acc: 0.69
Batch: 200; loss: 1.12; acc: 0.7
Batch: 220; loss: 1.16; acc: 0.69
Batch: 240; loss: 1.1; acc: 0.77
Batch: 260; loss: 1.51; acc: 0.47
Batch: 280; loss: 1.24; acc: 0.66
Batch: 300; loss: 1.16; acc: 0.69
Batch: 320; loss: 1.21; acc: 0.67
Batch: 340; loss: 1.18; acc: 0.7
Batch: 360; loss: 1.34; acc: 0.61
Batch: 380; loss: 1.18; acc: 0.78
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.21; acc: 0.69
Batch: 440; loss: 1.14; acc: 0.7
Batch: 460; loss: 1.49; acc: 0.53
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.25; acc: 0.58
Batch: 520; loss: 1.27; acc: 0.66
Batch: 540; loss: 1.16; acc: 0.72
Batch: 560; loss: 1.27; acc: 0.66
Batch: 580; loss: 1.11; acc: 0.72
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 1.21; acc: 0.64
Batch: 640; loss: 1.24; acc: 0.62
Batch: 660; loss: 1.16; acc: 0.7
Batch: 680; loss: 1.22; acc: 0.69
Batch: 700; loss: 1.2; acc: 0.61
Batch: 720; loss: 1.13; acc: 0.72
Batch: 740; loss: 1.07; acc: 0.75
Batch: 760; loss: 1.16; acc: 0.7
Batch: 780; loss: 1.24; acc: 0.67
Train Epoch over. train_loss: 1.23; train_accuracy: 0.67 

0.00015049644571263343
0.00014283588097896427
Batch: 0; loss: 1.21; acc: 0.69
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.17; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.78
Batch: 100; loss: 1.13; acc: 0.77
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 1.02; acc: 0.78
Val Epoch over. val_loss: 1.172076106830767; val_accuracy: 0.7048168789808917 

The current subspace-distance is: 0.00014283588097896427 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.37; acc: 0.58
Batch: 80; loss: 1.42; acc: 0.55
Batch: 100; loss: 1.36; acc: 0.52
Batch: 120; loss: 1.13; acc: 0.75
Batch: 140; loss: 1.17; acc: 0.7
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.29; acc: 0.61
Batch: 200; loss: 1.22; acc: 0.67
Batch: 220; loss: 1.38; acc: 0.62
Batch: 240; loss: 1.31; acc: 0.58
Batch: 260; loss: 1.1; acc: 0.69
Batch: 280; loss: 1.32; acc: 0.64
Batch: 300; loss: 1.3; acc: 0.62
Batch: 320; loss: 1.22; acc: 0.66
Batch: 340; loss: 1.2; acc: 0.64
Batch: 360; loss: 1.13; acc: 0.7
Batch: 380; loss: 1.16; acc: 0.7
Batch: 400; loss: 0.97; acc: 0.81
Batch: 420; loss: 1.38; acc: 0.59
Batch: 440; loss: 1.06; acc: 0.72
Batch: 460; loss: 1.08; acc: 0.72
Batch: 480; loss: 1.29; acc: 0.67
Batch: 500; loss: 1.34; acc: 0.64
Batch: 520; loss: 1.25; acc: 0.66
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.11; acc: 0.81
Batch: 580; loss: 1.3; acc: 0.72
Batch: 600; loss: 1.28; acc: 0.62
Batch: 620; loss: 1.23; acc: 0.72
Batch: 640; loss: 1.12; acc: 0.72
Batch: 660; loss: 1.19; acc: 0.64
Batch: 680; loss: 1.25; acc: 0.66
Batch: 700; loss: 1.18; acc: 0.73
Batch: 720; loss: 1.1; acc: 0.69
Batch: 740; loss: 1.18; acc: 0.75
Batch: 760; loss: 1.17; acc: 0.72
Batch: 780; loss: 1.18; acc: 0.69
Train Epoch over. train_loss: 1.21; train_accuracy: 0.68 

0.0001524668768979609
0.00014645431656390429
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.07; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.34; acc: 0.66
Batch: 140; loss: 1.01; acc: 0.8
Val Epoch over. val_loss: 1.1705406706803922; val_accuracy: 0.7029259554140127 

The current subspace-distance is: 0.00014645431656390429 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.12; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.69
Batch: 80; loss: 1.15; acc: 0.75
Batch: 100; loss: 1.25; acc: 0.61
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 1.22; acc: 0.64
Batch: 160; loss: 1.19; acc: 0.7
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.23; acc: 0.66
Batch: 220; loss: 1.2; acc: 0.69
Batch: 240; loss: 1.19; acc: 0.77
Batch: 260; loss: 1.2; acc: 0.69
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.4; acc: 0.61
Batch: 320; loss: 1.13; acc: 0.7
Batch: 340; loss: 1.13; acc: 0.72
Batch: 360; loss: 1.02; acc: 0.72
Batch: 380; loss: 1.17; acc: 0.73
Batch: 400; loss: 1.11; acc: 0.75
Batch: 420; loss: 1.15; acc: 0.75
Batch: 440; loss: 1.22; acc: 0.73
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 1.15; acc: 0.72
Batch: 500; loss: 1.25; acc: 0.73
Batch: 520; loss: 1.15; acc: 0.67
Batch: 540; loss: 1.2; acc: 0.64
Batch: 560; loss: 1.17; acc: 0.69
Batch: 580; loss: 1.1; acc: 0.77
Batch: 600; loss: 1.38; acc: 0.62
Batch: 620; loss: 1.11; acc: 0.67
Batch: 640; loss: 1.17; acc: 0.75
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.15; acc: 0.75
Batch: 700; loss: 1.24; acc: 0.72
Batch: 720; loss: 1.21; acc: 0.72
Batch: 740; loss: 1.35; acc: 0.58
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.2; train_accuracy: 0.68 

0.00015719550719950348
0.00014786701649427414
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.8
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.62
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1523671624766794; val_accuracy: 0.7039211783439491 

The current subspace-distance is: 0.00014786701649427414 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.14; acc: 0.73
Batch: 40; loss: 1.22; acc: 0.64
Batch: 60; loss: 1.17; acc: 0.7
Batch: 80; loss: 1.16; acc: 0.67
Batch: 100; loss: 1.25; acc: 0.69
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 1.11; acc: 0.69
Batch: 180; loss: 1.23; acc: 0.7
Batch: 200; loss: 1.09; acc: 0.75
Batch: 220; loss: 1.19; acc: 0.62
Batch: 240; loss: 1.19; acc: 0.66
Batch: 260; loss: 1.09; acc: 0.8
Batch: 280; loss: 1.05; acc: 0.77
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.32; acc: 0.61
Batch: 340; loss: 1.23; acc: 0.7
Batch: 360; loss: 1.25; acc: 0.67
Batch: 380; loss: 1.23; acc: 0.64
Batch: 400; loss: 1.21; acc: 0.69
Batch: 420; loss: 1.16; acc: 0.66
Batch: 440; loss: 1.22; acc: 0.67
Batch: 460; loss: 1.2; acc: 0.67
Batch: 480; loss: 1.27; acc: 0.59
Batch: 500; loss: 1.19; acc: 0.7
Batch: 520; loss: 1.39; acc: 0.62
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 1.08; acc: 0.64
Batch: 580; loss: 1.19; acc: 0.72
Batch: 600; loss: 1.33; acc: 0.64
Batch: 620; loss: 1.12; acc: 0.7
Batch: 640; loss: 1.08; acc: 0.75
Batch: 660; loss: 1.32; acc: 0.58
Batch: 680; loss: 1.19; acc: 0.72
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 1.16; acc: 0.67
Batch: 740; loss: 1.2; acc: 0.73
Batch: 760; loss: 1.39; acc: 0.62
Batch: 780; loss: 1.16; acc: 0.61
Train Epoch over. train_loss: 1.19; train_accuracy: 0.68 

0.00016000811592675745
0.00015200991765595973
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.33; acc: 0.59
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.31; acc: 0.62
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1518212636564946; val_accuracy: 0.7013335987261147 

The current subspace-distance is: 0.00015200991765595973 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.58
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 1.15; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 1.15; acc: 0.69
Batch: 100; loss: 1.33; acc: 0.58
Batch: 120; loss: 1.39; acc: 0.55
Batch: 140; loss: 1.06; acc: 0.8
Batch: 160; loss: 1.15; acc: 0.73
Batch: 180; loss: 1.08; acc: 0.75
Batch: 200; loss: 1.28; acc: 0.69
Batch: 220; loss: 1.25; acc: 0.62
Batch: 240; loss: 1.03; acc: 0.73
Batch: 260; loss: 1.05; acc: 0.78
Batch: 280; loss: 1.22; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 1.23; acc: 0.62
Batch: 340; loss: 1.17; acc: 0.72
Batch: 360; loss: 1.07; acc: 0.75
Batch: 380; loss: 1.05; acc: 0.73
Batch: 400; loss: 1.15; acc: 0.7
Batch: 420; loss: 1.23; acc: 0.62
Batch: 440; loss: 1.17; acc: 0.67
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 1.29; acc: 0.62
Batch: 500; loss: 1.15; acc: 0.69
Batch: 520; loss: 1.2; acc: 0.69
Batch: 540; loss: 1.21; acc: 0.64
Batch: 560; loss: 1.27; acc: 0.62
Batch: 580; loss: 1.24; acc: 0.67
Batch: 600; loss: 1.34; acc: 0.61
Batch: 620; loss: 1.09; acc: 0.7
Batch: 640; loss: 1.09; acc: 0.7
Batch: 660; loss: 1.19; acc: 0.72
Batch: 680; loss: 1.25; acc: 0.66
Batch: 700; loss: 1.21; acc: 0.62
Batch: 720; loss: 1.22; acc: 0.66
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 1.32; acc: 0.61
Batch: 780; loss: 1.14; acc: 0.7
Train Epoch over. train_loss: 1.18; train_accuracy: 0.68 

0.00016137762577272952
0.00015509511285927147
Batch: 0; loss: 1.2; acc: 0.64
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.83
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.1439962614873411; val_accuracy: 0.7043192675159236 

The current subspace-distance is: 0.00015509511285927147 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 1.09; acc: 0.73
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.23; acc: 0.62
Batch: 100; loss: 1.24; acc: 0.62
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.7
Batch: 160; loss: 1.27; acc: 0.66
Batch: 180; loss: 1.1; acc: 0.72
Batch: 200; loss: 1.18; acc: 0.69
Batch: 220; loss: 1.19; acc: 0.69
Batch: 240; loss: 1.13; acc: 0.77
Batch: 260; loss: 1.22; acc: 0.67
Batch: 280; loss: 1.1; acc: 0.73
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 1.24; acc: 0.69
Batch: 340; loss: 1.28; acc: 0.58
Batch: 360; loss: 1.31; acc: 0.59
Batch: 380; loss: 1.22; acc: 0.66
Batch: 400; loss: 1.46; acc: 0.61
Batch: 420; loss: 1.16; acc: 0.69
Batch: 440; loss: 1.14; acc: 0.78
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 1.12; acc: 0.66
Batch: 500; loss: 1.26; acc: 0.64
Batch: 520; loss: 1.14; acc: 0.67
Batch: 540; loss: 1.07; acc: 0.72
Batch: 560; loss: 1.22; acc: 0.72
Batch: 580; loss: 1.18; acc: 0.62
Batch: 600; loss: 1.1; acc: 0.77
Batch: 620; loss: 1.25; acc: 0.59
Batch: 640; loss: 0.95; acc: 0.78
Batch: 660; loss: 1.1; acc: 0.69
Batch: 680; loss: 1.25; acc: 0.58
Batch: 700; loss: 1.03; acc: 0.8
Batch: 720; loss: 0.98; acc: 0.77
Batch: 740; loss: 0.96; acc: 0.77
Batch: 760; loss: 1.11; acc: 0.73
Batch: 780; loss: 1.01; acc: 0.77
Train Epoch over. train_loss: 1.17; train_accuracy: 0.68 

0.00016356841661036015
0.00015430412895511836
Batch: 0; loss: 1.19; acc: 0.64
Batch: 20; loss: 1.34; acc: 0.59
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.83
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.1353419776175433; val_accuracy: 0.7063097133757962 

The current subspace-distance is: 0.00015430412895511836 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.72
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 1.12; acc: 0.72
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.29; acc: 0.69
Batch: 120; loss: 1.19; acc: 0.62
Batch: 140; loss: 1.17; acc: 0.7
Batch: 160; loss: 1.14; acc: 0.7
Batch: 180; loss: 1.0; acc: 0.81
Batch: 200; loss: 0.96; acc: 0.77
Batch: 220; loss: 1.26; acc: 0.64
Batch: 240; loss: 1.23; acc: 0.64
Batch: 260; loss: 1.34; acc: 0.53
Batch: 280; loss: 1.32; acc: 0.61
Batch: 300; loss: 1.41; acc: 0.59
Batch: 320; loss: 1.19; acc: 0.7
Batch: 340; loss: 1.18; acc: 0.67
Batch: 360; loss: 1.19; acc: 0.64
Batch: 380; loss: 1.03; acc: 0.75
Batch: 400; loss: 1.2; acc: 0.64
Batch: 420; loss: 1.26; acc: 0.64
Batch: 440; loss: 1.03; acc: 0.75
Batch: 460; loss: 1.19; acc: 0.64
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 1.09; acc: 0.78
Batch: 520; loss: 1.08; acc: 0.75
Batch: 540; loss: 1.31; acc: 0.61
Batch: 560; loss: 1.26; acc: 0.62
Batch: 580; loss: 0.98; acc: 0.77
Batch: 600; loss: 1.22; acc: 0.66
Batch: 620; loss: 1.23; acc: 0.64
Batch: 640; loss: 1.07; acc: 0.73
Batch: 660; loss: 1.08; acc: 0.73
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.16; acc: 0.64
Batch: 720; loss: 1.36; acc: 0.58
Batch: 740; loss: 1.21; acc: 0.66
Batch: 760; loss: 1.26; acc: 0.61
Batch: 780; loss: 1.11; acc: 0.7
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.0001658095425227657
0.00015814966172911227
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 1.33; acc: 0.59
Batch: 40; loss: 0.85; acc: 0.81
Batch: 60; loss: 1.13; acc: 0.69
Batch: 80; loss: 1.01; acc: 0.83
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.1246544023987595; val_accuracy: 0.7064092356687898 

The current subspace-distance is: 0.00015814966172911227 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 1.05; acc: 0.7
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.23; acc: 0.69
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.25; acc: 0.62
Batch: 160; loss: 1.16; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.56
Batch: 200; loss: 1.08; acc: 0.72
Batch: 220; loss: 1.08; acc: 0.73
Batch: 240; loss: 1.2; acc: 0.72
Batch: 260; loss: 1.02; acc: 0.78
Batch: 280; loss: 1.28; acc: 0.55
Batch: 300; loss: 1.18; acc: 0.72
Batch: 320; loss: 1.14; acc: 0.72
Batch: 340; loss: 1.3; acc: 0.61
Batch: 360; loss: 1.14; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.67
Batch: 400; loss: 1.23; acc: 0.64
Batch: 420; loss: 1.08; acc: 0.75
Batch: 440; loss: 1.16; acc: 0.66
Batch: 460; loss: 1.03; acc: 0.77
Batch: 480; loss: 1.16; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 1.1; acc: 0.8
Batch: 540; loss: 1.24; acc: 0.66
Batch: 560; loss: 1.04; acc: 0.66
Batch: 580; loss: 1.1; acc: 0.77
Batch: 600; loss: 1.24; acc: 0.59
Batch: 620; loss: 1.2; acc: 0.64
Batch: 640; loss: 1.07; acc: 0.77
Batch: 660; loss: 1.22; acc: 0.62
Batch: 680; loss: 1.21; acc: 0.66
Batch: 700; loss: 1.26; acc: 0.62
Batch: 720; loss: 1.23; acc: 0.66
Batch: 740; loss: 1.1; acc: 0.69
Batch: 760; loss: 0.98; acc: 0.77
Batch: 780; loss: 1.18; acc: 0.67
Train Epoch over. train_loss: 1.16; train_accuracy: 0.69 

0.00016955455066636205
0.00016115765902213752
Batch: 0; loss: 1.17; acc: 0.66
Batch: 20; loss: 1.31; acc: 0.58
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.69
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.97; acc: 0.81
Val Epoch over. val_loss: 1.114579625949738; val_accuracy: 0.7089968152866242 

The current subspace-distance is: 0.00016115765902213752 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 1.14; acc: 0.67
Batch: 40; loss: 1.33; acc: 0.59
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.27; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 0.96; acc: 0.83
Batch: 160; loss: 1.1; acc: 0.69
Batch: 180; loss: 1.06; acc: 0.78
Batch: 200; loss: 1.24; acc: 0.73
Batch: 220; loss: 1.09; acc: 0.77
Batch: 240; loss: 1.2; acc: 0.66
Batch: 260; loss: 1.15; acc: 0.69
Batch: 280; loss: 1.17; acc: 0.67
Batch: 300; loss: 1.14; acc: 0.69
Batch: 320; loss: 1.07; acc: 0.69
Batch: 340; loss: 1.33; acc: 0.64
Batch: 360; loss: 1.17; acc: 0.67
Batch: 380; loss: 1.11; acc: 0.73
Batch: 400; loss: 1.19; acc: 0.67
Batch: 420; loss: 1.17; acc: 0.7
Batch: 440; loss: 1.32; acc: 0.58
Batch: 460; loss: 1.25; acc: 0.69
Batch: 480; loss: 1.05; acc: 0.77
Batch: 500; loss: 1.29; acc: 0.61
Batch: 520; loss: 1.07; acc: 0.72
Batch: 540; loss: 1.24; acc: 0.58
Batch: 560; loss: 1.14; acc: 0.66
Batch: 580; loss: 1.41; acc: 0.62
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 1.17; acc: 0.66
Batch: 640; loss: 1.31; acc: 0.59
Batch: 660; loss: 1.21; acc: 0.67
Batch: 680; loss: 1.11; acc: 0.7
Batch: 700; loss: 1.21; acc: 0.64
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 1.23; acc: 0.62
Batch: 760; loss: 1.16; acc: 0.69
Batch: 780; loss: 1.17; acc: 0.64
Train Epoch over. train_loss: 1.15; train_accuracy: 0.69 

0.00017232312529813498
0.00016554606554564089
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.3; acc: 0.59
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.81
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.1076656481262985; val_accuracy: 0.7080015923566879 

The current subspace-distance is: 0.00016554606554564089 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 1.12; acc: 0.72
Batch: 60; loss: 1.02; acc: 0.8
Batch: 80; loss: 0.97; acc: 0.73
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.04; acc: 0.67
Batch: 140; loss: 1.18; acc: 0.69
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 1.28; acc: 0.59
Batch: 200; loss: 1.06; acc: 0.78
Batch: 220; loss: 1.02; acc: 0.77
Batch: 240; loss: 1.08; acc: 0.72
Batch: 260; loss: 1.18; acc: 0.62
Batch: 280; loss: 0.99; acc: 0.77
Batch: 300; loss: 0.93; acc: 0.86
Batch: 320; loss: 1.3; acc: 0.62
Batch: 340; loss: 0.98; acc: 0.75
Batch: 360; loss: 1.1; acc: 0.69
Batch: 380; loss: 0.99; acc: 0.73
Batch: 400; loss: 1.09; acc: 0.72
Batch: 420; loss: 1.18; acc: 0.75
Batch: 440; loss: 1.08; acc: 0.75
Batch: 460; loss: 1.3; acc: 0.59
Batch: 480; loss: 1.2; acc: 0.62
Batch: 500; loss: 1.05; acc: 0.69
Batch: 520; loss: 1.03; acc: 0.69
Batch: 540; loss: 1.16; acc: 0.64
Batch: 560; loss: 1.1; acc: 0.67
Batch: 580; loss: 1.3; acc: 0.64
Batch: 600; loss: 1.11; acc: 0.73
Batch: 620; loss: 1.2; acc: 0.66
Batch: 640; loss: 0.93; acc: 0.83
Batch: 660; loss: 1.04; acc: 0.67
Batch: 680; loss: 1.09; acc: 0.69
Batch: 700; loss: 1.23; acc: 0.66
Batch: 720; loss: 1.09; acc: 0.7
Batch: 740; loss: 1.15; acc: 0.77
Batch: 760; loss: 1.32; acc: 0.62
Batch: 780; loss: 1.32; acc: 0.56
Train Epoch over. train_loss: 1.14; train_accuracy: 0.69 

0.00017495155043434352
0.00016561347001697868
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 0.99; acc: 0.81
Batch: 100; loss: 1.08; acc: 0.69
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.1035412090599157; val_accuracy: 0.7107882165605095 

The current subspace-distance is: 0.00016561347001697868 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 1.34; acc: 0.56
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 1.29; acc: 0.56
Batch: 120; loss: 1.12; acc: 0.72
Batch: 140; loss: 1.15; acc: 0.64
Batch: 160; loss: 1.12; acc: 0.73
Batch: 180; loss: 1.25; acc: 0.69
Batch: 200; loss: 1.04; acc: 0.77
Batch: 220; loss: 1.08; acc: 0.69
Batch: 240; loss: 1.25; acc: 0.58
Batch: 260; loss: 1.15; acc: 0.7
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.06; acc: 0.73
Batch: 320; loss: 1.1; acc: 0.67
Batch: 340; loss: 1.32; acc: 0.61
Batch: 360; loss: 1.05; acc: 0.67
Batch: 380; loss: 1.1; acc: 0.67
Batch: 400; loss: 1.22; acc: 0.56
Batch: 420; loss: 1.25; acc: 0.64
Batch: 440; loss: 1.05; acc: 0.78
Batch: 460; loss: 1.28; acc: 0.62
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 1.13; acc: 0.64
Batch: 520; loss: 1.13; acc: 0.69
Batch: 540; loss: 1.02; acc: 0.77
Batch: 560; loss: 1.04; acc: 0.73
Batch: 580; loss: 1.17; acc: 0.72
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.11; acc: 0.7
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.1; acc: 0.72
Batch: 700; loss: 1.14; acc: 0.69
Batch: 720; loss: 1.25; acc: 0.58
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.72
Batch: 780; loss: 1.16; acc: 0.66
Train Epoch over. train_loss: 1.13; train_accuracy: 0.69 

0.00017736163863446563
0.0001701230794424191
Batch: 0; loss: 1.16; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.94; acc: 0.78
Val Epoch over. val_loss: 1.0853946087466684; val_accuracy: 0.7150676751592356 

The current subspace-distance is: 0.0001701230794424191 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 1.14; acc: 0.73
Batch: 40; loss: 1.13; acc: 0.7
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 1.31; acc: 0.66
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 1.07; acc: 0.73
Batch: 140; loss: 1.08; acc: 0.72
Batch: 160; loss: 1.15; acc: 0.61
Batch: 180; loss: 1.03; acc: 0.75
Batch: 200; loss: 1.12; acc: 0.67
Batch: 220; loss: 1.32; acc: 0.55
Batch: 240; loss: 1.22; acc: 0.67
Batch: 260; loss: 1.22; acc: 0.64
Batch: 280; loss: 1.27; acc: 0.56
Batch: 300; loss: 0.95; acc: 0.73
Batch: 320; loss: 1.09; acc: 0.72
Batch: 340; loss: 1.16; acc: 0.69
Batch: 360; loss: 1.04; acc: 0.75
Batch: 380; loss: 1.21; acc: 0.73
Batch: 400; loss: 1.02; acc: 0.69
Batch: 420; loss: 1.21; acc: 0.64
Batch: 440; loss: 1.04; acc: 0.72
Batch: 460; loss: 1.05; acc: 0.69
Batch: 480; loss: 1.12; acc: 0.7
Batch: 500; loss: 1.16; acc: 0.72
Batch: 520; loss: 1.3; acc: 0.67
Batch: 540; loss: 1.17; acc: 0.69
Batch: 560; loss: 1.07; acc: 0.77
Batch: 580; loss: 1.07; acc: 0.75
Batch: 600; loss: 1.23; acc: 0.61
Batch: 620; loss: 0.93; acc: 0.8
Batch: 640; loss: 1.16; acc: 0.7
Batch: 660; loss: 1.07; acc: 0.73
Batch: 680; loss: 1.16; acc: 0.72
Batch: 700; loss: 1.26; acc: 0.58
Batch: 720; loss: 1.07; acc: 0.72
Batch: 740; loss: 1.18; acc: 0.69
Batch: 760; loss: 1.0; acc: 0.77
Batch: 780; loss: 1.19; acc: 0.67
Train Epoch over. train_loss: 1.13; train_accuracy: 0.69 

0.00017465119890403003
0.0001672508369665593
Batch: 0; loss: 1.16; acc: 0.64
Batch: 20; loss: 1.28; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.69
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.0862471476481979; val_accuracy: 0.7136743630573248 

The current subspace-distance is: 0.0001672508369665593 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.95; acc: 0.72
Batch: 20; loss: 1.09; acc: 0.72
Batch: 40; loss: 1.03; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.62
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.06; acc: 0.73
Batch: 140; loss: 1.1; acc: 0.75
Batch: 160; loss: 1.25; acc: 0.67
Batch: 180; loss: 1.03; acc: 0.72
Batch: 200; loss: 1.16; acc: 0.73
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.14; acc: 0.59
Batch: 260; loss: 1.04; acc: 0.72
Batch: 280; loss: 1.2; acc: 0.62
Batch: 300; loss: 1.21; acc: 0.7
Batch: 320; loss: 0.97; acc: 0.77
Batch: 340; loss: 1.1; acc: 0.7
Batch: 360; loss: 1.06; acc: 0.8
Batch: 380; loss: 1.04; acc: 0.72
Batch: 400; loss: 1.1; acc: 0.72
Batch: 420; loss: 1.22; acc: 0.66
Batch: 440; loss: 1.13; acc: 0.66
Batch: 460; loss: 1.24; acc: 0.66
Batch: 480; loss: 1.26; acc: 0.58
Batch: 500; loss: 1.03; acc: 0.72
Batch: 520; loss: 1.13; acc: 0.64
Batch: 540; loss: 1.1; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.7
Batch: 580; loss: 1.29; acc: 0.53
Batch: 600; loss: 1.18; acc: 0.66
Batch: 620; loss: 1.16; acc: 0.7
Batch: 640; loss: 1.12; acc: 0.61
Batch: 660; loss: 1.11; acc: 0.75
Batch: 680; loss: 1.0; acc: 0.75
Batch: 700; loss: 0.97; acc: 0.73
Batch: 720; loss: 1.04; acc: 0.81
Batch: 740; loss: 1.02; acc: 0.75
Batch: 760; loss: 1.13; acc: 0.67
Batch: 780; loss: 0.99; acc: 0.72
Train Epoch over. train_loss: 1.13; train_accuracy: 0.69 

0.00017656887939665467
0.0001661150745349005
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.31; acc: 0.61
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 0.98; acc: 0.8
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.0932057750452855; val_accuracy: 0.71765525477707 

The current subspace-distance is: 0.0001661150745349005 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.78
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 1.0; acc: 0.77
Batch: 60; loss: 1.06; acc: 0.75
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.28; acc: 0.61
Batch: 120; loss: 1.09; acc: 0.67
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.19; acc: 0.62
Batch: 180; loss: 1.11; acc: 0.73
Batch: 200; loss: 1.07; acc: 0.75
Batch: 220; loss: 1.15; acc: 0.7
Batch: 240; loss: 1.13; acc: 0.69
Batch: 260; loss: 1.22; acc: 0.66
Batch: 280; loss: 1.24; acc: 0.66
Batch: 300; loss: 1.05; acc: 0.72
Batch: 320; loss: 1.2; acc: 0.66
Batch: 340; loss: 0.92; acc: 0.83
Batch: 360; loss: 1.04; acc: 0.73
Batch: 380; loss: 1.18; acc: 0.59
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.01; acc: 0.8
Batch: 440; loss: 1.09; acc: 0.7
Batch: 460; loss: 0.99; acc: 0.73
Batch: 480; loss: 1.09; acc: 0.73
Batch: 500; loss: 1.06; acc: 0.7
Batch: 520; loss: 1.14; acc: 0.66
Batch: 540; loss: 1.22; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 1.06; acc: 0.72
Batch: 600; loss: 1.08; acc: 0.77
Batch: 620; loss: 1.08; acc: 0.77
Batch: 640; loss: 1.05; acc: 0.77
Batch: 660; loss: 1.27; acc: 0.61
Batch: 680; loss: 1.1; acc: 0.73
Batch: 700; loss: 1.25; acc: 0.66
Batch: 720; loss: 0.97; acc: 0.73
Batch: 740; loss: 1.11; acc: 0.7
Batch: 760; loss: 1.08; acc: 0.75
Batch: 780; loss: 1.15; acc: 0.72
Train Epoch over. train_loss: 1.12; train_accuracy: 0.69 

0.00017741885676514357
0.00017303098866250366
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.69
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.69
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 1.0824604702603287; val_accuracy: 0.7197452229299363 

The current subspace-distance is: 0.00017303098866250366 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.75
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 1.11; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 1.07; acc: 0.78
Batch: 100; loss: 1.01; acc: 0.72
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.16; acc: 0.72
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.21; acc: 0.64
Batch: 200; loss: 1.13; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.72
Batch: 240; loss: 1.4; acc: 0.64
Batch: 260; loss: 1.07; acc: 0.78
Batch: 280; loss: 1.21; acc: 0.67
Batch: 300; loss: 1.11; acc: 0.72
Batch: 320; loss: 1.14; acc: 0.66
Batch: 340; loss: 1.06; acc: 0.72
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 0.9; acc: 0.8
Batch: 400; loss: 1.18; acc: 0.7
Batch: 420; loss: 1.22; acc: 0.67
Batch: 440; loss: 1.08; acc: 0.7
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 0.97; acc: 0.78
Batch: 500; loss: 1.11; acc: 0.73
Batch: 520; loss: 1.06; acc: 0.7
Batch: 540; loss: 1.11; acc: 0.75
Batch: 560; loss: 1.11; acc: 0.7
Batch: 580; loss: 1.05; acc: 0.72
Batch: 600; loss: 1.24; acc: 0.62
Batch: 620; loss: 1.21; acc: 0.62
Batch: 640; loss: 1.21; acc: 0.66
Batch: 660; loss: 1.13; acc: 0.7
Batch: 680; loss: 1.17; acc: 0.67
Batch: 700; loss: 1.11; acc: 0.73
Batch: 720; loss: 1.05; acc: 0.7
Batch: 740; loss: 1.18; acc: 0.59
Batch: 760; loss: 1.05; acc: 0.75
Batch: 780; loss: 0.95; acc: 0.83
Train Epoch over. train_loss: 1.12; train_accuracy: 0.69 

0.00017915558419190347
0.00017144456796813756
Batch: 0; loss: 1.14; acc: 0.67
Batch: 20; loss: 1.29; acc: 0.64
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.8
Val Epoch over. val_loss: 1.0704854764756124; val_accuracy: 0.7200437898089171 

The current subspace-distance is: 0.00017144456796813756 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.21; acc: 0.64
Batch: 40; loss: 1.24; acc: 0.56
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 1.18; acc: 0.61
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.69
Batch: 140; loss: 1.11; acc: 0.67
Batch: 160; loss: 1.1; acc: 0.62
Batch: 180; loss: 1.14; acc: 0.64
Batch: 200; loss: 1.18; acc: 0.59
Batch: 220; loss: 1.16; acc: 0.66
Batch: 240; loss: 1.0; acc: 0.72
Batch: 260; loss: 1.0; acc: 0.75
Batch: 280; loss: 1.04; acc: 0.77
Batch: 300; loss: 1.09; acc: 0.73
Batch: 320; loss: 1.02; acc: 0.72
Batch: 340; loss: 0.95; acc: 0.77
Batch: 360; loss: 1.18; acc: 0.62
Batch: 380; loss: 1.2; acc: 0.59
Batch: 400; loss: 1.04; acc: 0.73
Batch: 420; loss: 1.03; acc: 0.8
Batch: 440; loss: 1.01; acc: 0.77
Batch: 460; loss: 1.16; acc: 0.64
Batch: 480; loss: 1.17; acc: 0.7
Batch: 500; loss: 1.09; acc: 0.67
Batch: 520; loss: 1.11; acc: 0.77
Batch: 540; loss: 0.96; acc: 0.73
Batch: 560; loss: 1.28; acc: 0.61
Batch: 580; loss: 1.07; acc: 0.75
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.09; acc: 0.72
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.13; acc: 0.67
Batch: 680; loss: 1.13; acc: 0.67
Batch: 700; loss: 1.5; acc: 0.5
Batch: 720; loss: 1.11; acc: 0.73
Batch: 740; loss: 1.21; acc: 0.58
Batch: 760; loss: 1.15; acc: 0.62
Batch: 780; loss: 1.3; acc: 0.61
Train Epoch over. train_loss: 1.12; train_accuracy: 0.69 

0.00018031831132248044
0.00017222799942828715
Batch: 0; loss: 1.14; acc: 0.67
Batch: 20; loss: 1.29; acc: 0.64
Batch: 40; loss: 0.82; acc: 0.86
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 0.98; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 1.0807370330877364; val_accuracy: 0.7166600318471338 

The current subspace-distance is: 0.00017222799942828715 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.21; acc: 0.59
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 1.2; acc: 0.64
Batch: 100; loss: 1.24; acc: 0.69
Batch: 120; loss: 1.04; acc: 0.73
Batch: 140; loss: 1.16; acc: 0.72
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.05; acc: 0.75
Batch: 200; loss: 1.07; acc: 0.64
Batch: 220; loss: 1.06; acc: 0.75
Batch: 240; loss: 1.2; acc: 0.66
Batch: 260; loss: 1.19; acc: 0.62
Batch: 280; loss: 1.26; acc: 0.62
Batch: 300; loss: 1.28; acc: 0.64
Batch: 320; loss: 1.23; acc: 0.67
Batch: 340; loss: 1.04; acc: 0.75
Batch: 360; loss: 1.12; acc: 0.69
Batch: 380; loss: 1.24; acc: 0.66
Batch: 400; loss: 1.06; acc: 0.7
Batch: 420; loss: 1.28; acc: 0.59
Batch: 440; loss: 1.07; acc: 0.72
Batch: 460; loss: 1.14; acc: 0.59
Batch: 480; loss: 1.14; acc: 0.72
Batch: 500; loss: 1.2; acc: 0.69
Batch: 520; loss: 1.15; acc: 0.69
Batch: 540; loss: 1.25; acc: 0.55
Batch: 560; loss: 1.18; acc: 0.67
Batch: 580; loss: 1.35; acc: 0.58
Batch: 600; loss: 1.25; acc: 0.62
Batch: 620; loss: 1.19; acc: 0.62
Batch: 640; loss: 1.13; acc: 0.73
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 1.0; acc: 0.69
Batch: 700; loss: 1.15; acc: 0.64
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 1.14; acc: 0.67
Batch: 760; loss: 1.04; acc: 0.7
Batch: 780; loss: 1.06; acc: 0.75
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00018132792320102453
0.00017215017578564584
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.3; acc: 0.62
Batch: 40; loss: 0.82; acc: 0.88
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.04; acc: 0.7
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 0.95; acc: 0.78
Val Epoch over. val_loss: 1.0766338227660792; val_accuracy: 0.716062898089172 

The current subspace-distance is: 0.00017215017578564584 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.75
Batch: 20; loss: 1.07; acc: 0.7
Batch: 40; loss: 1.25; acc: 0.58
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 1.2; acc: 0.72
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.07; acc: 0.72
Batch: 160; loss: 0.99; acc: 0.73
Batch: 180; loss: 1.12; acc: 0.67
Batch: 200; loss: 0.96; acc: 0.78
Batch: 220; loss: 1.21; acc: 0.62
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.16; acc: 0.69
Batch: 280; loss: 1.21; acc: 0.67
Batch: 300; loss: 1.1; acc: 0.7
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.32; acc: 0.59
Batch: 360; loss: 0.87; acc: 0.77
Batch: 380; loss: 1.08; acc: 0.7
Batch: 400; loss: 0.89; acc: 0.86
Batch: 420; loss: 1.18; acc: 0.69
Batch: 440; loss: 1.03; acc: 0.7
Batch: 460; loss: 1.25; acc: 0.64
Batch: 480; loss: 0.96; acc: 0.81
Batch: 500; loss: 1.18; acc: 0.7
Batch: 520; loss: 1.13; acc: 0.62
Batch: 540; loss: 1.09; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.69
Batch: 580; loss: 1.16; acc: 0.62
Batch: 600; loss: 1.19; acc: 0.62
Batch: 620; loss: 1.16; acc: 0.69
Batch: 640; loss: 1.18; acc: 0.67
Batch: 660; loss: 0.99; acc: 0.75
Batch: 680; loss: 0.98; acc: 0.73
Batch: 700; loss: 1.11; acc: 0.7
Batch: 720; loss: 1.22; acc: 0.59
Batch: 740; loss: 1.01; acc: 0.73
Batch: 760; loss: 1.01; acc: 0.75
Batch: 780; loss: 1.0; acc: 0.81
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.0001803602062864229
0.00017351390852127224
Batch: 0; loss: 1.13; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.0684176205070155; val_accuracy: 0.7222332802547771 

The current subspace-distance is: 0.00017351390852127224 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.72
Batch: 20; loss: 1.06; acc: 0.77
Batch: 40; loss: 1.2; acc: 0.61
Batch: 60; loss: 0.91; acc: 0.84
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 1.08; acc: 0.77
Batch: 140; loss: 1.02; acc: 0.72
Batch: 160; loss: 1.19; acc: 0.7
Batch: 180; loss: 0.93; acc: 0.8
Batch: 200; loss: 1.14; acc: 0.64
Batch: 220; loss: 1.1; acc: 0.72
Batch: 240; loss: 1.2; acc: 0.67
Batch: 260; loss: 1.05; acc: 0.69
Batch: 280; loss: 1.05; acc: 0.8
Batch: 300; loss: 1.12; acc: 0.7
Batch: 320; loss: 0.91; acc: 0.83
Batch: 340; loss: 1.15; acc: 0.62
Batch: 360; loss: 1.22; acc: 0.67
Batch: 380; loss: 0.98; acc: 0.77
Batch: 400; loss: 1.26; acc: 0.66
Batch: 420; loss: 0.88; acc: 0.83
Batch: 440; loss: 1.25; acc: 0.67
Batch: 460; loss: 1.08; acc: 0.69
Batch: 480; loss: 1.1; acc: 0.69
Batch: 500; loss: 1.08; acc: 0.72
Batch: 520; loss: 0.98; acc: 0.77
Batch: 540; loss: 1.06; acc: 0.73
Batch: 560; loss: 1.09; acc: 0.73
Batch: 580; loss: 1.02; acc: 0.8
Batch: 600; loss: 1.13; acc: 0.62
Batch: 620; loss: 1.19; acc: 0.73
Batch: 640; loss: 1.26; acc: 0.62
Batch: 660; loss: 1.06; acc: 0.73
Batch: 680; loss: 1.09; acc: 0.67
Batch: 700; loss: 1.06; acc: 0.72
Batch: 720; loss: 1.34; acc: 0.55
Batch: 740; loss: 1.14; acc: 0.67
Batch: 760; loss: 1.01; acc: 0.73
Batch: 780; loss: 1.03; acc: 0.78
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.0001799084566300735
0.00017333697178401053
Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.64
Batch: 40; loss: 0.82; acc: 0.86
Batch: 60; loss: 1.09; acc: 0.67
Batch: 80; loss: 0.98; acc: 0.75
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.0717367905720023; val_accuracy: 0.7259156050955414 

The current subspace-distance is: 0.00017333697178401053 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 1.33; acc: 0.56
Batch: 60; loss: 1.02; acc: 0.73
Batch: 80; loss: 1.14; acc: 0.62
Batch: 100; loss: 1.12; acc: 0.64
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 1.08; acc: 0.69
Batch: 160; loss: 1.23; acc: 0.66
Batch: 180; loss: 1.03; acc: 0.7
Batch: 200; loss: 1.16; acc: 0.67
Batch: 220; loss: 0.92; acc: 0.75
Batch: 240; loss: 1.18; acc: 0.67
Batch: 260; loss: 1.1; acc: 0.69
Batch: 280; loss: 1.09; acc: 0.73
Batch: 300; loss: 1.03; acc: 0.72
Batch: 320; loss: 1.18; acc: 0.61
Batch: 340; loss: 1.42; acc: 0.58
Batch: 360; loss: 1.05; acc: 0.77
Batch: 380; loss: 0.96; acc: 0.72
Batch: 400; loss: 1.15; acc: 0.66
Batch: 420; loss: 1.0; acc: 0.77
Batch: 440; loss: 1.06; acc: 0.72
Batch: 460; loss: 1.17; acc: 0.7
Batch: 480; loss: 1.0; acc: 0.78
Batch: 500; loss: 1.17; acc: 0.73
Batch: 520; loss: 1.1; acc: 0.7
Batch: 540; loss: 0.98; acc: 0.77
Batch: 560; loss: 1.04; acc: 0.72
Batch: 580; loss: 1.12; acc: 0.69
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 1.16; acc: 0.67
Batch: 640; loss: 1.19; acc: 0.59
Batch: 660; loss: 1.17; acc: 0.64
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.18; acc: 0.64
Batch: 720; loss: 1.17; acc: 0.66
Batch: 740; loss: 0.96; acc: 0.78
Batch: 760; loss: 1.15; acc: 0.66
Batch: 780; loss: 1.15; acc: 0.69
Train Epoch over. train_loss: 1.1; train_accuracy: 0.7 

0.00018090700905304402
0.00017137272516265512
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.28; acc: 0.64
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.75
Batch: 100; loss: 1.03; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.0616981873087064; val_accuracy: 0.720640923566879 

The current subspace-distance is: 0.00017137272516265512 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 1.13; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.04; acc: 0.7
Batch: 120; loss: 1.07; acc: 0.7
Batch: 140; loss: 1.22; acc: 0.58
Batch: 160; loss: 1.19; acc: 0.66
Batch: 180; loss: 0.97; acc: 0.78
Batch: 200; loss: 1.09; acc: 0.69
Batch: 220; loss: 1.07; acc: 0.81
Batch: 240; loss: 1.06; acc: 0.7
Batch: 260; loss: 1.14; acc: 0.72
Batch: 280; loss: 0.96; acc: 0.77
Batch: 300; loss: 1.02; acc: 0.69
Batch: 320; loss: 1.22; acc: 0.58
Batch: 340; loss: 1.2; acc: 0.67
Batch: 360; loss: 1.25; acc: 0.58
Batch: 380; loss: 1.2; acc: 0.62
Batch: 400; loss: 1.12; acc: 0.69
Batch: 420; loss: 0.96; acc: 0.72
Batch: 440; loss: 1.12; acc: 0.75
Batch: 460; loss: 1.33; acc: 0.56
Batch: 480; loss: 1.08; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.62
Batch: 520; loss: 0.99; acc: 0.8
Batch: 540; loss: 1.06; acc: 0.78
Batch: 560; loss: 1.11; acc: 0.7
Batch: 580; loss: 1.06; acc: 0.8
Batch: 600; loss: 1.12; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.73
Batch: 640; loss: 1.02; acc: 0.7
Batch: 660; loss: 1.05; acc: 0.69
Batch: 680; loss: 1.07; acc: 0.78
Batch: 700; loss: 1.03; acc: 0.72
Batch: 720; loss: 1.12; acc: 0.69
Batch: 740; loss: 1.24; acc: 0.64
Batch: 760; loss: 1.12; acc: 0.75
Batch: 780; loss: 1.04; acc: 0.69
Train Epoch over. train_loss: 1.1; train_accuracy: 0.7 

0.00018350458412896842
0.00017574249068275094
Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.02; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.66
Batch: 140; loss: 0.92; acc: 0.8
Val Epoch over. val_loss: 1.0619635612342009; val_accuracy: 0.7216361464968153 

The current subspace-distance is: 0.00017574249068275094 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_15_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.5620723414746194

The number of parameters is: 263271

The number of individual parameters is:

13
234
13
13
19
37791
19
19
38
110466
38
38
64
109440
64
64
4096
64
640
10
64
64

nonzero elements in E: 52654196
elements in E: 52654200
fraction nonzero: 0.9999999240326508
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.11
Batch: 20; loss: 2.13; acc: 0.28
Batch: 40; loss: 2.06; acc: 0.23
Batch: 60; loss: 1.94; acc: 0.38
Batch: 80; loss: 1.94; acc: 0.36
Batch: 100; loss: 1.9; acc: 0.45
Batch: 120; loss: 1.76; acc: 0.5
Batch: 140; loss: 1.84; acc: 0.42
Batch: 160; loss: 1.8; acc: 0.55
Batch: 180; loss: 1.74; acc: 0.58
Batch: 200; loss: 1.68; acc: 0.53
Batch: 220; loss: 1.76; acc: 0.55
Batch: 240; loss: 1.69; acc: 0.55
Batch: 260; loss: 1.63; acc: 0.59
Batch: 280; loss: 1.66; acc: 0.59
Batch: 300; loss: 1.49; acc: 0.72
Batch: 320; loss: 1.65; acc: 0.58
Batch: 340; loss: 1.49; acc: 0.69
Batch: 360; loss: 1.5; acc: 0.73
Batch: 380; loss: 1.42; acc: 0.72
Batch: 400; loss: 1.43; acc: 0.7
Batch: 420; loss: 1.54; acc: 0.59
Batch: 440; loss: 1.47; acc: 0.67
Batch: 460; loss: 1.54; acc: 0.59
Batch: 480; loss: 1.38; acc: 0.78
Batch: 500; loss: 1.42; acc: 0.61
Batch: 520; loss: 1.36; acc: 0.67
Batch: 540; loss: 1.37; acc: 0.67
Batch: 560; loss: 1.5; acc: 0.67
Batch: 580; loss: 1.37; acc: 0.73
Batch: 600; loss: 1.41; acc: 0.67
Batch: 620; loss: 1.38; acc: 0.72
Batch: 640; loss: 1.3; acc: 0.75
Batch: 660; loss: 1.28; acc: 0.73
Batch: 680; loss: 1.41; acc: 0.73
Batch: 700; loss: 1.32; acc: 0.66
Batch: 720; loss: 1.31; acc: 0.7
Batch: 740; loss: 1.43; acc: 0.58
Batch: 760; loss: 1.22; acc: 0.8
Batch: 780; loss: 1.33; acc: 0.69
Train Epoch over. train_loss: 1.57; train_accuracy: 0.61 

5.912065535085276e-05
5.4480991821037605e-05
Batch: 0; loss: 1.31; acc: 0.81
Batch: 20; loss: 1.44; acc: 0.61
Batch: 40; loss: 1.01; acc: 0.89
Batch: 60; loss: 1.17; acc: 0.77
Batch: 80; loss: 1.12; acc: 0.8
Batch: 100; loss: 1.24; acc: 0.86
Batch: 120; loss: 1.39; acc: 0.7
Batch: 140; loss: 1.16; acc: 0.77
Val Epoch over. val_loss: 1.2601974344557259; val_accuracy: 0.7535828025477707 

The current subspace-distance is: 5.4480991821037605e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.8
Batch: 20; loss: 1.27; acc: 0.7
Batch: 40; loss: 1.21; acc: 0.75
Batch: 60; loss: 1.22; acc: 0.75
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.22; acc: 0.75
Batch: 120; loss: 1.31; acc: 0.7
Batch: 140; loss: 1.19; acc: 0.83
Batch: 160; loss: 1.29; acc: 0.73
Batch: 180; loss: 1.22; acc: 0.73
Batch: 200; loss: 1.21; acc: 0.72
Batch: 220; loss: 1.22; acc: 0.78
Batch: 240; loss: 1.28; acc: 0.73
Batch: 260; loss: 1.33; acc: 0.7
Batch: 280; loss: 1.2; acc: 0.69
Batch: 300; loss: 1.26; acc: 0.69
Batch: 320; loss: 1.45; acc: 0.56
Batch: 340; loss: 1.21; acc: 0.73
Batch: 360; loss: 1.32; acc: 0.67
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.09; acc: 0.86
Batch: 440; loss: 1.23; acc: 0.7
Batch: 460; loss: 1.22; acc: 0.75
Batch: 480; loss: 1.22; acc: 0.72
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 1.2; acc: 0.7
Batch: 540; loss: 1.16; acc: 0.8
Batch: 560; loss: 1.35; acc: 0.66
Batch: 580; loss: 1.09; acc: 0.75
Batch: 600; loss: 1.16; acc: 0.73
Batch: 620; loss: 1.27; acc: 0.64
Batch: 640; loss: 1.13; acc: 0.77
Batch: 660; loss: 1.2; acc: 0.73
Batch: 680; loss: 1.08; acc: 0.8
Batch: 700; loss: 1.09; acc: 0.77
Batch: 720; loss: 1.19; acc: 0.73
Batch: 740; loss: 1.26; acc: 0.67
Batch: 760; loss: 1.13; acc: 0.78
Batch: 780; loss: 1.17; acc: 0.73
Train Epoch over. train_loss: 1.23; train_accuracy: 0.73 

8.112403884297237e-05
7.61155752115883e-05
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 0.8; acc: 0.94
Batch: 60; loss: 1.05; acc: 0.78
Batch: 80; loss: 0.91; acc: 0.86
Batch: 100; loss: 1.06; acc: 0.81
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.78
Val Epoch over. val_loss: 1.0913320145789225; val_accuracy: 0.7719944267515924 

The current subspace-distance is: 7.61155752115883e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.75
Batch: 40; loss: 1.08; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.78
Batch: 80; loss: 1.2; acc: 0.67
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.17; acc: 0.77
Batch: 140; loss: 1.25; acc: 0.75
Batch: 160; loss: 1.22; acc: 0.7
Batch: 180; loss: 1.17; acc: 0.75
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 1.06; acc: 0.77
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.03; acc: 0.78
Batch: 280; loss: 1.01; acc: 0.8
Batch: 300; loss: 1.13; acc: 0.78
Batch: 320; loss: 1.08; acc: 0.75
Batch: 340; loss: 1.13; acc: 0.75
Batch: 360; loss: 1.12; acc: 0.78
Batch: 380; loss: 1.21; acc: 0.73
Batch: 400; loss: 1.11; acc: 0.75
Batch: 420; loss: 0.96; acc: 0.84
Batch: 440; loss: 1.28; acc: 0.59
Batch: 460; loss: 1.11; acc: 0.72
Batch: 480; loss: 1.2; acc: 0.67
Batch: 500; loss: 1.14; acc: 0.77
Batch: 520; loss: 1.08; acc: 0.77
Batch: 540; loss: 1.1; acc: 0.75
Batch: 560; loss: 1.13; acc: 0.77
Batch: 580; loss: 1.13; acc: 0.75
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 1.04; acc: 0.72
Batch: 640; loss: 0.95; acc: 0.81
Batch: 660; loss: 1.06; acc: 0.78
Batch: 680; loss: 0.99; acc: 0.78
Batch: 700; loss: 1.22; acc: 0.72
Batch: 720; loss: 1.03; acc: 0.78
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.72
Batch: 780; loss: 1.09; acc: 0.78
Train Epoch over. train_loss: 1.1; train_accuracy: 0.75 

9.849586058408022e-05
9.486406634096056e-05
Batch: 0; loss: 1.02; acc: 0.78
Batch: 20; loss: 1.15; acc: 0.75
Batch: 40; loss: 0.71; acc: 0.92
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.81; acc: 0.89
Batch: 100; loss: 0.99; acc: 0.83
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.8
Val Epoch over. val_loss: 0.9903360757098836; val_accuracy: 0.7891122611464968 

The current subspace-distance is: 9.486406634096056e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.78
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.07; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.18; acc: 0.69
Batch: 140; loss: 1.03; acc: 0.75
Batch: 160; loss: 1.0; acc: 0.8
Batch: 180; loss: 0.98; acc: 0.78
Batch: 200; loss: 1.04; acc: 0.7
Batch: 220; loss: 1.06; acc: 0.8
Batch: 240; loss: 1.01; acc: 0.78
Batch: 260; loss: 1.06; acc: 0.75
Batch: 280; loss: 1.17; acc: 0.66
Batch: 300; loss: 1.02; acc: 0.77
Batch: 320; loss: 1.09; acc: 0.75
Batch: 340; loss: 0.91; acc: 0.8
Batch: 360; loss: 1.09; acc: 0.73
Batch: 380; loss: 0.99; acc: 0.8
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 0.9; acc: 0.8
Batch: 440; loss: 0.88; acc: 0.86
Batch: 460; loss: 1.05; acc: 0.72
Batch: 480; loss: 1.05; acc: 0.8
Batch: 500; loss: 1.08; acc: 0.73
Batch: 520; loss: 0.98; acc: 0.75
Batch: 540; loss: 0.96; acc: 0.83
Batch: 560; loss: 1.03; acc: 0.77
Batch: 580; loss: 0.91; acc: 0.89
Batch: 600; loss: 0.91; acc: 0.83
Batch: 620; loss: 1.08; acc: 0.67
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 1.0; acc: 0.77
Batch: 680; loss: 0.97; acc: 0.75
Batch: 700; loss: 1.06; acc: 0.72
Batch: 720; loss: 1.11; acc: 0.69
Batch: 740; loss: 1.27; acc: 0.59
Batch: 760; loss: 1.12; acc: 0.62
Batch: 780; loss: 1.0; acc: 0.73
Train Epoch over. train_loss: 1.02; train_accuracy: 0.77 

0.00011381693184375763
0.00010827052756212652
Batch: 0; loss: 0.94; acc: 0.8
Batch: 20; loss: 1.09; acc: 0.72
Batch: 40; loss: 0.66; acc: 0.97
Batch: 60; loss: 0.88; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.91
Batch: 100; loss: 0.94; acc: 0.84
Batch: 120; loss: 1.09; acc: 0.67
Batch: 140; loss: 0.75; acc: 0.83
Val Epoch over. val_loss: 0.9273322725751597; val_accuracy: 0.8020501592356688 

The current subspace-distance is: 0.00010827052756212652 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.77
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.96; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.84
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 1.01; acc: 0.8
Batch: 140; loss: 1.03; acc: 0.72
Batch: 160; loss: 0.95; acc: 0.81
Batch: 180; loss: 0.89; acc: 0.81
Batch: 200; loss: 1.01; acc: 0.73
Batch: 220; loss: 1.06; acc: 0.77
Batch: 240; loss: 1.05; acc: 0.75
Batch: 260; loss: 1.03; acc: 0.75
Batch: 280; loss: 0.83; acc: 0.8
Batch: 300; loss: 0.85; acc: 0.91
Batch: 320; loss: 0.88; acc: 0.8
Batch: 340; loss: 1.07; acc: 0.73
Batch: 360; loss: 0.85; acc: 0.81
Batch: 380; loss: 1.03; acc: 0.69
Batch: 400; loss: 0.92; acc: 0.77
Batch: 420; loss: 0.91; acc: 0.84
Batch: 440; loss: 1.05; acc: 0.72
Batch: 460; loss: 0.99; acc: 0.8
Batch: 480; loss: 0.95; acc: 0.81
Batch: 500; loss: 0.88; acc: 0.77
Batch: 520; loss: 0.9; acc: 0.86
Batch: 540; loss: 0.83; acc: 0.84
Batch: 560; loss: 0.85; acc: 0.88
Batch: 580; loss: 1.01; acc: 0.72
Batch: 600; loss: 0.97; acc: 0.77
Batch: 620; loss: 0.88; acc: 0.81
Batch: 640; loss: 0.97; acc: 0.78
Batch: 660; loss: 1.02; acc: 0.7
Batch: 680; loss: 0.82; acc: 0.84
Batch: 700; loss: 0.86; acc: 0.84
Batch: 720; loss: 1.06; acc: 0.67
Batch: 740; loss: 0.9; acc: 0.81
Batch: 760; loss: 1.02; acc: 0.72
Batch: 780; loss: 1.0; acc: 0.77
Train Epoch over. train_loss: 0.96; train_accuracy: 0.78 

0.0001256372925126925
0.00012140585022279993
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.62; acc: 0.97
Batch: 60; loss: 0.81; acc: 0.86
Batch: 80; loss: 0.7; acc: 0.91
Batch: 100; loss: 0.9; acc: 0.84
Batch: 120; loss: 1.06; acc: 0.67
Batch: 140; loss: 0.71; acc: 0.86
Val Epoch over. val_loss: 0.878609519855232; val_accuracy: 0.8103105095541401 

The current subspace-distance is: 0.00012140585022279993 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.75
Batch: 20; loss: 0.91; acc: 0.81
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 0.9; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.84
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.95; acc: 0.77
Batch: 160; loss: 0.83; acc: 0.89
Batch: 180; loss: 1.01; acc: 0.73
Batch: 200; loss: 0.99; acc: 0.77
Batch: 220; loss: 0.76; acc: 0.83
Batch: 240; loss: 0.93; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.84
Batch: 280; loss: 0.93; acc: 0.8
Batch: 300; loss: 1.0; acc: 0.75
Batch: 320; loss: 1.02; acc: 0.73
Batch: 340; loss: 0.95; acc: 0.78
Batch: 360; loss: 0.83; acc: 0.84
Batch: 380; loss: 0.95; acc: 0.75
Batch: 400; loss: 0.89; acc: 0.81
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 1.07; acc: 0.72
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 0.93; acc: 0.78
Batch: 500; loss: 0.74; acc: 0.91
Batch: 520; loss: 0.86; acc: 0.84
Batch: 540; loss: 0.89; acc: 0.84
Batch: 560; loss: 0.98; acc: 0.72
Batch: 580; loss: 0.87; acc: 0.83
Batch: 600; loss: 1.03; acc: 0.78
Batch: 620; loss: 1.08; acc: 0.77
Batch: 640; loss: 0.94; acc: 0.77
Batch: 660; loss: 1.1; acc: 0.73
Batch: 680; loss: 1.05; acc: 0.72
Batch: 700; loss: 0.86; acc: 0.84
Batch: 720; loss: 0.96; acc: 0.86
Batch: 740; loss: 0.96; acc: 0.8
Batch: 760; loss: 0.87; acc: 0.77
Batch: 780; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.92; train_accuracy: 0.79 

0.00013632325863000005
0.00013129212311469018
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 1.03; acc: 0.67
Batch: 40; loss: 0.58; acc: 0.95
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.91
Batch: 100; loss: 0.85; acc: 0.84
Batch: 120; loss: 1.05; acc: 0.66
Batch: 140; loss: 0.68; acc: 0.84
Val Epoch over. val_loss: 0.8443896804645563; val_accuracy: 0.8092157643312102 

The current subspace-distance is: 0.00013129212311469018 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.88; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.72
Batch: 60; loss: 0.87; acc: 0.8
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.88
Batch: 140; loss: 0.95; acc: 0.77
Batch: 160; loss: 0.88; acc: 0.78
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.84; acc: 0.84
Batch: 220; loss: 0.96; acc: 0.75
Batch: 240; loss: 0.82; acc: 0.81
Batch: 260; loss: 0.92; acc: 0.81
Batch: 280; loss: 0.99; acc: 0.78
Batch: 300; loss: 0.82; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.77
Batch: 340; loss: 0.78; acc: 0.89
Batch: 360; loss: 0.98; acc: 0.81
Batch: 380; loss: 0.82; acc: 0.78
Batch: 400; loss: 0.88; acc: 0.77
Batch: 420; loss: 1.0; acc: 0.75
Batch: 440; loss: 0.8; acc: 0.81
Batch: 460; loss: 0.81; acc: 0.78
Batch: 480; loss: 0.84; acc: 0.81
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.86; acc: 0.8
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 0.78; acc: 0.83
Batch: 580; loss: 0.85; acc: 0.78
Batch: 600; loss: 0.81; acc: 0.81
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 0.96; acc: 0.78
Batch: 660; loss: 0.77; acc: 0.88
Batch: 680; loss: 0.88; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.89; acc: 0.75
Batch: 740; loss: 0.96; acc: 0.7
Batch: 760; loss: 0.84; acc: 0.8
Batch: 780; loss: 0.83; acc: 0.81
Train Epoch over. train_loss: 0.87; train_accuracy: 0.79 

0.00014713073323946446
0.00014065987488720566
Batch: 0; loss: 0.77; acc: 0.83
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.53; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.75; acc: 0.88
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.62; acc: 0.86
Val Epoch over. val_loss: 0.7801084685477482; val_accuracy: 0.8245421974522293 

The current subspace-distance is: 0.00014065987488720566 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.83
Batch: 40; loss: 0.92; acc: 0.81
Batch: 60; loss: 0.76; acc: 0.88
Batch: 80; loss: 0.85; acc: 0.78
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.82; acc: 0.86
Batch: 140; loss: 0.76; acc: 0.84
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 0.82; acc: 0.8
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.85; acc: 0.78
Batch: 240; loss: 0.86; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.88; acc: 0.8
Batch: 320; loss: 0.81; acc: 0.81
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.9; acc: 0.75
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.83
Batch: 440; loss: 0.75; acc: 0.84
Batch: 460; loss: 0.84; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.88
Batch: 500; loss: 0.83; acc: 0.83
Batch: 520; loss: 0.92; acc: 0.73
Batch: 540; loss: 0.77; acc: 0.84
Batch: 560; loss: 0.85; acc: 0.83
Batch: 580; loss: 0.94; acc: 0.75
Batch: 600; loss: 0.82; acc: 0.78
Batch: 620; loss: 1.01; acc: 0.7
Batch: 640; loss: 0.82; acc: 0.81
Batch: 660; loss: 0.83; acc: 0.8
Batch: 680; loss: 0.84; acc: 0.77
Batch: 700; loss: 0.93; acc: 0.73
Batch: 720; loss: 0.85; acc: 0.81
Batch: 740; loss: 0.76; acc: 0.86
Batch: 760; loss: 0.79; acc: 0.83
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

0.00015257764607667923
0.0001479435304645449
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.5; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.88
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.92
Val Epoch over. val_loss: 0.7582983344223848; val_accuracy: 0.8294187898089171 

The current subspace-distance is: 0.0001479435304645449 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.88
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.92; acc: 0.69
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.75; acc: 0.84
Batch: 180; loss: 0.98; acc: 0.72
Batch: 200; loss: 0.75; acc: 0.83
Batch: 220; loss: 0.81; acc: 0.77
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.86; acc: 0.83
Batch: 280; loss: 0.78; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.77
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.87; acc: 0.75
Batch: 360; loss: 0.75; acc: 0.77
Batch: 380; loss: 0.82; acc: 0.83
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.78
Batch: 440; loss: 0.61; acc: 0.95
Batch: 460; loss: 0.94; acc: 0.72
Batch: 480; loss: 0.68; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.91
Batch: 540; loss: 0.84; acc: 0.78
Batch: 560; loss: 0.89; acc: 0.75
Batch: 580; loss: 0.74; acc: 0.88
Batch: 600; loss: 0.98; acc: 0.72
Batch: 620; loss: 0.91; acc: 0.7
Batch: 640; loss: 0.8; acc: 0.86
Batch: 660; loss: 0.82; acc: 0.81
Batch: 680; loss: 0.79; acc: 0.83
Batch: 700; loss: 0.86; acc: 0.77
Batch: 720; loss: 0.9; acc: 0.75
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.81; acc: 0.78
Train Epoch over. train_loss: 0.8; train_accuracy: 0.81 

0.0001639346737647429
0.00015827020979486406
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.46; acc: 0.97
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.57; acc: 0.91
Val Epoch over. val_loss: 0.7163224586635638; val_accuracy: 0.8417595541401274 

The current subspace-distance is: 0.00015827020979486406 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.75
Batch: 20; loss: 0.89; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.84; acc: 0.77
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.72; acc: 0.89
Batch: 180; loss: 0.82; acc: 0.86
Batch: 200; loss: 0.82; acc: 0.77
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.8; acc: 0.78
Batch: 300; loss: 0.83; acc: 0.84
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.78; acc: 0.81
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.9; acc: 0.75
Batch: 400; loss: 0.89; acc: 0.75
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.85; acc: 0.73
Batch: 460; loss: 0.9; acc: 0.72
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.87; acc: 0.78
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 0.79; acc: 0.86
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.86
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.78
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.9; acc: 0.77
Batch: 720; loss: 0.86; acc: 0.75
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00017399377247784287
0.0001678098924458027
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.95
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6920107523347162; val_accuracy: 0.8451433121019108 

The current subspace-distance is: 0.0001678098924458027 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.92
Batch: 40; loss: 0.64; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.91
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.91; acc: 0.77
Batch: 160; loss: 0.88; acc: 0.75
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.71; acc: 0.84
Batch: 220; loss: 0.94; acc: 0.75
Batch: 240; loss: 0.76; acc: 0.83
Batch: 260; loss: 0.73; acc: 0.89
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.8; acc: 0.8
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.85; acc: 0.78
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.86
Batch: 460; loss: 0.89; acc: 0.78
Batch: 480; loss: 0.68; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.89
Batch: 520; loss: 0.85; acc: 0.77
Batch: 540; loss: 0.66; acc: 0.92
Batch: 560; loss: 0.84; acc: 0.75
Batch: 580; loss: 0.8; acc: 0.81
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.76; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.88
Batch: 680; loss: 0.71; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.92
Batch: 720; loss: 0.86; acc: 0.73
Batch: 740; loss: 0.8; acc: 0.8
Batch: 760; loss: 0.72; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00017509078315924853
0.00016743724700063467
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.54; acc: 0.91
Val Epoch over. val_loss: 0.6903207243248156; val_accuracy: 0.8431528662420382 

The current subspace-distance is: 0.00016743724700063467 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.71; acc: 0.88
Batch: 60; loss: 0.95; acc: 0.75
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.86
Batch: 160; loss: 0.75; acc: 0.84
Batch: 180; loss: 0.89; acc: 0.73
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 0.68; acc: 0.86
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 0.84; acc: 0.77
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.73; acc: 0.81
Batch: 340; loss: 0.88; acc: 0.67
Batch: 360; loss: 0.73; acc: 0.83
Batch: 380; loss: 0.63; acc: 0.91
Batch: 400; loss: 0.76; acc: 0.75
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.8
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.78; acc: 0.81
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.86
Batch: 620; loss: 0.63; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.76; acc: 0.81
Batch: 700; loss: 0.78; acc: 0.77
Batch: 720; loss: 0.71; acc: 0.84
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.0001733810786390677
0.00016791658708825707
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.91
Val Epoch over. val_loss: 0.6885551383161241; val_accuracy: 0.8428542993630573 

The current subspace-distance is: 0.00016791658708825707 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.65; acc: 0.8
Batch: 160; loss: 0.86; acc: 0.78
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.84; acc: 0.77
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.64; acc: 0.88
Batch: 320; loss: 0.82; acc: 0.77
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.84; acc: 0.81
Batch: 380; loss: 0.83; acc: 0.81
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.79; acc: 0.78
Batch: 460; loss: 1.01; acc: 0.66
Batch: 480; loss: 0.72; acc: 0.86
Batch: 500; loss: 0.86; acc: 0.8
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.91; acc: 0.75
Batch: 580; loss: 0.67; acc: 0.89
Batch: 600; loss: 0.76; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.74; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.91
Batch: 780; loss: 0.84; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.0001791152317309752
0.00017187184130307287
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.92; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.97
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6812746177433403; val_accuracy: 0.8441480891719745 

The current subspace-distance is: 0.00017187184130307287 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 0.7; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.87; acc: 0.73
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.75; acc: 0.78
Batch: 240; loss: 0.54; acc: 0.91
Batch: 260; loss: 0.73; acc: 0.8
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.81
Batch: 320; loss: 0.73; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.77
Batch: 380; loss: 0.58; acc: 0.91
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.84; acc: 0.78
Batch: 460; loss: 0.75; acc: 0.83
Batch: 480; loss: 0.78; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.58; acc: 0.91
Batch: 560; loss: 0.81; acc: 0.8
Batch: 580; loss: 0.82; acc: 0.75
Batch: 600; loss: 0.77; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.81
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.65; acc: 0.89
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.55; acc: 0.89
Batch: 720; loss: 0.79; acc: 0.81
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.72; acc: 0.83
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.00018250277207698673
0.00017362131620757282
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.91
Val Epoch over. val_loss: 0.6595530570692317; val_accuracy: 0.8497213375796179 

The current subspace-distance is: 0.00017362131620757282 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.74; acc: 0.86
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.77
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.88
Batch: 200; loss: 0.76; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.75
Batch: 280; loss: 0.8; acc: 0.83
Batch: 300; loss: 0.78; acc: 0.78
Batch: 320; loss: 0.97; acc: 0.7
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.62; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.91
Batch: 420; loss: 0.74; acc: 0.84
Batch: 440; loss: 0.85; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.81
Batch: 480; loss: 0.73; acc: 0.78
Batch: 500; loss: 0.86; acc: 0.73
Batch: 520; loss: 0.86; acc: 0.73
Batch: 540; loss: 0.77; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.63; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.75
Batch: 660; loss: 0.84; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.88
Batch: 700; loss: 0.8; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.88
Batch: 740; loss: 0.79; acc: 0.8
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.0001838325260905549
0.00017608801135793328
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6562546404304018; val_accuracy: 0.8508160828025477 

The current subspace-distance is: 0.00017608801135793328 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.84; acc: 0.8
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.83; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.66; acc: 0.81
Batch: 200; loss: 0.69; acc: 0.78
Batch: 220; loss: 0.71; acc: 0.88
Batch: 240; loss: 0.67; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.77; acc: 0.83
Batch: 320; loss: 0.77; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.88
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.76; acc: 0.81
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.79; acc: 0.77
Batch: 580; loss: 0.87; acc: 0.78
Batch: 600; loss: 0.71; acc: 0.86
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 0.6; acc: 0.89
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.79; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.97; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00018550126696936786
0.00017838829080574214
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6551161379950821; val_accuracy: 0.847531847133758 

The current subspace-distance is: 0.00017838829080574214 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.73; acc: 0.81
Batch: 160; loss: 0.59; acc: 0.94
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.82; acc: 0.78
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.81; acc: 0.77
Batch: 320; loss: 0.79; acc: 0.77
Batch: 340; loss: 0.86; acc: 0.75
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.74; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.8; acc: 0.7
Batch: 460; loss: 0.87; acc: 0.72
Batch: 480; loss: 0.7; acc: 0.86
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.65; acc: 0.92
Batch: 540; loss: 0.95; acc: 0.72
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.8; acc: 0.75
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.79; acc: 0.73
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.88
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.76; acc: 0.84
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00018704717513173819
0.00018229008128400892
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6556612110821305; val_accuracy: 0.8490246815286624 

The current subspace-distance is: 0.00018229008128400892 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.76; acc: 0.8
Batch: 160; loss: 0.88; acc: 0.78
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.85; acc: 0.77
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.75; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.85; acc: 0.8
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.94; acc: 0.75
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.73; acc: 0.81
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.84
Batch: 620; loss: 0.77; acc: 0.73
Batch: 640; loss: 0.75; acc: 0.83
Batch: 660; loss: 0.65; acc: 0.84
Batch: 680; loss: 0.89; acc: 0.75
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.81; acc: 0.78
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00018933851970359683
0.00018141245527658612
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.49; acc: 0.92
Val Epoch over. val_loss: 0.641351078156453; val_accuracy: 0.8518113057324841 

The current subspace-distance is: 0.00018141245527658612 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 0.71; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.74; acc: 0.83
Batch: 160; loss: 0.86; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.89
Batch: 240; loss: 0.8; acc: 0.8
Batch: 260; loss: 0.61; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.92
Batch: 300; loss: 0.79; acc: 0.78
Batch: 320; loss: 0.9; acc: 0.73
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.78; acc: 0.84
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.8; acc: 0.8
Batch: 460; loss: 0.7; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.89; acc: 0.72
Batch: 580; loss: 0.72; acc: 0.89
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.89; acc: 0.75
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.77; acc: 0.78
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00019493978470563889
0.00018522255413699895
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.49; acc: 0.91
Val Epoch over. val_loss: 0.6444470018717894; val_accuracy: 0.8486265923566879 

The current subspace-distance is: 0.00018522255413699895 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.78
Batch: 200; loss: 0.82; acc: 0.78
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.83; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.75; acc: 0.8
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 0.75; acc: 0.8
Batch: 520; loss: 0.62; acc: 0.91
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.78
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.77
Batch: 680; loss: 0.75; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00019549584249034524
0.00019002999761141837
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.6300483412423711; val_accuracy: 0.8505175159235668 

The current subspace-distance is: 0.00019002999761141837 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.75; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.81
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.85; acc: 0.8
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.63; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.79; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 0.72; acc: 0.83
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.85; acc: 0.81
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.73; acc: 0.77
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.77; acc: 0.78
Batch: 780; loss: 0.68; acc: 0.8
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00019510644779074937
0.00018748770526144654
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6281694872364117; val_accuracy: 0.8519108280254777 

The current subspace-distance is: 0.00018748770526144654 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.91; acc: 0.75
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.92; acc: 0.75
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.77
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.91
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.66; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.8; acc: 0.8
Batch: 320; loss: 0.8; acc: 0.77
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.76; acc: 0.77
Batch: 380; loss: 0.65; acc: 0.77
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.75; acc: 0.83
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.88; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.92
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.91
Batch: 680; loss: 0.72; acc: 0.8
Batch: 700; loss: 0.7; acc: 0.83
Batch: 720; loss: 0.73; acc: 0.75
Batch: 740; loss: 0.77; acc: 0.78
Batch: 760; loss: 0.73; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.0001956005289684981
0.00018618296599015594
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6261363967209105; val_accuracy: 0.8528065286624203 

The current subspace-distance is: 0.00018618296599015594 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.7; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.78
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.81
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.78; acc: 0.78
Batch: 200; loss: 0.79; acc: 0.8
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.81; acc: 0.8
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.79; acc: 0.78
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.82; acc: 0.77
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.73; acc: 0.73
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.0001984030968742445
0.0001886331447167322
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.625512097671533; val_accuracy: 0.8535031847133758 

The current subspace-distance is: 0.0001886331447167322 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.92
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.89
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.79; acc: 0.78
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.68; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.83
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.91
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.75; acc: 0.8
Batch: 480; loss: 0.67; acc: 0.84
Batch: 500; loss: 0.76; acc: 0.81
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.86; acc: 0.7
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.69; acc: 0.78
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.73; acc: 0.78
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.74; acc: 0.75
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00019641747348941863
0.0001888307451736182
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.88; acc: 0.7
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6378933832903576; val_accuracy: 0.850218949044586 

The current subspace-distance is: 0.0001888307451736182 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.8
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.8; acc: 0.72
Batch: 120; loss: 0.66; acc: 0.91
Batch: 140; loss: 0.64; acc: 0.92
Batch: 160; loss: 0.71; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.87; acc: 0.78
Batch: 240; loss: 0.82; acc: 0.75
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.73; acc: 0.8
Batch: 300; loss: 0.8; acc: 0.75
Batch: 320; loss: 0.7; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.8
Batch: 360; loss: 0.73; acc: 0.81
Batch: 380; loss: 0.79; acc: 0.8
Batch: 400; loss: 0.83; acc: 0.77
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.77; acc: 0.75
Batch: 580; loss: 0.86; acc: 0.75
Batch: 600; loss: 0.64; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.69; acc: 0.81
Batch: 660; loss: 0.74; acc: 0.81
Batch: 680; loss: 0.74; acc: 0.75
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.51; acc: 0.94
Batch: 780; loss: 0.68; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00019917152530979365
0.0001909193379106
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.86; acc: 0.7
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6192649500385211; val_accuracy: 0.8544984076433121 

The current subspace-distance is: 0.0001909193379106 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.75
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.71; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.8; acc: 0.78
Batch: 340; loss: 0.61; acc: 0.89
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.78; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.83
Batch: 480; loss: 0.77; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.92
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.86; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.89
Batch: 640; loss: 0.82; acc: 0.78
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.71; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00019912929565180093
0.0001903554511955008
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6214741460836617; val_accuracy: 0.8538017515923567 

The current subspace-distance is: 0.0001903554511955008 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.87; acc: 0.8
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.8
Batch: 200; loss: 0.72; acc: 0.84
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.91
Batch: 260; loss: 0.72; acc: 0.77
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.9; acc: 0.69
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.86
Batch: 380; loss: 0.79; acc: 0.77
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.91
Batch: 460; loss: 0.78; acc: 0.81
Batch: 480; loss: 0.85; acc: 0.75
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.86; acc: 0.72
Batch: 540; loss: 0.9; acc: 0.73
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.57; acc: 0.89
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.75; acc: 0.8
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.92
Batch: 760; loss: 0.62; acc: 0.91
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00019876478472724557
0.00019133997557219118
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.86; acc: 0.7
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6197220700181973; val_accuracy: 0.853702229299363 

The current subspace-distance is: 0.00019133997557219118 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.78; acc: 0.78
Batch: 200; loss: 0.55; acc: 0.92
Batch: 220; loss: 0.74; acc: 0.83
Batch: 240; loss: 0.83; acc: 0.8
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.59; acc: 0.91
Batch: 380; loss: 0.8; acc: 0.81
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.83; acc: 0.77
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.87; acc: 0.72
Batch: 500; loss: 0.78; acc: 0.78
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.86
Batch: 560; loss: 0.76; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.79; acc: 0.77
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.89; acc: 0.7
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.61; acc: 0.91
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.0001996788923861459
0.000189606798812747
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.88; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.6284420167564586; val_accuracy: 0.8523089171974523 

The current subspace-distance is: 0.000189606798812747 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.78; acc: 0.78
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.73; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.91
Batch: 240; loss: 0.91; acc: 0.69
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.84; acc: 0.75
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.77; acc: 0.75
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.78
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.69; acc: 0.89
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.94
Batch: 640; loss: 0.97; acc: 0.75
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.83
Batch: 700; loss: 0.67; acc: 0.86
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.71; acc: 0.8
Batch: 760; loss: 0.58; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00019918179896194488
0.00019257841631770134
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6111969461866246; val_accuracy: 0.8572850318471338 

The current subspace-distance is: 0.00019257841631770134 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.8
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.68; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.77; acc: 0.77
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.78; acc: 0.78
Batch: 260; loss: 0.7; acc: 0.84
Batch: 280; loss: 0.79; acc: 0.78
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.78
Batch: 540; loss: 0.72; acc: 0.77
Batch: 560; loss: 0.76; acc: 0.75
Batch: 580; loss: 0.75; acc: 0.8
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.72; acc: 0.8
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.81; acc: 0.83
Batch: 700; loss: 0.82; acc: 0.77
Batch: 720; loss: 0.65; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.0001995776838157326
0.0001915088068926707
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6245747759084034; val_accuracy: 0.8550955414012739 

The current subspace-distance is: 0.0001915088068926707 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_15_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.5620723414746194

The number of parameters is: 263271

The number of individual parameters is:

13
234
13
13
19
37791
19
19
38
110466
38
38
64
109440
64
64
4096
64
640
10
64
64

nonzero elements in E: 78981294
elements in E: 78981300
fraction nonzero: 0.9999999240326508
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.38; acc: 0.08
Batch: 20; loss: 2.14; acc: 0.22
Batch: 40; loss: 2.04; acc: 0.3
Batch: 60; loss: 1.84; acc: 0.48
Batch: 80; loss: 1.73; acc: 0.52
Batch: 100; loss: 1.64; acc: 0.67
Batch: 120; loss: 1.58; acc: 0.61
Batch: 140; loss: 1.63; acc: 0.62
Batch: 160; loss: 1.52; acc: 0.7
Batch: 180; loss: 1.5; acc: 0.64
Batch: 200; loss: 1.52; acc: 0.64
Batch: 220; loss: 1.47; acc: 0.73
Batch: 240; loss: 1.43; acc: 0.72
Batch: 260; loss: 1.47; acc: 0.67
Batch: 280; loss: 1.42; acc: 0.73
Batch: 300; loss: 1.41; acc: 0.69
Batch: 320; loss: 1.37; acc: 0.69
Batch: 340; loss: 1.38; acc: 0.67
Batch: 360; loss: 1.25; acc: 0.7
Batch: 380; loss: 1.4; acc: 0.75
Batch: 400; loss: 1.26; acc: 0.69
Batch: 420; loss: 1.3; acc: 0.67
Batch: 440; loss: 1.29; acc: 0.73
Batch: 460; loss: 1.24; acc: 0.77
Batch: 480; loss: 1.17; acc: 0.84
Batch: 500; loss: 1.32; acc: 0.64
Batch: 520; loss: 1.13; acc: 0.8
Batch: 540; loss: 1.16; acc: 0.8
Batch: 560; loss: 1.24; acc: 0.73
Batch: 580; loss: 1.27; acc: 0.72
Batch: 600; loss: 1.1; acc: 0.77
Batch: 620; loss: 1.22; acc: 0.77
Batch: 640; loss: 1.2; acc: 0.77
Batch: 660; loss: 1.2; acc: 0.69
Batch: 680; loss: 1.13; acc: 0.7
Batch: 700; loss: 1.18; acc: 0.77
Batch: 720; loss: 1.11; acc: 0.77
Batch: 740; loss: 1.09; acc: 0.83
Batch: 760; loss: 1.14; acc: 0.72
Batch: 780; loss: 1.2; acc: 0.77
Train Epoch over. train_loss: 1.4; train_accuracy: 0.67 

6.138514436315745e-05
5.7115601521218196e-05
Batch: 0; loss: 1.15; acc: 0.78
Batch: 20; loss: 1.37; acc: 0.62
Batch: 40; loss: 0.82; acc: 0.92
Batch: 60; loss: 0.97; acc: 0.81
Batch: 80; loss: 0.99; acc: 0.89
Batch: 100; loss: 1.12; acc: 0.77
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 1.0; acc: 0.84
Val Epoch over. val_loss: 1.0917085200358347; val_accuracy: 0.7866242038216561 

The current subspace-distance is: 5.7115601521218196e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.77
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 1.36; acc: 0.67
Batch: 60; loss: 1.14; acc: 0.75
Batch: 80; loss: 1.02; acc: 0.83
Batch: 100; loss: 1.2; acc: 0.78
Batch: 120; loss: 1.2; acc: 0.7
Batch: 140; loss: 1.02; acc: 0.77
Batch: 160; loss: 1.14; acc: 0.75
Batch: 180; loss: 1.08; acc: 0.86
Batch: 200; loss: 1.11; acc: 0.72
Batch: 220; loss: 1.06; acc: 0.81
Batch: 240; loss: 1.07; acc: 0.81
Batch: 260; loss: 1.11; acc: 0.78
Batch: 280; loss: 1.16; acc: 0.78
Batch: 300; loss: 0.93; acc: 0.88
Batch: 320; loss: 1.01; acc: 0.81
Batch: 340; loss: 1.0; acc: 0.81
Batch: 360; loss: 1.08; acc: 0.8
Batch: 380; loss: 1.03; acc: 0.75
Batch: 400; loss: 1.07; acc: 0.81
Batch: 420; loss: 0.94; acc: 0.88
Batch: 440; loss: 0.92; acc: 0.86
Batch: 460; loss: 1.12; acc: 0.77
Batch: 480; loss: 0.93; acc: 0.81
Batch: 500; loss: 1.0; acc: 0.88
Batch: 520; loss: 0.99; acc: 0.78
Batch: 540; loss: 1.06; acc: 0.8
Batch: 560; loss: 1.02; acc: 0.81
Batch: 580; loss: 1.0; acc: 0.8
Batch: 600; loss: 0.96; acc: 0.77
Batch: 620; loss: 1.07; acc: 0.78
Batch: 640; loss: 0.82; acc: 0.84
Batch: 660; loss: 1.11; acc: 0.75
Batch: 680; loss: 1.0; acc: 0.75
Batch: 700; loss: 0.76; acc: 0.92
Batch: 720; loss: 1.04; acc: 0.8
Batch: 740; loss: 1.04; acc: 0.75
Batch: 760; loss: 0.93; acc: 0.8
Batch: 780; loss: 0.94; acc: 0.83
Train Epoch over. train_loss: 1.05; train_accuracy: 0.78 

8.583969611208886e-05
8.05414529168047e-05
Batch: 0; loss: 0.98; acc: 0.84
Batch: 20; loss: 1.2; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.92
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.8; acc: 0.89
Batch: 100; loss: 0.96; acc: 0.8
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.79; acc: 0.89
Val Epoch over. val_loss: 0.9098192635615161; val_accuracy: 0.8331011146496815 

The current subspace-distance is: 8.05414529168047e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.78
Batch: 20; loss: 0.98; acc: 0.8
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 1.03; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.83
Batch: 120; loss: 0.85; acc: 0.86
Batch: 140; loss: 0.92; acc: 0.83
Batch: 160; loss: 0.93; acc: 0.83
Batch: 180; loss: 0.94; acc: 0.84
Batch: 200; loss: 1.11; acc: 0.73
Batch: 220; loss: 1.0; acc: 0.83
Batch: 240; loss: 1.04; acc: 0.8
Batch: 260; loss: 0.87; acc: 0.84
Batch: 280; loss: 0.92; acc: 0.83
Batch: 300; loss: 0.96; acc: 0.81
Batch: 320; loss: 1.0; acc: 0.81
Batch: 340; loss: 0.87; acc: 0.84
Batch: 360; loss: 0.99; acc: 0.8
Batch: 380; loss: 0.92; acc: 0.81
Batch: 400; loss: 0.77; acc: 0.89
Batch: 420; loss: 0.91; acc: 0.75
Batch: 440; loss: 0.85; acc: 0.83
Batch: 460; loss: 0.88; acc: 0.84
Batch: 480; loss: 0.79; acc: 0.88
Batch: 500; loss: 0.77; acc: 0.88
Batch: 520; loss: 0.78; acc: 0.86
Batch: 540; loss: 0.82; acc: 0.88
Batch: 560; loss: 0.92; acc: 0.88
Batch: 580; loss: 0.93; acc: 0.77
Batch: 600; loss: 0.85; acc: 0.86
Batch: 620; loss: 0.8; acc: 0.86
Batch: 640; loss: 0.87; acc: 0.88
Batch: 660; loss: 0.83; acc: 0.83
Batch: 680; loss: 0.88; acc: 0.81
Batch: 700; loss: 0.88; acc: 0.8
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.76; acc: 0.92
Batch: 760; loss: 0.86; acc: 0.84
Batch: 780; loss: 0.85; acc: 0.84
Train Epoch over. train_loss: 0.9; train_accuracy: 0.82 

0.0001046929246513173
0.00010017646127380431
Batch: 0; loss: 0.84; acc: 0.86
Batch: 20; loss: 1.13; acc: 0.73
Batch: 40; loss: 0.54; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.91
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.67; acc: 0.95
Val Epoch over. val_loss: 0.8034621621393094; val_accuracy: 0.8497213375796179 

The current subspace-distance is: 0.00010017646127380431 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.88
Batch: 40; loss: 0.85; acc: 0.81
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.81
Batch: 100; loss: 0.85; acc: 0.84
Batch: 120; loss: 0.85; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.91
Batch: 160; loss: 0.72; acc: 0.91
Batch: 180; loss: 0.88; acc: 0.84
Batch: 200; loss: 0.88; acc: 0.77
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.81; acc: 0.83
Batch: 260; loss: 0.82; acc: 0.8
Batch: 280; loss: 0.81; acc: 0.84
Batch: 300; loss: 0.76; acc: 0.89
Batch: 320; loss: 0.78; acc: 0.86
Batch: 340; loss: 0.76; acc: 0.89
Batch: 360; loss: 0.9; acc: 0.77
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.94; acc: 0.78
Batch: 420; loss: 0.86; acc: 0.86
Batch: 440; loss: 0.81; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.81; acc: 0.81
Batch: 500; loss: 0.83; acc: 0.86
Batch: 520; loss: 0.69; acc: 0.89
Batch: 540; loss: 0.74; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.86
Batch: 580; loss: 0.66; acc: 0.91
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.9; acc: 0.8
Batch: 640; loss: 0.75; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.86
Batch: 680; loss: 0.94; acc: 0.78
Batch: 700; loss: 0.86; acc: 0.8
Batch: 720; loss: 0.67; acc: 0.91
Batch: 740; loss: 0.8; acc: 0.8
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.79; acc: 0.83
Train Epoch over. train_loss: 0.81; train_accuracy: 0.84 

0.0001223068538820371
0.00011772735888371244
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 1.02; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.95
Val Epoch over. val_loss: 0.7246368371756973; val_accuracy: 0.8635549363057324 

The current subspace-distance is: 0.00011772735888371244 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.89
Batch: 40; loss: 0.87; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.86
Batch: 80; loss: 0.65; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.86
Batch: 140; loss: 0.74; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.83
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.8; acc: 0.89
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.84
Batch: 320; loss: 0.79; acc: 0.84
Batch: 340; loss: 0.81; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.75; acc: 0.83
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.65; acc: 0.89
Batch: 440; loss: 0.81; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.91
Batch: 480; loss: 0.68; acc: 0.86
Batch: 500; loss: 0.7; acc: 0.86
Batch: 520; loss: 0.76; acc: 0.81
Batch: 540; loss: 0.85; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.91
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.91
Batch: 680; loss: 0.69; acc: 0.88
Batch: 700; loss: 0.7; acc: 0.84
Batch: 720; loss: 0.74; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.91
Batch: 760; loss: 0.61; acc: 0.89
Batch: 780; loss: 0.78; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.85 

0.0001375210122205317
0.00013134878827258945
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.94; acc: 0.8
Batch: 40; loss: 0.38; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.97
Val Epoch over. val_loss: 0.6398965961234585; val_accuracy: 0.8821656050955414 

The current subspace-distance is: 0.00013134878827258945 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 0.63; acc: 0.92
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.64; acc: 0.86
Batch: 200; loss: 0.84; acc: 0.86
Batch: 220; loss: 0.74; acc: 0.81
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.71; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.67; acc: 0.88
Batch: 420; loss: 0.74; acc: 0.8
Batch: 440; loss: 0.75; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.83
Batch: 480; loss: 0.77; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.92
Batch: 520; loss: 0.68; acc: 0.88
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.91
Batch: 580; loss: 0.65; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.91
Batch: 680; loss: 0.71; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.88
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 0.68; train_accuracy: 0.86 

0.0001474550663260743
0.00014225157792679965
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.88; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.97
Val Epoch over. val_loss: 0.6037358671996245; val_accuracy: 0.8784832802547771 

The current subspace-distance is: 0.00014225157792679965 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.91
Batch: 140; loss: 0.64; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.91
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.58; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.91
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.91
Batch: 500; loss: 0.55; acc: 0.92
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.6; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.66; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.68; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.6; acc: 0.89
Batch: 680; loss: 0.72; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.86 

0.00016012835840228945
0.0001534161710878834
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.97
Val Epoch over. val_loss: 0.5571026331300188; val_accuracy: 0.8912221337579618 

The current subspace-distance is: 0.0001534161710878834 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.78; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.69; acc: 0.8
Batch: 180; loss: 0.65; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.94
Batch: 220; loss: 0.8; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.47; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.63; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.94
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.66; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.6; train_accuracy: 0.87 

0.00016977755876723677
0.0001632189960218966
Batch: 0; loss: 0.55; acc: 0.92
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.5230396902485258; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.0001632189960218966 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.63; acc: 0.89
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.94
Batch: 280; loss: 0.55; acc: 0.92
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00017987661703955382
0.00017110072076320648
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.490194853134216; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 0.00017110072076320648 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.94
Batch: 40; loss: 0.75; acc: 0.72
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.54; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.55; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.48; acc: 0.95
Batch: 640; loss: 0.82; acc: 0.75
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.95
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.00019032665295526385
0.00018347453442402184
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.47876794873529177; val_accuracy: 0.899781050955414 

The current subspace-distance is: 0.00018347453442402184 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.8; acc: 0.75
Batch: 340; loss: 0.47; acc: 0.95
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.77
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.94
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.94
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0001936407497851178
0.00018528266809880733
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.47166080117984943; val_accuracy: 0.9026671974522293 

The current subspace-distance is: 0.00018528266809880733 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.44; acc: 0.95
Batch: 220; loss: 0.71; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.95
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.95
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.89
Batch: 420; loss: 0.58; acc: 0.89
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.45; acc: 0.94
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019487569807097316
0.0001872057473519817
Batch: 0; loss: 0.46; acc: 0.95
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4617065994223212; val_accuracy: 0.9006767515923567 

The current subspace-distance is: 0.0001872057473519817 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.97
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019883134518750012
0.00019102463556919247
Batch: 0; loss: 0.47; acc: 0.95
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.45991226831439197; val_accuracy: 0.9038614649681529 

The current subspace-distance is: 0.00019102463556919247 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.95
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.76; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.44; acc: 0.94
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020035089983139187
0.00019372596580069512
Batch: 0; loss: 0.47; acc: 0.95
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4507449239872064; val_accuracy: 0.9031648089171974 

The current subspace-distance is: 0.00019372596580069512 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.72; acc: 0.75
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.81; acc: 0.78
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00020064513955730945
0.00019447192607913166
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4434990307707695; val_accuracy: 0.9046576433121019 

The current subspace-distance is: 0.00019447192607913166 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.69; acc: 0.77
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0002055090299108997
0.00019865023205056787
Batch: 0; loss: 0.45; acc: 0.95
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4364573334812359; val_accuracy: 0.9066480891719745 

The current subspace-distance is: 0.00019865023205056787 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.59; acc: 0.8
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00020792822760995477
0.0002004548005061224
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4374283616709861; val_accuracy: 0.9039609872611465 

The current subspace-distance is: 0.0002004548005061224 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.95
Batch: 620; loss: 0.64; acc: 0.8
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.0002102649596054107
0.00020257089636288583
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.43502723904931623; val_accuracy: 0.9052547770700637 

The current subspace-distance is: 0.00020257089636288583 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021368621673900634
0.0002032463817158714
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.42760615050792694; val_accuracy: 0.9076433121019108 

The current subspace-distance is: 0.0002032463817158714 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.91
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.81
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.0002128060586983338
0.0002022984845098108
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.42852408139948633; val_accuracy: 0.9048566878980892 

The current subspace-distance is: 0.0002022984845098108 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.34; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.61; acc: 0.8
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021600474428851157
0.0002068069443339482
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.41731500948310657; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.0002068069443339482 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021423978614620864
0.00020645845506805927
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.41845139452985897; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 0.00020645845506805927 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.8
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.81
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.97
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0002142251905752346
0.0002078669931506738
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.41974073544049717; val_accuracy: 0.9079418789808917 

The current subspace-distance is: 0.0002078669931506738 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.94
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.95
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0002158411080017686
0.0002073975483654067
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.41364768460677687; val_accuracy: 0.9078423566878981 

The current subspace-distance is: 0.0002073975483654067 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.94
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.94
Batch: 760; loss: 0.71; acc: 0.78
Batch: 780; loss: 0.33; acc: 0.98
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00021450669737532735
0.00020765764929819852
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.4142811819436444; val_accuracy: 0.9063495222929936 

The current subspace-distance is: 0.00020765764929819852 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.94
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.91
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.95
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00021716243645641953
0.00021013649529777467
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.41502253948503237; val_accuracy: 0.90734474522293 

The current subspace-distance is: 0.00021013649529777467 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00021756708156317472
0.0002088373148581013
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.41489101120620775; val_accuracy: 0.9048566878980892 

The current subspace-distance is: 0.0002088373148581013 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.97
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.6; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.97
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00021943530009593815
0.0002103320584865287
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.4145667477018514; val_accuracy: 0.9084394904458599 

The current subspace-distance is: 0.0002103320584865287 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.8
Batch: 280; loss: 0.35; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.97
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00022032653214409947
0.00021129108790773898
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.41148930028745323; val_accuracy: 0.9075437898089171 

The current subspace-distance is: 0.00021129108790773898 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.94
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.95
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00022128879209049046
0.0002116399264195934
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.40919414874474713; val_accuracy: 0.9083399681528662 

The current subspace-distance is: 0.0002116399264195934 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_15_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.5620723414746194

The number of parameters is: 263271

The number of individual parameters is:

13
234
13
13
19
37791
19
19
38
110466
38
38
64
109440
64
64
4096
64
640
10
64
64

nonzero elements in E: 105308390
elements in E: 105308400
fraction nonzero: 0.9999999050408135
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.41; acc: 0.06
Batch: 20; loss: 2.08; acc: 0.31
Batch: 40; loss: 1.76; acc: 0.61
Batch: 60; loss: 1.72; acc: 0.62
Batch: 80; loss: 1.57; acc: 0.72
Batch: 100; loss: 1.54; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.78
Batch: 140; loss: 1.5; acc: 0.66
Batch: 160; loss: 1.41; acc: 0.77
Batch: 180; loss: 1.43; acc: 0.72
Batch: 200; loss: 1.35; acc: 0.72
Batch: 220; loss: 1.35; acc: 0.73
Batch: 240; loss: 1.29; acc: 0.77
Batch: 260; loss: 1.34; acc: 0.78
Batch: 280; loss: 1.32; acc: 0.7
Batch: 300; loss: 1.22; acc: 0.75
Batch: 320; loss: 1.18; acc: 0.78
Batch: 340; loss: 1.12; acc: 0.83
Batch: 360; loss: 1.1; acc: 0.83
Batch: 380; loss: 1.2; acc: 0.73
Batch: 400; loss: 1.01; acc: 0.88
Batch: 420; loss: 1.11; acc: 0.83
Batch: 440; loss: 1.26; acc: 0.72
Batch: 460; loss: 0.99; acc: 0.89
Batch: 480; loss: 1.15; acc: 0.8
Batch: 500; loss: 1.03; acc: 0.81
Batch: 520; loss: 0.94; acc: 0.88
Batch: 540; loss: 1.24; acc: 0.78
Batch: 560; loss: 0.99; acc: 0.83
Batch: 580; loss: 1.01; acc: 0.83
Batch: 600; loss: 0.98; acc: 0.88
Batch: 620; loss: 0.97; acc: 0.81
Batch: 640; loss: 0.93; acc: 0.84
Batch: 660; loss: 1.06; acc: 0.77
Batch: 680; loss: 1.06; acc: 0.8
Batch: 700; loss: 1.01; acc: 0.83
Batch: 720; loss: 0.85; acc: 0.95
Batch: 740; loss: 1.02; acc: 0.77
Batch: 760; loss: 0.86; acc: 0.94
Batch: 780; loss: 0.9; acc: 0.92
Train Epoch over. train_loss: 1.23; train_accuracy: 0.76 

2.486837183823809e-05
8.704646461410448e-06
Batch: 0; loss: 0.96; acc: 0.89
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.91
Batch: 80; loss: 0.71; acc: 0.97
Batch: 100; loss: 0.88; acc: 0.88
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 0.77; acc: 0.94
Val Epoch over. val_loss: 0.8769939044478593; val_accuracy: 0.8664410828025477 

The current subspace-distance is: 8.704646461410448e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.88
Batch: 20; loss: 0.87; acc: 0.84
Batch: 40; loss: 0.97; acc: 0.89
Batch: 60; loss: 0.89; acc: 0.84
Batch: 80; loss: 0.84; acc: 0.86
Batch: 100; loss: 0.78; acc: 0.94
Batch: 120; loss: 0.92; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.94
Batch: 160; loss: 0.89; acc: 0.8
Batch: 180; loss: 0.88; acc: 0.84
Batch: 200; loss: 0.78; acc: 0.88
Batch: 220; loss: 0.85; acc: 0.84
Batch: 240; loss: 0.78; acc: 0.89
Batch: 260; loss: 0.8; acc: 0.86
Batch: 280; loss: 0.83; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.84
Batch: 320; loss: 0.87; acc: 0.84
Batch: 340; loss: 0.96; acc: 0.8
Batch: 360; loss: 0.95; acc: 0.78
Batch: 380; loss: 0.86; acc: 0.81
Batch: 400; loss: 0.9; acc: 0.8
Batch: 420; loss: 0.85; acc: 0.86
Batch: 440; loss: 0.86; acc: 0.88
Batch: 460; loss: 0.81; acc: 0.86
Batch: 480; loss: 0.76; acc: 0.91
Batch: 500; loss: 0.67; acc: 0.91
Batch: 520; loss: 0.86; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.94
Batch: 560; loss: 0.81; acc: 0.84
Batch: 580; loss: 0.99; acc: 0.73
Batch: 600; loss: 0.69; acc: 0.92
Batch: 620; loss: 0.84; acc: 0.81
Batch: 640; loss: 0.74; acc: 0.89
Batch: 660; loss: 0.86; acc: 0.78
Batch: 680; loss: 0.79; acc: 0.83
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.71; acc: 0.89
Batch: 740; loss: 0.67; acc: 0.91
Batch: 760; loss: 0.83; acc: 0.81
Batch: 780; loss: 0.76; acc: 0.89
Train Epoch over. train_loss: 0.84; train_accuracy: 0.85 

3.0487146432278678e-05
1.1480250577733386e-05
Batch: 0; loss: 0.77; acc: 0.89
Batch: 20; loss: 0.88; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.98
Batch: 60; loss: 0.67; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.95
Val Epoch over. val_loss: 0.7076589205082814; val_accuracy: 0.8825636942675159 

The current subspace-distance is: 1.1480250577733386e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.94
Batch: 20; loss: 0.85; acc: 0.83
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.86
Batch: 100; loss: 0.75; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.7; acc: 0.89
Batch: 200; loss: 0.73; acc: 0.89
Batch: 220; loss: 0.71; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.94
Batch: 260; loss: 0.73; acc: 0.88
Batch: 280; loss: 0.78; acc: 0.89
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.78; acc: 0.83
Batch: 360; loss: 0.67; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.94
Batch: 400; loss: 0.69; acc: 0.91
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.82; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.66; acc: 0.91
Batch: 520; loss: 0.67; acc: 0.89
Batch: 540; loss: 0.74; acc: 0.86
Batch: 560; loss: 0.67; acc: 0.91
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.68; acc: 0.91
Batch: 660; loss: 0.65; acc: 0.92
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.71; acc: 0.86
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.88
Train Epoch over. train_loss: 0.71; train_accuracy: 0.87 

3.4804797905962914e-05
1.4796213690715376e-05
Batch: 0; loss: 0.68; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.92
Val Epoch over. val_loss: 0.6208588801751471; val_accuracy: 0.8916202229299363 

The current subspace-distance is: 1.4796213690715376e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.81
Batch: 80; loss: 0.72; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.74; acc: 0.84
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.91
Batch: 300; loss: 0.6; acc: 0.89
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.88
Batch: 360; loss: 0.65; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.89
Batch: 400; loss: 0.68; acc: 0.89
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.92
Batch: 460; loss: 0.63; acc: 0.91
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.95
Batch: 520; loss: 0.67; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.94
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.84; acc: 0.77
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.92
Batch: 720; loss: 0.66; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.97
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.88 

3.7699643144151196e-05
1.62078213179484e-05
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.95
Val Epoch over. val_loss: 0.5575373345499586; val_accuracy: 0.9000796178343949 

The current subspace-distance is: 1.62078213179484e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.92
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.92
Batch: 420; loss: 0.65; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.94
Batch: 500; loss: 0.59; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.91
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.92
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.88
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.56; acc: 0.94
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.92
Train Epoch over. train_loss: 0.59; train_accuracy: 0.88 

4.086604531039484e-05
1.7477186702308245e-05
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.97
Val Epoch over. val_loss: 0.5108794773080546; val_accuracy: 0.9069466560509554 

The current subspace-distance is: 1.7477186702308245e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.94
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.94
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.7; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.331107629695907e-05
1.7893744370667264e-05
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.46154841970486243; val_accuracy: 0.9086385350318471 

The current subspace-distance is: 1.7893744370667264e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.91
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.94
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

4.6362696593860164e-05
1.985349263122771e-05
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.4345734510452125; val_accuracy: 0.9125199044585988 

The current subspace-distance is: 1.985349263122771e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.98
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.97
Batch: 540; loss: 0.52; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.8
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

4.966663254890591e-05
2.2923044525668956e-05
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4119340254432836; val_accuracy: 0.9175955414012739 

The current subspace-distance is: 2.2923044525668956e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.97
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.78
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.57; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.97
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.95
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.162262459634803e-05
2.29793422477087e-05
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.98
Val Epoch over. val_loss: 0.3957746716061975; val_accuracy: 0.9173964968152867 

The current subspace-distance is: 2.29793422477087e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.95
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.5; acc: 0.81
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.337296533980407e-05
2.2873095076647587e-05
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.3765619170324058; val_accuracy: 0.9201831210191083 

The current subspace-distance is: 2.2873095076647587e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.97
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.97
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.404757393989712e-05
2.3223035896080546e-05
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.37289979607815954; val_accuracy: 0.919187898089172 

The current subspace-distance is: 2.3223035896080546e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.98
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.97
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.564268940361217e-05
2.5617307983338833e-05
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.3726362129495402; val_accuracy: 0.9202826433121019 

The current subspace-distance is: 2.5617307983338833e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.95
Batch: 420; loss: 0.55; acc: 0.8
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.97
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.577878255280666e-05
2.4010727429413237e-05
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.36540974078664357; val_accuracy: 0.9193869426751592 

The current subspace-distance is: 2.4010727429413237e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.97
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.28; acc: 0.97
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.6500444770790637e-05
2.4494243916706182e-05
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3657130562955407; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 2.4494243916706182e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

5.7609406212577596e-05
2.5893063138937578e-05
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3589092188390197; val_accuracy: 0.9223726114649682 

The current subspace-distance is: 2.5893063138937578e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.97
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

5.6893793953349814e-05
2.581719854788389e-05
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.3571940521905377; val_accuracy: 0.9205812101910829 

The current subspace-distance is: 2.581719854788389e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.39; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

5.82980974286329e-05
2.640282218635548e-05
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.35067264991960706; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.640282218635548e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

5.8306635764893144e-05
2.694117029022891e-05
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3515235792109921; val_accuracy: 0.9212778662420382 

The current subspace-distance is: 2.694117029022891e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.97
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

5.910325489821844e-05
2.733177643676754e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.35151855913317126; val_accuracy: 0.9197850318471338 

The current subspace-distance is: 2.733177643676754e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.53; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.935330773354508e-05
2.705464794416912e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3435659259557724; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.705464794416912e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.21; acc: 0.98
Batch: 660; loss: 0.34; acc: 0.97
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.97
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.9 

5.962774594081566e-05
2.7463636797619984e-05
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3480466659281664; val_accuracy: 0.9214769108280255 

The current subspace-distance is: 2.7463636797619984e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.9339767176425084e-05
2.6614314265316352e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3405897386704281; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 2.6614314265316352e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.009856952005066e-05
2.835822306224145e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.33989772399899304; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.835822306224145e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.0141257563373074e-05
2.764055534498766e-05
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3370855715908822; val_accuracy: 0.9240644904458599 

The current subspace-distance is: 2.764055534498766e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.95
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.962518116575666e-05
2.701604535104707e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3369388839431629; val_accuracy: 0.9241640127388535 

The current subspace-distance is: 2.701604535104707e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.97
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.086818757466972e-05
2.8555143217090517e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3383967074429154; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 2.8555143217090517e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.98
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.028750794939697e-05
2.6946552679874003e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.34241843517798526; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.6946552679874003e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.98
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.080548700992949e-05
2.797628076223191e-05
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3350947491682259; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 2.797628076223191e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.97
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.06146568316035e-05
2.718436371651478e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.33746734641160175; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.718436371651478e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.95
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.8
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.97
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.090277747716755e-05
2.9299013476702385e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3315689841368396; val_accuracy: 0.9244625796178344 

The current subspace-distance is: 2.9299013476702385e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_15_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.5620723414746194

The number of parameters is: 263271

The number of individual parameters is:

13
234
13
13
19
37791
19
19
38
110466
38
38
64
109440
64
64
4096
64
640
10
64
64

nonzero elements in E: 131635490
elements in E: 131635500
fraction nonzero: 0.9999999240326508
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.36; acc: 0.14
Batch: 20; loss: 1.94; acc: 0.41
Batch: 40; loss: 1.81; acc: 0.56
Batch: 60; loss: 1.66; acc: 0.62
Batch: 80; loss: 1.52; acc: 0.73
Batch: 100; loss: 1.54; acc: 0.69
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.69
Batch: 160; loss: 1.46; acc: 0.62
Batch: 180; loss: 1.43; acc: 0.72
Batch: 200; loss: 1.29; acc: 0.83
Batch: 220; loss: 1.32; acc: 0.81
Batch: 240; loss: 1.21; acc: 0.84
Batch: 260; loss: 1.31; acc: 0.72
Batch: 280; loss: 1.25; acc: 0.8
Batch: 300; loss: 1.19; acc: 0.83
Batch: 320; loss: 1.16; acc: 0.81
Batch: 340; loss: 1.07; acc: 0.88
Batch: 360; loss: 1.11; acc: 0.83
Batch: 380; loss: 1.12; acc: 0.8
Batch: 400; loss: 1.0; acc: 0.83
Batch: 420; loss: 1.17; acc: 0.77
Batch: 440; loss: 1.03; acc: 0.84
Batch: 460; loss: 1.07; acc: 0.84
Batch: 480; loss: 0.97; acc: 0.88
Batch: 500; loss: 1.09; acc: 0.77
Batch: 520; loss: 1.05; acc: 0.83
Batch: 540; loss: 0.97; acc: 0.81
Batch: 560; loss: 0.97; acc: 0.83
Batch: 580; loss: 0.93; acc: 0.91
Batch: 600; loss: 1.02; acc: 0.81
Batch: 620; loss: 0.93; acc: 0.84
Batch: 640; loss: 1.02; acc: 0.8
Batch: 660; loss: 0.93; acc: 0.83
Batch: 680; loss: 0.91; acc: 0.84
Batch: 700; loss: 0.97; acc: 0.86
Batch: 720; loss: 1.03; acc: 0.83
Batch: 740; loss: 0.96; acc: 0.83
Batch: 760; loss: 1.08; acc: 0.77
Batch: 780; loss: 0.88; acc: 0.89
Train Epoch over. train_loss: 1.19; train_accuracy: 0.78 

2.4778544684522785e-05
9.319227501691785e-06
Batch: 0; loss: 0.92; acc: 0.89
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.57; acc: 0.95
Batch: 60; loss: 0.77; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.91
Batch: 100; loss: 0.85; acc: 0.92
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 0.73; acc: 0.88
Val Epoch over. val_loss: 0.8459983859092567; val_accuracy: 0.8646496815286624 

The current subspace-distance is: 9.319227501691785e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.8
Batch: 20; loss: 0.88; acc: 0.88
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.86
Batch: 80; loss: 0.92; acc: 0.83
Batch: 100; loss: 0.85; acc: 0.83
Batch: 120; loss: 1.01; acc: 0.86
Batch: 140; loss: 0.96; acc: 0.78
Batch: 160; loss: 0.89; acc: 0.81
Batch: 180; loss: 0.76; acc: 0.92
Batch: 200; loss: 0.9; acc: 0.84
Batch: 220; loss: 0.84; acc: 0.86
Batch: 240; loss: 0.85; acc: 0.8
Batch: 260; loss: 0.84; acc: 0.84
Batch: 280; loss: 0.83; acc: 0.89
Batch: 300; loss: 0.85; acc: 0.83
Batch: 320; loss: 0.77; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.91
Batch: 360; loss: 0.81; acc: 0.84
Batch: 380; loss: 0.78; acc: 0.89
Batch: 400; loss: 0.9; acc: 0.81
Batch: 420; loss: 0.82; acc: 0.88
Batch: 440; loss: 0.8; acc: 0.83
Batch: 460; loss: 0.91; acc: 0.83
Batch: 480; loss: 0.95; acc: 0.8
Batch: 500; loss: 0.85; acc: 0.88
Batch: 520; loss: 0.75; acc: 0.89
Batch: 540; loss: 0.73; acc: 0.91
Batch: 560; loss: 0.79; acc: 0.84
Batch: 580; loss: 0.78; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.89
Batch: 620; loss: 0.63; acc: 0.92
Batch: 640; loss: 0.61; acc: 0.94
Batch: 660; loss: 0.69; acc: 0.91
Batch: 680; loss: 0.78; acc: 0.88
Batch: 700; loss: 0.78; acc: 0.86
Batch: 720; loss: 0.82; acc: 0.83
Batch: 740; loss: 0.84; acc: 0.83
Batch: 760; loss: 0.88; acc: 0.84
Batch: 780; loss: 0.9; acc: 0.8
Train Epoch over. train_loss: 0.81; train_accuracy: 0.86 

3.0853465432301164e-05
1.2050026271026582e-05
Batch: 0; loss: 0.77; acc: 0.89
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.95
Batch: 100; loss: 0.73; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.6; acc: 0.94
Val Epoch over. val_loss: 0.6834373335549786; val_accuracy: 0.8854498407643312 

The current subspace-distance is: 1.2050026271026582e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.84
Batch: 40; loss: 0.81; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.78
Batch: 80; loss: 0.73; acc: 0.89
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.78; acc: 0.84
Batch: 160; loss: 0.83; acc: 0.86
Batch: 180; loss: 0.83; acc: 0.8
Batch: 200; loss: 0.74; acc: 0.86
Batch: 220; loss: 0.71; acc: 0.89
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.68; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.72; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.61; acc: 0.92
Batch: 380; loss: 0.6; acc: 0.92
Batch: 400; loss: 0.52; acc: 0.94
Batch: 420; loss: 0.69; acc: 0.89
Batch: 440; loss: 0.68; acc: 0.88
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.77; acc: 0.84
Batch: 560; loss: 0.77; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.94
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.73; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.94
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.92
Batch: 780; loss: 0.57; acc: 0.94
Train Epoch over. train_loss: 0.67; train_accuracy: 0.88 

3.5479108191793784e-05
1.532698115624953e-05
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.95
Val Epoch over. val_loss: 0.5536820229831015; val_accuracy: 0.9040605095541401 

The current subspace-distance is: 1.532698115624953e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.8
Batch: 160; loss: 0.55; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.95
Batch: 200; loss: 0.54; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.95
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.97
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.95
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.77
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.95
Train Epoch over. train_loss: 0.57; train_accuracy: 0.89 

3.9839458622736856e-05
1.8336846551392227e-05
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.473768712323942; val_accuracy: 0.9125199044585988 

The current subspace-distance is: 1.8336846551392227e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.95
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.94
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.94
Batch: 340; loss: 0.6; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.97
Batch: 720; loss: 0.46; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.9 

4.4108473957749084e-05
2.0321676856838167e-05
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.4155924675191284; val_accuracy: 0.919984076433121 

The current subspace-distance is: 2.0321676856838167e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.34; acc: 1.0
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.94
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.61; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

4.6577661123592407e-05
2.1707779524149373e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.37609099060486834; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.1707779524149373e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.98
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.98
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.0006543460767716e-05
2.21262398554245e-05
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3434194007022366; val_accuracy: 0.9272492038216561 

The current subspace-distance is: 2.21262398554245e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.237380537437275e-05
2.2691023332299665e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3234645333259728; val_accuracy: 0.9294386942675159 

The current subspace-distance is: 2.2691023332299665e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.22; acc: 1.0
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.496264930116013e-05
2.5311566787422635e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.305150211094671; val_accuracy: 0.9271496815286624 

The current subspace-distance is: 2.5311566787422635e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.84
Batch: 240; loss: 0.38; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.6771285017021e-05
2.47191474045394e-05
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.29611579808080274; val_accuracy: 0.9296377388535032 

The current subspace-distance is: 2.47191474045394e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.15; acc: 1.0
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.717641397495754e-05
2.6153631552006118e-05
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.29599339310910294; val_accuracy: 0.9295382165605095 

The current subspace-distance is: 2.6153631552006118e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.98
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.97
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.861771933268756e-05
2.6226052796118893e-05
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2948392743992198; val_accuracy: 0.9306329617834395 

The current subspace-distance is: 2.6226052796118893e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.97
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.98
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.94536068092566e-05
2.724007936194539e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.28510660986611797; val_accuracy: 0.9331210191082803 

The current subspace-distance is: 2.724007936194539e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.98
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.91045682085678e-05
2.6479014195501804e-05
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2814277781612554; val_accuracy: 0.9327229299363057 

The current subspace-distance is: 2.6479014195501804e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.0307800595182925e-05
2.6503992557991296e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2774393985605544; val_accuracy: 0.9323248407643312 

The current subspace-distance is: 2.6503992557991296e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.98
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.98
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.98
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.972223152639344e-05
2.6306061045033857e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.27443254838703546; val_accuracy: 0.933718152866242 

The current subspace-distance is: 2.6306061045033857e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.98
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.21; acc: 1.0
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.07197689532768e-05
2.7582236725720577e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2740510076189497; val_accuracy: 0.9341162420382165 

The current subspace-distance is: 2.7582236725720577e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.83
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.113883864600211e-05
2.7152738766744733e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2711459895133213; val_accuracy: 0.9334195859872612 

The current subspace-distance is: 2.7152738766744733e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.97
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.98
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.160087650641799e-05
2.8385054974933155e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.26889048687591677; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.8385054974933155e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.12; acc: 1.0
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.98
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.198161281645298e-05
2.7846570446854457e-05
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.26298331711322637; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.7846570446854457e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.98
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.332475459203124e-05
3.0465691452263854e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26175773319355244; val_accuracy: 0.9354100318471338 

The current subspace-distance is: 3.0465691452263854e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.204505916684866e-05
2.7378040613257326e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.26305520691119944; val_accuracy: 0.9353105095541401 

The current subspace-distance is: 2.7378040613257326e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.17; acc: 1.0
Batch: 520; loss: 0.26; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.187068356666714e-05
2.7467114705359563e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26521844084665275; val_accuracy: 0.9343152866242038 

The current subspace-distance is: 2.7467114705359563e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.13; acc: 1.0
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.297734216786921e-05
2.872283585020341e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.26147699341842323; val_accuracy: 0.9342157643312102 

The current subspace-distance is: 2.872283585020341e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.302923429757357e-05
2.7988917281618342e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2581101994320845; val_accuracy: 0.934812898089172 

The current subspace-distance is: 2.7988917281618342e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.98
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.17; acc: 0.98
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.339723040582612e-05
2.9229795472929254e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2609289565663429; val_accuracy: 0.9370023885350318 

The current subspace-distance is: 2.9229795472929254e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.359716644510627e-05
3.0699691706104204e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.2611698936789658; val_accuracy: 0.9347133757961783 

The current subspace-distance is: 3.0699691706104204e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.307150761131197e-05
2.9160453777876683e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.25909403920363466; val_accuracy: 0.9366042993630573 

The current subspace-distance is: 2.9160453777876683e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.36960394331254e-05
2.9484386686817743e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.25847081764108815; val_accuracy: 0.9345143312101911 

The current subspace-distance is: 2.9484386686817743e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.367520109051839e-05
2.866762224584818e-05
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.25588968227718284; val_accuracy: 0.9366042993630573 

The current subspace-distance is: 2.866762224584818e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_15_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:47/N_15_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
