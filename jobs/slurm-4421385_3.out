model : table13slim
N : 3
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 4.928869038633508

The number of parameters is: 275071

The number of individual parameters is:

40
400
40
40
60
50400
60
60
119
149940
119
119
64
68544
64
64
4096
64
640
10
64
64

nonzero elements in E: 13753548
elements in E: 13753550
fraction nonzero: 0.9999998545829986
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.45; acc: 0.03
Batch: 20; loss: 2.38; acc: 0.12
Batch: 40; loss: 2.34; acc: 0.11
Batch: 60; loss: 2.26; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.22; acc: 0.22
Batch: 120; loss: 2.27; acc: 0.2
Batch: 140; loss: 2.15; acc: 0.25
Batch: 160; loss: 2.14; acc: 0.28
Batch: 180; loss: 2.03; acc: 0.36
Batch: 200; loss: 2.16; acc: 0.22
Batch: 220; loss: 2.13; acc: 0.27
Batch: 240; loss: 2.06; acc: 0.36
Batch: 260; loss: 2.12; acc: 0.23
Batch: 280; loss: 2.11; acc: 0.27
Batch: 300; loss: 2.06; acc: 0.34
Batch: 320; loss: 2.07; acc: 0.33
Batch: 340; loss: 2.05; acc: 0.34
Batch: 360; loss: 2.14; acc: 0.25
Batch: 380; loss: 2.2; acc: 0.23
Batch: 400; loss: 2.03; acc: 0.41
Batch: 420; loss: 2.0; acc: 0.38
Batch: 440; loss: 2.11; acc: 0.34
Batch: 460; loss: 2.04; acc: 0.36
Batch: 480; loss: 2.09; acc: 0.3
Batch: 500; loss: 2.06; acc: 0.28
Batch: 520; loss: 1.89; acc: 0.42
Batch: 540; loss: 2.0; acc: 0.33
Batch: 560; loss: 2.14; acc: 0.25
Batch: 580; loss: 2.09; acc: 0.25
Batch: 600; loss: 2.05; acc: 0.3
Batch: 620; loss: 1.95; acc: 0.44
Batch: 640; loss: 2.0; acc: 0.3
Batch: 660; loss: 2.0; acc: 0.39
Batch: 680; loss: 1.93; acc: 0.36
Batch: 700; loss: 2.04; acc: 0.3
Batch: 720; loss: 1.95; acc: 0.41
Batch: 740; loss: 2.03; acc: 0.34
Batch: 760; loss: 2.02; acc: 0.31
Batch: 780; loss: 2.02; acc: 0.39
Train Epoch over. train_loss: 2.08; train_accuracy: 0.31 

2.3002887246548198e-05
3.871057742799167e-06
Batch: 0; loss: 1.98; acc: 0.3
Batch: 20; loss: 2.0; acc: 0.33
Batch: 40; loss: 1.8; acc: 0.55
Batch: 60; loss: 1.94; acc: 0.36
Batch: 80; loss: 1.9; acc: 0.42
Batch: 100; loss: 1.91; acc: 0.42
Batch: 120; loss: 2.01; acc: 0.36
Batch: 140; loss: 1.86; acc: 0.48
Val Epoch over. val_loss: 1.9276838484843066; val_accuracy: 0.4138136942675159 

The current subspace-distance is: 3.871057742799167e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.81; acc: 0.55
Batch: 20; loss: 1.91; acc: 0.45
Batch: 40; loss: 1.88; acc: 0.41
Batch: 60; loss: 1.87; acc: 0.42
Batch: 80; loss: 1.98; acc: 0.39
Batch: 100; loss: 1.86; acc: 0.5
Batch: 120; loss: 1.92; acc: 0.42
Batch: 140; loss: 1.89; acc: 0.44
Batch: 160; loss: 2.02; acc: 0.38
Batch: 180; loss: 1.86; acc: 0.45
Batch: 200; loss: 1.92; acc: 0.38
Batch: 220; loss: 1.87; acc: 0.44
Batch: 240; loss: 1.8; acc: 0.42
Batch: 260; loss: 1.89; acc: 0.38
Batch: 280; loss: 1.84; acc: 0.42
Batch: 300; loss: 1.92; acc: 0.36
Batch: 320; loss: 1.95; acc: 0.44
Batch: 340; loss: 1.97; acc: 0.36
Batch: 360; loss: 2.0; acc: 0.3
Batch: 380; loss: 1.95; acc: 0.47
Batch: 400; loss: 1.95; acc: 0.42
Batch: 420; loss: 1.85; acc: 0.34
Batch: 440; loss: 2.02; acc: 0.36
Batch: 460; loss: 1.74; acc: 0.48
Batch: 480; loss: 1.81; acc: 0.48
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.82; acc: 0.45
Batch: 540; loss: 1.93; acc: 0.31
Batch: 560; loss: 1.78; acc: 0.59
Batch: 580; loss: 1.87; acc: 0.42
Batch: 600; loss: 1.93; acc: 0.33
Batch: 620; loss: 1.95; acc: 0.38
Batch: 640; loss: 1.72; acc: 0.52
Batch: 660; loss: 1.85; acc: 0.48
Batch: 680; loss: 2.01; acc: 0.38
Batch: 700; loss: 1.89; acc: 0.36
Batch: 720; loss: 1.88; acc: 0.48
Batch: 740; loss: 1.94; acc: 0.38
Batch: 760; loss: 1.85; acc: 0.44
Batch: 780; loss: 1.94; acc: 0.38
Train Epoch over. train_loss: 1.89; train_accuracy: 0.42 

2.5310744604212232e-05
5.903041255805874e-06
Batch: 0; loss: 1.82; acc: 0.42
Batch: 20; loss: 1.84; acc: 0.44
Batch: 40; loss: 1.69; acc: 0.58
Batch: 60; loss: 1.81; acc: 0.41
Batch: 80; loss: 1.75; acc: 0.47
Batch: 100; loss: 1.82; acc: 0.56
Batch: 120; loss: 1.87; acc: 0.38
Batch: 140; loss: 1.66; acc: 0.55
Val Epoch over. val_loss: 1.8078123649973779; val_accuracy: 0.4632762738853503 

The current subspace-distance is: 5.903041255805874e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.5
Batch: 20; loss: 1.92; acc: 0.36
Batch: 40; loss: 1.93; acc: 0.39
Batch: 60; loss: 1.85; acc: 0.41
Batch: 80; loss: 1.82; acc: 0.47
Batch: 100; loss: 1.75; acc: 0.58
Batch: 120; loss: 1.85; acc: 0.45
Batch: 140; loss: 1.84; acc: 0.47
Batch: 160; loss: 1.79; acc: 0.48
Batch: 180; loss: 1.67; acc: 0.52
Batch: 200; loss: 1.91; acc: 0.38
Batch: 220; loss: 2.03; acc: 0.27
Batch: 240; loss: 1.82; acc: 0.44
Batch: 260; loss: 2.01; acc: 0.27
Batch: 280; loss: 1.81; acc: 0.39
Batch: 300; loss: 1.7; acc: 0.52
Batch: 320; loss: 1.77; acc: 0.47
Batch: 340; loss: 1.81; acc: 0.42
Batch: 360; loss: 1.82; acc: 0.38
Batch: 380; loss: 1.95; acc: 0.36
Batch: 400; loss: 1.86; acc: 0.41
Batch: 420; loss: 1.83; acc: 0.38
Batch: 440; loss: 1.83; acc: 0.42
Batch: 460; loss: 1.81; acc: 0.45
Batch: 480; loss: 1.75; acc: 0.53
Batch: 500; loss: 1.8; acc: 0.39
Batch: 520; loss: 1.95; acc: 0.39
Batch: 540; loss: 1.82; acc: 0.42
Batch: 560; loss: 1.92; acc: 0.36
Batch: 580; loss: 1.65; acc: 0.61
Batch: 600; loss: 1.9; acc: 0.34
Batch: 620; loss: 1.73; acc: 0.5
Batch: 640; loss: 1.95; acc: 0.36
Batch: 660; loss: 1.84; acc: 0.44
Batch: 680; loss: 1.8; acc: 0.45
Batch: 700; loss: 1.8; acc: 0.44
Batch: 720; loss: 1.85; acc: 0.39
Batch: 740; loss: 1.93; acc: 0.36
Batch: 760; loss: 1.76; acc: 0.44
Batch: 780; loss: 1.73; acc: 0.47
Train Epoch over. train_loss: 1.81; train_accuracy: 0.46 

2.7262469302513637e-05
5.004343620385043e-06
Batch: 0; loss: 1.78; acc: 0.44
Batch: 20; loss: 1.77; acc: 0.52
Batch: 40; loss: 1.59; acc: 0.62
Batch: 60; loss: 1.73; acc: 0.44
Batch: 80; loss: 1.68; acc: 0.52
Batch: 100; loss: 1.76; acc: 0.5
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.57; acc: 0.58
Val Epoch over. val_loss: 1.7562061312851633; val_accuracy: 0.4872611464968153 

The current subspace-distance is: 5.004343620385043e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.45
Batch: 20; loss: 1.92; acc: 0.39
Batch: 40; loss: 1.89; acc: 0.42
Batch: 60; loss: 1.72; acc: 0.53
Batch: 80; loss: 1.73; acc: 0.52
Batch: 100; loss: 1.77; acc: 0.53
Batch: 120; loss: 1.66; acc: 0.61
Batch: 140; loss: 1.92; acc: 0.39
Batch: 160; loss: 1.72; acc: 0.52
Batch: 180; loss: 1.75; acc: 0.52
Batch: 200; loss: 1.88; acc: 0.34
Batch: 220; loss: 1.73; acc: 0.52
Batch: 240; loss: 1.66; acc: 0.59
Batch: 260; loss: 1.94; acc: 0.39
Batch: 280; loss: 1.82; acc: 0.44
Batch: 300; loss: 1.91; acc: 0.36
Batch: 320; loss: 1.68; acc: 0.5
Batch: 340; loss: 1.78; acc: 0.48
Batch: 360; loss: 1.78; acc: 0.52
Batch: 380; loss: 1.77; acc: 0.47
Batch: 400; loss: 1.81; acc: 0.47
Batch: 420; loss: 1.74; acc: 0.41
Batch: 440; loss: 1.74; acc: 0.55
Batch: 460; loss: 1.72; acc: 0.58
Batch: 480; loss: 1.73; acc: 0.47
Batch: 500; loss: 1.9; acc: 0.41
Batch: 520; loss: 1.73; acc: 0.45
Batch: 540; loss: 1.94; acc: 0.33
Batch: 560; loss: 1.83; acc: 0.41
Batch: 580; loss: 1.71; acc: 0.53
Batch: 600; loss: 1.88; acc: 0.38
Batch: 620; loss: 1.84; acc: 0.44
Batch: 640; loss: 1.69; acc: 0.56
Batch: 660; loss: 1.77; acc: 0.52
Batch: 680; loss: 1.79; acc: 0.48
Batch: 700; loss: 1.74; acc: 0.5
Batch: 720; loss: 1.79; acc: 0.45
Batch: 740; loss: 1.83; acc: 0.42
Batch: 760; loss: 1.71; acc: 0.56
Batch: 780; loss: 1.85; acc: 0.39
Train Epoch over. train_loss: 1.77; train_accuracy: 0.47 

2.7642445274977945e-05
5.100869202578906e-06
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.78; acc: 0.45
Batch: 40; loss: 1.56; acc: 0.59
Batch: 60; loss: 1.66; acc: 0.44
Batch: 80; loss: 1.65; acc: 0.59
Batch: 100; loss: 1.71; acc: 0.52
Batch: 120; loss: 1.78; acc: 0.45
Batch: 140; loss: 1.53; acc: 0.66
Val Epoch over. val_loss: 1.7243295627035153; val_accuracy: 0.502687101910828 

The current subspace-distance is: 5.100869202578906e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.83; acc: 0.41
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.71; acc: 0.52
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.81; acc: 0.42
Batch: 100; loss: 1.77; acc: 0.39
Batch: 120; loss: 1.76; acc: 0.48
Batch: 140; loss: 1.75; acc: 0.48
Batch: 160; loss: 1.59; acc: 0.58
Batch: 180; loss: 1.72; acc: 0.52
Batch: 200; loss: 1.81; acc: 0.47
Batch: 220; loss: 1.8; acc: 0.47
Batch: 240; loss: 1.65; acc: 0.61
Batch: 260; loss: 1.81; acc: 0.39
Batch: 280; loss: 1.88; acc: 0.44
Batch: 300; loss: 1.59; acc: 0.67
Batch: 320; loss: 1.7; acc: 0.55
Batch: 340; loss: 1.9; acc: 0.38
Batch: 360; loss: 1.73; acc: 0.47
Batch: 380; loss: 1.84; acc: 0.42
Batch: 400; loss: 1.72; acc: 0.56
Batch: 420; loss: 1.63; acc: 0.52
Batch: 440; loss: 1.7; acc: 0.53
Batch: 460; loss: 1.7; acc: 0.55
Batch: 480; loss: 1.76; acc: 0.44
Batch: 500; loss: 1.67; acc: 0.61
Batch: 520; loss: 1.78; acc: 0.48
Batch: 540; loss: 1.68; acc: 0.5
Batch: 560; loss: 1.84; acc: 0.45
Batch: 580; loss: 1.73; acc: 0.59
Batch: 600; loss: 1.65; acc: 0.61
Batch: 620; loss: 1.77; acc: 0.41
Batch: 640; loss: 1.74; acc: 0.56
Batch: 660; loss: 1.61; acc: 0.53
Batch: 680; loss: 1.76; acc: 0.5
Batch: 700; loss: 1.73; acc: 0.45
Batch: 720; loss: 1.79; acc: 0.39
Batch: 740; loss: 1.66; acc: 0.55
Batch: 760; loss: 1.77; acc: 0.44
Batch: 780; loss: 1.83; acc: 0.38
Train Epoch over. train_loss: 1.74; train_accuracy: 0.49 

2.7720549041987397e-05
8.041072760534007e-06
Batch: 0; loss: 1.77; acc: 0.41
Batch: 20; loss: 1.78; acc: 0.47
Batch: 40; loss: 1.51; acc: 0.59
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.58
Batch: 100; loss: 1.68; acc: 0.53
Batch: 120; loss: 1.77; acc: 0.45
Batch: 140; loss: 1.51; acc: 0.62
Val Epoch over. val_loss: 1.6874525934267954; val_accuracy: 0.5225915605095541 

The current subspace-distance is: 8.041072760534007e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.48
Batch: 20; loss: 1.65; acc: 0.64
Batch: 40; loss: 1.66; acc: 0.56
Batch: 60; loss: 1.9; acc: 0.38
Batch: 80; loss: 1.73; acc: 0.52
Batch: 100; loss: 1.69; acc: 0.56
Batch: 120; loss: 1.72; acc: 0.52
Batch: 140; loss: 1.7; acc: 0.56
Batch: 160; loss: 1.94; acc: 0.42
Batch: 180; loss: 1.86; acc: 0.42
Batch: 200; loss: 1.67; acc: 0.48
Batch: 220; loss: 1.74; acc: 0.52
Batch: 240; loss: 1.89; acc: 0.42
Batch: 260; loss: 1.77; acc: 0.48
Batch: 280; loss: 1.54; acc: 0.66
Batch: 300; loss: 1.74; acc: 0.58
Batch: 320; loss: 1.58; acc: 0.55
Batch: 340; loss: 1.85; acc: 0.48
Batch: 360; loss: 1.74; acc: 0.45
Batch: 380; loss: 1.74; acc: 0.52
Batch: 400; loss: 1.63; acc: 0.55
Batch: 420; loss: 1.67; acc: 0.56
Batch: 440; loss: 1.72; acc: 0.47
Batch: 460; loss: 1.76; acc: 0.48
Batch: 480; loss: 1.61; acc: 0.61
Batch: 500; loss: 1.68; acc: 0.55
Batch: 520; loss: 1.8; acc: 0.42
Batch: 540; loss: 1.82; acc: 0.36
Batch: 560; loss: 1.68; acc: 0.45
Batch: 580; loss: 1.6; acc: 0.53
Batch: 600; loss: 1.78; acc: 0.44
Batch: 620; loss: 1.75; acc: 0.48
Batch: 640; loss: 1.72; acc: 0.48
Batch: 660; loss: 1.58; acc: 0.66
Batch: 680; loss: 1.7; acc: 0.48
Batch: 700; loss: 1.78; acc: 0.42
Batch: 720; loss: 1.72; acc: 0.47
Batch: 740; loss: 1.66; acc: 0.59
Batch: 760; loss: 1.79; acc: 0.45
Batch: 780; loss: 1.72; acc: 0.53
Train Epoch over. train_loss: 1.72; train_accuracy: 0.5 

2.9595084924949333e-05
7.378283953585196e-06
Batch: 0; loss: 1.75; acc: 0.45
Batch: 20; loss: 1.8; acc: 0.44
Batch: 40; loss: 1.5; acc: 0.58
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.75; acc: 0.47
Batch: 140; loss: 1.5; acc: 0.64
Val Epoch over. val_loss: 1.6651217238918232; val_accuracy: 0.5295581210191083 

The current subspace-distance is: 7.378283953585196e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.5
Batch: 40; loss: 1.76; acc: 0.47
Batch: 60; loss: 1.65; acc: 0.56
Batch: 80; loss: 1.66; acc: 0.58
Batch: 100; loss: 1.7; acc: 0.47
Batch: 120; loss: 1.65; acc: 0.62
Batch: 140; loss: 1.63; acc: 0.55
Batch: 160; loss: 1.64; acc: 0.56
Batch: 180; loss: 1.71; acc: 0.5
Batch: 200; loss: 1.87; acc: 0.45
Batch: 220; loss: 1.69; acc: 0.5
Batch: 240; loss: 1.59; acc: 0.62
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.61; acc: 0.56
Batch: 300; loss: 1.71; acc: 0.5
Batch: 320; loss: 1.63; acc: 0.56
Batch: 340; loss: 1.58; acc: 0.64
Batch: 360; loss: 1.69; acc: 0.56
Batch: 380; loss: 1.71; acc: 0.48
Batch: 400; loss: 1.64; acc: 0.56
Batch: 420; loss: 1.77; acc: 0.52
Batch: 440; loss: 1.69; acc: 0.5
Batch: 460; loss: 1.71; acc: 0.52
Batch: 480; loss: 1.65; acc: 0.55
Batch: 500; loss: 1.76; acc: 0.5
Batch: 520; loss: 1.7; acc: 0.47
Batch: 540; loss: 1.58; acc: 0.53
Batch: 560; loss: 1.73; acc: 0.52
Batch: 580; loss: 1.6; acc: 0.56
Batch: 600; loss: 1.78; acc: 0.52
Batch: 620; loss: 1.78; acc: 0.39
Batch: 640; loss: 1.74; acc: 0.5
Batch: 660; loss: 1.66; acc: 0.61
Batch: 680; loss: 1.67; acc: 0.5
Batch: 700; loss: 1.65; acc: 0.56
Batch: 720; loss: 1.79; acc: 0.42
Batch: 740; loss: 1.75; acc: 0.42
Batch: 760; loss: 1.72; acc: 0.44
Batch: 780; loss: 1.69; acc: 0.42
Train Epoch over. train_loss: 1.7; train_accuracy: 0.51 

3.015373476955574e-05
6.7860364652005956e-06
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.8; acc: 0.47
Batch: 40; loss: 1.48; acc: 0.55
Batch: 60; loss: 1.57; acc: 0.58
Batch: 80; loss: 1.57; acc: 0.55
Batch: 100; loss: 1.7; acc: 0.5
Batch: 120; loss: 1.75; acc: 0.5
Batch: 140; loss: 1.5; acc: 0.66
Val Epoch over. val_loss: 1.6517617877121944; val_accuracy: 0.5372213375796179 

The current subspace-distance is: 6.7860364652005956e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.5
Batch: 20; loss: 1.6; acc: 0.58
Batch: 40; loss: 1.66; acc: 0.53
Batch: 60; loss: 1.82; acc: 0.45
Batch: 80; loss: 1.77; acc: 0.44
Batch: 100; loss: 1.75; acc: 0.47
Batch: 120; loss: 1.6; acc: 0.55
Batch: 140; loss: 1.6; acc: 0.53
Batch: 160; loss: 1.62; acc: 0.56
Batch: 180; loss: 1.62; acc: 0.59
Batch: 200; loss: 1.58; acc: 0.62
Batch: 220; loss: 1.67; acc: 0.53
Batch: 240; loss: 1.67; acc: 0.5
Batch: 260; loss: 1.69; acc: 0.45
Batch: 280; loss: 1.61; acc: 0.5
Batch: 300; loss: 1.57; acc: 0.61
Batch: 320; loss: 1.59; acc: 0.59
Batch: 340; loss: 1.66; acc: 0.58
Batch: 360; loss: 1.65; acc: 0.56
Batch: 380; loss: 1.61; acc: 0.55
Batch: 400; loss: 1.64; acc: 0.55
Batch: 420; loss: 1.87; acc: 0.39
Batch: 440; loss: 1.65; acc: 0.56
Batch: 460; loss: 1.68; acc: 0.5
Batch: 480; loss: 1.68; acc: 0.56
Batch: 500; loss: 1.66; acc: 0.53
Batch: 520; loss: 1.56; acc: 0.56
Batch: 540; loss: 1.67; acc: 0.5
Batch: 560; loss: 1.6; acc: 0.58
Batch: 580; loss: 1.57; acc: 0.48
Batch: 600; loss: 1.65; acc: 0.53
Batch: 620; loss: 1.72; acc: 0.47
Batch: 640; loss: 1.67; acc: 0.55
Batch: 660; loss: 1.9; acc: 0.41
Batch: 680; loss: 1.73; acc: 0.47
Batch: 700; loss: 1.75; acc: 0.45
Batch: 720; loss: 1.53; acc: 0.64
Batch: 740; loss: 1.59; acc: 0.56
Batch: 760; loss: 1.63; acc: 0.52
Batch: 780; loss: 1.51; acc: 0.62
Train Epoch over. train_loss: 1.68; train_accuracy: 0.51 

3.117839150945656e-05
8.74025408847956e-06
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.8; acc: 0.41
Batch: 40; loss: 1.45; acc: 0.61
Batch: 60; loss: 1.57; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.44
Batch: 140; loss: 1.5; acc: 0.62
Val Epoch over. val_loss: 1.6265530305303586; val_accuracy: 0.5392117834394905 

The current subspace-distance is: 8.74025408847956e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.57; acc: 0.62
Batch: 20; loss: 1.58; acc: 0.59
Batch: 40; loss: 1.75; acc: 0.44
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.63; acc: 0.53
Batch: 100; loss: 1.66; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.71; acc: 0.48
Batch: 160; loss: 1.68; acc: 0.48
Batch: 180; loss: 1.55; acc: 0.56
Batch: 200; loss: 1.72; acc: 0.44
Batch: 220; loss: 1.7; acc: 0.53
Batch: 240; loss: 1.67; acc: 0.52
Batch: 260; loss: 1.79; acc: 0.42
Batch: 280; loss: 1.6; acc: 0.47
Batch: 300; loss: 1.67; acc: 0.55
Batch: 320; loss: 1.57; acc: 0.58
Batch: 340; loss: 1.7; acc: 0.5
Batch: 360; loss: 1.59; acc: 0.62
Batch: 380; loss: 1.78; acc: 0.5
Batch: 400; loss: 1.58; acc: 0.52
Batch: 420; loss: 1.72; acc: 0.42
Batch: 440; loss: 1.7; acc: 0.5
Batch: 460; loss: 1.56; acc: 0.5
Batch: 480; loss: 1.63; acc: 0.56
Batch: 500; loss: 1.67; acc: 0.47
Batch: 520; loss: 1.63; acc: 0.48
Batch: 540; loss: 1.69; acc: 0.5
Batch: 560; loss: 1.71; acc: 0.48
Batch: 580; loss: 1.55; acc: 0.59
Batch: 600; loss: 1.64; acc: 0.53
Batch: 620; loss: 1.61; acc: 0.58
Batch: 640; loss: 1.64; acc: 0.55
Batch: 660; loss: 1.69; acc: 0.52
Batch: 680; loss: 1.62; acc: 0.5
Batch: 700; loss: 1.58; acc: 0.56
Batch: 720; loss: 1.81; acc: 0.47
Batch: 740; loss: 1.54; acc: 0.59
Batch: 760; loss: 1.66; acc: 0.48
Batch: 780; loss: 1.75; acc: 0.44
Train Epoch over. train_loss: 1.66; train_accuracy: 0.51 

3.181647844030522e-05
9.684167707746383e-06
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.81; acc: 0.36
Batch: 40; loss: 1.43; acc: 0.55
Batch: 60; loss: 1.58; acc: 0.59
Batch: 80; loss: 1.52; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.5
Batch: 120; loss: 1.76; acc: 0.44
Batch: 140; loss: 1.55; acc: 0.58
Val Epoch over. val_loss: 1.619423419806608; val_accuracy: 0.5351313694267515 

The current subspace-distance is: 9.684167707746383e-06 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.44
Batch: 40; loss: 1.68; acc: 0.58
Batch: 60; loss: 1.92; acc: 0.31
Batch: 80; loss: 1.49; acc: 0.61
Batch: 100; loss: 1.72; acc: 0.5
Batch: 120; loss: 1.65; acc: 0.48
Batch: 140; loss: 1.63; acc: 0.55
Batch: 160; loss: 1.68; acc: 0.52
Batch: 180; loss: 1.59; acc: 0.59
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.68; acc: 0.53
Batch: 240; loss: 1.63; acc: 0.52
Batch: 260; loss: 1.66; acc: 0.5
Batch: 280; loss: 1.61; acc: 0.52
Batch: 300; loss: 1.6; acc: 0.59
Batch: 320; loss: 1.74; acc: 0.5
Batch: 340; loss: 1.84; acc: 0.34
Batch: 360; loss: 1.6; acc: 0.5
Batch: 380; loss: 1.74; acc: 0.52
Batch: 400; loss: 1.63; acc: 0.52
Batch: 420; loss: 1.6; acc: 0.53
Batch: 440; loss: 1.57; acc: 0.5
Batch: 460; loss: 1.72; acc: 0.48
Batch: 480; loss: 1.7; acc: 0.48
Batch: 500; loss: 1.64; acc: 0.59
Batch: 520; loss: 1.69; acc: 0.48
Batch: 540; loss: 1.67; acc: 0.59
Batch: 560; loss: 1.76; acc: 0.41
Batch: 580; loss: 1.72; acc: 0.5
Batch: 600; loss: 1.64; acc: 0.44
Batch: 620; loss: 1.57; acc: 0.64
Batch: 640; loss: 1.49; acc: 0.61
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.63; acc: 0.5
Batch: 700; loss: 1.74; acc: 0.44
Batch: 720; loss: 1.56; acc: 0.61
Batch: 740; loss: 1.48; acc: 0.59
Batch: 760; loss: 1.63; acc: 0.56
Batch: 780; loss: 1.63; acc: 0.47
Train Epoch over. train_loss: 1.65; train_accuracy: 0.52 

3.3182717743329704e-05
9.453723578189965e-06
Batch: 0; loss: 1.66; acc: 0.5
Batch: 20; loss: 1.79; acc: 0.36
Batch: 40; loss: 1.39; acc: 0.58
Batch: 60; loss: 1.57; acc: 0.56
Batch: 80; loss: 1.48; acc: 0.61
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.54; acc: 0.56
Val Epoch over. val_loss: 1.5925657172111949; val_accuracy: 0.5399084394904459 

The current subspace-distance is: 9.453723578189965e-06 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.53; acc: 0.61
Batch: 60; loss: 1.74; acc: 0.42
Batch: 80; loss: 1.52; acc: 0.52
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.57; acc: 0.62
Batch: 160; loss: 1.62; acc: 0.58
Batch: 180; loss: 1.65; acc: 0.5
Batch: 200; loss: 1.82; acc: 0.44
Batch: 220; loss: 1.7; acc: 0.44
Batch: 240; loss: 1.67; acc: 0.52
Batch: 260; loss: 1.77; acc: 0.55
Batch: 280; loss: 1.58; acc: 0.55
Batch: 300; loss: 1.62; acc: 0.55
Batch: 320; loss: 1.73; acc: 0.44
Batch: 340; loss: 1.76; acc: 0.44
Batch: 360; loss: 1.55; acc: 0.66
Batch: 380; loss: 1.6; acc: 0.52
Batch: 400; loss: 1.56; acc: 0.55
Batch: 420; loss: 1.65; acc: 0.48
Batch: 440; loss: 1.66; acc: 0.55
Batch: 460; loss: 1.63; acc: 0.5
Batch: 480; loss: 1.61; acc: 0.59
Batch: 500; loss: 1.59; acc: 0.44
Batch: 520; loss: 1.77; acc: 0.41
Batch: 540; loss: 1.66; acc: 0.48
Batch: 560; loss: 1.63; acc: 0.47
Batch: 580; loss: 1.68; acc: 0.56
Batch: 600; loss: 1.72; acc: 0.45
Batch: 620; loss: 1.57; acc: 0.55
Batch: 640; loss: 1.62; acc: 0.52
Batch: 660; loss: 1.62; acc: 0.48
Batch: 680; loss: 1.59; acc: 0.59
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.49; acc: 0.59
Batch: 740; loss: 1.69; acc: 0.5
Batch: 760; loss: 1.72; acc: 0.47
Batch: 780; loss: 1.54; acc: 0.56
Train Epoch over. train_loss: 1.63; train_accuracy: 0.52 

3.427146657486446e-05
8.60013642522972e-06
Batch: 0; loss: 1.68; acc: 0.53
Batch: 20; loss: 1.8; acc: 0.39
Batch: 40; loss: 1.39; acc: 0.61
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.48; acc: 0.59
Batch: 100; loss: 1.64; acc: 0.53
Batch: 120; loss: 1.75; acc: 0.47
Batch: 140; loss: 1.54; acc: 0.59
Val Epoch over. val_loss: 1.599668382079738; val_accuracy: 0.5465764331210191 

The current subspace-distance is: 8.60013642522972e-06 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.58
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 1.74; acc: 0.47
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.61; acc: 0.5
Batch: 100; loss: 1.69; acc: 0.44
Batch: 120; loss: 1.77; acc: 0.39
Batch: 140; loss: 1.63; acc: 0.5
Batch: 160; loss: 1.65; acc: 0.47
Batch: 180; loss: 1.51; acc: 0.61
Batch: 200; loss: 1.62; acc: 0.56
Batch: 220; loss: 1.67; acc: 0.52
Batch: 240; loss: 1.52; acc: 0.62
Batch: 260; loss: 1.71; acc: 0.5
Batch: 280; loss: 1.58; acc: 0.55
Batch: 300; loss: 1.6; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.55
Batch: 340; loss: 1.64; acc: 0.56
Batch: 360; loss: 1.67; acc: 0.48
Batch: 380; loss: 1.71; acc: 0.44
Batch: 400; loss: 1.77; acc: 0.47
Batch: 420; loss: 1.52; acc: 0.56
Batch: 440; loss: 1.64; acc: 0.53
Batch: 460; loss: 1.59; acc: 0.58
Batch: 480; loss: 1.67; acc: 0.55
Batch: 500; loss: 1.51; acc: 0.58
Batch: 520; loss: 1.6; acc: 0.52
Batch: 540; loss: 1.74; acc: 0.39
Batch: 560; loss: 1.67; acc: 0.5
Batch: 580; loss: 1.5; acc: 0.56
Batch: 600; loss: 1.58; acc: 0.52
Batch: 620; loss: 1.73; acc: 0.44
Batch: 640; loss: 1.69; acc: 0.59
Batch: 660; loss: 1.6; acc: 0.56
Batch: 680; loss: 1.54; acc: 0.62
Batch: 700; loss: 1.63; acc: 0.52
Batch: 720; loss: 1.64; acc: 0.48
Batch: 740; loss: 1.53; acc: 0.58
Batch: 760; loss: 1.72; acc: 0.48
Batch: 780; loss: 1.58; acc: 0.53
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.434143218328245e-05
9.134038009506185e-06
Batch: 0; loss: 1.67; acc: 0.55
Batch: 20; loss: 1.79; acc: 0.38
Batch: 40; loss: 1.38; acc: 0.61
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.47; acc: 0.61
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.53; acc: 0.58
Val Epoch over. val_loss: 1.5916953694288898; val_accuracy: 0.5485668789808917 

The current subspace-distance is: 9.134038009506185e-06 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.66
Batch: 20; loss: 1.69; acc: 0.47
Batch: 40; loss: 1.68; acc: 0.52
Batch: 60; loss: 1.81; acc: 0.33
Batch: 80; loss: 1.57; acc: 0.52
Batch: 100; loss: 1.5; acc: 0.64
Batch: 120; loss: 1.77; acc: 0.39
Batch: 140; loss: 1.56; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.55
Batch: 180; loss: 1.59; acc: 0.48
Batch: 200; loss: 1.57; acc: 0.55
Batch: 220; loss: 1.64; acc: 0.47
Batch: 240; loss: 1.64; acc: 0.5
Batch: 260; loss: 1.62; acc: 0.53
Batch: 280; loss: 1.64; acc: 0.55
Batch: 300; loss: 1.66; acc: 0.5
Batch: 320; loss: 1.5; acc: 0.59
Batch: 340; loss: 1.62; acc: 0.56
Batch: 360; loss: 1.54; acc: 0.58
Batch: 380; loss: 1.57; acc: 0.53
Batch: 400; loss: 1.61; acc: 0.48
Batch: 420; loss: 1.58; acc: 0.58
Batch: 440; loss: 1.65; acc: 0.59
Batch: 460; loss: 1.52; acc: 0.64
Batch: 480; loss: 1.7; acc: 0.45
Batch: 500; loss: 1.63; acc: 0.45
Batch: 520; loss: 1.62; acc: 0.53
Batch: 540; loss: 1.66; acc: 0.47
Batch: 560; loss: 1.57; acc: 0.53
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.53; acc: 0.58
Batch: 620; loss: 1.38; acc: 0.62
Batch: 640; loss: 1.71; acc: 0.44
Batch: 660; loss: 1.69; acc: 0.44
Batch: 680; loss: 1.63; acc: 0.52
Batch: 700; loss: 1.52; acc: 0.62
Batch: 720; loss: 1.68; acc: 0.44
Batch: 740; loss: 1.62; acc: 0.45
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.47
Train Epoch over. train_loss: 1.62; train_accuracy: 0.53 

3.5402288631303236e-05
1.1584488674998283e-05
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.77; acc: 0.36
Batch: 40; loss: 1.35; acc: 0.61
Batch: 60; loss: 1.56; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.62
Batch: 100; loss: 1.61; acc: 0.5
Batch: 120; loss: 1.74; acc: 0.42
Batch: 140; loss: 1.51; acc: 0.59
Val Epoch over. val_loss: 1.576203615042814; val_accuracy: 0.550656847133758 

The current subspace-distance is: 1.1584488674998283e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.49; acc: 0.58
Batch: 40; loss: 1.62; acc: 0.55
Batch: 60; loss: 1.57; acc: 0.61
Batch: 80; loss: 1.73; acc: 0.5
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.65; acc: 0.59
Batch: 140; loss: 1.67; acc: 0.52
Batch: 160; loss: 1.56; acc: 0.59
Batch: 180; loss: 1.54; acc: 0.59
Batch: 200; loss: 1.6; acc: 0.52
Batch: 220; loss: 1.56; acc: 0.61
Batch: 240; loss: 1.62; acc: 0.5
Batch: 260; loss: 1.65; acc: 0.45
Batch: 280; loss: 1.76; acc: 0.44
Batch: 300; loss: 1.85; acc: 0.39
Batch: 320; loss: 1.57; acc: 0.5
Batch: 340; loss: 1.58; acc: 0.66
Batch: 360; loss: 1.67; acc: 0.53
Batch: 380; loss: 1.62; acc: 0.5
Batch: 400; loss: 1.49; acc: 0.61
Batch: 420; loss: 1.62; acc: 0.52
Batch: 440; loss: 1.58; acc: 0.58
Batch: 460; loss: 1.62; acc: 0.55
Batch: 480; loss: 1.74; acc: 0.48
Batch: 500; loss: 1.72; acc: 0.44
Batch: 520; loss: 1.63; acc: 0.55
Batch: 540; loss: 1.64; acc: 0.5
Batch: 560; loss: 1.63; acc: 0.55
Batch: 580; loss: 1.64; acc: 0.52
Batch: 600; loss: 1.68; acc: 0.48
Batch: 620; loss: 1.38; acc: 0.55
Batch: 640; loss: 1.64; acc: 0.45
Batch: 660; loss: 1.6; acc: 0.53
Batch: 680; loss: 1.63; acc: 0.52
Batch: 700; loss: 1.51; acc: 0.62
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.56; acc: 0.62
Batch: 760; loss: 1.71; acc: 0.53
Batch: 780; loss: 1.55; acc: 0.62
Train Epoch over. train_loss: 1.62; train_accuracy: 0.53 

3.61482088919729e-05
9.655145731812809e-06
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.77; acc: 0.38
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.72; acc: 0.45
Batch: 140; loss: 1.52; acc: 0.59
Val Epoch over. val_loss: 1.5731045319016572; val_accuracy: 0.5522492038216561 

The current subspace-distance is: 9.655145731812809e-06 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.69; acc: 0.5
Batch: 20; loss: 1.61; acc: 0.58
Batch: 40; loss: 1.65; acc: 0.45
Batch: 60; loss: 1.7; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.55
Batch: 100; loss: 1.65; acc: 0.48
Batch: 120; loss: 1.53; acc: 0.59
Batch: 140; loss: 1.53; acc: 0.59
Batch: 160; loss: 1.7; acc: 0.47
Batch: 180; loss: 1.67; acc: 0.5
Batch: 200; loss: 1.62; acc: 0.48
Batch: 220; loss: 1.54; acc: 0.56
Batch: 240; loss: 1.67; acc: 0.56
Batch: 260; loss: 1.58; acc: 0.55
Batch: 280; loss: 1.63; acc: 0.55
Batch: 300; loss: 1.64; acc: 0.53
Batch: 320; loss: 1.68; acc: 0.48
Batch: 340; loss: 1.54; acc: 0.56
Batch: 360; loss: 1.54; acc: 0.56
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.58
Batch: 420; loss: 1.65; acc: 0.55
Batch: 440; loss: 1.65; acc: 0.44
Batch: 460; loss: 1.57; acc: 0.56
Batch: 480; loss: 1.75; acc: 0.47
Batch: 500; loss: 1.67; acc: 0.47
Batch: 520; loss: 1.59; acc: 0.59
Batch: 540; loss: 1.61; acc: 0.52
Batch: 560; loss: 1.7; acc: 0.5
Batch: 580; loss: 1.63; acc: 0.58
Batch: 600; loss: 1.59; acc: 0.5
Batch: 620; loss: 1.62; acc: 0.48
Batch: 640; loss: 1.52; acc: 0.64
Batch: 660; loss: 1.47; acc: 0.66
Batch: 680; loss: 1.6; acc: 0.55
Batch: 700; loss: 1.61; acc: 0.59
Batch: 720; loss: 1.68; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.74; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.53 

3.5959066735813394e-05
1.0411693438072689e-05
Batch: 0; loss: 1.64; acc: 0.52
Batch: 20; loss: 1.77; acc: 0.38
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.57; acc: 0.5
Batch: 80; loss: 1.44; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.61
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.52; acc: 0.56
Val Epoch over. val_loss: 1.5734216567057713; val_accuracy: 0.5569267515923567 

The current subspace-distance is: 1.0411693438072689e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.71; acc: 0.53
Batch: 40; loss: 1.64; acc: 0.5
Batch: 60; loss: 1.69; acc: 0.47
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.39; acc: 0.73
Batch: 120; loss: 1.61; acc: 0.55
Batch: 140; loss: 1.53; acc: 0.61
Batch: 160; loss: 1.58; acc: 0.58
Batch: 180; loss: 1.65; acc: 0.52
Batch: 200; loss: 1.64; acc: 0.58
Batch: 220; loss: 1.57; acc: 0.59
Batch: 240; loss: 1.52; acc: 0.64
Batch: 260; loss: 1.84; acc: 0.41
Batch: 280; loss: 1.51; acc: 0.59
Batch: 300; loss: 1.56; acc: 0.58
Batch: 320; loss: 1.67; acc: 0.44
Batch: 340; loss: 1.45; acc: 0.69
Batch: 360; loss: 1.64; acc: 0.5
Batch: 380; loss: 1.59; acc: 0.5
Batch: 400; loss: 1.57; acc: 0.62
Batch: 420; loss: 1.63; acc: 0.56
Batch: 440; loss: 1.56; acc: 0.61
Batch: 460; loss: 1.61; acc: 0.64
Batch: 480; loss: 1.68; acc: 0.53
Batch: 500; loss: 1.54; acc: 0.56
Batch: 520; loss: 1.66; acc: 0.5
Batch: 540; loss: 1.69; acc: 0.52
Batch: 560; loss: 1.81; acc: 0.42
Batch: 580; loss: 1.65; acc: 0.44
Batch: 600; loss: 1.71; acc: 0.47
Batch: 620; loss: 1.57; acc: 0.52
Batch: 640; loss: 1.51; acc: 0.56
Batch: 660; loss: 1.58; acc: 0.55
Batch: 680; loss: 1.55; acc: 0.56
Batch: 700; loss: 1.56; acc: 0.53
Batch: 720; loss: 1.47; acc: 0.64
Batch: 740; loss: 1.62; acc: 0.47
Batch: 760; loss: 1.63; acc: 0.5
Batch: 780; loss: 1.54; acc: 0.55
Train Epoch over. train_loss: 1.6; train_accuracy: 0.54 

3.581469354685396e-05
7.719061613897793e-06
Batch: 0; loss: 1.63; acc: 0.55
Batch: 20; loss: 1.76; acc: 0.36
Batch: 40; loss: 1.29; acc: 0.59
Batch: 60; loss: 1.56; acc: 0.5
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.71; acc: 0.44
Batch: 140; loss: 1.51; acc: 0.55
Val Epoch over. val_loss: 1.564446416630107; val_accuracy: 0.5507563694267515 

The current subspace-distance is: 7.719061613897793e-06 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.56
Batch: 20; loss: 1.66; acc: 0.47
Batch: 40; loss: 1.56; acc: 0.58
Batch: 60; loss: 1.57; acc: 0.55
Batch: 80; loss: 1.64; acc: 0.52
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.58; acc: 0.58
Batch: 160; loss: 1.68; acc: 0.48
Batch: 180; loss: 1.61; acc: 0.56
Batch: 200; loss: 1.51; acc: 0.59
Batch: 220; loss: 1.58; acc: 0.53
Batch: 240; loss: 1.52; acc: 0.55
Batch: 260; loss: 1.56; acc: 0.53
Batch: 280; loss: 1.69; acc: 0.5
Batch: 300; loss: 1.62; acc: 0.48
Batch: 320; loss: 1.58; acc: 0.53
Batch: 340; loss: 1.48; acc: 0.61
Batch: 360; loss: 1.69; acc: 0.45
Batch: 380; loss: 1.63; acc: 0.42
Batch: 400; loss: 1.56; acc: 0.52
Batch: 420; loss: 1.42; acc: 0.69
Batch: 440; loss: 1.7; acc: 0.5
Batch: 460; loss: 1.66; acc: 0.48
Batch: 480; loss: 1.62; acc: 0.52
Batch: 500; loss: 1.57; acc: 0.53
Batch: 520; loss: 1.53; acc: 0.58
Batch: 540; loss: 1.51; acc: 0.61
Batch: 560; loss: 1.58; acc: 0.59
Batch: 580; loss: 1.63; acc: 0.47
Batch: 600; loss: 1.69; acc: 0.41
Batch: 620; loss: 1.61; acc: 0.52
Batch: 640; loss: 1.62; acc: 0.47
Batch: 660; loss: 1.55; acc: 0.56
Batch: 680; loss: 1.49; acc: 0.61
Batch: 700; loss: 1.52; acc: 0.64
Batch: 720; loss: 1.58; acc: 0.52
Batch: 740; loss: 1.44; acc: 0.69
Batch: 760; loss: 1.61; acc: 0.52
Batch: 780; loss: 1.59; acc: 0.48
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

3.7877409340580925e-05
1.1836245903396048e-05
Batch: 0; loss: 1.6; acc: 0.52
Batch: 20; loss: 1.72; acc: 0.41
Batch: 40; loss: 1.26; acc: 0.66
Batch: 60; loss: 1.52; acc: 0.52
Batch: 80; loss: 1.38; acc: 0.64
Batch: 100; loss: 1.56; acc: 0.61
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.56
Val Epoch over. val_loss: 1.5403783746585724; val_accuracy: 0.5686703821656051 

The current subspace-distance is: 1.1836245903396048e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.65; acc: 0.55
Batch: 40; loss: 1.6; acc: 0.59
Batch: 60; loss: 1.6; acc: 0.47
Batch: 80; loss: 1.47; acc: 0.67
Batch: 100; loss: 1.64; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.55
Batch: 140; loss: 1.73; acc: 0.47
Batch: 160; loss: 1.59; acc: 0.53
Batch: 180; loss: 1.6; acc: 0.44
Batch: 200; loss: 1.84; acc: 0.45
Batch: 220; loss: 1.53; acc: 0.59
Batch: 240; loss: 1.61; acc: 0.56
Batch: 260; loss: 1.55; acc: 0.59
Batch: 280; loss: 1.58; acc: 0.5
Batch: 300; loss: 1.65; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.67
Batch: 340; loss: 1.5; acc: 0.62
Batch: 360; loss: 1.46; acc: 0.62
Batch: 380; loss: 1.64; acc: 0.5
Batch: 400; loss: 1.58; acc: 0.59
Batch: 420; loss: 1.62; acc: 0.56
Batch: 440; loss: 1.68; acc: 0.42
Batch: 460; loss: 1.63; acc: 0.52
Batch: 480; loss: 1.51; acc: 0.59
Batch: 500; loss: 1.5; acc: 0.61
Batch: 520; loss: 1.56; acc: 0.56
Batch: 540; loss: 1.54; acc: 0.52
Batch: 560; loss: 1.56; acc: 0.55
Batch: 580; loss: 1.46; acc: 0.64
Batch: 600; loss: 1.49; acc: 0.55
Batch: 620; loss: 1.48; acc: 0.69
Batch: 640; loss: 1.5; acc: 0.55
Batch: 660; loss: 1.58; acc: 0.52
Batch: 680; loss: 1.62; acc: 0.47
Batch: 700; loss: 1.64; acc: 0.47
Batch: 720; loss: 1.65; acc: 0.53
Batch: 740; loss: 1.64; acc: 0.44
Batch: 760; loss: 1.53; acc: 0.62
Batch: 780; loss: 1.7; acc: 0.52
Train Epoch over. train_loss: 1.58; train_accuracy: 0.55 

3.868464409606531e-05
1.0801195458043367e-05
Batch: 0; loss: 1.59; acc: 0.55
Batch: 20; loss: 1.7; acc: 0.44
Batch: 40; loss: 1.25; acc: 0.66
Batch: 60; loss: 1.51; acc: 0.53
Batch: 80; loss: 1.36; acc: 0.67
Batch: 100; loss: 1.56; acc: 0.64
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.48; acc: 0.55
Val Epoch over. val_loss: 1.5321640171063173; val_accuracy: 0.5732484076433121 

The current subspace-distance is: 1.0801195458043367e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.61
Batch: 20; loss: 1.51; acc: 0.58
Batch: 40; loss: 1.65; acc: 0.53
Batch: 60; loss: 1.46; acc: 0.62
Batch: 80; loss: 1.56; acc: 0.59
Batch: 100; loss: 1.56; acc: 0.56
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.7; acc: 0.48
Batch: 160; loss: 1.49; acc: 0.58
Batch: 180; loss: 1.57; acc: 0.55
Batch: 200; loss: 1.5; acc: 0.59
Batch: 220; loss: 1.67; acc: 0.53
Batch: 240; loss: 1.57; acc: 0.55
Batch: 260; loss: 1.55; acc: 0.59
Batch: 280; loss: 1.66; acc: 0.44
Batch: 300; loss: 1.51; acc: 0.47
Batch: 320; loss: 1.53; acc: 0.61
Batch: 340; loss: 1.62; acc: 0.52
Batch: 360; loss: 1.56; acc: 0.55
Batch: 380; loss: 1.62; acc: 0.45
Batch: 400; loss: 1.42; acc: 0.69
Batch: 420; loss: 1.67; acc: 0.47
Batch: 440; loss: 1.46; acc: 0.59
Batch: 460; loss: 1.68; acc: 0.38
Batch: 480; loss: 1.58; acc: 0.53
Batch: 500; loss: 1.69; acc: 0.42
Batch: 520; loss: 1.61; acc: 0.47
Batch: 540; loss: 1.72; acc: 0.44
Batch: 560; loss: 1.58; acc: 0.55
Batch: 580; loss: 1.48; acc: 0.59
Batch: 600; loss: 1.61; acc: 0.61
Batch: 620; loss: 1.59; acc: 0.48
Batch: 640; loss: 1.65; acc: 0.52
Batch: 660; loss: 1.49; acc: 0.58
Batch: 680; loss: 1.5; acc: 0.66
Batch: 700; loss: 1.49; acc: 0.58
Batch: 720; loss: 1.51; acc: 0.62
Batch: 740; loss: 1.61; acc: 0.48
Batch: 760; loss: 1.57; acc: 0.52
Batch: 780; loss: 1.69; acc: 0.47
Train Epoch over. train_loss: 1.57; train_accuracy: 0.55 

3.9531627407995984e-05
1.3135771041561384e-05
Batch: 0; loss: 1.58; acc: 0.58
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.23; acc: 0.67
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.54; acc: 0.66
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.46; acc: 0.55
Val Epoch over. val_loss: 1.5196620947236468; val_accuracy: 0.5747412420382165 

The current subspace-distance is: 1.3135771041561384e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.58
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.57; acc: 0.53
Batch: 60; loss: 1.52; acc: 0.62
Batch: 80; loss: 1.38; acc: 0.61
Batch: 100; loss: 1.78; acc: 0.44
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.51; acc: 0.53
Batch: 160; loss: 1.6; acc: 0.55
Batch: 180; loss: 1.61; acc: 0.52
Batch: 200; loss: 1.51; acc: 0.61
Batch: 220; loss: 1.59; acc: 0.5
Batch: 240; loss: 1.47; acc: 0.58
Batch: 260; loss: 1.53; acc: 0.55
Batch: 280; loss: 1.58; acc: 0.5
Batch: 300; loss: 1.48; acc: 0.61
Batch: 320; loss: 1.65; acc: 0.5
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.55; acc: 0.58
Batch: 380; loss: 1.51; acc: 0.66
Batch: 400; loss: 1.53; acc: 0.59
Batch: 420; loss: 1.66; acc: 0.52
Batch: 440; loss: 1.5; acc: 0.61
Batch: 460; loss: 1.48; acc: 0.64
Batch: 480; loss: 1.52; acc: 0.56
Batch: 500; loss: 1.54; acc: 0.59
Batch: 520; loss: 1.45; acc: 0.66
Batch: 540; loss: 1.59; acc: 0.55
Batch: 560; loss: 1.72; acc: 0.5
Batch: 580; loss: 1.5; acc: 0.56
Batch: 600; loss: 1.67; acc: 0.47
Batch: 620; loss: 1.52; acc: 0.64
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.48; acc: 0.59
Batch: 680; loss: 1.55; acc: 0.55
Batch: 700; loss: 1.46; acc: 0.61
Batch: 720; loss: 1.64; acc: 0.47
Batch: 740; loss: 1.55; acc: 0.47
Batch: 760; loss: 1.57; acc: 0.52
Batch: 780; loss: 1.54; acc: 0.62
Train Epoch over. train_loss: 1.56; train_accuracy: 0.55 

3.855525210383348e-05
9.832941032072995e-06
Batch: 0; loss: 1.57; acc: 0.56
Batch: 20; loss: 1.65; acc: 0.41
Batch: 40; loss: 1.22; acc: 0.69
Batch: 60; loss: 1.48; acc: 0.58
Batch: 80; loss: 1.32; acc: 0.7
Batch: 100; loss: 1.55; acc: 0.62
Batch: 120; loss: 1.65; acc: 0.52
Batch: 140; loss: 1.46; acc: 0.5
Val Epoch over. val_loss: 1.5165564710167563; val_accuracy: 0.5755374203821656 

The current subspace-distance is: 9.832941032072995e-06 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.52
Batch: 20; loss: 1.65; acc: 0.52
Batch: 40; loss: 1.55; acc: 0.52
Batch: 60; loss: 1.52; acc: 0.67
Batch: 80; loss: 1.5; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.62
Batch: 120; loss: 1.46; acc: 0.64
Batch: 140; loss: 1.49; acc: 0.61
Batch: 160; loss: 1.52; acc: 0.58
Batch: 180; loss: 1.52; acc: 0.59
Batch: 200; loss: 1.55; acc: 0.64
Batch: 220; loss: 1.57; acc: 0.55
Batch: 240; loss: 1.61; acc: 0.55
Batch: 260; loss: 1.65; acc: 0.5
Batch: 280; loss: 1.59; acc: 0.53
Batch: 300; loss: 1.55; acc: 0.5
Batch: 320; loss: 1.65; acc: 0.44
Batch: 340; loss: 1.63; acc: 0.52
Batch: 360; loss: 1.53; acc: 0.55
Batch: 380; loss: 1.5; acc: 0.61
Batch: 400; loss: 1.4; acc: 0.64
Batch: 420; loss: 1.65; acc: 0.5
Batch: 440; loss: 1.67; acc: 0.53
Batch: 460; loss: 1.6; acc: 0.52
Batch: 480; loss: 1.59; acc: 0.52
Batch: 500; loss: 1.54; acc: 0.58
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.51; acc: 0.62
Batch: 560; loss: 1.55; acc: 0.56
Batch: 580; loss: 1.65; acc: 0.52
Batch: 600; loss: 1.72; acc: 0.45
Batch: 620; loss: 1.47; acc: 0.7
Batch: 640; loss: 1.58; acc: 0.53
Batch: 660; loss: 1.49; acc: 0.59
Batch: 680; loss: 1.48; acc: 0.61
Batch: 700; loss: 1.66; acc: 0.53
Batch: 720; loss: 1.62; acc: 0.5
Batch: 740; loss: 1.65; acc: 0.5
Batch: 760; loss: 1.4; acc: 0.61
Batch: 780; loss: 1.52; acc: 0.61
Train Epoch over. train_loss: 1.55; train_accuracy: 0.55 

3.974321953137405e-05
1.080058154911967e-05
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.64; acc: 0.45
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.46; acc: 0.58
Batch: 80; loss: 1.31; acc: 0.7
Batch: 100; loss: 1.54; acc: 0.64
Batch: 120; loss: 1.63; acc: 0.5
Batch: 140; loss: 1.45; acc: 0.56
Val Epoch over. val_loss: 1.5046292080241404; val_accuracy: 0.5813097133757962 

The current subspace-distance is: 1.080058154911967e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.64
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.66; acc: 0.44
Batch: 60; loss: 1.6; acc: 0.47
Batch: 80; loss: 1.47; acc: 0.66
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 1.58; acc: 0.52
Batch: 160; loss: 1.54; acc: 0.55
Batch: 180; loss: 1.62; acc: 0.52
Batch: 200; loss: 1.61; acc: 0.47
Batch: 220; loss: 1.55; acc: 0.53
Batch: 240; loss: 1.56; acc: 0.62
Batch: 260; loss: 1.55; acc: 0.52
Batch: 280; loss: 1.68; acc: 0.44
Batch: 300; loss: 1.46; acc: 0.59
Batch: 320; loss: 1.47; acc: 0.58
Batch: 340; loss: 1.71; acc: 0.52
Batch: 360; loss: 1.54; acc: 0.48
Batch: 380; loss: 1.54; acc: 0.55
Batch: 400; loss: 1.54; acc: 0.47
Batch: 420; loss: 1.44; acc: 0.59
Batch: 440; loss: 1.59; acc: 0.62
Batch: 460; loss: 1.6; acc: 0.53
Batch: 480; loss: 1.5; acc: 0.59
Batch: 500; loss: 1.48; acc: 0.59
Batch: 520; loss: 1.37; acc: 0.69
Batch: 540; loss: 1.58; acc: 0.56
Batch: 560; loss: 1.55; acc: 0.52
Batch: 580; loss: 1.61; acc: 0.52
Batch: 600; loss: 1.54; acc: 0.56
Batch: 620; loss: 1.35; acc: 0.69
Batch: 640; loss: 1.68; acc: 0.5
Batch: 660; loss: 1.51; acc: 0.58
Batch: 680; loss: 1.57; acc: 0.53
Batch: 700; loss: 1.68; acc: 0.47
Batch: 720; loss: 1.47; acc: 0.58
Batch: 740; loss: 1.65; acc: 0.45
Batch: 760; loss: 1.55; acc: 0.47
Batch: 780; loss: 1.54; acc: 0.56
Train Epoch over. train_loss: 1.55; train_accuracy: 0.55 

3.9845650462666526e-05
1.312301083089551e-05
Batch: 0; loss: 1.59; acc: 0.52
Batch: 20; loss: 1.66; acc: 0.41
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.48; acc: 0.58
Batch: 80; loss: 1.33; acc: 0.7
Batch: 100; loss: 1.56; acc: 0.64
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.47; acc: 0.5
Val Epoch over. val_loss: 1.5238508472017422; val_accuracy: 0.576234076433121 

The current subspace-distance is: 1.312301083089551e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.56
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.43; acc: 0.64
Batch: 60; loss: 1.56; acc: 0.47
Batch: 80; loss: 1.66; acc: 0.53
Batch: 100; loss: 1.48; acc: 0.64
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.64; acc: 0.5
Batch: 160; loss: 1.48; acc: 0.66
Batch: 180; loss: 1.58; acc: 0.52
Batch: 200; loss: 1.58; acc: 0.56
Batch: 220; loss: 1.61; acc: 0.5
Batch: 240; loss: 1.7; acc: 0.47
Batch: 260; loss: 1.53; acc: 0.55
Batch: 280; loss: 1.49; acc: 0.59
Batch: 300; loss: 1.39; acc: 0.69
Batch: 320; loss: 1.5; acc: 0.55
Batch: 340; loss: 1.63; acc: 0.55
Batch: 360; loss: 1.58; acc: 0.59
Batch: 380; loss: 1.64; acc: 0.48
Batch: 400; loss: 1.46; acc: 0.59
Batch: 420; loss: 1.46; acc: 0.64
Batch: 440; loss: 1.59; acc: 0.45
Batch: 460; loss: 1.51; acc: 0.53
Batch: 480; loss: 1.72; acc: 0.47
Batch: 500; loss: 1.51; acc: 0.58
Batch: 520; loss: 1.64; acc: 0.45
Batch: 540; loss: 1.62; acc: 0.56
Batch: 560; loss: 1.51; acc: 0.53
Batch: 580; loss: 1.58; acc: 0.53
Batch: 600; loss: 1.52; acc: 0.66
Batch: 620; loss: 1.69; acc: 0.44
Batch: 640; loss: 1.51; acc: 0.55
Batch: 660; loss: 1.45; acc: 0.67
Batch: 680; loss: 1.49; acc: 0.56
Batch: 700; loss: 1.53; acc: 0.53
Batch: 720; loss: 1.68; acc: 0.53
Batch: 740; loss: 1.44; acc: 0.62
Batch: 760; loss: 1.65; acc: 0.44
Batch: 780; loss: 1.68; acc: 0.58
Train Epoch over. train_loss: 1.55; train_accuracy: 0.56 

3.9925071178004146e-05
1.1442646609793883e-05
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.32; acc: 0.69
Batch: 100; loss: 1.56; acc: 0.64
Batch: 120; loss: 1.63; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.52
Val Epoch over. val_loss: 1.507099629967076; val_accuracy: 0.5709593949044586 

The current subspace-distance is: 1.1442646609793883e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.42; acc: 0.7
Batch: 20; loss: 1.54; acc: 0.56
Batch: 40; loss: 1.74; acc: 0.44
Batch: 60; loss: 1.5; acc: 0.55
Batch: 80; loss: 1.57; acc: 0.45
Batch: 100; loss: 1.5; acc: 0.58
Batch: 120; loss: 1.58; acc: 0.5
Batch: 140; loss: 1.62; acc: 0.55
Batch: 160; loss: 1.51; acc: 0.59
Batch: 180; loss: 1.4; acc: 0.64
Batch: 200; loss: 1.53; acc: 0.55
Batch: 220; loss: 1.61; acc: 0.45
Batch: 240; loss: 1.54; acc: 0.58
Batch: 260; loss: 1.54; acc: 0.58
Batch: 280; loss: 1.42; acc: 0.67
Batch: 300; loss: 1.47; acc: 0.64
Batch: 320; loss: 1.56; acc: 0.52
Batch: 340; loss: 1.47; acc: 0.61
Batch: 360; loss: 1.52; acc: 0.64
Batch: 380; loss: 1.64; acc: 0.52
Batch: 400; loss: 1.71; acc: 0.5
Batch: 420; loss: 1.4; acc: 0.66
Batch: 440; loss: 1.56; acc: 0.59
Batch: 460; loss: 1.47; acc: 0.64
Batch: 480; loss: 1.54; acc: 0.55
Batch: 500; loss: 1.51; acc: 0.58
Batch: 520; loss: 1.53; acc: 0.59
Batch: 540; loss: 1.56; acc: 0.56
Batch: 560; loss: 1.65; acc: 0.52
Batch: 580; loss: 1.75; acc: 0.39
Batch: 600; loss: 1.49; acc: 0.52
Batch: 620; loss: 1.48; acc: 0.59
Batch: 640; loss: 1.5; acc: 0.61
Batch: 660; loss: 1.49; acc: 0.59
Batch: 680; loss: 1.67; acc: 0.53
Batch: 700; loss: 1.53; acc: 0.52
Batch: 720; loss: 1.5; acc: 0.5
Batch: 740; loss: 1.7; acc: 0.42
Batch: 760; loss: 1.55; acc: 0.5
Batch: 780; loss: 1.54; acc: 0.59
Train Epoch over. train_loss: 1.54; train_accuracy: 0.56 

4.0014187106862664e-05
1.1708550118783023e-05
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.66; acc: 0.41
Batch: 40; loss: 1.22; acc: 0.7
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.33; acc: 0.72
Batch: 100; loss: 1.56; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.48; acc: 0.47
Val Epoch over. val_loss: 1.5181731889202337; val_accuracy: 0.5750398089171974 

The current subspace-distance is: 1.1708550118783023e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.59
Batch: 20; loss: 1.58; acc: 0.59
Batch: 40; loss: 1.54; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.52
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.73; acc: 0.47
Batch: 120; loss: 1.49; acc: 0.56
Batch: 140; loss: 1.63; acc: 0.55
Batch: 160; loss: 1.63; acc: 0.48
Batch: 180; loss: 1.72; acc: 0.48
Batch: 200; loss: 1.51; acc: 0.59
Batch: 220; loss: 1.49; acc: 0.61
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.54; acc: 0.61
Batch: 280; loss: 1.62; acc: 0.45
Batch: 300; loss: 1.54; acc: 0.59
Batch: 320; loss: 1.62; acc: 0.5
Batch: 340; loss: 1.63; acc: 0.52
Batch: 360; loss: 1.57; acc: 0.5
Batch: 380; loss: 1.55; acc: 0.58
Batch: 400; loss: 1.49; acc: 0.56
Batch: 420; loss: 1.53; acc: 0.66
Batch: 440; loss: 1.58; acc: 0.56
Batch: 460; loss: 1.55; acc: 0.55
Batch: 480; loss: 1.6; acc: 0.59
Batch: 500; loss: 1.57; acc: 0.56
Batch: 520; loss: 1.41; acc: 0.64
Batch: 540; loss: 1.56; acc: 0.59
Batch: 560; loss: 1.59; acc: 0.58
Batch: 580; loss: 1.66; acc: 0.5
Batch: 600; loss: 1.62; acc: 0.45
Batch: 620; loss: 1.7; acc: 0.48
Batch: 640; loss: 1.57; acc: 0.55
Batch: 660; loss: 1.53; acc: 0.53
Batch: 680; loss: 1.55; acc: 0.55
Batch: 700; loss: 1.62; acc: 0.53
Batch: 720; loss: 1.5; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.62
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.34; acc: 0.67
Train Epoch over. train_loss: 1.54; train_accuracy: 0.56 

4.117452772334218e-05
1.5018125850474462e-05
Batch: 0; loss: 1.57; acc: 0.53
Batch: 20; loss: 1.65; acc: 0.44
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.3; acc: 0.67
Batch: 100; loss: 1.55; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.45; acc: 0.5
Val Epoch over. val_loss: 1.505290067119963; val_accuracy: 0.5790207006369427 

The current subspace-distance is: 1.5018125850474462e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.52; acc: 0.58
Batch: 60; loss: 1.49; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.47; acc: 0.61
Batch: 120; loss: 1.55; acc: 0.45
Batch: 140; loss: 1.53; acc: 0.52
Batch: 160; loss: 1.56; acc: 0.55
Batch: 180; loss: 1.45; acc: 0.59
Batch: 200; loss: 1.52; acc: 0.59
Batch: 220; loss: 1.5; acc: 0.53
Batch: 240; loss: 1.59; acc: 0.59
Batch: 260; loss: 1.46; acc: 0.55
Batch: 280; loss: 1.65; acc: 0.42
Batch: 300; loss: 1.72; acc: 0.44
Batch: 320; loss: 1.47; acc: 0.62
Batch: 340; loss: 1.56; acc: 0.56
Batch: 360; loss: 1.47; acc: 0.58
Batch: 380; loss: 1.42; acc: 0.59
Batch: 400; loss: 1.55; acc: 0.55
Batch: 420; loss: 1.54; acc: 0.52
Batch: 440; loss: 1.38; acc: 0.67
Batch: 460; loss: 1.7; acc: 0.52
Batch: 480; loss: 1.63; acc: 0.56
Batch: 500; loss: 1.5; acc: 0.56
Batch: 520; loss: 1.58; acc: 0.58
Batch: 540; loss: 1.5; acc: 0.58
Batch: 560; loss: 1.47; acc: 0.66
Batch: 580; loss: 1.55; acc: 0.55
Batch: 600; loss: 1.57; acc: 0.52
Batch: 620; loss: 1.45; acc: 0.62
Batch: 640; loss: 1.53; acc: 0.58
Batch: 660; loss: 1.51; acc: 0.56
Batch: 680; loss: 1.5; acc: 0.61
Batch: 700; loss: 1.53; acc: 0.56
Batch: 720; loss: 1.63; acc: 0.53
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.51; acc: 0.61
Batch: 780; loss: 1.46; acc: 0.62
Train Epoch over. train_loss: 1.54; train_accuracy: 0.56 

4.007209281553514e-05
1.2091859389329329e-05
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.62; acc: 0.44
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.45; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.72
Batch: 100; loss: 1.54; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.44; acc: 0.59
Val Epoch over. val_loss: 1.4994579546011177; val_accuracy: 0.5797173566878981 

The current subspace-distance is: 1.2091859389329329e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.53
Batch: 20; loss: 1.44; acc: 0.59
Batch: 40; loss: 1.64; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.61
Batch: 80; loss: 1.37; acc: 0.66
Batch: 100; loss: 1.51; acc: 0.58
Batch: 120; loss: 1.63; acc: 0.44
Batch: 140; loss: 1.51; acc: 0.58
Batch: 160; loss: 1.59; acc: 0.58
Batch: 180; loss: 1.51; acc: 0.59
Batch: 200; loss: 1.34; acc: 0.72
Batch: 220; loss: 1.47; acc: 0.56
Batch: 240; loss: 1.48; acc: 0.58
Batch: 260; loss: 1.47; acc: 0.59
Batch: 280; loss: 1.47; acc: 0.62
Batch: 300; loss: 1.44; acc: 0.66
Batch: 320; loss: 1.44; acc: 0.59
Batch: 340; loss: 1.7; acc: 0.41
Batch: 360; loss: 1.56; acc: 0.58
Batch: 380; loss: 1.48; acc: 0.62
Batch: 400; loss: 1.75; acc: 0.5
Batch: 420; loss: 1.53; acc: 0.61
Batch: 440; loss: 1.43; acc: 0.62
Batch: 460; loss: 1.43; acc: 0.64
Batch: 480; loss: 1.63; acc: 0.52
Batch: 500; loss: 1.53; acc: 0.59
Batch: 520; loss: 1.68; acc: 0.42
Batch: 540; loss: 1.43; acc: 0.66
Batch: 560; loss: 1.53; acc: 0.53
Batch: 580; loss: 1.55; acc: 0.5
Batch: 600; loss: 1.78; acc: 0.42
Batch: 620; loss: 1.65; acc: 0.48
Batch: 640; loss: 1.7; acc: 0.45
Batch: 660; loss: 1.49; acc: 0.58
Batch: 680; loss: 1.56; acc: 0.55
Batch: 700; loss: 1.47; acc: 0.59
Batch: 720; loss: 1.53; acc: 0.53
Batch: 740; loss: 1.46; acc: 0.52
Batch: 760; loss: 1.61; acc: 0.56
Batch: 780; loss: 1.47; acc: 0.59
Train Epoch over. train_loss: 1.54; train_accuracy: 0.56 

4.094467294635251e-05
1.4281466974352952e-05
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.65; acc: 0.42
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.46; acc: 0.58
Batch: 80; loss: 1.31; acc: 0.67
Batch: 100; loss: 1.55; acc: 0.62
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.47; acc: 0.48
Val Epoch over. val_loss: 1.5051700544964737; val_accuracy: 0.5694665605095541 

The current subspace-distance is: 1.4281466974352952e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.54; acc: 0.58
Batch: 60; loss: 1.42; acc: 0.64
Batch: 80; loss: 1.54; acc: 0.56
Batch: 100; loss: 1.57; acc: 0.52
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 1.56; acc: 0.52
Batch: 160; loss: 1.58; acc: 0.53
Batch: 180; loss: 1.48; acc: 0.69
Batch: 200; loss: 1.55; acc: 0.59
Batch: 220; loss: 1.52; acc: 0.58
Batch: 240; loss: 1.54; acc: 0.56
Batch: 260; loss: 1.5; acc: 0.55
Batch: 280; loss: 1.56; acc: 0.53
Batch: 300; loss: 1.7; acc: 0.48
Batch: 320; loss: 1.68; acc: 0.47
Batch: 340; loss: 1.5; acc: 0.52
Batch: 360; loss: 1.69; acc: 0.47
Batch: 380; loss: 1.49; acc: 0.58
Batch: 400; loss: 1.46; acc: 0.62
Batch: 420; loss: 1.49; acc: 0.58
Batch: 440; loss: 1.54; acc: 0.53
Batch: 460; loss: 1.43; acc: 0.66
Batch: 480; loss: 1.35; acc: 0.67
Batch: 500; loss: 1.37; acc: 0.7
Batch: 520; loss: 1.49; acc: 0.56
Batch: 540; loss: 1.76; acc: 0.42
Batch: 560; loss: 1.56; acc: 0.56
Batch: 580; loss: 1.59; acc: 0.47
Batch: 600; loss: 1.57; acc: 0.5
Batch: 620; loss: 1.55; acc: 0.53
Batch: 640; loss: 1.47; acc: 0.61
Batch: 660; loss: 1.32; acc: 0.66
Batch: 680; loss: 1.38; acc: 0.64
Batch: 700; loss: 1.48; acc: 0.62
Batch: 720; loss: 1.58; acc: 0.52
Batch: 740; loss: 1.6; acc: 0.5
Batch: 760; loss: 1.55; acc: 0.48
Batch: 780; loss: 1.73; acc: 0.41
Train Epoch over. train_loss: 1.54; train_accuracy: 0.56 

4.052104850416072e-05
1.2617368156497832e-05
Batch: 0; loss: 1.55; acc: 0.53
Batch: 20; loss: 1.63; acc: 0.44
Batch: 40; loss: 1.18; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.29; acc: 0.69
Batch: 100; loss: 1.54; acc: 0.64
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.43; acc: 0.55
Val Epoch over. val_loss: 1.491655968556738; val_accuracy: 0.5765326433121019 

The current subspace-distance is: 1.2617368156497832e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.62
Batch: 20; loss: 1.57; acc: 0.53
Batch: 40; loss: 1.48; acc: 0.66
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.64; acc: 0.48
Batch: 100; loss: 1.51; acc: 0.61
Batch: 120; loss: 1.61; acc: 0.52
Batch: 140; loss: 1.64; acc: 0.52
Batch: 160; loss: 1.52; acc: 0.62
Batch: 180; loss: 1.5; acc: 0.61
Batch: 200; loss: 1.65; acc: 0.52
Batch: 220; loss: 1.49; acc: 0.62
Batch: 240; loss: 1.54; acc: 0.56
Batch: 260; loss: 1.52; acc: 0.59
Batch: 280; loss: 1.42; acc: 0.66
Batch: 300; loss: 1.45; acc: 0.61
Batch: 320; loss: 1.58; acc: 0.47
Batch: 340; loss: 1.47; acc: 0.53
Batch: 360; loss: 1.53; acc: 0.56
Batch: 380; loss: 1.44; acc: 0.56
Batch: 400; loss: 1.6; acc: 0.5
Batch: 420; loss: 1.56; acc: 0.55
Batch: 440; loss: 1.55; acc: 0.53
Batch: 460; loss: 1.46; acc: 0.66
Batch: 480; loss: 1.49; acc: 0.53
Batch: 500; loss: 1.66; acc: 0.52
Batch: 520; loss: 1.71; acc: 0.48
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.6; acc: 0.45
Batch: 580; loss: 1.51; acc: 0.59
Batch: 600; loss: 1.48; acc: 0.64
Batch: 620; loss: 1.43; acc: 0.66
Batch: 640; loss: 1.33; acc: 0.64
Batch: 660; loss: 1.43; acc: 0.64
Batch: 680; loss: 1.51; acc: 0.61
Batch: 700; loss: 1.62; acc: 0.53
Batch: 720; loss: 1.54; acc: 0.52
Batch: 740; loss: 1.57; acc: 0.53
Batch: 760; loss: 1.43; acc: 0.66
Batch: 780; loss: 1.58; acc: 0.53
Train Epoch over. train_loss: 1.54; train_accuracy: 0.56 

4.066156907356344e-05
1.0451364687469322e-05
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.63; acc: 0.44
Batch: 40; loss: 1.2; acc: 0.73
Batch: 60; loss: 1.45; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.69
Batch: 100; loss: 1.56; acc: 0.62
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.45; acc: 0.48
Val Epoch over. val_loss: 1.5060478319787676; val_accuracy: 0.57921974522293 

The current subspace-distance is: 1.0451364687469322e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.53
Batch: 20; loss: 1.38; acc: 0.62
Batch: 40; loss: 1.43; acc: 0.66
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.69
Batch: 100; loss: 1.56; acc: 0.58
Batch: 120; loss: 1.56; acc: 0.58
Batch: 140; loss: 1.56; acc: 0.53
Batch: 160; loss: 1.61; acc: 0.5
Batch: 180; loss: 1.59; acc: 0.53
Batch: 200; loss: 1.6; acc: 0.44
Batch: 220; loss: 1.49; acc: 0.62
Batch: 240; loss: 1.61; acc: 0.5
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.52; acc: 0.61
Batch: 300; loss: 1.64; acc: 0.47
Batch: 320; loss: 1.58; acc: 0.48
Batch: 340; loss: 1.47; acc: 0.61
Batch: 360; loss: 1.59; acc: 0.53
Batch: 380; loss: 1.44; acc: 0.56
Batch: 400; loss: 1.59; acc: 0.48
Batch: 420; loss: 1.47; acc: 0.59
Batch: 440; loss: 1.65; acc: 0.44
Batch: 460; loss: 1.45; acc: 0.62
Batch: 480; loss: 1.46; acc: 0.56
Batch: 500; loss: 1.62; acc: 0.48
Batch: 520; loss: 1.4; acc: 0.61
Batch: 540; loss: 1.44; acc: 0.72
Batch: 560; loss: 1.47; acc: 0.56
Batch: 580; loss: 1.41; acc: 0.64
Batch: 600; loss: 1.51; acc: 0.56
Batch: 620; loss: 1.39; acc: 0.59
Batch: 640; loss: 1.48; acc: 0.56
Batch: 660; loss: 1.67; acc: 0.55
Batch: 680; loss: 1.6; acc: 0.52
Batch: 700; loss: 1.61; acc: 0.58
Batch: 720; loss: 1.53; acc: 0.56
Batch: 740; loss: 1.54; acc: 0.61
Batch: 760; loss: 1.44; acc: 0.62
Batch: 780; loss: 1.42; acc: 0.66
Train Epoch over. train_loss: 1.53; train_accuracy: 0.56 

4.164242636761628e-05
1.4231362911232281e-05
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.63; acc: 0.44
Batch: 40; loss: 1.19; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.69
Batch: 100; loss: 1.55; acc: 0.67
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.45; acc: 0.48
Val Epoch over. val_loss: 1.4995077483972925; val_accuracy: 0.574343152866242 

The current subspace-distance is: 1.4231362911232281e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_3_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.928869038633508

The number of parameters is: 275071

The number of individual parameters is:

40
400
40
40
60
50400
60
60
119
149940
119
119
64
68544
64
64
4096
64
640
10
64
64

nonzero elements in E: 27507097
elements in E: 27507100
fraction nonzero: 0.999999890937249
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.45; acc: 0.05
Batch: 20; loss: 2.41; acc: 0.09
Batch: 40; loss: 2.15; acc: 0.2
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.08; acc: 0.25
Batch: 100; loss: 2.08; acc: 0.38
Batch: 120; loss: 2.07; acc: 0.23
Batch: 140; loss: 1.96; acc: 0.33
Batch: 160; loss: 1.94; acc: 0.39
Batch: 180; loss: 1.92; acc: 0.38
Batch: 200; loss: 1.87; acc: 0.44
Batch: 220; loss: 1.8; acc: 0.55
Batch: 240; loss: 1.8; acc: 0.44
Batch: 260; loss: 1.82; acc: 0.42
Batch: 280; loss: 1.75; acc: 0.5
Batch: 300; loss: 1.86; acc: 0.47
Batch: 320; loss: 1.79; acc: 0.47
Batch: 340; loss: 1.74; acc: 0.55
Batch: 360; loss: 1.77; acc: 0.56
Batch: 380; loss: 1.78; acc: 0.45
Batch: 400; loss: 1.79; acc: 0.48
Batch: 420; loss: 1.71; acc: 0.58
Batch: 440; loss: 1.76; acc: 0.58
Batch: 460; loss: 1.67; acc: 0.59
Batch: 480; loss: 1.69; acc: 0.52
Batch: 500; loss: 1.65; acc: 0.53
Batch: 520; loss: 1.62; acc: 0.59
Batch: 540; loss: 1.61; acc: 0.59
Batch: 560; loss: 1.58; acc: 0.66
Batch: 580; loss: 1.65; acc: 0.52
Batch: 600; loss: 1.49; acc: 0.72
Batch: 620; loss: 1.53; acc: 0.69
Batch: 640; loss: 1.57; acc: 0.64
Batch: 660; loss: 1.58; acc: 0.59
Batch: 680; loss: 1.43; acc: 0.72
Batch: 700; loss: 1.49; acc: 0.66
Batch: 720; loss: 1.51; acc: 0.69
Batch: 740; loss: 1.52; acc: 0.69
Batch: 760; loss: 1.59; acc: 0.62
Batch: 780; loss: 1.56; acc: 0.62
Train Epoch over. train_loss: 1.79; train_accuracy: 0.49 

5.668422090820968e-05
5.1099556003464386e-05
Batch: 0; loss: 1.48; acc: 0.62
Batch: 20; loss: 1.62; acc: 0.56
Batch: 40; loss: 1.35; acc: 0.77
Batch: 60; loss: 1.45; acc: 0.67
Batch: 80; loss: 1.45; acc: 0.72
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.72
Val Epoch over. val_loss: 1.5142922515322448; val_accuracy: 0.6378383757961783 

The current subspace-distance is: 5.1099556003464386e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.73
Batch: 20; loss: 1.65; acc: 0.59
Batch: 40; loss: 1.46; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.66
Batch: 80; loss: 1.65; acc: 0.56
Batch: 100; loss: 1.62; acc: 0.56
Batch: 120; loss: 1.44; acc: 0.7
Batch: 140; loss: 1.54; acc: 0.56
Batch: 160; loss: 1.56; acc: 0.66
Batch: 180; loss: 1.56; acc: 0.56
Batch: 200; loss: 1.5; acc: 0.64
Batch: 220; loss: 1.49; acc: 0.58
Batch: 240; loss: 1.53; acc: 0.61
Batch: 260; loss: 1.42; acc: 0.7
Batch: 280; loss: 1.54; acc: 0.62
Batch: 300; loss: 1.52; acc: 0.61
Batch: 320; loss: 1.53; acc: 0.62
Batch: 340; loss: 1.52; acc: 0.64
Batch: 360; loss: 1.58; acc: 0.61
Batch: 380; loss: 1.39; acc: 0.75
Batch: 400; loss: 1.34; acc: 0.75
Batch: 420; loss: 1.37; acc: 0.78
Batch: 440; loss: 1.41; acc: 0.73
Batch: 460; loss: 1.42; acc: 0.7
Batch: 480; loss: 1.5; acc: 0.62
Batch: 500; loss: 1.45; acc: 0.67
Batch: 520; loss: 1.38; acc: 0.7
Batch: 540; loss: 1.52; acc: 0.66
Batch: 560; loss: 1.61; acc: 0.52
Batch: 580; loss: 1.43; acc: 0.73
Batch: 600; loss: 1.46; acc: 0.7
Batch: 620; loss: 1.51; acc: 0.7
Batch: 640; loss: 1.45; acc: 0.77
Batch: 660; loss: 1.34; acc: 0.75
Batch: 680; loss: 1.45; acc: 0.69
Batch: 700; loss: 1.42; acc: 0.72
Batch: 720; loss: 1.46; acc: 0.62
Batch: 740; loss: 1.32; acc: 0.77
Batch: 760; loss: 1.33; acc: 0.73
Batch: 780; loss: 1.45; acc: 0.58
Train Epoch over. train_loss: 1.49; train_accuracy: 0.64 

7.124649710021913e-05
6.612127617700025e-05
Batch: 0; loss: 1.34; acc: 0.73
Batch: 20; loss: 1.52; acc: 0.64
Batch: 40; loss: 1.25; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.67
Batch: 80; loss: 1.33; acc: 0.77
Batch: 100; loss: 1.4; acc: 0.69
Batch: 120; loss: 1.45; acc: 0.64
Batch: 140; loss: 1.33; acc: 0.75
Val Epoch over. val_loss: 1.417156270355176; val_accuracy: 0.6757563694267515 

The current subspace-distance is: 6.612127617700025e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.67
Batch: 20; loss: 1.39; acc: 0.64
Batch: 40; loss: 1.45; acc: 0.59
Batch: 60; loss: 1.45; acc: 0.62
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.39; acc: 0.66
Batch: 120; loss: 1.33; acc: 0.77
Batch: 140; loss: 1.48; acc: 0.67
Batch: 160; loss: 1.39; acc: 0.69
Batch: 180; loss: 1.36; acc: 0.7
Batch: 200; loss: 1.35; acc: 0.8
Batch: 220; loss: 1.57; acc: 0.53
Batch: 240; loss: 1.52; acc: 0.64
Batch: 260; loss: 1.31; acc: 0.78
Batch: 280; loss: 1.38; acc: 0.69
Batch: 300; loss: 1.42; acc: 0.67
Batch: 320; loss: 1.5; acc: 0.56
Batch: 340; loss: 1.42; acc: 0.69
Batch: 360; loss: 1.47; acc: 0.66
Batch: 380; loss: 1.38; acc: 0.64
Batch: 400; loss: 1.33; acc: 0.7
Batch: 420; loss: 1.58; acc: 0.58
Batch: 440; loss: 1.35; acc: 0.75
Batch: 460; loss: 1.4; acc: 0.67
Batch: 480; loss: 1.39; acc: 0.72
Batch: 500; loss: 1.44; acc: 0.73
Batch: 520; loss: 1.34; acc: 0.7
Batch: 540; loss: 1.42; acc: 0.61
Batch: 560; loss: 1.38; acc: 0.72
Batch: 580; loss: 1.41; acc: 0.72
Batch: 600; loss: 1.33; acc: 0.75
Batch: 620; loss: 1.37; acc: 0.78
Batch: 640; loss: 1.43; acc: 0.67
Batch: 660; loss: 1.37; acc: 0.69
Batch: 680; loss: 1.51; acc: 0.59
Batch: 700; loss: 1.39; acc: 0.67
Batch: 720; loss: 1.42; acc: 0.62
Batch: 740; loss: 1.39; acc: 0.72
Batch: 760; loss: 1.25; acc: 0.81
Batch: 780; loss: 1.42; acc: 0.64
Train Epoch over. train_loss: 1.42; train_accuracy: 0.67 

8.424757106695324e-05
7.970618753461167e-05
Batch: 0; loss: 1.27; acc: 0.72
Batch: 20; loss: 1.42; acc: 0.62
Batch: 40; loss: 1.12; acc: 0.81
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.19; acc: 0.86
Batch: 100; loss: 1.31; acc: 0.77
Batch: 120; loss: 1.41; acc: 0.67
Batch: 140; loss: 1.22; acc: 0.86
Val Epoch over. val_loss: 1.3375546985371098; val_accuracy: 0.7082006369426752 

The current subspace-distance is: 7.970618753461167e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.56
Batch: 20; loss: 1.41; acc: 0.69
Batch: 40; loss: 1.43; acc: 0.56
Batch: 60; loss: 1.56; acc: 0.58
Batch: 80; loss: 1.36; acc: 0.67
Batch: 100; loss: 1.33; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.66
Batch: 140; loss: 1.57; acc: 0.55
Batch: 160; loss: 1.36; acc: 0.73
Batch: 180; loss: 1.43; acc: 0.66
Batch: 200; loss: 1.44; acc: 0.62
Batch: 220; loss: 1.41; acc: 0.59
Batch: 240; loss: 1.34; acc: 0.75
Batch: 260; loss: 1.32; acc: 0.7
Batch: 280; loss: 1.41; acc: 0.58
Batch: 300; loss: 1.38; acc: 0.72
Batch: 320; loss: 1.29; acc: 0.72
Batch: 340; loss: 1.3; acc: 0.72
Batch: 360; loss: 1.42; acc: 0.64
Batch: 380; loss: 1.39; acc: 0.69
Batch: 400; loss: 1.45; acc: 0.66
Batch: 420; loss: 1.4; acc: 0.67
Batch: 440; loss: 1.3; acc: 0.75
Batch: 460; loss: 1.31; acc: 0.67
Batch: 480; loss: 1.48; acc: 0.66
Batch: 500; loss: 1.34; acc: 0.7
Batch: 520; loss: 1.34; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.77
Batch: 560; loss: 1.21; acc: 0.84
Batch: 580; loss: 1.42; acc: 0.56
Batch: 600; loss: 1.25; acc: 0.73
Batch: 620; loss: 1.35; acc: 0.66
Batch: 640; loss: 1.41; acc: 0.62
Batch: 660; loss: 1.41; acc: 0.61
Batch: 680; loss: 1.37; acc: 0.67
Batch: 700; loss: 1.45; acc: 0.62
Batch: 720; loss: 1.29; acc: 0.8
Batch: 740; loss: 1.3; acc: 0.72
Batch: 760; loss: 1.51; acc: 0.62
Batch: 780; loss: 1.36; acc: 0.73
Train Epoch over. train_loss: 1.35; train_accuracy: 0.69 

9.8116957815364e-05
9.295085328631103e-05
Batch: 0; loss: 1.24; acc: 0.72
Batch: 20; loss: 1.41; acc: 0.66
Batch: 40; loss: 1.04; acc: 0.84
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.15; acc: 0.83
Batch: 100; loss: 1.26; acc: 0.77
Batch: 120; loss: 1.37; acc: 0.67
Batch: 140; loss: 1.15; acc: 0.91
Val Epoch over. val_loss: 1.2874758691544745; val_accuracy: 0.7245222929936306 

The current subspace-distance is: 9.295085328631103e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.66
Batch: 20; loss: 1.37; acc: 0.64
Batch: 40; loss: 1.33; acc: 0.67
Batch: 60; loss: 1.41; acc: 0.66
Batch: 80; loss: 1.28; acc: 0.78
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.23; acc: 0.77
Batch: 140; loss: 1.26; acc: 0.72
Batch: 160; loss: 1.4; acc: 0.67
Batch: 180; loss: 1.36; acc: 0.66
Batch: 200; loss: 1.33; acc: 0.59
Batch: 220; loss: 1.33; acc: 0.67
Batch: 240; loss: 1.27; acc: 0.69
Batch: 260; loss: 1.43; acc: 0.67
Batch: 280; loss: 1.22; acc: 0.81
Batch: 300; loss: 1.38; acc: 0.61
Batch: 320; loss: 1.3; acc: 0.66
Batch: 340; loss: 1.28; acc: 0.69
Batch: 360; loss: 1.34; acc: 0.66
Batch: 380; loss: 1.26; acc: 0.7
Batch: 400; loss: 1.36; acc: 0.64
Batch: 420; loss: 1.34; acc: 0.69
Batch: 440; loss: 1.38; acc: 0.69
Batch: 460; loss: 1.16; acc: 0.78
Batch: 480; loss: 1.33; acc: 0.7
Batch: 500; loss: 1.3; acc: 0.72
Batch: 520; loss: 1.23; acc: 0.7
Batch: 540; loss: 1.44; acc: 0.59
Batch: 560; loss: 1.35; acc: 0.66
Batch: 580; loss: 1.34; acc: 0.67
Batch: 600; loss: 1.28; acc: 0.7
Batch: 620; loss: 1.29; acc: 0.72
Batch: 640; loss: 1.41; acc: 0.56
Batch: 660; loss: 1.27; acc: 0.67
Batch: 680; loss: 1.26; acc: 0.69
Batch: 700; loss: 1.14; acc: 0.8
Batch: 720; loss: 1.33; acc: 0.62
Batch: 740; loss: 1.12; acc: 0.83
Batch: 760; loss: 1.39; acc: 0.67
Batch: 780; loss: 1.34; acc: 0.66
Train Epoch over. train_loss: 1.31; train_accuracy: 0.69 

0.00010451212438056245
0.00010109389404533431
Batch: 0; loss: 1.27; acc: 0.7
Batch: 20; loss: 1.43; acc: 0.62
Batch: 40; loss: 0.99; acc: 0.88
Batch: 60; loss: 1.23; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.84
Batch: 100; loss: 1.24; acc: 0.75
Batch: 120; loss: 1.36; acc: 0.67
Batch: 140; loss: 1.14; acc: 0.83
Val Epoch over. val_loss: 1.2577077719815977; val_accuracy: 0.7239251592356688 

The current subspace-distance is: 0.00010109389404533431 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.69
Batch: 20; loss: 1.3; acc: 0.73
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.31; acc: 0.67
Batch: 80; loss: 1.2; acc: 0.75
Batch: 100; loss: 1.3; acc: 0.67
Batch: 120; loss: 1.26; acc: 0.69
Batch: 140; loss: 1.35; acc: 0.61
Batch: 160; loss: 1.46; acc: 0.58
Batch: 180; loss: 1.29; acc: 0.72
Batch: 200; loss: 1.4; acc: 0.58
Batch: 220; loss: 1.29; acc: 0.67
Batch: 240; loss: 1.27; acc: 0.73
Batch: 260; loss: 1.29; acc: 0.69
Batch: 280; loss: 1.43; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.78
Batch: 320; loss: 1.23; acc: 0.75
Batch: 340; loss: 1.39; acc: 0.66
Batch: 360; loss: 1.3; acc: 0.67
Batch: 380; loss: 1.39; acc: 0.61
Batch: 400; loss: 1.25; acc: 0.77
Batch: 420; loss: 1.34; acc: 0.61
Batch: 440; loss: 1.25; acc: 0.69
Batch: 460; loss: 1.25; acc: 0.69
Batch: 480; loss: 1.29; acc: 0.64
Batch: 500; loss: 1.48; acc: 0.59
Batch: 520; loss: 1.34; acc: 0.62
Batch: 540; loss: 1.17; acc: 0.77
Batch: 560; loss: 1.25; acc: 0.66
Batch: 580; loss: 1.23; acc: 0.69
Batch: 600; loss: 1.31; acc: 0.67
Batch: 620; loss: 1.36; acc: 0.61
Batch: 640; loss: 1.14; acc: 0.8
Batch: 660; loss: 1.26; acc: 0.72
Batch: 680; loss: 1.3; acc: 0.61
Batch: 700; loss: 1.25; acc: 0.67
Batch: 720; loss: 1.24; acc: 0.7
Batch: 740; loss: 1.18; acc: 0.73
Batch: 760; loss: 1.39; acc: 0.58
Batch: 780; loss: 1.25; acc: 0.77
Train Epoch over. train_loss: 1.29; train_accuracy: 0.69 

0.00011334331793477759
0.00010635300714056939
Batch: 0; loss: 1.32; acc: 0.67
Batch: 20; loss: 1.45; acc: 0.64
Batch: 40; loss: 0.97; acc: 0.88
Batch: 60; loss: 1.22; acc: 0.73
Batch: 80; loss: 1.08; acc: 0.86
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.36; acc: 0.67
Batch: 140; loss: 1.13; acc: 0.86
Val Epoch over. val_loss: 1.251084202793753; val_accuracy: 0.720640923566879 

The current subspace-distance is: 0.00010635300714056939 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.66
Batch: 20; loss: 1.28; acc: 0.75
Batch: 40; loss: 1.17; acc: 0.77
Batch: 60; loss: 1.26; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.69
Batch: 100; loss: 1.29; acc: 0.66
Batch: 120; loss: 1.21; acc: 0.75
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.17; acc: 0.72
Batch: 180; loss: 1.38; acc: 0.58
Batch: 200; loss: 1.28; acc: 0.72
Batch: 220; loss: 1.2; acc: 0.77
Batch: 240; loss: 1.27; acc: 0.66
Batch: 260; loss: 1.27; acc: 0.7
Batch: 280; loss: 1.4; acc: 0.61
Batch: 300; loss: 1.33; acc: 0.7
Batch: 320; loss: 1.32; acc: 0.66
Batch: 340; loss: 1.34; acc: 0.69
Batch: 360; loss: 1.23; acc: 0.72
Batch: 380; loss: 1.1; acc: 0.83
Batch: 400; loss: 1.26; acc: 0.75
Batch: 420; loss: 1.34; acc: 0.67
Batch: 440; loss: 1.31; acc: 0.66
Batch: 460; loss: 1.33; acc: 0.67
Batch: 480; loss: 1.25; acc: 0.72
Batch: 500; loss: 1.27; acc: 0.73
Batch: 520; loss: 1.29; acc: 0.75
Batch: 540; loss: 1.4; acc: 0.59
Batch: 560; loss: 1.12; acc: 0.8
Batch: 580; loss: 1.22; acc: 0.73
Batch: 600; loss: 1.31; acc: 0.64
Batch: 620; loss: 1.3; acc: 0.7
Batch: 640; loss: 1.2; acc: 0.73
Batch: 660; loss: 1.26; acc: 0.72
Batch: 680; loss: 1.3; acc: 0.62
Batch: 700; loss: 1.36; acc: 0.69
Batch: 720; loss: 1.3; acc: 0.72
Batch: 740; loss: 1.22; acc: 0.67
Batch: 760; loss: 1.22; acc: 0.73
Batch: 780; loss: 1.2; acc: 0.72
Train Epoch over. train_loss: 1.27; train_accuracy: 0.69 

0.0001184958455269225
0.00011208033538423479
Batch: 0; loss: 1.3; acc: 0.64
Batch: 20; loss: 1.41; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.86
Batch: 60; loss: 1.2; acc: 0.73
Batch: 80; loss: 1.04; acc: 0.86
Batch: 100; loss: 1.22; acc: 0.75
Batch: 120; loss: 1.32; acc: 0.7
Batch: 140; loss: 1.09; acc: 0.84
Val Epoch over. val_loss: 1.2178068768446613; val_accuracy: 0.7291003184713376 

The current subspace-distance is: 0.00011208033538423479 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.72
Batch: 20; loss: 1.19; acc: 0.75
Batch: 40; loss: 1.1; acc: 0.81
Batch: 60; loss: 1.23; acc: 0.69
Batch: 80; loss: 1.13; acc: 0.75
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.43; acc: 0.58
Batch: 140; loss: 1.33; acc: 0.59
Batch: 160; loss: 1.27; acc: 0.69
Batch: 180; loss: 1.19; acc: 0.77
Batch: 200; loss: 1.3; acc: 0.67
Batch: 220; loss: 1.22; acc: 0.73
Batch: 240; loss: 1.11; acc: 0.84
Batch: 260; loss: 1.18; acc: 0.72
Batch: 280; loss: 1.35; acc: 0.56
Batch: 300; loss: 1.32; acc: 0.59
Batch: 320; loss: 1.32; acc: 0.67
Batch: 340; loss: 1.23; acc: 0.69
Batch: 360; loss: 1.17; acc: 0.69
Batch: 380; loss: 1.38; acc: 0.69
Batch: 400; loss: 1.25; acc: 0.69
Batch: 420; loss: 1.54; acc: 0.48
Batch: 440; loss: 1.16; acc: 0.73
Batch: 460; loss: 1.15; acc: 0.75
Batch: 480; loss: 1.2; acc: 0.78
Batch: 500; loss: 1.14; acc: 0.75
Batch: 520; loss: 1.18; acc: 0.75
Batch: 540; loss: 1.37; acc: 0.56
Batch: 560; loss: 1.23; acc: 0.7
Batch: 580; loss: 1.29; acc: 0.72
Batch: 600; loss: 1.23; acc: 0.69
Batch: 620; loss: 1.2; acc: 0.77
Batch: 640; loss: 1.29; acc: 0.62
Batch: 660; loss: 1.29; acc: 0.72
Batch: 680; loss: 1.25; acc: 0.62
Batch: 700; loss: 1.15; acc: 0.83
Batch: 720; loss: 1.24; acc: 0.69
Batch: 740; loss: 1.28; acc: 0.7
Batch: 760; loss: 1.27; acc: 0.69
Batch: 780; loss: 1.22; acc: 0.73
Train Epoch over. train_loss: 1.26; train_accuracy: 0.7 

0.00012253086606506258
0.00011629408254520968
Batch: 0; loss: 1.3; acc: 0.66
Batch: 20; loss: 1.38; acc: 0.59
Batch: 40; loss: 0.92; acc: 0.86
Batch: 60; loss: 1.18; acc: 0.75
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.21; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.83
Val Epoch over. val_loss: 1.2010382656838483; val_accuracy: 0.7289012738853503 

The current subspace-distance is: 0.00011629408254520968 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 1.28; acc: 0.66
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.12; acc: 0.81
Batch: 100; loss: 1.37; acc: 0.59
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.7
Batch: 160; loss: 1.17; acc: 0.8
Batch: 180; loss: 1.4; acc: 0.66
Batch: 200; loss: 1.21; acc: 0.78
Batch: 220; loss: 1.26; acc: 0.66
Batch: 240; loss: 1.24; acc: 0.73
Batch: 260; loss: 1.18; acc: 0.72
Batch: 280; loss: 1.23; acc: 0.72
Batch: 300; loss: 1.16; acc: 0.73
Batch: 320; loss: 1.27; acc: 0.7
Batch: 340; loss: 1.1; acc: 0.78
Batch: 360; loss: 1.28; acc: 0.67
Batch: 380; loss: 1.18; acc: 0.73
Batch: 400; loss: 1.35; acc: 0.64
Batch: 420; loss: 1.33; acc: 0.66
Batch: 440; loss: 1.25; acc: 0.7
Batch: 460; loss: 1.27; acc: 0.64
Batch: 480; loss: 1.24; acc: 0.72
Batch: 500; loss: 1.16; acc: 0.78
Batch: 520; loss: 1.02; acc: 0.84
Batch: 540; loss: 1.28; acc: 0.73
Batch: 560; loss: 1.35; acc: 0.67
Batch: 580; loss: 1.27; acc: 0.67
Batch: 600; loss: 1.33; acc: 0.61
Batch: 620; loss: 1.18; acc: 0.75
Batch: 640; loss: 1.07; acc: 0.8
Batch: 660; loss: 1.27; acc: 0.7
Batch: 680; loss: 1.2; acc: 0.75
Batch: 700; loss: 1.18; acc: 0.72
Batch: 720; loss: 1.2; acc: 0.7
Batch: 740; loss: 1.37; acc: 0.61
Batch: 760; loss: 1.16; acc: 0.77
Batch: 780; loss: 1.17; acc: 0.73
Train Epoch over. train_loss: 1.25; train_accuracy: 0.69 

0.0001253926457138732
0.00011942780838580802
Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 1.36; acc: 0.58
Batch: 40; loss: 0.92; acc: 0.84
Batch: 60; loss: 1.17; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.83
Batch: 100; loss: 1.2; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.83
Val Epoch over. val_loss: 1.198413482517194; val_accuracy: 0.7224323248407644 

The current subspace-distance is: 0.00011942780838580802 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.75
Batch: 20; loss: 1.21; acc: 0.75
Batch: 40; loss: 1.17; acc: 0.75
Batch: 60; loss: 1.15; acc: 0.73
Batch: 80; loss: 1.29; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.78
Batch: 120; loss: 1.26; acc: 0.75
Batch: 140; loss: 1.27; acc: 0.77
Batch: 160; loss: 1.31; acc: 0.59
Batch: 180; loss: 1.18; acc: 0.77
Batch: 200; loss: 1.22; acc: 0.8
Batch: 220; loss: 1.22; acc: 0.69
Batch: 240; loss: 1.04; acc: 0.84
Batch: 260; loss: 1.16; acc: 0.78
Batch: 280; loss: 1.32; acc: 0.69
Batch: 300; loss: 1.19; acc: 0.73
Batch: 320; loss: 1.24; acc: 0.67
Batch: 340; loss: 1.17; acc: 0.66
Batch: 360; loss: 1.3; acc: 0.62
Batch: 380; loss: 1.15; acc: 0.75
Batch: 400; loss: 1.29; acc: 0.67
Batch: 420; loss: 1.21; acc: 0.72
Batch: 440; loss: 1.18; acc: 0.75
Batch: 460; loss: 1.12; acc: 0.72
Batch: 480; loss: 1.22; acc: 0.72
Batch: 500; loss: 1.2; acc: 0.72
Batch: 520; loss: 1.29; acc: 0.61
Batch: 540; loss: 1.15; acc: 0.73
Batch: 560; loss: 1.21; acc: 0.75
Batch: 580; loss: 1.28; acc: 0.66
Batch: 600; loss: 1.32; acc: 0.56
Batch: 620; loss: 1.11; acc: 0.78
Batch: 640; loss: 1.19; acc: 0.73
Batch: 660; loss: 1.2; acc: 0.72
Batch: 680; loss: 1.17; acc: 0.75
Batch: 700; loss: 1.32; acc: 0.62
Batch: 720; loss: 1.25; acc: 0.69
Batch: 740; loss: 1.29; acc: 0.7
Batch: 760; loss: 1.19; acc: 0.73
Batch: 780; loss: 1.29; acc: 0.62
Train Epoch over. train_loss: 1.23; train_accuracy: 0.69 

0.00012934747792314738
0.00012329396849963814
Batch: 0; loss: 1.29; acc: 0.59
Batch: 20; loss: 1.31; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.84
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.81
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.72
Batch: 140; loss: 1.02; acc: 0.86
Val Epoch over. val_loss: 1.1739535840453617; val_accuracy: 0.7293988853503185 

The current subspace-distance is: 0.00012329396849963814 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.14; acc: 0.73
Batch: 20; loss: 1.22; acc: 0.73
Batch: 40; loss: 1.23; acc: 0.75
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.22; acc: 0.7
Batch: 100; loss: 1.01; acc: 0.84
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 1.25; acc: 0.66
Batch: 160; loss: 1.15; acc: 0.75
Batch: 180; loss: 1.3; acc: 0.66
Batch: 200; loss: 1.13; acc: 0.75
Batch: 220; loss: 1.36; acc: 0.64
Batch: 240; loss: 1.2; acc: 0.7
Batch: 260; loss: 1.15; acc: 0.72
Batch: 280; loss: 1.29; acc: 0.69
Batch: 300; loss: 1.14; acc: 0.75
Batch: 320; loss: 1.11; acc: 0.78
Batch: 340; loss: 1.13; acc: 0.75
Batch: 360; loss: 1.26; acc: 0.67
Batch: 380; loss: 1.3; acc: 0.66
Batch: 400; loss: 1.23; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.7
Batch: 440; loss: 1.3; acc: 0.67
Batch: 460; loss: 1.19; acc: 0.73
Batch: 480; loss: 1.28; acc: 0.69
Batch: 500; loss: 1.1; acc: 0.78
Batch: 520; loss: 1.27; acc: 0.66
Batch: 540; loss: 1.21; acc: 0.77
Batch: 560; loss: 1.2; acc: 0.69
Batch: 580; loss: 1.28; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.27; acc: 0.66
Batch: 640; loss: 1.14; acc: 0.72
Batch: 660; loss: 1.18; acc: 0.69
Batch: 680; loss: 1.11; acc: 0.83
Batch: 700; loss: 1.36; acc: 0.69
Batch: 720; loss: 1.22; acc: 0.72
Batch: 740; loss: 1.16; acc: 0.75
Batch: 760; loss: 1.23; acc: 0.69
Batch: 780; loss: 1.1; acc: 0.78
Train Epoch over. train_loss: 1.23; train_accuracy: 0.69 

0.00013238507381174713
0.0001249329507118091
Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 0.91; acc: 0.84
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.83
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.73
Batch: 140; loss: 1.02; acc: 0.89
Val Epoch over. val_loss: 1.1823062892932041; val_accuracy: 0.7264132165605095 

The current subspace-distance is: 0.0001249329507118091 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.78
Batch: 20; loss: 1.16; acc: 0.8
Batch: 40; loss: 1.22; acc: 0.64
Batch: 60; loss: 1.33; acc: 0.56
Batch: 80; loss: 1.3; acc: 0.7
Batch: 100; loss: 1.24; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 1.12; acc: 0.73
Batch: 160; loss: 1.36; acc: 0.67
Batch: 180; loss: 1.24; acc: 0.69
Batch: 200; loss: 1.12; acc: 0.77
Batch: 220; loss: 1.45; acc: 0.64
Batch: 240; loss: 1.32; acc: 0.64
Batch: 260; loss: 1.26; acc: 0.7
Batch: 280; loss: 1.31; acc: 0.56
Batch: 300; loss: 1.18; acc: 0.67
Batch: 320; loss: 1.12; acc: 0.69
Batch: 340; loss: 1.25; acc: 0.64
Batch: 360; loss: 1.2; acc: 0.7
Batch: 380; loss: 1.17; acc: 0.7
Batch: 400; loss: 1.29; acc: 0.72
Batch: 420; loss: 1.21; acc: 0.73
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.29; acc: 0.64
Batch: 480; loss: 1.27; acc: 0.64
Batch: 500; loss: 1.23; acc: 0.7
Batch: 520; loss: 1.17; acc: 0.72
Batch: 540; loss: 1.25; acc: 0.66
Batch: 560; loss: 1.21; acc: 0.66
Batch: 580; loss: 1.31; acc: 0.62
Batch: 600; loss: 1.47; acc: 0.53
Batch: 620; loss: 1.18; acc: 0.73
Batch: 640; loss: 1.24; acc: 0.67
Batch: 660; loss: 1.3; acc: 0.67
Batch: 680; loss: 1.09; acc: 0.73
Batch: 700; loss: 1.28; acc: 0.7
Batch: 720; loss: 1.29; acc: 0.61
Batch: 740; loss: 1.25; acc: 0.72
Batch: 760; loss: 1.18; acc: 0.69
Batch: 780; loss: 1.2; acc: 0.69
Train Epoch over. train_loss: 1.22; train_accuracy: 0.69 

0.0001323573087574914
0.00012604759831447154
Batch: 0; loss: 1.29; acc: 0.61
Batch: 20; loss: 1.31; acc: 0.61
Batch: 40; loss: 0.9; acc: 0.84
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.81
Batch: 100; loss: 1.16; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.72
Batch: 140; loss: 1.01; acc: 0.89
Val Epoch over. val_loss: 1.1732051786343762; val_accuracy: 0.7259156050955414 

The current subspace-distance is: 0.00012604759831447154 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.21; acc: 0.72
Batch: 40; loss: 1.21; acc: 0.75
Batch: 60; loss: 1.26; acc: 0.66
Batch: 80; loss: 1.13; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 1.19; acc: 0.7
Batch: 160; loss: 1.14; acc: 0.78
Batch: 180; loss: 1.1; acc: 0.75
Batch: 200; loss: 1.23; acc: 0.67
Batch: 220; loss: 1.23; acc: 0.69
Batch: 240; loss: 1.22; acc: 0.62
Batch: 260; loss: 1.23; acc: 0.66
Batch: 280; loss: 1.27; acc: 0.64
Batch: 300; loss: 1.31; acc: 0.69
Batch: 320; loss: 1.14; acc: 0.73
Batch: 340; loss: 1.33; acc: 0.56
Batch: 360; loss: 1.07; acc: 0.77
Batch: 380; loss: 1.22; acc: 0.67
Batch: 400; loss: 1.07; acc: 0.77
Batch: 420; loss: 1.27; acc: 0.67
Batch: 440; loss: 1.16; acc: 0.7
Batch: 460; loss: 1.21; acc: 0.72
Batch: 480; loss: 1.11; acc: 0.73
Batch: 500; loss: 1.2; acc: 0.75
Batch: 520; loss: 1.39; acc: 0.58
Batch: 540; loss: 1.26; acc: 0.72
Batch: 560; loss: 1.34; acc: 0.62
Batch: 580; loss: 1.39; acc: 0.53
Batch: 600; loss: 1.26; acc: 0.64
Batch: 620; loss: 1.2; acc: 0.72
Batch: 640; loss: 1.07; acc: 0.81
Batch: 660; loss: 1.15; acc: 0.73
Batch: 680; loss: 1.15; acc: 0.78
Batch: 700; loss: 1.21; acc: 0.77
Batch: 720; loss: 1.3; acc: 0.66
Batch: 740; loss: 1.17; acc: 0.69
Batch: 760; loss: 1.18; acc: 0.69
Batch: 780; loss: 1.18; acc: 0.72
Train Epoch over. train_loss: 1.22; train_accuracy: 0.69 

0.00013218932144809514
0.00012828613398596644
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.91; acc: 0.84
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.83
Batch: 100; loss: 1.16; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.77
Batch: 140; loss: 1.01; acc: 0.91
Val Epoch over. val_loss: 1.176673518244628; val_accuracy: 0.7321855095541401 

The current subspace-distance is: 0.00012828613398596644 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 1.28; acc: 0.69
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.56
Batch: 100; loss: 1.24; acc: 0.69
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.24; acc: 0.69
Batch: 180; loss: 1.23; acc: 0.73
Batch: 200; loss: 1.26; acc: 0.72
Batch: 220; loss: 1.2; acc: 0.77
Batch: 240; loss: 1.32; acc: 0.69
Batch: 260; loss: 1.24; acc: 0.64
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.1; acc: 0.78
Batch: 320; loss: 1.28; acc: 0.62
Batch: 340; loss: 1.16; acc: 0.69
Batch: 360; loss: 1.24; acc: 0.62
Batch: 380; loss: 1.15; acc: 0.75
Batch: 400; loss: 1.3; acc: 0.69
Batch: 420; loss: 1.23; acc: 0.75
Batch: 440; loss: 1.14; acc: 0.78
Batch: 460; loss: 1.23; acc: 0.66
Batch: 480; loss: 1.19; acc: 0.7
Batch: 500; loss: 1.36; acc: 0.61
Batch: 520; loss: 1.23; acc: 0.7
Batch: 540; loss: 1.23; acc: 0.72
Batch: 560; loss: 1.42; acc: 0.61
Batch: 580; loss: 1.34; acc: 0.61
Batch: 600; loss: 1.03; acc: 0.83
Batch: 620; loss: 1.2; acc: 0.67
Batch: 640; loss: 1.21; acc: 0.73
Batch: 660; loss: 1.26; acc: 0.66
Batch: 680; loss: 1.17; acc: 0.67
Batch: 700; loss: 1.2; acc: 0.67
Batch: 720; loss: 1.22; acc: 0.66
Batch: 740; loss: 1.25; acc: 0.75
Batch: 760; loss: 1.24; acc: 0.7
Batch: 780; loss: 1.36; acc: 0.56
Train Epoch over. train_loss: 1.22; train_accuracy: 0.7 

0.00013187667354941368
0.00012563547352328897
Batch: 0; loss: 1.31; acc: 0.61
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.91; acc: 0.86
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.16; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.73
Batch: 140; loss: 1.01; acc: 0.91
Val Epoch over. val_loss: 1.1772384730873593; val_accuracy: 0.7335788216560509 

The current subspace-distance is: 0.00012563547352328897 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.32; acc: 0.62
Batch: 20; loss: 1.28; acc: 0.73
Batch: 40; loss: 1.12; acc: 0.72
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 1.14; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.83
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.31; acc: 0.58
Batch: 160; loss: 1.18; acc: 0.8
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.31; acc: 0.69
Batch: 220; loss: 1.14; acc: 0.78
Batch: 240; loss: 1.33; acc: 0.62
Batch: 260; loss: 1.02; acc: 0.83
Batch: 280; loss: 1.14; acc: 0.8
Batch: 300; loss: 1.34; acc: 0.59
Batch: 320; loss: 1.18; acc: 0.7
Batch: 340; loss: 1.11; acc: 0.78
Batch: 360; loss: 1.3; acc: 0.73
Batch: 380; loss: 1.21; acc: 0.77
Batch: 400; loss: 1.2; acc: 0.72
Batch: 420; loss: 1.32; acc: 0.58
Batch: 440; loss: 1.13; acc: 0.78
Batch: 460; loss: 1.31; acc: 0.59
Batch: 480; loss: 1.12; acc: 0.78
Batch: 500; loss: 1.29; acc: 0.7
Batch: 520; loss: 1.32; acc: 0.56
Batch: 540; loss: 1.17; acc: 0.73
Batch: 560; loss: 1.13; acc: 0.75
Batch: 580; loss: 1.29; acc: 0.67
Batch: 600; loss: 1.29; acc: 0.58
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 1.14; acc: 0.75
Batch: 660; loss: 1.12; acc: 0.75
Batch: 680; loss: 1.3; acc: 0.61
Batch: 700; loss: 1.22; acc: 0.67
Batch: 720; loss: 1.23; acc: 0.72
Batch: 740; loss: 1.14; acc: 0.73
Batch: 760; loss: 1.25; acc: 0.69
Batch: 780; loss: 1.14; acc: 0.72
Train Epoch over. train_loss: 1.22; train_accuracy: 0.7 

0.00013530245632864535
0.0001298799179494381
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.15; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.77
Batch: 140; loss: 1.01; acc: 0.89
Val Epoch over. val_loss: 1.170654400139098; val_accuracy: 0.7309912420382165 

The current subspace-distance is: 0.0001298799179494381 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 1.34; acc: 0.66
Batch: 40; loss: 1.27; acc: 0.69
Batch: 60; loss: 1.32; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.66
Batch: 100; loss: 1.21; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.69
Batch: 140; loss: 1.28; acc: 0.69
Batch: 160; loss: 1.18; acc: 0.75
Batch: 180; loss: 1.37; acc: 0.62
Batch: 200; loss: 1.16; acc: 0.72
Batch: 220; loss: 1.3; acc: 0.66
Batch: 240; loss: 1.23; acc: 0.69
Batch: 260; loss: 1.18; acc: 0.69
Batch: 280; loss: 1.19; acc: 0.69
Batch: 300; loss: 1.19; acc: 0.7
Batch: 320; loss: 1.32; acc: 0.66
Batch: 340; loss: 1.21; acc: 0.69
Batch: 360; loss: 1.33; acc: 0.61
Batch: 380; loss: 1.19; acc: 0.75
Batch: 400; loss: 1.29; acc: 0.67
Batch: 420; loss: 1.18; acc: 0.7
Batch: 440; loss: 1.11; acc: 0.8
Batch: 460; loss: 1.15; acc: 0.75
Batch: 480; loss: 1.21; acc: 0.73
Batch: 500; loss: 1.34; acc: 0.61
Batch: 520; loss: 1.19; acc: 0.72
Batch: 540; loss: 1.44; acc: 0.58
Batch: 560; loss: 1.24; acc: 0.69
Batch: 580; loss: 1.1; acc: 0.72
Batch: 600; loss: 1.26; acc: 0.67
Batch: 620; loss: 1.17; acc: 0.69
Batch: 640; loss: 1.3; acc: 0.55
Batch: 660; loss: 1.2; acc: 0.7
Batch: 680; loss: 1.18; acc: 0.75
Batch: 700; loss: 1.17; acc: 0.72
Batch: 720; loss: 1.23; acc: 0.72
Batch: 740; loss: 1.16; acc: 0.7
Batch: 760; loss: 1.33; acc: 0.62
Batch: 780; loss: 1.18; acc: 0.7
Train Epoch over. train_loss: 1.21; train_accuracy: 0.7 

0.00013531075092032552
0.00012895496911369264
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.92; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.75
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.17; acc: 0.77
Batch: 120; loss: 1.22; acc: 0.77
Batch: 140; loss: 1.0; acc: 0.91
Val Epoch over. val_loss: 1.171207105657857; val_accuracy: 0.7309912420382165 

The current subspace-distance is: 0.00012895496911369264 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.81
Batch: 20; loss: 1.18; acc: 0.72
Batch: 40; loss: 1.17; acc: 0.75
Batch: 60; loss: 1.27; acc: 0.72
Batch: 80; loss: 1.19; acc: 0.72
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.19; acc: 0.7
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.28; acc: 0.66
Batch: 180; loss: 1.31; acc: 0.61
Batch: 200; loss: 1.24; acc: 0.67
Batch: 220; loss: 1.22; acc: 0.66
Batch: 240; loss: 1.16; acc: 0.69
Batch: 260; loss: 1.19; acc: 0.64
Batch: 280; loss: 1.35; acc: 0.52
Batch: 300; loss: 1.25; acc: 0.67
Batch: 320; loss: 1.2; acc: 0.73
Batch: 340; loss: 1.17; acc: 0.67
Batch: 360; loss: 1.42; acc: 0.58
Batch: 380; loss: 1.27; acc: 0.66
Batch: 400; loss: 1.29; acc: 0.7
Batch: 420; loss: 1.19; acc: 0.73
Batch: 440; loss: 1.24; acc: 0.73
Batch: 460; loss: 1.39; acc: 0.62
Batch: 480; loss: 1.2; acc: 0.67
Batch: 500; loss: 1.16; acc: 0.73
Batch: 520; loss: 1.09; acc: 0.78
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.24; acc: 0.73
Batch: 580; loss: 1.07; acc: 0.81
Batch: 600; loss: 1.11; acc: 0.77
Batch: 620; loss: 1.2; acc: 0.7
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.19; acc: 0.73
Batch: 680; loss: 1.24; acc: 0.64
Batch: 700; loss: 1.25; acc: 0.61
Batch: 720; loss: 1.18; acc: 0.75
Batch: 740; loss: 1.21; acc: 0.69
Batch: 760; loss: 1.22; acc: 0.7
Batch: 780; loss: 1.2; acc: 0.64
Train Epoch over. train_loss: 1.21; train_accuracy: 0.7 

0.00013873906573280692
0.0001338617003057152
Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 1.27; acc: 0.62
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.86
Batch: 100; loss: 1.15; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.77
Batch: 140; loss: 1.0; acc: 0.89
Val Epoch over. val_loss: 1.165074298715895; val_accuracy: 0.7338773885350318 

The current subspace-distance is: 0.0001338617003057152 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.77
Batch: 40; loss: 1.3; acc: 0.64
Batch: 60; loss: 1.25; acc: 0.55
Batch: 80; loss: 1.14; acc: 0.72
Batch: 100; loss: 1.37; acc: 0.58
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.16; acc: 0.77
Batch: 160; loss: 1.14; acc: 0.7
Batch: 180; loss: 1.08; acc: 0.7
Batch: 200; loss: 1.28; acc: 0.66
Batch: 220; loss: 1.14; acc: 0.7
Batch: 240; loss: 1.12; acc: 0.77
Batch: 260; loss: 1.14; acc: 0.75
Batch: 280; loss: 1.3; acc: 0.66
Batch: 300; loss: 1.18; acc: 0.7
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.21; acc: 0.75
Batch: 360; loss: 1.19; acc: 0.75
Batch: 380; loss: 1.15; acc: 0.77
Batch: 400; loss: 1.26; acc: 0.64
Batch: 420; loss: 1.19; acc: 0.72
Batch: 440; loss: 1.22; acc: 0.7
Batch: 460; loss: 1.19; acc: 0.72
Batch: 480; loss: 1.07; acc: 0.73
Batch: 500; loss: 1.33; acc: 0.64
Batch: 520; loss: 1.13; acc: 0.81
Batch: 540; loss: 1.2; acc: 0.7
Batch: 560; loss: 1.18; acc: 0.73
Batch: 580; loss: 1.19; acc: 0.7
Batch: 600; loss: 1.34; acc: 0.64
Batch: 620; loss: 1.12; acc: 0.72
Batch: 640; loss: 1.15; acc: 0.77
Batch: 660; loss: 1.25; acc: 0.69
Batch: 680; loss: 1.3; acc: 0.62
Batch: 700; loss: 1.25; acc: 0.61
Batch: 720; loss: 1.39; acc: 0.62
Batch: 740; loss: 1.16; acc: 0.77
Batch: 760; loss: 1.27; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.72
Train Epoch over. train_loss: 1.2; train_accuracy: 0.7 

0.00013785940245725214
0.00013234168000053614
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.14; acc: 0.75
Batch: 120; loss: 1.19; acc: 0.73
Batch: 140; loss: 1.0; acc: 0.86
Val Epoch over. val_loss: 1.1595199977516368; val_accuracy: 0.7290007961783439 

The current subspace-distance is: 0.00013234168000053614 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 1.0; acc: 0.83
Batch: 60; loss: 1.26; acc: 0.66
Batch: 80; loss: 1.26; acc: 0.61
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.26; acc: 0.69
Batch: 160; loss: 1.15; acc: 0.69
Batch: 180; loss: 1.15; acc: 0.73
Batch: 200; loss: 1.16; acc: 0.75
Batch: 220; loss: 1.26; acc: 0.67
Batch: 240; loss: 1.33; acc: 0.62
Batch: 260; loss: 1.1; acc: 0.78
Batch: 280; loss: 1.19; acc: 0.72
Batch: 300; loss: 1.17; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.83
Batch: 340; loss: 1.22; acc: 0.72
Batch: 360; loss: 1.07; acc: 0.78
Batch: 380; loss: 1.29; acc: 0.67
Batch: 400; loss: 1.22; acc: 0.67
Batch: 420; loss: 1.21; acc: 0.67
Batch: 440; loss: 1.18; acc: 0.67
Batch: 460; loss: 1.18; acc: 0.73
Batch: 480; loss: 1.19; acc: 0.67
Batch: 500; loss: 1.24; acc: 0.7
Batch: 520; loss: 1.15; acc: 0.78
Batch: 540; loss: 1.26; acc: 0.7
Batch: 560; loss: 1.08; acc: 0.73
Batch: 580; loss: 1.16; acc: 0.7
Batch: 600; loss: 1.21; acc: 0.72
Batch: 620; loss: 1.14; acc: 0.75
Batch: 640; loss: 1.13; acc: 0.73
Batch: 660; loss: 1.2; acc: 0.62
Batch: 680; loss: 1.17; acc: 0.73
Batch: 700; loss: 1.26; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.72
Batch: 740; loss: 1.24; acc: 0.66
Batch: 760; loss: 1.19; acc: 0.73
Batch: 780; loss: 1.25; acc: 0.62
Train Epoch over. train_loss: 1.2; train_accuracy: 0.7 

0.00014176122203934938
0.00013387521903496236
Batch: 0; loss: 1.28; acc: 0.67
Batch: 20; loss: 1.25; acc: 0.66
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.11; acc: 0.77
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 0.98; acc: 0.88
Val Epoch over. val_loss: 1.1436693884764508; val_accuracy: 0.7383558917197452 

The current subspace-distance is: 0.00013387521903496236 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.66
Batch: 40; loss: 1.29; acc: 0.64
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.19; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.73
Batch: 140; loss: 1.14; acc: 0.78
Batch: 160; loss: 1.17; acc: 0.69
Batch: 180; loss: 1.16; acc: 0.77
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 1.25; acc: 0.66
Batch: 240; loss: 1.14; acc: 0.75
Batch: 260; loss: 1.19; acc: 0.72
Batch: 280; loss: 1.32; acc: 0.56
Batch: 300; loss: 1.24; acc: 0.69
Batch: 320; loss: 1.17; acc: 0.72
Batch: 340; loss: 1.17; acc: 0.77
Batch: 360; loss: 1.23; acc: 0.7
Batch: 380; loss: 1.31; acc: 0.64
Batch: 400; loss: 1.22; acc: 0.64
Batch: 420; loss: 1.15; acc: 0.75
Batch: 440; loss: 1.11; acc: 0.78
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.18; acc: 0.73
Batch: 500; loss: 1.19; acc: 0.64
Batch: 520; loss: 1.11; acc: 0.73
Batch: 540; loss: 1.08; acc: 0.72
Batch: 560; loss: 1.17; acc: 0.72
Batch: 580; loss: 1.1; acc: 0.77
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.12; acc: 0.7
Batch: 640; loss: 1.26; acc: 0.67
Batch: 660; loss: 1.27; acc: 0.62
Batch: 680; loss: 1.3; acc: 0.62
Batch: 700; loss: 1.31; acc: 0.67
Batch: 720; loss: 1.29; acc: 0.62
Batch: 740; loss: 1.27; acc: 0.62
Batch: 760; loss: 1.27; acc: 0.67
Batch: 780; loss: 1.19; acc: 0.66
Train Epoch over. train_loss: 1.19; train_accuracy: 0.7 

0.00013894842413719743
0.00013287784531712532
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.66
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.84
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 0.97; acc: 0.86
Val Epoch over. val_loss: 1.1368205710581154; val_accuracy: 0.7377587579617835 

The current subspace-distance is: 0.00013287784531712532 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.26; acc: 0.67
Batch: 20; loss: 1.14; acc: 0.73
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 1.15; acc: 0.73
Batch: 100; loss: 1.25; acc: 0.61
Batch: 120; loss: 1.2; acc: 0.75
Batch: 140; loss: 1.29; acc: 0.69
Batch: 160; loss: 1.21; acc: 0.67
Batch: 180; loss: 1.31; acc: 0.66
Batch: 200; loss: 1.05; acc: 0.75
Batch: 220; loss: 1.24; acc: 0.69
Batch: 240; loss: 1.16; acc: 0.67
Batch: 260; loss: 1.3; acc: 0.59
Batch: 280; loss: 1.1; acc: 0.78
Batch: 300; loss: 0.98; acc: 0.83
Batch: 320; loss: 1.25; acc: 0.72
Batch: 340; loss: 1.22; acc: 0.72
Batch: 360; loss: 1.35; acc: 0.59
Batch: 380; loss: 1.37; acc: 0.62
Batch: 400; loss: 1.19; acc: 0.69
Batch: 420; loss: 1.13; acc: 0.73
Batch: 440; loss: 1.15; acc: 0.77
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.33; acc: 0.62
Batch: 500; loss: 1.05; acc: 0.75
Batch: 520; loss: 1.0; acc: 0.83
Batch: 540; loss: 1.3; acc: 0.67
Batch: 560; loss: 1.3; acc: 0.7
Batch: 580; loss: 1.21; acc: 0.72
Batch: 600; loss: 1.12; acc: 0.75
Batch: 620; loss: 1.35; acc: 0.56
Batch: 640; loss: 1.17; acc: 0.67
Batch: 660; loss: 1.2; acc: 0.69
Batch: 680; loss: 1.21; acc: 0.72
Batch: 700; loss: 1.12; acc: 0.73
Batch: 720; loss: 1.19; acc: 0.62
Batch: 740; loss: 1.05; acc: 0.8
Batch: 760; loss: 1.14; acc: 0.73
Batch: 780; loss: 1.2; acc: 0.73
Train Epoch over. train_loss: 1.19; train_accuracy: 0.7 

0.00013990956358611584
0.00013358340947888792
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.84
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 0.97; acc: 0.89
Val Epoch over. val_loss: 1.1362659111144437; val_accuracy: 0.7402468152866242 

The current subspace-distance is: 0.00013358340947888792 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.12; acc: 0.75
Batch: 20; loss: 1.22; acc: 0.7
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 1.23; acc: 0.7
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.17; acc: 0.75
Batch: 160; loss: 1.26; acc: 0.72
Batch: 180; loss: 1.17; acc: 0.73
Batch: 200; loss: 1.1; acc: 0.72
Batch: 220; loss: 1.19; acc: 0.72
Batch: 240; loss: 1.2; acc: 0.77
Batch: 260; loss: 1.21; acc: 0.66
Batch: 280; loss: 1.34; acc: 0.69
Batch: 300; loss: 1.1; acc: 0.72
Batch: 320; loss: 1.34; acc: 0.61
Batch: 340; loss: 1.37; acc: 0.56
Batch: 360; loss: 1.27; acc: 0.69
Batch: 380; loss: 1.11; acc: 0.72
Batch: 400; loss: 1.31; acc: 0.64
Batch: 420; loss: 1.1; acc: 0.72
Batch: 440; loss: 1.08; acc: 0.75
Batch: 460; loss: 1.25; acc: 0.69
Batch: 480; loss: 1.31; acc: 0.59
Batch: 500; loss: 1.18; acc: 0.75
Batch: 520; loss: 1.24; acc: 0.72
Batch: 540; loss: 1.22; acc: 0.69
Batch: 560; loss: 1.19; acc: 0.7
Batch: 580; loss: 1.22; acc: 0.67
Batch: 600; loss: 1.25; acc: 0.77
Batch: 620; loss: 1.31; acc: 0.61
Batch: 640; loss: 1.25; acc: 0.61
Batch: 660; loss: 1.1; acc: 0.69
Batch: 680; loss: 1.1; acc: 0.75
Batch: 700; loss: 1.38; acc: 0.66
Batch: 720; loss: 1.34; acc: 0.62
Batch: 740; loss: 1.19; acc: 0.75
Batch: 760; loss: 1.26; acc: 0.69
Batch: 780; loss: 1.29; acc: 0.55
Train Epoch over. train_loss: 1.19; train_accuracy: 0.7 

0.0001440757478121668
0.00013593400944955647
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.91; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.18; acc: 0.72
Batch: 140; loss: 0.98; acc: 0.89
Val Epoch over. val_loss: 1.1430649901651273; val_accuracy: 0.7377587579617835 

The current subspace-distance is: 0.00013593400944955647 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.64
Batch: 40; loss: 1.36; acc: 0.58
Batch: 60; loss: 1.16; acc: 0.75
Batch: 80; loss: 1.14; acc: 0.67
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.17; acc: 0.7
Batch: 140; loss: 1.16; acc: 0.69
Batch: 160; loss: 1.18; acc: 0.78
Batch: 180; loss: 1.11; acc: 0.77
Batch: 200; loss: 1.25; acc: 0.66
Batch: 220; loss: 1.17; acc: 0.73
Batch: 240; loss: 1.09; acc: 0.75
Batch: 260; loss: 1.26; acc: 0.67
Batch: 280; loss: 1.1; acc: 0.78
Batch: 300; loss: 1.15; acc: 0.78
Batch: 320; loss: 1.27; acc: 0.62
Batch: 340; loss: 1.13; acc: 0.73
Batch: 360; loss: 1.04; acc: 0.84
Batch: 380; loss: 1.16; acc: 0.77
Batch: 400; loss: 1.11; acc: 0.7
Batch: 420; loss: 1.06; acc: 0.8
Batch: 440; loss: 1.19; acc: 0.7
Batch: 460; loss: 1.22; acc: 0.62
Batch: 480; loss: 1.19; acc: 0.64
Batch: 500; loss: 1.22; acc: 0.69
Batch: 520; loss: 1.22; acc: 0.67
Batch: 540; loss: 1.16; acc: 0.73
Batch: 560; loss: 1.11; acc: 0.72
Batch: 580; loss: 1.24; acc: 0.72
Batch: 600; loss: 1.24; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.7
Batch: 640; loss: 1.11; acc: 0.77
Batch: 660; loss: 1.27; acc: 0.66
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.32; acc: 0.64
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.72
Batch: 780; loss: 1.23; acc: 0.66
Train Epoch over. train_loss: 1.19; train_accuracy: 0.7 

0.0001410007826052606
0.0001349348167423159
Batch: 0; loss: 1.27; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.66
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 0.97; acc: 0.89
Val Epoch over. val_loss: 1.135627303153846; val_accuracy: 0.7420382165605095 

The current subspace-distance is: 0.0001349348167423159 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.75
Batch: 20; loss: 1.19; acc: 0.72
Batch: 40; loss: 1.19; acc: 0.66
Batch: 60; loss: 1.31; acc: 0.66
Batch: 80; loss: 1.29; acc: 0.69
Batch: 100; loss: 1.16; acc: 0.77
Batch: 120; loss: 1.18; acc: 0.73
Batch: 140; loss: 1.13; acc: 0.75
Batch: 160; loss: 1.24; acc: 0.67
Batch: 180; loss: 1.29; acc: 0.64
Batch: 200; loss: 1.3; acc: 0.62
Batch: 220; loss: 1.28; acc: 0.61
Batch: 240; loss: 1.12; acc: 0.75
Batch: 260; loss: 1.12; acc: 0.78
Batch: 280; loss: 1.28; acc: 0.59
Batch: 300; loss: 1.06; acc: 0.7
Batch: 320; loss: 1.14; acc: 0.69
Batch: 340; loss: 1.12; acc: 0.7
Batch: 360; loss: 1.14; acc: 0.72
Batch: 380; loss: 1.16; acc: 0.75
Batch: 400; loss: 1.13; acc: 0.75
Batch: 420; loss: 1.22; acc: 0.69
Batch: 440; loss: 1.12; acc: 0.73
Batch: 460; loss: 1.29; acc: 0.64
Batch: 480; loss: 1.19; acc: 0.73
Batch: 500; loss: 1.14; acc: 0.73
Batch: 520; loss: 1.2; acc: 0.66
Batch: 540; loss: 1.2; acc: 0.66
Batch: 560; loss: 1.31; acc: 0.61
Batch: 580; loss: 1.08; acc: 0.78
Batch: 600; loss: 1.3; acc: 0.62
Batch: 620; loss: 1.18; acc: 0.69
Batch: 640; loss: 1.28; acc: 0.62
Batch: 660; loss: 1.24; acc: 0.72
Batch: 680; loss: 1.17; acc: 0.69
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.11; acc: 0.72
Batch: 740; loss: 1.12; acc: 0.77
Batch: 760; loss: 1.32; acc: 0.66
Batch: 780; loss: 1.19; acc: 0.67
Train Epoch over. train_loss: 1.19; train_accuracy: 0.7 

0.0001423820067429915
0.00013444472278933972
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.83
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 0.96; acc: 0.88
Val Epoch over. val_loss: 1.129788852041694; val_accuracy: 0.7378582802547771 

The current subspace-distance is: 0.00013444472278933972 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.17; acc: 0.7
Batch: 20; loss: 1.21; acc: 0.69
Batch: 40; loss: 1.18; acc: 0.73
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.17; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 1.19; acc: 0.64
Batch: 160; loss: 1.13; acc: 0.73
Batch: 180; loss: 1.24; acc: 0.66
Batch: 200; loss: 1.29; acc: 0.66
Batch: 220; loss: 1.18; acc: 0.7
Batch: 240; loss: 1.32; acc: 0.58
Batch: 260; loss: 1.41; acc: 0.56
Batch: 280; loss: 1.27; acc: 0.67
Batch: 300; loss: 1.22; acc: 0.67
Batch: 320; loss: 1.22; acc: 0.7
Batch: 340; loss: 1.23; acc: 0.61
Batch: 360; loss: 1.21; acc: 0.7
Batch: 380; loss: 1.25; acc: 0.67
Batch: 400; loss: 1.28; acc: 0.7
Batch: 420; loss: 1.23; acc: 0.72
Batch: 440; loss: 1.01; acc: 0.83
Batch: 460; loss: 1.22; acc: 0.77
Batch: 480; loss: 1.24; acc: 0.67
Batch: 500; loss: 1.09; acc: 0.75
Batch: 520; loss: 1.2; acc: 0.69
Batch: 540; loss: 0.96; acc: 0.88
Batch: 560; loss: 1.25; acc: 0.64
Batch: 580; loss: 1.1; acc: 0.81
Batch: 600; loss: 1.22; acc: 0.67
Batch: 620; loss: 1.22; acc: 0.72
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 1.22; acc: 0.66
Batch: 680; loss: 1.18; acc: 0.69
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.13; acc: 0.73
Batch: 740; loss: 1.27; acc: 0.64
Batch: 760; loss: 1.15; acc: 0.69
Batch: 780; loss: 1.16; acc: 0.75
Train Epoch over. train_loss: 1.18; train_accuracy: 0.7 

0.00014206950436346233
0.00013540254440158606
Batch: 0; loss: 1.27; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.84
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.72
Batch: 140; loss: 0.97; acc: 0.88
Val Epoch over. val_loss: 1.1302870500619244; val_accuracy: 0.7395501592356688 

The current subspace-distance is: 0.00013540254440158606 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.7
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.1; acc: 0.75
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.69
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.29; acc: 0.67
Batch: 180; loss: 1.33; acc: 0.64
Batch: 200; loss: 1.08; acc: 0.78
Batch: 220; loss: 1.17; acc: 0.73
Batch: 240; loss: 1.16; acc: 0.69
Batch: 260; loss: 1.08; acc: 0.73
Batch: 280; loss: 1.27; acc: 0.64
Batch: 300; loss: 1.26; acc: 0.64
Batch: 320; loss: 1.27; acc: 0.64
Batch: 340; loss: 1.14; acc: 0.67
Batch: 360; loss: 1.23; acc: 0.66
Batch: 380; loss: 1.16; acc: 0.77
Batch: 400; loss: 1.23; acc: 0.66
Batch: 420; loss: 1.06; acc: 0.75
Batch: 440; loss: 1.19; acc: 0.75
Batch: 460; loss: 1.16; acc: 0.72
Batch: 480; loss: 1.31; acc: 0.56
Batch: 500; loss: 1.24; acc: 0.66
Batch: 520; loss: 1.1; acc: 0.75
Batch: 540; loss: 1.09; acc: 0.78
Batch: 560; loss: 1.21; acc: 0.69
Batch: 580; loss: 1.11; acc: 0.75
Batch: 600; loss: 1.22; acc: 0.67
Batch: 620; loss: 1.0; acc: 0.8
Batch: 640; loss: 1.07; acc: 0.75
Batch: 660; loss: 1.05; acc: 0.77
Batch: 680; loss: 1.06; acc: 0.72
Batch: 700; loss: 1.29; acc: 0.58
Batch: 720; loss: 1.21; acc: 0.7
Batch: 740; loss: 1.23; acc: 0.7
Batch: 760; loss: 1.23; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.66
Train Epoch over. train_loss: 1.18; train_accuracy: 0.7 

0.00014345326053444296
0.00013764544564764947
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.23; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.75
Batch: 140; loss: 0.96; acc: 0.89
Val Epoch over. val_loss: 1.1309185798760433; val_accuracy: 0.7385549363057324 

The current subspace-distance is: 0.00013764544564764947 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 1.14; acc: 0.75
Batch: 40; loss: 1.0; acc: 0.8
Batch: 60; loss: 1.24; acc: 0.59
Batch: 80; loss: 1.2; acc: 0.67
Batch: 100; loss: 1.19; acc: 0.72
Batch: 120; loss: 1.19; acc: 0.78
Batch: 140; loss: 1.15; acc: 0.8
Batch: 160; loss: 1.14; acc: 0.77
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 1.2; acc: 0.69
Batch: 220; loss: 1.27; acc: 0.72
Batch: 240; loss: 1.26; acc: 0.72
Batch: 260; loss: 1.05; acc: 0.8
Batch: 280; loss: 1.19; acc: 0.73
Batch: 300; loss: 1.06; acc: 0.78
Batch: 320; loss: 1.17; acc: 0.77
Batch: 340; loss: 1.23; acc: 0.72
Batch: 360; loss: 1.25; acc: 0.62
Batch: 380; loss: 1.15; acc: 0.69
Batch: 400; loss: 1.12; acc: 0.75
Batch: 420; loss: 1.3; acc: 0.66
Batch: 440; loss: 1.14; acc: 0.72
Batch: 460; loss: 1.16; acc: 0.7
Batch: 480; loss: 1.11; acc: 0.77
Batch: 500; loss: 1.15; acc: 0.67
Batch: 520; loss: 1.13; acc: 0.77
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.26; acc: 0.66
Batch: 580; loss: 1.27; acc: 0.61
Batch: 600; loss: 1.11; acc: 0.78
Batch: 620; loss: 1.23; acc: 0.67
Batch: 640; loss: 1.23; acc: 0.67
Batch: 660; loss: 1.18; acc: 0.7
Batch: 680; loss: 0.93; acc: 0.88
Batch: 700; loss: 1.31; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.72
Batch: 740; loss: 1.15; acc: 0.77
Batch: 760; loss: 1.2; acc: 0.69
Batch: 780; loss: 1.2; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.71 

0.0001423908834112808
0.00013532293087337166
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.62
Batch: 40; loss: 0.88; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 0.98; acc: 0.88
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.14; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.88
Val Epoch over. val_loss: 1.1075231144382696; val_accuracy: 0.7483081210191083 

The current subspace-distance is: 0.00013532293087337166 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.7
Batch: 40; loss: 1.33; acc: 0.61
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 1.27; acc: 0.66
Batch: 100; loss: 1.34; acc: 0.56
Batch: 120; loss: 1.02; acc: 0.8
Batch: 140; loss: 1.02; acc: 0.78
Batch: 160; loss: 1.2; acc: 0.72
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 1.23; acc: 0.61
Batch: 220; loss: 1.04; acc: 0.83
Batch: 240; loss: 1.14; acc: 0.75
Batch: 260; loss: 1.33; acc: 0.62
Batch: 280; loss: 1.14; acc: 0.73
Batch: 300; loss: 1.13; acc: 0.75
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.31; acc: 0.62
Batch: 360; loss: 1.04; acc: 0.75
Batch: 380; loss: 1.15; acc: 0.75
Batch: 400; loss: 1.34; acc: 0.55
Batch: 420; loss: 1.2; acc: 0.69
Batch: 440; loss: 1.15; acc: 0.73
Batch: 460; loss: 1.22; acc: 0.72
Batch: 480; loss: 1.04; acc: 0.78
Batch: 500; loss: 1.19; acc: 0.7
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 1.32; acc: 0.59
Batch: 580; loss: 1.42; acc: 0.56
Batch: 600; loss: 1.29; acc: 0.61
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 1.11; acc: 0.75
Batch: 660; loss: 1.17; acc: 0.72
Batch: 680; loss: 1.27; acc: 0.67
Batch: 700; loss: 1.18; acc: 0.62
Batch: 720; loss: 1.16; acc: 0.8
Batch: 740; loss: 1.2; acc: 0.7
Batch: 760; loss: 1.36; acc: 0.64
Batch: 780; loss: 1.23; acc: 0.72
Train Epoch over. train_loss: 1.18; train_accuracy: 0.71 

0.00014163547893986106
0.00013588399451691657
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 0.9; acc: 0.84
Batch: 60; loss: 1.12; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.86
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 0.95; acc: 0.88
Val Epoch over. val_loss: 1.1238780082411068; val_accuracy: 0.7437300955414012 

The current subspace-distance is: 0.00013588399451691657 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 1.29; acc: 0.66
Batch: 40; loss: 1.1; acc: 0.73
Batch: 60; loss: 1.19; acc: 0.73
Batch: 80; loss: 1.19; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.73
Batch: 140; loss: 1.19; acc: 0.72
Batch: 160; loss: 1.29; acc: 0.56
Batch: 180; loss: 1.13; acc: 0.73
Batch: 200; loss: 1.22; acc: 0.7
Batch: 220; loss: 1.17; acc: 0.75
Batch: 240; loss: 1.2; acc: 0.77
Batch: 260; loss: 1.27; acc: 0.7
Batch: 280; loss: 1.16; acc: 0.69
Batch: 300; loss: 1.17; acc: 0.72
Batch: 320; loss: 1.09; acc: 0.7
Batch: 340; loss: 1.17; acc: 0.67
Batch: 360; loss: 1.12; acc: 0.7
Batch: 380; loss: 1.23; acc: 0.56
Batch: 400; loss: 1.29; acc: 0.61
Batch: 420; loss: 1.04; acc: 0.81
Batch: 440; loss: 1.18; acc: 0.75
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 1.04; acc: 0.84
Batch: 500; loss: 1.14; acc: 0.72
Batch: 520; loss: 1.32; acc: 0.66
Batch: 540; loss: 1.15; acc: 0.75
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 1.04; acc: 0.8
Batch: 600; loss: 1.01; acc: 0.78
Batch: 620; loss: 1.27; acc: 0.67
Batch: 640; loss: 1.12; acc: 0.75
Batch: 660; loss: 1.08; acc: 0.75
Batch: 680; loss: 1.17; acc: 0.72
Batch: 700; loss: 1.03; acc: 0.78
Batch: 720; loss: 1.1; acc: 0.8
Batch: 740; loss: 1.14; acc: 0.73
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.29; acc: 0.67
Train Epoch over. train_loss: 1.17; train_accuracy: 0.71 

0.00014484929852187634
0.0001391320110997185
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.9; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.83
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 0.97; acc: 0.88
Val Epoch over. val_loss: 1.1301236164038349; val_accuracy: 0.7399482484076433 

The current subspace-distance is: 0.0001391320110997185 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.17; acc: 0.73
Batch: 20; loss: 1.19; acc: 0.7
Batch: 40; loss: 1.14; acc: 0.72
Batch: 60; loss: 1.1; acc: 0.77
Batch: 80; loss: 1.22; acc: 0.7
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 1.06; acc: 0.77
Batch: 140; loss: 1.05; acc: 0.75
Batch: 160; loss: 1.27; acc: 0.61
Batch: 180; loss: 1.09; acc: 0.7
Batch: 200; loss: 1.38; acc: 0.64
Batch: 220; loss: 1.27; acc: 0.61
Batch: 240; loss: 1.29; acc: 0.67
Batch: 260; loss: 1.21; acc: 0.66
Batch: 280; loss: 1.3; acc: 0.62
Batch: 300; loss: 1.0; acc: 0.81
Batch: 320; loss: 1.13; acc: 0.73
Batch: 340; loss: 1.13; acc: 0.73
Batch: 360; loss: 1.23; acc: 0.69
Batch: 380; loss: 1.22; acc: 0.73
Batch: 400; loss: 1.05; acc: 0.78
Batch: 420; loss: 1.19; acc: 0.73
Batch: 440; loss: 1.19; acc: 0.78
Batch: 460; loss: 1.06; acc: 0.77
Batch: 480; loss: 1.16; acc: 0.69
Batch: 500; loss: 1.18; acc: 0.66
Batch: 520; loss: 1.18; acc: 0.67
Batch: 540; loss: 1.09; acc: 0.77
Batch: 560; loss: 1.21; acc: 0.67
Batch: 580; loss: 1.14; acc: 0.7
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.33; acc: 0.52
Batch: 640; loss: 1.37; acc: 0.59
Batch: 660; loss: 1.26; acc: 0.69
Batch: 680; loss: 1.2; acc: 0.62
Batch: 700; loss: 1.34; acc: 0.64
Batch: 720; loss: 1.17; acc: 0.69
Batch: 740; loss: 1.3; acc: 0.64
Batch: 760; loss: 1.22; acc: 0.69
Batch: 780; loss: 1.24; acc: 0.62
Train Epoch over. train_loss: 1.17; train_accuracy: 0.71 

0.0001438924518879503
0.00013761270383838564
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.86
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.84
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.15; acc: 0.73
Batch: 140; loss: 0.95; acc: 0.89
Val Epoch over. val_loss: 1.1197681544692653; val_accuracy: 0.7452229299363057 

The current subspace-distance is: 0.00013761270383838564 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_3_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.928869038633508

The number of parameters is: 275071

The number of individual parameters is:

40
400
40
40
60
50400
60
60
119
149940
119
119
64
68544
64
64
4096
64
640
10
64
64

nonzero elements in E: 55014195
elements in E: 55014200
fraction nonzero: 0.9999999091143741
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.36; acc: 0.11
Batch: 20; loss: 2.13; acc: 0.22
Batch: 40; loss: 2.0; acc: 0.34
Batch: 60; loss: 2.05; acc: 0.38
Batch: 80; loss: 1.91; acc: 0.36
Batch: 100; loss: 1.94; acc: 0.41
Batch: 120; loss: 1.81; acc: 0.47
Batch: 140; loss: 1.86; acc: 0.44
Batch: 160; loss: 1.74; acc: 0.53
Batch: 180; loss: 1.72; acc: 0.53
Batch: 200; loss: 1.77; acc: 0.53
Batch: 220; loss: 1.73; acc: 0.48
Batch: 240; loss: 1.56; acc: 0.7
Batch: 260; loss: 1.73; acc: 0.53
Batch: 280; loss: 1.68; acc: 0.52
Batch: 300; loss: 1.8; acc: 0.45
Batch: 320; loss: 1.54; acc: 0.7
Batch: 340; loss: 1.7; acc: 0.53
Batch: 360; loss: 1.48; acc: 0.7
Batch: 380; loss: 1.64; acc: 0.55
Batch: 400; loss: 1.54; acc: 0.61
Batch: 420; loss: 1.56; acc: 0.64
Batch: 440; loss: 1.46; acc: 0.69
Batch: 460; loss: 1.55; acc: 0.62
Batch: 480; loss: 1.59; acc: 0.59
Batch: 500; loss: 1.54; acc: 0.64
Batch: 520; loss: 1.56; acc: 0.61
Batch: 540; loss: 1.43; acc: 0.64
Batch: 560; loss: 1.47; acc: 0.73
Batch: 580; loss: 1.56; acc: 0.59
Batch: 600; loss: 1.38; acc: 0.7
Batch: 620; loss: 1.44; acc: 0.66
Batch: 640; loss: 1.45; acc: 0.62
Batch: 660; loss: 1.38; acc: 0.75
Batch: 680; loss: 1.46; acc: 0.61
Batch: 700; loss: 1.3; acc: 0.78
Batch: 720; loss: 1.4; acc: 0.72
Batch: 740; loss: 1.44; acc: 0.67
Batch: 760; loss: 1.32; acc: 0.77
Batch: 780; loss: 1.2; acc: 0.81
Train Epoch over. train_loss: 1.62; train_accuracy: 0.59 

6.0133465012768283e-05
5.4958400141913444e-05
Batch: 0; loss: 1.38; acc: 0.66
Batch: 20; loss: 1.44; acc: 0.61
Batch: 40; loss: 1.07; acc: 0.86
Batch: 60; loss: 1.23; acc: 0.75
Batch: 80; loss: 1.25; acc: 0.77
Batch: 100; loss: 1.41; acc: 0.72
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.19; acc: 0.84
Val Epoch over. val_loss: 1.3383462766932834; val_accuracy: 0.7220342356687898 

The current subspace-distance is: 5.4958400141913444e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.39; acc: 0.66
Batch: 20; loss: 1.36; acc: 0.69
Batch: 40; loss: 1.36; acc: 0.69
Batch: 60; loss: 1.32; acc: 0.7
Batch: 80; loss: 1.29; acc: 0.72
Batch: 100; loss: 1.26; acc: 0.81
Batch: 120; loss: 1.23; acc: 0.78
Batch: 140; loss: 1.32; acc: 0.69
Batch: 160; loss: 1.34; acc: 0.69
Batch: 180; loss: 1.5; acc: 0.67
Batch: 200; loss: 1.33; acc: 0.7
Batch: 220; loss: 1.32; acc: 0.7
Batch: 240; loss: 1.36; acc: 0.66
Batch: 260; loss: 1.23; acc: 0.77
Batch: 280; loss: 1.25; acc: 0.67
Batch: 300; loss: 1.34; acc: 0.69
Batch: 320; loss: 1.37; acc: 0.72
Batch: 340; loss: 1.34; acc: 0.62
Batch: 360; loss: 1.24; acc: 0.8
Batch: 380; loss: 1.17; acc: 0.81
Batch: 400; loss: 1.43; acc: 0.55
Batch: 420; loss: 1.41; acc: 0.62
Batch: 440; loss: 1.22; acc: 0.75
Batch: 460; loss: 1.17; acc: 0.8
Batch: 480; loss: 1.2; acc: 0.78
Batch: 500; loss: 1.34; acc: 0.66
Batch: 520; loss: 1.24; acc: 0.75
Batch: 540; loss: 1.15; acc: 0.8
Batch: 560; loss: 1.33; acc: 0.72
Batch: 580; loss: 1.32; acc: 0.72
Batch: 600; loss: 1.31; acc: 0.7
Batch: 620; loss: 1.23; acc: 0.77
Batch: 640; loss: 1.12; acc: 0.77
Batch: 660; loss: 1.42; acc: 0.59
Batch: 680; loss: 1.31; acc: 0.75
Batch: 700; loss: 1.22; acc: 0.73
Batch: 720; loss: 1.15; acc: 0.8
Batch: 740; loss: 1.19; acc: 0.75
Batch: 760; loss: 1.18; acc: 0.77
Batch: 780; loss: 1.08; acc: 0.8
Train Epoch over. train_loss: 1.26; train_accuracy: 0.73 

8.351312135346234e-05
7.963418465806171e-05
Batch: 0; loss: 1.15; acc: 0.77
Batch: 20; loss: 1.28; acc: 0.67
Batch: 40; loss: 0.86; acc: 0.86
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 1.18; acc: 0.8
Batch: 120; loss: 1.25; acc: 0.73
Batch: 140; loss: 1.05; acc: 0.86
Val Epoch over. val_loss: 1.1280530577252625; val_accuracy: 0.7775676751592356 

The current subspace-distance is: 7.963418465806171e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.88
Batch: 40; loss: 1.09; acc: 0.86
Batch: 60; loss: 1.06; acc: 0.8
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.28; acc: 0.75
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.02; acc: 0.86
Batch: 160; loss: 1.1; acc: 0.77
Batch: 180; loss: 1.24; acc: 0.7
Batch: 200; loss: 1.14; acc: 0.81
Batch: 220; loss: 1.11; acc: 0.84
Batch: 240; loss: 1.16; acc: 0.75
Batch: 260; loss: 1.2; acc: 0.75
Batch: 280; loss: 1.17; acc: 0.75
Batch: 300; loss: 1.07; acc: 0.84
Batch: 320; loss: 1.21; acc: 0.73
Batch: 340; loss: 1.06; acc: 0.78
Batch: 360; loss: 1.15; acc: 0.73
Batch: 380; loss: 1.24; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.77
Batch: 440; loss: 1.16; acc: 0.78
Batch: 460; loss: 1.26; acc: 0.67
Batch: 480; loss: 1.11; acc: 0.75
Batch: 500; loss: 1.0; acc: 0.83
Batch: 520; loss: 1.2; acc: 0.75
Batch: 540; loss: 1.16; acc: 0.67
Batch: 560; loss: 1.05; acc: 0.75
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.69
Batch: 620; loss: 1.02; acc: 0.78
Batch: 640; loss: 1.14; acc: 0.73
Batch: 660; loss: 0.94; acc: 0.88
Batch: 680; loss: 1.1; acc: 0.73
Batch: 700; loss: 1.09; acc: 0.75
Batch: 720; loss: 0.94; acc: 0.81
Batch: 740; loss: 0.96; acc: 0.84
Batch: 760; loss: 0.97; acc: 0.8
Batch: 780; loss: 1.14; acc: 0.73
Train Epoch over. train_loss: 1.11; train_accuracy: 0.76 

0.00010013853170676157
9.57149823079817e-05
Batch: 0; loss: 0.99; acc: 0.83
Batch: 20; loss: 1.15; acc: 0.73
Batch: 40; loss: 0.72; acc: 0.92
Batch: 60; loss: 0.9; acc: 0.84
Batch: 80; loss: 0.86; acc: 0.86
Batch: 100; loss: 1.03; acc: 0.84
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 0.89; acc: 0.91
Val Epoch over. val_loss: 1.0062459802171986; val_accuracy: 0.8056329617834395 

The current subspace-distance is: 9.57149823079817e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.78
Batch: 40; loss: 1.13; acc: 0.77
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 1.21; acc: 0.66
Batch: 120; loss: 0.99; acc: 0.81
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 0.99; acc: 0.83
Batch: 180; loss: 1.08; acc: 0.7
Batch: 200; loss: 1.04; acc: 0.73
Batch: 220; loss: 0.97; acc: 0.84
Batch: 240; loss: 0.93; acc: 0.81
Batch: 260; loss: 1.09; acc: 0.75
Batch: 280; loss: 0.97; acc: 0.8
Batch: 300; loss: 0.95; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.8
Batch: 340; loss: 1.16; acc: 0.66
Batch: 360; loss: 1.1; acc: 0.69
Batch: 380; loss: 1.11; acc: 0.77
Batch: 400; loss: 0.84; acc: 0.86
Batch: 420; loss: 0.9; acc: 0.83
Batch: 440; loss: 1.04; acc: 0.75
Batch: 460; loss: 1.14; acc: 0.66
Batch: 480; loss: 1.15; acc: 0.62
Batch: 500; loss: 1.1; acc: 0.77
Batch: 520; loss: 0.95; acc: 0.75
Batch: 540; loss: 0.9; acc: 0.88
Batch: 560; loss: 0.96; acc: 0.77
Batch: 580; loss: 0.88; acc: 0.84
Batch: 600; loss: 1.09; acc: 0.72
Batch: 620; loss: 0.9; acc: 0.83
Batch: 640; loss: 1.09; acc: 0.7
Batch: 660; loss: 1.03; acc: 0.77
Batch: 680; loss: 0.83; acc: 0.88
Batch: 700; loss: 1.06; acc: 0.73
Batch: 720; loss: 0.9; acc: 0.8
Batch: 740; loss: 0.98; acc: 0.84
Batch: 760; loss: 1.0; acc: 0.83
Batch: 780; loss: 1.0; acc: 0.73
Train Epoch over. train_loss: 1.02; train_accuracy: 0.78 

0.0001134216072387062
0.0001094504987122491
Batch: 0; loss: 0.9; acc: 0.89
Batch: 20; loss: 1.07; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.92
Batch: 60; loss: 0.83; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.91
Batch: 100; loss: 0.9; acc: 0.83
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 0.79; acc: 0.91
Val Epoch over. val_loss: 0.930437935006087; val_accuracy: 0.8176751592356688 

The current subspace-distance is: 0.0001094504987122491 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.81
Batch: 20; loss: 0.91; acc: 0.84
Batch: 40; loss: 1.07; acc: 0.77
Batch: 60; loss: 1.03; acc: 0.8
Batch: 80; loss: 0.84; acc: 0.91
Batch: 100; loss: 0.92; acc: 0.77
Batch: 120; loss: 1.04; acc: 0.73
Batch: 140; loss: 0.95; acc: 0.83
Batch: 160; loss: 0.87; acc: 0.83
Batch: 180; loss: 0.9; acc: 0.83
Batch: 200; loss: 0.97; acc: 0.8
Batch: 220; loss: 0.9; acc: 0.8
Batch: 240; loss: 1.06; acc: 0.73
Batch: 260; loss: 0.99; acc: 0.78
Batch: 280; loss: 0.92; acc: 0.81
Batch: 300; loss: 0.94; acc: 0.81
Batch: 320; loss: 1.06; acc: 0.78
Batch: 340; loss: 1.04; acc: 0.75
Batch: 360; loss: 1.07; acc: 0.69
Batch: 380; loss: 1.05; acc: 0.72
Batch: 400; loss: 1.05; acc: 0.72
Batch: 420; loss: 0.99; acc: 0.75
Batch: 440; loss: 1.02; acc: 0.77
Batch: 460; loss: 0.89; acc: 0.83
Batch: 480; loss: 1.03; acc: 0.77
Batch: 500; loss: 1.07; acc: 0.75
Batch: 520; loss: 0.84; acc: 0.83
Batch: 540; loss: 0.89; acc: 0.86
Batch: 560; loss: 0.98; acc: 0.75
Batch: 580; loss: 0.87; acc: 0.8
Batch: 600; loss: 0.89; acc: 0.83
Batch: 620; loss: 0.86; acc: 0.84
Batch: 640; loss: 0.88; acc: 0.89
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.97; acc: 0.83
Batch: 700; loss: 1.13; acc: 0.73
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 0.83; acc: 0.89
Batch: 760; loss: 0.87; acc: 0.84
Batch: 780; loss: 1.07; acc: 0.78
Train Epoch over. train_loss: 0.96; train_accuracy: 0.79 

0.00012881116708740592
0.00012339731620159
Batch: 0; loss: 0.86; acc: 0.89
Batch: 20; loss: 1.06; acc: 0.72
Batch: 40; loss: 0.61; acc: 0.91
Batch: 60; loss: 0.81; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.86; acc: 0.83
Batch: 120; loss: 1.03; acc: 0.73
Batch: 140; loss: 0.74; acc: 0.89
Val Epoch over. val_loss: 0.8796122719527809; val_accuracy: 0.8203622611464968 

The current subspace-distance is: 0.00012339731620159 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.84
Batch: 20; loss: 1.09; acc: 0.75
Batch: 40; loss: 0.91; acc: 0.78
Batch: 60; loss: 0.92; acc: 0.86
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 0.94; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.81
Batch: 140; loss: 1.14; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.77
Batch: 180; loss: 0.94; acc: 0.8
Batch: 200; loss: 0.87; acc: 0.8
Batch: 220; loss: 0.79; acc: 0.88
Batch: 240; loss: 0.84; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.84
Batch: 280; loss: 1.03; acc: 0.75
Batch: 300; loss: 0.79; acc: 0.86
Batch: 320; loss: 0.92; acc: 0.8
Batch: 340; loss: 0.94; acc: 0.83
Batch: 360; loss: 0.84; acc: 0.83
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 0.87; acc: 0.86
Batch: 440; loss: 0.97; acc: 0.78
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 0.9; acc: 0.8
Batch: 500; loss: 0.81; acc: 0.83
Batch: 520; loss: 1.09; acc: 0.72
Batch: 540; loss: 0.95; acc: 0.81
Batch: 560; loss: 1.0; acc: 0.75
Batch: 580; loss: 0.93; acc: 0.8
Batch: 600; loss: 0.95; acc: 0.75
Batch: 620; loss: 0.88; acc: 0.81
Batch: 640; loss: 0.95; acc: 0.73
Batch: 660; loss: 0.79; acc: 0.86
Batch: 680; loss: 0.82; acc: 0.81
Batch: 700; loss: 1.0; acc: 0.69
Batch: 720; loss: 0.78; acc: 0.84
Batch: 740; loss: 0.89; acc: 0.84
Batch: 760; loss: 1.02; acc: 0.8
Batch: 780; loss: 1.0; acc: 0.73
Train Epoch over. train_loss: 0.92; train_accuracy: 0.8 

0.00014047564764041454
0.00013573859177995473
Batch: 0; loss: 0.81; acc: 0.86
Batch: 20; loss: 1.04; acc: 0.73
Batch: 40; loss: 0.57; acc: 0.94
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.86
Batch: 120; loss: 1.0; acc: 0.72
Batch: 140; loss: 0.7; acc: 0.91
Val Epoch over. val_loss: 0.8392720476836916; val_accuracy: 0.8266321656050956 

The current subspace-distance is: 0.00013573859177995473 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 0.81; acc: 0.84
Batch: 40; loss: 0.96; acc: 0.81
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.85; acc: 0.86
Batch: 100; loss: 0.95; acc: 0.75
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 0.85; acc: 0.78
Batch: 160; loss: 0.76; acc: 0.86
Batch: 180; loss: 0.96; acc: 0.8
Batch: 200; loss: 0.86; acc: 0.84
Batch: 220; loss: 1.0; acc: 0.72
Batch: 240; loss: 0.94; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.84
Batch: 280; loss: 0.93; acc: 0.8
Batch: 300; loss: 0.94; acc: 0.81
Batch: 320; loss: 0.82; acc: 0.88
Batch: 340; loss: 0.82; acc: 0.86
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.98; acc: 0.75
Batch: 400; loss: 0.91; acc: 0.81
Batch: 420; loss: 0.97; acc: 0.77
Batch: 440; loss: 0.89; acc: 0.84
Batch: 460; loss: 0.84; acc: 0.81
Batch: 480; loss: 0.93; acc: 0.8
Batch: 500; loss: 0.85; acc: 0.84
Batch: 520; loss: 0.81; acc: 0.88
Batch: 540; loss: 0.71; acc: 0.89
Batch: 560; loss: 0.97; acc: 0.75
Batch: 580; loss: 0.81; acc: 0.81
Batch: 600; loss: 0.86; acc: 0.81
Batch: 620; loss: 0.89; acc: 0.73
Batch: 640; loss: 0.75; acc: 0.84
Batch: 660; loss: 0.83; acc: 0.89
Batch: 680; loss: 0.93; acc: 0.8
Batch: 700; loss: 0.93; acc: 0.77
Batch: 720; loss: 0.92; acc: 0.75
Batch: 740; loss: 0.79; acc: 0.86
Batch: 760; loss: 0.81; acc: 0.81
Batch: 780; loss: 0.86; acc: 0.8
Train Epoch over. train_loss: 0.88; train_accuracy: 0.8 

0.00015240286302287132
0.00014579910202883184
Batch: 0; loss: 0.73; acc: 0.89
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 0.55; acc: 0.94
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.83
Batch: 120; loss: 0.98; acc: 0.67
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.800044349804046; val_accuracy: 0.8302149681528662 

The current subspace-distance is: 0.00014579910202883184 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.78
Batch: 20; loss: 0.89; acc: 0.81
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.85; acc: 0.81
Batch: 100; loss: 0.92; acc: 0.77
Batch: 120; loss: 0.83; acc: 0.86
Batch: 140; loss: 0.78; acc: 0.84
Batch: 160; loss: 0.77; acc: 0.91
Batch: 180; loss: 0.85; acc: 0.8
Batch: 200; loss: 0.95; acc: 0.73
Batch: 220; loss: 0.7; acc: 0.94
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.92; acc: 0.8
Batch: 280; loss: 0.91; acc: 0.77
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 1.06; acc: 0.67
Batch: 340; loss: 0.77; acc: 0.83
Batch: 360; loss: 0.86; acc: 0.83
Batch: 380; loss: 0.85; acc: 0.78
Batch: 400; loss: 0.82; acc: 0.84
Batch: 420; loss: 0.87; acc: 0.77
Batch: 440; loss: 0.83; acc: 0.88
Batch: 460; loss: 0.69; acc: 0.88
Batch: 480; loss: 0.83; acc: 0.8
Batch: 500; loss: 0.95; acc: 0.81
Batch: 520; loss: 0.78; acc: 0.84
Batch: 540; loss: 0.85; acc: 0.84
Batch: 560; loss: 0.83; acc: 0.78
Batch: 580; loss: 0.68; acc: 0.89
Batch: 600; loss: 0.79; acc: 0.89
Batch: 620; loss: 0.88; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.98; acc: 0.7
Batch: 700; loss: 0.94; acc: 0.81
Batch: 720; loss: 0.86; acc: 0.8
Batch: 740; loss: 0.95; acc: 0.75
Batch: 760; loss: 0.86; acc: 0.8
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.85; train_accuracy: 0.81 

0.00016166594286914915
0.00015703754615969956
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 0.53; acc: 0.94
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.79; acc: 0.84
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 0.7715858629175053; val_accuracy: 0.8351910828025477 

The current subspace-distance is: 0.00015703754615969956 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.77
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.75
Batch: 80; loss: 0.94; acc: 0.8
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.95; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.83
Batch: 180; loss: 0.84; acc: 0.83
Batch: 200; loss: 0.87; acc: 0.8
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.81; acc: 0.86
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 0.88; acc: 0.81
Batch: 300; loss: 0.84; acc: 0.77
Batch: 320; loss: 0.94; acc: 0.78
Batch: 340; loss: 0.83; acc: 0.8
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.92; acc: 0.78
Batch: 400; loss: 0.83; acc: 0.8
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.93; acc: 0.81
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.87; acc: 0.8
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.73; acc: 0.89
Batch: 580; loss: 0.89; acc: 0.78
Batch: 600; loss: 0.85; acc: 0.86
Batch: 620; loss: 0.76; acc: 0.84
Batch: 640; loss: 0.83; acc: 0.72
Batch: 660; loss: 0.79; acc: 0.8
Batch: 680; loss: 0.8; acc: 0.78
Batch: 700; loss: 0.87; acc: 0.77
Batch: 720; loss: 0.7; acc: 0.84
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.88; acc: 0.77
Batch: 780; loss: 0.86; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.81 

0.00017288677918259054
0.00016718347615096718
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.7327944535738343; val_accuracy: 0.8365843949044586 

The current subspace-distance is: 0.00016718347615096718 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.84
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.85; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.87; acc: 0.77
Batch: 160; loss: 0.67; acc: 0.86
Batch: 180; loss: 0.8; acc: 0.81
Batch: 200; loss: 0.82; acc: 0.8
Batch: 220; loss: 0.8; acc: 0.83
Batch: 240; loss: 0.86; acc: 0.81
Batch: 260; loss: 0.71; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.88
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.94; acc: 0.73
Batch: 340; loss: 0.78; acc: 0.81
Batch: 360; loss: 0.83; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.93; acc: 0.78
Batch: 440; loss: 0.85; acc: 0.78
Batch: 460; loss: 0.75; acc: 0.83
Batch: 480; loss: 0.87; acc: 0.75
Batch: 500; loss: 0.83; acc: 0.8
Batch: 520; loss: 0.82; acc: 0.83
Batch: 540; loss: 0.83; acc: 0.78
Batch: 560; loss: 0.64; acc: 0.89
Batch: 580; loss: 0.85; acc: 0.78
Batch: 600; loss: 0.84; acc: 0.78
Batch: 620; loss: 0.73; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.85; acc: 0.8
Batch: 680; loss: 0.86; acc: 0.78
Batch: 700; loss: 0.79; acc: 0.83
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.78; acc: 0.81
Batch: 760; loss: 0.72; acc: 0.86
Batch: 780; loss: 0.96; acc: 0.73
Train Epoch over. train_loss: 0.79; train_accuracy: 0.82 

0.00018114058184437454
0.0001759321166900918
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.48; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.53; acc: 0.89
Val Epoch over. val_loss: 0.7076452530113755; val_accuracy: 0.8417595541401274 

The current subspace-distance is: 0.0001759321166900918 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.72; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.78
Batch: 160; loss: 0.84; acc: 0.78
Batch: 180; loss: 0.76; acc: 0.84
Batch: 200; loss: 0.96; acc: 0.72
Batch: 220; loss: 0.89; acc: 0.73
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 0.83; acc: 0.8
Batch: 300; loss: 0.88; acc: 0.8
Batch: 320; loss: 0.63; acc: 0.94
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.86
Batch: 400; loss: 0.81; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.82; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.83
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.82; acc: 0.78
Batch: 520; loss: 0.69; acc: 0.92
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.76; acc: 0.86
Batch: 580; loss: 0.91; acc: 0.78
Batch: 600; loss: 0.89; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.78; acc: 0.75
Batch: 660; loss: 0.85; acc: 0.81
Batch: 680; loss: 0.94; acc: 0.78
Batch: 700; loss: 0.69; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.84
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.83; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.86
Train Epoch over. train_loss: 0.77; train_accuracy: 0.82 

0.0001846685481723398
0.00017806079995352775
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.87; acc: 0.73
Batch: 40; loss: 0.48; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.54; acc: 0.89
Val Epoch over. val_loss: 0.7038703553236214; val_accuracy: 0.8434514331210191 

The current subspace-distance is: 0.00017806079995352775 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.94; acc: 0.72
Batch: 100; loss: 0.64; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.8; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.88; acc: 0.78
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.89
Batch: 240; loss: 0.66; acc: 0.86
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.89
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 0.82; acc: 0.83
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.79; acc: 0.78
Batch: 380; loss: 0.75; acc: 0.84
Batch: 400; loss: 0.7; acc: 0.8
Batch: 420; loss: 0.68; acc: 0.88
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.94; acc: 0.72
Batch: 520; loss: 0.84; acc: 0.83
Batch: 540; loss: 0.69; acc: 0.84
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.79; acc: 0.88
Batch: 600; loss: 0.72; acc: 0.81
Batch: 620; loss: 0.86; acc: 0.83
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.9; acc: 0.78
Batch: 720; loss: 0.68; acc: 0.83
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.76; train_accuracy: 0.82 

0.00018721370724961162
0.00018077057029586285
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.88
Val Epoch over. val_loss: 0.6966199559770572; val_accuracy: 0.8419585987261147 

The current subspace-distance is: 0.00018077057029586285 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.95
Batch: 20; loss: 0.72; acc: 0.88
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 1.0; acc: 0.75
Batch: 80; loss: 0.77; acc: 0.81
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.72; acc: 0.84
Batch: 220; loss: 0.86; acc: 0.75
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.75; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.86
Batch: 300; loss: 0.62; acc: 0.91
Batch: 320; loss: 0.84; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.93; acc: 0.69
Batch: 380; loss: 0.84; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.89
Batch: 440; loss: 0.82; acc: 0.77
Batch: 460; loss: 0.76; acc: 0.8
Batch: 480; loss: 0.82; acc: 0.75
Batch: 500; loss: 0.75; acc: 0.78
Batch: 520; loss: 0.75; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.91
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.81; acc: 0.78
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.82; acc: 0.83
Batch: 760; loss: 0.82; acc: 0.81
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.76; train_accuracy: 0.82 

0.00018927118799183518
0.00018043164163827896
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.6903091169846286; val_accuracy: 0.8443471337579618 

The current subspace-distance is: 0.00018043164163827896 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 1.03; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.88
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.89; acc: 0.75
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.97; acc: 0.8
Batch: 280; loss: 0.83; acc: 0.81
Batch: 300; loss: 0.77; acc: 0.86
Batch: 320; loss: 0.74; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.86
Batch: 360; loss: 0.7; acc: 0.88
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.79; acc: 0.86
Batch: 440; loss: 0.79; acc: 0.81
Batch: 460; loss: 0.86; acc: 0.77
Batch: 480; loss: 0.74; acc: 0.83
Batch: 500; loss: 0.74; acc: 0.84
Batch: 520; loss: 0.86; acc: 0.81
Batch: 540; loss: 0.78; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.83
Batch: 660; loss: 0.86; acc: 0.72
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.83; acc: 0.8
Batch: 720; loss: 0.82; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.76; acc: 0.89
Batch: 780; loss: 0.8; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00019314231758471578
0.00018593377899378538
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.5; acc: 0.89
Val Epoch over. val_loss: 0.6840287553276986; val_accuracy: 0.8470342356687898 

The current subspace-distance is: 0.00018593377899378538 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.88
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.81; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.89
Batch: 200; loss: 0.81; acc: 0.77
Batch: 220; loss: 0.89; acc: 0.84
Batch: 240; loss: 0.62; acc: 0.89
Batch: 260; loss: 0.85; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.78; acc: 0.81
Batch: 320; loss: 0.61; acc: 0.94
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.96; acc: 0.75
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.84; acc: 0.8
Batch: 540; loss: 0.61; acc: 0.91
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.64; acc: 0.89
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.81; acc: 0.78
Batch: 720; loss: 0.83; acc: 0.78
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.77; acc: 0.81
Batch: 780; loss: 0.79; acc: 0.81
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.00019392510876059532
0.00018835661467164755
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.48; acc: 0.89
Val Epoch over. val_loss: 0.6675539415353423; val_accuracy: 0.8459394904458599 

The current subspace-distance is: 0.00018835661467164755 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.79; acc: 0.72
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.89
Batch: 180; loss: 0.86; acc: 0.78
Batch: 200; loss: 0.75; acc: 0.83
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.89; acc: 0.77
Batch: 280; loss: 0.61; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.89
Batch: 320; loss: 0.78; acc: 0.81
Batch: 340; loss: 0.94; acc: 0.7
Batch: 360; loss: 0.78; acc: 0.81
Batch: 380; loss: 0.73; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.9; acc: 0.75
Batch: 460; loss: 0.66; acc: 0.86
Batch: 480; loss: 0.78; acc: 0.84
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.72; acc: 0.83
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.62; acc: 0.89
Batch: 580; loss: 0.71; acc: 0.88
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.81
Batch: 640; loss: 0.68; acc: 0.91
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.92; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.99; acc: 0.81
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.00019500798953231424
0.00018821067351382226
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 0.49; acc: 0.89
Val Epoch over. val_loss: 0.6723032413394587; val_accuracy: 0.8451433121019108 

The current subspace-distance is: 0.00018821067351382226 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 0.78; acc: 0.72
Batch: 80; loss: 0.76; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.89
Batch: 160; loss: 0.8; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.69; acc: 0.86
Batch: 220; loss: 0.83; acc: 0.84
Batch: 240; loss: 0.83; acc: 0.73
Batch: 260; loss: 0.64; acc: 0.88
Batch: 280; loss: 0.66; acc: 0.89
Batch: 300; loss: 0.74; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.83; acc: 0.73
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.57; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 0.77; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.84
Batch: 500; loss: 0.83; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.89; acc: 0.7
Batch: 580; loss: 0.71; acc: 0.86
Batch: 600; loss: 0.83; acc: 0.8
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.84; acc: 0.75
Batch: 660; loss: 0.62; acc: 0.89
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.66; acc: 0.83
Batch: 760; loss: 0.73; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.00019834282284136862
0.00019347014313098043
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.89
Val Epoch over. val_loss: 0.6605930375825068; val_accuracy: 0.849422770700637 

The current subspace-distance is: 0.00019347014313098043 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.7; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.89; acc: 0.77
Batch: 160; loss: 0.68; acc: 0.89
Batch: 180; loss: 0.81; acc: 0.83
Batch: 200; loss: 0.75; acc: 0.8
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.93; acc: 0.77
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.92; acc: 0.66
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.78; acc: 0.8
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.9; acc: 0.75
Batch: 400; loss: 0.86; acc: 0.8
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.8; acc: 0.73
Batch: 460; loss: 1.0; acc: 0.69
Batch: 480; loss: 0.8; acc: 0.77
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.72; acc: 0.81
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.74; acc: 0.86
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.71; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00020075046631973237
0.0001928341225720942
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.89
Val Epoch over. val_loss: 0.6576498467831096; val_accuracy: 0.849422770700637 

The current subspace-distance is: 0.0001928341225720942 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.88
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.94; acc: 0.7
Batch: 240; loss: 0.82; acc: 0.78
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.85; acc: 0.77
Batch: 320; loss: 0.79; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.89
Batch: 380; loss: 0.65; acc: 0.89
Batch: 400; loss: 0.73; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.81; acc: 0.77
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.76; acc: 0.73
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.75; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.81
Batch: 760; loss: 0.82; acc: 0.78
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.0002044477587332949
0.00019726426398847252
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.89
Val Epoch over. val_loss: 0.6501509824376197; val_accuracy: 0.8500199044585988 

The current subspace-distance is: 0.00019726426398847252 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.91
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.83; acc: 0.84
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.75; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.86; acc: 0.75
Batch: 380; loss: 0.75; acc: 0.84
Batch: 400; loss: 0.72; acc: 0.78
Batch: 420; loss: 0.72; acc: 0.8
Batch: 440; loss: 0.69; acc: 0.84
Batch: 460; loss: 0.83; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.7; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.86; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.81; acc: 0.8
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.75; acc: 0.83
Batch: 760; loss: 0.81; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.78
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020742912602145225
0.0002015664504142478
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.89
Val Epoch over. val_loss: 0.6455755207189329; val_accuracy: 0.8479299363057324 

The current subspace-distance is: 0.0002015664504142478 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.75; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.73; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.72; acc: 0.89
Batch: 280; loss: 0.78; acc: 0.75
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.89
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.88; acc: 0.77
Batch: 440; loss: 0.64; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.72; acc: 0.83
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.67; acc: 0.86
Batch: 620; loss: 0.83; acc: 0.73
Batch: 640; loss: 0.87; acc: 0.77
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.91
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.71; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020754554134327918
0.00020109250908717513
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.45; acc: 0.89
Val Epoch over. val_loss: 0.6365653073332113; val_accuracy: 0.8524084394904459 

The current subspace-distance is: 0.00020109250908717513 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.85; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.73; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.72; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.8; acc: 0.69
Batch: 260; loss: 0.77; acc: 0.89
Batch: 280; loss: 0.63; acc: 0.89
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.81; acc: 0.8
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.8; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.88
Batch: 420; loss: 0.71; acc: 0.86
Batch: 440; loss: 0.69; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.88; acc: 0.78
Batch: 540; loss: 0.91; acc: 0.73
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.74; acc: 0.78
Batch: 600; loss: 0.76; acc: 0.77
Batch: 620; loss: 0.8; acc: 0.83
Batch: 640; loss: 0.8; acc: 0.81
Batch: 660; loss: 0.79; acc: 0.83
Batch: 680; loss: 0.88; acc: 0.78
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00021071046649012715
0.00020072043116670102
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.45; acc: 0.91
Val Epoch over. val_loss: 0.6439579947359243; val_accuracy: 0.8525079617834395 

The current subspace-distance is: 0.00020072043116670102 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.85; acc: 0.75
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.8
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.64; acc: 0.89
Batch: 360; loss: 0.8; acc: 0.78
Batch: 380; loss: 0.6; acc: 0.92
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.69; acc: 0.86
Batch: 480; loss: 0.65; acc: 0.84
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.88; acc: 0.75
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.79; acc: 0.75
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.86
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.7; acc: 0.83
Batch: 720; loss: 0.78; acc: 0.75
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.73; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.000208887126063928
0.00020125361334066838
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.7; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.6340459626951035; val_accuracy: 0.8500199044585988 

The current subspace-distance is: 0.00020125361334066838 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.78; acc: 0.84
Batch: 180; loss: 0.78; acc: 0.77
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.78
Batch: 320; loss: 0.8; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.98
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.75; acc: 0.83
Batch: 440; loss: 0.84; acc: 0.75
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 1.05; acc: 0.67
Batch: 560; loss: 0.88; acc: 0.69
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.76; acc: 0.75
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.88
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00021025484602432698
0.00020442828827071935
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.6369727663933091; val_accuracy: 0.8520103503184714 

The current subspace-distance is: 0.00020442828827071935 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.84
Batch: 240; loss: 0.62; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.84
Batch: 280; loss: 0.78; acc: 0.81
Batch: 300; loss: 0.78; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.86; acc: 0.75
Batch: 360; loss: 0.87; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.92; acc: 0.72
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.79; acc: 0.81
Batch: 500; loss: 0.7; acc: 0.88
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.83; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.8
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.7; acc: 0.84
Batch: 720; loss: 0.74; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.88
Batch: 780; loss: 0.83; acc: 0.73
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00020904213306494057
0.00020372364087961614
Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.6310965741515919; val_accuracy: 0.8523089171974523 

The current subspace-distance is: 0.00020372364087961614 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.92
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.77; acc: 0.8
Batch: 220; loss: 0.82; acc: 0.77
Batch: 240; loss: 0.77; acc: 0.8
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.69; acc: 0.88
Batch: 320; loss: 0.77; acc: 0.83
Batch: 340; loss: 0.92; acc: 0.75
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.84; acc: 0.73
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.94
Batch: 460; loss: 0.71; acc: 0.81
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.84
Batch: 580; loss: 0.76; acc: 0.78
Batch: 600; loss: 0.75; acc: 0.78
Batch: 620; loss: 0.82; acc: 0.75
Batch: 640; loss: 0.87; acc: 0.77
Batch: 660; loss: 0.87; acc: 0.77
Batch: 680; loss: 0.89; acc: 0.75
Batch: 700; loss: 0.78; acc: 0.78
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.83
Batch: 780; loss: 0.57; acc: 0.89
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00021021204884164035
0.00020363391377031803
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.6401306043384941; val_accuracy: 0.851015127388535 

The current subspace-distance is: 0.00020363391377031803 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.83; acc: 0.75
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.78; acc: 0.8
Batch: 220; loss: 0.96; acc: 0.72
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.94; acc: 0.78
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.92
Batch: 360; loss: 0.78; acc: 0.78
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.89
Batch: 420; loss: 0.8; acc: 0.77
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.76; acc: 0.8
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.73; acc: 0.73
Batch: 580; loss: 0.71; acc: 0.81
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.84; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.8
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.67; acc: 0.91
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00021312516764737666
0.0002062374696834013
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.43; acc: 0.91
Val Epoch over. val_loss: 0.6268677337534109; val_accuracy: 0.8533041401273885 

The current subspace-distance is: 0.0002062374696834013 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.81; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.86
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.89
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.84; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.75; acc: 0.77
Batch: 420; loss: 0.77; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.73; acc: 0.83
Batch: 480; loss: 0.8; acc: 0.75
Batch: 500; loss: 0.75; acc: 0.8
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.43; acc: 0.94
Batch: 560; loss: 0.86; acc: 0.77
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.6; acc: 0.89
Batch: 620; loss: 0.91; acc: 0.7
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.76; acc: 0.8
Batch: 700; loss: 0.8; acc: 0.77
Batch: 720; loss: 0.81; acc: 0.77
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.78; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002115537936333567
0.00020574776863213629
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.6309412415999516; val_accuracy: 0.8519108280254777 

The current subspace-distance is: 0.00020574776863213629 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.78
Batch: 100; loss: 0.72; acc: 0.73
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.72; acc: 0.84
Batch: 160; loss: 0.69; acc: 0.88
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.89
Batch: 260; loss: 0.77; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.85; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.67; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.62; acc: 0.91
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.71; acc: 0.8
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.5; acc: 0.94
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.81
Batch: 640; loss: 0.77; acc: 0.84
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.83; acc: 0.77
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.78; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.91
Batch: 780; loss: 0.78; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002135341928806156
0.00020371127175167203
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.6331081238521892; val_accuracy: 0.8515127388535032 

The current subspace-distance is: 0.00020371127175167203 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.85; acc: 0.72
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.74; acc: 0.88
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.8
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.99; acc: 0.69
Batch: 360; loss: 0.47; acc: 0.92
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.9; acc: 0.73
Batch: 420; loss: 0.58; acc: 0.91
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.67; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.87; acc: 0.78
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.57; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.94
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.68; acc: 0.78
Batch: 680; loss: 0.92; acc: 0.75
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.91
Batch: 740; loss: 0.73; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00021372854826040566
0.00021102865866851062
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.6277460353389667; val_accuracy: 0.8527070063694268 

The current subspace-distance is: 0.00021102865866851062 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_3_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.928869038633508

The number of parameters is: 275071

The number of individual parameters is:

40
400
40
40
60
50400
60
60
119
149940
119
119
64
68544
64
64
4096
64
640
10
64
64

nonzero elements in E: 82521291
elements in E: 82521300
fraction nonzero: 0.999999890937249
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.37; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.14
Batch: 40; loss: 1.97; acc: 0.42
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.76; acc: 0.53
Batch: 100; loss: 1.64; acc: 0.64
Batch: 120; loss: 1.74; acc: 0.52
Batch: 140; loss: 1.71; acc: 0.56
Batch: 160; loss: 1.61; acc: 0.62
Batch: 180; loss: 1.73; acc: 0.53
Batch: 200; loss: 1.5; acc: 0.67
Batch: 220; loss: 1.51; acc: 0.66
Batch: 240; loss: 1.43; acc: 0.69
Batch: 260; loss: 1.46; acc: 0.62
Batch: 280; loss: 1.37; acc: 0.78
Batch: 300; loss: 1.5; acc: 0.62
Batch: 320; loss: 1.44; acc: 0.66
Batch: 340; loss: 1.34; acc: 0.78
Batch: 360; loss: 1.39; acc: 0.73
Batch: 380; loss: 1.31; acc: 0.77
Batch: 400; loss: 1.25; acc: 0.73
Batch: 420; loss: 1.33; acc: 0.72
Batch: 440; loss: 1.24; acc: 0.81
Batch: 460; loss: 1.15; acc: 0.8
Batch: 480; loss: 1.23; acc: 0.73
Batch: 500; loss: 1.16; acc: 0.81
Batch: 520; loss: 1.26; acc: 0.72
Batch: 540; loss: 1.11; acc: 0.8
Batch: 560; loss: 1.18; acc: 0.78
Batch: 580; loss: 1.12; acc: 0.83
Batch: 600; loss: 1.18; acc: 0.81
Batch: 620; loss: 1.15; acc: 0.84
Batch: 640; loss: 1.26; acc: 0.72
Batch: 660; loss: 1.13; acc: 0.81
Batch: 680; loss: 1.21; acc: 0.77
Batch: 700; loss: 1.1; acc: 0.88
Batch: 720; loss: 1.11; acc: 0.86
Batch: 740; loss: 1.05; acc: 0.83
Batch: 760; loss: 1.11; acc: 0.84
Batch: 780; loss: 1.12; acc: 0.8
Train Epoch over. train_loss: 1.41; train_accuracy: 0.68 

6.224626849871129e-05
5.723039066651836e-05
Batch: 0; loss: 1.01; acc: 0.94
Batch: 20; loss: 1.14; acc: 0.77
Batch: 40; loss: 0.83; acc: 0.91
Batch: 60; loss: 1.03; acc: 0.86
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.06; acc: 0.91
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.94; acc: 0.89
Val Epoch over. val_loss: 1.0706448425912554; val_accuracy: 0.8124004777070064 

The current subspace-distance is: 5.723039066651836e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.84
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 1.07; acc: 0.84
Batch: 60; loss: 1.04; acc: 0.77
Batch: 80; loss: 1.11; acc: 0.8
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.04; acc: 0.83
Batch: 140; loss: 1.07; acc: 0.88
Batch: 160; loss: 1.16; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.81
Batch: 200; loss: 1.09; acc: 0.77
Batch: 220; loss: 1.05; acc: 0.78
Batch: 240; loss: 1.11; acc: 0.7
Batch: 260; loss: 1.09; acc: 0.8
Batch: 280; loss: 1.08; acc: 0.77
Batch: 300; loss: 1.02; acc: 0.83
Batch: 320; loss: 0.94; acc: 0.88
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.08; acc: 0.81
Batch: 380; loss: 0.91; acc: 0.89
Batch: 400; loss: 0.98; acc: 0.84
Batch: 420; loss: 1.0; acc: 0.77
Batch: 440; loss: 1.1; acc: 0.75
Batch: 460; loss: 1.0; acc: 0.83
Batch: 480; loss: 1.11; acc: 0.73
Batch: 500; loss: 1.01; acc: 0.78
Batch: 520; loss: 1.01; acc: 0.77
Batch: 540; loss: 0.93; acc: 0.88
Batch: 560; loss: 1.0; acc: 0.8
Batch: 580; loss: 0.92; acc: 0.83
Batch: 600; loss: 0.97; acc: 0.83
Batch: 620; loss: 1.01; acc: 0.8
Batch: 640; loss: 1.04; acc: 0.77
Batch: 660; loss: 0.99; acc: 0.83
Batch: 680; loss: 0.97; acc: 0.84
Batch: 700; loss: 0.92; acc: 0.84
Batch: 720; loss: 1.03; acc: 0.78
Batch: 740; loss: 0.94; acc: 0.81
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 0.96; acc: 0.77
Train Epoch over. train_loss: 1.01; train_accuracy: 0.82 

8.266177610494196e-05
7.715897663729265e-05
Batch: 0; loss: 0.79; acc: 0.95
Batch: 20; loss: 0.98; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.95
Batch: 60; loss: 0.83; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.92
Batch: 100; loss: 0.88; acc: 0.89
Batch: 120; loss: 1.03; acc: 0.75
Batch: 140; loss: 0.74; acc: 0.92
Val Epoch over. val_loss: 0.8709195992749208; val_accuracy: 0.8515127388535032 

The current subspace-distance is: 7.715897663729265e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 0.9; acc: 0.88
Batch: 40; loss: 0.98; acc: 0.75
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 0.96; acc: 0.81
Batch: 100; loss: 1.02; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.92; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.88
Batch: 180; loss: 0.86; acc: 0.83
Batch: 200; loss: 1.1; acc: 0.78
Batch: 220; loss: 0.9; acc: 0.83
Batch: 240; loss: 0.86; acc: 0.8
Batch: 260; loss: 0.81; acc: 0.88
Batch: 280; loss: 0.83; acc: 0.81
Batch: 300; loss: 0.9; acc: 0.83
Batch: 320; loss: 0.92; acc: 0.81
Batch: 340; loss: 0.82; acc: 0.84
Batch: 360; loss: 0.81; acc: 0.83
Batch: 380; loss: 0.8; acc: 0.83
Batch: 400; loss: 0.87; acc: 0.81
Batch: 420; loss: 0.84; acc: 0.83
Batch: 440; loss: 0.86; acc: 0.83
Batch: 460; loss: 0.94; acc: 0.8
Batch: 480; loss: 0.9; acc: 0.78
Batch: 500; loss: 0.84; acc: 0.84
Batch: 520; loss: 0.91; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.84
Batch: 560; loss: 0.82; acc: 0.89
Batch: 580; loss: 0.91; acc: 0.84
Batch: 600; loss: 0.79; acc: 0.86
Batch: 620; loss: 0.9; acc: 0.88
Batch: 640; loss: 0.73; acc: 0.91
Batch: 660; loss: 0.74; acc: 0.81
Batch: 680; loss: 0.9; acc: 0.81
Batch: 700; loss: 0.86; acc: 0.81
Batch: 720; loss: 0.95; acc: 0.77
Batch: 740; loss: 0.73; acc: 0.89
Batch: 760; loss: 0.73; acc: 0.86
Batch: 780; loss: 0.82; acc: 0.83
Train Epoch over. train_loss: 0.88; train_accuracy: 0.83 

9.903631143970415e-05
9.38458542805165e-05
Batch: 0; loss: 0.68; acc: 0.92
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.52; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.94
Batch: 100; loss: 0.78; acc: 0.91
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.63; acc: 0.94
Val Epoch over. val_loss: 0.7639743716094145; val_accuracy: 0.8617635350318471 

The current subspace-distance is: 9.38458542805165e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.91
Batch: 20; loss: 0.75; acc: 0.86
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.83
Batch: 140; loss: 0.88; acc: 0.83
Batch: 160; loss: 0.79; acc: 0.88
Batch: 180; loss: 0.8; acc: 0.81
Batch: 200; loss: 0.78; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.89
Batch: 240; loss: 0.75; acc: 0.88
Batch: 260; loss: 0.86; acc: 0.83
Batch: 280; loss: 0.8; acc: 0.8
Batch: 300; loss: 0.86; acc: 0.78
Batch: 320; loss: 0.87; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.82; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.89
Batch: 400; loss: 0.76; acc: 0.88
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.7; acc: 0.91
Batch: 460; loss: 0.69; acc: 0.89
Batch: 480; loss: 0.69; acc: 0.86
Batch: 500; loss: 0.81; acc: 0.91
Batch: 520; loss: 0.78; acc: 0.88
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.88
Batch: 580; loss: 0.81; acc: 0.81
Batch: 600; loss: 0.73; acc: 0.88
Batch: 620; loss: 0.8; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.88
Batch: 660; loss: 0.79; acc: 0.83
Batch: 680; loss: 0.75; acc: 0.83
Batch: 700; loss: 0.73; acc: 0.88
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.94
Batch: 760; loss: 0.65; acc: 0.92
Batch: 780; loss: 0.65; acc: 0.94
Train Epoch over. train_loss: 0.79; train_accuracy: 0.84 

0.00011143444862682372
0.00010668034519767389
Batch: 0; loss: 0.62; acc: 0.92
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.94
Batch: 100; loss: 0.74; acc: 0.91
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.56; acc: 0.95
Val Epoch over. val_loss: 0.7029731385647111; val_accuracy: 0.8668391719745223 

The current subspace-distance is: 0.00010668034519767389 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.83
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.74; acc: 0.94
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.76; acc: 0.84
Batch: 160; loss: 0.85; acc: 0.78
Batch: 180; loss: 0.8; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.91
Batch: 220; loss: 0.83; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.91
Batch: 280; loss: 0.74; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.68; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.89
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.92
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 0.79; acc: 0.8
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.74; acc: 0.89
Batch: 620; loss: 0.74; acc: 0.86
Batch: 640; loss: 0.75; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.92
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.91
Batch: 740; loss: 0.73; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.91
Batch: 780; loss: 0.78; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.85 

0.00012367749877739698
0.00011861228995257989
Batch: 0; loss: 0.58; acc: 0.97
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.49; acc: 0.95
Val Epoch over. val_loss: 0.644290167245136; val_accuracy: 0.8722133757961783 

The current subspace-distance is: 0.00011861228995257989 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.91
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.92
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.92
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.75; acc: 0.77
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.83; acc: 0.78
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.92
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.94; acc: 0.75
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.91
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.91
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.77; acc: 0.75
Batch: 700; loss: 0.6; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.85 

0.00013426884834188968
0.00012918311404064298
Batch: 0; loss: 0.54; acc: 0.94
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.95
Val Epoch over. val_loss: 0.598144504484857; val_accuracy: 0.8762937898089171 

The current subspace-distance is: 0.00012918311404064298 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.84
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.7; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.81; acc: 0.77
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.92
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.66; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.91
Batch: 520; loss: 0.63; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.91
Batch: 620; loss: 0.67; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.92
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.64; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.92
Batch: 720; loss: 0.7; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.91
Batch: 780; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.86 

0.00014453078620135784
0.00013741232396569103
Batch: 0; loss: 0.51; acc: 0.94
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.4; acc: 0.95
Val Epoch over. val_loss: 0.5666280388832092; val_accuracy: 0.8811703821656051 

The current subspace-distance is: 0.00013741232396569103 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.77; acc: 0.78
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.67; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.92
Batch: 260; loss: 0.64; acc: 0.91
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.91
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.89
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.73; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.61; train_accuracy: 0.86 

0.00015327398432418704
0.00014660436136182398
Batch: 0; loss: 0.47; acc: 0.95
Batch: 20; loss: 0.7; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.5309368292237543; val_accuracy: 0.8851512738853503 

The current subspace-distance is: 0.00014660436136182398 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.94
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.94
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.95
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00016279809642583132
0.00015616900054737926
Batch: 0; loss: 0.46; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.5074852225696964; val_accuracy: 0.8904259554140127 

The current subspace-distance is: 0.00015616900054737926 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.94
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.6; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.91
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 0.75; acc: 0.77
Batch: 360; loss: 0.73; acc: 0.77
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.92
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.78
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00016795611009001732
0.0001606922160135582
Batch: 0; loss: 0.44; acc: 0.98
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.47686872512671596; val_accuracy: 0.8938097133757962 

The current subspace-distance is: 0.0001606922160135582 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.92
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.95
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.97
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.54; acc: 0.89
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.89
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00017165103054139763
0.00016545603284612298
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.47353360056877136; val_accuracy: 0.8937101910828026 

The current subspace-distance is: 0.00016545603284612298 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.94
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.37; acc: 0.97
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.97
Batch: 420; loss: 0.68; acc: 0.77
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00017542418208904564
0.00016567556303925812
Batch: 0; loss: 0.44; acc: 0.97
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4740050313579049; val_accuracy: 0.893312101910828 

The current subspace-distance is: 0.00016567556303925812 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.83
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.66; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.73
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.95
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00017601935542188585
0.00016675333608873188
Batch: 0; loss: 0.44; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.73
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.4747276285271736; val_accuracy: 0.8928144904458599 

The current subspace-distance is: 0.00016675333608873188 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.95
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.94
Batch: 460; loss: 0.52; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.94
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00017663757898844779
0.00016970676369965076
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.46856668752849484; val_accuracy: 0.8940087579617835 

The current subspace-distance is: 0.00016970676369965076 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.32; acc: 0.97
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.94
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.97
Batch: 780; loss: 0.44; acc: 0.95
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0001772959076333791
0.00017056366777978837
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.45906198299994133; val_accuracy: 0.8954020700636943 

The current subspace-distance is: 0.00017056366777978837 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.77; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.79; acc: 0.81
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.94
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.94
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00018083602481056005
0.00017335620941594243
Batch: 0; loss: 0.42; acc: 0.97
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.46730355757057285; val_accuracy: 0.8942078025477707 

The current subspace-distance is: 0.00017335620941594243 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.97
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.86
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.67; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00018264735990669578
0.0001749430812196806
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.4532643585068405; val_accuracy: 0.8965963375796179 

The current subspace-distance is: 0.0001749430812196806 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.95
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.69; acc: 0.78
Batch: 440; loss: 0.6; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.95
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.95
Batch: 640; loss: 0.45; acc: 0.94
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.61; acc: 0.8
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.0001858823379734531
0.00017863453831523657
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.45259741850339685; val_accuracy: 0.8941082802547771 

The current subspace-distance is: 0.00017863453831523657 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00018546453793533146
0.00017935378127731383
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.45572466104273585; val_accuracy: 0.894406847133758 

The current subspace-distance is: 0.00017935378127731383 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.95
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00018861936405301094
0.00018002440629061311
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.4478116792858027; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 0.00018002440629061311 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.65; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.42; acc: 0.95
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00018732230819296092
0.00017974113870877773
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.4495812155258883; val_accuracy: 0.8949044585987261 

The current subspace-distance is: 0.00017974113870877773 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.78
Batch: 220; loss: 0.58; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.8
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00018763600382953882
0.00018072121019940823
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.4427352610287393; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 0.00018072121019940823 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00018897671543527395
0.0001799497113097459
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.73
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.44854651182700117; val_accuracy: 0.8951035031847133 

The current subspace-distance is: 0.0001799497113097459 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.95
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.48; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.8
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.97
Batch: 740; loss: 0.81; acc: 0.81
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0001890014245873317
0.0001827465748647228
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.44838801519885946; val_accuracy: 0.895203025477707 

The current subspace-distance is: 0.0001827465748647228 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.39; acc: 0.97
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.71; acc: 0.77
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00018984080816153437
0.0001811509282561019
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.45325400514207825; val_accuracy: 0.894406847133758 

The current subspace-distance is: 0.0001811509282561019 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.97
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.94
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.81
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.69; acc: 0.77
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00019117380725219846
0.0001844241633079946
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.45092318354138905; val_accuracy: 0.8927149681528662 

The current subspace-distance is: 0.0001844241633079946 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.95
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.95
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.31; acc: 0.97
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00019263096328359097
0.00018240026838611811
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.44237655363265116; val_accuracy: 0.8957006369426752 

The current subspace-distance is: 0.00018240026838611811 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.95
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.49; acc: 0.92
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0001899033522931859
0.0001813715643947944
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.44499239686188424; val_accuracy: 0.8950039808917197 

The current subspace-distance is: 0.0001813715643947944 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.94
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00019270365010015666
0.00018405997252557427
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.43887617595636164; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 0.00018405997252557427 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.77
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.81
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.97
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.97
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.33; acc: 0.98
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0001911386352730915
0.0001831879635574296
Batch: 0; loss: 0.4; acc: 0.97
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.44571499564465444; val_accuracy: 0.8949044585987261 

The current subspace-distance is: 0.0001831879635574296 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_3_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.928869038633508

The number of parameters is: 275071

The number of individual parameters is:

40
400
40
40
60
50400
60
60
119
149940
119
119
64
68544
64
64
4096
64
640
10
64
64

nonzero elements in E: 110028389
elements in E: 110028400
fraction nonzero: 0.9999999000258115
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.49; acc: 0.08
Batch: 20; loss: 2.12; acc: 0.3
Batch: 40; loss: 1.8; acc: 0.5
Batch: 60; loss: 1.77; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.66
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.5; acc: 0.72
Batch: 140; loss: 1.4; acc: 0.83
Batch: 160; loss: 1.4; acc: 0.77
Batch: 180; loss: 1.31; acc: 0.75
Batch: 200; loss: 1.25; acc: 0.8
Batch: 220; loss: 1.29; acc: 0.75
Batch: 240; loss: 1.29; acc: 0.72
Batch: 260; loss: 1.2; acc: 0.83
Batch: 280; loss: 1.34; acc: 0.66
Batch: 300; loss: 1.16; acc: 0.81
Batch: 320; loss: 1.28; acc: 0.72
Batch: 340; loss: 1.13; acc: 0.81
Batch: 360; loss: 1.23; acc: 0.77
Batch: 380; loss: 1.27; acc: 0.69
Batch: 400; loss: 1.21; acc: 0.77
Batch: 420; loss: 1.1; acc: 0.86
Batch: 440; loss: 1.27; acc: 0.72
Batch: 460; loss: 1.1; acc: 0.84
Batch: 480; loss: 1.01; acc: 0.81
Batch: 500; loss: 1.06; acc: 0.88
Batch: 520; loss: 1.13; acc: 0.75
Batch: 540; loss: 1.07; acc: 0.78
Batch: 560; loss: 1.14; acc: 0.77
Batch: 580; loss: 1.07; acc: 0.83
Batch: 600; loss: 1.01; acc: 0.83
Batch: 620; loss: 0.89; acc: 0.92
Batch: 640; loss: 0.9; acc: 0.91
Batch: 660; loss: 0.99; acc: 0.8
Batch: 680; loss: 1.22; acc: 0.69
Batch: 700; loss: 1.06; acc: 0.78
Batch: 720; loss: 1.01; acc: 0.81
Batch: 740; loss: 0.89; acc: 0.84
Batch: 760; loss: 0.99; acc: 0.8
Batch: 780; loss: 1.03; acc: 0.8
Train Epoch over. train_loss: 1.27; train_accuracy: 0.73 

2.4995128114824183e-05
8.215407433453947e-06
Batch: 0; loss: 0.91; acc: 0.84
Batch: 20; loss: 1.1; acc: 0.77
Batch: 40; loss: 0.67; acc: 0.97
Batch: 60; loss: 0.86; acc: 0.86
Batch: 80; loss: 0.87; acc: 0.89
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.81; acc: 0.88
Val Epoch over. val_loss: 0.9274173667476435; val_accuracy: 0.830015923566879 

The current subspace-distance is: 8.215407433453947e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.81
Batch: 20; loss: 0.95; acc: 0.81
Batch: 40; loss: 1.05; acc: 0.78
Batch: 60; loss: 0.9; acc: 0.83
Batch: 80; loss: 0.91; acc: 0.83
Batch: 100; loss: 0.95; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.8
Batch: 160; loss: 0.88; acc: 0.84
Batch: 180; loss: 0.93; acc: 0.84
Batch: 200; loss: 0.92; acc: 0.78
Batch: 220; loss: 0.92; acc: 0.83
Batch: 240; loss: 0.89; acc: 0.84
Batch: 260; loss: 0.88; acc: 0.86
Batch: 280; loss: 0.97; acc: 0.8
Batch: 300; loss: 0.98; acc: 0.77
Batch: 320; loss: 0.88; acc: 0.86
Batch: 340; loss: 0.75; acc: 0.91
Batch: 360; loss: 0.83; acc: 0.84
Batch: 380; loss: 0.9; acc: 0.83
Batch: 400; loss: 0.87; acc: 0.84
Batch: 420; loss: 0.81; acc: 0.88
Batch: 440; loss: 0.82; acc: 0.81
Batch: 460; loss: 0.87; acc: 0.83
Batch: 480; loss: 0.75; acc: 0.86
Batch: 500; loss: 0.73; acc: 0.91
Batch: 520; loss: 0.81; acc: 0.86
Batch: 540; loss: 0.69; acc: 0.95
Batch: 560; loss: 0.76; acc: 0.89
Batch: 580; loss: 0.79; acc: 0.88
Batch: 600; loss: 0.8; acc: 0.91
Batch: 620; loss: 0.73; acc: 0.88
Batch: 640; loss: 0.84; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.91
Batch: 700; loss: 0.86; acc: 0.8
Batch: 720; loss: 0.88; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.84
Batch: 760; loss: 0.95; acc: 0.84
Batch: 780; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.87; train_accuracy: 0.84 

3.0142904506647028e-05
1.1940018339373637e-05
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.51; acc: 0.98
Batch: 60; loss: 0.69; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.89
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.92
Val Epoch over. val_loss: 0.7326846276498904; val_accuracy: 0.8633558917197452 

The current subspace-distance is: 1.1940018339373637e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 0.79; acc: 0.89
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.79; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.83
Batch: 140; loss: 0.76; acc: 0.88
Batch: 160; loss: 0.84; acc: 0.83
Batch: 180; loss: 0.75; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.89
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.81; acc: 0.86
Batch: 260; loss: 0.73; acc: 0.88
Batch: 280; loss: 0.81; acc: 0.86
Batch: 300; loss: 0.74; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.86
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.88
Batch: 420; loss: 0.68; acc: 0.92
Batch: 440; loss: 0.66; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.94
Batch: 480; loss: 0.6; acc: 0.91
Batch: 500; loss: 0.74; acc: 0.84
Batch: 520; loss: 0.73; acc: 0.89
Batch: 540; loss: 0.79; acc: 0.83
Batch: 560; loss: 0.81; acc: 0.8
Batch: 580; loss: 0.74; acc: 0.89
Batch: 600; loss: 0.77; acc: 0.83
Batch: 620; loss: 0.81; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.63; acc: 0.92
Batch: 720; loss: 0.81; acc: 0.86
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.91
Batch: 780; loss: 0.68; acc: 0.91
Train Epoch over. train_loss: 0.74; train_accuracy: 0.86 

3.526697764755227e-05
1.4056669897399843e-05
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.9; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.97
Batch: 100; loss: 0.6; acc: 0.91
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6435333127808419; val_accuracy: 0.8797770700636943 

The current subspace-distance is: 1.4056669897399843e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 0.74; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.91
Batch: 140; loss: 0.84; acc: 0.81
Batch: 160; loss: 0.78; acc: 0.81
Batch: 180; loss: 0.74; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.91
Batch: 220; loss: 0.83; acc: 0.83
Batch: 240; loss: 0.64; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.88
Batch: 280; loss: 0.68; acc: 0.88
Batch: 300; loss: 0.63; acc: 0.92
Batch: 320; loss: 0.74; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.94
Batch: 380; loss: 0.63; acc: 0.91
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.91
Batch: 440; loss: 0.76; acc: 0.88
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.75
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.92
Batch: 540; loss: 0.74; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.98
Batch: 600; loss: 0.72; acc: 0.8
Batch: 620; loss: 0.75; acc: 0.86
Batch: 640; loss: 0.7; acc: 0.77
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.62; acc: 0.91
Batch: 700; loss: 0.65; acc: 0.91
Batch: 720; loss: 0.76; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.89
Train Epoch over. train_loss: 0.67; train_accuracy: 0.87 

3.7745976442238316e-05
1.5067476851982065e-05
Batch: 0; loss: 0.49; acc: 0.97
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.95
Val Epoch over. val_loss: 0.587823889058107; val_accuracy: 0.8866441082802548 

The current subspace-distance is: 1.5067476851982065e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.88
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.91
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.73; acc: 0.83
Batch: 260; loss: 0.62; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.91
Batch: 300; loss: 0.77; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.92
Batch: 400; loss: 0.82; acc: 0.78
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.92
Batch: 500; loss: 0.6; acc: 0.92
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.91
Batch: 560; loss: 0.64; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.88
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.63; train_accuracy: 0.87 

4.041154170408845e-05
1.6885476725292392e-05
Batch: 0; loss: 0.43; acc: 0.98
Batch: 20; loss: 0.81; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.5363987900648907; val_accuracy: 0.8946058917197452 

The current subspace-distance is: 1.6885476725292392e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.89
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.61; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.71; acc: 0.78
Batch: 400; loss: 0.51; acc: 0.92
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.94
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.94
Train Epoch over. train_loss: 0.58; train_accuracy: 0.88 

4.355634519015439e-05
1.888802034955006e-05
Batch: 0; loss: 0.4; acc: 0.98
Batch: 20; loss: 0.74; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.50179986342503; val_accuracy: 0.9021695859872612 

The current subspace-distance is: 1.888802034955006e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.94
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.97
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.94
Batch: 180; loss: 0.57; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.6; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.92
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.73; acc: 0.78
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.6076427679508924e-05
1.9389459339436144e-05
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.4651962775903143; val_accuracy: 0.9078423566878981 

The current subspace-distance is: 1.9389459339436144e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.64; acc: 0.8
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.92
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.5; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.97
Batch: 460; loss: 0.39; acc: 0.95
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.78; acc: 0.77
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.97
Batch: 700; loss: 0.5; acc: 0.94
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

4.813173654838465e-05
2.0920386305078864e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.44810506672995865; val_accuracy: 0.9110270700636943 

The current subspace-distance is: 2.0920386305078864e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.97
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.97
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.97
Batch: 640; loss: 0.34; acc: 0.97
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

5.073756619822234e-05
2.3099111785995774e-05
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4249501962950275; val_accuracy: 0.9123208598726115 

The current subspace-distance is: 2.3099111785995774e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.97
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.97
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.97
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.94
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

5.209900700720027e-05
2.2343205273500644e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.41479897603487514; val_accuracy: 0.9145103503184714 

The current subspace-distance is: 2.2343205273500644e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.97
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.47; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.98
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.58; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.335045716492459e-05
2.2239475583774038e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.41337170940675555; val_accuracy: 0.911922770700637 

The current subspace-distance is: 2.2239475583774038e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.95
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.4131454817252234e-05
2.3840128051233478e-05
Batch: 0; loss: 0.33; acc: 0.97
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.40744214130055373; val_accuracy: 0.9148089171974523 

The current subspace-distance is: 2.3840128051233478e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.97
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.48287971469108e-05
2.5315928724012338e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.40138645403704065; val_accuracy: 0.9156050955414012 

The current subspace-distance is: 2.5315928724012338e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.95
Batch: 380; loss: 0.58; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.95
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.29; acc: 1.0
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.98
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.5454831453971565e-05
2.3892333047115244e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.39862102061320265; val_accuracy: 0.9159036624203821 

The current subspace-distance is: 2.3892333047115244e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.97
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.98
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.97
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.519411206478253e-05
2.425863203825429e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.39070779246509457; val_accuracy: 0.914609872611465 

The current subspace-distance is: 2.425863203825429e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.95
Batch: 220; loss: 0.43; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.97
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.585973849520087e-05
2.531926475057844e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.39826850536142944; val_accuracy: 0.9166003184713376 

The current subspace-distance is: 2.531926475057844e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.97
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.97
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.641093957819976e-05
2.6185727620031685e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3924449170660821; val_accuracy: 0.9149084394904459 

The current subspace-distance is: 2.6185727620031685e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.690621765097603e-05
2.4687764380360022e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.38503866998633; val_accuracy: 0.9166998407643312 

The current subspace-distance is: 2.4687764380360022e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.97
Batch: 700; loss: 0.67; acc: 0.78
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.8282606914872304e-05
2.7471900466480292e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3804887133608958; val_accuracy: 0.9170979299363057 

The current subspace-distance is: 2.7471900466480292e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.830395093653351e-05
2.7101532396045513e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3761040943253572; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.7101532396045513e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.95
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.81
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.7675679272506386e-05
2.5415571144549176e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3726162943680575; val_accuracy: 0.9169984076433121 

The current subspace-distance is: 2.5415571144549176e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.95
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.98
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.97
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.97
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.23; acc: 0.98
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.827754648635164e-05
2.5258170353481546e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3753707791399804; val_accuracy: 0.9170979299363057 

The current subspace-distance is: 2.5258170353481546e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.32; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.872637848369777e-05
2.6092624466400594e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3772781598529998; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 2.6092624466400594e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.97
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.98
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.97
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.95
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.870484164915979e-05
2.6505747882765718e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3744679852655739; val_accuracy: 0.9166003184713376 

The current subspace-distance is: 2.6505747882765718e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.94
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.81
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.47; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.83663240831811e-05
2.6537471057963558e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3786613123052439; val_accuracy: 0.9164012738853503 

The current subspace-distance is: 2.6537471057963558e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.9120131481904536e-05
2.6621446522767656e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.37056004649894253; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 2.6621446522767656e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.95
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.966719254502095e-05
2.7745303668780252e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.37136565632880875; val_accuracy: 0.918093152866242 

The current subspace-distance is: 2.7745303668780252e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.8392870414536446e-05
2.724622390815057e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.373399899358962; val_accuracy: 0.9165007961783439 

The current subspace-distance is: 2.724622390815057e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.97
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.98
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.920451440033503e-05
2.696275623748079e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3716050562015764; val_accuracy: 0.9179936305732485 

The current subspace-distance is: 2.696275623748079e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.9279485867591575e-05
2.7537647838471457e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.37383522169225536; val_accuracy: 0.9182921974522293 

The current subspace-distance is: 2.7537647838471457e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_3_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.928869038633508

The number of parameters is: 275071

The number of individual parameters is:

40
400
40
40
60
50400
60
60
119
149940
119
119
64
68544
64
64
4096
64
640
10
64
64

nonzero elements in E: 137535487
elements in E: 137535500
fraction nonzero: 0.9999999054789491
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.46; acc: 0.03
Batch: 20; loss: 2.09; acc: 0.28
Batch: 40; loss: 1.84; acc: 0.44
Batch: 60; loss: 1.64; acc: 0.59
Batch: 80; loss: 1.6; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.7
Batch: 120; loss: 1.39; acc: 0.72
Batch: 140; loss: 1.41; acc: 0.66
Batch: 160; loss: 1.28; acc: 0.78
Batch: 180; loss: 1.3; acc: 0.75
Batch: 200; loss: 1.18; acc: 0.75
Batch: 220; loss: 1.39; acc: 0.7
Batch: 240; loss: 1.19; acc: 0.75
Batch: 260; loss: 1.09; acc: 0.84
Batch: 280; loss: 1.03; acc: 0.84
Batch: 300; loss: 1.23; acc: 0.69
Batch: 320; loss: 1.17; acc: 0.73
Batch: 340; loss: 1.07; acc: 0.81
Batch: 360; loss: 1.04; acc: 0.81
Batch: 380; loss: 1.16; acc: 0.8
Batch: 400; loss: 1.0; acc: 0.84
Batch: 420; loss: 0.96; acc: 0.88
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 0.95; acc: 0.84
Batch: 480; loss: 1.07; acc: 0.8
Batch: 500; loss: 1.0; acc: 0.86
Batch: 520; loss: 1.05; acc: 0.78
Batch: 540; loss: 1.01; acc: 0.78
Batch: 560; loss: 0.9; acc: 0.83
Batch: 580; loss: 1.08; acc: 0.77
Batch: 600; loss: 1.02; acc: 0.78
Batch: 620; loss: 1.0; acc: 0.73
Batch: 640; loss: 0.91; acc: 0.86
Batch: 660; loss: 0.9; acc: 0.83
Batch: 680; loss: 0.92; acc: 0.84
Batch: 700; loss: 0.93; acc: 0.84
Batch: 720; loss: 0.82; acc: 0.86
Batch: 740; loss: 0.9; acc: 0.84
Batch: 760; loss: 1.03; acc: 0.81
Batch: 780; loss: 0.77; acc: 0.91
Train Epoch over. train_loss: 1.16; train_accuracy: 0.76 

2.5284618459409103e-05
7.923551493149716e-06
Batch: 0; loss: 0.78; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.95
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.95
Batch: 100; loss: 0.74; acc: 0.92
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 0.67; acc: 0.92
Val Epoch over. val_loss: 0.8147919345053898; val_accuracy: 0.8707205414012739 

The current subspace-distance is: 7.923551493149716e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.81
Batch: 20; loss: 0.86; acc: 0.88
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.83
Batch: 80; loss: 0.88; acc: 0.88
Batch: 100; loss: 0.87; acc: 0.83
Batch: 120; loss: 0.7; acc: 0.95
Batch: 140; loss: 0.79; acc: 0.89
Batch: 160; loss: 0.85; acc: 0.84
Batch: 180; loss: 0.72; acc: 0.91
Batch: 200; loss: 0.9; acc: 0.81
Batch: 220; loss: 0.83; acc: 0.88
Batch: 240; loss: 0.76; acc: 0.86
Batch: 260; loss: 0.79; acc: 0.88
Batch: 280; loss: 0.85; acc: 0.86
Batch: 300; loss: 0.75; acc: 0.86
Batch: 320; loss: 0.89; acc: 0.81
Batch: 340; loss: 0.8; acc: 0.92
Batch: 360; loss: 0.77; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.95
Batch: 400; loss: 0.74; acc: 0.84
Batch: 420; loss: 0.91; acc: 0.81
Batch: 440; loss: 0.78; acc: 0.91
Batch: 460; loss: 0.77; acc: 0.88
Batch: 480; loss: 0.9; acc: 0.77
Batch: 500; loss: 0.76; acc: 0.86
Batch: 520; loss: 0.92; acc: 0.75
Batch: 540; loss: 0.79; acc: 0.84
Batch: 560; loss: 0.75; acc: 0.84
Batch: 580; loss: 0.8; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.88
Batch: 620; loss: 0.71; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.89
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.67; acc: 0.91
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.91
Batch: 760; loss: 0.65; acc: 0.88
Batch: 780; loss: 0.77; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.87 

3.0279979910119437e-05
1.0875231055251788e-05
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.79; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.95
Batch: 100; loss: 0.56; acc: 0.94
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.6496938474618705; val_accuracy: 0.8907245222929936 

The current subspace-distance is: 1.0875231055251788e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.88
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.92
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 0.68; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.94
Batch: 160; loss: 0.66; acc: 0.89
Batch: 180; loss: 0.79; acc: 0.81
Batch: 200; loss: 0.62; acc: 0.94
Batch: 220; loss: 0.67; acc: 0.91
Batch: 240; loss: 0.69; acc: 0.94
Batch: 260; loss: 0.66; acc: 0.89
Batch: 280; loss: 0.71; acc: 0.88
Batch: 300; loss: 0.62; acc: 0.92
Batch: 320; loss: 0.68; acc: 0.89
Batch: 340; loss: 0.78; acc: 0.86
Batch: 360; loss: 0.75; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.95
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.86
Batch: 520; loss: 0.61; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.6; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.89
Batch: 640; loss: 0.82; acc: 0.83
Batch: 660; loss: 0.7; acc: 0.88
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.92
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.98
Train Epoch over. train_loss: 0.66; train_accuracy: 0.88 

3.455632031545974e-05
1.3346393643587362e-05
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.72; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.91
Val Epoch over. val_loss: 0.5708563274638668; val_accuracy: 0.8996815286624203 

The current subspace-distance is: 1.3346393643587362e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.7; acc: 0.89
Batch: 40; loss: 0.61; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.92
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.92
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.58; acc: 0.92
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.89
Batch: 380; loss: 0.76; acc: 0.77
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.57; acc: 0.91
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.92
Batch: 700; loss: 0.71; acc: 0.78
Batch: 720; loss: 0.44; acc: 0.97
Batch: 740; loss: 0.72; acc: 0.86
Batch: 760; loss: 0.71; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.59; train_accuracy: 0.89 

3.735447535291314e-05
1.4672075849375688e-05
Batch: 0; loss: 0.48; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.92
Val Epoch over. val_loss: 0.5182928249334834; val_accuracy: 0.8990843949044586 

The current subspace-distance is: 1.4672075849375688e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.97
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.45; acc: 0.94
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.92
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.41; acc: 0.95
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.94
Batch: 500; loss: 0.6; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.95
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.71; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.0698439988773316e-05
1.6930456695263274e-05
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.4701549344381709; val_accuracy: 0.910031847133758 

The current subspace-distance is: 1.6930456695263274e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.95
Batch: 260; loss: 0.47; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.98
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.49; train_accuracy: 0.9 

4.327790884417482e-05
1.8501508748158813e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4245826694046616; val_accuracy: 0.9144108280254777 

The current subspace-distance is: 1.8501508748158813e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.97
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.58; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

4.591743345372379e-05
2.0496980141615495e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.38027484003145984; val_accuracy: 0.9195859872611465 

The current subspace-distance is: 2.0496980141615495e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.94
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.8571997467661276e-05
2.1744512196164578e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3608211519042398; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.1744512196164578e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.53; acc: 0.8
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.101479837321676e-05
2.2819143850938417e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3424519993317355; val_accuracy: 0.9242635350318471 

The current subspace-distance is: 2.2819143850938417e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.97
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.98
Batch: 500; loss: 0.28; acc: 0.97
Batch: 520; loss: 0.52; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.275669536786154e-05
2.3051316020428203e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.33083775947997524; val_accuracy: 0.9253582802547771 

The current subspace-distance is: 2.3051316020428203e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.4070333135314286e-05
2.3875791157479398e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3181125440510215; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.3875791157479398e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.377026172936894e-05
2.3410124413203448e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.31169076021879344; val_accuracy: 0.9311305732484076 

The current subspace-distance is: 2.3410124413203448e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.457243605633266e-05
2.371554364799522e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3080011832581204; val_accuracy: 0.9309315286624203 

The current subspace-distance is: 2.371554364799522e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

5.572493319050409e-05
2.3648786736885086e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3067770233010031; val_accuracy: 0.9314291401273885 

The current subspace-distance is: 2.3648786736885086e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.98
Batch: 220; loss: 0.31; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.98
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.655201312038116e-05
2.7222044082009234e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.301421286099276; val_accuracy: 0.930234872611465 

The current subspace-distance is: 2.7222044082009234e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.98
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.669785969075747e-05
2.485753793735057e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.3005549057748667; val_accuracy: 0.9311305732484076 

The current subspace-distance is: 2.485753793735057e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.19; acc: 1.0
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.660754322889261e-05
2.5387007553945296e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3017586441176712; val_accuracy: 0.9312300955414012 

The current subspace-distance is: 2.5387007553945296e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.97
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.746533497585915e-05
2.564690657891333e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.29920245759236586; val_accuracy: 0.9304339171974523 

The current subspace-distance is: 2.564690657891333e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.98
Batch: 380; loss: 0.29; acc: 0.98
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.725573646486737e-05
2.486856647010427e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2906864491902339; val_accuracy: 0.9322253184713376 

The current subspace-distance is: 2.486856647010427e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.98
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.802400119137019e-05
2.5667612135293894e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2923339520385311; val_accuracy: 0.9325238853503185 

The current subspace-distance is: 2.5667612135293894e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.855035487911664e-05
2.6251158487866633e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.28902546506208976; val_accuracy: 0.9325238853503185 

The current subspace-distance is: 2.6251158487866633e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.7718672906048596e-05
2.4983972252812237e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2875172769188122; val_accuracy: 0.9333200636942676 

The current subspace-distance is: 2.4983972252812237e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.904479621676728e-05
2.6796631573233753e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.28822827704583004; val_accuracy: 0.9338176751592356 

The current subspace-distance is: 2.6796631573233753e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.9254991356283426e-05
2.65880735241808e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2858732258248481; val_accuracy: 0.9332205414012739 

The current subspace-distance is: 2.65880735241808e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.98
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.8802266721613705e-05
2.5638906663516536e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.28458598740161606; val_accuracy: 0.9338176751592356 

The current subspace-distance is: 2.5638906663516536e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.947227691649459e-05
2.6757850719150156e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.28356898110953105; val_accuracy: 0.9333200636942676 

The current subspace-distance is: 2.6757850719150156e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.98
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.87739996262826e-05
2.6229399736621417e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2852576188505835; val_accuracy: 0.9326234076433121 

The current subspace-distance is: 2.6229399736621417e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.18; acc: 1.0
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.98
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.926279118284583e-05
2.633954500197433e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2847667269551071; val_accuracy: 0.9334195859872612 

The current subspace-distance is: 2.633954500197433e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.98
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.57; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.9364221669966355e-05
2.555985156504903e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2852390605458029; val_accuracy: 0.9320262738853503 

The current subspace-distance is: 2.555985156504903e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.25; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.890064494451508e-05
2.5679315513116308e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2850267693495295; val_accuracy: 0.9322253184713376 

The current subspace-distance is: 2.5679315513116308e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_3_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_3_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
