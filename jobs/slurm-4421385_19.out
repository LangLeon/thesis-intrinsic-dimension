model : table13slim
N : 3
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:47

Channel scaling factor: 3.25

The number of parameters is: 266027

The number of individual parameters is:

26
260
26
26
39
42588
39
39
78
127764
78
78
64
89856
64
64
4096
64
640
10
64
64

nonzero elements in E: 13301348
elements in E: 13301350
fraction nonzero: 0.9999998496393223
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.16
Batch: 60; loss: 2.23; acc: 0.19
Batch: 80; loss: 2.12; acc: 0.28
Batch: 100; loss: 2.21; acc: 0.17
Batch: 120; loss: 2.2; acc: 0.27
Batch: 140; loss: 2.12; acc: 0.25
Batch: 160; loss: 2.09; acc: 0.25
Batch: 180; loss: 2.02; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.25
Batch: 220; loss: 2.04; acc: 0.31
Batch: 240; loss: 2.02; acc: 0.36
Batch: 260; loss: 2.08; acc: 0.3
Batch: 280; loss: 2.04; acc: 0.38
Batch: 300; loss: 1.99; acc: 0.31
Batch: 320; loss: 2.03; acc: 0.36
Batch: 340; loss: 1.93; acc: 0.38
Batch: 360; loss: 1.98; acc: 0.33
Batch: 380; loss: 2.07; acc: 0.3
Batch: 400; loss: 2.0; acc: 0.27
Batch: 420; loss: 2.03; acc: 0.36
Batch: 440; loss: 2.04; acc: 0.28
Batch: 460; loss: 1.89; acc: 0.41
Batch: 480; loss: 2.08; acc: 0.25
Batch: 500; loss: 2.0; acc: 0.36
Batch: 520; loss: 2.06; acc: 0.36
Batch: 540; loss: 1.86; acc: 0.39
Batch: 560; loss: 1.89; acc: 0.45
Batch: 580; loss: 1.87; acc: 0.39
Batch: 600; loss: 1.93; acc: 0.38
Batch: 620; loss: 1.88; acc: 0.47
Batch: 640; loss: 1.83; acc: 0.39
Batch: 660; loss: 2.03; acc: 0.3
Batch: 680; loss: 1.83; acc: 0.48
Batch: 700; loss: 1.98; acc: 0.34
Batch: 720; loss: 1.8; acc: 0.44
Batch: 740; loss: 1.83; acc: 0.42
Batch: 760; loss: 1.93; acc: 0.33
Batch: 780; loss: 1.93; acc: 0.38
Train Epoch over. train_loss: 2.01; train_accuracy: 0.33 

2.305225825693924e-05
5.635823072225321e-06
Batch: 0; loss: 1.89; acc: 0.39
Batch: 20; loss: 1.91; acc: 0.38
Batch: 40; loss: 1.63; acc: 0.58
Batch: 60; loss: 1.77; acc: 0.52
Batch: 80; loss: 1.77; acc: 0.53
Batch: 100; loss: 1.77; acc: 0.53
Batch: 120; loss: 1.78; acc: 0.45
Batch: 140; loss: 1.85; acc: 0.47
Val Epoch over. val_loss: 1.8570104920940034; val_accuracy: 0.4189888535031847 

The current subspace-distance is: 5.635823072225321e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.44
Batch: 20; loss: 1.83; acc: 0.47
Batch: 40; loss: 1.82; acc: 0.45
Batch: 60; loss: 1.83; acc: 0.47
Batch: 80; loss: 1.76; acc: 0.52
Batch: 100; loss: 1.79; acc: 0.47
Batch: 120; loss: 1.88; acc: 0.44
Batch: 140; loss: 1.8; acc: 0.5
Batch: 160; loss: 1.77; acc: 0.56
Batch: 180; loss: 1.78; acc: 0.42
Batch: 200; loss: 1.76; acc: 0.48
Batch: 220; loss: 1.87; acc: 0.42
Batch: 240; loss: 1.84; acc: 0.41
Batch: 260; loss: 1.78; acc: 0.45
Batch: 280; loss: 1.75; acc: 0.47
Batch: 300; loss: 1.78; acc: 0.45
Batch: 320; loss: 1.81; acc: 0.5
Batch: 340; loss: 1.87; acc: 0.45
Batch: 360; loss: 1.84; acc: 0.42
Batch: 380; loss: 1.71; acc: 0.55
Batch: 400; loss: 1.71; acc: 0.45
Batch: 420; loss: 1.74; acc: 0.5
Batch: 440; loss: 1.76; acc: 0.5
Batch: 460; loss: 1.92; acc: 0.36
Batch: 480; loss: 1.82; acc: 0.47
Batch: 500; loss: 1.93; acc: 0.38
Batch: 520; loss: 1.86; acc: 0.45
Batch: 540; loss: 1.77; acc: 0.5
Batch: 560; loss: 1.77; acc: 0.5
Batch: 580; loss: 1.82; acc: 0.5
Batch: 600; loss: 1.88; acc: 0.39
Batch: 620; loss: 1.73; acc: 0.47
Batch: 640; loss: 1.81; acc: 0.48
Batch: 660; loss: 1.9; acc: 0.38
Batch: 680; loss: 1.84; acc: 0.5
Batch: 700; loss: 1.76; acc: 0.52
Batch: 720; loss: 1.75; acc: 0.53
Batch: 740; loss: 1.8; acc: 0.47
Batch: 760; loss: 1.83; acc: 0.39
Batch: 780; loss: 1.86; acc: 0.48
Train Epoch over. train_loss: 1.81; train_accuracy: 0.45 

2.6418898414704017e-05
8.744927981751971e-06
Batch: 0; loss: 1.79; acc: 0.47
Batch: 20; loss: 1.8; acc: 0.42
Batch: 40; loss: 1.54; acc: 0.62
Batch: 60; loss: 1.63; acc: 0.59
Batch: 80; loss: 1.68; acc: 0.53
Batch: 100; loss: 1.7; acc: 0.61
Batch: 120; loss: 1.73; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.52
Val Epoch over. val_loss: 1.7620825304347238; val_accuracy: 0.4822850318471338 

The current subspace-distance is: 8.744927981751971e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.88; acc: 0.42
Batch: 20; loss: 1.84; acc: 0.44
Batch: 40; loss: 1.76; acc: 0.56
Batch: 60; loss: 1.81; acc: 0.44
Batch: 80; loss: 1.8; acc: 0.42
Batch: 100; loss: 1.94; acc: 0.44
Batch: 120; loss: 1.69; acc: 0.58
Batch: 140; loss: 1.7; acc: 0.48
Batch: 160; loss: 1.76; acc: 0.5
Batch: 180; loss: 1.82; acc: 0.53
Batch: 200; loss: 1.72; acc: 0.56
Batch: 220; loss: 1.93; acc: 0.41
Batch: 240; loss: 1.94; acc: 0.27
Batch: 260; loss: 1.74; acc: 0.5
Batch: 280; loss: 1.85; acc: 0.36
Batch: 300; loss: 1.76; acc: 0.5
Batch: 320; loss: 1.81; acc: 0.44
Batch: 340; loss: 1.77; acc: 0.52
Batch: 360; loss: 1.77; acc: 0.47
Batch: 380; loss: 1.73; acc: 0.52
Batch: 400; loss: 1.86; acc: 0.42
Batch: 420; loss: 1.86; acc: 0.41
Batch: 440; loss: 1.83; acc: 0.41
Batch: 460; loss: 1.73; acc: 0.41
Batch: 480; loss: 1.76; acc: 0.47
Batch: 500; loss: 1.88; acc: 0.41
Batch: 520; loss: 1.7; acc: 0.55
Batch: 540; loss: 1.79; acc: 0.52
Batch: 560; loss: 1.82; acc: 0.45
Batch: 580; loss: 1.82; acc: 0.45
Batch: 600; loss: 1.79; acc: 0.38
Batch: 620; loss: 1.76; acc: 0.45
Batch: 640; loss: 1.89; acc: 0.44
Batch: 660; loss: 1.76; acc: 0.5
Batch: 680; loss: 1.74; acc: 0.61
Batch: 700; loss: 1.83; acc: 0.42
Batch: 720; loss: 1.64; acc: 0.56
Batch: 740; loss: 1.75; acc: 0.48
Batch: 760; loss: 1.82; acc: 0.42
Batch: 780; loss: 1.69; acc: 0.5
Train Epoch over. train_loss: 1.78; train_accuracy: 0.47 

2.7326290364726447e-05
8.139040801324882e-06
Batch: 0; loss: 1.79; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.47
Batch: 40; loss: 1.56; acc: 0.62
Batch: 60; loss: 1.62; acc: 0.59
Batch: 80; loss: 1.67; acc: 0.52
Batch: 100; loss: 1.71; acc: 0.61
Batch: 120; loss: 1.73; acc: 0.52
Batch: 140; loss: 1.65; acc: 0.53
Val Epoch over. val_loss: 1.7451604900846056; val_accuracy: 0.4919386942675159 

The current subspace-distance is: 8.139040801324882e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.87; acc: 0.39
Batch: 20; loss: 1.82; acc: 0.52
Batch: 40; loss: 1.73; acc: 0.52
Batch: 60; loss: 1.81; acc: 0.44
Batch: 80; loss: 1.82; acc: 0.39
Batch: 100; loss: 1.85; acc: 0.42
Batch: 120; loss: 1.75; acc: 0.47
Batch: 140; loss: 1.8; acc: 0.45
Batch: 160; loss: 1.87; acc: 0.39
Batch: 180; loss: 1.77; acc: 0.42
Batch: 200; loss: 1.76; acc: 0.45
Batch: 220; loss: 1.79; acc: 0.47
Batch: 240; loss: 1.82; acc: 0.5
Batch: 260; loss: 1.79; acc: 0.47
Batch: 280; loss: 1.78; acc: 0.55
Batch: 300; loss: 1.74; acc: 0.47
Batch: 320; loss: 1.91; acc: 0.36
Batch: 340; loss: 1.67; acc: 0.56
Batch: 360; loss: 1.74; acc: 0.42
Batch: 380; loss: 1.84; acc: 0.41
Batch: 400; loss: 1.81; acc: 0.5
Batch: 420; loss: 1.69; acc: 0.53
Batch: 440; loss: 1.72; acc: 0.52
Batch: 460; loss: 1.73; acc: 0.52
Batch: 480; loss: 1.75; acc: 0.45
Batch: 500; loss: 1.75; acc: 0.5
Batch: 520; loss: 1.83; acc: 0.48
Batch: 540; loss: 1.75; acc: 0.55
Batch: 560; loss: 1.69; acc: 0.5
Batch: 580; loss: 1.73; acc: 0.48
Batch: 600; loss: 1.72; acc: 0.5
Batch: 620; loss: 1.75; acc: 0.52
Batch: 640; loss: 1.67; acc: 0.61
Batch: 660; loss: 1.74; acc: 0.47
Batch: 680; loss: 1.68; acc: 0.53
Batch: 700; loss: 1.82; acc: 0.45
Batch: 720; loss: 1.83; acc: 0.39
Batch: 740; loss: 1.64; acc: 0.53
Batch: 760; loss: 1.82; acc: 0.41
Batch: 780; loss: 1.81; acc: 0.41
Train Epoch over. train_loss: 1.77; train_accuracy: 0.47 

2.8791655495297164e-05
1.0222235687251668e-05
Batch: 0; loss: 1.77; acc: 0.48
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.57; acc: 0.61
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.56
Batch: 100; loss: 1.73; acc: 0.5
Batch: 120; loss: 1.73; acc: 0.52
Batch: 140; loss: 1.62; acc: 0.62
Val Epoch over. val_loss: 1.740087244161375; val_accuracy: 0.4929339171974522 

The current subspace-distance is: 1.0222235687251668e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.47
Batch: 20; loss: 1.83; acc: 0.41
Batch: 40; loss: 1.69; acc: 0.5
Batch: 60; loss: 1.86; acc: 0.39
Batch: 80; loss: 1.75; acc: 0.47
Batch: 100; loss: 1.76; acc: 0.42
Batch: 120; loss: 1.73; acc: 0.48
Batch: 140; loss: 1.73; acc: 0.45
Batch: 160; loss: 1.8; acc: 0.42
Batch: 180; loss: 1.73; acc: 0.47
Batch: 200; loss: 1.75; acc: 0.59
Batch: 220; loss: 1.73; acc: 0.5
Batch: 240; loss: 1.76; acc: 0.47
Batch: 260; loss: 1.65; acc: 0.53
Batch: 280; loss: 1.65; acc: 0.61
Batch: 300; loss: 1.66; acc: 0.56
Batch: 320; loss: 1.68; acc: 0.59
Batch: 340; loss: 1.78; acc: 0.44
Batch: 360; loss: 1.76; acc: 0.44
Batch: 380; loss: 1.72; acc: 0.47
Batch: 400; loss: 1.76; acc: 0.45
Batch: 420; loss: 1.68; acc: 0.5
Batch: 440; loss: 1.65; acc: 0.58
Batch: 460; loss: 1.74; acc: 0.52
Batch: 480; loss: 1.75; acc: 0.48
Batch: 500; loss: 1.84; acc: 0.44
Batch: 520; loss: 1.7; acc: 0.52
Batch: 540; loss: 1.82; acc: 0.45
Batch: 560; loss: 1.75; acc: 0.47
Batch: 580; loss: 1.82; acc: 0.33
Batch: 600; loss: 1.76; acc: 0.5
Batch: 620; loss: 1.7; acc: 0.44
Batch: 640; loss: 1.66; acc: 0.56
Batch: 660; loss: 1.68; acc: 0.56
Batch: 680; loss: 1.67; acc: 0.55
Batch: 700; loss: 1.64; acc: 0.58
Batch: 720; loss: 1.74; acc: 0.48
Batch: 740; loss: 1.74; acc: 0.47
Batch: 760; loss: 1.79; acc: 0.45
Batch: 780; loss: 1.69; acc: 0.5
Train Epoch over. train_loss: 1.75; train_accuracy: 0.48 

2.9689790608244948e-05
1.029536360874772e-05
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.76; acc: 0.42
Batch: 40; loss: 1.57; acc: 0.61
Batch: 60; loss: 1.6; acc: 0.61
Batch: 80; loss: 1.63; acc: 0.61
Batch: 100; loss: 1.72; acc: 0.52
Batch: 120; loss: 1.74; acc: 0.55
Batch: 140; loss: 1.57; acc: 0.59
Val Epoch over. val_loss: 1.7117245394712801; val_accuracy: 0.5119426751592356 

The current subspace-distance is: 1.029536360874772e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.59
Batch: 40; loss: 1.65; acc: 0.59
Batch: 60; loss: 1.8; acc: 0.47
Batch: 80; loss: 1.63; acc: 0.52
Batch: 100; loss: 1.74; acc: 0.48
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.77; acc: 0.48
Batch: 160; loss: 1.73; acc: 0.5
Batch: 180; loss: 1.77; acc: 0.42
Batch: 200; loss: 1.73; acc: 0.45
Batch: 220; loss: 1.83; acc: 0.44
Batch: 240; loss: 1.74; acc: 0.39
Batch: 260; loss: 1.71; acc: 0.48
Batch: 280; loss: 1.76; acc: 0.5
Batch: 300; loss: 1.72; acc: 0.47
Batch: 320; loss: 1.7; acc: 0.47
Batch: 340; loss: 1.69; acc: 0.48
Batch: 360; loss: 1.7; acc: 0.5
Batch: 380; loss: 1.68; acc: 0.55
Batch: 400; loss: 1.68; acc: 0.47
Batch: 420; loss: 1.64; acc: 0.59
Batch: 440; loss: 1.75; acc: 0.42
Batch: 460; loss: 1.71; acc: 0.59
Batch: 480; loss: 1.69; acc: 0.5
Batch: 500; loss: 1.72; acc: 0.47
Batch: 520; loss: 1.71; acc: 0.39
Batch: 540; loss: 1.72; acc: 0.45
Batch: 560; loss: 1.82; acc: 0.47
Batch: 580; loss: 1.74; acc: 0.44
Batch: 600; loss: 1.63; acc: 0.5
Batch: 620; loss: 1.66; acc: 0.5
Batch: 640; loss: 1.83; acc: 0.36
Batch: 660; loss: 1.65; acc: 0.53
Batch: 680; loss: 1.64; acc: 0.61
Batch: 700; loss: 1.71; acc: 0.44
Batch: 720; loss: 1.66; acc: 0.52
Batch: 740; loss: 1.63; acc: 0.56
Batch: 760; loss: 1.65; acc: 0.5
Batch: 780; loss: 1.76; acc: 0.42
Train Epoch over. train_loss: 1.71; train_accuracy: 0.5 

3.084663694608025e-05
8.609532414993737e-06
Batch: 0; loss: 1.65; acc: 0.5
Batch: 20; loss: 1.71; acc: 0.45
Batch: 40; loss: 1.55; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.58
Batch: 80; loss: 1.59; acc: 0.59
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.67
Val Epoch over. val_loss: 1.6636679248445352; val_accuracy: 0.5268710191082803 

The current subspace-distance is: 8.609532414993737e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.56
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.67; acc: 0.48
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.92; acc: 0.38
Batch: 100; loss: 1.72; acc: 0.48
Batch: 120; loss: 1.71; acc: 0.55
Batch: 140; loss: 1.6; acc: 0.53
Batch: 160; loss: 1.6; acc: 0.55
Batch: 180; loss: 1.68; acc: 0.52
Batch: 200; loss: 1.64; acc: 0.52
Batch: 220; loss: 1.73; acc: 0.48
Batch: 240; loss: 1.73; acc: 0.45
Batch: 260; loss: 1.72; acc: 0.47
Batch: 280; loss: 1.61; acc: 0.58
Batch: 300; loss: 1.61; acc: 0.5
Batch: 320; loss: 1.65; acc: 0.48
Batch: 340; loss: 1.71; acc: 0.48
Batch: 360; loss: 1.49; acc: 0.64
Batch: 380; loss: 1.7; acc: 0.47
Batch: 400; loss: 1.61; acc: 0.53
Batch: 420; loss: 1.57; acc: 0.59
Batch: 440; loss: 1.64; acc: 0.55
Batch: 460; loss: 1.64; acc: 0.55
Batch: 480; loss: 1.66; acc: 0.55
Batch: 500; loss: 1.66; acc: 0.48
Batch: 520; loss: 1.59; acc: 0.61
Batch: 540; loss: 1.6; acc: 0.47
Batch: 560; loss: 1.71; acc: 0.48
Batch: 580; loss: 1.76; acc: 0.48
Batch: 600; loss: 1.64; acc: 0.47
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.58; acc: 0.67
Batch: 660; loss: 1.64; acc: 0.47
Batch: 680; loss: 1.58; acc: 0.53
Batch: 700; loss: 1.66; acc: 0.48
Batch: 720; loss: 1.66; acc: 0.56
Batch: 740; loss: 1.58; acc: 0.5
Batch: 760; loss: 1.67; acc: 0.55
Batch: 780; loss: 1.67; acc: 0.53
Train Epoch over. train_loss: 1.67; train_accuracy: 0.51 

3.49496076523792e-05
1.5980474927346222e-05
Batch: 0; loss: 1.62; acc: 0.53
Batch: 20; loss: 1.69; acc: 0.45
Batch: 40; loss: 1.52; acc: 0.66
Batch: 60; loss: 1.57; acc: 0.55
Batch: 80; loss: 1.57; acc: 0.59
Batch: 100; loss: 1.63; acc: 0.52
Batch: 120; loss: 1.77; acc: 0.48
Batch: 140; loss: 1.43; acc: 0.64
Val Epoch over. val_loss: 1.6351973456182298; val_accuracy: 0.5322452229299363 

The current subspace-distance is: 1.5980474927346222e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.41
Batch: 20; loss: 1.76; acc: 0.36
Batch: 40; loss: 1.64; acc: 0.55
Batch: 60; loss: 1.68; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.56
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.45
Batch: 160; loss: 1.63; acc: 0.5
Batch: 180; loss: 1.61; acc: 0.53
Batch: 200; loss: 1.69; acc: 0.47
Batch: 220; loss: 1.64; acc: 0.55
Batch: 240; loss: 1.67; acc: 0.5
Batch: 260; loss: 1.59; acc: 0.56
Batch: 280; loss: 1.69; acc: 0.45
Batch: 300; loss: 1.53; acc: 0.67
Batch: 320; loss: 1.63; acc: 0.58
Batch: 340; loss: 1.59; acc: 0.56
Batch: 360; loss: 1.63; acc: 0.53
Batch: 380; loss: 1.64; acc: 0.5
Batch: 400; loss: 1.76; acc: 0.36
Batch: 420; loss: 1.68; acc: 0.45
Batch: 440; loss: 1.64; acc: 0.52
Batch: 460; loss: 1.74; acc: 0.38
Batch: 480; loss: 1.63; acc: 0.48
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.64; acc: 0.41
Batch: 540; loss: 1.67; acc: 0.42
Batch: 560; loss: 1.64; acc: 0.53
Batch: 580; loss: 1.76; acc: 0.39
Batch: 600; loss: 1.59; acc: 0.58
Batch: 620; loss: 1.63; acc: 0.53
Batch: 640; loss: 1.69; acc: 0.48
Batch: 660; loss: 1.69; acc: 0.42
Batch: 680; loss: 1.61; acc: 0.55
Batch: 700; loss: 1.69; acc: 0.45
Batch: 720; loss: 1.74; acc: 0.47
Batch: 740; loss: 1.48; acc: 0.62
Batch: 760; loss: 1.63; acc: 0.55
Batch: 780; loss: 1.6; acc: 0.55
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.41648337780498e-05
1.0742436643340625e-05
Batch: 0; loss: 1.6; acc: 0.55
Batch: 20; loss: 1.67; acc: 0.45
Batch: 40; loss: 1.46; acc: 0.59
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.54; acc: 0.58
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.77; acc: 0.47
Batch: 140; loss: 1.4; acc: 0.73
Val Epoch over. val_loss: 1.6119299360141632; val_accuracy: 0.5305533439490446 

The current subspace-distance is: 1.0742436643340625e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.52
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.83; acc: 0.42
Batch: 60; loss: 1.49; acc: 0.66
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.59; acc: 0.53
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.77; acc: 0.47
Batch: 160; loss: 1.75; acc: 0.38
Batch: 180; loss: 1.63; acc: 0.47
Batch: 200; loss: 1.55; acc: 0.55
Batch: 220; loss: 1.75; acc: 0.45
Batch: 240; loss: 1.61; acc: 0.52
Batch: 260; loss: 1.62; acc: 0.52
Batch: 280; loss: 1.69; acc: 0.42
Batch: 300; loss: 1.59; acc: 0.52
Batch: 320; loss: 1.7; acc: 0.41
Batch: 340; loss: 1.55; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.62
Batch: 380; loss: 1.66; acc: 0.48
Batch: 400; loss: 1.54; acc: 0.53
Batch: 420; loss: 1.74; acc: 0.48
Batch: 440; loss: 1.63; acc: 0.45
Batch: 460; loss: 1.5; acc: 0.56
Batch: 480; loss: 1.62; acc: 0.53
Batch: 500; loss: 1.65; acc: 0.44
Batch: 520; loss: 1.7; acc: 0.52
Batch: 540; loss: 1.61; acc: 0.56
Batch: 560; loss: 1.61; acc: 0.47
Batch: 580; loss: 1.7; acc: 0.48
Batch: 600; loss: 1.66; acc: 0.48
Batch: 620; loss: 1.58; acc: 0.56
Batch: 640; loss: 1.63; acc: 0.48
Batch: 660; loss: 1.59; acc: 0.61
Batch: 680; loss: 1.62; acc: 0.52
Batch: 700; loss: 1.65; acc: 0.52
Batch: 720; loss: 1.59; acc: 0.52
Batch: 740; loss: 1.5; acc: 0.56
Batch: 760; loss: 1.64; acc: 0.48
Batch: 780; loss: 1.66; acc: 0.48
Train Epoch over. train_loss: 1.63; train_accuracy: 0.51 

3.689721415867098e-05
1.6249903637799434e-05
Batch: 0; loss: 1.59; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.44
Batch: 40; loss: 1.39; acc: 0.66
Batch: 60; loss: 1.5; acc: 0.53
Batch: 80; loss: 1.5; acc: 0.61
Batch: 100; loss: 1.6; acc: 0.53
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.37; acc: 0.72
Val Epoch over. val_loss: 1.5721158449816857; val_accuracy: 0.544984076433121 

The current subspace-distance is: 1.6249903637799434e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.53
Batch: 40; loss: 1.65; acc: 0.52
Batch: 60; loss: 1.6; acc: 0.48
Batch: 80; loss: 1.7; acc: 0.42
Batch: 100; loss: 1.61; acc: 0.45
Batch: 120; loss: 1.64; acc: 0.58
Batch: 140; loss: 1.63; acc: 0.5
Batch: 160; loss: 1.63; acc: 0.48
Batch: 180; loss: 1.58; acc: 0.59
Batch: 200; loss: 1.59; acc: 0.48
Batch: 220; loss: 1.63; acc: 0.45
Batch: 240; loss: 1.55; acc: 0.5
Batch: 260; loss: 1.57; acc: 0.56
Batch: 280; loss: 1.49; acc: 0.55
Batch: 300; loss: 1.53; acc: 0.56
Batch: 320; loss: 1.68; acc: 0.47
Batch: 340; loss: 1.51; acc: 0.58
Batch: 360; loss: 1.56; acc: 0.61
Batch: 380; loss: 1.69; acc: 0.47
Batch: 400; loss: 1.65; acc: 0.45
Batch: 420; loss: 1.56; acc: 0.5
Batch: 440; loss: 1.69; acc: 0.44
Batch: 460; loss: 1.69; acc: 0.47
Batch: 480; loss: 1.65; acc: 0.47
Batch: 500; loss: 1.51; acc: 0.55
Batch: 520; loss: 1.55; acc: 0.53
Batch: 540; loss: 1.58; acc: 0.48
Batch: 560; loss: 1.49; acc: 0.64
Batch: 580; loss: 1.65; acc: 0.5
Batch: 600; loss: 1.68; acc: 0.48
Batch: 620; loss: 1.7; acc: 0.36
Batch: 640; loss: 1.63; acc: 0.52
Batch: 660; loss: 1.57; acc: 0.47
Batch: 680; loss: 1.62; acc: 0.48
Batch: 700; loss: 1.6; acc: 0.5
Batch: 720; loss: 1.59; acc: 0.56
Batch: 740; loss: 1.63; acc: 0.48
Batch: 760; loss: 1.51; acc: 0.55
Batch: 780; loss: 1.51; acc: 0.59
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

3.810842827078886e-05
1.6398975276388228e-05
Batch: 0; loss: 1.59; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.44
Batch: 40; loss: 1.31; acc: 0.66
Batch: 60; loss: 1.46; acc: 0.55
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.58; acc: 0.53
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.37; acc: 0.72
Val Epoch over. val_loss: 1.5390414014743392; val_accuracy: 0.5463773885350318 

The current subspace-distance is: 1.6398975276388228e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.63; acc: 0.53
Batch: 40; loss: 1.61; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.45
Batch: 80; loss: 1.52; acc: 0.53
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.63; acc: 0.45
Batch: 140; loss: 1.66; acc: 0.41
Batch: 160; loss: 1.63; acc: 0.52
Batch: 180; loss: 1.53; acc: 0.55
Batch: 200; loss: 1.62; acc: 0.52
Batch: 220; loss: 1.56; acc: 0.56
Batch: 240; loss: 1.44; acc: 0.58
Batch: 260; loss: 1.85; acc: 0.38
Batch: 280; loss: 1.56; acc: 0.47
Batch: 300; loss: 1.53; acc: 0.5
Batch: 320; loss: 1.48; acc: 0.5
Batch: 340; loss: 1.43; acc: 0.59
Batch: 360; loss: 1.51; acc: 0.59
Batch: 380; loss: 1.46; acc: 0.59
Batch: 400; loss: 1.64; acc: 0.53
Batch: 420; loss: 1.52; acc: 0.59
Batch: 440; loss: 1.58; acc: 0.56
Batch: 460; loss: 1.49; acc: 0.52
Batch: 480; loss: 1.58; acc: 0.5
Batch: 500; loss: 1.61; acc: 0.52
Batch: 520; loss: 1.49; acc: 0.58
Batch: 540; loss: 1.36; acc: 0.61
Batch: 560; loss: 1.69; acc: 0.53
Batch: 580; loss: 1.49; acc: 0.56
Batch: 600; loss: 1.61; acc: 0.56
Batch: 620; loss: 1.41; acc: 0.58
Batch: 640; loss: 1.56; acc: 0.55
Batch: 660; loss: 1.47; acc: 0.58
Batch: 680; loss: 1.5; acc: 0.62
Batch: 700; loss: 1.48; acc: 0.53
Batch: 720; loss: 1.52; acc: 0.56
Batch: 740; loss: 1.52; acc: 0.58
Batch: 760; loss: 1.54; acc: 0.47
Batch: 780; loss: 1.61; acc: 0.5
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

4.090792572242208e-05
2.062258863588795e-05
Batch: 0; loss: 1.58; acc: 0.47
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.28; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.47; acc: 0.59
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.35; acc: 0.7
Val Epoch over. val_loss: 1.5229586567848352; val_accuracy: 0.5501592356687898 

The current subspace-distance is: 2.062258863588795e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.44
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 1.79; acc: 0.44
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.64
Batch: 100; loss: 1.46; acc: 0.55
Batch: 120; loss: 1.6; acc: 0.48
Batch: 140; loss: 1.66; acc: 0.44
Batch: 160; loss: 1.71; acc: 0.45
Batch: 180; loss: 1.59; acc: 0.55
Batch: 200; loss: 1.56; acc: 0.52
Batch: 220; loss: 1.5; acc: 0.62
Batch: 240; loss: 1.48; acc: 0.56
Batch: 260; loss: 1.6; acc: 0.48
Batch: 280; loss: 1.56; acc: 0.47
Batch: 300; loss: 1.63; acc: 0.52
Batch: 320; loss: 1.64; acc: 0.52
Batch: 340; loss: 1.77; acc: 0.38
Batch: 360; loss: 1.55; acc: 0.5
Batch: 380; loss: 1.58; acc: 0.5
Batch: 400; loss: 1.61; acc: 0.56
Batch: 420; loss: 1.64; acc: 0.48
Batch: 440; loss: 1.36; acc: 0.64
Batch: 460; loss: 1.66; acc: 0.47
Batch: 480; loss: 1.45; acc: 0.52
Batch: 500; loss: 1.58; acc: 0.48
Batch: 520; loss: 1.53; acc: 0.55
Batch: 540; loss: 1.34; acc: 0.7
Batch: 560; loss: 1.62; acc: 0.44
Batch: 580; loss: 1.64; acc: 0.55
Batch: 600; loss: 1.44; acc: 0.62
Batch: 620; loss: 1.51; acc: 0.48
Batch: 640; loss: 1.58; acc: 0.56
Batch: 660; loss: 1.59; acc: 0.5
Batch: 680; loss: 1.56; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.48
Batch: 720; loss: 1.57; acc: 0.53
Batch: 740; loss: 1.69; acc: 0.45
Batch: 760; loss: 1.56; acc: 0.55
Batch: 780; loss: 1.51; acc: 0.55
Train Epoch over. train_loss: 1.56; train_accuracy: 0.52 

4.207215170026757e-05
1.8484490283299237e-05
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.62; acc: 0.45
Batch: 40; loss: 1.25; acc: 0.7
Batch: 60; loss: 1.43; acc: 0.56
Batch: 80; loss: 1.45; acc: 0.59
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.37; acc: 0.67
Val Epoch over. val_loss: 1.5145194894948584; val_accuracy: 0.5522492038216561 

The current subspace-distance is: 1.8484490283299237e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.47; acc: 0.55
Batch: 20; loss: 1.55; acc: 0.59
Batch: 40; loss: 1.57; acc: 0.48
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.43; acc: 0.64
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.61; acc: 0.47
Batch: 140; loss: 1.55; acc: 0.45
Batch: 160; loss: 1.74; acc: 0.39
Batch: 180; loss: 1.58; acc: 0.47
Batch: 200; loss: 1.57; acc: 0.53
Batch: 220; loss: 1.62; acc: 0.45
Batch: 240; loss: 1.64; acc: 0.53
Batch: 260; loss: 1.59; acc: 0.5
Batch: 280; loss: 1.48; acc: 0.55
Batch: 300; loss: 1.61; acc: 0.56
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.58; acc: 0.53
Batch: 360; loss: 1.51; acc: 0.5
Batch: 380; loss: 1.48; acc: 0.56
Batch: 400; loss: 1.57; acc: 0.48
Batch: 420; loss: 1.48; acc: 0.56
Batch: 440; loss: 1.62; acc: 0.48
Batch: 460; loss: 1.57; acc: 0.53
Batch: 480; loss: 1.55; acc: 0.58
Batch: 500; loss: 1.52; acc: 0.55
Batch: 520; loss: 1.51; acc: 0.53
Batch: 540; loss: 1.55; acc: 0.52
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.47; acc: 0.62
Batch: 600; loss: 1.58; acc: 0.47
Batch: 620; loss: 1.68; acc: 0.55
Batch: 640; loss: 1.49; acc: 0.5
Batch: 660; loss: 1.61; acc: 0.48
Batch: 680; loss: 1.58; acc: 0.47
Batch: 700; loss: 1.58; acc: 0.58
Batch: 720; loss: 1.57; acc: 0.52
Batch: 740; loss: 1.6; acc: 0.44
Batch: 760; loss: 1.52; acc: 0.52
Batch: 780; loss: 1.56; acc: 0.55
Train Epoch over. train_loss: 1.55; train_accuracy: 0.53 

4.1381081246072426e-05
1.4837455637461971e-05
Batch: 0; loss: 1.58; acc: 0.5
Batch: 20; loss: 1.58; acc: 0.55
Batch: 40; loss: 1.22; acc: 0.7
Batch: 60; loss: 1.41; acc: 0.55
Batch: 80; loss: 1.44; acc: 0.58
Batch: 100; loss: 1.54; acc: 0.55
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.4940370222565476; val_accuracy: 0.5591162420382165 

The current subspace-distance is: 1.4837455637461971e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.49; acc: 0.55
Batch: 20; loss: 1.43; acc: 0.59
Batch: 40; loss: 1.77; acc: 0.36
Batch: 60; loss: 1.66; acc: 0.44
Batch: 80; loss: 1.65; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.49; acc: 0.53
Batch: 140; loss: 1.55; acc: 0.52
Batch: 160; loss: 1.86; acc: 0.31
Batch: 180; loss: 1.54; acc: 0.5
Batch: 200; loss: 1.51; acc: 0.47
Batch: 220; loss: 1.47; acc: 0.58
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 1.68; acc: 0.5
Batch: 280; loss: 1.58; acc: 0.45
Batch: 300; loss: 1.5; acc: 0.58
Batch: 320; loss: 1.48; acc: 0.56
Batch: 340; loss: 1.54; acc: 0.59
Batch: 360; loss: 1.45; acc: 0.5
Batch: 380; loss: 1.67; acc: 0.42
Batch: 400; loss: 1.56; acc: 0.53
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.58; acc: 0.58
Batch: 460; loss: 1.64; acc: 0.52
Batch: 480; loss: 1.58; acc: 0.45
Batch: 500; loss: 1.53; acc: 0.55
Batch: 520; loss: 1.53; acc: 0.62
Batch: 540; loss: 1.63; acc: 0.45
Batch: 560; loss: 1.52; acc: 0.52
Batch: 580; loss: 1.65; acc: 0.41
Batch: 600; loss: 1.36; acc: 0.61
Batch: 620; loss: 1.51; acc: 0.56
Batch: 640; loss: 1.5; acc: 0.55
Batch: 660; loss: 1.63; acc: 0.48
Batch: 680; loss: 1.65; acc: 0.47
Batch: 700; loss: 1.49; acc: 0.58
Batch: 720; loss: 1.52; acc: 0.62
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.57; acc: 0.52
Batch: 780; loss: 1.61; acc: 0.47
Train Epoch over. train_loss: 1.53; train_accuracy: 0.53 

4.24573227064684e-05
1.879794945125468e-05
Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.56; acc: 0.5
Batch: 40; loss: 1.2; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.58
Batch: 80; loss: 1.42; acc: 0.66
Batch: 100; loss: 1.53; acc: 0.5
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.33; acc: 0.72
Val Epoch over. val_loss: 1.4772589669865408; val_accuracy: 0.5719546178343949 

The current subspace-distance is: 1.879794945125468e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.43; acc: 0.62
Batch: 20; loss: 1.54; acc: 0.55
Batch: 40; loss: 1.36; acc: 0.64
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.53; acc: 0.61
Batch: 100; loss: 1.79; acc: 0.45
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 1.58; acc: 0.47
Batch: 160; loss: 1.45; acc: 0.58
Batch: 180; loss: 1.59; acc: 0.45
Batch: 200; loss: 1.37; acc: 0.59
Batch: 220; loss: 1.46; acc: 0.58
Batch: 240; loss: 1.54; acc: 0.47
Batch: 260; loss: 1.69; acc: 0.41
Batch: 280; loss: 1.54; acc: 0.55
Batch: 300; loss: 1.46; acc: 0.53
Batch: 320; loss: 1.63; acc: 0.56
Batch: 340; loss: 1.71; acc: 0.55
Batch: 360; loss: 1.61; acc: 0.48
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.46; acc: 0.55
Batch: 420; loss: 1.49; acc: 0.55
Batch: 440; loss: 1.59; acc: 0.55
Batch: 460; loss: 1.47; acc: 0.42
Batch: 480; loss: 1.52; acc: 0.53
Batch: 500; loss: 1.37; acc: 0.56
Batch: 520; loss: 1.39; acc: 0.66
Batch: 540; loss: 1.41; acc: 0.67
Batch: 560; loss: 1.54; acc: 0.45
Batch: 580; loss: 1.54; acc: 0.56
Batch: 600; loss: 1.61; acc: 0.58
Batch: 620; loss: 1.5; acc: 0.53
Batch: 640; loss: 1.54; acc: 0.47
Batch: 660; loss: 1.53; acc: 0.52
Batch: 680; loss: 1.37; acc: 0.62
Batch: 700; loss: 1.43; acc: 0.59
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.71; acc: 0.41
Batch: 760; loss: 1.68; acc: 0.44
Batch: 780; loss: 1.42; acc: 0.58
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.419392644194886e-05
1.8757056750473566e-05
Batch: 0; loss: 1.54; acc: 0.52
Batch: 20; loss: 1.53; acc: 0.5
Batch: 40; loss: 1.18; acc: 0.72
Batch: 60; loss: 1.37; acc: 0.58
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.52; acc: 0.48
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.34; acc: 0.73
Val Epoch over. val_loss: 1.4685248569318443; val_accuracy: 0.5775278662420382 

The current subspace-distance is: 1.8757056750473566e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.51; acc: 0.58
Batch: 20; loss: 1.59; acc: 0.41
Batch: 40; loss: 1.5; acc: 0.53
Batch: 60; loss: 1.41; acc: 0.62
Batch: 80; loss: 1.59; acc: 0.38
Batch: 100; loss: 1.52; acc: 0.58
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 1.53; acc: 0.59
Batch: 160; loss: 1.58; acc: 0.45
Batch: 180; loss: 1.39; acc: 0.66
Batch: 200; loss: 1.45; acc: 0.61
Batch: 220; loss: 1.38; acc: 0.58
Batch: 240; loss: 1.5; acc: 0.58
Batch: 260; loss: 1.5; acc: 0.55
Batch: 280; loss: 1.58; acc: 0.48
Batch: 300; loss: 1.54; acc: 0.47
Batch: 320; loss: 1.47; acc: 0.61
Batch: 340; loss: 1.63; acc: 0.44
Batch: 360; loss: 1.41; acc: 0.66
Batch: 380; loss: 1.58; acc: 0.55
Batch: 400; loss: 1.6; acc: 0.53
Batch: 420; loss: 1.62; acc: 0.48
Batch: 440; loss: 1.43; acc: 0.58
Batch: 460; loss: 1.44; acc: 0.5
Batch: 480; loss: 1.52; acc: 0.47
Batch: 500; loss: 1.44; acc: 0.61
Batch: 520; loss: 1.44; acc: 0.58
Batch: 540; loss: 1.6; acc: 0.39
Batch: 560; loss: 1.61; acc: 0.53
Batch: 580; loss: 1.48; acc: 0.52
Batch: 600; loss: 1.39; acc: 0.67
Batch: 620; loss: 1.54; acc: 0.58
Batch: 640; loss: 1.65; acc: 0.45
Batch: 660; loss: 1.57; acc: 0.53
Batch: 680; loss: 1.56; acc: 0.52
Batch: 700; loss: 1.47; acc: 0.5
Batch: 720; loss: 1.64; acc: 0.44
Batch: 740; loss: 1.37; acc: 0.59
Batch: 760; loss: 1.53; acc: 0.48
Batch: 780; loss: 1.54; acc: 0.48
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.379855454317294e-05
1.7946573279914446e-05
Batch: 0; loss: 1.54; acc: 0.5
Batch: 20; loss: 1.53; acc: 0.5
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.37; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.53; acc: 0.47
Batch: 120; loss: 1.69; acc: 0.45
Batch: 140; loss: 1.34; acc: 0.69
Val Epoch over. val_loss: 1.4643696166907147; val_accuracy: 0.5715565286624203 

The current subspace-distance is: 1.7946573279914446e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.48
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 1.57; acc: 0.55
Batch: 60; loss: 1.5; acc: 0.52
Batch: 80; loss: 1.57; acc: 0.53
Batch: 100; loss: 1.64; acc: 0.36
Batch: 120; loss: 1.53; acc: 0.48
Batch: 140; loss: 1.55; acc: 0.58
Batch: 160; loss: 1.52; acc: 0.52
Batch: 180; loss: 1.51; acc: 0.55
Batch: 200; loss: 1.56; acc: 0.59
Batch: 220; loss: 1.46; acc: 0.59
Batch: 240; loss: 1.35; acc: 0.56
Batch: 260; loss: 1.63; acc: 0.45
Batch: 280; loss: 1.44; acc: 0.59
Batch: 300; loss: 1.54; acc: 0.56
Batch: 320; loss: 1.59; acc: 0.47
Batch: 340; loss: 1.44; acc: 0.53
Batch: 360; loss: 1.4; acc: 0.66
Batch: 380; loss: 1.34; acc: 0.62
Batch: 400; loss: 1.4; acc: 0.7
Batch: 420; loss: 1.41; acc: 0.58
Batch: 440; loss: 1.56; acc: 0.5
Batch: 460; loss: 1.53; acc: 0.45
Batch: 480; loss: 1.36; acc: 0.67
Batch: 500; loss: 1.31; acc: 0.61
Batch: 520; loss: 1.41; acc: 0.59
Batch: 540; loss: 1.47; acc: 0.64
Batch: 560; loss: 1.59; acc: 0.52
Batch: 580; loss: 1.53; acc: 0.56
Batch: 600; loss: 1.73; acc: 0.41
Batch: 620; loss: 1.56; acc: 0.41
Batch: 640; loss: 1.6; acc: 0.45
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.68; acc: 0.48
Batch: 700; loss: 1.43; acc: 0.56
Batch: 720; loss: 1.43; acc: 0.56
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.37; acc: 0.66
Batch: 780; loss: 1.5; acc: 0.53
Train Epoch over. train_loss: 1.5; train_accuracy: 0.55 

4.542982787825167e-05
1.859485200839117e-05
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.5; acc: 0.5
Batch: 40; loss: 1.14; acc: 0.72
Batch: 60; loss: 1.34; acc: 0.58
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.5; acc: 0.48
Batch: 120; loss: 1.67; acc: 0.45
Batch: 140; loss: 1.33; acc: 0.67
Val Epoch over. val_loss: 1.4450922384383573; val_accuracy: 0.5791202229299363 

The current subspace-distance is: 1.859485200839117e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.54; acc: 0.56
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 1.49; acc: 0.58
Batch: 80; loss: 1.48; acc: 0.53
Batch: 100; loss: 1.51; acc: 0.56
Batch: 120; loss: 1.36; acc: 0.64
Batch: 140; loss: 1.51; acc: 0.5
Batch: 160; loss: 1.49; acc: 0.55
Batch: 180; loss: 1.52; acc: 0.58
Batch: 200; loss: 1.51; acc: 0.52
Batch: 220; loss: 1.57; acc: 0.5
Batch: 240; loss: 1.63; acc: 0.48
Batch: 260; loss: 1.31; acc: 0.66
Batch: 280; loss: 1.28; acc: 0.72
Batch: 300; loss: 1.44; acc: 0.61
Batch: 320; loss: 1.58; acc: 0.56
Batch: 340; loss: 1.48; acc: 0.52
Batch: 360; loss: 1.58; acc: 0.5
Batch: 380; loss: 1.74; acc: 0.39
Batch: 400; loss: 1.49; acc: 0.52
Batch: 420; loss: 1.43; acc: 0.67
Batch: 440; loss: 1.4; acc: 0.61
Batch: 460; loss: 1.44; acc: 0.56
Batch: 480; loss: 1.57; acc: 0.5
Batch: 500; loss: 1.3; acc: 0.69
Batch: 520; loss: 1.55; acc: 0.56
Batch: 540; loss: 1.4; acc: 0.58
Batch: 560; loss: 1.46; acc: 0.47
Batch: 580; loss: 1.41; acc: 0.64
Batch: 600; loss: 1.56; acc: 0.52
Batch: 620; loss: 1.58; acc: 0.53
Batch: 640; loss: 1.66; acc: 0.39
Batch: 660; loss: 1.38; acc: 0.58
Batch: 680; loss: 1.48; acc: 0.56
Batch: 700; loss: 1.39; acc: 0.62
Batch: 720; loss: 1.6; acc: 0.5
Batch: 740; loss: 1.52; acc: 0.52
Batch: 760; loss: 1.48; acc: 0.53
Batch: 780; loss: 1.51; acc: 0.55
Train Epoch over. train_loss: 1.49; train_accuracy: 0.55 

4.6255641791503876e-05
1.8191813069279306e-05
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.49; acc: 0.53
Batch: 40; loss: 1.13; acc: 0.75
Batch: 60; loss: 1.34; acc: 0.58
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.5; acc: 0.48
Batch: 120; loss: 1.67; acc: 0.42
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.4420384669759472; val_accuracy: 0.5812101910828026 

The current subspace-distance is: 1.8191813069279306e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.45; acc: 0.62
Batch: 20; loss: 1.74; acc: 0.48
Batch: 40; loss: 1.42; acc: 0.55
Batch: 60; loss: 1.48; acc: 0.52
Batch: 80; loss: 1.43; acc: 0.55
Batch: 100; loss: 1.53; acc: 0.47
Batch: 120; loss: 1.4; acc: 0.64
Batch: 140; loss: 1.64; acc: 0.45
Batch: 160; loss: 1.63; acc: 0.48
Batch: 180; loss: 1.54; acc: 0.55
Batch: 200; loss: 1.44; acc: 0.56
Batch: 220; loss: 1.41; acc: 0.58
Batch: 240; loss: 1.5; acc: 0.5
Batch: 260; loss: 1.39; acc: 0.59
Batch: 280; loss: 1.56; acc: 0.52
Batch: 300; loss: 1.47; acc: 0.58
Batch: 320; loss: 1.55; acc: 0.52
Batch: 340; loss: 1.7; acc: 0.47
Batch: 360; loss: 1.39; acc: 0.61
Batch: 380; loss: 1.42; acc: 0.58
Batch: 400; loss: 1.43; acc: 0.56
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.49; acc: 0.53
Batch: 460; loss: 1.63; acc: 0.42
Batch: 480; loss: 1.39; acc: 0.58
Batch: 500; loss: 1.44; acc: 0.55
Batch: 520; loss: 1.57; acc: 0.42
Batch: 540; loss: 1.51; acc: 0.52
Batch: 560; loss: 1.52; acc: 0.5
Batch: 580; loss: 1.37; acc: 0.61
Batch: 600; loss: 1.52; acc: 0.56
Batch: 620; loss: 1.45; acc: 0.61
Batch: 640; loss: 1.56; acc: 0.5
Batch: 660; loss: 1.4; acc: 0.48
Batch: 680; loss: 1.45; acc: 0.52
Batch: 700; loss: 1.62; acc: 0.42
Batch: 720; loss: 1.51; acc: 0.59
Batch: 740; loss: 1.51; acc: 0.53
Batch: 760; loss: 1.6; acc: 0.42
Batch: 780; loss: 1.47; acc: 0.55
Train Epoch over. train_loss: 1.48; train_accuracy: 0.55 

4.7231635107891634e-05
2.1290776203386486e-05
Batch: 0; loss: 1.51; acc: 0.53
Batch: 20; loss: 1.48; acc: 0.5
Batch: 40; loss: 1.13; acc: 0.73
Batch: 60; loss: 1.33; acc: 0.61
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.51; acc: 0.45
Batch: 120; loss: 1.68; acc: 0.42
Batch: 140; loss: 1.32; acc: 0.64
Val Epoch over. val_loss: 1.440193000113129; val_accuracy: 0.576234076433121 

The current subspace-distance is: 2.1290776203386486e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.45; acc: 0.55
Batch: 20; loss: 1.56; acc: 0.53
Batch: 40; loss: 1.53; acc: 0.48
Batch: 60; loss: 1.51; acc: 0.53
Batch: 80; loss: 1.5; acc: 0.56
Batch: 100; loss: 1.46; acc: 0.55
Batch: 120; loss: 1.47; acc: 0.59
Batch: 140; loss: 1.53; acc: 0.59
Batch: 160; loss: 1.38; acc: 0.7
Batch: 180; loss: 1.55; acc: 0.5
Batch: 200; loss: 1.42; acc: 0.58
Batch: 220; loss: 1.48; acc: 0.53
Batch: 240; loss: 1.42; acc: 0.61
Batch: 260; loss: 1.44; acc: 0.58
Batch: 280; loss: 1.36; acc: 0.58
Batch: 300; loss: 1.38; acc: 0.61
Batch: 320; loss: 1.4; acc: 0.66
Batch: 340; loss: 1.43; acc: 0.61
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.34; acc: 0.69
Batch: 400; loss: 1.54; acc: 0.5
Batch: 420; loss: 1.58; acc: 0.55
Batch: 440; loss: 1.45; acc: 0.61
Batch: 460; loss: 1.41; acc: 0.55
Batch: 480; loss: 1.29; acc: 0.67
Batch: 500; loss: 1.57; acc: 0.47
Batch: 520; loss: 1.71; acc: 0.38
Batch: 540; loss: 1.45; acc: 0.59
Batch: 560; loss: 1.51; acc: 0.58
Batch: 580; loss: 1.48; acc: 0.53
Batch: 600; loss: 1.47; acc: 0.56
Batch: 620; loss: 1.59; acc: 0.5
Batch: 640; loss: 1.4; acc: 0.66
Batch: 660; loss: 1.4; acc: 0.58
Batch: 680; loss: 1.22; acc: 0.73
Batch: 700; loss: 1.47; acc: 0.59
Batch: 720; loss: 1.4; acc: 0.52
Batch: 740; loss: 1.58; acc: 0.53
Batch: 760; loss: 1.62; acc: 0.45
Batch: 780; loss: 1.62; acc: 0.58
Train Epoch over. train_loss: 1.48; train_accuracy: 0.55 

4.663114668801427e-05
1.9121798686683178e-05
Batch: 0; loss: 1.5; acc: 0.53
Batch: 20; loss: 1.47; acc: 0.55
Batch: 40; loss: 1.11; acc: 0.77
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.49; acc: 0.47
Batch: 120; loss: 1.66; acc: 0.45
Batch: 140; loss: 1.3; acc: 0.66
Val Epoch over. val_loss: 1.4285276513190786; val_accuracy: 0.5833001592356688 

The current subspace-distance is: 1.9121798686683178e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.33; acc: 0.61
Batch: 20; loss: 1.41; acc: 0.61
Batch: 40; loss: 1.46; acc: 0.58
Batch: 60; loss: 1.52; acc: 0.47
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.58; acc: 0.56
Batch: 120; loss: 1.52; acc: 0.59
Batch: 140; loss: 1.51; acc: 0.48
Batch: 160; loss: 1.52; acc: 0.58
Batch: 180; loss: 1.43; acc: 0.58
Batch: 200; loss: 1.29; acc: 0.61
Batch: 220; loss: 1.55; acc: 0.56
Batch: 240; loss: 1.42; acc: 0.58
Batch: 260; loss: 1.46; acc: 0.62
Batch: 280; loss: 1.54; acc: 0.56
Batch: 300; loss: 1.52; acc: 0.53
Batch: 320; loss: 1.48; acc: 0.52
Batch: 340; loss: 1.33; acc: 0.64
Batch: 360; loss: 1.39; acc: 0.61
Batch: 380; loss: 1.41; acc: 0.61
Batch: 400; loss: 1.43; acc: 0.59
Batch: 420; loss: 1.49; acc: 0.52
Batch: 440; loss: 1.67; acc: 0.41
Batch: 460; loss: 1.54; acc: 0.52
Batch: 480; loss: 1.48; acc: 0.56
Batch: 500; loss: 1.37; acc: 0.62
Batch: 520; loss: 1.71; acc: 0.36
Batch: 540; loss: 1.35; acc: 0.61
Batch: 560; loss: 1.43; acc: 0.64
Batch: 580; loss: 1.42; acc: 0.61
Batch: 600; loss: 1.37; acc: 0.59
Batch: 620; loss: 1.29; acc: 0.67
Batch: 640; loss: 1.31; acc: 0.67
Batch: 660; loss: 1.56; acc: 0.39
Batch: 680; loss: 1.27; acc: 0.72
Batch: 700; loss: 1.48; acc: 0.48
Batch: 720; loss: 1.4; acc: 0.61
Batch: 740; loss: 1.5; acc: 0.59
Batch: 760; loss: 1.35; acc: 0.59
Batch: 780; loss: 1.5; acc: 0.58
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.563963375403546e-05
1.4864684999338351e-05
Batch: 0; loss: 1.5; acc: 0.53
Batch: 20; loss: 1.48; acc: 0.53
Batch: 40; loss: 1.11; acc: 0.77
Batch: 60; loss: 1.32; acc: 0.56
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.5; acc: 0.45
Batch: 120; loss: 1.68; acc: 0.42
Batch: 140; loss: 1.32; acc: 0.64
Val Epoch over. val_loss: 1.4311735850230904; val_accuracy: 0.5730493630573248 

The current subspace-distance is: 1.4864684999338351e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.41; acc: 0.55
Batch: 20; loss: 1.54; acc: 0.48
Batch: 40; loss: 1.44; acc: 0.55
Batch: 60; loss: 1.45; acc: 0.64
Batch: 80; loss: 1.31; acc: 0.62
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.56
Batch: 160; loss: 1.68; acc: 0.45
Batch: 180; loss: 1.47; acc: 0.48
Batch: 200; loss: 1.51; acc: 0.53
Batch: 220; loss: 1.34; acc: 0.58
Batch: 240; loss: 1.54; acc: 0.59
Batch: 260; loss: 1.58; acc: 0.5
Batch: 280; loss: 1.53; acc: 0.5
Batch: 300; loss: 1.4; acc: 0.64
Batch: 320; loss: 1.41; acc: 0.59
Batch: 340; loss: 1.34; acc: 0.7
Batch: 360; loss: 1.47; acc: 0.59
Batch: 380; loss: 1.39; acc: 0.59
Batch: 400; loss: 1.65; acc: 0.52
Batch: 420; loss: 1.52; acc: 0.52
Batch: 440; loss: 1.48; acc: 0.58
Batch: 460; loss: 1.35; acc: 0.59
Batch: 480; loss: 1.38; acc: 0.55
Batch: 500; loss: 1.39; acc: 0.58
Batch: 520; loss: 1.43; acc: 0.55
Batch: 540; loss: 1.46; acc: 0.5
Batch: 560; loss: 1.54; acc: 0.47
Batch: 580; loss: 1.45; acc: 0.52
Batch: 600; loss: 1.7; acc: 0.48
Batch: 620; loss: 1.47; acc: 0.58
Batch: 640; loss: 1.34; acc: 0.59
Batch: 660; loss: 1.61; acc: 0.47
Batch: 680; loss: 1.51; acc: 0.52
Batch: 700; loss: 1.39; acc: 0.67
Batch: 720; loss: 1.4; acc: 0.61
Batch: 740; loss: 1.52; acc: 0.53
Batch: 760; loss: 1.38; acc: 0.64
Batch: 780; loss: 1.49; acc: 0.55
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.8651319957571104e-05
2.024082641582936e-05
Batch: 0; loss: 1.49; acc: 0.53
Batch: 20; loss: 1.47; acc: 0.52
Batch: 40; loss: 1.11; acc: 0.77
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.49; acc: 0.45
Batch: 120; loss: 1.67; acc: 0.42
Batch: 140; loss: 1.3; acc: 0.64
Val Epoch over. val_loss: 1.4279302100467075; val_accuracy: 0.580015923566879 

The current subspace-distance is: 2.024082641582936e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.41; acc: 0.55
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 1.5; acc: 0.58
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.48; acc: 0.53
Batch: 100; loss: 1.56; acc: 0.53
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.5; acc: 0.55
Batch: 160; loss: 1.29; acc: 0.72
Batch: 180; loss: 1.52; acc: 0.56
Batch: 200; loss: 1.49; acc: 0.52
Batch: 220; loss: 1.48; acc: 0.53
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 1.47; acc: 0.56
Batch: 280; loss: 1.23; acc: 0.7
Batch: 300; loss: 1.36; acc: 0.61
Batch: 320; loss: 1.5; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.47
Batch: 360; loss: 1.34; acc: 0.66
Batch: 380; loss: 1.62; acc: 0.45
Batch: 400; loss: 1.43; acc: 0.55
Batch: 420; loss: 1.42; acc: 0.62
Batch: 440; loss: 1.41; acc: 0.61
Batch: 460; loss: 1.51; acc: 0.52
Batch: 480; loss: 1.36; acc: 0.66
Batch: 500; loss: 1.75; acc: 0.47
Batch: 520; loss: 1.5; acc: 0.53
Batch: 540; loss: 1.47; acc: 0.55
Batch: 560; loss: 1.55; acc: 0.45
Batch: 580; loss: 1.36; acc: 0.58
Batch: 600; loss: 1.49; acc: 0.55
Batch: 620; loss: 1.35; acc: 0.64
Batch: 640; loss: 1.43; acc: 0.58
Batch: 660; loss: 1.64; acc: 0.52
Batch: 680; loss: 1.58; acc: 0.5
Batch: 700; loss: 1.47; acc: 0.56
Batch: 720; loss: 1.36; acc: 0.61
Batch: 740; loss: 1.32; acc: 0.61
Batch: 760; loss: 1.56; acc: 0.58
Batch: 780; loss: 1.51; acc: 0.47
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.74679509352427e-05
1.7650023437454365e-05
Batch: 0; loss: 1.5; acc: 0.48
Batch: 20; loss: 1.48; acc: 0.52
Batch: 40; loss: 1.11; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.5; acc: 0.45
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.29; acc: 0.7
Val Epoch over. val_loss: 1.4275757613455413; val_accuracy: 0.5679737261146497 

The current subspace-distance is: 1.7650023437454365e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.28; acc: 0.67
Batch: 40; loss: 1.61; acc: 0.44
Batch: 60; loss: 1.4; acc: 0.59
Batch: 80; loss: 1.52; acc: 0.52
Batch: 100; loss: 1.47; acc: 0.52
Batch: 120; loss: 1.56; acc: 0.47
Batch: 140; loss: 1.29; acc: 0.62
Batch: 160; loss: 1.62; acc: 0.55
Batch: 180; loss: 1.59; acc: 0.47
Batch: 200; loss: 1.62; acc: 0.42
Batch: 220; loss: 1.45; acc: 0.55
Batch: 240; loss: 1.36; acc: 0.62
Batch: 260; loss: 1.54; acc: 0.55
Batch: 280; loss: 1.33; acc: 0.69
Batch: 300; loss: 1.55; acc: 0.52
Batch: 320; loss: 1.4; acc: 0.62
Batch: 340; loss: 1.51; acc: 0.53
Batch: 360; loss: 1.54; acc: 0.55
Batch: 380; loss: 1.45; acc: 0.55
Batch: 400; loss: 1.4; acc: 0.7
Batch: 420; loss: 1.36; acc: 0.62
Batch: 440; loss: 1.39; acc: 0.55
Batch: 460; loss: 1.37; acc: 0.59
Batch: 480; loss: 1.47; acc: 0.56
Batch: 500; loss: 1.59; acc: 0.55
Batch: 520; loss: 1.67; acc: 0.45
Batch: 540; loss: 1.3; acc: 0.62
Batch: 560; loss: 1.49; acc: 0.48
Batch: 580; loss: 1.4; acc: 0.59
Batch: 600; loss: 1.27; acc: 0.66
Batch: 620; loss: 1.38; acc: 0.62
Batch: 640; loss: 1.41; acc: 0.53
Batch: 660; loss: 1.45; acc: 0.48
Batch: 680; loss: 1.48; acc: 0.47
Batch: 700; loss: 1.56; acc: 0.5
Batch: 720; loss: 1.45; acc: 0.56
Batch: 740; loss: 1.28; acc: 0.66
Batch: 760; loss: 1.41; acc: 0.56
Batch: 780; loss: 1.48; acc: 0.62
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.8542427975917235e-05
2.232988117611967e-05
Batch: 0; loss: 1.48; acc: 0.5
Batch: 20; loss: 1.47; acc: 0.52
Batch: 40; loss: 1.09; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.59
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.48; acc: 0.42
Batch: 120; loss: 1.68; acc: 0.42
Batch: 140; loss: 1.28; acc: 0.69
Val Epoch over. val_loss: 1.4155933689919247; val_accuracy: 0.5791202229299363 

The current subspace-distance is: 2.232988117611967e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.46; acc: 0.47
Batch: 20; loss: 1.48; acc: 0.56
Batch: 40; loss: 1.38; acc: 0.62
Batch: 60; loss: 1.4; acc: 0.61
Batch: 80; loss: 1.64; acc: 0.41
Batch: 100; loss: 1.41; acc: 0.56
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.53; acc: 0.55
Batch: 160; loss: 1.48; acc: 0.61
Batch: 180; loss: 1.56; acc: 0.56
Batch: 200; loss: 1.43; acc: 0.55
Batch: 220; loss: 1.54; acc: 0.5
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.49; acc: 0.61
Batch: 280; loss: 1.38; acc: 0.58
Batch: 300; loss: 1.47; acc: 0.48
Batch: 320; loss: 1.51; acc: 0.45
Batch: 340; loss: 1.56; acc: 0.45
Batch: 360; loss: 1.54; acc: 0.5
Batch: 380; loss: 1.36; acc: 0.64
Batch: 400; loss: 1.38; acc: 0.52
Batch: 420; loss: 1.66; acc: 0.44
Batch: 440; loss: 1.24; acc: 0.64
Batch: 460; loss: 1.46; acc: 0.53
Batch: 480; loss: 1.46; acc: 0.55
Batch: 500; loss: 1.44; acc: 0.55
Batch: 520; loss: 1.41; acc: 0.62
Batch: 540; loss: 1.53; acc: 0.55
Batch: 560; loss: 1.27; acc: 0.66
Batch: 580; loss: 1.5; acc: 0.5
Batch: 600; loss: 1.44; acc: 0.58
Batch: 620; loss: 1.49; acc: 0.5
Batch: 640; loss: 1.35; acc: 0.64
Batch: 660; loss: 1.37; acc: 0.56
Batch: 680; loss: 1.52; acc: 0.44
Batch: 700; loss: 1.39; acc: 0.62
Batch: 720; loss: 1.35; acc: 0.64
Batch: 740; loss: 1.44; acc: 0.58
Batch: 760; loss: 1.55; acc: 0.5
Batch: 780; loss: 1.25; acc: 0.69
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.842634007218294e-05
2.1257554180920124e-05
Batch: 0; loss: 1.48; acc: 0.52
Batch: 20; loss: 1.46; acc: 0.52
Batch: 40; loss: 1.1; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.48; acc: 0.47
Batch: 120; loss: 1.67; acc: 0.44
Batch: 140; loss: 1.29; acc: 0.67
Val Epoch over. val_loss: 1.4170754563276935; val_accuracy: 0.5766321656050956 

The current subspace-distance is: 2.1257554180920124e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.64; acc: 0.45
Batch: 20; loss: 1.53; acc: 0.5
Batch: 40; loss: 1.58; acc: 0.5
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.47; acc: 0.53
Batch: 100; loss: 1.38; acc: 0.69
Batch: 120; loss: 1.46; acc: 0.59
Batch: 140; loss: 1.4; acc: 0.55
Batch: 160; loss: 1.6; acc: 0.55
Batch: 180; loss: 1.4; acc: 0.53
Batch: 200; loss: 1.56; acc: 0.47
Batch: 220; loss: 1.33; acc: 0.64
Batch: 240; loss: 1.52; acc: 0.45
Batch: 260; loss: 1.58; acc: 0.47
Batch: 280; loss: 1.56; acc: 0.52
Batch: 300; loss: 1.6; acc: 0.47
Batch: 320; loss: 1.42; acc: 0.64
Batch: 340; loss: 1.61; acc: 0.5
Batch: 360; loss: 1.53; acc: 0.47
Batch: 380; loss: 1.47; acc: 0.53
Batch: 400; loss: 1.47; acc: 0.58
Batch: 420; loss: 1.55; acc: 0.5
Batch: 440; loss: 1.45; acc: 0.55
Batch: 460; loss: 1.49; acc: 0.53
Batch: 480; loss: 1.6; acc: 0.52
Batch: 500; loss: 1.5; acc: 0.52
Batch: 520; loss: 1.44; acc: 0.56
Batch: 540; loss: 1.37; acc: 0.66
Batch: 560; loss: 1.53; acc: 0.56
Batch: 580; loss: 1.47; acc: 0.47
Batch: 600; loss: 1.4; acc: 0.61
Batch: 620; loss: 1.53; acc: 0.52
Batch: 640; loss: 1.45; acc: 0.64
Batch: 660; loss: 1.5; acc: 0.52
Batch: 680; loss: 1.42; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.45; acc: 0.55
Batch: 740; loss: 1.53; acc: 0.5
Batch: 760; loss: 1.66; acc: 0.47
Batch: 780; loss: 1.42; acc: 0.53
Train Epoch over. train_loss: 1.46; train_accuracy: 0.55 

4.785476630786434e-05
1.9216442524339072e-05
Batch: 0; loss: 1.48; acc: 0.53
Batch: 20; loss: 1.47; acc: 0.52
Batch: 40; loss: 1.1; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.59
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.48; acc: 0.45
Batch: 120; loss: 1.66; acc: 0.45
Batch: 140; loss: 1.31; acc: 0.62
Val Epoch over. val_loss: 1.422151180589275; val_accuracy: 0.5804140127388535 

The current subspace-distance is: 1.9216442524339072e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.36; acc: 0.61
Batch: 20; loss: 1.5; acc: 0.48
Batch: 40; loss: 1.42; acc: 0.55
Batch: 60; loss: 1.44; acc: 0.67
Batch: 80; loss: 1.36; acc: 0.69
Batch: 100; loss: 1.44; acc: 0.5
Batch: 120; loss: 1.35; acc: 0.66
Batch: 140; loss: 1.48; acc: 0.48
Batch: 160; loss: 1.3; acc: 0.7
Batch: 180; loss: 1.46; acc: 0.58
Batch: 200; loss: 1.51; acc: 0.52
Batch: 220; loss: 1.54; acc: 0.53
Batch: 240; loss: 1.53; acc: 0.53
Batch: 260; loss: 1.33; acc: 0.66
Batch: 280; loss: 1.35; acc: 0.62
Batch: 300; loss: 1.43; acc: 0.56
Batch: 320; loss: 1.52; acc: 0.55
Batch: 340; loss: 1.31; acc: 0.59
Batch: 360; loss: 1.55; acc: 0.53
Batch: 380; loss: 1.57; acc: 0.47
Batch: 400; loss: 1.58; acc: 0.59
Batch: 420; loss: 1.44; acc: 0.52
Batch: 440; loss: 1.59; acc: 0.53
Batch: 460; loss: 1.51; acc: 0.56
Batch: 480; loss: 1.33; acc: 0.66
Batch: 500; loss: 1.6; acc: 0.52
Batch: 520; loss: 1.55; acc: 0.5
Batch: 540; loss: 1.61; acc: 0.47
Batch: 560; loss: 1.37; acc: 0.61
Batch: 580; loss: 1.44; acc: 0.53
Batch: 600; loss: 1.42; acc: 0.61
Batch: 620; loss: 1.62; acc: 0.44
Batch: 640; loss: 1.52; acc: 0.55
Batch: 660; loss: 1.41; acc: 0.61
Batch: 680; loss: 1.36; acc: 0.62
Batch: 700; loss: 1.35; acc: 0.61
Batch: 720; loss: 1.36; acc: 0.64
Batch: 740; loss: 1.46; acc: 0.53
Batch: 760; loss: 1.4; acc: 0.56
Batch: 780; loss: 1.37; acc: 0.66
Train Epoch over. train_loss: 1.47; train_accuracy: 0.55 

4.794517735717818e-05
2.012541881413199e-05
Batch: 0; loss: 1.48; acc: 0.5
Batch: 20; loss: 1.47; acc: 0.52
Batch: 40; loss: 1.09; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.58
Batch: 80; loss: 1.4; acc: 0.62
Batch: 100; loss: 1.48; acc: 0.47
Batch: 120; loss: 1.65; acc: 0.45
Batch: 140; loss: 1.29; acc: 0.7
Val Epoch over. val_loss: 1.415206012452484; val_accuracy: 0.5742436305732485 

The current subspace-distance is: 2.012541881413199e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.5
Batch: 20; loss: 1.49; acc: 0.45
Batch: 40; loss: 1.36; acc: 0.56
Batch: 60; loss: 1.43; acc: 0.5
Batch: 80; loss: 1.3; acc: 0.67
Batch: 100; loss: 1.63; acc: 0.48
Batch: 120; loss: 1.52; acc: 0.61
Batch: 140; loss: 1.65; acc: 0.45
Batch: 160; loss: 1.61; acc: 0.48
Batch: 180; loss: 1.38; acc: 0.59
Batch: 200; loss: 1.52; acc: 0.58
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.41; acc: 0.64
Batch: 260; loss: 1.39; acc: 0.58
Batch: 280; loss: 1.46; acc: 0.47
Batch: 300; loss: 1.51; acc: 0.53
Batch: 320; loss: 1.53; acc: 0.47
Batch: 340; loss: 1.48; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.59
Batch: 380; loss: 1.54; acc: 0.47
Batch: 400; loss: 1.35; acc: 0.62
Batch: 420; loss: 1.38; acc: 0.62
Batch: 440; loss: 1.59; acc: 0.47
Batch: 460; loss: 1.41; acc: 0.62
Batch: 480; loss: 1.34; acc: 0.55
Batch: 500; loss: 1.49; acc: 0.59
Batch: 520; loss: 1.59; acc: 0.47
Batch: 540; loss: 1.53; acc: 0.55
Batch: 560; loss: 1.46; acc: 0.55
Batch: 580; loss: 1.4; acc: 0.56
Batch: 600; loss: 1.48; acc: 0.61
Batch: 620; loss: 1.41; acc: 0.64
Batch: 640; loss: 1.55; acc: 0.47
Batch: 660; loss: 1.47; acc: 0.5
Batch: 680; loss: 1.6; acc: 0.44
Batch: 700; loss: 1.43; acc: 0.59
Batch: 720; loss: 1.46; acc: 0.48
Batch: 740; loss: 1.43; acc: 0.53
Batch: 760; loss: 1.43; acc: 0.59
Batch: 780; loss: 1.39; acc: 0.55
Train Epoch over. train_loss: 1.46; train_accuracy: 0.55 

4.9782673158915713e-05
2.2155836632009596e-05
Batch: 0; loss: 1.49; acc: 0.5
Batch: 20; loss: 1.46; acc: 0.53
Batch: 40; loss: 1.1; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.66
Batch: 100; loss: 1.49; acc: 0.44
Batch: 120; loss: 1.67; acc: 0.45
Batch: 140; loss: 1.27; acc: 0.67
Val Epoch over. val_loss: 1.4210219652789413; val_accuracy: 0.5766321656050956 

The current subspace-distance is: 2.2155836632009596e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.53
Batch: 20; loss: 1.37; acc: 0.59
Batch: 40; loss: 1.41; acc: 0.58
Batch: 60; loss: 1.49; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.59
Batch: 100; loss: 1.44; acc: 0.56
Batch: 120; loss: 1.54; acc: 0.47
Batch: 140; loss: 1.45; acc: 0.47
Batch: 160; loss: 1.62; acc: 0.52
Batch: 180; loss: 1.5; acc: 0.53
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.46; acc: 0.56
Batch: 240; loss: 1.44; acc: 0.55
Batch: 260; loss: 1.52; acc: 0.53
Batch: 280; loss: 1.55; acc: 0.58
Batch: 300; loss: 1.6; acc: 0.53
Batch: 320; loss: 1.48; acc: 0.55
Batch: 340; loss: 1.43; acc: 0.56
Batch: 360; loss: 1.47; acc: 0.55
Batch: 380; loss: 1.47; acc: 0.53
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.51; acc: 0.53
Batch: 440; loss: 1.59; acc: 0.48
Batch: 460; loss: 1.2; acc: 0.69
Batch: 480; loss: 1.57; acc: 0.48
Batch: 500; loss: 1.43; acc: 0.56
Batch: 520; loss: 1.36; acc: 0.58
Batch: 540; loss: 1.37; acc: 0.61
Batch: 560; loss: 1.47; acc: 0.56
Batch: 580; loss: 1.4; acc: 0.58
Batch: 600; loss: 1.46; acc: 0.53
Batch: 620; loss: 1.58; acc: 0.53
Batch: 640; loss: 1.26; acc: 0.69
Batch: 660; loss: 1.61; acc: 0.45
Batch: 680; loss: 1.34; acc: 0.61
Batch: 700; loss: 1.62; acc: 0.52
Batch: 720; loss: 1.43; acc: 0.61
Batch: 740; loss: 1.61; acc: 0.48
Batch: 760; loss: 1.39; acc: 0.59
Batch: 780; loss: 1.64; acc: 0.56
Train Epoch over. train_loss: 1.46; train_accuracy: 0.55 

4.866577000939287e-05
2.001283246499952e-05
Batch: 0; loss: 1.48; acc: 0.5
Batch: 20; loss: 1.48; acc: 0.5
Batch: 40; loss: 1.09; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.49; acc: 0.42
Batch: 120; loss: 1.67; acc: 0.42
Batch: 140; loss: 1.3; acc: 0.62
Val Epoch over. val_loss: 1.4199108925594646; val_accuracy: 0.5733479299363057 

The current subspace-distance is: 2.001283246499952e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.66
Batch: 40; loss: 1.47; acc: 0.48
Batch: 60; loss: 1.34; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.55
Batch: 100; loss: 1.44; acc: 0.52
Batch: 120; loss: 1.34; acc: 0.64
Batch: 140; loss: 1.49; acc: 0.47
Batch: 160; loss: 1.52; acc: 0.5
Batch: 180; loss: 1.58; acc: 0.59
Batch: 200; loss: 1.5; acc: 0.59
Batch: 220; loss: 1.29; acc: 0.69
Batch: 240; loss: 1.58; acc: 0.52
Batch: 260; loss: 1.52; acc: 0.52
Batch: 280; loss: 1.61; acc: 0.42
Batch: 300; loss: 1.32; acc: 0.64
Batch: 320; loss: 1.46; acc: 0.59
Batch: 340; loss: 1.38; acc: 0.5
Batch: 360; loss: 1.28; acc: 0.62
Batch: 380; loss: 1.43; acc: 0.55
Batch: 400; loss: 1.38; acc: 0.66
Batch: 420; loss: 1.45; acc: 0.47
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.39; acc: 0.64
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.55; acc: 0.55
Batch: 520; loss: 1.4; acc: 0.61
Batch: 540; loss: 1.51; acc: 0.5
Batch: 560; loss: 1.36; acc: 0.53
Batch: 580; loss: 1.48; acc: 0.58
Batch: 600; loss: 1.47; acc: 0.52
Batch: 620; loss: 1.53; acc: 0.47
Batch: 640; loss: 1.46; acc: 0.58
Batch: 660; loss: 1.51; acc: 0.59
Batch: 680; loss: 1.43; acc: 0.55
Batch: 700; loss: 1.27; acc: 0.67
Batch: 720; loss: 1.39; acc: 0.56
Batch: 740; loss: 1.51; acc: 0.55
Batch: 760; loss: 1.62; acc: 0.39
Batch: 780; loss: 1.53; acc: 0.45
Train Epoch over. train_loss: 1.46; train_accuracy: 0.55 

4.9167501856572926e-05
2.085015512420796e-05
Batch: 0; loss: 1.49; acc: 0.5
Batch: 20; loss: 1.48; acc: 0.53
Batch: 40; loss: 1.09; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.5; acc: 0.42
Batch: 120; loss: 1.68; acc: 0.44
Batch: 140; loss: 1.29; acc: 0.67
Val Epoch over. val_loss: 1.4196854780434043; val_accuracy: 0.5688694267515924 

The current subspace-distance is: 2.085015512420796e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_3_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.25

The number of parameters is: 266027

The number of individual parameters is:

26
260
26
26
39
42588
39
39
78
127764
78
78
64
89856
64
64
4096
64
640
10
64
64

nonzero elements in E: 26602697
elements in E: 26602700
fraction nonzero: 0.9999998872294917
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.08
Batch: 20; loss: 2.24; acc: 0.14
Batch: 40; loss: 2.2; acc: 0.16
Batch: 60; loss: 2.03; acc: 0.28
Batch: 80; loss: 2.09; acc: 0.27
Batch: 100; loss: 1.98; acc: 0.31
Batch: 120; loss: 2.08; acc: 0.25
Batch: 140; loss: 2.04; acc: 0.31
Batch: 160; loss: 2.01; acc: 0.27
Batch: 180; loss: 1.98; acc: 0.34
Batch: 200; loss: 1.84; acc: 0.41
Batch: 220; loss: 1.86; acc: 0.44
Batch: 240; loss: 1.78; acc: 0.55
Batch: 260; loss: 1.78; acc: 0.48
Batch: 280; loss: 1.87; acc: 0.47
Batch: 300; loss: 1.82; acc: 0.45
Batch: 320; loss: 1.85; acc: 0.45
Batch: 340; loss: 1.81; acc: 0.45
Batch: 360; loss: 1.77; acc: 0.5
Batch: 380; loss: 1.8; acc: 0.48
Batch: 400; loss: 1.78; acc: 0.44
Batch: 420; loss: 1.79; acc: 0.47
Batch: 440; loss: 1.77; acc: 0.42
Batch: 460; loss: 1.68; acc: 0.62
Batch: 480; loss: 1.75; acc: 0.5
Batch: 500; loss: 1.73; acc: 0.55
Batch: 520; loss: 1.82; acc: 0.44
Batch: 540; loss: 1.67; acc: 0.53
Batch: 560; loss: 1.6; acc: 0.61
Batch: 580; loss: 1.65; acc: 0.52
Batch: 600; loss: 1.75; acc: 0.47
Batch: 620; loss: 1.69; acc: 0.59
Batch: 640; loss: 1.62; acc: 0.59
Batch: 660; loss: 1.61; acc: 0.67
Batch: 680; loss: 1.62; acc: 0.59
Batch: 700; loss: 1.61; acc: 0.53
Batch: 720; loss: 1.71; acc: 0.48
Batch: 740; loss: 1.66; acc: 0.55
Batch: 760; loss: 1.63; acc: 0.58
Batch: 780; loss: 1.69; acc: 0.52
Train Epoch over. train_loss: 1.82; train_accuracy: 0.45 

5.125582902110182e-05
4.612025077221915e-05
Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.42; acc: 0.7
Batch: 60; loss: 1.6; acc: 0.58
Batch: 80; loss: 1.59; acc: 0.62
Batch: 100; loss: 1.56; acc: 0.62
Batch: 120; loss: 1.69; acc: 0.45
Batch: 140; loss: 1.58; acc: 0.59
Val Epoch over. val_loss: 1.606777535122671; val_accuracy: 0.5828025477707006 

The current subspace-distance is: 4.612025077221915e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.57; acc: 0.62
Batch: 40; loss: 1.62; acc: 0.53
Batch: 60; loss: 1.62; acc: 0.52
Batch: 80; loss: 1.51; acc: 0.64
Batch: 100; loss: 1.53; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.52
Batch: 140; loss: 1.63; acc: 0.53
Batch: 160; loss: 1.59; acc: 0.61
Batch: 180; loss: 1.65; acc: 0.53
Batch: 200; loss: 1.69; acc: 0.58
Batch: 220; loss: 1.78; acc: 0.45
Batch: 240; loss: 1.57; acc: 0.66
Batch: 260; loss: 1.49; acc: 0.66
Batch: 280; loss: 1.62; acc: 0.61
Batch: 300; loss: 1.62; acc: 0.55
Batch: 320; loss: 1.56; acc: 0.59
Batch: 340; loss: 1.6; acc: 0.53
Batch: 360; loss: 1.55; acc: 0.61
Batch: 380; loss: 1.51; acc: 0.61
Batch: 400; loss: 1.52; acc: 0.61
Batch: 420; loss: 1.57; acc: 0.64
Batch: 440; loss: 1.63; acc: 0.56
Batch: 460; loss: 1.43; acc: 0.72
Batch: 480; loss: 1.6; acc: 0.58
Batch: 500; loss: 1.54; acc: 0.56
Batch: 520; loss: 1.48; acc: 0.69
Batch: 540; loss: 1.57; acc: 0.61
Batch: 560; loss: 1.48; acc: 0.67
Batch: 580; loss: 1.46; acc: 0.64
Batch: 600; loss: 1.42; acc: 0.61
Batch: 620; loss: 1.45; acc: 0.62
Batch: 640; loss: 1.51; acc: 0.61
Batch: 660; loss: 1.49; acc: 0.59
Batch: 680; loss: 1.71; acc: 0.53
Batch: 700; loss: 1.48; acc: 0.59
Batch: 720; loss: 1.65; acc: 0.47
Batch: 740; loss: 1.47; acc: 0.61
Batch: 760; loss: 1.6; acc: 0.52
Batch: 780; loss: 1.47; acc: 0.69
Train Epoch over. train_loss: 1.56; train_accuracy: 0.59 

6.971975381020457e-05
6.320897955447435e-05
Batch: 0; loss: 1.55; acc: 0.52
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.3; acc: 0.75
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.5; acc: 0.64
Batch: 100; loss: 1.39; acc: 0.69
Batch: 120; loss: 1.59; acc: 0.52
Batch: 140; loss: 1.45; acc: 0.7
Val Epoch over. val_loss: 1.477321977827959; val_accuracy: 0.6388335987261147 

The current subspace-distance is: 6.320897955447435e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.53
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.54; acc: 0.56
Batch: 60; loss: 1.51; acc: 0.66
Batch: 80; loss: 1.45; acc: 0.69
Batch: 100; loss: 1.55; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.69
Batch: 140; loss: 1.51; acc: 0.59
Batch: 160; loss: 1.46; acc: 0.64
Batch: 180; loss: 1.36; acc: 0.73
Batch: 200; loss: 1.46; acc: 0.66
Batch: 220; loss: 1.5; acc: 0.64
Batch: 240; loss: 1.48; acc: 0.64
Batch: 260; loss: 1.63; acc: 0.56
Batch: 280; loss: 1.58; acc: 0.52
Batch: 300; loss: 1.47; acc: 0.67
Batch: 320; loss: 1.51; acc: 0.61
Batch: 340; loss: 1.45; acc: 0.67
Batch: 360; loss: 1.56; acc: 0.61
Batch: 380; loss: 1.47; acc: 0.69
Batch: 400; loss: 1.38; acc: 0.8
Batch: 420; loss: 1.5; acc: 0.66
Batch: 440; loss: 1.49; acc: 0.59
Batch: 460; loss: 1.44; acc: 0.64
Batch: 480; loss: 1.5; acc: 0.66
Batch: 500; loss: 1.47; acc: 0.67
Batch: 520; loss: 1.59; acc: 0.58
Batch: 540; loss: 1.56; acc: 0.58
Batch: 560; loss: 1.42; acc: 0.62
Batch: 580; loss: 1.5; acc: 0.64
Batch: 600; loss: 1.54; acc: 0.59
Batch: 620; loss: 1.37; acc: 0.64
Batch: 640; loss: 1.48; acc: 0.61
Batch: 660; loss: 1.45; acc: 0.64
Batch: 680; loss: 1.44; acc: 0.75
Batch: 700; loss: 1.63; acc: 0.52
Batch: 720; loss: 1.44; acc: 0.69
Batch: 740; loss: 1.48; acc: 0.66
Batch: 760; loss: 1.5; acc: 0.69
Batch: 780; loss: 1.46; acc: 0.66
Train Epoch over. train_loss: 1.49; train_accuracy: 0.63 

8.036998042371124e-05
7.544852269347757e-05
Batch: 0; loss: 1.5; acc: 0.55
Batch: 20; loss: 1.56; acc: 0.56
Batch: 40; loss: 1.27; acc: 0.75
Batch: 60; loss: 1.41; acc: 0.64
Batch: 80; loss: 1.43; acc: 0.66
Batch: 100; loss: 1.33; acc: 0.78
Batch: 120; loss: 1.54; acc: 0.62
Batch: 140; loss: 1.4; acc: 0.7
Val Epoch over. val_loss: 1.427978618129803; val_accuracy: 0.6670979299363057 

The current subspace-distance is: 7.544852269347757e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.5; acc: 0.67
Batch: 20; loss: 1.45; acc: 0.7
Batch: 40; loss: 1.42; acc: 0.72
Batch: 60; loss: 1.43; acc: 0.66
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.5; acc: 0.59
Batch: 120; loss: 1.38; acc: 0.67
Batch: 140; loss: 1.44; acc: 0.66
Batch: 160; loss: 1.69; acc: 0.48
Batch: 180; loss: 1.41; acc: 0.69
Batch: 200; loss: 1.45; acc: 0.67
Batch: 220; loss: 1.46; acc: 0.66
Batch: 240; loss: 1.51; acc: 0.55
Batch: 260; loss: 1.56; acc: 0.56
Batch: 280; loss: 1.38; acc: 0.75
Batch: 300; loss: 1.55; acc: 0.58
Batch: 320; loss: 1.49; acc: 0.66
Batch: 340; loss: 1.39; acc: 0.69
Batch: 360; loss: 1.41; acc: 0.67
Batch: 380; loss: 1.46; acc: 0.72
Batch: 400; loss: 1.55; acc: 0.55
Batch: 420; loss: 1.35; acc: 0.72
Batch: 440; loss: 1.4; acc: 0.66
Batch: 460; loss: 1.44; acc: 0.66
Batch: 480; loss: 1.47; acc: 0.58
Batch: 500; loss: 1.48; acc: 0.61
Batch: 520; loss: 1.35; acc: 0.75
Batch: 540; loss: 1.4; acc: 0.69
Batch: 560; loss: 1.54; acc: 0.58
Batch: 580; loss: 1.46; acc: 0.66
Batch: 600; loss: 1.47; acc: 0.61
Batch: 620; loss: 1.65; acc: 0.48
Batch: 640; loss: 1.38; acc: 0.72
Batch: 660; loss: 1.42; acc: 0.62
Batch: 680; loss: 1.3; acc: 0.77
Batch: 700; loss: 1.45; acc: 0.7
Batch: 720; loss: 1.52; acc: 0.56
Batch: 740; loss: 1.48; acc: 0.61
Batch: 760; loss: 1.48; acc: 0.62
Batch: 780; loss: 1.5; acc: 0.64
Train Epoch over. train_loss: 1.44; train_accuracy: 0.65 

9.077772119781002e-05
8.680918108439073e-05
Batch: 0; loss: 1.43; acc: 0.73
Batch: 20; loss: 1.55; acc: 0.62
Batch: 40; loss: 1.21; acc: 0.8
Batch: 60; loss: 1.37; acc: 0.66
Batch: 80; loss: 1.35; acc: 0.67
Batch: 100; loss: 1.32; acc: 0.78
Batch: 120; loss: 1.5; acc: 0.64
Batch: 140; loss: 1.35; acc: 0.73
Val Epoch over. val_loss: 1.380263267808659; val_accuracy: 0.693172770700637 

The current subspace-distance is: 8.680918108439073e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.64
Batch: 20; loss: 1.22; acc: 0.72
Batch: 40; loss: 1.39; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.69
Batch: 80; loss: 1.39; acc: 0.58
Batch: 100; loss: 1.46; acc: 0.72
Batch: 120; loss: 1.42; acc: 0.67
Batch: 140; loss: 1.41; acc: 0.73
Batch: 160; loss: 1.42; acc: 0.7
Batch: 180; loss: 1.52; acc: 0.59
Batch: 200; loss: 1.42; acc: 0.67
Batch: 220; loss: 1.42; acc: 0.66
Batch: 240; loss: 1.48; acc: 0.61
Batch: 260; loss: 1.39; acc: 0.69
Batch: 280; loss: 1.4; acc: 0.67
Batch: 300; loss: 1.38; acc: 0.61
Batch: 320; loss: 1.31; acc: 0.67
Batch: 340; loss: 1.52; acc: 0.61
Batch: 360; loss: 1.36; acc: 0.66
Batch: 380; loss: 1.45; acc: 0.59
Batch: 400; loss: 1.44; acc: 0.66
Batch: 420; loss: 1.49; acc: 0.59
Batch: 440; loss: 1.3; acc: 0.77
Batch: 460; loss: 1.45; acc: 0.62
Batch: 480; loss: 1.22; acc: 0.69
Batch: 500; loss: 1.59; acc: 0.59
Batch: 520; loss: 1.37; acc: 0.69
Batch: 540; loss: 1.31; acc: 0.67
Batch: 560; loss: 1.32; acc: 0.73
Batch: 580; loss: 1.3; acc: 0.73
Batch: 600; loss: 1.43; acc: 0.62
Batch: 620; loss: 1.38; acc: 0.69
Batch: 640; loss: 1.55; acc: 0.58
Batch: 660; loss: 1.44; acc: 0.62
Batch: 680; loss: 1.44; acc: 0.64
Batch: 700; loss: 1.32; acc: 0.72
Batch: 720; loss: 1.29; acc: 0.72
Batch: 740; loss: 1.28; acc: 0.75
Batch: 760; loss: 1.2; acc: 0.78
Batch: 780; loss: 1.38; acc: 0.62
Train Epoch over. train_loss: 1.38; train_accuracy: 0.67 

0.00010084882524097338
9.578953904565424e-05
Batch: 0; loss: 1.3; acc: 0.78
Batch: 20; loss: 1.51; acc: 0.56
Batch: 40; loss: 1.07; acc: 0.84
Batch: 60; loss: 1.29; acc: 0.7
Batch: 80; loss: 1.29; acc: 0.73
Batch: 100; loss: 1.25; acc: 0.78
Batch: 120; loss: 1.41; acc: 0.67
Batch: 140; loss: 1.25; acc: 0.72
Val Epoch over. val_loss: 1.3042067076749861; val_accuracy: 0.7062101910828026 

The current subspace-distance is: 9.578953904565424e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.64
Batch: 20; loss: 1.46; acc: 0.58
Batch: 40; loss: 1.33; acc: 0.77
Batch: 60; loss: 1.48; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.64
Batch: 100; loss: 1.46; acc: 0.62
Batch: 120; loss: 1.39; acc: 0.67
Batch: 140; loss: 1.19; acc: 0.8
Batch: 160; loss: 1.28; acc: 0.75
Batch: 180; loss: 1.4; acc: 0.59
Batch: 200; loss: 1.41; acc: 0.62
Batch: 220; loss: 1.49; acc: 0.55
Batch: 240; loss: 1.39; acc: 0.66
Batch: 260; loss: 1.34; acc: 0.66
Batch: 280; loss: 1.36; acc: 0.67
Batch: 300; loss: 1.32; acc: 0.67
Batch: 320; loss: 1.34; acc: 0.62
Batch: 340; loss: 1.22; acc: 0.7
Batch: 360; loss: 1.47; acc: 0.58
Batch: 380; loss: 1.31; acc: 0.72
Batch: 400; loss: 1.33; acc: 0.66
Batch: 420; loss: 1.27; acc: 0.73
Batch: 440; loss: 1.35; acc: 0.67
Batch: 460; loss: 1.3; acc: 0.66
Batch: 480; loss: 1.29; acc: 0.69
Batch: 500; loss: 1.34; acc: 0.73
Batch: 520; loss: 1.32; acc: 0.72
Batch: 540; loss: 1.23; acc: 0.75
Batch: 560; loss: 1.29; acc: 0.7
Batch: 580; loss: 1.26; acc: 0.77
Batch: 600; loss: 1.4; acc: 0.66
Batch: 620; loss: 1.27; acc: 0.67
Batch: 640; loss: 1.32; acc: 0.69
Batch: 660; loss: 1.28; acc: 0.73
Batch: 680; loss: 1.21; acc: 0.75
Batch: 700; loss: 1.3; acc: 0.62
Batch: 720; loss: 1.46; acc: 0.56
Batch: 740; loss: 1.23; acc: 0.73
Batch: 760; loss: 1.37; acc: 0.61
Batch: 780; loss: 1.22; acc: 0.7
Train Epoch over. train_loss: 1.33; train_accuracy: 0.68 

0.00011035516217816621
0.00010517652845010161
Batch: 0; loss: 1.24; acc: 0.77
Batch: 20; loss: 1.44; acc: 0.62
Batch: 40; loss: 0.99; acc: 0.89
Batch: 60; loss: 1.23; acc: 0.73
Batch: 80; loss: 1.24; acc: 0.77
Batch: 100; loss: 1.21; acc: 0.72
Batch: 120; loss: 1.38; acc: 0.69
Batch: 140; loss: 1.2; acc: 0.7
Val Epoch over. val_loss: 1.2556106922732797; val_accuracy: 0.71984474522293 

The current subspace-distance is: 0.00010517652845010161 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.77
Batch: 20; loss: 1.24; acc: 0.73
Batch: 40; loss: 1.36; acc: 0.59
Batch: 60; loss: 1.27; acc: 0.72
Batch: 80; loss: 1.28; acc: 0.69
Batch: 100; loss: 1.31; acc: 0.56
Batch: 120; loss: 1.38; acc: 0.69
Batch: 140; loss: 1.3; acc: 0.67
Batch: 160; loss: 1.22; acc: 0.72
Batch: 180; loss: 1.2; acc: 0.77
Batch: 200; loss: 1.39; acc: 0.72
Batch: 220; loss: 1.34; acc: 0.62
Batch: 240; loss: 1.35; acc: 0.72
Batch: 260; loss: 1.39; acc: 0.66
Batch: 280; loss: 1.33; acc: 0.64
Batch: 300; loss: 1.33; acc: 0.73
Batch: 320; loss: 1.49; acc: 0.53
Batch: 340; loss: 1.28; acc: 0.59
Batch: 360; loss: 1.35; acc: 0.64
Batch: 380; loss: 1.18; acc: 0.72
Batch: 400; loss: 1.32; acc: 0.61
Batch: 420; loss: 1.32; acc: 0.66
Batch: 440; loss: 1.33; acc: 0.59
Batch: 460; loss: 1.42; acc: 0.64
Batch: 480; loss: 1.35; acc: 0.62
Batch: 500; loss: 1.42; acc: 0.59
Batch: 520; loss: 1.2; acc: 0.78
Batch: 540; loss: 1.21; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.72
Batch: 580; loss: 1.39; acc: 0.64
Batch: 600; loss: 1.33; acc: 0.72
Batch: 620; loss: 1.24; acc: 0.69
Batch: 640; loss: 1.47; acc: 0.62
Batch: 660; loss: 1.29; acc: 0.59
Batch: 680; loss: 1.2; acc: 0.77
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 1.17; acc: 0.75
Batch: 740; loss: 1.32; acc: 0.67
Batch: 760; loss: 1.15; acc: 0.8
Batch: 780; loss: 1.4; acc: 0.56
Train Epoch over. train_loss: 1.28; train_accuracy: 0.69 

0.00012021674046991393
0.00011463573173386976
Batch: 0; loss: 1.18; acc: 0.7
Batch: 20; loss: 1.39; acc: 0.62
Batch: 40; loss: 0.96; acc: 0.89
Batch: 60; loss: 1.2; acc: 0.73
Batch: 80; loss: 1.2; acc: 0.78
Batch: 100; loss: 1.17; acc: 0.78
Batch: 120; loss: 1.36; acc: 0.67
Batch: 140; loss: 1.19; acc: 0.78
Val Epoch over. val_loss: 1.2207535607799602; val_accuracy: 0.7216361464968153 

The current subspace-distance is: 0.00011463573173386976 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.72
Batch: 20; loss: 1.17; acc: 0.72
Batch: 40; loss: 1.3; acc: 0.67
Batch: 60; loss: 1.26; acc: 0.7
Batch: 80; loss: 1.24; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.8
Batch: 120; loss: 1.33; acc: 0.64
Batch: 140; loss: 1.37; acc: 0.67
Batch: 160; loss: 1.26; acc: 0.66
Batch: 180; loss: 1.16; acc: 0.81
Batch: 200; loss: 1.23; acc: 0.67
Batch: 220; loss: 1.2; acc: 0.77
Batch: 240; loss: 1.16; acc: 0.75
Batch: 260; loss: 1.16; acc: 0.72
Batch: 280; loss: 1.17; acc: 0.72
Batch: 300; loss: 1.21; acc: 0.73
Batch: 320; loss: 1.25; acc: 0.69
Batch: 340; loss: 1.33; acc: 0.67
Batch: 360; loss: 1.24; acc: 0.7
Batch: 380; loss: 1.3; acc: 0.64
Batch: 400; loss: 1.21; acc: 0.78
Batch: 420; loss: 1.21; acc: 0.73
Batch: 440; loss: 1.28; acc: 0.75
Batch: 460; loss: 1.29; acc: 0.77
Batch: 480; loss: 1.22; acc: 0.7
Batch: 500; loss: 1.16; acc: 0.72
Batch: 520; loss: 1.16; acc: 0.72
Batch: 540; loss: 1.02; acc: 0.83
Batch: 560; loss: 1.09; acc: 0.78
Batch: 580; loss: 1.26; acc: 0.66
Batch: 600; loss: 1.23; acc: 0.77
Batch: 620; loss: 1.23; acc: 0.73
Batch: 640; loss: 1.28; acc: 0.73
Batch: 660; loss: 1.28; acc: 0.7
Batch: 680; loss: 1.16; acc: 0.75
Batch: 700; loss: 1.24; acc: 0.62
Batch: 720; loss: 1.23; acc: 0.69
Batch: 740; loss: 1.2; acc: 0.75
Batch: 760; loss: 1.31; acc: 0.59
Batch: 780; loss: 1.27; acc: 0.73
Train Epoch over. train_loss: 1.24; train_accuracy: 0.7 

0.00012863118899986148
0.00012271640298422426
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 0.94; acc: 0.86
Batch: 60; loss: 1.13; acc: 0.77
Batch: 80; loss: 1.18; acc: 0.8
Batch: 100; loss: 1.13; acc: 0.8
Batch: 120; loss: 1.33; acc: 0.66
Batch: 140; loss: 1.15; acc: 0.75
Val Epoch over. val_loss: 1.186416131675623; val_accuracy: 0.7300955414012739 

The current subspace-distance is: 0.00012271640298422426 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.66
Batch: 20; loss: 1.2; acc: 0.73
Batch: 40; loss: 1.17; acc: 0.73
Batch: 60; loss: 1.13; acc: 0.8
Batch: 80; loss: 1.3; acc: 0.66
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.36; acc: 0.66
Batch: 140; loss: 1.2; acc: 0.73
Batch: 160; loss: 1.19; acc: 0.75
Batch: 180; loss: 1.27; acc: 0.59
Batch: 200; loss: 1.2; acc: 0.7
Batch: 220; loss: 1.17; acc: 0.7
Batch: 240; loss: 1.17; acc: 0.73
Batch: 260; loss: 1.3; acc: 0.62
Batch: 280; loss: 1.22; acc: 0.61
Batch: 300; loss: 1.28; acc: 0.67
Batch: 320; loss: 1.17; acc: 0.75
Batch: 340; loss: 1.13; acc: 0.73
Batch: 360; loss: 1.2; acc: 0.7
Batch: 380; loss: 1.33; acc: 0.67
Batch: 400; loss: 1.32; acc: 0.69
Batch: 420; loss: 1.21; acc: 0.75
Batch: 440; loss: 1.16; acc: 0.78
Batch: 460; loss: 1.13; acc: 0.73
Batch: 480; loss: 1.26; acc: 0.72
Batch: 500; loss: 1.34; acc: 0.62
Batch: 520; loss: 1.17; acc: 0.77
Batch: 540; loss: 1.21; acc: 0.72
Batch: 560; loss: 1.16; acc: 0.77
Batch: 580; loss: 1.35; acc: 0.66
Batch: 600; loss: 1.16; acc: 0.72
Batch: 620; loss: 1.11; acc: 0.8
Batch: 640; loss: 1.1; acc: 0.73
Batch: 660; loss: 1.16; acc: 0.73
Batch: 680; loss: 1.32; acc: 0.64
Batch: 700; loss: 1.12; acc: 0.73
Batch: 720; loss: 1.08; acc: 0.83
Batch: 740; loss: 1.08; acc: 0.81
Batch: 760; loss: 1.27; acc: 0.67
Batch: 780; loss: 1.26; acc: 0.75
Train Epoch over. train_loss: 1.21; train_accuracy: 0.7 

0.0001367638324154541
0.0001287163613596931
Batch: 0; loss: 1.09; acc: 0.75
Batch: 20; loss: 1.27; acc: 0.7
Batch: 40; loss: 0.93; acc: 0.86
Batch: 60; loss: 1.08; acc: 0.75
Batch: 80; loss: 1.14; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.8
Batch: 120; loss: 1.31; acc: 0.67
Batch: 140; loss: 1.1; acc: 0.73
Val Epoch over. val_loss: 1.1516918148964075; val_accuracy: 0.7392515923566879 

The current subspace-distance is: 0.0001287163613596931 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.78
Batch: 20; loss: 1.31; acc: 0.61
Batch: 40; loss: 1.14; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.72
Batch: 100; loss: 1.01; acc: 0.84
Batch: 120; loss: 1.3; acc: 0.69
Batch: 140; loss: 1.36; acc: 0.62
Batch: 160; loss: 1.13; acc: 0.73
Batch: 180; loss: 1.05; acc: 0.77
Batch: 200; loss: 1.15; acc: 0.73
Batch: 220; loss: 1.32; acc: 0.66
Batch: 240; loss: 1.13; acc: 0.73
Batch: 260; loss: 1.21; acc: 0.61
Batch: 280; loss: 1.18; acc: 0.72
Batch: 300; loss: 1.17; acc: 0.77
Batch: 320; loss: 1.25; acc: 0.69
Batch: 340; loss: 1.13; acc: 0.7
Batch: 360; loss: 1.12; acc: 0.75
Batch: 380; loss: 1.29; acc: 0.64
Batch: 400; loss: 1.23; acc: 0.67
Batch: 420; loss: 1.19; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.67
Batch: 460; loss: 1.07; acc: 0.77
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 1.14; acc: 0.75
Batch: 520; loss: 1.17; acc: 0.77
Batch: 540; loss: 1.16; acc: 0.66
Batch: 560; loss: 1.2; acc: 0.7
Batch: 580; loss: 1.34; acc: 0.66
Batch: 600; loss: 1.44; acc: 0.56
Batch: 620; loss: 1.08; acc: 0.77
Batch: 640; loss: 1.24; acc: 0.69
Batch: 660; loss: 1.22; acc: 0.59
Batch: 680; loss: 1.24; acc: 0.69
Batch: 700; loss: 1.22; acc: 0.67
Batch: 720; loss: 1.12; acc: 0.67
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.73
Batch: 780; loss: 1.38; acc: 0.59
Train Epoch over. train_loss: 1.2; train_accuracy: 0.7 

0.00014024425763636827
0.00013420262257568538
Batch: 0; loss: 1.07; acc: 0.75
Batch: 20; loss: 1.27; acc: 0.7
Batch: 40; loss: 0.91; acc: 0.84
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 1.11; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.77
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 1.06; acc: 0.75
Val Epoch over. val_loss: 1.129428611059857; val_accuracy: 0.726015127388535 

The current subspace-distance is: 0.00013420262257568538 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.15; acc: 0.69
Batch: 20; loss: 1.17; acc: 0.75
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 1.02; acc: 0.86
Batch: 80; loss: 1.31; acc: 0.59
Batch: 100; loss: 1.17; acc: 0.72
Batch: 120; loss: 1.24; acc: 0.7
Batch: 140; loss: 1.08; acc: 0.78
Batch: 160; loss: 1.2; acc: 0.67
Batch: 180; loss: 1.23; acc: 0.69
Batch: 200; loss: 1.09; acc: 0.78
Batch: 220; loss: 1.03; acc: 0.86
Batch: 240; loss: 1.18; acc: 0.72
Batch: 260; loss: 1.28; acc: 0.64
Batch: 280; loss: 1.07; acc: 0.8
Batch: 300; loss: 1.09; acc: 0.72
Batch: 320; loss: 1.17; acc: 0.73
Batch: 340; loss: 1.12; acc: 0.69
Batch: 360; loss: 1.21; acc: 0.62
Batch: 380; loss: 1.15; acc: 0.66
Batch: 400; loss: 1.1; acc: 0.72
Batch: 420; loss: 1.18; acc: 0.75
Batch: 440; loss: 1.28; acc: 0.64
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.19; acc: 0.7
Batch: 500; loss: 1.21; acc: 0.67
Batch: 520; loss: 1.06; acc: 0.75
Batch: 540; loss: 1.24; acc: 0.62
Batch: 560; loss: 1.22; acc: 0.67
Batch: 580; loss: 0.99; acc: 0.81
Batch: 600; loss: 1.27; acc: 0.66
Batch: 620; loss: 1.17; acc: 0.77
Batch: 640; loss: 1.05; acc: 0.78
Batch: 660; loss: 1.2; acc: 0.64
Batch: 680; loss: 1.23; acc: 0.72
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.32; acc: 0.67
Batch: 740; loss: 1.19; acc: 0.64
Batch: 760; loss: 1.04; acc: 0.8
Batch: 780; loss: 1.04; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.7 

0.00014301465125754476
0.00013558338105212897
Batch: 0; loss: 1.05; acc: 0.73
Batch: 20; loss: 1.25; acc: 0.7
Batch: 40; loss: 0.89; acc: 0.86
Batch: 60; loss: 1.02; acc: 0.78
Batch: 80; loss: 1.1; acc: 0.78
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.27; acc: 0.62
Batch: 140; loss: 1.04; acc: 0.78
Val Epoch over. val_loss: 1.1190068740753611; val_accuracy: 0.727906050955414 

The current subspace-distance is: 0.00013558338105212897 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 1.08; acc: 0.72
Batch: 60; loss: 1.14; acc: 0.75
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 1.23; acc: 0.59
Batch: 160; loss: 1.06; acc: 0.77
Batch: 180; loss: 1.1; acc: 0.73
Batch: 200; loss: 1.23; acc: 0.67
Batch: 220; loss: 1.23; acc: 0.69
Batch: 240; loss: 1.3; acc: 0.64
Batch: 260; loss: 1.12; acc: 0.72
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.2; acc: 0.62
Batch: 320; loss: 1.16; acc: 0.69
Batch: 340; loss: 1.11; acc: 0.7
Batch: 360; loss: 1.06; acc: 0.73
Batch: 380; loss: 1.08; acc: 0.73
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.25; acc: 0.7
Batch: 440; loss: 1.23; acc: 0.69
Batch: 460; loss: 1.17; acc: 0.66
Batch: 480; loss: 1.19; acc: 0.7
Batch: 500; loss: 1.27; acc: 0.56
Batch: 520; loss: 1.26; acc: 0.67
Batch: 540; loss: 1.15; acc: 0.72
Batch: 560; loss: 1.0; acc: 0.83
Batch: 580; loss: 1.3; acc: 0.62
Batch: 600; loss: 1.18; acc: 0.72
Batch: 620; loss: 1.1; acc: 0.8
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.7
Batch: 680; loss: 1.29; acc: 0.61
Batch: 700; loss: 1.29; acc: 0.64
Batch: 720; loss: 1.25; acc: 0.64
Batch: 740; loss: 1.13; acc: 0.78
Batch: 760; loss: 1.2; acc: 0.75
Batch: 780; loss: 1.0; acc: 0.75
Train Epoch over. train_loss: 1.17; train_accuracy: 0.7 

0.0001451128046028316
0.00014040074893273413
Batch: 0; loss: 1.06; acc: 0.78
Batch: 20; loss: 1.22; acc: 0.7
Batch: 40; loss: 0.9; acc: 0.86
Batch: 60; loss: 1.03; acc: 0.77
Batch: 80; loss: 1.11; acc: 0.77
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.26; acc: 0.59
Batch: 140; loss: 1.03; acc: 0.77
Val Epoch over. val_loss: 1.1139033201393809; val_accuracy: 0.7331807324840764 

The current subspace-distance is: 0.00014040074893273413 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.72
Batch: 40; loss: 1.22; acc: 0.75
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.75
Batch: 100; loss: 1.07; acc: 0.81
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.67
Batch: 160; loss: 1.2; acc: 0.67
Batch: 180; loss: 1.27; acc: 0.64
Batch: 200; loss: 1.23; acc: 0.61
Batch: 220; loss: 1.13; acc: 0.69
Batch: 240; loss: 1.06; acc: 0.8
Batch: 260; loss: 1.16; acc: 0.69
Batch: 280; loss: 1.04; acc: 0.7
Batch: 300; loss: 1.05; acc: 0.77
Batch: 320; loss: 1.07; acc: 0.72
Batch: 340; loss: 1.1; acc: 0.7
Batch: 360; loss: 1.11; acc: 0.8
Batch: 380; loss: 1.15; acc: 0.72
Batch: 400; loss: 1.22; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.67
Batch: 440; loss: 1.07; acc: 0.77
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.22; acc: 0.7
Batch: 500; loss: 1.22; acc: 0.62
Batch: 520; loss: 1.19; acc: 0.72
Batch: 540; loss: 1.24; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 1.02; acc: 0.78
Batch: 620; loss: 1.07; acc: 0.75
Batch: 640; loss: 1.32; acc: 0.66
Batch: 660; loss: 1.06; acc: 0.77
Batch: 680; loss: 1.09; acc: 0.66
Batch: 700; loss: 1.42; acc: 0.58
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 1.33; acc: 0.61
Batch: 760; loss: 1.2; acc: 0.69
Batch: 780; loss: 1.15; acc: 0.77
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.00014376406033989042
0.0001397965825162828
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 1.22; acc: 0.69
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 1.11; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.61
Batch: 140; loss: 1.02; acc: 0.8
Val Epoch over. val_loss: 1.1167691862507232; val_accuracy: 0.7227308917197452 

The current subspace-distance is: 0.0001397965825162828 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 1.09; acc: 0.77
Batch: 40; loss: 1.09; acc: 0.69
Batch: 60; loss: 1.09; acc: 0.69
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.17; acc: 0.67
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 1.03; acc: 0.8
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 1.18; acc: 0.67
Batch: 200; loss: 1.25; acc: 0.62
Batch: 220; loss: 1.03; acc: 0.73
Batch: 240; loss: 1.29; acc: 0.61
Batch: 260; loss: 1.22; acc: 0.69
Batch: 280; loss: 1.18; acc: 0.72
Batch: 300; loss: 1.12; acc: 0.7
Batch: 320; loss: 1.21; acc: 0.67
Batch: 340; loss: 1.04; acc: 0.81
Batch: 360; loss: 1.15; acc: 0.73
Batch: 380; loss: 1.32; acc: 0.67
Batch: 400; loss: 1.17; acc: 0.66
Batch: 420; loss: 1.05; acc: 0.81
Batch: 440; loss: 1.07; acc: 0.77
Batch: 460; loss: 1.25; acc: 0.64
Batch: 480; loss: 1.29; acc: 0.59
Batch: 500; loss: 1.16; acc: 0.69
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 0.98; acc: 0.73
Batch: 560; loss: 0.97; acc: 0.81
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.14; acc: 0.69
Batch: 620; loss: 1.15; acc: 0.67
Batch: 640; loss: 1.31; acc: 0.67
Batch: 660; loss: 1.05; acc: 0.77
Batch: 680; loss: 1.15; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.66
Batch: 720; loss: 1.21; acc: 0.64
Batch: 740; loss: 1.16; acc: 0.66
Batch: 760; loss: 1.1; acc: 0.73
Batch: 780; loss: 1.18; acc: 0.69
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.00015155451546888798
0.00014357107284013182
Batch: 0; loss: 1.03; acc: 0.78
Batch: 20; loss: 1.19; acc: 0.69
Batch: 40; loss: 0.87; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.77
Batch: 80; loss: 1.1; acc: 0.78
Batch: 100; loss: 1.05; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.0; acc: 0.78
Val Epoch over. val_loss: 1.1011100035564156; val_accuracy: 0.7335788216560509 

The current subspace-distance is: 0.00014357107284013182 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.21; acc: 0.7
Batch: 20; loss: 1.06; acc: 0.7
Batch: 40; loss: 1.32; acc: 0.61
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 0.98; acc: 0.81
Batch: 100; loss: 1.01; acc: 0.73
Batch: 120; loss: 1.05; acc: 0.78
Batch: 140; loss: 1.0; acc: 0.81
Batch: 160; loss: 1.17; acc: 0.7
Batch: 180; loss: 1.15; acc: 0.69
Batch: 200; loss: 1.32; acc: 0.52
Batch: 220; loss: 1.08; acc: 0.69
Batch: 240; loss: 1.24; acc: 0.64
Batch: 260; loss: 1.15; acc: 0.66
Batch: 280; loss: 1.22; acc: 0.66
Batch: 300; loss: 1.14; acc: 0.77
Batch: 320; loss: 1.09; acc: 0.73
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 1.23; acc: 0.64
Batch: 380; loss: 1.08; acc: 0.73
Batch: 400; loss: 1.1; acc: 0.67
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.3; acc: 0.64
Batch: 460; loss: 1.11; acc: 0.73
Batch: 480; loss: 0.96; acc: 0.77
Batch: 500; loss: 1.15; acc: 0.77
Batch: 520; loss: 1.34; acc: 0.58
Batch: 540; loss: 1.16; acc: 0.75
Batch: 560; loss: 1.03; acc: 0.75
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.11; acc: 0.81
Batch: 620; loss: 1.16; acc: 0.67
Batch: 640; loss: 1.29; acc: 0.66
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.16; acc: 0.67
Batch: 700; loss: 0.9; acc: 0.81
Batch: 720; loss: 1.2; acc: 0.69
Batch: 740; loss: 1.13; acc: 0.64
Batch: 760; loss: 1.26; acc: 0.62
Batch: 780; loss: 1.18; acc: 0.69
Train Epoch over. train_loss: 1.15; train_accuracy: 0.7 

0.0001504153769928962
0.00014371282304637134
Batch: 0; loss: 1.03; acc: 0.78
Batch: 20; loss: 1.19; acc: 0.69
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.77
Batch: 80; loss: 1.1; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.01; acc: 0.75
Val Epoch over. val_loss: 1.1026311224433267; val_accuracy: 0.7275079617834395 

The current subspace-distance is: 0.00014371282304637134 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.04; acc: 0.8
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.21; acc: 0.59
Batch: 100; loss: 1.17; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.59
Batch: 140; loss: 1.24; acc: 0.59
Batch: 160; loss: 1.08; acc: 0.75
Batch: 180; loss: 1.04; acc: 0.77
Batch: 200; loss: 1.16; acc: 0.7
Batch: 220; loss: 1.2; acc: 0.69
Batch: 240; loss: 1.1; acc: 0.75
Batch: 260; loss: 1.24; acc: 0.66
Batch: 280; loss: 1.21; acc: 0.69
Batch: 300; loss: 1.29; acc: 0.72
Batch: 320; loss: 0.99; acc: 0.77
Batch: 340; loss: 1.13; acc: 0.72
Batch: 360; loss: 1.17; acc: 0.67
Batch: 380; loss: 1.09; acc: 0.78
Batch: 400; loss: 1.24; acc: 0.56
Batch: 420; loss: 1.0; acc: 0.77
Batch: 440; loss: 1.1; acc: 0.7
Batch: 460; loss: 1.17; acc: 0.69
Batch: 480; loss: 1.08; acc: 0.77
Batch: 500; loss: 1.13; acc: 0.69
Batch: 520; loss: 1.02; acc: 0.73
Batch: 540; loss: 1.19; acc: 0.69
Batch: 560; loss: 1.02; acc: 0.77
Batch: 580; loss: 1.12; acc: 0.69
Batch: 600; loss: 1.19; acc: 0.67
Batch: 620; loss: 1.11; acc: 0.75
Batch: 640; loss: 1.28; acc: 0.59
Batch: 660; loss: 1.11; acc: 0.75
Batch: 680; loss: 1.1; acc: 0.77
Batch: 700; loss: 1.25; acc: 0.62
Batch: 720; loss: 1.0; acc: 0.77
Batch: 740; loss: 1.08; acc: 0.75
Batch: 760; loss: 1.32; acc: 0.66
Batch: 780; loss: 1.34; acc: 0.61
Train Epoch over. train_loss: 1.14; train_accuracy: 0.7 

0.0001558762596687302
0.00014800069038756192
Batch: 0; loss: 1.02; acc: 0.77
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 1.09; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 0.99; acc: 0.77
Val Epoch over. val_loss: 1.0913482976567215; val_accuracy: 0.7325835987261147 

The current subspace-distance is: 0.00014800069038756192 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.32; acc: 0.56
Batch: 40; loss: 1.21; acc: 0.75
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.24; acc: 0.59
Batch: 120; loss: 1.15; acc: 0.72
Batch: 140; loss: 1.18; acc: 0.64
Batch: 160; loss: 1.03; acc: 0.75
Batch: 180; loss: 1.0; acc: 0.81
Batch: 200; loss: 1.11; acc: 0.72
Batch: 220; loss: 1.1; acc: 0.73
Batch: 240; loss: 1.19; acc: 0.69
Batch: 260; loss: 1.12; acc: 0.77
Batch: 280; loss: 0.95; acc: 0.8
Batch: 300; loss: 1.2; acc: 0.69
Batch: 320; loss: 1.22; acc: 0.66
Batch: 340; loss: 1.24; acc: 0.67
Batch: 360; loss: 1.04; acc: 0.77
Batch: 380; loss: 1.0; acc: 0.77
Batch: 400; loss: 0.97; acc: 0.78
Batch: 420; loss: 1.06; acc: 0.72
Batch: 440; loss: 1.22; acc: 0.67
Batch: 460; loss: 1.08; acc: 0.77
Batch: 480; loss: 1.12; acc: 0.7
Batch: 500; loss: 1.09; acc: 0.72
Batch: 520; loss: 1.23; acc: 0.67
Batch: 540; loss: 1.06; acc: 0.72
Batch: 560; loss: 1.11; acc: 0.77
Batch: 580; loss: 1.04; acc: 0.78
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.12; acc: 0.67
Batch: 660; loss: 1.17; acc: 0.66
Batch: 680; loss: 1.15; acc: 0.73
Batch: 700; loss: 0.97; acc: 0.8
Batch: 720; loss: 1.25; acc: 0.61
Batch: 740; loss: 1.21; acc: 0.66
Batch: 760; loss: 1.11; acc: 0.75
Batch: 780; loss: 1.15; acc: 0.66
Train Epoch over. train_loss: 1.14; train_accuracy: 0.7 

0.000153905144543387
0.00014755972370039672
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 0.88; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.77
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.78
Val Epoch over. val_loss: 1.0898683200216597; val_accuracy: 0.7383558917197452 

The current subspace-distance is: 0.00014755972370039672 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.72
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 1.13; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.73
Batch: 80; loss: 1.13; acc: 0.64
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.03; acc: 0.8
Batch: 140; loss: 1.13; acc: 0.77
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 1.03; acc: 0.72
Batch: 200; loss: 1.1; acc: 0.7
Batch: 220; loss: 1.04; acc: 0.73
Batch: 240; loss: 1.14; acc: 0.69
Batch: 260; loss: 1.29; acc: 0.66
Batch: 280; loss: 1.15; acc: 0.7
Batch: 300; loss: 1.21; acc: 0.69
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 1.19; acc: 0.69
Batch: 360; loss: 1.26; acc: 0.62
Batch: 380; loss: 1.3; acc: 0.59
Batch: 400; loss: 1.01; acc: 0.7
Batch: 420; loss: 1.25; acc: 0.64
Batch: 440; loss: 1.1; acc: 0.69
Batch: 460; loss: 1.21; acc: 0.69
Batch: 480; loss: 1.14; acc: 0.62
Batch: 500; loss: 1.24; acc: 0.64
Batch: 520; loss: 1.24; acc: 0.66
Batch: 540; loss: 1.07; acc: 0.78
Batch: 560; loss: 1.1; acc: 0.67
Batch: 580; loss: 0.91; acc: 0.81
Batch: 600; loss: 1.25; acc: 0.66
Batch: 620; loss: 1.14; acc: 0.7
Batch: 640; loss: 1.1; acc: 0.75
Batch: 660; loss: 1.16; acc: 0.67
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 1.09; acc: 0.77
Batch: 740; loss: 1.1; acc: 0.66
Batch: 760; loss: 0.95; acc: 0.81
Batch: 780; loss: 1.08; acc: 0.77
Train Epoch over. train_loss: 1.13; train_accuracy: 0.7 

0.00015736529894638807
0.00014975889644119889
Batch: 0; loss: 1.01; acc: 0.77
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.0783768554402005; val_accuracy: 0.7347730891719745 

The current subspace-distance is: 0.00014975889644119889 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.69
Batch: 20; loss: 1.07; acc: 0.8
Batch: 40; loss: 1.22; acc: 0.62
Batch: 60; loss: 1.14; acc: 0.67
Batch: 80; loss: 1.14; acc: 0.73
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 1.26; acc: 0.61
Batch: 160; loss: 1.1; acc: 0.7
Batch: 180; loss: 1.06; acc: 0.75
Batch: 200; loss: 1.08; acc: 0.78
Batch: 220; loss: 1.13; acc: 0.67
Batch: 240; loss: 0.97; acc: 0.75
Batch: 260; loss: 1.06; acc: 0.72
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.05; acc: 0.67
Batch: 320; loss: 1.23; acc: 0.67
Batch: 340; loss: 1.1; acc: 0.75
Batch: 360; loss: 1.06; acc: 0.69
Batch: 380; loss: 1.18; acc: 0.7
Batch: 400; loss: 0.99; acc: 0.73
Batch: 420; loss: 1.05; acc: 0.8
Batch: 440; loss: 1.07; acc: 0.78
Batch: 460; loss: 1.16; acc: 0.72
Batch: 480; loss: 1.12; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.7
Batch: 520; loss: 1.1; acc: 0.77
Batch: 540; loss: 0.96; acc: 0.72
Batch: 560; loss: 1.02; acc: 0.69
Batch: 580; loss: 1.07; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.69
Batch: 620; loss: 1.09; acc: 0.72
Batch: 640; loss: 1.12; acc: 0.73
Batch: 660; loss: 0.98; acc: 0.77
Batch: 680; loss: 1.13; acc: 0.75
Batch: 700; loss: 1.22; acc: 0.69
Batch: 720; loss: 1.03; acc: 0.69
Batch: 740; loss: 1.09; acc: 0.69
Batch: 760; loss: 0.96; acc: 0.8
Batch: 780; loss: 1.15; acc: 0.72
Train Epoch over. train_loss: 1.13; train_accuracy: 0.7 

0.00016194955969695002
0.00015327981964219362
Batch: 0; loss: 1.01; acc: 0.78
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.85; acc: 0.77
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.07; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.73
Batch: 120; loss: 1.18; acc: 0.62
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.0683665928567292; val_accuracy: 0.7337778662420382 

The current subspace-distance is: 0.00015327981964219362 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.05; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 1.1; acc: 0.72
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 0.91; acc: 0.83
Batch: 140; loss: 1.11; acc: 0.77
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 1.24; acc: 0.62
Batch: 200; loss: 0.92; acc: 0.77
Batch: 220; loss: 1.27; acc: 0.59
Batch: 240; loss: 1.3; acc: 0.69
Batch: 260; loss: 1.08; acc: 0.75
Batch: 280; loss: 1.12; acc: 0.73
Batch: 300; loss: 1.08; acc: 0.7
Batch: 320; loss: 1.12; acc: 0.73
Batch: 340; loss: 1.15; acc: 0.77
Batch: 360; loss: 1.39; acc: 0.58
Batch: 380; loss: 1.16; acc: 0.61
Batch: 400; loss: 1.11; acc: 0.69
Batch: 420; loss: 1.07; acc: 0.67
Batch: 440; loss: 1.17; acc: 0.72
Batch: 460; loss: 1.07; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.77
Batch: 500; loss: 1.05; acc: 0.73
Batch: 520; loss: 1.22; acc: 0.66
Batch: 540; loss: 1.25; acc: 0.55
Batch: 560; loss: 1.23; acc: 0.61
Batch: 580; loss: 1.08; acc: 0.7
Batch: 600; loss: 1.02; acc: 0.69
Batch: 620; loss: 1.02; acc: 0.75
Batch: 640; loss: 1.2; acc: 0.7
Batch: 660; loss: 1.16; acc: 0.75
Batch: 680; loss: 0.99; acc: 0.77
Batch: 700; loss: 1.23; acc: 0.64
Batch: 720; loss: 1.14; acc: 0.67
Batch: 740; loss: 1.25; acc: 0.61
Batch: 760; loss: 1.23; acc: 0.64
Batch: 780; loss: 1.07; acc: 0.75
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.0001634809304960072
0.00015544655616395175
Batch: 0; loss: 0.98; acc: 0.83
Batch: 20; loss: 1.13; acc: 0.67
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 0.98; acc: 0.77
Batch: 120; loss: 1.15; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.81
Val Epoch over. val_loss: 1.0531852921103215; val_accuracy: 0.7409434713375797 

The current subspace-distance is: 0.00015544655616395175 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.95; acc: 0.84
Batch: 20; loss: 1.1; acc: 0.72
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.14; acc: 0.75
Batch: 140; loss: 1.13; acc: 0.67
Batch: 160; loss: 1.13; acc: 0.75
Batch: 180; loss: 0.99; acc: 0.75
Batch: 200; loss: 1.18; acc: 0.66
Batch: 220; loss: 1.05; acc: 0.7
Batch: 240; loss: 1.15; acc: 0.64
Batch: 260; loss: 1.25; acc: 0.59
Batch: 280; loss: 1.01; acc: 0.78
Batch: 300; loss: 1.17; acc: 0.75
Batch: 320; loss: 1.04; acc: 0.73
Batch: 340; loss: 1.13; acc: 0.67
Batch: 360; loss: 1.14; acc: 0.73
Batch: 380; loss: 1.1; acc: 0.67
Batch: 400; loss: 1.13; acc: 0.7
Batch: 420; loss: 0.98; acc: 0.75
Batch: 440; loss: 1.02; acc: 0.73
Batch: 460; loss: 1.23; acc: 0.58
Batch: 480; loss: 1.07; acc: 0.72
Batch: 500; loss: 1.23; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.78
Batch: 540; loss: 1.3; acc: 0.56
Batch: 560; loss: 1.23; acc: 0.62
Batch: 580; loss: 1.18; acc: 0.72
Batch: 600; loss: 1.19; acc: 0.7
Batch: 620; loss: 1.23; acc: 0.7
Batch: 640; loss: 1.1; acc: 0.78
Batch: 660; loss: 1.13; acc: 0.69
Batch: 680; loss: 1.08; acc: 0.73
Batch: 700; loss: 0.96; acc: 0.75
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 1.19; acc: 0.62
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.00015892795636318624
0.00015378068201243877
Batch: 0; loss: 1.0; acc: 0.77
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.16; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.0642894487472097; val_accuracy: 0.732484076433121 

The current subspace-distance is: 0.00015378068201243877 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.78
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 1.22; acc: 0.61
Batch: 80; loss: 1.1; acc: 0.72
Batch: 100; loss: 1.18; acc: 0.66
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 1.21; acc: 0.69
Batch: 160; loss: 1.03; acc: 0.73
Batch: 180; loss: 1.13; acc: 0.7
Batch: 200; loss: 1.17; acc: 0.7
Batch: 220; loss: 0.99; acc: 0.77
Batch: 240; loss: 1.17; acc: 0.69
Batch: 260; loss: 1.15; acc: 0.75
Batch: 280; loss: 1.1; acc: 0.73
Batch: 300; loss: 1.08; acc: 0.75
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 1.11; acc: 0.73
Batch: 360; loss: 1.37; acc: 0.67
Batch: 380; loss: 0.97; acc: 0.73
Batch: 400; loss: 0.98; acc: 0.72
Batch: 420; loss: 1.27; acc: 0.58
Batch: 440; loss: 1.09; acc: 0.72
Batch: 460; loss: 1.18; acc: 0.61
Batch: 480; loss: 1.35; acc: 0.61
Batch: 500; loss: 0.9; acc: 0.86
Batch: 520; loss: 1.11; acc: 0.75
Batch: 540; loss: 1.26; acc: 0.62
Batch: 560; loss: 1.15; acc: 0.66
Batch: 580; loss: 1.04; acc: 0.72
Batch: 600; loss: 1.14; acc: 0.7
Batch: 620; loss: 1.02; acc: 0.75
Batch: 640; loss: 0.94; acc: 0.83
Batch: 660; loss: 1.06; acc: 0.64
Batch: 680; loss: 1.16; acc: 0.7
Batch: 700; loss: 1.21; acc: 0.66
Batch: 720; loss: 1.14; acc: 0.64
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.17; acc: 0.67
Batch: 780; loss: 1.19; acc: 0.64
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.00015987103688530624
0.00015349780733231455
Batch: 0; loss: 1.0; acc: 0.77
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 0.95; acc: 0.78
Val Epoch over. val_loss: 1.0661945540434237; val_accuracy: 0.7276074840764332 

The current subspace-distance is: 0.00015349780733231455 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.72
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 1.11; acc: 0.7
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.44; acc: 0.56
Batch: 100; loss: 1.19; acc: 0.67
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 0.99; acc: 0.81
Batch: 160; loss: 1.13; acc: 0.75
Batch: 180; loss: 1.13; acc: 0.75
Batch: 200; loss: 0.97; acc: 0.8
Batch: 220; loss: 1.12; acc: 0.75
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.38; acc: 0.55
Batch: 280; loss: 1.01; acc: 0.78
Batch: 300; loss: 1.05; acc: 0.72
Batch: 320; loss: 1.21; acc: 0.66
Batch: 340; loss: 1.02; acc: 0.81
Batch: 360; loss: 1.07; acc: 0.75
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 1.07; acc: 0.78
Batch: 420; loss: 0.95; acc: 0.73
Batch: 440; loss: 1.08; acc: 0.78
Batch: 460; loss: 0.92; acc: 0.77
Batch: 480; loss: 1.33; acc: 0.61
Batch: 500; loss: 1.14; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.72
Batch: 540; loss: 1.21; acc: 0.73
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 0.99; acc: 0.72
Batch: 600; loss: 1.04; acc: 0.72
Batch: 620; loss: 0.98; acc: 0.77
Batch: 640; loss: 1.24; acc: 0.61
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.03; acc: 0.72
Batch: 700; loss: 1.08; acc: 0.72
Batch: 720; loss: 1.06; acc: 0.75
Batch: 740; loss: 1.18; acc: 0.69
Batch: 760; loss: 1.21; acc: 0.62
Batch: 780; loss: 1.25; acc: 0.66
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.0001604249409865588
0.0001545004779472947
Batch: 0; loss: 0.99; acc: 0.78
Batch: 20; loss: 1.13; acc: 0.67
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.77
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 1.15; acc: 0.67
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.054773225146494; val_accuracy: 0.7401472929936306 

The current subspace-distance is: 0.0001545004779472947 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 1.12; acc: 0.69
Batch: 40; loss: 1.07; acc: 0.75
Batch: 60; loss: 1.12; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.59
Batch: 100; loss: 1.12; acc: 0.77
Batch: 120; loss: 1.09; acc: 0.69
Batch: 140; loss: 1.19; acc: 0.67
Batch: 160; loss: 1.38; acc: 0.61
Batch: 180; loss: 1.22; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.73
Batch: 220; loss: 1.25; acc: 0.62
Batch: 240; loss: 1.02; acc: 0.83
Batch: 260; loss: 1.2; acc: 0.73
Batch: 280; loss: 0.98; acc: 0.73
Batch: 300; loss: 0.98; acc: 0.77
Batch: 320; loss: 1.12; acc: 0.66
Batch: 340; loss: 1.14; acc: 0.62
Batch: 360; loss: 1.21; acc: 0.67
Batch: 380; loss: 1.05; acc: 0.72
Batch: 400; loss: 1.09; acc: 0.69
Batch: 420; loss: 1.25; acc: 0.58
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 0.99; acc: 0.78
Batch: 480; loss: 1.11; acc: 0.7
Batch: 500; loss: 1.15; acc: 0.61
Batch: 520; loss: 1.16; acc: 0.66
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 1.08; acc: 0.75
Batch: 600; loss: 1.04; acc: 0.69
Batch: 620; loss: 0.97; acc: 0.81
Batch: 640; loss: 1.13; acc: 0.72
Batch: 660; loss: 1.24; acc: 0.67
Batch: 680; loss: 1.05; acc: 0.72
Batch: 700; loss: 1.16; acc: 0.7
Batch: 720; loss: 1.14; acc: 0.73
Batch: 740; loss: 1.15; acc: 0.69
Batch: 760; loss: 1.1; acc: 0.66
Batch: 780; loss: 1.22; acc: 0.59
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.0001610507461009547
0.00015703229291830212
Batch: 0; loss: 1.0; acc: 0.77
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 0.95; acc: 0.78
Val Epoch over. val_loss: 1.061374367422359; val_accuracy: 0.7301950636942676 

The current subspace-distance is: 0.00015703229291830212 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 1.07; acc: 0.8
Batch: 40; loss: 1.4; acc: 0.59
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 1.0; acc: 0.77
Batch: 160; loss: 1.19; acc: 0.69
Batch: 180; loss: 1.05; acc: 0.73
Batch: 200; loss: 1.1; acc: 0.73
Batch: 220; loss: 1.16; acc: 0.7
Batch: 240; loss: 1.36; acc: 0.59
Batch: 260; loss: 1.06; acc: 0.75
Batch: 280; loss: 1.08; acc: 0.67
Batch: 300; loss: 0.97; acc: 0.75
Batch: 320; loss: 1.21; acc: 0.67
Batch: 340; loss: 1.13; acc: 0.67
Batch: 360; loss: 1.16; acc: 0.59
Batch: 380; loss: 1.0; acc: 0.75
Batch: 400; loss: 1.05; acc: 0.81
Batch: 420; loss: 1.14; acc: 0.73
Batch: 440; loss: 0.98; acc: 0.8
Batch: 460; loss: 1.17; acc: 0.72
Batch: 480; loss: 1.45; acc: 0.53
Batch: 500; loss: 1.31; acc: 0.56
Batch: 520; loss: 0.96; acc: 0.77
Batch: 540; loss: 1.08; acc: 0.67
Batch: 560; loss: 1.18; acc: 0.66
Batch: 580; loss: 1.08; acc: 0.75
Batch: 600; loss: 1.14; acc: 0.69
Batch: 620; loss: 0.98; acc: 0.77
Batch: 640; loss: 1.26; acc: 0.62
Batch: 660; loss: 1.04; acc: 0.8
Batch: 680; loss: 1.13; acc: 0.72
Batch: 700; loss: 1.02; acc: 0.7
Batch: 720; loss: 1.13; acc: 0.73
Batch: 740; loss: 1.06; acc: 0.67
Batch: 760; loss: 1.2; acc: 0.72
Batch: 780; loss: 1.08; acc: 0.77
Train Epoch over. train_loss: 1.11; train_accuracy: 0.71 

0.000164491924806498
0.0001552732865093276
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.81
Val Epoch over. val_loss: 1.0553642742952722; val_accuracy: 0.7405453821656051 

The current subspace-distance is: 0.0001552732865093276 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 1.19; acc: 0.67
Batch: 60; loss: 0.98; acc: 0.83
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 1.08; acc: 0.72
Batch: 160; loss: 1.23; acc: 0.61
Batch: 180; loss: 0.98; acc: 0.7
Batch: 200; loss: 1.13; acc: 0.73
Batch: 220; loss: 1.16; acc: 0.67
Batch: 240; loss: 1.06; acc: 0.77
Batch: 260; loss: 1.3; acc: 0.59
Batch: 280; loss: 1.12; acc: 0.72
Batch: 300; loss: 1.19; acc: 0.69
Batch: 320; loss: 1.18; acc: 0.66
Batch: 340; loss: 1.1; acc: 0.75
Batch: 360; loss: 1.18; acc: 0.59
Batch: 380; loss: 1.11; acc: 0.64
Batch: 400; loss: 1.25; acc: 0.62
Batch: 420; loss: 1.14; acc: 0.64
Batch: 440; loss: 1.21; acc: 0.69
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 1.06; acc: 0.73
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.13; acc: 0.67
Batch: 540; loss: 1.13; acc: 0.66
Batch: 560; loss: 1.06; acc: 0.73
Batch: 580; loss: 1.09; acc: 0.73
Batch: 600; loss: 1.2; acc: 0.58
Batch: 620; loss: 1.29; acc: 0.64
Batch: 640; loss: 1.2; acc: 0.67
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.3; acc: 0.67
Batch: 700; loss: 1.13; acc: 0.72
Batch: 720; loss: 0.95; acc: 0.78
Batch: 740; loss: 1.24; acc: 0.69
Batch: 760; loss: 1.17; acc: 0.61
Batch: 780; loss: 1.12; acc: 0.72
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00016386348579544574
0.00015617893950548023
Batch: 0; loss: 0.98; acc: 0.81
Batch: 20; loss: 1.13; acc: 0.66
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 0.99; acc: 0.77
Batch: 120; loss: 1.15; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.0526683960750605; val_accuracy: 0.7359673566878981 

The current subspace-distance is: 0.00015617893950548023 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.75
Batch: 20; loss: 1.02; acc: 0.69
Batch: 40; loss: 1.22; acc: 0.61
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 0.98; acc: 0.78
Batch: 140; loss: 1.08; acc: 0.75
Batch: 160; loss: 1.42; acc: 0.59
Batch: 180; loss: 0.97; acc: 0.77
Batch: 200; loss: 1.16; acc: 0.72
Batch: 220; loss: 1.24; acc: 0.67
Batch: 240; loss: 1.32; acc: 0.53
Batch: 260; loss: 0.95; acc: 0.8
Batch: 280; loss: 1.07; acc: 0.77
Batch: 300; loss: 1.02; acc: 0.72
Batch: 320; loss: 1.09; acc: 0.72
Batch: 340; loss: 1.06; acc: 0.66
Batch: 360; loss: 1.25; acc: 0.62
Batch: 380; loss: 0.95; acc: 0.83
Batch: 400; loss: 1.13; acc: 0.67
Batch: 420; loss: 1.07; acc: 0.73
Batch: 440; loss: 0.92; acc: 0.8
Batch: 460; loss: 1.08; acc: 0.69
Batch: 480; loss: 1.19; acc: 0.64
Batch: 500; loss: 1.08; acc: 0.73
Batch: 520; loss: 1.23; acc: 0.69
Batch: 540; loss: 1.38; acc: 0.61
Batch: 560; loss: 1.16; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.67
Batch: 600; loss: 1.0; acc: 0.8
Batch: 620; loss: 1.22; acc: 0.7
Batch: 640; loss: 1.18; acc: 0.62
Batch: 660; loss: 1.12; acc: 0.75
Batch: 680; loss: 1.06; acc: 0.66
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.66
Batch: 740; loss: 0.99; acc: 0.81
Batch: 760; loss: 0.94; acc: 0.83
Batch: 780; loss: 1.08; acc: 0.75
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00016729744675103575
0.00016041322669479996
Batch: 0; loss: 0.98; acc: 0.8
Batch: 20; loss: 1.13; acc: 0.66
Batch: 40; loss: 0.84; acc: 0.8
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 1.0544720341445535; val_accuracy: 0.7347730891719745 

The current subspace-distance is: 0.00016041322669479996 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.07; acc: 0.77
Batch: 20; loss: 1.07; acc: 0.77
Batch: 40; loss: 0.86; acc: 0.81
Batch: 60; loss: 1.12; acc: 0.64
Batch: 80; loss: 1.26; acc: 0.64
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 1.16; acc: 0.56
Batch: 160; loss: 1.33; acc: 0.58
Batch: 180; loss: 1.1; acc: 0.73
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 1.36; acc: 0.59
Batch: 240; loss: 1.07; acc: 0.67
Batch: 260; loss: 1.08; acc: 0.73
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.14; acc: 0.67
Batch: 320; loss: 1.07; acc: 0.72
Batch: 340; loss: 1.33; acc: 0.56
Batch: 360; loss: 1.2; acc: 0.7
Batch: 380; loss: 1.0; acc: 0.72
Batch: 400; loss: 1.19; acc: 0.73
Batch: 420; loss: 1.21; acc: 0.66
Batch: 440; loss: 1.01; acc: 0.77
Batch: 460; loss: 0.96; acc: 0.75
Batch: 480; loss: 1.01; acc: 0.75
Batch: 500; loss: 1.1; acc: 0.75
Batch: 520; loss: 1.12; acc: 0.72
Batch: 540; loss: 1.05; acc: 0.72
Batch: 560; loss: 1.24; acc: 0.55
Batch: 580; loss: 1.21; acc: 0.66
Batch: 600; loss: 0.95; acc: 0.75
Batch: 620; loss: 1.05; acc: 0.8
Batch: 640; loss: 1.25; acc: 0.69
Batch: 660; loss: 1.12; acc: 0.64
Batch: 680; loss: 1.01; acc: 0.72
Batch: 700; loss: 0.97; acc: 0.77
Batch: 720; loss: 1.22; acc: 0.64
Batch: 740; loss: 0.95; acc: 0.75
Batch: 760; loss: 1.1; acc: 0.75
Batch: 780; loss: 1.08; acc: 0.75
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00016737460100557655
0.00015808336320333183
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 1.13; acc: 0.66
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 1.07; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.0604506085632712; val_accuracy: 0.728702229299363 

The current subspace-distance is: 0.00015808336320333183 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.34; acc: 0.56
Batch: 20; loss: 1.0; acc: 0.78
Batch: 40; loss: 0.97; acc: 0.77
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 1.0; acc: 0.75
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.03; acc: 0.69
Batch: 140; loss: 1.15; acc: 0.75
Batch: 160; loss: 1.09; acc: 0.72
Batch: 180; loss: 1.2; acc: 0.66
Batch: 200; loss: 0.97; acc: 0.73
Batch: 220; loss: 1.04; acc: 0.72
Batch: 240; loss: 1.24; acc: 0.67
Batch: 260; loss: 1.04; acc: 0.67
Batch: 280; loss: 1.15; acc: 0.67
Batch: 300; loss: 1.07; acc: 0.7
Batch: 320; loss: 1.12; acc: 0.72
Batch: 340; loss: 1.13; acc: 0.72
Batch: 360; loss: 1.27; acc: 0.62
Batch: 380; loss: 1.11; acc: 0.66
Batch: 400; loss: 1.19; acc: 0.7
Batch: 420; loss: 1.17; acc: 0.73
Batch: 440; loss: 1.15; acc: 0.7
Batch: 460; loss: 1.08; acc: 0.7
Batch: 480; loss: 1.11; acc: 0.73
Batch: 500; loss: 1.18; acc: 0.66
Batch: 520; loss: 1.06; acc: 0.73
Batch: 540; loss: 1.2; acc: 0.66
Batch: 560; loss: 1.18; acc: 0.66
Batch: 580; loss: 1.15; acc: 0.69
Batch: 600; loss: 1.0; acc: 0.72
Batch: 620; loss: 1.17; acc: 0.69
Batch: 640; loss: 1.13; acc: 0.72
Batch: 660; loss: 1.2; acc: 0.66
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 1.15; acc: 0.72
Batch: 720; loss: 1.03; acc: 0.78
Batch: 740; loss: 0.94; acc: 0.78
Batch: 760; loss: 1.39; acc: 0.61
Batch: 780; loss: 1.01; acc: 0.78
Train Epoch over. train_loss: 1.11; train_accuracy: 0.71 

0.0001667189208092168
0.00015979487216100097
Batch: 0; loss: 0.98; acc: 0.81
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.96; acc: 0.73
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 0.98; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 0.91; acc: 0.81
Val Epoch over. val_loss: 1.0436322977588435; val_accuracy: 0.7409434713375797 

The current subspace-distance is: 0.00015979487216100097 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 1.09; acc: 0.7
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 1.23; acc: 0.66
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.07; acc: 0.75
Batch: 140; loss: 1.25; acc: 0.59
Batch: 160; loss: 1.15; acc: 0.64
Batch: 180; loss: 1.2; acc: 0.67
Batch: 200; loss: 0.99; acc: 0.8
Batch: 220; loss: 0.91; acc: 0.84
Batch: 240; loss: 1.05; acc: 0.78
Batch: 260; loss: 1.15; acc: 0.66
Batch: 280; loss: 1.06; acc: 0.72
Batch: 300; loss: 1.1; acc: 0.72
Batch: 320; loss: 0.99; acc: 0.73
Batch: 340; loss: 1.17; acc: 0.69
Batch: 360; loss: 1.22; acc: 0.62
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 1.13; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.12; acc: 0.66
Batch: 460; loss: 1.06; acc: 0.72
Batch: 480; loss: 1.09; acc: 0.67
Batch: 500; loss: 1.26; acc: 0.58
Batch: 520; loss: 0.95; acc: 0.77
Batch: 540; loss: 1.01; acc: 0.8
Batch: 560; loss: 1.11; acc: 0.73
Batch: 580; loss: 1.0; acc: 0.75
Batch: 600; loss: 1.09; acc: 0.73
Batch: 620; loss: 1.12; acc: 0.69
Batch: 640; loss: 1.19; acc: 0.67
Batch: 660; loss: 1.1; acc: 0.67
Batch: 680; loss: 0.83; acc: 0.83
Batch: 700; loss: 0.97; acc: 0.8
Batch: 720; loss: 1.06; acc: 0.73
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 1.06; acc: 0.75
Batch: 780; loss: 1.02; acc: 0.69
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00016695560771040618
0.00015880305727478117
Batch: 0; loss: 0.98; acc: 0.81
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 0.84; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 0.98; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 0.92; acc: 0.78
Val Epoch over. val_loss: 1.0453223749330849; val_accuracy: 0.7390525477707006 

The current subspace-distance is: 0.00015880305727478117 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_3_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.25

The number of parameters is: 266027

The number of individual parameters is:

26
260
26
26
39
42588
39
39
78
127764
78
78
64
89856
64
64
4096
64
640
10
64
64

nonzero elements in E: 53205395
elements in E: 53205400
fraction nonzero: 0.9999999060245764
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.25; acc: 0.16
Batch: 20; loss: 2.08; acc: 0.25
Batch: 40; loss: 1.95; acc: 0.44
Batch: 60; loss: 1.93; acc: 0.53
Batch: 80; loss: 1.83; acc: 0.53
Batch: 100; loss: 1.76; acc: 0.53
Batch: 120; loss: 1.83; acc: 0.58
Batch: 140; loss: 1.73; acc: 0.61
Batch: 160; loss: 1.57; acc: 0.69
Batch: 180; loss: 1.71; acc: 0.56
Batch: 200; loss: 1.66; acc: 0.58
Batch: 220; loss: 1.5; acc: 0.66
Batch: 240; loss: 1.55; acc: 0.67
Batch: 260; loss: 1.5; acc: 0.72
Batch: 280; loss: 1.5; acc: 0.73
Batch: 300; loss: 1.5; acc: 0.67
Batch: 320; loss: 1.59; acc: 0.72
Batch: 340; loss: 1.67; acc: 0.55
Batch: 360; loss: 1.6; acc: 0.62
Batch: 380; loss: 1.45; acc: 0.77
Batch: 400; loss: 1.54; acc: 0.66
Batch: 420; loss: 1.48; acc: 0.72
Batch: 440; loss: 1.52; acc: 0.67
Batch: 460; loss: 1.42; acc: 0.69
Batch: 480; loss: 1.45; acc: 0.73
Batch: 500; loss: 1.3; acc: 0.84
Batch: 520; loss: 1.32; acc: 0.8
Batch: 540; loss: 1.43; acc: 0.73
Batch: 560; loss: 1.47; acc: 0.69
Batch: 580; loss: 1.43; acc: 0.73
Batch: 600; loss: 1.39; acc: 0.7
Batch: 620; loss: 1.43; acc: 0.73
Batch: 640; loss: 1.46; acc: 0.75
Batch: 660; loss: 1.4; acc: 0.77
Batch: 680; loss: 1.5; acc: 0.59
Batch: 700; loss: 1.25; acc: 0.86
Batch: 720; loss: 1.35; acc: 0.8
Batch: 740; loss: 1.47; acc: 0.62
Batch: 760; loss: 1.32; acc: 0.75
Batch: 780; loss: 1.41; acc: 0.69
Train Epoch over. train_loss: 1.55; train_accuracy: 0.65 

5.393243918661028e-05
4.803708361578174e-05
Batch: 0; loss: 1.36; acc: 0.73
Batch: 20; loss: 1.52; acc: 0.61
Batch: 40; loss: 1.11; acc: 0.84
Batch: 60; loss: 1.34; acc: 0.72
Batch: 80; loss: 1.31; acc: 0.83
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.42; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.8
Val Epoch over. val_loss: 1.3274723488813753; val_accuracy: 0.7475119426751592 

The current subspace-distance is: 4.803708361578174e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.72
Batch: 20; loss: 1.35; acc: 0.73
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.23; acc: 0.81
Batch: 80; loss: 1.27; acc: 0.8
Batch: 100; loss: 1.34; acc: 0.7
Batch: 120; loss: 1.38; acc: 0.67
Batch: 140; loss: 1.23; acc: 0.83
Batch: 160; loss: 1.32; acc: 0.77
Batch: 180; loss: 1.24; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.73
Batch: 220; loss: 1.44; acc: 0.66
Batch: 240; loss: 1.34; acc: 0.64
Batch: 260; loss: 1.34; acc: 0.7
Batch: 280; loss: 1.32; acc: 0.72
Batch: 300; loss: 1.25; acc: 0.73
Batch: 320; loss: 1.33; acc: 0.72
Batch: 340; loss: 1.28; acc: 0.72
Batch: 360; loss: 1.28; acc: 0.72
Batch: 380; loss: 1.29; acc: 0.77
Batch: 400; loss: 1.23; acc: 0.73
Batch: 420; loss: 1.24; acc: 0.78
Batch: 440; loss: 1.27; acc: 0.7
Batch: 460; loss: 1.25; acc: 0.75
Batch: 480; loss: 1.27; acc: 0.81
Batch: 500; loss: 1.21; acc: 0.78
Batch: 520; loss: 1.34; acc: 0.64
Batch: 540; loss: 1.23; acc: 0.67
Batch: 560; loss: 1.21; acc: 0.77
Batch: 580; loss: 1.08; acc: 0.83
Batch: 600; loss: 1.2; acc: 0.77
Batch: 620; loss: 1.22; acc: 0.72
Batch: 640; loss: 1.13; acc: 0.88
Batch: 660; loss: 1.19; acc: 0.75
Batch: 680; loss: 1.3; acc: 0.66
Batch: 700; loss: 1.18; acc: 0.73
Batch: 720; loss: 1.19; acc: 0.75
Batch: 740; loss: 1.2; acc: 0.75
Batch: 760; loss: 1.15; acc: 0.77
Batch: 780; loss: 1.18; acc: 0.78
Train Epoch over. train_loss: 1.27; train_accuracy: 0.73 

7.331185770453885e-05
6.86718849465251e-05
Batch: 0; loss: 1.19; acc: 0.73
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 0.95; acc: 0.81
Batch: 60; loss: 1.09; acc: 0.8
Batch: 80; loss: 1.13; acc: 0.8
Batch: 100; loss: 1.2; acc: 0.8
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 1.02; acc: 0.86
Val Epoch over. val_loss: 1.168910291164544; val_accuracy: 0.7684116242038217 

The current subspace-distance is: 6.86718849465251e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.81
Batch: 20; loss: 1.16; acc: 0.75
Batch: 40; loss: 1.36; acc: 0.69
Batch: 60; loss: 1.17; acc: 0.81
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.23; acc: 0.69
Batch: 120; loss: 1.21; acc: 0.73
Batch: 140; loss: 1.1; acc: 0.81
Batch: 160; loss: 1.1; acc: 0.75
Batch: 180; loss: 1.26; acc: 0.67
Batch: 200; loss: 1.22; acc: 0.73
Batch: 220; loss: 1.21; acc: 0.66
Batch: 240; loss: 1.05; acc: 0.81
Batch: 260; loss: 1.09; acc: 0.75
Batch: 280; loss: 1.21; acc: 0.72
Batch: 300; loss: 1.06; acc: 0.83
Batch: 320; loss: 1.2; acc: 0.66
Batch: 340; loss: 1.09; acc: 0.84
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.15; acc: 0.77
Batch: 400; loss: 1.05; acc: 0.8
Batch: 420; loss: 1.31; acc: 0.67
Batch: 440; loss: 1.1; acc: 0.73
Batch: 460; loss: 1.06; acc: 0.83
Batch: 480; loss: 1.18; acc: 0.77
Batch: 500; loss: 1.12; acc: 0.78
Batch: 520; loss: 1.1; acc: 0.81
Batch: 540; loss: 1.04; acc: 0.77
Batch: 560; loss: 1.23; acc: 0.69
Batch: 580; loss: 1.07; acc: 0.8
Batch: 600; loss: 1.17; acc: 0.7
Batch: 620; loss: 1.16; acc: 0.69
Batch: 640; loss: 1.06; acc: 0.77
Batch: 660; loss: 1.21; acc: 0.69
Batch: 680; loss: 1.13; acc: 0.73
Batch: 700; loss: 0.99; acc: 0.83
Batch: 720; loss: 1.12; acc: 0.77
Batch: 740; loss: 1.08; acc: 0.77
Batch: 760; loss: 1.05; acc: 0.8
Batch: 780; loss: 1.14; acc: 0.73
Train Epoch over. train_loss: 1.13; train_accuracy: 0.76 

9.143375791609287e-05
8.697815064806491e-05
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.28; acc: 0.66
Batch: 40; loss: 0.8; acc: 0.86
Batch: 60; loss: 0.89; acc: 0.86
Batch: 80; loss: 0.96; acc: 0.83
Batch: 100; loss: 1.05; acc: 0.81
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 0.87; acc: 0.89
Val Epoch over. val_loss: 1.0217057166585497; val_accuracy: 0.8019506369426752 

The current subspace-distance is: 8.697815064806491e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.75
Batch: 20; loss: 1.06; acc: 0.78
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 1.05; acc: 0.81
Batch: 100; loss: 0.99; acc: 0.81
Batch: 120; loss: 1.01; acc: 0.81
Batch: 140; loss: 1.13; acc: 0.78
Batch: 160; loss: 0.87; acc: 0.91
Batch: 180; loss: 1.17; acc: 0.7
Batch: 200; loss: 1.05; acc: 0.78
Batch: 220; loss: 0.98; acc: 0.8
Batch: 240; loss: 0.95; acc: 0.89
Batch: 260; loss: 0.99; acc: 0.77
Batch: 280; loss: 0.97; acc: 0.81
Batch: 300; loss: 1.05; acc: 0.77
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.03; acc: 0.78
Batch: 360; loss: 1.08; acc: 0.75
Batch: 380; loss: 1.08; acc: 0.77
Batch: 400; loss: 1.02; acc: 0.81
Batch: 420; loss: 0.94; acc: 0.83
Batch: 440; loss: 1.02; acc: 0.8
Batch: 460; loss: 1.17; acc: 0.73
Batch: 480; loss: 0.89; acc: 0.89
Batch: 500; loss: 1.06; acc: 0.77
Batch: 520; loss: 1.05; acc: 0.75
Batch: 540; loss: 0.97; acc: 0.78
Batch: 560; loss: 0.95; acc: 0.83
Batch: 580; loss: 1.02; acc: 0.8
Batch: 600; loss: 0.96; acc: 0.8
Batch: 620; loss: 0.92; acc: 0.83
Batch: 640; loss: 0.92; acc: 0.84
Batch: 660; loss: 0.9; acc: 0.81
Batch: 680; loss: 0.99; acc: 0.81
Batch: 700; loss: 1.17; acc: 0.69
Batch: 720; loss: 1.25; acc: 0.69
Batch: 740; loss: 1.1; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.91
Batch: 780; loss: 1.0; acc: 0.81
Train Epoch over. train_loss: 1.04; train_accuracy: 0.78 

0.00010768815991468728
0.0001023370714392513
Batch: 0; loss: 0.93; acc: 0.86
Batch: 20; loss: 1.24; acc: 0.69
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.81
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.96; acc: 0.83
Batch: 120; loss: 1.07; acc: 0.78
Batch: 140; loss: 0.8; acc: 0.89
Val Epoch over. val_loss: 0.9446231658291665; val_accuracy: 0.8148885350318471 

The current subspace-distance is: 0.0001023370714392513 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.92
Batch: 20; loss: 0.9; acc: 0.88
Batch: 40; loss: 0.98; acc: 0.81
Batch: 60; loss: 0.92; acc: 0.83
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 0.98; acc: 0.81
Batch: 120; loss: 1.04; acc: 0.7
Batch: 140; loss: 1.01; acc: 0.77
Batch: 160; loss: 1.0; acc: 0.77
Batch: 180; loss: 0.96; acc: 0.81
Batch: 200; loss: 1.1; acc: 0.77
Batch: 220; loss: 0.92; acc: 0.83
Batch: 240; loss: 0.99; acc: 0.78
Batch: 260; loss: 0.9; acc: 0.84
Batch: 280; loss: 1.07; acc: 0.72
Batch: 300; loss: 0.84; acc: 0.88
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 1.17; acc: 0.69
Batch: 360; loss: 1.06; acc: 0.72
Batch: 380; loss: 0.9; acc: 0.92
Batch: 400; loss: 0.93; acc: 0.78
Batch: 420; loss: 0.83; acc: 0.86
Batch: 440; loss: 0.88; acc: 0.89
Batch: 460; loss: 0.9; acc: 0.83
Batch: 480; loss: 0.95; acc: 0.75
Batch: 500; loss: 0.9; acc: 0.88
Batch: 520; loss: 1.08; acc: 0.78
Batch: 540; loss: 1.14; acc: 0.69
Batch: 560; loss: 0.92; acc: 0.8
Batch: 580; loss: 0.98; acc: 0.8
Batch: 600; loss: 1.04; acc: 0.75
Batch: 620; loss: 1.15; acc: 0.72
Batch: 640; loss: 0.9; acc: 0.78
Batch: 660; loss: 1.07; acc: 0.77
Batch: 680; loss: 0.98; acc: 0.77
Batch: 700; loss: 0.93; acc: 0.8
Batch: 720; loss: 1.05; acc: 0.7
Batch: 740; loss: 1.03; acc: 0.75
Batch: 760; loss: 0.86; acc: 0.83
Batch: 780; loss: 0.82; acc: 0.81
Train Epoch over. train_loss: 0.96; train_accuracy: 0.8 

0.0001224470470333472
0.0001171541734947823
Batch: 0; loss: 0.84; acc: 0.88
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 0.66; acc: 0.86
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.79; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.88
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 0.68; acc: 0.89
Val Epoch over. val_loss: 0.8671530610436846; val_accuracy: 0.8299164012738853 

The current subspace-distance is: 0.0001171541734947823 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.8
Batch: 20; loss: 1.0; acc: 0.8
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.88
Batch: 100; loss: 0.96; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.89
Batch: 160; loss: 0.85; acc: 0.81
Batch: 180; loss: 0.83; acc: 0.84
Batch: 200; loss: 0.85; acc: 0.83
Batch: 220; loss: 0.99; acc: 0.77
Batch: 240; loss: 0.9; acc: 0.83
Batch: 260; loss: 1.01; acc: 0.72
Batch: 280; loss: 0.82; acc: 0.84
Batch: 300; loss: 0.76; acc: 0.88
Batch: 320; loss: 0.88; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.92
Batch: 360; loss: 0.91; acc: 0.8
Batch: 380; loss: 0.98; acc: 0.78
Batch: 400; loss: 0.94; acc: 0.78
Batch: 420; loss: 0.84; acc: 0.84
Batch: 440; loss: 0.89; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.83
Batch: 480; loss: 0.87; acc: 0.8
Batch: 500; loss: 0.83; acc: 0.86
Batch: 520; loss: 0.87; acc: 0.8
Batch: 540; loss: 0.95; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.97
Batch: 580; loss: 0.8; acc: 0.88
Batch: 600; loss: 0.9; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.91
Batch: 640; loss: 0.95; acc: 0.8
Batch: 660; loss: 0.79; acc: 0.84
Batch: 680; loss: 0.94; acc: 0.8
Batch: 700; loss: 0.99; acc: 0.78
Batch: 720; loss: 0.87; acc: 0.78
Batch: 740; loss: 0.73; acc: 0.91
Batch: 760; loss: 0.77; acc: 0.88
Batch: 780; loss: 0.84; acc: 0.81
Train Epoch over. train_loss: 0.9; train_accuracy: 0.81 

0.00013488036347553134
0.00013083661906421185
Batch: 0; loss: 0.8; acc: 0.88
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.72; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.86
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.91
Val Epoch over. val_loss: 0.8060841499620183; val_accuracy: 0.8395700636942676 

The current subspace-distance is: 0.00013083661906421185 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.8; acc: 0.86
Batch: 60; loss: 0.81; acc: 0.83
Batch: 80; loss: 0.84; acc: 0.81
Batch: 100; loss: 0.88; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.86
Batch: 140; loss: 0.97; acc: 0.77
Batch: 160; loss: 0.86; acc: 0.78
Batch: 180; loss: 0.81; acc: 0.84
Batch: 200; loss: 0.74; acc: 0.88
Batch: 220; loss: 0.94; acc: 0.75
Batch: 240; loss: 1.02; acc: 0.75
Batch: 260; loss: 0.94; acc: 0.78
Batch: 280; loss: 0.93; acc: 0.78
Batch: 300; loss: 0.78; acc: 0.86
Batch: 320; loss: 0.87; acc: 0.78
Batch: 340; loss: 1.04; acc: 0.77
Batch: 360; loss: 0.83; acc: 0.88
Batch: 380; loss: 0.88; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.83
Batch: 420; loss: 0.92; acc: 0.8
Batch: 440; loss: 0.82; acc: 0.83
Batch: 460; loss: 0.84; acc: 0.78
Batch: 480; loss: 0.93; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.89
Batch: 540; loss: 0.83; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.84; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.81
Batch: 620; loss: 0.68; acc: 0.91
Batch: 640; loss: 0.85; acc: 0.8
Batch: 660; loss: 0.87; acc: 0.77
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 0.84; acc: 0.84
Batch: 720; loss: 0.89; acc: 0.75
Batch: 740; loss: 0.77; acc: 0.83
Batch: 760; loss: 0.8; acc: 0.83
Batch: 780; loss: 0.77; acc: 0.84
Train Epoch over. train_loss: 0.84; train_accuracy: 0.82 

0.00014756392920389771
0.00014143218868412077
Batch: 0; loss: 0.74; acc: 0.91
Batch: 20; loss: 0.99; acc: 0.75
Batch: 40; loss: 0.57; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.7527721186352384; val_accuracy: 0.8472332802547771 

The current subspace-distance is: 0.00014143218868412077 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.7
Batch: 20; loss: 1.03; acc: 0.69
Batch: 40; loss: 0.94; acc: 0.72
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.88; acc: 0.84
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.77
Batch: 160; loss: 0.87; acc: 0.83
Batch: 180; loss: 0.8; acc: 0.81
Batch: 200; loss: 0.9; acc: 0.83
Batch: 220; loss: 0.77; acc: 0.84
Batch: 240; loss: 0.97; acc: 0.73
Batch: 260; loss: 0.78; acc: 0.75
Batch: 280; loss: 0.8; acc: 0.81
Batch: 300; loss: 1.07; acc: 0.67
Batch: 320; loss: 0.75; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.91
Batch: 360; loss: 0.86; acc: 0.8
Batch: 380; loss: 0.69; acc: 0.91
Batch: 400; loss: 0.78; acc: 0.77
Batch: 420; loss: 0.83; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.83
Batch: 460; loss: 0.83; acc: 0.8
Batch: 480; loss: 0.8; acc: 0.83
Batch: 500; loss: 0.81; acc: 0.84
Batch: 520; loss: 0.73; acc: 0.86
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.68; acc: 0.86
Batch: 580; loss: 0.84; acc: 0.81
Batch: 600; loss: 0.86; acc: 0.77
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 0.75; acc: 0.81
Batch: 660; loss: 0.85; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.92
Batch: 720; loss: 0.68; acc: 0.83
Batch: 740; loss: 0.81; acc: 0.8
Batch: 760; loss: 0.68; acc: 0.86
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.8; train_accuracy: 0.82 

0.00016105917165987194
0.00015503053145948797
Batch: 0; loss: 0.7; acc: 0.91
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.92
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.7111110797353611; val_accuracy: 0.8499203821656051 

The current subspace-distance is: 0.00015503053145948797 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.86; acc: 0.73
Batch: 120; loss: 0.78; acc: 0.84
Batch: 140; loss: 0.8; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.73; acc: 0.86
Batch: 200; loss: 0.77; acc: 0.83
Batch: 220; loss: 0.9; acc: 0.78
Batch: 240; loss: 0.73; acc: 0.88
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 0.77; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.88
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.7; acc: 0.89
Batch: 360; loss: 0.83; acc: 0.81
Batch: 380; loss: 0.71; acc: 0.81
Batch: 400; loss: 0.82; acc: 0.81
Batch: 420; loss: 0.79; acc: 0.83
Batch: 440; loss: 0.95; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.72; acc: 0.84
Batch: 500; loss: 0.81; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.82; acc: 0.78
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.89
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.69; acc: 0.88
Batch: 700; loss: 0.75; acc: 0.88
Batch: 720; loss: 0.7; acc: 0.89
Batch: 740; loss: 0.75; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.84
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.76; train_accuracy: 0.83 

0.0001709953066892922
0.00016368676733691245
Batch: 0; loss: 0.66; acc: 0.91
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.47; acc: 0.89
Val Epoch over. val_loss: 0.685340720187327; val_accuracy: 0.8514132165605095 

The current subspace-distance is: 0.00016368676733691245 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.88
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.85; acc: 0.83
Batch: 60; loss: 0.94; acc: 0.77
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.75; acc: 0.8
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.93; acc: 0.75
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.79; acc: 0.84
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.73; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.86; acc: 0.8
Batch: 320; loss: 0.77; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.72; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.76; acc: 0.84
Batch: 440; loss: 0.82; acc: 0.81
Batch: 460; loss: 0.79; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.91
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.77; acc: 0.78
Batch: 540; loss: 0.79; acc: 0.75
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.66; acc: 0.88
Batch: 600; loss: 0.88; acc: 0.77
Batch: 620; loss: 0.76; acc: 0.83
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.00018090687808580697
0.00017281195323448628
Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.94
Val Epoch over. val_loss: 0.6549012674267884; val_accuracy: 0.8576831210191083 

The current subspace-distance is: 0.00017281195323448628 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.91
Batch: 140; loss: 0.81; acc: 0.81
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.89; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.8; acc: 0.8
Batch: 260; loss: 1.03; acc: 0.7
Batch: 280; loss: 0.8; acc: 0.81
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.57; acc: 0.91
Batch: 360; loss: 0.77; acc: 0.81
Batch: 380; loss: 0.82; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.87; acc: 0.77
Batch: 500; loss: 0.55; acc: 0.94
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.77; acc: 0.86
Batch: 620; loss: 0.84; acc: 0.77
Batch: 640; loss: 0.74; acc: 0.84
Batch: 660; loss: 0.7; acc: 0.8
Batch: 680; loss: 0.82; acc: 0.75
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.72; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.78; acc: 0.84
Batch: 780; loss: 0.72; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.00018327213183511049
0.00017637670680414885
Batch: 0; loss: 0.64; acc: 0.92
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.94
Val Epoch over. val_loss: 0.6552650670337069; val_accuracy: 0.8575835987261147 

The current subspace-distance is: 0.00017637670680414885 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 0.94; acc: 0.78
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.77; acc: 0.81
Batch: 240; loss: 0.61; acc: 0.89
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.89
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.71; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.78
Batch: 500; loss: 0.66; acc: 0.8
Batch: 520; loss: 0.88; acc: 0.8
Batch: 540; loss: 0.76; acc: 0.8
Batch: 560; loss: 0.81; acc: 0.73
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.89; acc: 0.75
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.79; acc: 0.75
Batch: 700; loss: 0.82; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.95
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.77; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00018533773254603148
0.00017532570927869529
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.6479469378283069; val_accuracy: 0.8617635350318471 

The current subspace-distance is: 0.00017532570927869529 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.7; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.82; acc: 0.81
Batch: 160; loss: 0.61; acc: 0.91
Batch: 180; loss: 0.89; acc: 0.75
Batch: 200; loss: 0.77; acc: 0.8
Batch: 220; loss: 0.61; acc: 0.91
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.78
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.67; acc: 0.89
Batch: 400; loss: 0.77; acc: 0.83
Batch: 420; loss: 0.73; acc: 0.83
Batch: 440; loss: 0.73; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.96; acc: 0.73
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.84; acc: 0.77
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.8; acc: 0.81
Batch: 700; loss: 0.78; acc: 0.72
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.88
Batch: 780; loss: 0.72; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00018972849647980183
0.0001804468920454383
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.95
Val Epoch over. val_loss: 0.6442666911775139; val_accuracy: 0.8562898089171974 

The current subspace-distance is: 0.0001804468920454383 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.67; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.72; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.89
Batch: 340; loss: 0.91; acc: 0.73
Batch: 360; loss: 0.55; acc: 0.94
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.89
Batch: 440; loss: 0.64; acc: 0.88
Batch: 460; loss: 0.73; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.83
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.83; acc: 0.8
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.78; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.92; acc: 0.73
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.77; acc: 0.75
Batch: 760; loss: 0.69; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00019097757467534393
0.00018295861082151532
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6384787441818578; val_accuracy: 0.861265923566879 

The current subspace-distance is: 0.00018295861082151532 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.88
Batch: 40; loss: 0.83; acc: 0.81
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.84; acc: 0.81
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.92
Batch: 240; loss: 0.65; acc: 0.89
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.92
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.83; acc: 0.81
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.67; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.94
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.67; acc: 0.88
Batch: 640; loss: 0.71; acc: 0.78
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.74; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.89
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.91
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00019226210133638233
0.00018484109023120254
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.84; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6424583160573509; val_accuracy: 0.8591759554140127 

The current subspace-distance is: 0.00018484109023120254 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.9; acc: 0.69
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 0.82; acc: 0.75
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.81; acc: 0.78
Batch: 160; loss: 0.93; acc: 0.77
Batch: 180; loss: 0.52; acc: 0.94
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.73; acc: 0.78
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.74; acc: 0.86
Batch: 300; loss: 0.86; acc: 0.75
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.91
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.89
Batch: 420; loss: 0.73; acc: 0.78
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.7; acc: 0.88
Batch: 500; loss: 0.73; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.88
Batch: 540; loss: 0.67; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.92
Batch: 580; loss: 0.78; acc: 0.73
Batch: 600; loss: 0.69; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.74; acc: 0.81
Batch: 680; loss: 0.79; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.75; acc: 0.81
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.54; acc: 0.91
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.0001937343186000362
0.00018641662609297782
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6279608034024573; val_accuracy: 0.8581807324840764 

The current subspace-distance is: 0.00018641662609297782 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.94
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.71; acc: 0.84
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.94
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.82; acc: 0.72
Batch: 300; loss: 0.81; acc: 0.78
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.91
Batch: 380; loss: 0.76; acc: 0.75
Batch: 400; loss: 0.84; acc: 0.77
Batch: 420; loss: 0.93; acc: 0.72
Batch: 440; loss: 0.66; acc: 0.86
Batch: 460; loss: 0.82; acc: 0.81
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.79; acc: 0.84
Batch: 520; loss: 0.85; acc: 0.83
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.76; acc: 0.77
Batch: 640; loss: 0.57; acc: 0.91
Batch: 660; loss: 0.8; acc: 0.8
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.82; acc: 0.81
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00019474103464744985
0.00018713319150265306
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.623597577878624; val_accuracy: 0.8603702229299363 

The current subspace-distance is: 0.00018713319150265306 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.91; acc: 0.73
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.73; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.84; acc: 0.78
Batch: 240; loss: 0.74; acc: 0.88
Batch: 260; loss: 0.8; acc: 0.78
Batch: 280; loss: 0.6; acc: 0.89
Batch: 300; loss: 0.71; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.84
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.84
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.74; acc: 0.84
Batch: 420; loss: 0.83; acc: 0.78
Batch: 440; loss: 0.61; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.59; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.73; acc: 0.81
Batch: 600; loss: 0.79; acc: 0.81
Batch: 620; loss: 0.66; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.92
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00019915521261282265
0.00019073155999649316
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.4; acc: 0.95
Val Epoch over. val_loss: 0.62579757335839; val_accuracy: 0.8595740445859873 

The current subspace-distance is: 0.00019073155999649316 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.86; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.84
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.81
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.61; acc: 0.91
Batch: 300; loss: 0.69; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.77; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.85; acc: 0.77
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.63; acc: 0.88
Batch: 620; loss: 0.93; acc: 0.73
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.46; acc: 0.97
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.67; acc: 0.88
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00020220261649228632
0.0001919405476655811
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.6080348468889856; val_accuracy: 0.8630573248407644 

The current subspace-distance is: 0.0001919405476655811 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.81
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.76; acc: 0.81
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.76; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.72; acc: 0.88
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.7; acc: 0.83
Batch: 340; loss: 0.7; acc: 0.86
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.91; acc: 0.69
Batch: 440; loss: 0.65; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.7; acc: 0.8
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.65; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.89
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020331185078248382
0.00019353114475961775
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.8; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.5997817017090549; val_accuracy: 0.8638535031847133 

The current subspace-distance is: 0.00019353114475961775 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.77
Batch: 180; loss: 0.64; acc: 0.91
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.8; acc: 0.78
Batch: 240; loss: 0.54; acc: 0.92
Batch: 260; loss: 0.69; acc: 0.88
Batch: 280; loss: 0.73; acc: 0.84
Batch: 300; loss: 0.92; acc: 0.72
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.52; acc: 0.91
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.76; acc: 0.75
Batch: 420; loss: 0.69; acc: 0.88
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.73; acc: 0.77
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.89
Batch: 540; loss: 0.67; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.77; acc: 0.78
Batch: 720; loss: 0.76; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.71; acc: 0.91
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020481314277276397
0.00019564919057302177
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.89
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.6107160001044061; val_accuracy: 0.8607683121019108 

The current subspace-distance is: 0.00019564919057302177 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.81; acc: 0.78
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.88
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.76; acc: 0.78
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.78
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.86; acc: 0.77
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020138437685091048
0.0001970365847228095
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.6170092703430516; val_accuracy: 0.8568869426751592 

The current subspace-distance is: 0.0001970365847228095 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.86
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.94
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.88
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.89; acc: 0.72
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.73; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.79; acc: 0.78
Batch: 580; loss: 0.72; acc: 0.78
Batch: 600; loss: 0.64; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.8
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020482140826061368
0.0001963293761946261
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.6032988205077542; val_accuracy: 0.861265923566879 

The current subspace-distance is: 0.0001963293761946261 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.94
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.79; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.92
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.78; acc: 0.8
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.66; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.74; acc: 0.78
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.94
Batch: 640; loss: 0.78; acc: 0.77
Batch: 660; loss: 0.71; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.84
Batch: 780; loss: 0.85; acc: 0.7
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.0002061956183752045
0.0001971970486920327
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.6047916131414426; val_accuracy: 0.863156847133758 

The current subspace-distance is: 0.0001971970486920327 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.75; acc: 0.78
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.76; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.91
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.86
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.78; acc: 0.78
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.8; acc: 0.8
Batch: 580; loss: 0.65; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.78
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.7; acc: 0.86
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.73; acc: 0.77
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020495045464485884
0.00019861514738295227
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.8; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5944542524161612; val_accuracy: 0.8637539808917197 

The current subspace-distance is: 0.00019861514738295227 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.76; acc: 0.8
Batch: 160; loss: 0.59; acc: 0.86
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.88
Batch: 340; loss: 0.78; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.94
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.74; acc: 0.81
Batch: 540; loss: 0.86; acc: 0.73
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.63; acc: 0.89
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.58; acc: 0.91
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.75
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020809833949897438
0.00020014253095723689
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.6021530277030483; val_accuracy: 0.8618630573248408 

The current subspace-distance is: 0.00020014253095723689 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.92
Batch: 80; loss: 0.72; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.62; acc: 0.88
Batch: 220; loss: 0.74; acc: 0.83
Batch: 240; loss: 0.49; acc: 0.94
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.54; acc: 0.94
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.67; acc: 0.86
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.84; acc: 0.77
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.78; acc: 0.78
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.0002104497980326414
0.00020159082487225533
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.5892840113229812; val_accuracy: 0.8667396496815286 

The current subspace-distance is: 0.00020159082487225533 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.77
Batch: 20; loss: 0.59; acc: 0.91
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.73; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.94
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.63; acc: 0.86
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.81; acc: 0.75
Batch: 420; loss: 0.63; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.92
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.64; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.87; acc: 0.75
Batch: 640; loss: 0.53; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.81
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.0002073671348625794
0.00019992204033769667
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5944789353829281; val_accuracy: 0.8662420382165605 

The current subspace-distance is: 0.00019992204033769667 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.72; acc: 0.81
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.81; acc: 0.78
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.79; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.73
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.84; acc: 0.72
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.74; acc: 0.83
Batch: 520; loss: 0.81; acc: 0.8
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.91
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 0.57; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.7; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.94
Batch: 700; loss: 0.74; acc: 0.77
Batch: 720; loss: 0.72; acc: 0.83
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.69; acc: 0.86
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00021004975133109838
0.00020203631720505655
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.593533664751964; val_accuracy: 0.8619625796178344 

The current subspace-distance is: 0.00020203631720505655 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.86; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.67; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.84; acc: 0.78
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.78; acc: 0.75
Batch: 400; loss: 0.76; acc: 0.77
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.71; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 0.79; acc: 0.73
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.84
Batch: 680; loss: 0.8; acc: 0.78
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.88; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.0002090360940201208
0.00020037690410390496
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.5986437634297996; val_accuracy: 0.8625597133757962 

The current subspace-distance is: 0.00020037690410390496 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_3_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.25

The number of parameters is: 266027

The number of individual parameters is:

26
260
26
26
39
42588
39
39
78
127764
78
78
64
89856
64
64
4096
64
640
10
64
64

nonzero elements in E: 79808091
elements in E: 79808100
fraction nonzero: 0.9999998872294917
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.08
Batch: 20; loss: 1.94; acc: 0.36
Batch: 40; loss: 1.97; acc: 0.39
Batch: 60; loss: 1.72; acc: 0.5
Batch: 80; loss: 1.68; acc: 0.58
Batch: 100; loss: 1.61; acc: 0.61
Batch: 120; loss: 1.57; acc: 0.59
Batch: 140; loss: 1.63; acc: 0.61
Batch: 160; loss: 1.46; acc: 0.62
Batch: 180; loss: 1.53; acc: 0.64
Batch: 200; loss: 1.54; acc: 0.67
Batch: 220; loss: 1.4; acc: 0.75
Batch: 240; loss: 1.35; acc: 0.73
Batch: 260; loss: 1.31; acc: 0.75
Batch: 280; loss: 1.48; acc: 0.59
Batch: 300; loss: 1.53; acc: 0.56
Batch: 320; loss: 1.44; acc: 0.66
Batch: 340; loss: 1.43; acc: 0.62
Batch: 360; loss: 1.25; acc: 0.75
Batch: 380; loss: 1.24; acc: 0.72
Batch: 400; loss: 1.32; acc: 0.73
Batch: 420; loss: 1.28; acc: 0.73
Batch: 440; loss: 1.24; acc: 0.73
Batch: 460; loss: 1.33; acc: 0.67
Batch: 480; loss: 1.21; acc: 0.78
Batch: 500; loss: 1.21; acc: 0.77
Batch: 520; loss: 1.27; acc: 0.69
Batch: 540; loss: 1.22; acc: 0.72
Batch: 560; loss: 1.2; acc: 0.78
Batch: 580; loss: 1.27; acc: 0.75
Batch: 600; loss: 1.11; acc: 0.8
Batch: 620; loss: 1.25; acc: 0.8
Batch: 640; loss: 1.2; acc: 0.77
Batch: 660; loss: 1.0; acc: 0.81
Batch: 680; loss: 1.13; acc: 0.77
Batch: 700; loss: 1.11; acc: 0.81
Batch: 720; loss: 1.19; acc: 0.75
Batch: 740; loss: 1.13; acc: 0.73
Batch: 760; loss: 1.13; acc: 0.81
Batch: 780; loss: 1.12; acc: 0.81
Train Epoch over. train_loss: 1.38; train_accuracy: 0.68 

6.02000072831288e-05
5.51087359781377e-05
Batch: 0; loss: 1.16; acc: 0.83
Batch: 20; loss: 1.24; acc: 0.77
Batch: 40; loss: 0.81; acc: 0.88
Batch: 60; loss: 1.02; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.84
Batch: 100; loss: 1.03; acc: 0.86
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.0; acc: 0.84
Val Epoch over. val_loss: 1.0950107703543013; val_accuracy: 0.8043391719745223 

The current subspace-distance is: 5.51087359781377e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.81
Batch: 20; loss: 1.15; acc: 0.84
Batch: 40; loss: 1.19; acc: 0.78
Batch: 60; loss: 1.3; acc: 0.69
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.81
Batch: 120; loss: 1.06; acc: 0.8
Batch: 140; loss: 1.18; acc: 0.75
Batch: 160; loss: 1.13; acc: 0.7
Batch: 180; loss: 1.04; acc: 0.78
Batch: 200; loss: 1.02; acc: 0.81
Batch: 220; loss: 1.0; acc: 0.83
Batch: 240; loss: 1.02; acc: 0.86
Batch: 260; loss: 1.03; acc: 0.75
Batch: 280; loss: 1.08; acc: 0.83
Batch: 300; loss: 1.25; acc: 0.69
Batch: 320; loss: 1.05; acc: 0.78
Batch: 340; loss: 1.07; acc: 0.86
Batch: 360; loss: 1.05; acc: 0.8
Batch: 380; loss: 1.03; acc: 0.84
Batch: 400; loss: 0.96; acc: 0.88
Batch: 420; loss: 1.06; acc: 0.77
Batch: 440; loss: 0.98; acc: 0.81
Batch: 460; loss: 0.93; acc: 0.84
Batch: 480; loss: 1.08; acc: 0.8
Batch: 500; loss: 1.03; acc: 0.84
Batch: 520; loss: 1.1; acc: 0.77
Batch: 540; loss: 1.01; acc: 0.81
Batch: 560; loss: 0.92; acc: 0.88
Batch: 580; loss: 1.15; acc: 0.69
Batch: 600; loss: 1.01; acc: 0.84
Batch: 620; loss: 0.99; acc: 0.81
Batch: 640; loss: 0.86; acc: 0.88
Batch: 660; loss: 1.06; acc: 0.8
Batch: 680; loss: 0.99; acc: 0.83
Batch: 700; loss: 0.98; acc: 0.78
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 1.05; acc: 0.75
Batch: 760; loss: 0.97; acc: 0.8
Batch: 780; loss: 0.85; acc: 0.92
Train Epoch over. train_loss: 1.05; train_accuracy: 0.81 

8.403459651162848e-05
7.906452810857445e-05
Batch: 0; loss: 0.89; acc: 0.91
Batch: 20; loss: 1.02; acc: 0.81
Batch: 40; loss: 0.67; acc: 0.95
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.94
Batch: 100; loss: 0.89; acc: 0.84
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 0.77; acc: 0.86
Val Epoch over. val_loss: 0.9039635453254554; val_accuracy: 0.8457404458598726 

The current subspace-distance is: 7.906452810857445e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.83
Batch: 40; loss: 0.95; acc: 0.84
Batch: 60; loss: 0.98; acc: 0.83
Batch: 80; loss: 0.93; acc: 0.84
Batch: 100; loss: 0.93; acc: 0.86
Batch: 120; loss: 1.07; acc: 0.78
Batch: 140; loss: 0.94; acc: 0.8
Batch: 160; loss: 1.05; acc: 0.78
Batch: 180; loss: 0.86; acc: 0.89
Batch: 200; loss: 0.9; acc: 0.86
Batch: 220; loss: 0.95; acc: 0.83
Batch: 240; loss: 0.89; acc: 0.88
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 0.85; acc: 0.84
Batch: 300; loss: 0.94; acc: 0.84
Batch: 320; loss: 0.88; acc: 0.84
Batch: 340; loss: 0.88; acc: 0.92
Batch: 360; loss: 0.79; acc: 0.91
Batch: 380; loss: 0.82; acc: 0.89
Batch: 400; loss: 0.79; acc: 0.89
Batch: 420; loss: 0.92; acc: 0.8
Batch: 440; loss: 0.92; acc: 0.84
Batch: 460; loss: 0.79; acc: 0.88
Batch: 480; loss: 0.81; acc: 0.89
Batch: 500; loss: 0.89; acc: 0.83
Batch: 520; loss: 0.78; acc: 0.88
Batch: 540; loss: 0.87; acc: 0.83
Batch: 560; loss: 0.81; acc: 0.92
Batch: 580; loss: 0.8; acc: 0.88
Batch: 600; loss: 0.88; acc: 0.83
Batch: 620; loss: 0.9; acc: 0.83
Batch: 640; loss: 0.79; acc: 0.91
Batch: 660; loss: 0.81; acc: 0.86
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.85; acc: 0.88
Batch: 720; loss: 0.74; acc: 0.91
Batch: 740; loss: 0.88; acc: 0.84
Batch: 760; loss: 0.89; acc: 0.83
Batch: 780; loss: 0.77; acc: 0.91
Train Epoch over. train_loss: 0.9; train_accuracy: 0.84 

0.00010348433715989813
9.919455624185503e-05
Batch: 0; loss: 0.74; acc: 0.94
Batch: 20; loss: 0.93; acc: 0.83
Batch: 40; loss: 0.61; acc: 0.95
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.7; acc: 0.92
Batch: 100; loss: 0.81; acc: 0.88
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 0.63; acc: 0.91
Val Epoch over. val_loss: 0.7916503146195867; val_accuracy: 0.8672372611464968 

The current subspace-distance is: 9.919455624185503e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.83
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 0.88; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.86
Batch: 80; loss: 0.76; acc: 0.86
Batch: 100; loss: 0.82; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.91
Batch: 140; loss: 0.93; acc: 0.83
Batch: 160; loss: 0.91; acc: 0.84
Batch: 180; loss: 0.96; acc: 0.81
Batch: 200; loss: 0.92; acc: 0.78
Batch: 220; loss: 0.86; acc: 0.81
Batch: 240; loss: 0.77; acc: 0.91
Batch: 260; loss: 1.03; acc: 0.72
Batch: 280; loss: 0.76; acc: 0.92
Batch: 300; loss: 0.75; acc: 0.86
Batch: 320; loss: 0.8; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.89
Batch: 360; loss: 0.82; acc: 0.88
Batch: 380; loss: 0.91; acc: 0.8
Batch: 400; loss: 0.88; acc: 0.81
Batch: 420; loss: 0.92; acc: 0.8
Batch: 440; loss: 0.8; acc: 0.88
Batch: 460; loss: 0.95; acc: 0.8
Batch: 480; loss: 0.68; acc: 0.91
Batch: 500; loss: 0.77; acc: 0.84
Batch: 520; loss: 0.81; acc: 0.83
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.85; acc: 0.83
Batch: 580; loss: 0.82; acc: 0.84
Batch: 600; loss: 0.85; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.63; acc: 0.92
Batch: 660; loss: 0.7; acc: 0.86
Batch: 680; loss: 0.92; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.91
Batch: 720; loss: 0.68; acc: 0.88
Batch: 740; loss: 0.8; acc: 0.8
Batch: 760; loss: 0.76; acc: 0.88
Batch: 780; loss: 0.71; acc: 0.91
Train Epoch over. train_loss: 0.81; train_accuracy: 0.85 

0.0001177477024612017
0.00011284043284831569
Batch: 0; loss: 0.63; acc: 0.94
Batch: 20; loss: 0.88; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.92
Batch: 100; loss: 0.74; acc: 0.89
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.7057289012298462; val_accuracy: 0.8660429936305732 

The current subspace-distance is: 0.00011284043284831569 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.88
Batch: 40; loss: 0.83; acc: 0.81
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.81
Batch: 100; loss: 0.84; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.89
Batch: 140; loss: 0.83; acc: 0.86
Batch: 160; loss: 0.88; acc: 0.77
Batch: 180; loss: 0.83; acc: 0.83
Batch: 200; loss: 0.8; acc: 0.83
Batch: 220; loss: 0.73; acc: 0.89
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.79; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.84; acc: 0.8
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.61; acc: 0.88
Batch: 360; loss: 0.73; acc: 0.83
Batch: 380; loss: 0.85; acc: 0.8
Batch: 400; loss: 0.82; acc: 0.8
Batch: 420; loss: 0.76; acc: 0.86
Batch: 440; loss: 0.84; acc: 0.8
Batch: 460; loss: 0.63; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.92
Batch: 500; loss: 0.67; acc: 0.91
Batch: 520; loss: 0.73; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.92
Batch: 560; loss: 0.66; acc: 0.91
Batch: 580; loss: 0.75; acc: 0.84
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.94
Batch: 640; loss: 0.87; acc: 0.78
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.81; acc: 0.8
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.86
Batch: 760; loss: 0.87; acc: 0.75
Batch: 780; loss: 0.74; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.85 

0.00013259367551654577
0.00012780254473909736
Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.48; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.92
Batch: 100; loss: 0.7; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6580713941792774; val_accuracy: 0.8713176751592356 

The current subspace-distance is: 0.00012780254473909736 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.91
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.91
Batch: 140; loss: 0.81; acc: 0.81
Batch: 160; loss: 0.8; acc: 0.81
Batch: 180; loss: 0.76; acc: 0.84
Batch: 200; loss: 0.73; acc: 0.86
Batch: 220; loss: 0.67; acc: 0.88
Batch: 240; loss: 0.74; acc: 0.89
Batch: 260; loss: 0.77; acc: 0.73
Batch: 280; loss: 0.82; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.84
Batch: 320; loss: 0.78; acc: 0.8
Batch: 340; loss: 0.6; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.7; acc: 0.88
Batch: 400; loss: 0.68; acc: 0.89
Batch: 420; loss: 0.73; acc: 0.88
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.6; acc: 0.91
Batch: 480; loss: 0.71; acc: 0.84
Batch: 500; loss: 0.89; acc: 0.75
Batch: 520; loss: 0.84; acc: 0.8
Batch: 540; loss: 0.61; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.95
Batch: 580; loss: 0.65; acc: 0.86
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.63; acc: 0.92
Batch: 640; loss: 0.76; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.92
Batch: 680; loss: 0.86; acc: 0.78
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.86
Batch: 760; loss: 0.74; acc: 0.84
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.85 

0.00014415907207876444
0.0001395318831782788
Batch: 0; loss: 0.5; acc: 0.95
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.6125005884155347; val_accuracy: 0.8763933121019108 

The current subspace-distance is: 0.0001395318831782788 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.94
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.7; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.84
Batch: 160; loss: 0.84; acc: 0.77
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.63; acc: 0.89
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.65; acc: 0.89
Batch: 380; loss: 0.77; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.88
Batch: 440; loss: 0.69; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.81
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.81
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.88
Batch: 600; loss: 0.52; acc: 0.94
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.74; acc: 0.83
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.94
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.86 

0.00015821562556084245
0.00015059098950587213
Batch: 0; loss: 0.47; acc: 0.97
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.5671078114752557; val_accuracy: 0.8825636942675159 

The current subspace-distance is: 0.00015059098950587213 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.92
Batch: 80; loss: 0.72; acc: 0.73
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.54; acc: 0.91
Batch: 360; loss: 0.83; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.89
Batch: 440; loss: 0.7; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.69; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.92
Batch: 780; loss: 0.66; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.87 

0.00016756870900280774
0.00015949085354804993
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.74; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5407709289508261; val_accuracy: 0.8890326433121019 

The current subspace-distance is: 0.00015949085354804993 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.92
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.92
Batch: 320; loss: 0.78; acc: 0.77
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.71; acc: 0.83
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.94
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.68; acc: 0.78
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.85; acc: 0.77
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.00017677740834187716
0.00016913055151235312
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.5165539233927514; val_accuracy: 0.8904259554140127 

The current subspace-distance is: 0.00016913055151235312 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.91
Batch: 300; loss: 0.75; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.98
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.95
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.92
Batch: 500; loss: 0.6; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.94
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.76; acc: 0.81
Batch: 680; loss: 0.57; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.0001854821457527578
0.00018051924416795373
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.4957694369516555; val_accuracy: 0.893312101910828 

The current subspace-distance is: 0.00018051924416795373 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.67; acc: 0.75
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.92
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.00019079695630352944
0.00018331377941649407
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.483356552993416; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 0.00018331377941649407 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.0001924719545058906
0.0001856866292655468
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.4821681178109661; val_accuracy: 0.896297770700637 

The current subspace-distance is: 0.0001856866292655468 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.94
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.65; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.68; acc: 0.77
Batch: 400; loss: 0.66; acc: 0.8
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.61; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.94
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0001938831410370767
0.00018721161177381873
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.477395975475858; val_accuracy: 0.8985867834394905 

The current subspace-distance is: 0.00018721161177381873 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.91; acc: 0.8
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.95
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.42; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.000197283792658709
0.00019000920292455703
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4692987550975411; val_accuracy: 0.897890127388535 

The current subspace-distance is: 0.00019000920292455703 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00019893587159458548
0.00019046434317715466
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.98
Val Epoch over. val_loss: 0.47360653083795196; val_accuracy: 0.8947054140127388 

The current subspace-distance is: 0.00019046434317715466 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.66; acc: 0.78
Batch: 520; loss: 0.52; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.95
Batch: 680; loss: 0.64; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020095492072869092
0.00019363891624379903
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4634783000323423; val_accuracy: 0.8966958598726115 

The current subspace-distance is: 0.00019363891624379903 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.92
Batch: 620; loss: 0.67; acc: 0.78
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0002046564914053306
0.00019541273650247604
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.45303699839267003; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 0.00019541273650247604 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.56; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.83
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.97
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.94
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020373119332361966
0.0001975186896743253
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.45076053622801593; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 0.0001975186896743253 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.92
Batch: 160; loss: 0.54; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.95
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.65; acc: 0.81
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.75; acc: 0.75
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020666126511059701
0.00020120697445236146
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.4467881829685466; val_accuracy: 0.9011743630573248 

The current subspace-distance is: 0.00020120697445236146 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.97
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.95
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.000208739991649054
0.00020112929632887244
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.44886805572707184; val_accuracy: 0.9001791401273885 

The current subspace-distance is: 0.00020112929632887244 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.94
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.72; acc: 0.73
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0002082868741126731
0.0002041853149421513
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.4413303897069518; val_accuracy: 0.9009753184713376 

The current subspace-distance is: 0.0002041853149421513 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.95
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 0.5; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.64; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.97
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.46; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0002117541152983904
0.0002024180139414966
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.440516137962888; val_accuracy: 0.9022691082802548 

The current subspace-distance is: 0.0002024180139414966 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.8
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.95
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.61; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00021045964967925102
0.0002022398984991014
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.4389423289496428; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 0.0002022398984991014 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.66; acc: 0.8
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.92
Batch: 780; loss: 0.53; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00021292059682309628
0.00020482842228375375
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.43850312301307726; val_accuracy: 0.900577229299363 

The current subspace-distance is: 0.00020482842228375375 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.78; acc: 0.75
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021205321536399424
0.00020290316024329513
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.4347890453163985; val_accuracy: 0.9018710191082803 

The current subspace-distance is: 0.00020290316024329513 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.98
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.81; acc: 0.8
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.97
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0002127294719684869
0.00020438453066162765
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.43317671015763737; val_accuracy: 0.9033638535031847 

The current subspace-distance is: 0.00020438453066162765 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.79; acc: 0.7
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.92
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021324398403521627
0.00020471129391808063
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.42809259882018824; val_accuracy: 0.9046576433121019 

The current subspace-distance is: 0.00020471129391808063 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.65; acc: 0.77
Batch: 300; loss: 0.52; acc: 0.92
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.62; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.55; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021271282457746565
0.00020634385873563588
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.42259113481090327; val_accuracy: 0.904359076433121 

The current subspace-distance is: 0.00020634385873563588 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021611603733617812
0.00020629310165531933
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.4257541017927182; val_accuracy: 0.9044585987261147 

The current subspace-distance is: 0.00020629310165531933 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021712931629735976
0.0002090168127324432
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.42395062583267307; val_accuracy: 0.9040605095541401 

The current subspace-distance is: 0.0002090168127324432 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_3_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.25

The number of parameters is: 266027

The number of individual parameters is:

26
260
26
26
39
42588
39
39
78
127764
78
78
64
89856
64
64
4096
64
640
10
64
64

nonzero elements in E: 106410789
elements in E: 106410800
fraction nonzero: 0.9999998966270341
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.34; acc: 0.08
Batch: 20; loss: 1.94; acc: 0.33
Batch: 40; loss: 1.87; acc: 0.45
Batch: 60; loss: 1.67; acc: 0.62
Batch: 80; loss: 1.72; acc: 0.48
Batch: 100; loss: 1.44; acc: 0.75
Batch: 120; loss: 1.48; acc: 0.69
Batch: 140; loss: 1.37; acc: 0.78
Batch: 160; loss: 1.46; acc: 0.69
Batch: 180; loss: 1.52; acc: 0.61
Batch: 200; loss: 1.33; acc: 0.78
Batch: 220; loss: 1.37; acc: 0.69
Batch: 240; loss: 1.18; acc: 0.86
Batch: 260; loss: 1.28; acc: 0.69
Batch: 280; loss: 1.36; acc: 0.73
Batch: 300; loss: 1.31; acc: 0.66
Batch: 320; loss: 1.27; acc: 0.75
Batch: 340; loss: 1.1; acc: 0.83
Batch: 360; loss: 1.26; acc: 0.69
Batch: 380; loss: 1.15; acc: 0.77
Batch: 400; loss: 1.2; acc: 0.8
Batch: 420; loss: 1.31; acc: 0.7
Batch: 440; loss: 1.17; acc: 0.77
Batch: 460; loss: 1.09; acc: 0.84
Batch: 480; loss: 1.11; acc: 0.84
Batch: 500; loss: 1.01; acc: 0.89
Batch: 520; loss: 1.01; acc: 0.83
Batch: 540; loss: 1.19; acc: 0.73
Batch: 560; loss: 1.07; acc: 0.81
Batch: 580; loss: 1.0; acc: 0.8
Batch: 600; loss: 1.04; acc: 0.83
Batch: 620; loss: 0.99; acc: 0.81
Batch: 640; loss: 1.07; acc: 0.81
Batch: 660; loss: 1.01; acc: 0.83
Batch: 680; loss: 0.89; acc: 0.89
Batch: 700; loss: 1.08; acc: 0.75
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 0.87; acc: 0.84
Batch: 760; loss: 0.82; acc: 0.89
Batch: 780; loss: 1.0; acc: 0.73
Train Epoch over. train_loss: 1.25; train_accuracy: 0.74 

2.6359179173596203e-05
9.279479854740202e-06
Batch: 0; loss: 0.86; acc: 0.88
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.95
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.86; acc: 0.92
Batch: 100; loss: 0.85; acc: 0.84
Batch: 120; loss: 1.15; acc: 0.7
Batch: 140; loss: 0.79; acc: 0.89
Val Epoch over. val_loss: 0.905815919493414; val_accuracy: 0.8415605095541401 

The current subspace-distance is: 9.279479854740202e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.86
Batch: 40; loss: 0.89; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.91
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.89
Batch: 120; loss: 1.07; acc: 0.78
Batch: 140; loss: 0.85; acc: 0.88
Batch: 160; loss: 0.85; acc: 0.91
Batch: 180; loss: 0.9; acc: 0.81
Batch: 200; loss: 0.84; acc: 0.88
Batch: 220; loss: 0.82; acc: 0.88
Batch: 240; loss: 0.97; acc: 0.81
Batch: 260; loss: 0.98; acc: 0.75
Batch: 280; loss: 0.81; acc: 0.88
Batch: 300; loss: 0.79; acc: 0.88
Batch: 320; loss: 0.73; acc: 0.91
Batch: 340; loss: 0.87; acc: 0.8
Batch: 360; loss: 0.85; acc: 0.83
Batch: 380; loss: 0.96; acc: 0.83
Batch: 400; loss: 0.77; acc: 0.86
Batch: 420; loss: 0.91; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.88
Batch: 480; loss: 0.77; acc: 0.88
Batch: 500; loss: 0.78; acc: 0.84
Batch: 520; loss: 0.98; acc: 0.73
Batch: 540; loss: 0.83; acc: 0.86
Batch: 560; loss: 0.68; acc: 0.91
Batch: 580; loss: 0.77; acc: 0.86
Batch: 600; loss: 0.9; acc: 0.8
Batch: 620; loss: 0.73; acc: 0.86
Batch: 640; loss: 0.77; acc: 0.89
Batch: 660; loss: 0.71; acc: 0.92
Batch: 680; loss: 0.87; acc: 0.83
Batch: 700; loss: 0.65; acc: 0.89
Batch: 720; loss: 0.76; acc: 0.88
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.76; acc: 0.88
Batch: 780; loss: 0.59; acc: 0.91
Train Epoch over. train_loss: 0.84; train_accuracy: 0.85 

3.19278406095691e-05
1.4064426977711264e-05
Batch: 0; loss: 0.6; acc: 0.97
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.51; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.92
Val Epoch over. val_loss: 0.6881801430966444; val_accuracy: 0.8781847133757962 

The current subspace-distance is: 1.4064426977711264e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.71; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.91
Batch: 80; loss: 0.72; acc: 0.91
Batch: 100; loss: 0.77; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.89
Batch: 160; loss: 0.71; acc: 0.95
Batch: 180; loss: 0.81; acc: 0.83
Batch: 200; loss: 0.83; acc: 0.8
Batch: 220; loss: 0.65; acc: 0.89
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.92
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.67; acc: 0.91
Batch: 320; loss: 0.7; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.91
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.77; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.84; acc: 0.8
Batch: 560; loss: 0.62; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.89
Batch: 620; loss: 0.74; acc: 0.89
Batch: 640; loss: 0.74; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.94
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.94
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.6; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.68; acc: 0.89
Train Epoch over. train_loss: 0.69; train_accuracy: 0.87 

3.675303378258832e-05
1.6317681001964957e-05
Batch: 0; loss: 0.51; acc: 0.94
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.5803058997840639; val_accuracy: 0.8947054140127388 

The current subspace-distance is: 1.6317681001964957e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.91
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.76; acc: 0.8
Batch: 160; loss: 0.62; acc: 0.92
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.92
Batch: 240; loss: 0.61; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.94
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.58; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.76; acc: 0.78
Batch: 420; loss: 0.67; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.57; acc: 0.94
Batch: 560; loss: 0.7; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.95
Batch: 600; loss: 0.52; acc: 0.92
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.95
Batch: 660; loss: 0.62; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.94
Train Epoch over. train_loss: 0.61; train_accuracy: 0.88 

3.979235771112144e-05
1.6597507055848837e-05
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5238459715797643; val_accuracy: 0.9026671974522293 

The current subspace-distance is: 1.6597507055848837e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.95
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.56; acc: 0.91
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.95
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.95
Batch: 340; loss: 0.75; acc: 0.8
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.62; acc: 0.8
Batch: 520; loss: 0.41; acc: 0.95
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.75; acc: 0.75
Batch: 600; loss: 0.55; acc: 0.92
Batch: 620; loss: 0.79; acc: 0.75
Batch: 640; loss: 0.48; acc: 0.94
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.7; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.94
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.56; train_accuracy: 0.88 

4.342618558439426e-05
2.0672987375291996e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.4876948399528576; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 2.0672987375291996e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.95
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.94
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.79; acc: 0.77
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.95
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

4.587939110933803e-05
2.0919633243465796e-05
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.45218301635638924; val_accuracy: 0.9099323248407644 

The current subspace-distance is: 2.0919633243465796e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.94
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.8
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

4.804779382538982e-05
2.128416053892579e-05
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.42679310395459463; val_accuracy: 0.912718949044586 

The current subspace-distance is: 2.128416053892579e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.98
Batch: 500; loss: 0.39; acc: 0.95
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

5.0625178118934855e-05
2.4450600903946906e-05
Batch: 0; loss: 0.36; acc: 0.98
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.94
Val Epoch over. val_loss: 0.4116719961166382; val_accuracy: 0.913515127388535 

The current subspace-distance is: 2.4450600903946906e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.97
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.196338315727189e-05
2.4516662961104885e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.3959248979949647; val_accuracy: 0.9156050955414012 

The current subspace-distance is: 2.4516662961104885e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.98
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.98
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.97
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.285307270241901e-05
2.4497558115399443e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.3689684900128917; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 2.4497558115399443e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.514948861673474e-05
2.6066994905704632e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.3736403894842051; val_accuracy: 0.9193869426751592 

The current subspace-distance is: 2.6066994905704632e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.539209087146446e-05
2.6336820155847818e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.374826601831017; val_accuracy: 0.9195859872611465 

The current subspace-distance is: 2.6336820155847818e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.98
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.95
Batch: 660; loss: 0.49; acc: 0.81
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.98
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.75
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.547201362787746e-05
2.4950415536295623e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3664442944298884; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 2.4950415536295623e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.51; acc: 0.81
Batch: 680; loss: 0.33; acc: 0.98
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

5.594269168796018e-05
2.6448920834809542e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.35824264073447815; val_accuracy: 0.9204816878980892 

The current subspace-distance is: 2.6448920834809542e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.98
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.98
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.72
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

5.637262438540347e-05
2.691669942578301e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3557491383165311; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.691669942578301e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.97
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.94
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.664670607075095e-05
2.5668654416222125e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.35278577893782576; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 2.5668654416222125e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.8
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

5.7225381169700995e-05
2.6804742446984164e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3551749533908382; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.6804742446984164e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.95
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.74705918552354e-05
2.566711737017613e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.347033918663195; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 2.566711737017613e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.739622429246083e-05
2.5807470592553727e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3468639562084417; val_accuracy: 0.9225716560509554 

The current subspace-distance is: 2.5807470592553727e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.59; acc: 0.8
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.22; acc: 1.0
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.771578798885457e-05
2.687995402084198e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3437248044143057; val_accuracy: 0.9234673566878981 

The current subspace-distance is: 2.687995402084198e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.97
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.97
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.843424878548831e-05
2.697216041269712e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.34633697958508874; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.697216041269712e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.98
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.97
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.83
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.69; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.882333425688557e-05
2.6845193133340217e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3411408193456899; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 2.6845193133340217e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.886524741072208e-05
2.6205041649518535e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.34324419308619897; val_accuracy: 0.9239649681528662 

The current subspace-distance is: 2.6205041649518535e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.9006462834076956e-05
2.8368522180244327e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3446262048403169; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 2.8368522180244327e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.937361856922507e-05
2.8592543458216824e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3365956961064582; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.8592543458216824e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.9946480178041384e-05
2.9698407161049545e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3400040405571081; val_accuracy: 0.9244625796178344 

The current subspace-distance is: 2.9698407161049545e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.97
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.9698224504245445e-05
2.795764521579258e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3384179546479966; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.795764521579258e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.21; acc: 1.0
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.81
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.955011511105113e-05
2.8061682314728387e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3362922186304809; val_accuracy: 0.9235668789808917 

The current subspace-distance is: 2.8061682314728387e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.9281814174028113e-05
2.7512622182257473e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3404220734622068; val_accuracy: 0.9228702229299363 

The current subspace-distance is: 2.7512622182257473e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.97
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.83
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.008599666529335e-05
2.7873353246832266e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.337948210964537; val_accuracy: 0.9234673566878981 

The current subspace-distance is: 2.7873353246832266e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_3_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 3.25

The number of parameters is: 266027

The number of individual parameters is:

26
260
26
26
39
42588
39
39
78
127764
78
78
64
89856
64
64
4096
64
640
10
64
64

nonzero elements in E: 133013489
elements in E: 133013500
fraction nonzero: 0.9999999173016273
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.42; acc: 0.03
Batch: 20; loss: 1.89; acc: 0.42
Batch: 40; loss: 1.81; acc: 0.52
Batch: 60; loss: 1.72; acc: 0.47
Batch: 80; loss: 1.64; acc: 0.59
Batch: 100; loss: 1.39; acc: 0.7
Batch: 120; loss: 1.51; acc: 0.59
Batch: 140; loss: 1.63; acc: 0.58
Batch: 160; loss: 1.44; acc: 0.67
Batch: 180; loss: 1.36; acc: 0.67
Batch: 200; loss: 1.27; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.8
Batch: 240; loss: 1.23; acc: 0.75
Batch: 260; loss: 1.22; acc: 0.75
Batch: 280; loss: 1.31; acc: 0.72
Batch: 300; loss: 1.11; acc: 0.83
Batch: 320; loss: 1.15; acc: 0.72
Batch: 340; loss: 1.1; acc: 0.77
Batch: 360; loss: 1.15; acc: 0.75
Batch: 380; loss: 1.13; acc: 0.73
Batch: 400; loss: 1.0; acc: 0.83
Batch: 420; loss: 0.9; acc: 0.94
Batch: 440; loss: 0.99; acc: 0.81
Batch: 460; loss: 1.07; acc: 0.75
Batch: 480; loss: 0.96; acc: 0.8
Batch: 500; loss: 1.05; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.84
Batch: 540; loss: 0.94; acc: 0.81
Batch: 560; loss: 1.02; acc: 0.78
Batch: 580; loss: 0.88; acc: 0.84
Batch: 600; loss: 0.87; acc: 0.89
Batch: 620; loss: 0.79; acc: 0.97
Batch: 640; loss: 0.99; acc: 0.81
Batch: 660; loss: 0.81; acc: 0.86
Batch: 680; loss: 0.87; acc: 0.84
Batch: 700; loss: 0.89; acc: 0.88
Batch: 720; loss: 0.89; acc: 0.83
Batch: 740; loss: 0.89; acc: 0.83
Batch: 760; loss: 0.85; acc: 0.86
Batch: 780; loss: 0.95; acc: 0.81
Train Epoch over. train_loss: 1.14; train_accuracy: 0.76 

2.6768393581733108e-05
9.488054274697788e-06
Batch: 0; loss: 0.75; acc: 0.92
Batch: 20; loss: 1.01; acc: 0.77
Batch: 40; loss: 0.54; acc: 0.97
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.68; acc: 0.91
Batch: 100; loss: 0.77; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.81
Batch: 140; loss: 0.64; acc: 0.88
Val Epoch over. val_loss: 0.7870950653294849; val_accuracy: 0.8632563694267515 

The current subspace-distance is: 9.488054274697788e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.92
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 0.89; acc: 0.8
Batch: 80; loss: 0.82; acc: 0.86
Batch: 100; loss: 0.84; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.92
Batch: 160; loss: 0.85; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.83
Batch: 200; loss: 0.87; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.84; acc: 0.84
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.61; acc: 0.97
Batch: 320; loss: 0.81; acc: 0.89
Batch: 340; loss: 0.82; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.92
Batch: 380; loss: 0.76; acc: 0.84
Batch: 400; loss: 0.79; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.88
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.71; acc: 0.86
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.76; acc: 0.81
Batch: 540; loss: 0.74; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.94
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.78; acc: 0.83
Batch: 620; loss: 0.6; acc: 0.91
Batch: 640; loss: 0.61; acc: 0.91
Batch: 660; loss: 0.77; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.6; acc: 0.97
Batch: 720; loss: 0.48; acc: 0.95
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.67; acc: 0.88
Train Epoch over. train_loss: 0.73; train_accuracy: 0.86 

3.185241075698286e-05
1.286152473767288e-05
Batch: 0; loss: 0.54; acc: 0.94
Batch: 20; loss: 0.84; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6081999071464417; val_accuracy: 0.8885350318471338 

The current subspace-distance is: 1.286152473767288e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.95
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.94
Batch: 160; loss: 0.73; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.91
Batch: 240; loss: 0.66; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.88
Batch: 300; loss: 0.73; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.89
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.88
Batch: 460; loss: 0.69; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.95
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.95
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.94
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.95
Batch: 720; loss: 0.67; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.95
Batch: 780; loss: 0.56; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

3.6724628444062546e-05
1.4746407032362185e-05
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5120149570855366; val_accuracy: 0.9022691082802548 

The current subspace-distance is: 1.4746407032362185e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.81; acc: 0.78
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.95
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.95
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.92
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.95
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.67; acc: 0.83
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

3.994805956608616e-05
1.6957814295892604e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4564260827128295; val_accuracy: 0.9091361464968153 

The current subspace-distance is: 1.6957814295892604e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.92
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.38; acc: 0.95
Batch: 180; loss: 0.48; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.97
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.62; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.97
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.95
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.95
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

4.323673420003615e-05
1.912796142278239e-05
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.62; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.41212093801635086; val_accuracy: 0.9163017515923567 

The current subspace-distance is: 1.912796142278239e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.97
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.97
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.97
Batch: 620; loss: 0.41; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.97
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

4.576586070470512e-05
1.908648118842393e-05
Batch: 0; loss: 0.33; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.38260643715691417; val_accuracy: 0.9187898089171974 

The current subspace-distance is: 1.908648118842393e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.97
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.97
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.98
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.98
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.804139825864695e-05
2.2086047465563752e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3692860380289661; val_accuracy: 0.9219745222929936 

The current subspace-distance is: 2.2086047465563752e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.56; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.41; acc: 0.95
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.042161137680523e-05
2.186736855946947e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3439572000769293; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 2.186736855946947e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.98
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.190545562072657e-05
2.2593134417547844e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.19; acc: 1.0
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3363596598149105; val_accuracy: 0.926453025477707 

The current subspace-distance is: 2.2593134417547844e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.423268521553837e-05
2.3917140424600802e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3213715073504266; val_accuracy: 0.9280453821656051 

The current subspace-distance is: 2.3917140424600802e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.98
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.97
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.4722517234040424e-05
2.2421825633500703e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3167605926846243; val_accuracy: 0.9292396496815286 

The current subspace-distance is: 2.2421825633500703e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.98
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.534525917028077e-05
2.439007948851213e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.313327709343403; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.439007948851213e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.98
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.18; acc: 1.0
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.98
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.5447089835070074e-05
2.418165786366444e-05
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3133232375239111; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.418165786366444e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.6425091315759346e-05
2.4724402464926243e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.30661628494976434; val_accuracy: 0.9297372611464968 

The current subspace-distance is: 2.4724402464926243e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.78
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.671024337061681e-05
2.462851261952892e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.29831164178385094; val_accuracy: 0.9325238853503185 

The current subspace-distance is: 2.462851261952892e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.61; acc: 0.8
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.7901666878024116e-05
2.557733932917472e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3000580245617089; val_accuracy: 0.9320262738853503 

The current subspace-distance is: 2.557733932917472e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.97
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.98
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.828532812302001e-05
2.6120071197510697e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.29741901776213553; val_accuracy: 0.9321257961783439 

The current subspace-distance is: 2.6120071197510697e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.98
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.864798367838375e-05
2.626995228638407e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2949471686296402; val_accuracy: 0.933718152866242 

The current subspace-distance is: 2.626995228638407e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.98
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.97
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.98
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.95328638155479e-05
2.6642364900908433e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.29096138225809026; val_accuracy: 0.934812898089172 

The current subspace-distance is: 2.6642364900908433e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.98
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.973673250991851e-05
2.6971130864694715e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2870878083216157; val_accuracy: 0.933718152866242 

The current subspace-distance is: 2.6971130864694715e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.98
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.953749132459052e-05
2.6617870389600284e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2910506629450306; val_accuracy: 0.9335191082802548 

The current subspace-distance is: 2.6617870389600284e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.81
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.012012454448268e-05
2.6655396140995435e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2797257969523691; val_accuracy: 0.9336186305732485 

The current subspace-distance is: 2.6655396140995435e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.972770304651931e-05
2.666884392965585e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.28284064001718145; val_accuracy: 0.9367038216560509 

The current subspace-distance is: 2.666884392965585e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.97
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.9452111599966884e-05
2.4768060029600747e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.28363453805636446; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.4768060029600747e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.0150312492623925e-05
2.675470750546083e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.28262165098623104; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.675470750546083e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.97
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.01279316470027e-05
2.5955245291697793e-05
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2847492039962939; val_accuracy: 0.9331210191082803 

The current subspace-distance is: 2.5955245291697793e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.98
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.98
Batch: 540; loss: 0.39; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.11287759966217e-05
2.9105458452249877e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2818088071646204; val_accuracy: 0.9365047770700637 

The current subspace-distance is: 2.9105458452249877e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.98
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.98
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.98
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.034546095179394e-05
2.7792668333859183e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.27983593788875893; val_accuracy: 0.9362062101910829 

The current subspace-distance is: 2.7792668333859183e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.98
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.059476800146513e-05
2.834421866282355e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.28404767972648526; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.834421866282355e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.21; acc: 1.0
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.98
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.98
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.059267616365105e-05
2.5988729248638265e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2770398967205339; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.5988729248638265e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_3_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:47/N_3_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
