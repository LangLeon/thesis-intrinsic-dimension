model : table13slim
N : 1
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:47

Channel scaling factor: 13.5

The number of parameters is: 279504

The number of individual parameters is:

108
432
108
108
162
52488
162
162
324
157464
324
324
64
62208
64
64
4096
64
640
10
64
64

nonzero elements in E: 13975198
elements in E: 13975200
fraction nonzero: 0.9999998568893469
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.43; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.09
Batch: 40; loss: 2.36; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.19
Batch: 80; loss: 2.15; acc: 0.23
Batch: 100; loss: 2.25; acc: 0.25
Batch: 120; loss: 2.23; acc: 0.17
Batch: 140; loss: 2.04; acc: 0.42
Batch: 160; loss: 2.13; acc: 0.19
Batch: 180; loss: 2.16; acc: 0.25
Batch: 200; loss: 2.14; acc: 0.19
Batch: 220; loss: 2.21; acc: 0.27
Batch: 240; loss: 2.07; acc: 0.27
Batch: 260; loss: 2.14; acc: 0.23
Batch: 280; loss: 2.02; acc: 0.33
Batch: 300; loss: 2.23; acc: 0.22
Batch: 320; loss: 2.0; acc: 0.31
Batch: 340; loss: 2.03; acc: 0.36
Batch: 360; loss: 2.1; acc: 0.28
Batch: 380; loss: 2.02; acc: 0.33
Batch: 400; loss: 2.08; acc: 0.27
Batch: 420; loss: 2.09; acc: 0.3
Batch: 440; loss: 2.0; acc: 0.33
Batch: 460; loss: 2.12; acc: 0.33
Batch: 480; loss: 2.06; acc: 0.41
Batch: 500; loss: 2.06; acc: 0.42
Batch: 520; loss: 2.04; acc: 0.31
Batch: 540; loss: 2.13; acc: 0.2
Batch: 560; loss: 2.13; acc: 0.19
Batch: 580; loss: 2.02; acc: 0.34
Batch: 600; loss: 2.03; acc: 0.33
Batch: 620; loss: 2.03; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.25
Batch: 660; loss: 2.1; acc: 0.27
Batch: 680; loss: 2.1; acc: 0.25
Batch: 700; loss: 2.09; acc: 0.2
Batch: 720; loss: 2.04; acc: 0.36
Batch: 740; loss: 2.05; acc: 0.31
Batch: 760; loss: 2.04; acc: 0.3
Batch: 780; loss: 1.98; acc: 0.36
Train Epoch over. train_loss: 2.11; train_accuracy: 0.27 

2.7601827241596766e-05
3.698078216984868e-06
Batch: 0; loss: 1.93; acc: 0.38
Batch: 20; loss: 2.25; acc: 0.19
Batch: 40; loss: 1.98; acc: 0.31
Batch: 60; loss: 2.0; acc: 0.36
Batch: 80; loss: 2.01; acc: 0.28
Batch: 100; loss: 1.99; acc: 0.44
Batch: 120; loss: 2.01; acc: 0.38
Batch: 140; loss: 2.07; acc: 0.3
Val Epoch over. val_loss: 2.044585041939073; val_accuracy: 0.3062300955414013 

The current subspace-distance is: 3.698078216984868e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.05; acc: 0.25
Batch: 20; loss: 1.98; acc: 0.3
Batch: 40; loss: 1.96; acc: 0.39
Batch: 60; loss: 2.16; acc: 0.2
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 2.05; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.27
Batch: 140; loss: 1.97; acc: 0.36
Batch: 160; loss: 2.08; acc: 0.22
Batch: 180; loss: 2.0; acc: 0.38
Batch: 200; loss: 2.06; acc: 0.3
Batch: 220; loss: 2.01; acc: 0.3
Batch: 240; loss: 1.96; acc: 0.47
Batch: 260; loss: 2.1; acc: 0.23
Batch: 280; loss: 2.0; acc: 0.33
Batch: 300; loss: 2.07; acc: 0.27
Batch: 320; loss: 1.89; acc: 0.39
Batch: 340; loss: 2.05; acc: 0.36
Batch: 360; loss: 2.01; acc: 0.42
Batch: 380; loss: 1.92; acc: 0.3
Batch: 400; loss: 2.07; acc: 0.3
Batch: 420; loss: 1.98; acc: 0.44
Batch: 440; loss: 2.02; acc: 0.33
Batch: 460; loss: 2.01; acc: 0.34
Batch: 480; loss: 2.15; acc: 0.25
Batch: 500; loss: 1.98; acc: 0.34
Batch: 520; loss: 2.1; acc: 0.28
Batch: 540; loss: 2.11; acc: 0.2
Batch: 560; loss: 1.91; acc: 0.45
Batch: 580; loss: 2.08; acc: 0.23
Batch: 600; loss: 1.93; acc: 0.39
Batch: 620; loss: 1.96; acc: 0.47
Batch: 640; loss: 2.12; acc: 0.3
Batch: 660; loss: 1.98; acc: 0.33
Batch: 680; loss: 1.96; acc: 0.36
Batch: 700; loss: 2.05; acc: 0.33
Batch: 720; loss: 1.96; acc: 0.34
Batch: 740; loss: 1.98; acc: 0.45
Batch: 760; loss: 1.9; acc: 0.48
Batch: 780; loss: 1.99; acc: 0.31
Train Epoch over. train_loss: 2.02; train_accuracy: 0.33 

2.909590875788126e-05
4.840762358071515e-06
Batch: 0; loss: 1.9; acc: 0.44
Batch: 20; loss: 2.13; acc: 0.23
Batch: 40; loss: 1.89; acc: 0.42
Batch: 60; loss: 1.85; acc: 0.44
Batch: 80; loss: 1.92; acc: 0.44
Batch: 100; loss: 1.97; acc: 0.41
Batch: 120; loss: 1.95; acc: 0.41
Batch: 140; loss: 1.97; acc: 0.36
Val Epoch over. val_loss: 1.9662692357020772; val_accuracy: 0.3665406050955414 

The current subspace-distance is: 4.840762358071515e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.0; acc: 0.41
Batch: 20; loss: 1.94; acc: 0.39
Batch: 40; loss: 2.09; acc: 0.27
Batch: 60; loss: 2.0; acc: 0.36
Batch: 80; loss: 1.95; acc: 0.47
Batch: 100; loss: 1.9; acc: 0.5
Batch: 120; loss: 1.97; acc: 0.34
Batch: 140; loss: 1.99; acc: 0.38
Batch: 160; loss: 1.93; acc: 0.39
Batch: 180; loss: 2.0; acc: 0.31
Batch: 200; loss: 2.03; acc: 0.31
Batch: 220; loss: 2.2; acc: 0.2
Batch: 240; loss: 1.9; acc: 0.42
Batch: 260; loss: 1.94; acc: 0.39
Batch: 280; loss: 1.97; acc: 0.36
Batch: 300; loss: 2.0; acc: 0.39
Batch: 320; loss: 2.05; acc: 0.28
Batch: 340; loss: 2.07; acc: 0.31
Batch: 360; loss: 2.02; acc: 0.38
Batch: 380; loss: 2.0; acc: 0.34
Batch: 400; loss: 2.02; acc: 0.3
Batch: 420; loss: 1.94; acc: 0.36
Batch: 440; loss: 1.94; acc: 0.47
Batch: 460; loss: 1.97; acc: 0.36
Batch: 480; loss: 2.06; acc: 0.36
Batch: 500; loss: 2.02; acc: 0.3
Batch: 520; loss: 2.08; acc: 0.36
Batch: 540; loss: 1.94; acc: 0.45
Batch: 560; loss: 1.94; acc: 0.31
Batch: 580; loss: 1.97; acc: 0.38
Batch: 600; loss: 2.01; acc: 0.36
Batch: 620; loss: 2.01; acc: 0.34
Batch: 640; loss: 2.03; acc: 0.34
Batch: 660; loss: 2.13; acc: 0.25
Batch: 680; loss: 1.97; acc: 0.31
Batch: 700; loss: 1.98; acc: 0.38
Batch: 720; loss: 2.04; acc: 0.34
Batch: 740; loss: 2.05; acc: 0.27
Batch: 760; loss: 1.96; acc: 0.39
Batch: 780; loss: 2.0; acc: 0.34
Train Epoch over. train_loss: 2.0; train_accuracy: 0.34 

2.9456743504852057e-05
4.276190793461865e-06
Batch: 0; loss: 1.9; acc: 0.44
Batch: 20; loss: 2.17; acc: 0.28
Batch: 40; loss: 1.87; acc: 0.45
Batch: 60; loss: 1.85; acc: 0.48
Batch: 80; loss: 1.92; acc: 0.41
Batch: 100; loss: 1.97; acc: 0.36
Batch: 120; loss: 1.94; acc: 0.41
Batch: 140; loss: 1.96; acc: 0.41
Val Epoch over. val_loss: 1.9622411097690557; val_accuracy: 0.38326035031847133 

The current subspace-distance is: 4.276190793461865e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 2.02; acc: 0.23
Batch: 40; loss: 1.96; acc: 0.36
Batch: 60; loss: 1.96; acc: 0.38
Batch: 80; loss: 1.99; acc: 0.3
Batch: 100; loss: 2.11; acc: 0.28
Batch: 120; loss: 2.08; acc: 0.25
Batch: 140; loss: 1.99; acc: 0.34
Batch: 160; loss: 2.01; acc: 0.31
Batch: 180; loss: 2.01; acc: 0.41
Batch: 200; loss: 1.99; acc: 0.41
Batch: 220; loss: 2.02; acc: 0.41
Batch: 240; loss: 2.02; acc: 0.34
Batch: 260; loss: 2.04; acc: 0.31
Batch: 280; loss: 2.1; acc: 0.23
Batch: 300; loss: 1.92; acc: 0.38
Batch: 320; loss: 1.97; acc: 0.34
Batch: 340; loss: 2.08; acc: 0.27
Batch: 360; loss: 1.98; acc: 0.33
Batch: 380; loss: 1.98; acc: 0.39
Batch: 400; loss: 1.91; acc: 0.42
Batch: 420; loss: 2.02; acc: 0.47
Batch: 440; loss: 2.04; acc: 0.25
Batch: 460; loss: 1.93; acc: 0.36
Batch: 480; loss: 2.07; acc: 0.33
Batch: 500; loss: 1.89; acc: 0.42
Batch: 520; loss: 1.91; acc: 0.45
Batch: 540; loss: 2.01; acc: 0.41
Batch: 560; loss: 1.88; acc: 0.44
Batch: 580; loss: 2.01; acc: 0.33
Batch: 600; loss: 1.97; acc: 0.34
Batch: 620; loss: 2.04; acc: 0.33
Batch: 640; loss: 1.92; acc: 0.41
Batch: 660; loss: 2.06; acc: 0.25
Batch: 680; loss: 2.02; acc: 0.39
Batch: 700; loss: 1.95; acc: 0.39
Batch: 720; loss: 2.02; acc: 0.3
Batch: 740; loss: 1.9; acc: 0.44
Batch: 760; loss: 1.92; acc: 0.45
Batch: 780; loss: 2.03; acc: 0.33
Train Epoch over. train_loss: 1.99; train_accuracy: 0.35 

2.9344895665417425e-05
5.183399480301887e-06
Batch: 0; loss: 1.92; acc: 0.44
Batch: 20; loss: 2.17; acc: 0.22
Batch: 40; loss: 1.92; acc: 0.42
Batch: 60; loss: 1.85; acc: 0.5
Batch: 80; loss: 1.97; acc: 0.39
Batch: 100; loss: 1.98; acc: 0.36
Batch: 120; loss: 1.94; acc: 0.41
Batch: 140; loss: 1.98; acc: 0.38
Val Epoch over. val_loss: 1.977646893756405; val_accuracy: 0.37480095541401276 

The current subspace-distance is: 5.183399480301887e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.0; acc: 0.39
Batch: 20; loss: 1.91; acc: 0.41
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 2.02; acc: 0.33
Batch: 80; loss: 1.92; acc: 0.33
Batch: 100; loss: 2.05; acc: 0.31
Batch: 120; loss: 1.96; acc: 0.45
Batch: 140; loss: 1.99; acc: 0.31
Batch: 160; loss: 1.95; acc: 0.36
Batch: 180; loss: 2.02; acc: 0.38
Batch: 200; loss: 1.96; acc: 0.36
Batch: 220; loss: 2.02; acc: 0.33
Batch: 240; loss: 2.03; acc: 0.41
Batch: 260; loss: 2.12; acc: 0.28
Batch: 280; loss: 1.89; acc: 0.41
Batch: 300; loss: 1.95; acc: 0.34
Batch: 320; loss: 1.97; acc: 0.31
Batch: 340; loss: 2.09; acc: 0.23
Batch: 360; loss: 1.92; acc: 0.38
Batch: 380; loss: 2.03; acc: 0.42
Batch: 400; loss: 2.03; acc: 0.33
Batch: 420; loss: 2.03; acc: 0.38
Batch: 440; loss: 2.07; acc: 0.23
Batch: 460; loss: 2.06; acc: 0.27
Batch: 480; loss: 2.04; acc: 0.27
Batch: 500; loss: 1.89; acc: 0.48
Batch: 520; loss: 1.98; acc: 0.34
Batch: 540; loss: 1.95; acc: 0.41
Batch: 560; loss: 2.01; acc: 0.25
Batch: 580; loss: 1.94; acc: 0.48
Batch: 600; loss: 1.98; acc: 0.39
Batch: 620; loss: 1.9; acc: 0.44
Batch: 640; loss: 2.02; acc: 0.38
Batch: 660; loss: 2.1; acc: 0.25
Batch: 680; loss: 2.06; acc: 0.27
Batch: 700; loss: 2.0; acc: 0.34
Batch: 720; loss: 2.03; acc: 0.31
Batch: 740; loss: 2.02; acc: 0.33
Batch: 760; loss: 1.95; acc: 0.42
Batch: 780; loss: 1.98; acc: 0.39
Train Epoch over. train_loss: 1.99; train_accuracy: 0.36 

2.8866741558886133e-05
4.963419087289367e-06
Batch: 0; loss: 1.92; acc: 0.45
Batch: 20; loss: 2.12; acc: 0.3
Batch: 40; loss: 1.91; acc: 0.45
Batch: 60; loss: 1.8; acc: 0.55
Batch: 80; loss: 1.9; acc: 0.44
Batch: 100; loss: 1.93; acc: 0.48
Batch: 120; loss: 1.95; acc: 0.38
Batch: 140; loss: 1.94; acc: 0.47
Val Epoch over. val_loss: 1.9581546487322279; val_accuracy: 0.39988057324840764 

The current subspace-distance is: 4.963419087289367e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.85; acc: 0.5
Batch: 20; loss: 1.9; acc: 0.41
Batch: 40; loss: 1.94; acc: 0.41
Batch: 60; loss: 2.03; acc: 0.44
Batch: 80; loss: 1.92; acc: 0.41
Batch: 100; loss: 1.99; acc: 0.34
Batch: 120; loss: 1.88; acc: 0.44
Batch: 140; loss: 1.99; acc: 0.36
Batch: 160; loss: 1.93; acc: 0.42
Batch: 180; loss: 2.0; acc: 0.36
Batch: 200; loss: 1.95; acc: 0.31
Batch: 220; loss: 1.94; acc: 0.5
Batch: 240; loss: 1.95; acc: 0.33
Batch: 260; loss: 1.92; acc: 0.39
Batch: 280; loss: 1.88; acc: 0.39
Batch: 300; loss: 1.93; acc: 0.42
Batch: 320; loss: 1.98; acc: 0.39
Batch: 340; loss: 1.95; acc: 0.38
Batch: 360; loss: 2.0; acc: 0.33
Batch: 380; loss: 1.94; acc: 0.38
Batch: 400; loss: 2.06; acc: 0.28
Batch: 420; loss: 1.94; acc: 0.47
Batch: 440; loss: 2.1; acc: 0.3
Batch: 460; loss: 1.91; acc: 0.39
Batch: 480; loss: 2.07; acc: 0.33
Batch: 500; loss: 1.94; acc: 0.44
Batch: 520; loss: 2.0; acc: 0.3
Batch: 540; loss: 1.86; acc: 0.45
Batch: 560; loss: 1.89; acc: 0.41
Batch: 580; loss: 1.98; acc: 0.33
Batch: 600; loss: 2.03; acc: 0.45
Batch: 620; loss: 1.92; acc: 0.44
Batch: 640; loss: 1.87; acc: 0.42
Batch: 660; loss: 2.0; acc: 0.41
Batch: 680; loss: 1.99; acc: 0.34
Batch: 700; loss: 1.97; acc: 0.34
Batch: 720; loss: 1.9; acc: 0.36
Batch: 740; loss: 2.11; acc: 0.28
Batch: 760; loss: 1.91; acc: 0.44
Batch: 780; loss: 1.98; acc: 0.34
Train Epoch over. train_loss: 1.97; train_accuracy: 0.37 

3.0016210075700656e-05
5.129275905346731e-06
Batch: 0; loss: 1.89; acc: 0.45
Batch: 20; loss: 2.07; acc: 0.39
Batch: 40; loss: 1.86; acc: 0.47
Batch: 60; loss: 1.82; acc: 0.53
Batch: 80; loss: 1.88; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.42
Batch: 120; loss: 1.92; acc: 0.42
Batch: 140; loss: 1.95; acc: 0.36
Val Epoch over. val_loss: 1.9389762886010917; val_accuracy: 0.39928343949044587 

The current subspace-distance is: 5.129275905346731e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.98; acc: 0.39
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.95; acc: 0.36
Batch: 60; loss: 1.92; acc: 0.38
Batch: 80; loss: 2.07; acc: 0.28
Batch: 100; loss: 2.0; acc: 0.31
Batch: 120; loss: 1.94; acc: 0.28
Batch: 140; loss: 2.05; acc: 0.36
Batch: 160; loss: 1.93; acc: 0.42
Batch: 180; loss: 1.92; acc: 0.44
Batch: 200; loss: 2.0; acc: 0.34
Batch: 220; loss: 1.95; acc: 0.38
Batch: 240; loss: 1.94; acc: 0.31
Batch: 260; loss: 1.85; acc: 0.44
Batch: 280; loss: 1.95; acc: 0.34
Batch: 300; loss: 2.01; acc: 0.38
Batch: 320; loss: 1.95; acc: 0.34
Batch: 340; loss: 1.99; acc: 0.31
Batch: 360; loss: 1.94; acc: 0.39
Batch: 380; loss: 1.9; acc: 0.39
Batch: 400; loss: 1.95; acc: 0.38
Batch: 420; loss: 1.98; acc: 0.27
Batch: 440; loss: 1.89; acc: 0.45
Batch: 460; loss: 1.93; acc: 0.39
Batch: 480; loss: 1.9; acc: 0.42
Batch: 500; loss: 1.81; acc: 0.45
Batch: 520; loss: 1.94; acc: 0.39
Batch: 540; loss: 1.97; acc: 0.34
Batch: 560; loss: 1.83; acc: 0.42
Batch: 580; loss: 1.92; acc: 0.44
Batch: 600; loss: 2.01; acc: 0.28
Batch: 620; loss: 1.85; acc: 0.38
Batch: 640; loss: 2.04; acc: 0.34
Batch: 660; loss: 1.9; acc: 0.42
Batch: 680; loss: 1.96; acc: 0.38
Batch: 700; loss: 1.87; acc: 0.39
Batch: 720; loss: 1.91; acc: 0.44
Batch: 740; loss: 1.94; acc: 0.38
Batch: 760; loss: 1.97; acc: 0.28
Batch: 780; loss: 1.99; acc: 0.31
Train Epoch over. train_loss: 1.94; train_accuracy: 0.37 

2.928701178461779e-05
5.573272574110888e-06
Batch: 0; loss: 1.84; acc: 0.41
Batch: 20; loss: 2.09; acc: 0.34
Batch: 40; loss: 1.81; acc: 0.58
Batch: 60; loss: 1.85; acc: 0.47
Batch: 80; loss: 1.84; acc: 0.45
Batch: 100; loss: 1.93; acc: 0.38
Batch: 120; loss: 1.92; acc: 0.44
Batch: 140; loss: 1.87; acc: 0.47
Val Epoch over. val_loss: 1.9124591639087458; val_accuracy: 0.40505573248407645 

The current subspace-distance is: 5.573272574110888e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.98; acc: 0.34
Batch: 20; loss: 1.84; acc: 0.41
Batch: 40; loss: 1.88; acc: 0.42
Batch: 60; loss: 2.02; acc: 0.36
Batch: 80; loss: 1.92; acc: 0.39
Batch: 100; loss: 1.91; acc: 0.34
Batch: 120; loss: 1.9; acc: 0.45
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 1.81; acc: 0.39
Batch: 180; loss: 1.89; acc: 0.36
Batch: 200; loss: 1.95; acc: 0.34
Batch: 220; loss: 1.97; acc: 0.45
Batch: 240; loss: 1.84; acc: 0.44
Batch: 260; loss: 1.89; acc: 0.41
Batch: 280; loss: 2.03; acc: 0.27
Batch: 300; loss: 1.89; acc: 0.41
Batch: 320; loss: 2.02; acc: 0.34
Batch: 340; loss: 1.96; acc: 0.31
Batch: 360; loss: 2.01; acc: 0.33
Batch: 380; loss: 1.99; acc: 0.34
Batch: 400; loss: 1.95; acc: 0.39
Batch: 420; loss: 1.91; acc: 0.44
Batch: 440; loss: 1.93; acc: 0.39
Batch: 460; loss: 1.85; acc: 0.42
Batch: 480; loss: 1.94; acc: 0.3
Batch: 500; loss: 1.94; acc: 0.45
Batch: 520; loss: 1.88; acc: 0.38
Batch: 540; loss: 2.02; acc: 0.36
Batch: 560; loss: 1.84; acc: 0.48
Batch: 580; loss: 1.97; acc: 0.39
Batch: 600; loss: 1.91; acc: 0.39
Batch: 620; loss: 1.97; acc: 0.38
Batch: 640; loss: 1.97; acc: 0.31
Batch: 660; loss: 2.05; acc: 0.33
Batch: 680; loss: 1.89; acc: 0.31
Batch: 700; loss: 1.91; acc: 0.38
Batch: 720; loss: 1.97; acc: 0.39
Batch: 740; loss: 1.94; acc: 0.33
Batch: 760; loss: 1.87; acc: 0.3
Batch: 780; loss: 1.92; acc: 0.33
Train Epoch over. train_loss: 1.92; train_accuracy: 0.38 

2.9700269806198776e-05
5.598771622317145e-06
Batch: 0; loss: 1.85; acc: 0.44
Batch: 20; loss: 2.12; acc: 0.31
Batch: 40; loss: 1.78; acc: 0.56
Batch: 60; loss: 1.86; acc: 0.45
Batch: 80; loss: 1.8; acc: 0.47
Batch: 100; loss: 1.89; acc: 0.39
Batch: 120; loss: 1.93; acc: 0.34
Batch: 140; loss: 1.82; acc: 0.52
Val Epoch over. val_loss: 1.8982582297294763; val_accuracy: 0.39888535031847133 

The current subspace-distance is: 5.598771622317145e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.01; acc: 0.39
Batch: 20; loss: 1.92; acc: 0.36
Batch: 40; loss: 2.08; acc: 0.31
Batch: 60; loss: 1.85; acc: 0.39
Batch: 80; loss: 2.05; acc: 0.3
Batch: 100; loss: 1.8; acc: 0.41
Batch: 120; loss: 1.94; acc: 0.39
Batch: 140; loss: 1.93; acc: 0.38
Batch: 160; loss: 1.85; acc: 0.42
Batch: 180; loss: 1.96; acc: 0.39
Batch: 200; loss: 1.99; acc: 0.31
Batch: 220; loss: 1.91; acc: 0.41
Batch: 240; loss: 2.05; acc: 0.23
Batch: 260; loss: 1.98; acc: 0.31
Batch: 280; loss: 1.93; acc: 0.36
Batch: 300; loss: 1.92; acc: 0.42
Batch: 320; loss: 1.77; acc: 0.48
Batch: 340; loss: 1.87; acc: 0.44
Batch: 360; loss: 1.92; acc: 0.33
Batch: 380; loss: 1.95; acc: 0.3
Batch: 400; loss: 1.87; acc: 0.47
Batch: 420; loss: 1.87; acc: 0.39
Batch: 440; loss: 2.01; acc: 0.33
Batch: 460; loss: 1.88; acc: 0.41
Batch: 480; loss: 1.85; acc: 0.42
Batch: 500; loss: 2.04; acc: 0.28
Batch: 520; loss: 1.89; acc: 0.48
Batch: 540; loss: 1.87; acc: 0.39
Batch: 560; loss: 1.85; acc: 0.38
Batch: 580; loss: 1.96; acc: 0.33
Batch: 600; loss: 1.85; acc: 0.44
Batch: 620; loss: 1.94; acc: 0.39
Batch: 640; loss: 1.99; acc: 0.34
Batch: 660; loss: 1.89; acc: 0.27
Batch: 680; loss: 1.91; acc: 0.41
Batch: 700; loss: 1.9; acc: 0.34
Batch: 720; loss: 1.92; acc: 0.34
Batch: 740; loss: 1.94; acc: 0.36
Batch: 760; loss: 1.96; acc: 0.33
Batch: 780; loss: 1.98; acc: 0.36
Train Epoch over. train_loss: 1.91; train_accuracy: 0.38 

3.145271330140531e-05
6.319162366708042e-06
Batch: 0; loss: 1.85; acc: 0.39
Batch: 20; loss: 2.1; acc: 0.3
Batch: 40; loss: 1.76; acc: 0.56
Batch: 60; loss: 1.84; acc: 0.5
Batch: 80; loss: 1.78; acc: 0.44
Batch: 100; loss: 1.89; acc: 0.42
Batch: 120; loss: 1.94; acc: 0.34
Batch: 140; loss: 1.78; acc: 0.48
Val Epoch over. val_loss: 1.8824202672691102; val_accuracy: 0.4007762738853503 

The current subspace-distance is: 6.319162366708042e-06 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.88; acc: 0.48
Batch: 20; loss: 2.05; acc: 0.27
Batch: 40; loss: 1.94; acc: 0.39
Batch: 60; loss: 2.0; acc: 0.27
Batch: 80; loss: 2.02; acc: 0.2
Batch: 100; loss: 1.82; acc: 0.42
Batch: 120; loss: 1.85; acc: 0.38
Batch: 140; loss: 1.97; acc: 0.33
Batch: 160; loss: 1.94; acc: 0.28
Batch: 180; loss: 1.82; acc: 0.38
Batch: 200; loss: 1.84; acc: 0.36
Batch: 220; loss: 1.94; acc: 0.34
Batch: 240; loss: 1.76; acc: 0.5
Batch: 260; loss: 1.89; acc: 0.42
Batch: 280; loss: 2.0; acc: 0.3
Batch: 300; loss: 1.91; acc: 0.36
Batch: 320; loss: 1.87; acc: 0.44
Batch: 340; loss: 1.85; acc: 0.47
Batch: 360; loss: 1.72; acc: 0.48
Batch: 380; loss: 1.98; acc: 0.36
Batch: 400; loss: 1.91; acc: 0.33
Batch: 420; loss: 1.95; acc: 0.39
Batch: 440; loss: 1.87; acc: 0.38
Batch: 460; loss: 1.86; acc: 0.45
Batch: 480; loss: 1.88; acc: 0.36
Batch: 500; loss: 1.86; acc: 0.42
Batch: 520; loss: 2.03; acc: 0.22
Batch: 540; loss: 1.87; acc: 0.5
Batch: 560; loss: 1.94; acc: 0.36
Batch: 580; loss: 1.87; acc: 0.42
Batch: 600; loss: 1.98; acc: 0.31
Batch: 620; loss: 1.91; acc: 0.31
Batch: 640; loss: 1.84; acc: 0.39
Batch: 660; loss: 1.77; acc: 0.39
Batch: 680; loss: 1.95; acc: 0.36
Batch: 700; loss: 1.96; acc: 0.39
Batch: 720; loss: 1.86; acc: 0.44
Batch: 740; loss: 1.83; acc: 0.48
Batch: 760; loss: 1.9; acc: 0.31
Batch: 780; loss: 1.83; acc: 0.36
Train Epoch over. train_loss: 1.9; train_accuracy: 0.38 

3.213137460988946e-05
7.232618827401893e-06
Batch: 0; loss: 1.85; acc: 0.45
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.75; acc: 0.59
Batch: 60; loss: 1.82; acc: 0.47
Batch: 80; loss: 1.75; acc: 0.5
Batch: 100; loss: 1.85; acc: 0.5
Batch: 120; loss: 1.93; acc: 0.36
Batch: 140; loss: 1.75; acc: 0.52
Val Epoch over. val_loss: 1.8797507939065339; val_accuracy: 0.39729299363057324 

The current subspace-distance is: 7.232618827401893e-06 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.84; acc: 0.41
Batch: 20; loss: 1.97; acc: 0.31
Batch: 40; loss: 1.81; acc: 0.41
Batch: 60; loss: 1.91; acc: 0.31
Batch: 80; loss: 1.98; acc: 0.38
Batch: 100; loss: 1.85; acc: 0.44
Batch: 120; loss: 1.9; acc: 0.38
Batch: 140; loss: 1.8; acc: 0.41
Batch: 160; loss: 1.93; acc: 0.33
Batch: 180; loss: 1.84; acc: 0.45
Batch: 200; loss: 1.93; acc: 0.41
Batch: 220; loss: 1.96; acc: 0.36
Batch: 240; loss: 1.83; acc: 0.39
Batch: 260; loss: 1.81; acc: 0.5
Batch: 280; loss: 1.94; acc: 0.39
Batch: 300; loss: 1.8; acc: 0.47
Batch: 320; loss: 1.83; acc: 0.41
Batch: 340; loss: 1.93; acc: 0.36
Batch: 360; loss: 1.94; acc: 0.31
Batch: 380; loss: 1.82; acc: 0.41
Batch: 400; loss: 1.88; acc: 0.42
Batch: 420; loss: 1.94; acc: 0.31
Batch: 440; loss: 1.88; acc: 0.38
Batch: 460; loss: 1.93; acc: 0.38
Batch: 480; loss: 1.86; acc: 0.39
Batch: 500; loss: 1.81; acc: 0.5
Batch: 520; loss: 1.96; acc: 0.38
Batch: 540; loss: 1.97; acc: 0.36
Batch: 560; loss: 1.91; acc: 0.28
Batch: 580; loss: 1.85; acc: 0.36
Batch: 600; loss: 1.95; acc: 0.42
Batch: 620; loss: 1.78; acc: 0.52
Batch: 640; loss: 1.87; acc: 0.44
Batch: 660; loss: 1.87; acc: 0.44
Batch: 680; loss: 1.84; acc: 0.3
Batch: 700; loss: 1.99; acc: 0.34
Batch: 720; loss: 1.81; acc: 0.52
Batch: 740; loss: 1.92; acc: 0.42
Batch: 760; loss: 1.8; acc: 0.48
Batch: 780; loss: 1.93; acc: 0.27
Train Epoch over. train_loss: 1.9; train_accuracy: 0.38 

3.180510611855425e-05
7.380012903013267e-06
Batch: 0; loss: 1.88; acc: 0.34
Batch: 20; loss: 2.05; acc: 0.33
Batch: 40; loss: 1.76; acc: 0.52
Batch: 60; loss: 1.81; acc: 0.53
Batch: 80; loss: 1.75; acc: 0.48
Batch: 100; loss: 1.88; acc: 0.45
Batch: 120; loss: 1.94; acc: 0.38
Batch: 140; loss: 1.75; acc: 0.48
Val Epoch over. val_loss: 1.8737283011150967; val_accuracy: 0.39341162420382164 

The current subspace-distance is: 7.380012903013267e-06 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.79; acc: 0.48
Batch: 20; loss: 1.97; acc: 0.33
Batch: 40; loss: 2.07; acc: 0.22
Batch: 60; loss: 1.79; acc: 0.5
Batch: 80; loss: 1.83; acc: 0.39
Batch: 100; loss: 2.0; acc: 0.3
Batch: 120; loss: 2.03; acc: 0.28
Batch: 140; loss: 1.87; acc: 0.39
Batch: 160; loss: 1.97; acc: 0.36
Batch: 180; loss: 2.02; acc: 0.3
Batch: 200; loss: 2.0; acc: 0.38
Batch: 220; loss: 1.83; acc: 0.38
Batch: 240; loss: 1.82; acc: 0.45
Batch: 260; loss: 1.84; acc: 0.42
Batch: 280; loss: 1.89; acc: 0.42
Batch: 300; loss: 1.85; acc: 0.36
Batch: 320; loss: 1.89; acc: 0.39
Batch: 340; loss: 1.88; acc: 0.38
Batch: 360; loss: 1.75; acc: 0.39
Batch: 380; loss: 1.94; acc: 0.39
Batch: 400; loss: 1.79; acc: 0.41
Batch: 420; loss: 1.91; acc: 0.38
Batch: 440; loss: 1.92; acc: 0.36
Batch: 460; loss: 1.93; acc: 0.33
Batch: 480; loss: 1.81; acc: 0.47
Batch: 500; loss: 1.8; acc: 0.48
Batch: 520; loss: 1.81; acc: 0.47
Batch: 540; loss: 1.91; acc: 0.33
Batch: 560; loss: 1.78; acc: 0.48
Batch: 580; loss: 2.0; acc: 0.3
Batch: 600; loss: 1.93; acc: 0.34
Batch: 620; loss: 1.78; acc: 0.44
Batch: 640; loss: 2.02; acc: 0.2
Batch: 660; loss: 1.91; acc: 0.39
Batch: 680; loss: 1.83; acc: 0.38
Batch: 700; loss: 1.84; acc: 0.41
Batch: 720; loss: 1.95; acc: 0.3
Batch: 740; loss: 1.92; acc: 0.34
Batch: 760; loss: 2.02; acc: 0.27
Batch: 780; loss: 1.95; acc: 0.33
Train Epoch over. train_loss: 1.89; train_accuracy: 0.38 

3.168961848132312e-05
6.506776571768569e-06
Batch: 0; loss: 1.87; acc: 0.34
Batch: 20; loss: 2.06; acc: 0.3
Batch: 40; loss: 1.75; acc: 0.53
Batch: 60; loss: 1.79; acc: 0.47
Batch: 80; loss: 1.73; acc: 0.48
Batch: 100; loss: 1.85; acc: 0.47
Batch: 120; loss: 1.94; acc: 0.36
Batch: 140; loss: 1.74; acc: 0.5
Val Epoch over. val_loss: 1.8665475298644632; val_accuracy: 0.3958001592356688 

The current subspace-distance is: 6.506776571768569e-06 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.87; acc: 0.44
Batch: 20; loss: 1.82; acc: 0.47
Batch: 40; loss: 1.87; acc: 0.41
Batch: 60; loss: 1.8; acc: 0.48
Batch: 80; loss: 1.84; acc: 0.38
Batch: 100; loss: 1.92; acc: 0.41
Batch: 120; loss: 1.88; acc: 0.38
Batch: 140; loss: 2.01; acc: 0.31
Batch: 160; loss: 1.92; acc: 0.38
Batch: 180; loss: 1.85; acc: 0.34
Batch: 200; loss: 1.9; acc: 0.39
Batch: 220; loss: 1.82; acc: 0.47
Batch: 240; loss: 1.87; acc: 0.44
Batch: 260; loss: 1.83; acc: 0.36
Batch: 280; loss: 1.85; acc: 0.34
Batch: 300; loss: 1.9; acc: 0.38
Batch: 320; loss: 1.93; acc: 0.41
Batch: 340; loss: 2.02; acc: 0.3
Batch: 360; loss: 1.79; acc: 0.47
Batch: 380; loss: 1.91; acc: 0.3
Batch: 400; loss: 1.96; acc: 0.36
Batch: 420; loss: 1.91; acc: 0.34
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.85; acc: 0.44
Batch: 480; loss: 1.83; acc: 0.47
Batch: 500; loss: 1.84; acc: 0.44
Batch: 520; loss: 1.84; acc: 0.44
Batch: 540; loss: 1.97; acc: 0.39
Batch: 560; loss: 1.75; acc: 0.47
Batch: 580; loss: 1.81; acc: 0.48
Batch: 600; loss: 1.8; acc: 0.5
Batch: 620; loss: 1.85; acc: 0.38
Batch: 640; loss: 1.93; acc: 0.27
Batch: 660; loss: 2.0; acc: 0.28
Batch: 680; loss: 1.98; acc: 0.36
Batch: 700; loss: 1.98; acc: 0.36
Batch: 720; loss: 1.84; acc: 0.44
Batch: 740; loss: 1.9; acc: 0.34
Batch: 760; loss: 1.82; acc: 0.44
Batch: 780; loss: 1.86; acc: 0.47
Train Epoch over. train_loss: 1.89; train_accuracy: 0.38 

3.214485332136974e-05
7.03085288478178e-06
Batch: 0; loss: 1.89; acc: 0.33
Batch: 20; loss: 2.03; acc: 0.31
Batch: 40; loss: 1.76; acc: 0.53
Batch: 60; loss: 1.8; acc: 0.53
Batch: 80; loss: 1.73; acc: 0.45
Batch: 100; loss: 1.86; acc: 0.48
Batch: 120; loss: 1.95; acc: 0.36
Batch: 140; loss: 1.74; acc: 0.5
Val Epoch over. val_loss: 1.8687433095494652; val_accuracy: 0.39440684713375795 

The current subspace-distance is: 7.03085288478178e-06 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.91; acc: 0.42
Batch: 40; loss: 1.86; acc: 0.42
Batch: 60; loss: 1.86; acc: 0.42
Batch: 80; loss: 1.85; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.91; acc: 0.36
Batch: 140; loss: 1.79; acc: 0.36
Batch: 160; loss: 1.81; acc: 0.48
Batch: 180; loss: 1.81; acc: 0.52
Batch: 200; loss: 1.9; acc: 0.36
Batch: 220; loss: 1.91; acc: 0.39
Batch: 240; loss: 1.86; acc: 0.41
Batch: 260; loss: 1.93; acc: 0.33
Batch: 280; loss: 1.95; acc: 0.31
Batch: 300; loss: 1.75; acc: 0.47
Batch: 320; loss: 1.78; acc: 0.5
Batch: 340; loss: 1.88; acc: 0.3
Batch: 360; loss: 1.87; acc: 0.44
Batch: 380; loss: 2.08; acc: 0.28
Batch: 400; loss: 1.86; acc: 0.38
Batch: 420; loss: 1.85; acc: 0.45
Batch: 440; loss: 1.85; acc: 0.41
Batch: 460; loss: 1.84; acc: 0.45
Batch: 480; loss: 1.87; acc: 0.38
Batch: 500; loss: 1.93; acc: 0.31
Batch: 520; loss: 1.81; acc: 0.42
Batch: 540; loss: 1.99; acc: 0.28
Batch: 560; loss: 1.95; acc: 0.33
Batch: 580; loss: 2.04; acc: 0.27
Batch: 600; loss: 1.77; acc: 0.42
Batch: 620; loss: 1.98; acc: 0.27
Batch: 640; loss: 2.07; acc: 0.31
Batch: 660; loss: 1.87; acc: 0.31
Batch: 680; loss: 1.92; acc: 0.3
Batch: 700; loss: 1.91; acc: 0.28
Batch: 720; loss: 1.93; acc: 0.34
Batch: 740; loss: 1.85; acc: 0.39
Batch: 760; loss: 1.82; acc: 0.42
Batch: 780; loss: 1.74; acc: 0.53
Train Epoch over. train_loss: 1.89; train_accuracy: 0.38 

3.205764005542733e-05
6.997584932832979e-06
Batch: 0; loss: 1.89; acc: 0.31
Batch: 20; loss: 2.04; acc: 0.33
Batch: 40; loss: 1.74; acc: 0.52
Batch: 60; loss: 1.79; acc: 0.5
Batch: 80; loss: 1.74; acc: 0.48
Batch: 100; loss: 1.85; acc: 0.5
Batch: 120; loss: 1.96; acc: 0.36
Batch: 140; loss: 1.74; acc: 0.45
Val Epoch over. val_loss: 1.870613609909252; val_accuracy: 0.3940087579617834 

The current subspace-distance is: 6.997584932832979e-06 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.9; acc: 0.39
Batch: 20; loss: 1.83; acc: 0.45
Batch: 40; loss: 1.82; acc: 0.44
Batch: 60; loss: 1.92; acc: 0.36
Batch: 80; loss: 1.91; acc: 0.3
Batch: 100; loss: 1.92; acc: 0.36
Batch: 120; loss: 2.0; acc: 0.3
Batch: 140; loss: 1.95; acc: 0.36
Batch: 160; loss: 1.88; acc: 0.44
Batch: 180; loss: 1.88; acc: 0.3
Batch: 200; loss: 1.95; acc: 0.31
Batch: 220; loss: 1.93; acc: 0.36
Batch: 240; loss: 1.96; acc: 0.38
Batch: 260; loss: 1.86; acc: 0.47
Batch: 280; loss: 2.01; acc: 0.33
Batch: 300; loss: 1.91; acc: 0.33
Batch: 320; loss: 1.87; acc: 0.41
Batch: 340; loss: 1.85; acc: 0.34
Batch: 360; loss: 1.86; acc: 0.42
Batch: 380; loss: 1.81; acc: 0.42
Batch: 400; loss: 1.88; acc: 0.34
Batch: 420; loss: 1.95; acc: 0.3
Batch: 440; loss: 1.82; acc: 0.48
Batch: 460; loss: 1.93; acc: 0.34
Batch: 480; loss: 1.87; acc: 0.38
Batch: 500; loss: 1.81; acc: 0.41
Batch: 520; loss: 1.96; acc: 0.33
Batch: 540; loss: 1.8; acc: 0.48
Batch: 560; loss: 1.81; acc: 0.41
Batch: 580; loss: 1.82; acc: 0.45
Batch: 600; loss: 1.84; acc: 0.38
Batch: 620; loss: 1.97; acc: 0.34
Batch: 640; loss: 1.84; acc: 0.38
Batch: 660; loss: 1.93; acc: 0.36
Batch: 680; loss: 2.02; acc: 0.33
Batch: 700; loss: 2.09; acc: 0.33
Batch: 720; loss: 1.95; acc: 0.28
Batch: 740; loss: 1.87; acc: 0.42
Batch: 760; loss: 1.95; acc: 0.39
Batch: 780; loss: 1.88; acc: 0.41
Train Epoch over. train_loss: 1.89; train_accuracy: 0.38 

3.248136636102572e-05
6.694401236018166e-06
Batch: 0; loss: 1.9; acc: 0.34
Batch: 20; loss: 2.04; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.52
Batch: 60; loss: 1.79; acc: 0.5
Batch: 80; loss: 1.74; acc: 0.47
Batch: 100; loss: 1.86; acc: 0.52
Batch: 120; loss: 1.96; acc: 0.38
Batch: 140; loss: 1.74; acc: 0.45
Val Epoch over. val_loss: 1.8725731463948632; val_accuracy: 0.39699442675159236 

The current subspace-distance is: 6.694401236018166e-06 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.87; acc: 0.34
Batch: 20; loss: 1.92; acc: 0.33
Batch: 40; loss: 1.84; acc: 0.44
Batch: 60; loss: 1.93; acc: 0.36
Batch: 80; loss: 1.86; acc: 0.41
Batch: 100; loss: 1.75; acc: 0.52
Batch: 120; loss: 1.9; acc: 0.36
Batch: 140; loss: 1.89; acc: 0.45
Batch: 160; loss: 1.87; acc: 0.5
Batch: 180; loss: 1.82; acc: 0.5
Batch: 200; loss: 1.85; acc: 0.39
Batch: 220; loss: 1.84; acc: 0.44
Batch: 240; loss: 1.98; acc: 0.31
Batch: 260; loss: 1.8; acc: 0.38
Batch: 280; loss: 1.87; acc: 0.39
Batch: 300; loss: 1.81; acc: 0.42
Batch: 320; loss: 1.82; acc: 0.41
Batch: 340; loss: 1.84; acc: 0.45
Batch: 360; loss: 2.0; acc: 0.27
Batch: 380; loss: 1.75; acc: 0.47
Batch: 400; loss: 1.77; acc: 0.52
Batch: 420; loss: 1.95; acc: 0.39
Batch: 440; loss: 1.86; acc: 0.38
Batch: 460; loss: 2.02; acc: 0.28
Batch: 480; loss: 1.84; acc: 0.36
Batch: 500; loss: 1.88; acc: 0.41
Batch: 520; loss: 1.84; acc: 0.42
Batch: 540; loss: 2.01; acc: 0.3
Batch: 560; loss: 1.84; acc: 0.42
Batch: 580; loss: 1.81; acc: 0.44
Batch: 600; loss: 2.0; acc: 0.33
Batch: 620; loss: 2.05; acc: 0.31
Batch: 640; loss: 1.9; acc: 0.34
Batch: 660; loss: 1.93; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.41
Batch: 700; loss: 1.94; acc: 0.3
Batch: 720; loss: 1.94; acc: 0.33
Batch: 740; loss: 1.84; acc: 0.44
Batch: 760; loss: 1.85; acc: 0.44
Batch: 780; loss: 1.93; acc: 0.3
Train Epoch over. train_loss: 1.89; train_accuracy: 0.38 

3.2273575925501063e-05
8.23699065222172e-06
Batch: 0; loss: 1.92; acc: 0.34
Batch: 20; loss: 2.03; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.5
Batch: 60; loss: 1.77; acc: 0.52
Batch: 80; loss: 1.72; acc: 0.56
Batch: 100; loss: 1.85; acc: 0.52
Batch: 120; loss: 1.96; acc: 0.36
Batch: 140; loss: 1.72; acc: 0.5
Val Epoch over. val_loss: 1.8657838759148957; val_accuracy: 0.3986863057324841 

The current subspace-distance is: 8.23699065222172e-06 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.92; acc: 0.41
Batch: 20; loss: 1.79; acc: 0.47
Batch: 40; loss: 2.11; acc: 0.25
Batch: 60; loss: 1.93; acc: 0.39
Batch: 80; loss: 1.86; acc: 0.42
Batch: 100; loss: 1.9; acc: 0.3
Batch: 120; loss: 1.82; acc: 0.47
Batch: 140; loss: 1.81; acc: 0.36
Batch: 160; loss: 1.87; acc: 0.48
Batch: 180; loss: 1.81; acc: 0.42
Batch: 200; loss: 1.9; acc: 0.41
Batch: 220; loss: 1.86; acc: 0.47
Batch: 240; loss: 1.86; acc: 0.44
Batch: 260; loss: 1.88; acc: 0.36
Batch: 280; loss: 1.89; acc: 0.42
Batch: 300; loss: 1.89; acc: 0.36
Batch: 320; loss: 1.88; acc: 0.44
Batch: 340; loss: 1.92; acc: 0.33
Batch: 360; loss: 1.91; acc: 0.38
Batch: 380; loss: 1.92; acc: 0.3
Batch: 400; loss: 1.93; acc: 0.34
Batch: 420; loss: 2.03; acc: 0.27
Batch: 440; loss: 1.92; acc: 0.36
Batch: 460; loss: 1.79; acc: 0.47
Batch: 480; loss: 2.01; acc: 0.28
Batch: 500; loss: 1.9; acc: 0.31
Batch: 520; loss: 1.81; acc: 0.53
Batch: 540; loss: 1.91; acc: 0.34
Batch: 560; loss: 1.93; acc: 0.33
Batch: 580; loss: 1.88; acc: 0.39
Batch: 600; loss: 1.84; acc: 0.36
Batch: 620; loss: 1.89; acc: 0.38
Batch: 640; loss: 1.79; acc: 0.45
Batch: 660; loss: 1.94; acc: 0.42
Batch: 680; loss: 1.86; acc: 0.41
Batch: 700; loss: 1.8; acc: 0.41
Batch: 720; loss: 1.87; acc: 0.38
Batch: 740; loss: 1.79; acc: 0.44
Batch: 760; loss: 1.79; acc: 0.48
Batch: 780; loss: 1.89; acc: 0.36
Train Epoch over. train_loss: 1.88; train_accuracy: 0.38 

3.246221604058519e-05
6.923112323420355e-06
Batch: 0; loss: 1.9; acc: 0.38
Batch: 20; loss: 2.01; acc: 0.39
Batch: 40; loss: 1.71; acc: 0.5
Batch: 60; loss: 1.74; acc: 0.47
Batch: 80; loss: 1.69; acc: 0.59
Batch: 100; loss: 1.8; acc: 0.52
Batch: 120; loss: 1.95; acc: 0.38
Batch: 140; loss: 1.71; acc: 0.53
Val Epoch over. val_loss: 1.8491137103669961; val_accuracy: 0.40764331210191085 

The current subspace-distance is: 6.923112323420355e-06 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.84; acc: 0.41
Batch: 20; loss: 1.94; acc: 0.33
Batch: 40; loss: 1.78; acc: 0.44
Batch: 60; loss: 1.76; acc: 0.45
Batch: 80; loss: 1.84; acc: 0.44
Batch: 100; loss: 1.81; acc: 0.41
Batch: 120; loss: 1.94; acc: 0.27
Batch: 140; loss: 1.98; acc: 0.22
Batch: 160; loss: 1.85; acc: 0.44
Batch: 180; loss: 1.92; acc: 0.36
Batch: 200; loss: 1.91; acc: 0.34
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.91; acc: 0.31
Batch: 260; loss: 1.88; acc: 0.38
Batch: 280; loss: 1.97; acc: 0.3
Batch: 300; loss: 1.79; acc: 0.42
Batch: 320; loss: 1.85; acc: 0.42
Batch: 340; loss: 1.82; acc: 0.42
Batch: 360; loss: 2.02; acc: 0.34
Batch: 380; loss: 2.02; acc: 0.31
Batch: 400; loss: 1.82; acc: 0.41
Batch: 420; loss: 1.95; acc: 0.31
Batch: 440; loss: 1.8; acc: 0.55
Batch: 460; loss: 2.02; acc: 0.27
Batch: 480; loss: 1.93; acc: 0.42
Batch: 500; loss: 1.73; acc: 0.5
Batch: 520; loss: 1.99; acc: 0.34
Batch: 540; loss: 1.77; acc: 0.41
Batch: 560; loss: 1.87; acc: 0.36
Batch: 580; loss: 1.9; acc: 0.33
Batch: 600; loss: 1.77; acc: 0.38
Batch: 620; loss: 1.85; acc: 0.39
Batch: 640; loss: 1.82; acc: 0.5
Batch: 660; loss: 1.83; acc: 0.38
Batch: 680; loss: 1.86; acc: 0.38
Batch: 700; loss: 1.79; acc: 0.39
Batch: 720; loss: 1.91; acc: 0.36
Batch: 740; loss: 1.86; acc: 0.38
Batch: 760; loss: 1.78; acc: 0.39
Batch: 780; loss: 1.92; acc: 0.38
Train Epoch over. train_loss: 1.88; train_accuracy: 0.38 

3.237792770960368e-05
7.4349504757265095e-06
Batch: 0; loss: 1.9; acc: 0.34
Batch: 20; loss: 2.01; acc: 0.38
Batch: 40; loss: 1.7; acc: 0.5
Batch: 60; loss: 1.73; acc: 0.48
Batch: 80; loss: 1.69; acc: 0.56
Batch: 100; loss: 1.79; acc: 0.47
Batch: 120; loss: 1.95; acc: 0.41
Batch: 140; loss: 1.68; acc: 0.55
Val Epoch over. val_loss: 1.8450977323920863; val_accuracy: 0.4083399681528662 

The current subspace-distance is: 7.4349504757265095e-06 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.83; acc: 0.52
Batch: 20; loss: 1.96; acc: 0.34
Batch: 40; loss: 1.91; acc: 0.39
Batch: 60; loss: 1.87; acc: 0.41
Batch: 80; loss: 1.88; acc: 0.34
Batch: 100; loss: 1.93; acc: 0.28
Batch: 120; loss: 1.81; acc: 0.36
Batch: 140; loss: 1.86; acc: 0.38
Batch: 160; loss: 2.03; acc: 0.31
Batch: 180; loss: 1.99; acc: 0.3
Batch: 200; loss: 1.9; acc: 0.42
Batch: 220; loss: 1.84; acc: 0.38
Batch: 240; loss: 1.92; acc: 0.34
Batch: 260; loss: 1.89; acc: 0.39
Batch: 280; loss: 1.88; acc: 0.39
Batch: 300; loss: 1.92; acc: 0.39
Batch: 320; loss: 1.73; acc: 0.5
Batch: 340; loss: 1.88; acc: 0.45
Batch: 360; loss: 1.84; acc: 0.44
Batch: 380; loss: 1.76; acc: 0.47
Batch: 400; loss: 1.82; acc: 0.42
Batch: 420; loss: 1.97; acc: 0.36
Batch: 440; loss: 1.87; acc: 0.38
Batch: 460; loss: 1.87; acc: 0.42
Batch: 480; loss: 1.92; acc: 0.34
Batch: 500; loss: 1.79; acc: 0.41
Batch: 520; loss: 1.94; acc: 0.33
Batch: 540; loss: 1.85; acc: 0.45
Batch: 560; loss: 1.83; acc: 0.47
Batch: 580; loss: 1.75; acc: 0.52
Batch: 600; loss: 2.0; acc: 0.3
Batch: 620; loss: 1.85; acc: 0.44
Batch: 640; loss: 1.86; acc: 0.48
Batch: 660; loss: 1.82; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.39
Batch: 700; loss: 1.82; acc: 0.44
Batch: 720; loss: 1.79; acc: 0.45
Batch: 740; loss: 1.91; acc: 0.41
Batch: 760; loss: 1.96; acc: 0.36
Batch: 780; loss: 1.9; acc: 0.34
Train Epoch over. train_loss: 1.87; train_accuracy: 0.39 

3.307626684545539e-05
6.9539996729872655e-06
Batch: 0; loss: 1.89; acc: 0.36
Batch: 20; loss: 2.0; acc: 0.38
Batch: 40; loss: 1.68; acc: 0.5
Batch: 60; loss: 1.71; acc: 0.48
Batch: 80; loss: 1.68; acc: 0.55
Batch: 100; loss: 1.77; acc: 0.45
Batch: 120; loss: 1.93; acc: 0.38
Batch: 140; loss: 1.69; acc: 0.56
Val Epoch over. val_loss: 1.8379271698605484; val_accuracy: 0.4130175159235669 

The current subspace-distance is: 6.9539996729872655e-06 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.77; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.44
Batch: 40; loss: 1.86; acc: 0.38
Batch: 60; loss: 1.88; acc: 0.23
Batch: 80; loss: 1.87; acc: 0.36
Batch: 100; loss: 2.15; acc: 0.25
Batch: 120; loss: 1.92; acc: 0.34
Batch: 140; loss: 1.89; acc: 0.36
Batch: 160; loss: 1.94; acc: 0.36
Batch: 180; loss: 1.88; acc: 0.39
Batch: 200; loss: 1.95; acc: 0.41
Batch: 220; loss: 1.81; acc: 0.47
Batch: 240; loss: 1.88; acc: 0.39
Batch: 260; loss: 1.91; acc: 0.42
Batch: 280; loss: 1.9; acc: 0.34
Batch: 300; loss: 1.86; acc: 0.39
Batch: 320; loss: 1.97; acc: 0.31
Batch: 340; loss: 1.78; acc: 0.45
Batch: 360; loss: 1.68; acc: 0.61
Batch: 380; loss: 1.73; acc: 0.47
Batch: 400; loss: 1.93; acc: 0.38
Batch: 420; loss: 1.79; acc: 0.47
Batch: 440; loss: 1.86; acc: 0.39
Batch: 460; loss: 1.84; acc: 0.52
Batch: 480; loss: 1.86; acc: 0.34
Batch: 500; loss: 1.88; acc: 0.38
Batch: 520; loss: 1.74; acc: 0.52
Batch: 540; loss: 1.94; acc: 0.38
Batch: 560; loss: 1.89; acc: 0.38
Batch: 580; loss: 1.96; acc: 0.36
Batch: 600; loss: 1.76; acc: 0.42
Batch: 620; loss: 2.0; acc: 0.38
Batch: 640; loss: 1.87; acc: 0.45
Batch: 660; loss: 1.77; acc: 0.45
Batch: 680; loss: 1.86; acc: 0.41
Batch: 700; loss: 1.93; acc: 0.28
Batch: 720; loss: 1.93; acc: 0.42
Batch: 740; loss: 1.88; acc: 0.39
Batch: 760; loss: 1.84; acc: 0.39
Batch: 780; loss: 1.78; acc: 0.41
Train Epoch over. train_loss: 1.87; train_accuracy: 0.39 

3.356437446200289e-05
7.597520834679017e-06
Batch: 0; loss: 1.89; acc: 0.36
Batch: 20; loss: 2.01; acc: 0.39
Batch: 40; loss: 1.69; acc: 0.55
Batch: 60; loss: 1.71; acc: 0.48
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.93; acc: 0.38
Batch: 140; loss: 1.69; acc: 0.52
Val Epoch over. val_loss: 1.844819352125666; val_accuracy: 0.40923566878980894 

The current subspace-distance is: 7.597520834679017e-06 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.84; acc: 0.36
Batch: 20; loss: 1.89; acc: 0.38
Batch: 40; loss: 1.94; acc: 0.3
Batch: 60; loss: 1.82; acc: 0.41
Batch: 80; loss: 1.77; acc: 0.45
Batch: 100; loss: 1.83; acc: 0.41
Batch: 120; loss: 1.87; acc: 0.36
Batch: 140; loss: 2.09; acc: 0.27
Batch: 160; loss: 1.9; acc: 0.36
Batch: 180; loss: 1.88; acc: 0.34
Batch: 200; loss: 1.94; acc: 0.27
Batch: 220; loss: 1.91; acc: 0.28
Batch: 240; loss: 1.78; acc: 0.47
Batch: 260; loss: 1.87; acc: 0.39
Batch: 280; loss: 1.97; acc: 0.38
Batch: 300; loss: 1.83; acc: 0.38
Batch: 320; loss: 1.82; acc: 0.45
Batch: 340; loss: 1.87; acc: 0.39
Batch: 360; loss: 1.87; acc: 0.3
Batch: 380; loss: 1.82; acc: 0.41
Batch: 400; loss: 1.85; acc: 0.39
Batch: 420; loss: 1.84; acc: 0.38
Batch: 440; loss: 1.86; acc: 0.27
Batch: 460; loss: 1.98; acc: 0.3
Batch: 480; loss: 1.89; acc: 0.34
Batch: 500; loss: 1.71; acc: 0.45
Batch: 520; loss: 1.81; acc: 0.47
Batch: 540; loss: 1.88; acc: 0.38
Batch: 560; loss: 1.92; acc: 0.33
Batch: 580; loss: 1.99; acc: 0.31
Batch: 600; loss: 2.01; acc: 0.3
Batch: 620; loss: 1.84; acc: 0.38
Batch: 640; loss: 1.81; acc: 0.48
Batch: 660; loss: 1.94; acc: 0.47
Batch: 680; loss: 1.79; acc: 0.47
Batch: 700; loss: 1.82; acc: 0.44
Batch: 720; loss: 1.74; acc: 0.5
Batch: 740; loss: 1.8; acc: 0.45
Batch: 760; loss: 1.79; acc: 0.42
Batch: 780; loss: 1.8; acc: 0.41
Train Epoch over. train_loss: 1.87; train_accuracy: 0.39 

3.287202707724646e-05
8.004055416677147e-06
Batch: 0; loss: 1.88; acc: 0.36
Batch: 20; loss: 2.0; acc: 0.39
Batch: 40; loss: 1.68; acc: 0.53
Batch: 60; loss: 1.71; acc: 0.48
Batch: 80; loss: 1.69; acc: 0.5
Batch: 100; loss: 1.79; acc: 0.41
Batch: 120; loss: 1.92; acc: 0.34
Batch: 140; loss: 1.68; acc: 0.55
Val Epoch over. val_loss: 1.8366953210466226; val_accuracy: 0.41779458598726116 

The current subspace-distance is: 8.004055416677147e-06 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.84; acc: 0.36
Batch: 20; loss: 1.8; acc: 0.41
Batch: 40; loss: 1.83; acc: 0.47
Batch: 60; loss: 1.96; acc: 0.33
Batch: 80; loss: 1.87; acc: 0.44
Batch: 100; loss: 1.87; acc: 0.45
Batch: 120; loss: 1.86; acc: 0.39
Batch: 140; loss: 1.67; acc: 0.48
Batch: 160; loss: 1.71; acc: 0.61
Batch: 180; loss: 1.74; acc: 0.38
Batch: 200; loss: 1.83; acc: 0.38
Batch: 220; loss: 1.94; acc: 0.28
Batch: 240; loss: 1.81; acc: 0.36
Batch: 260; loss: 1.87; acc: 0.42
Batch: 280; loss: 1.96; acc: 0.34
Batch: 300; loss: 1.86; acc: 0.41
Batch: 320; loss: 1.89; acc: 0.38
Batch: 340; loss: 1.91; acc: 0.41
Batch: 360; loss: 1.88; acc: 0.41
Batch: 380; loss: 1.82; acc: 0.45
Batch: 400; loss: 1.72; acc: 0.56
Batch: 420; loss: 1.88; acc: 0.39
Batch: 440; loss: 2.14; acc: 0.25
Batch: 460; loss: 1.92; acc: 0.34
Batch: 480; loss: 1.74; acc: 0.53
Batch: 500; loss: 1.81; acc: 0.38
Batch: 520; loss: 1.89; acc: 0.42
Batch: 540; loss: 1.9; acc: 0.31
Batch: 560; loss: 1.86; acc: 0.38
Batch: 580; loss: 1.97; acc: 0.25
Batch: 600; loss: 1.68; acc: 0.5
Batch: 620; loss: 1.82; acc: 0.41
Batch: 640; loss: 1.83; acc: 0.36
Batch: 660; loss: 1.91; acc: 0.33
Batch: 680; loss: 1.93; acc: 0.36
Batch: 700; loss: 1.82; acc: 0.38
Batch: 720; loss: 1.81; acc: 0.45
Batch: 740; loss: 1.8; acc: 0.42
Batch: 760; loss: 1.95; acc: 0.31
Batch: 780; loss: 1.78; acc: 0.39
Train Epoch over. train_loss: 1.86; train_accuracy: 0.39 

3.354415821377188e-05
7.229383300000336e-06
Batch: 0; loss: 1.89; acc: 0.38
Batch: 20; loss: 2.0; acc: 0.41
Batch: 40; loss: 1.68; acc: 0.58
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.79; acc: 0.42
Batch: 120; loss: 1.92; acc: 0.34
Batch: 140; loss: 1.69; acc: 0.5
Val Epoch over. val_loss: 1.8394453707773974; val_accuracy: 0.41630175159235666 

The current subspace-distance is: 7.229383300000336e-06 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.83; acc: 0.39
Batch: 20; loss: 1.79; acc: 0.38
Batch: 40; loss: 1.95; acc: 0.36
Batch: 60; loss: 1.86; acc: 0.41
Batch: 80; loss: 1.89; acc: 0.42
Batch: 100; loss: 1.71; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.56
Batch: 140; loss: 1.85; acc: 0.34
Batch: 160; loss: 2.01; acc: 0.3
Batch: 180; loss: 1.88; acc: 0.33
Batch: 200; loss: 1.85; acc: 0.42
Batch: 220; loss: 1.84; acc: 0.45
Batch: 240; loss: 1.8; acc: 0.42
Batch: 260; loss: 1.87; acc: 0.39
Batch: 280; loss: 1.86; acc: 0.41
Batch: 300; loss: 1.84; acc: 0.36
Batch: 320; loss: 1.86; acc: 0.33
Batch: 340; loss: 1.93; acc: 0.28
Batch: 360; loss: 1.92; acc: 0.36
Batch: 380; loss: 1.96; acc: 0.39
Batch: 400; loss: 1.91; acc: 0.36
Batch: 420; loss: 1.89; acc: 0.36
Batch: 440; loss: 1.82; acc: 0.38
Batch: 460; loss: 1.8; acc: 0.41
Batch: 480; loss: 1.89; acc: 0.38
Batch: 500; loss: 1.85; acc: 0.34
Batch: 520; loss: 1.89; acc: 0.42
Batch: 540; loss: 1.96; acc: 0.33
Batch: 560; loss: 1.91; acc: 0.31
Batch: 580; loss: 1.89; acc: 0.39
Batch: 600; loss: 1.83; acc: 0.39
Batch: 620; loss: 2.12; acc: 0.22
Batch: 640; loss: 1.81; acc: 0.39
Batch: 660; loss: 1.85; acc: 0.39
Batch: 680; loss: 1.88; acc: 0.41
Batch: 700; loss: 1.76; acc: 0.41
Batch: 720; loss: 2.08; acc: 0.22
Batch: 740; loss: 1.96; acc: 0.28
Batch: 760; loss: 1.96; acc: 0.33
Batch: 780; loss: 1.78; acc: 0.5
Train Epoch over. train_loss: 1.86; train_accuracy: 0.39 

3.352451676619239e-05
7.672223546251189e-06
Batch: 0; loss: 1.87; acc: 0.38
Batch: 20; loss: 1.98; acc: 0.44
Batch: 40; loss: 1.67; acc: 0.55
Batch: 60; loss: 1.69; acc: 0.5
Batch: 80; loss: 1.7; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.44
Batch: 120; loss: 1.91; acc: 0.36
Batch: 140; loss: 1.69; acc: 0.5
Val Epoch over. val_loss: 1.8324896233856298; val_accuracy: 0.4257563694267516 

The current subspace-distance is: 7.672223546251189e-06 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.88; acc: 0.36
Batch: 20; loss: 1.79; acc: 0.48
Batch: 40; loss: 1.9; acc: 0.39
Batch: 60; loss: 1.85; acc: 0.44
Batch: 80; loss: 1.87; acc: 0.38
Batch: 100; loss: 1.87; acc: 0.38
Batch: 120; loss: 1.76; acc: 0.53
Batch: 140; loss: 1.83; acc: 0.36
Batch: 160; loss: 1.77; acc: 0.44
Batch: 180; loss: 1.93; acc: 0.3
Batch: 200; loss: 1.89; acc: 0.38
Batch: 220; loss: 1.93; acc: 0.31
Batch: 240; loss: 1.86; acc: 0.42
Batch: 260; loss: 1.77; acc: 0.47
Batch: 280; loss: 1.91; acc: 0.31
Batch: 300; loss: 1.96; acc: 0.31
Batch: 320; loss: 1.79; acc: 0.44
Batch: 340; loss: 1.98; acc: 0.38
Batch: 360; loss: 1.87; acc: 0.34
Batch: 380; loss: 2.03; acc: 0.33
Batch: 400; loss: 1.91; acc: 0.31
Batch: 420; loss: 1.83; acc: 0.36
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 2.0; acc: 0.36
Batch: 480; loss: 1.78; acc: 0.34
Batch: 500; loss: 1.88; acc: 0.28
Batch: 520; loss: 1.84; acc: 0.42
Batch: 540; loss: 1.77; acc: 0.58
Batch: 560; loss: 1.92; acc: 0.34
Batch: 580; loss: 1.81; acc: 0.5
Batch: 600; loss: 1.84; acc: 0.42
Batch: 620; loss: 1.83; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.44
Batch: 660; loss: 1.88; acc: 0.45
Batch: 680; loss: 1.82; acc: 0.47
Batch: 700; loss: 1.97; acc: 0.31
Batch: 720; loss: 1.82; acc: 0.36
Batch: 740; loss: 1.8; acc: 0.41
Batch: 760; loss: 1.99; acc: 0.33
Batch: 780; loss: 1.86; acc: 0.41
Train Epoch over. train_loss: 1.85; train_accuracy: 0.39 

3.286943683633581e-05
7.83096402301453e-06
Batch: 0; loss: 1.87; acc: 0.39
Batch: 20; loss: 1.99; acc: 0.38
Batch: 40; loss: 1.67; acc: 0.53
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.7; acc: 0.48
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.91; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.53
Val Epoch over. val_loss: 1.8278373905048249; val_accuracy: 0.4156050955414013 

The current subspace-distance is: 7.83096402301453e-06 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.94; acc: 0.3
Batch: 20; loss: 1.87; acc: 0.41
Batch: 40; loss: 1.8; acc: 0.48
Batch: 60; loss: 1.81; acc: 0.44
Batch: 80; loss: 1.72; acc: 0.48
Batch: 100; loss: 1.81; acc: 0.39
Batch: 120; loss: 1.83; acc: 0.44
Batch: 140; loss: 1.83; acc: 0.38
Batch: 160; loss: 1.85; acc: 0.39
Batch: 180; loss: 1.82; acc: 0.42
Batch: 200; loss: 1.87; acc: 0.45
Batch: 220; loss: 1.85; acc: 0.39
Batch: 240; loss: 1.83; acc: 0.44
Batch: 260; loss: 1.81; acc: 0.36
Batch: 280; loss: 1.99; acc: 0.39
Batch: 300; loss: 1.94; acc: 0.33
Batch: 320; loss: 1.79; acc: 0.44
Batch: 340; loss: 1.8; acc: 0.38
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.7; acc: 0.45
Batch: 400; loss: 1.83; acc: 0.42
Batch: 420; loss: 1.96; acc: 0.33
Batch: 440; loss: 1.86; acc: 0.38
Batch: 460; loss: 1.9; acc: 0.39
Batch: 480; loss: 1.88; acc: 0.36
Batch: 500; loss: 1.83; acc: 0.38
Batch: 520; loss: 1.88; acc: 0.42
Batch: 540; loss: 1.92; acc: 0.33
Batch: 560; loss: 1.86; acc: 0.48
Batch: 580; loss: 1.75; acc: 0.45
Batch: 600; loss: 1.81; acc: 0.45
Batch: 620; loss: 1.81; acc: 0.45
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.78; acc: 0.44
Batch: 680; loss: 1.9; acc: 0.39
Batch: 700; loss: 1.91; acc: 0.42
Batch: 720; loss: 1.81; acc: 0.38
Batch: 740; loss: 1.98; acc: 0.31
Batch: 760; loss: 1.8; acc: 0.34
Batch: 780; loss: 1.92; acc: 0.33
Train Epoch over. train_loss: 1.85; train_accuracy: 0.4 

3.3317726774839684e-05
7.864382496336475e-06
Batch: 0; loss: 1.84; acc: 0.41
Batch: 20; loss: 1.99; acc: 0.41
Batch: 40; loss: 1.65; acc: 0.56
Batch: 60; loss: 1.67; acc: 0.52
Batch: 80; loss: 1.7; acc: 0.5
Batch: 100; loss: 1.75; acc: 0.45
Batch: 120; loss: 1.89; acc: 0.38
Batch: 140; loss: 1.69; acc: 0.52
Val Epoch over. val_loss: 1.8207090865274904; val_accuracy: 0.42585589171974525 

The current subspace-distance is: 7.864382496336475e-06 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.79; acc: 0.48
Batch: 20; loss: 1.88; acc: 0.39
Batch: 40; loss: 2.01; acc: 0.34
Batch: 60; loss: 1.79; acc: 0.47
Batch: 80; loss: 1.92; acc: 0.38
Batch: 100; loss: 1.98; acc: 0.2
Batch: 120; loss: 1.74; acc: 0.47
Batch: 140; loss: 1.86; acc: 0.34
Batch: 160; loss: 1.96; acc: 0.33
Batch: 180; loss: 1.83; acc: 0.47
Batch: 200; loss: 1.73; acc: 0.55
Batch: 220; loss: 1.84; acc: 0.33
Batch: 240; loss: 1.85; acc: 0.42
Batch: 260; loss: 1.71; acc: 0.47
Batch: 280; loss: 1.78; acc: 0.42
Batch: 300; loss: 1.82; acc: 0.41
Batch: 320; loss: 1.77; acc: 0.41
Batch: 340; loss: 1.78; acc: 0.41
Batch: 360; loss: 1.86; acc: 0.44
Batch: 380; loss: 1.84; acc: 0.39
Batch: 400; loss: 1.8; acc: 0.45
Batch: 420; loss: 1.75; acc: 0.5
Batch: 440; loss: 1.69; acc: 0.45
Batch: 460; loss: 1.89; acc: 0.38
Batch: 480; loss: 1.74; acc: 0.39
Batch: 500; loss: 1.83; acc: 0.36
Batch: 520; loss: 1.89; acc: 0.42
Batch: 540; loss: 1.8; acc: 0.5
Batch: 560; loss: 1.96; acc: 0.36
Batch: 580; loss: 1.83; acc: 0.41
Batch: 600; loss: 1.77; acc: 0.42
Batch: 620; loss: 1.8; acc: 0.42
Batch: 640; loss: 1.67; acc: 0.53
Batch: 660; loss: 1.83; acc: 0.44
Batch: 680; loss: 1.83; acc: 0.38
Batch: 700; loss: 1.89; acc: 0.38
Batch: 720; loss: 1.82; acc: 0.45
Batch: 740; loss: 1.88; acc: 0.41
Batch: 760; loss: 1.84; acc: 0.31
Batch: 780; loss: 1.87; acc: 0.36
Train Epoch over. train_loss: 1.85; train_accuracy: 0.4 

3.3598040317883715e-05
6.993454007897526e-06
Batch: 0; loss: 1.85; acc: 0.41
Batch: 20; loss: 1.98; acc: 0.41
Batch: 40; loss: 1.66; acc: 0.55
Batch: 60; loss: 1.67; acc: 0.5
Batch: 80; loss: 1.71; acc: 0.52
Batch: 100; loss: 1.78; acc: 0.44
Batch: 120; loss: 1.89; acc: 0.39
Batch: 140; loss: 1.69; acc: 0.52
Val Epoch over. val_loss: 1.8245901545141912; val_accuracy: 0.42456210191082805 

The current subspace-distance is: 6.993454007897526e-06 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.95; acc: 0.34
Batch: 40; loss: 1.88; acc: 0.42
Batch: 60; loss: 1.83; acc: 0.42
Batch: 80; loss: 1.86; acc: 0.38
Batch: 100; loss: 1.85; acc: 0.34
Batch: 120; loss: 1.96; acc: 0.28
Batch: 140; loss: 1.74; acc: 0.53
Batch: 160; loss: 1.82; acc: 0.41
Batch: 180; loss: 1.87; acc: 0.41
Batch: 200; loss: 1.83; acc: 0.39
Batch: 220; loss: 1.88; acc: 0.38
Batch: 240; loss: 1.9; acc: 0.33
Batch: 260; loss: 1.7; acc: 0.48
Batch: 280; loss: 1.79; acc: 0.41
Batch: 300; loss: 1.71; acc: 0.5
Batch: 320; loss: 1.87; acc: 0.34
Batch: 340; loss: 1.85; acc: 0.41
Batch: 360; loss: 1.87; acc: 0.34
Batch: 380; loss: 1.76; acc: 0.41
Batch: 400; loss: 1.91; acc: 0.33
Batch: 420; loss: 1.89; acc: 0.41
Batch: 440; loss: 1.86; acc: 0.38
Batch: 460; loss: 1.68; acc: 0.52
Batch: 480; loss: 1.96; acc: 0.33
Batch: 500; loss: 1.87; acc: 0.38
Batch: 520; loss: 1.83; acc: 0.38
Batch: 540; loss: 1.69; acc: 0.47
Batch: 560; loss: 1.79; acc: 0.47
Batch: 580; loss: 1.85; acc: 0.44
Batch: 600; loss: 1.81; acc: 0.45
Batch: 620; loss: 1.87; acc: 0.39
Batch: 640; loss: 1.79; acc: 0.41
Batch: 660; loss: 1.83; acc: 0.38
Batch: 680; loss: 1.7; acc: 0.47
Batch: 700; loss: 1.88; acc: 0.34
Batch: 720; loss: 1.97; acc: 0.23
Batch: 740; loss: 1.76; acc: 0.44
Batch: 760; loss: 1.93; acc: 0.38
Batch: 780; loss: 1.83; acc: 0.41
Train Epoch over. train_loss: 1.85; train_accuracy: 0.4 

3.298263254691847e-05
7.411792466882616e-06
Batch: 0; loss: 1.83; acc: 0.42
Batch: 20; loss: 1.98; acc: 0.41
Batch: 40; loss: 1.65; acc: 0.55
Batch: 60; loss: 1.66; acc: 0.5
Batch: 80; loss: 1.7; acc: 0.5
Batch: 100; loss: 1.77; acc: 0.45
Batch: 120; loss: 1.88; acc: 0.39
Batch: 140; loss: 1.69; acc: 0.5
Val Epoch over. val_loss: 1.8172054374293916; val_accuracy: 0.4287420382165605 

The current subspace-distance is: 7.411792466882616e-06 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.68; acc: 0.48
Batch: 20; loss: 2.03; acc: 0.3
Batch: 40; loss: 1.91; acc: 0.41
Batch: 60; loss: 1.84; acc: 0.44
Batch: 80; loss: 1.9; acc: 0.44
Batch: 100; loss: 1.9; acc: 0.33
Batch: 120; loss: 1.79; acc: 0.38
Batch: 140; loss: 1.74; acc: 0.47
Batch: 160; loss: 1.83; acc: 0.38
Batch: 180; loss: 1.76; acc: 0.47
Batch: 200; loss: 1.84; acc: 0.42
Batch: 220; loss: 1.79; acc: 0.39
Batch: 240; loss: 1.91; acc: 0.45
Batch: 260; loss: 1.89; acc: 0.33
Batch: 280; loss: 1.83; acc: 0.33
Batch: 300; loss: 1.79; acc: 0.47
Batch: 320; loss: 1.79; acc: 0.44
Batch: 340; loss: 1.9; acc: 0.39
Batch: 360; loss: 1.88; acc: 0.39
Batch: 380; loss: 2.0; acc: 0.27
Batch: 400; loss: 1.82; acc: 0.45
Batch: 420; loss: 1.82; acc: 0.47
Batch: 440; loss: 1.88; acc: 0.39
Batch: 460; loss: 1.75; acc: 0.53
Batch: 480; loss: 1.72; acc: 0.47
Batch: 500; loss: 1.77; acc: 0.44
Batch: 520; loss: 1.87; acc: 0.41
Batch: 540; loss: 1.76; acc: 0.44
Batch: 560; loss: 1.93; acc: 0.33
Batch: 580; loss: 1.92; acc: 0.39
Batch: 600; loss: 1.86; acc: 0.31
Batch: 620; loss: 1.8; acc: 0.44
Batch: 640; loss: 1.92; acc: 0.34
Batch: 660; loss: 1.87; acc: 0.48
Batch: 680; loss: 1.78; acc: 0.45
Batch: 700; loss: 1.74; acc: 0.42
Batch: 720; loss: 1.85; acc: 0.39
Batch: 740; loss: 1.75; acc: 0.48
Batch: 760; loss: 1.73; acc: 0.45
Batch: 780; loss: 1.95; acc: 0.3
Train Epoch over. train_loss: 1.84; train_accuracy: 0.4 

3.345301956869662e-05
8.462890036753379e-06
Batch: 0; loss: 1.82; acc: 0.42
Batch: 20; loss: 1.98; acc: 0.41
Batch: 40; loss: 1.64; acc: 0.53
Batch: 60; loss: 1.65; acc: 0.48
Batch: 80; loss: 1.69; acc: 0.48
Batch: 100; loss: 1.76; acc: 0.47
Batch: 120; loss: 1.87; acc: 0.33
Batch: 140; loss: 1.69; acc: 0.52
Val Epoch over. val_loss: 1.8114267952123266; val_accuracy: 0.4238654458598726 

The current subspace-distance is: 8.462890036753379e-06 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.89; acc: 0.42
Batch: 20; loss: 1.94; acc: 0.34
Batch: 40; loss: 1.84; acc: 0.36
Batch: 60; loss: 1.85; acc: 0.27
Batch: 80; loss: 2.05; acc: 0.33
Batch: 100; loss: 1.98; acc: 0.22
Batch: 120; loss: 1.8; acc: 0.42
Batch: 140; loss: 1.67; acc: 0.55
Batch: 160; loss: 1.86; acc: 0.41
Batch: 180; loss: 1.68; acc: 0.44
Batch: 200; loss: 1.73; acc: 0.5
Batch: 220; loss: 1.92; acc: 0.33
Batch: 240; loss: 1.87; acc: 0.25
Batch: 260; loss: 1.75; acc: 0.5
Batch: 280; loss: 1.94; acc: 0.33
Batch: 300; loss: 1.75; acc: 0.47
Batch: 320; loss: 1.75; acc: 0.45
Batch: 340; loss: 1.85; acc: 0.34
Batch: 360; loss: 1.8; acc: 0.48
Batch: 380; loss: 1.78; acc: 0.45
Batch: 400; loss: 1.93; acc: 0.38
Batch: 420; loss: 1.79; acc: 0.42
Batch: 440; loss: 1.83; acc: 0.34
Batch: 460; loss: 1.93; acc: 0.31
Batch: 480; loss: 1.91; acc: 0.34
Batch: 500; loss: 1.85; acc: 0.39
Batch: 520; loss: 1.75; acc: 0.5
Batch: 540; loss: 1.76; acc: 0.47
Batch: 560; loss: 1.78; acc: 0.41
Batch: 580; loss: 1.85; acc: 0.33
Batch: 600; loss: 1.82; acc: 0.45
Batch: 620; loss: 1.87; acc: 0.39
Batch: 640; loss: 1.68; acc: 0.52
Batch: 660; loss: 1.79; acc: 0.44
Batch: 680; loss: 1.93; acc: 0.33
Batch: 700; loss: 1.86; acc: 0.36
Batch: 720; loss: 1.74; acc: 0.48
Batch: 740; loss: 1.94; acc: 0.38
Batch: 760; loss: 1.9; acc: 0.34
Batch: 780; loss: 1.79; acc: 0.45
Train Epoch over. train_loss: 1.84; train_accuracy: 0.4 

3.3608786907279864e-05
7.5504794949665666e-06
Batch: 0; loss: 1.83; acc: 0.42
Batch: 20; loss: 1.97; acc: 0.41
Batch: 40; loss: 1.66; acc: 0.53
Batch: 60; loss: 1.67; acc: 0.53
Batch: 80; loss: 1.71; acc: 0.45
Batch: 100; loss: 1.78; acc: 0.48
Batch: 120; loss: 1.88; acc: 0.41
Batch: 140; loss: 1.7; acc: 0.48
Val Epoch over. val_loss: 1.8201063598037526; val_accuracy: 0.4278463375796178 

The current subspace-distance is: 7.5504794949665666e-06 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.94; acc: 0.36
Batch: 20; loss: 1.95; acc: 0.38
Batch: 40; loss: 1.87; acc: 0.39
Batch: 60; loss: 1.93; acc: 0.28
Batch: 80; loss: 1.76; acc: 0.48
Batch: 100; loss: 1.85; acc: 0.34
Batch: 120; loss: 1.89; acc: 0.38
Batch: 140; loss: 1.92; acc: 0.39
Batch: 160; loss: 1.88; acc: 0.41
Batch: 180; loss: 1.89; acc: 0.31
Batch: 200; loss: 1.8; acc: 0.41
Batch: 220; loss: 1.82; acc: 0.36
Batch: 240; loss: 1.92; acc: 0.42
Batch: 260; loss: 1.79; acc: 0.36
Batch: 280; loss: 1.77; acc: 0.36
Batch: 300; loss: 1.91; acc: 0.38
Batch: 320; loss: 1.9; acc: 0.42
Batch: 340; loss: 1.83; acc: 0.41
Batch: 360; loss: 1.68; acc: 0.52
Batch: 380; loss: 2.05; acc: 0.25
Batch: 400; loss: 1.75; acc: 0.52
Batch: 420; loss: 1.78; acc: 0.47
Batch: 440; loss: 1.97; acc: 0.36
Batch: 460; loss: 1.91; acc: 0.42
Batch: 480; loss: 1.91; acc: 0.33
Batch: 500; loss: 1.83; acc: 0.41
Batch: 520; loss: 1.87; acc: 0.41
Batch: 540; loss: 1.78; acc: 0.44
Batch: 560; loss: 1.78; acc: 0.48
Batch: 580; loss: 1.81; acc: 0.45
Batch: 600; loss: 1.82; acc: 0.42
Batch: 620; loss: 1.96; acc: 0.33
Batch: 640; loss: 1.89; acc: 0.3
Batch: 660; loss: 1.79; acc: 0.44
Batch: 680; loss: 1.93; acc: 0.34
Batch: 700; loss: 1.9; acc: 0.3
Batch: 720; loss: 1.72; acc: 0.55
Batch: 740; loss: 1.79; acc: 0.52
Batch: 760; loss: 1.86; acc: 0.39
Batch: 780; loss: 1.81; acc: 0.39
Train Epoch over. train_loss: 1.84; train_accuracy: 0.41 

3.456749254837632e-05
8.305750270665158e-06
Batch: 0; loss: 1.81; acc: 0.45
Batch: 20; loss: 1.96; acc: 0.41
Batch: 40; loss: 1.64; acc: 0.5
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.69; acc: 0.48
Batch: 100; loss: 1.75; acc: 0.5
Batch: 120; loss: 1.86; acc: 0.41
Batch: 140; loss: 1.7; acc: 0.48
Val Epoch over. val_loss: 1.8069917290074051; val_accuracy: 0.4313296178343949 

The current subspace-distance is: 8.305750270665158e-06 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_1_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 13.5

The number of parameters is: 279504

The number of individual parameters is:

108
432
108
108
162
52488
162
162
324
157464
324
324
64
62208
64
64
4096
64
640
10
64
64

nonzero elements in E: 27950397
elements in E: 27950400
fraction nonzero: 0.9999998926670102
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.34; acc: 0.2
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.06; acc: 0.3
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.11; acc: 0.23
Batch: 100; loss: 2.03; acc: 0.28
Batch: 120; loss: 2.02; acc: 0.33
Batch: 140; loss: 1.97; acc: 0.3
Batch: 160; loss: 2.16; acc: 0.12
Batch: 180; loss: 1.94; acc: 0.33
Batch: 200; loss: 2.0; acc: 0.36
Batch: 220; loss: 1.99; acc: 0.41
Batch: 240; loss: 1.94; acc: 0.41
Batch: 260; loss: 1.92; acc: 0.41
Batch: 280; loss: 1.9; acc: 0.38
Batch: 300; loss: 1.87; acc: 0.44
Batch: 320; loss: 1.97; acc: 0.31
Batch: 340; loss: 1.85; acc: 0.41
Batch: 360; loss: 1.94; acc: 0.38
Batch: 380; loss: 1.92; acc: 0.31
Batch: 400; loss: 1.84; acc: 0.47
Batch: 420; loss: 1.98; acc: 0.31
Batch: 440; loss: 1.9; acc: 0.38
Batch: 460; loss: 1.78; acc: 0.45
Batch: 480; loss: 1.82; acc: 0.44
Batch: 500; loss: 1.88; acc: 0.33
Batch: 520; loss: 1.9; acc: 0.36
Batch: 540; loss: 1.92; acc: 0.33
Batch: 560; loss: 1.85; acc: 0.38
Batch: 580; loss: 1.96; acc: 0.39
Batch: 600; loss: 1.89; acc: 0.3
Batch: 620; loss: 1.92; acc: 0.34
Batch: 640; loss: 1.76; acc: 0.5
Batch: 660; loss: 1.7; acc: 0.48
Batch: 680; loss: 1.83; acc: 0.33
Batch: 700; loss: 1.98; acc: 0.33
Batch: 720; loss: 1.97; acc: 0.33
Batch: 740; loss: 1.98; acc: 0.3
Batch: 760; loss: 1.97; acc: 0.31
Batch: 780; loss: 1.98; acc: 0.33
Train Epoch over. train_loss: 1.94; train_accuracy: 0.36 

4.4632488425122574e-05
3.444046888034791e-05
Batch: 0; loss: 1.73; acc: 0.55
Batch: 20; loss: 1.93; acc: 0.44
Batch: 40; loss: 1.66; acc: 0.56
Batch: 60; loss: 1.73; acc: 0.47
Batch: 80; loss: 1.76; acc: 0.39
Batch: 100; loss: 1.82; acc: 0.44
Batch: 120; loss: 1.81; acc: 0.45
Batch: 140; loss: 1.85; acc: 0.45
Val Epoch over. val_loss: 1.8226916045899604; val_accuracy: 0.4133160828025478 

The current subspace-distance is: 3.444046888034791e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.98; acc: 0.33
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.8; acc: 0.48
Batch: 80; loss: 1.91; acc: 0.38
Batch: 100; loss: 1.83; acc: 0.44
Batch: 120; loss: 1.9; acc: 0.41
Batch: 140; loss: 1.8; acc: 0.42
Batch: 160; loss: 1.93; acc: 0.36
Batch: 180; loss: 1.86; acc: 0.39
Batch: 200; loss: 1.77; acc: 0.44
Batch: 220; loss: 1.88; acc: 0.28
Batch: 240; loss: 1.81; acc: 0.45
Batch: 260; loss: 1.89; acc: 0.38
Batch: 280; loss: 1.96; acc: 0.34
Batch: 300; loss: 1.84; acc: 0.39
Batch: 320; loss: 1.87; acc: 0.42
Batch: 340; loss: 1.87; acc: 0.41
Batch: 360; loss: 2.05; acc: 0.31
Batch: 380; loss: 1.86; acc: 0.39
Batch: 400; loss: 1.84; acc: 0.44
Batch: 420; loss: 1.83; acc: 0.41
Batch: 440; loss: 1.84; acc: 0.44
Batch: 460; loss: 1.9; acc: 0.42
Batch: 480; loss: 1.74; acc: 0.45
Batch: 500; loss: 1.74; acc: 0.5
Batch: 520; loss: 1.65; acc: 0.52
Batch: 540; loss: 1.82; acc: 0.39
Batch: 560; loss: 1.82; acc: 0.42
Batch: 580; loss: 1.79; acc: 0.39
Batch: 600; loss: 1.76; acc: 0.48
Batch: 620; loss: 1.81; acc: 0.41
Batch: 640; loss: 1.73; acc: 0.5
Batch: 660; loss: 1.92; acc: 0.33
Batch: 680; loss: 1.9; acc: 0.45
Batch: 700; loss: 1.68; acc: 0.52
Batch: 720; loss: 1.81; acc: 0.41
Batch: 740; loss: 1.82; acc: 0.42
Batch: 760; loss: 1.79; acc: 0.45
Batch: 780; loss: 1.73; acc: 0.45
Train Epoch over. train_loss: 1.81; train_accuracy: 0.42 

5.0122867833124474e-05
4.094183532288298e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.98; acc: 0.38
Batch: 40; loss: 1.65; acc: 0.48
Batch: 60; loss: 1.67; acc: 0.53
Batch: 80; loss: 1.73; acc: 0.42
Batch: 100; loss: 1.8; acc: 0.42
Batch: 120; loss: 1.78; acc: 0.55
Batch: 140; loss: 1.72; acc: 0.52
Val Epoch over. val_loss: 1.7934977598251052; val_accuracy: 0.4267515923566879 

The current subspace-distance is: 4.094183532288298e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.82; acc: 0.45
Batch: 20; loss: 1.75; acc: 0.39
Batch: 40; loss: 1.7; acc: 0.41
Batch: 60; loss: 1.79; acc: 0.47
Batch: 80; loss: 1.71; acc: 0.55
Batch: 100; loss: 1.77; acc: 0.44
Batch: 120; loss: 1.78; acc: 0.42
Batch: 140; loss: 1.92; acc: 0.33
Batch: 160; loss: 1.88; acc: 0.39
Batch: 180; loss: 1.76; acc: 0.5
Batch: 200; loss: 1.89; acc: 0.38
Batch: 220; loss: 1.88; acc: 0.38
Batch: 240; loss: 1.79; acc: 0.42
Batch: 260; loss: 1.84; acc: 0.34
Batch: 280; loss: 1.8; acc: 0.41
Batch: 300; loss: 1.67; acc: 0.5
Batch: 320; loss: 1.81; acc: 0.41
Batch: 340; loss: 1.68; acc: 0.5
Batch: 360; loss: 1.6; acc: 0.56
Batch: 380; loss: 1.74; acc: 0.41
Batch: 400; loss: 1.83; acc: 0.38
Batch: 420; loss: 1.81; acc: 0.42
Batch: 440; loss: 1.74; acc: 0.45
Batch: 460; loss: 1.66; acc: 0.45
Batch: 480; loss: 1.71; acc: 0.47
Batch: 500; loss: 1.79; acc: 0.44
Batch: 520; loss: 1.7; acc: 0.53
Batch: 540; loss: 1.92; acc: 0.3
Batch: 560; loss: 1.99; acc: 0.34
Batch: 580; loss: 1.67; acc: 0.55
Batch: 600; loss: 1.72; acc: 0.47
Batch: 620; loss: 1.65; acc: 0.58
Batch: 640; loss: 1.8; acc: 0.38
Batch: 660; loss: 1.77; acc: 0.45
Batch: 680; loss: 1.86; acc: 0.39
Batch: 700; loss: 1.89; acc: 0.38
Batch: 720; loss: 1.82; acc: 0.38
Batch: 740; loss: 1.79; acc: 0.42
Batch: 760; loss: 1.73; acc: 0.48
Batch: 780; loss: 1.59; acc: 0.48
Train Epoch over. train_loss: 1.77; train_accuracy: 0.44 

5.498334940057248e-05
4.691516005550511e-05
Batch: 0; loss: 1.69; acc: 0.42
Batch: 20; loss: 1.84; acc: 0.44
Batch: 40; loss: 1.59; acc: 0.55
Batch: 60; loss: 1.56; acc: 0.48
Batch: 80; loss: 1.66; acc: 0.5
Batch: 100; loss: 1.73; acc: 0.42
Batch: 120; loss: 1.78; acc: 0.44
Batch: 140; loss: 1.7; acc: 0.44
Val Epoch over. val_loss: 1.746067785153723; val_accuracy: 0.43789808917197454 

The current subspace-distance is: 4.691516005550511e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.57; acc: 0.59
Batch: 40; loss: 1.82; acc: 0.42
Batch: 60; loss: 1.69; acc: 0.44
Batch: 80; loss: 1.65; acc: 0.52
Batch: 100; loss: 1.77; acc: 0.42
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.75; acc: 0.47
Batch: 160; loss: 1.64; acc: 0.5
Batch: 180; loss: 1.74; acc: 0.45
Batch: 200; loss: 1.69; acc: 0.52
Batch: 220; loss: 1.63; acc: 0.5
Batch: 240; loss: 1.72; acc: 0.42
Batch: 260; loss: 1.82; acc: 0.33
Batch: 280; loss: 1.72; acc: 0.42
Batch: 300; loss: 1.73; acc: 0.45
Batch: 320; loss: 1.82; acc: 0.38
Batch: 340; loss: 1.8; acc: 0.42
Batch: 360; loss: 1.8; acc: 0.41
Batch: 380; loss: 1.77; acc: 0.42
Batch: 400; loss: 1.76; acc: 0.45
Batch: 420; loss: 1.76; acc: 0.42
Batch: 440; loss: 1.78; acc: 0.36
Batch: 460; loss: 1.85; acc: 0.38
Batch: 480; loss: 1.93; acc: 0.31
Batch: 500; loss: 1.71; acc: 0.44
Batch: 520; loss: 1.82; acc: 0.44
Batch: 540; loss: 1.87; acc: 0.38
Batch: 560; loss: 1.77; acc: 0.41
Batch: 580; loss: 1.75; acc: 0.48
Batch: 600; loss: 1.79; acc: 0.47
Batch: 620; loss: 1.75; acc: 0.44
Batch: 640; loss: 1.85; acc: 0.38
Batch: 660; loss: 1.73; acc: 0.5
Batch: 680; loss: 1.88; acc: 0.39
Batch: 700; loss: 1.79; acc: 0.42
Batch: 720; loss: 1.75; acc: 0.42
Batch: 740; loss: 1.79; acc: 0.45
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.76; acc: 0.41
Train Epoch over. train_loss: 1.74; train_accuracy: 0.45 

6.20602659182623e-05
5.305546801537275e-05
Batch: 0; loss: 1.66; acc: 0.48
Batch: 20; loss: 1.83; acc: 0.42
Batch: 40; loss: 1.53; acc: 0.59
Batch: 60; loss: 1.52; acc: 0.62
Batch: 80; loss: 1.68; acc: 0.53
Batch: 100; loss: 1.71; acc: 0.48
Batch: 120; loss: 1.71; acc: 0.42
Batch: 140; loss: 1.63; acc: 0.58
Val Epoch over. val_loss: 1.7093021148329328; val_accuracy: 0.4723328025477707 

The current subspace-distance is: 5.305546801537275e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.48
Batch: 20; loss: 1.74; acc: 0.39
Batch: 40; loss: 1.62; acc: 0.53
Batch: 60; loss: 1.58; acc: 0.58
Batch: 80; loss: 1.71; acc: 0.52
Batch: 100; loss: 1.73; acc: 0.45
Batch: 120; loss: 1.61; acc: 0.52
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.81; acc: 0.34
Batch: 180; loss: 1.72; acc: 0.44
Batch: 200; loss: 1.9; acc: 0.42
Batch: 220; loss: 1.71; acc: 0.45
Batch: 240; loss: 1.77; acc: 0.41
Batch: 260; loss: 1.66; acc: 0.52
Batch: 280; loss: 1.62; acc: 0.52
Batch: 300; loss: 1.72; acc: 0.39
Batch: 320; loss: 1.66; acc: 0.5
Batch: 340; loss: 1.69; acc: 0.48
Batch: 360; loss: 1.73; acc: 0.47
Batch: 380; loss: 1.76; acc: 0.36
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.66; acc: 0.52
Batch: 440; loss: 1.54; acc: 0.61
Batch: 460; loss: 1.63; acc: 0.5
Batch: 480; loss: 1.79; acc: 0.38
Batch: 500; loss: 1.84; acc: 0.36
Batch: 520; loss: 1.84; acc: 0.38
Batch: 540; loss: 1.68; acc: 0.52
Batch: 560; loss: 1.64; acc: 0.45
Batch: 580; loss: 1.75; acc: 0.39
Batch: 600; loss: 1.66; acc: 0.48
Batch: 620; loss: 1.64; acc: 0.56
Batch: 640; loss: 1.66; acc: 0.47
Batch: 660; loss: 1.41; acc: 0.69
Batch: 680; loss: 1.64; acc: 0.48
Batch: 700; loss: 1.59; acc: 0.52
Batch: 720; loss: 1.6; acc: 0.55
Batch: 740; loss: 1.69; acc: 0.53
Batch: 760; loss: 1.67; acc: 0.53
Batch: 780; loss: 1.65; acc: 0.41
Train Epoch over. train_loss: 1.7; train_accuracy: 0.47 

6.933264376129955e-05
6.261534872464836e-05
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.45
Batch: 40; loss: 1.48; acc: 0.55
Batch: 60; loss: 1.43; acc: 0.67
Batch: 80; loss: 1.62; acc: 0.5
Batch: 100; loss: 1.68; acc: 0.45
Batch: 120; loss: 1.65; acc: 0.52
Batch: 140; loss: 1.56; acc: 0.62
Val Epoch over. val_loss: 1.6702784679497882; val_accuracy: 0.48815684713375795 

The current subspace-distance is: 6.261534872464836e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.53
Batch: 20; loss: 1.78; acc: 0.47
Batch: 40; loss: 1.85; acc: 0.44
Batch: 60; loss: 1.6; acc: 0.52
Batch: 80; loss: 1.71; acc: 0.39
Batch: 100; loss: 1.67; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.82; acc: 0.33
Batch: 160; loss: 1.55; acc: 0.53
Batch: 180; loss: 1.7; acc: 0.41
Batch: 200; loss: 1.72; acc: 0.42
Batch: 220; loss: 1.77; acc: 0.42
Batch: 240; loss: 1.57; acc: 0.56
Batch: 260; loss: 1.62; acc: 0.47
Batch: 280; loss: 1.8; acc: 0.48
Batch: 300; loss: 1.61; acc: 0.55
Batch: 320; loss: 1.66; acc: 0.47
Batch: 340; loss: 1.69; acc: 0.44
Batch: 360; loss: 1.65; acc: 0.42
Batch: 380; loss: 1.67; acc: 0.52
Batch: 400; loss: 1.61; acc: 0.44
Batch: 420; loss: 1.82; acc: 0.39
Batch: 440; loss: 1.77; acc: 0.34
Batch: 460; loss: 1.83; acc: 0.42
Batch: 480; loss: 1.56; acc: 0.55
Batch: 500; loss: 1.7; acc: 0.55
Batch: 520; loss: 1.79; acc: 0.41
Batch: 540; loss: 1.84; acc: 0.39
Batch: 560; loss: 1.77; acc: 0.39
Batch: 580; loss: 1.73; acc: 0.48
Batch: 600; loss: 1.81; acc: 0.41
Batch: 620; loss: 1.74; acc: 0.48
Batch: 640; loss: 1.62; acc: 0.48
Batch: 660; loss: 1.74; acc: 0.45
Batch: 680; loss: 1.61; acc: 0.5
Batch: 700; loss: 1.71; acc: 0.44
Batch: 720; loss: 1.68; acc: 0.44
Batch: 740; loss: 1.57; acc: 0.56
Batch: 760; loss: 1.72; acc: 0.42
Batch: 780; loss: 1.71; acc: 0.42
Train Epoch over. train_loss: 1.68; train_accuracy: 0.48 

7.615474896738306e-05
6.866926560178399e-05
Batch: 0; loss: 1.52; acc: 0.55
Batch: 20; loss: 1.78; acc: 0.45
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.48
Batch: 100; loss: 1.67; acc: 0.48
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.57; acc: 0.58
Val Epoch over. val_loss: 1.6742449361047926; val_accuracy: 0.46994426751592355 

The current subspace-distance is: 6.866926560178399e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 1.69; acc: 0.45
Batch: 40; loss: 1.67; acc: 0.52
Batch: 60; loss: 1.8; acc: 0.42
Batch: 80; loss: 1.57; acc: 0.58
Batch: 100; loss: 1.8; acc: 0.39
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.72; acc: 0.48
Batch: 160; loss: 1.76; acc: 0.33
Batch: 180; loss: 1.61; acc: 0.52
Batch: 200; loss: 1.54; acc: 0.55
Batch: 220; loss: 1.66; acc: 0.45
Batch: 240; loss: 1.69; acc: 0.45
Batch: 260; loss: 1.85; acc: 0.36
Batch: 280; loss: 1.7; acc: 0.48
Batch: 300; loss: 1.68; acc: 0.53
Batch: 320; loss: 1.6; acc: 0.5
Batch: 340; loss: 1.52; acc: 0.53
Batch: 360; loss: 1.63; acc: 0.59
Batch: 380; loss: 1.6; acc: 0.52
Batch: 400; loss: 1.78; acc: 0.42
Batch: 420; loss: 1.72; acc: 0.44
Batch: 440; loss: 1.71; acc: 0.45
Batch: 460; loss: 1.6; acc: 0.47
Batch: 480; loss: 1.62; acc: 0.55
Batch: 500; loss: 1.75; acc: 0.42
Batch: 520; loss: 1.68; acc: 0.44
Batch: 540; loss: 1.69; acc: 0.58
Batch: 560; loss: 1.61; acc: 0.53
Batch: 580; loss: 1.7; acc: 0.41
Batch: 600; loss: 1.57; acc: 0.53
Batch: 620; loss: 1.53; acc: 0.52
Batch: 640; loss: 1.68; acc: 0.39
Batch: 660; loss: 1.66; acc: 0.45
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.64; acc: 0.52
Batch: 720; loss: 1.62; acc: 0.53
Batch: 740; loss: 1.57; acc: 0.55
Batch: 760; loss: 1.63; acc: 0.58
Batch: 780; loss: 1.58; acc: 0.53
Train Epoch over. train_loss: 1.65; train_accuracy: 0.49 

8.181484008673579e-05
7.57084708311595e-05
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.51; acc: 0.45
Batch: 60; loss: 1.53; acc: 0.56
Batch: 80; loss: 1.58; acc: 0.45
Batch: 100; loss: 1.71; acc: 0.44
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.61; acc: 0.55
Val Epoch over. val_loss: 1.7127589928876064; val_accuracy: 0.4381966560509554 

The current subspace-distance is: 7.57084708311595e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.57; acc: 0.56
Batch: 60; loss: 1.7; acc: 0.41
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.68; acc: 0.45
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.53
Batch: 160; loss: 1.7; acc: 0.45
Batch: 180; loss: 1.74; acc: 0.45
Batch: 200; loss: 1.61; acc: 0.47
Batch: 220; loss: 1.59; acc: 0.56
Batch: 240; loss: 1.59; acc: 0.53
Batch: 260; loss: 1.74; acc: 0.45
Batch: 280; loss: 1.63; acc: 0.47
Batch: 300; loss: 1.64; acc: 0.48
Batch: 320; loss: 1.61; acc: 0.5
Batch: 340; loss: 1.84; acc: 0.38
Batch: 360; loss: 1.54; acc: 0.56
Batch: 380; loss: 1.64; acc: 0.48
Batch: 400; loss: 1.79; acc: 0.39
Batch: 420; loss: 1.69; acc: 0.44
Batch: 440; loss: 1.73; acc: 0.47
Batch: 460; loss: 1.94; acc: 0.3
Batch: 480; loss: 1.7; acc: 0.45
Batch: 500; loss: 1.51; acc: 0.55
Batch: 520; loss: 1.75; acc: 0.47
Batch: 540; loss: 1.76; acc: 0.36
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.66; acc: 0.47
Batch: 600; loss: 1.86; acc: 0.41
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.66; acc: 0.45
Batch: 660; loss: 1.55; acc: 0.55
Batch: 680; loss: 1.76; acc: 0.38
Batch: 700; loss: 1.61; acc: 0.56
Batch: 720; loss: 1.45; acc: 0.55
Batch: 740; loss: 1.73; acc: 0.44
Batch: 760; loss: 1.51; acc: 0.59
Batch: 780; loss: 1.61; acc: 0.52
Train Epoch over. train_loss: 1.64; train_accuracy: 0.49 

8.604506001574919e-05
7.963558164192364e-05
Batch: 0; loss: 1.51; acc: 0.59
Batch: 20; loss: 1.78; acc: 0.39
Batch: 40; loss: 1.45; acc: 0.47
Batch: 60; loss: 1.43; acc: 0.58
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.63; acc: 0.5
Batch: 140; loss: 1.56; acc: 0.52
Val Epoch over. val_loss: 1.6406668325897995; val_accuracy: 0.4753184713375796 

The current subspace-distance is: 7.963558164192364e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.5
Batch: 20; loss: 1.75; acc: 0.44
Batch: 40; loss: 1.54; acc: 0.56
Batch: 60; loss: 1.74; acc: 0.42
Batch: 80; loss: 1.92; acc: 0.36
Batch: 100; loss: 1.7; acc: 0.39
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.79; acc: 0.44
Batch: 160; loss: 1.47; acc: 0.61
Batch: 180; loss: 1.66; acc: 0.42
Batch: 200; loss: 1.58; acc: 0.58
Batch: 220; loss: 1.64; acc: 0.47
Batch: 240; loss: 1.43; acc: 0.56
Batch: 260; loss: 1.5; acc: 0.61
Batch: 280; loss: 1.7; acc: 0.41
Batch: 300; loss: 1.62; acc: 0.5
Batch: 320; loss: 1.48; acc: 0.55
Batch: 340; loss: 1.8; acc: 0.34
Batch: 360; loss: 1.6; acc: 0.5
Batch: 380; loss: 1.59; acc: 0.52
Batch: 400; loss: 1.8; acc: 0.33
Batch: 420; loss: 1.77; acc: 0.45
Batch: 440; loss: 1.63; acc: 0.53
Batch: 460; loss: 1.51; acc: 0.56
Batch: 480; loss: 1.73; acc: 0.5
Batch: 500; loss: 1.61; acc: 0.58
Batch: 520; loss: 1.66; acc: 0.5
Batch: 540; loss: 1.6; acc: 0.52
Batch: 560; loss: 1.5; acc: 0.58
Batch: 580; loss: 1.66; acc: 0.45
Batch: 600; loss: 1.63; acc: 0.45
Batch: 620; loss: 1.7; acc: 0.38
Batch: 640; loss: 1.65; acc: 0.5
Batch: 660; loss: 1.58; acc: 0.53
Batch: 680; loss: 1.54; acc: 0.56
Batch: 700; loss: 1.82; acc: 0.42
Batch: 720; loss: 1.49; acc: 0.59
Batch: 740; loss: 1.51; acc: 0.53
Batch: 760; loss: 1.58; acc: 0.59
Batch: 780; loss: 1.56; acc: 0.56
Train Epoch over. train_loss: 1.62; train_accuracy: 0.5 

8.841508679324761e-05
8.18684056866914e-05
Batch: 0; loss: 1.51; acc: 0.56
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.39; acc: 0.58
Batch: 60; loss: 1.41; acc: 0.53
Batch: 80; loss: 1.44; acc: 0.66
Batch: 100; loss: 1.66; acc: 0.41
Batch: 120; loss: 1.6; acc: 0.55
Batch: 140; loss: 1.49; acc: 0.59
Val Epoch over. val_loss: 1.5864819652715307; val_accuracy: 0.5354299363057324 

The current subspace-distance is: 8.18684056866914e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.45
Batch: 20; loss: 1.61; acc: 0.58
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.52; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.5
Batch: 100; loss: 1.57; acc: 0.52
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.62; acc: 0.5
Batch: 160; loss: 1.43; acc: 0.69
Batch: 180; loss: 1.57; acc: 0.52
Batch: 200; loss: 1.59; acc: 0.53
Batch: 220; loss: 1.64; acc: 0.48
Batch: 240; loss: 1.65; acc: 0.42
Batch: 260; loss: 1.59; acc: 0.45
Batch: 280; loss: 1.49; acc: 0.53
Batch: 300; loss: 1.52; acc: 0.59
Batch: 320; loss: 1.64; acc: 0.5
Batch: 340; loss: 1.57; acc: 0.56
Batch: 360; loss: 1.57; acc: 0.52
Batch: 380; loss: 1.63; acc: 0.5
Batch: 400; loss: 1.69; acc: 0.45
Batch: 420; loss: 1.55; acc: 0.59
Batch: 440; loss: 1.54; acc: 0.52
Batch: 460; loss: 1.57; acc: 0.58
Batch: 480; loss: 1.65; acc: 0.47
Batch: 500; loss: 1.61; acc: 0.52
Batch: 520; loss: 1.63; acc: 0.48
Batch: 540; loss: 1.65; acc: 0.42
Batch: 560; loss: 1.49; acc: 0.56
Batch: 580; loss: 1.58; acc: 0.56
Batch: 600; loss: 1.62; acc: 0.5
Batch: 620; loss: 1.5; acc: 0.62
Batch: 640; loss: 1.46; acc: 0.59
Batch: 660; loss: 1.62; acc: 0.5
Batch: 680; loss: 1.62; acc: 0.41
Batch: 700; loss: 1.64; acc: 0.48
Batch: 720; loss: 1.61; acc: 0.5
Batch: 740; loss: 1.45; acc: 0.61
Batch: 760; loss: 1.68; acc: 0.47
Batch: 780; loss: 1.85; acc: 0.36
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

9.265711560146883e-05
8.495040674461052e-05
Batch: 0; loss: 1.45; acc: 0.56
Batch: 20; loss: 1.73; acc: 0.42
Batch: 40; loss: 1.41; acc: 0.59
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.43; acc: 0.66
Batch: 100; loss: 1.66; acc: 0.47
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.47; acc: 0.64
Val Epoch over. val_loss: 1.5934731466754986; val_accuracy: 0.49950238853503187 

The current subspace-distance is: 8.495040674461052e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.73; acc: 0.45
Batch: 20; loss: 1.53; acc: 0.62
Batch: 40; loss: 1.61; acc: 0.5
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.49; acc: 0.56
Batch: 100; loss: 1.52; acc: 0.58
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.51; acc: 0.62
Batch: 160; loss: 1.72; acc: 0.48
Batch: 180; loss: 1.35; acc: 0.64
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.73; acc: 0.44
Batch: 240; loss: 1.62; acc: 0.48
Batch: 260; loss: 1.5; acc: 0.59
Batch: 280; loss: 1.67; acc: 0.45
Batch: 300; loss: 1.57; acc: 0.56
Batch: 320; loss: 1.46; acc: 0.61
Batch: 340; loss: 1.61; acc: 0.56
Batch: 360; loss: 1.51; acc: 0.59
Batch: 380; loss: 1.6; acc: 0.52
Batch: 400; loss: 1.71; acc: 0.47
Batch: 420; loss: 1.53; acc: 0.53
Batch: 440; loss: 1.55; acc: 0.58
Batch: 460; loss: 1.6; acc: 0.52
Batch: 480; loss: 1.71; acc: 0.44
Batch: 500; loss: 1.65; acc: 0.44
Batch: 520; loss: 1.49; acc: 0.56
Batch: 540; loss: 1.77; acc: 0.42
Batch: 560; loss: 1.58; acc: 0.48
Batch: 580; loss: 1.72; acc: 0.47
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.53; acc: 0.53
Batch: 640; loss: 1.66; acc: 0.48
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.55; acc: 0.58
Batch: 700; loss: 1.66; acc: 0.52
Batch: 720; loss: 1.64; acc: 0.44
Batch: 740; loss: 1.54; acc: 0.61
Batch: 760; loss: 1.56; acc: 0.55
Batch: 780; loss: 1.68; acc: 0.45
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

9.375598165206611e-05
8.676364086568356e-05
Batch: 0; loss: 1.5; acc: 0.61
Batch: 20; loss: 1.68; acc: 0.52
Batch: 40; loss: 1.37; acc: 0.64
Batch: 60; loss: 1.4; acc: 0.61
Batch: 80; loss: 1.44; acc: 0.66
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.57; acc: 0.56
Batch: 140; loss: 1.46; acc: 0.7
Val Epoch over. val_loss: 1.5641575512612702; val_accuracy: 0.5504578025477707 

The current subspace-distance is: 8.676364086568356e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.52; acc: 0.62
Batch: 20; loss: 1.5; acc: 0.53
Batch: 40; loss: 1.65; acc: 0.44
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.58; acc: 0.52
Batch: 100; loss: 1.55; acc: 0.56
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.62; acc: 0.55
Batch: 160; loss: 1.55; acc: 0.56
Batch: 180; loss: 1.56; acc: 0.47
Batch: 200; loss: 1.47; acc: 0.55
Batch: 220; loss: 1.51; acc: 0.66
Batch: 240; loss: 1.51; acc: 0.58
Batch: 260; loss: 1.62; acc: 0.47
Batch: 280; loss: 1.59; acc: 0.53
Batch: 300; loss: 1.71; acc: 0.44
Batch: 320; loss: 1.79; acc: 0.44
Batch: 340; loss: 1.48; acc: 0.55
Batch: 360; loss: 1.54; acc: 0.52
Batch: 380; loss: 1.57; acc: 0.56
Batch: 400; loss: 1.56; acc: 0.59
Batch: 420; loss: 1.57; acc: 0.53
Batch: 440; loss: 1.68; acc: 0.42
Batch: 460; loss: 1.53; acc: 0.55
Batch: 480; loss: 1.57; acc: 0.52
Batch: 500; loss: 1.45; acc: 0.58
Batch: 520; loss: 1.69; acc: 0.47
Batch: 540; loss: 1.61; acc: 0.59
Batch: 560; loss: 1.45; acc: 0.58
Batch: 580; loss: 1.62; acc: 0.52
Batch: 600; loss: 1.59; acc: 0.5
Batch: 620; loss: 1.63; acc: 0.52
Batch: 640; loss: 1.61; acc: 0.52
Batch: 660; loss: 1.74; acc: 0.42
Batch: 680; loss: 1.72; acc: 0.47
Batch: 700; loss: 1.66; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.44
Batch: 740; loss: 1.76; acc: 0.47
Batch: 760; loss: 1.52; acc: 0.5
Batch: 780; loss: 1.68; acc: 0.47
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

9.294412302551791e-05
8.46624025143683e-05
Batch: 0; loss: 1.51; acc: 0.53
Batch: 20; loss: 1.72; acc: 0.52
Batch: 40; loss: 1.36; acc: 0.61
Batch: 60; loss: 1.42; acc: 0.62
Batch: 80; loss: 1.43; acc: 0.66
Batch: 100; loss: 1.66; acc: 0.47
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 1.5743038175971644; val_accuracy: 0.5390127388535032 

The current subspace-distance is: 8.46624025143683e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.44; acc: 0.61
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.74; acc: 0.48
Batch: 60; loss: 1.65; acc: 0.52
Batch: 80; loss: 1.51; acc: 0.62
Batch: 100; loss: 1.39; acc: 0.61
Batch: 120; loss: 1.71; acc: 0.38
Batch: 140; loss: 1.67; acc: 0.58
Batch: 160; loss: 1.67; acc: 0.45
Batch: 180; loss: 1.97; acc: 0.31
Batch: 200; loss: 1.63; acc: 0.48
Batch: 220; loss: 1.59; acc: 0.55
Batch: 240; loss: 1.57; acc: 0.61
Batch: 260; loss: 1.65; acc: 0.47
Batch: 280; loss: 1.56; acc: 0.53
Batch: 300; loss: 1.56; acc: 0.56
Batch: 320; loss: 1.7; acc: 0.5
Batch: 340; loss: 1.62; acc: 0.5
Batch: 360; loss: 1.65; acc: 0.44
Batch: 380; loss: 1.66; acc: 0.41
Batch: 400; loss: 1.64; acc: 0.47
Batch: 420; loss: 1.51; acc: 0.56
Batch: 440; loss: 1.64; acc: 0.52
Batch: 460; loss: 1.6; acc: 0.53
Batch: 480; loss: 1.75; acc: 0.47
Batch: 500; loss: 1.65; acc: 0.45
Batch: 520; loss: 1.48; acc: 0.64
Batch: 540; loss: 1.75; acc: 0.45
Batch: 560; loss: 1.49; acc: 0.55
Batch: 580; loss: 1.64; acc: 0.47
Batch: 600; loss: 1.62; acc: 0.52
Batch: 620; loss: 1.59; acc: 0.53
Batch: 640; loss: 1.57; acc: 0.55
Batch: 660; loss: 1.64; acc: 0.47
Batch: 680; loss: 1.61; acc: 0.56
Batch: 700; loss: 1.51; acc: 0.55
Batch: 720; loss: 1.58; acc: 0.59
Batch: 740; loss: 1.67; acc: 0.48
Batch: 760; loss: 1.6; acc: 0.53
Batch: 780; loss: 1.53; acc: 0.56
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

9.408991900272667e-05
8.718338358448818e-05
Batch: 0; loss: 1.48; acc: 0.62
Batch: 20; loss: 1.71; acc: 0.53
Batch: 40; loss: 1.34; acc: 0.61
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.43; acc: 0.7
Batch: 100; loss: 1.63; acc: 0.53
Batch: 120; loss: 1.58; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.7
Val Epoch over. val_loss: 1.5595600802427645; val_accuracy: 0.5592157643312102 

The current subspace-distance is: 8.718338358448818e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.79; acc: 0.42
Batch: 20; loss: 1.7; acc: 0.42
Batch: 40; loss: 1.54; acc: 0.52
Batch: 60; loss: 1.53; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.48
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.61; acc: 0.56
Batch: 140; loss: 1.67; acc: 0.47
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.84; acc: 0.44
Batch: 200; loss: 1.68; acc: 0.48
Batch: 220; loss: 1.49; acc: 0.59
Batch: 240; loss: 1.59; acc: 0.52
Batch: 260; loss: 1.65; acc: 0.5
Batch: 280; loss: 1.58; acc: 0.53
Batch: 300; loss: 1.59; acc: 0.56
Batch: 320; loss: 1.41; acc: 0.56
Batch: 340; loss: 1.77; acc: 0.41
Batch: 360; loss: 1.47; acc: 0.56
Batch: 380; loss: 1.58; acc: 0.48
Batch: 400; loss: 1.61; acc: 0.52
Batch: 420; loss: 1.61; acc: 0.55
Batch: 440; loss: 1.35; acc: 0.58
Batch: 460; loss: 1.7; acc: 0.41
Batch: 480; loss: 1.57; acc: 0.52
Batch: 500; loss: 1.63; acc: 0.52
Batch: 520; loss: 1.61; acc: 0.53
Batch: 540; loss: 1.42; acc: 0.56
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.55; acc: 0.55
Batch: 600; loss: 1.79; acc: 0.41
Batch: 620; loss: 1.54; acc: 0.52
Batch: 640; loss: 1.68; acc: 0.38
Batch: 660; loss: 1.71; acc: 0.5
Batch: 680; loss: 1.51; acc: 0.56
Batch: 700; loss: 1.49; acc: 0.58
Batch: 720; loss: 1.58; acc: 0.45
Batch: 740; loss: 1.63; acc: 0.44
Batch: 760; loss: 1.67; acc: 0.48
Batch: 780; loss: 1.6; acc: 0.61
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

9.476888953940943e-05
8.706680819159374e-05
Batch: 0; loss: 1.46; acc: 0.62
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.34; acc: 0.62
Batch: 60; loss: 1.41; acc: 0.61
Batch: 80; loss: 1.42; acc: 0.66
Batch: 100; loss: 1.63; acc: 0.5
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.48; acc: 0.69
Val Epoch over. val_loss: 1.558803934959849; val_accuracy: 0.551453025477707 

The current subspace-distance is: 8.706680819159374e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.76; acc: 0.42
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.61; acc: 0.61
Batch: 60; loss: 1.67; acc: 0.53
Batch: 80; loss: 1.42; acc: 0.61
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.64; acc: 0.53
Batch: 140; loss: 1.67; acc: 0.48
Batch: 160; loss: 1.5; acc: 0.58
Batch: 180; loss: 1.64; acc: 0.52
Batch: 200; loss: 1.67; acc: 0.52
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.65; acc: 0.53
Batch: 260; loss: 1.69; acc: 0.42
Batch: 280; loss: 1.53; acc: 0.55
Batch: 300; loss: 1.51; acc: 0.61
Batch: 320; loss: 1.57; acc: 0.53
Batch: 340; loss: 1.41; acc: 0.61
Batch: 360; loss: 1.73; acc: 0.52
Batch: 380; loss: 1.44; acc: 0.58
Batch: 400; loss: 1.37; acc: 0.62
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.6; acc: 0.55
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.68; acc: 0.52
Batch: 520; loss: 1.72; acc: 0.44
Batch: 540; loss: 1.52; acc: 0.48
Batch: 560; loss: 1.59; acc: 0.47
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.67; acc: 0.45
Batch: 620; loss: 1.56; acc: 0.55
Batch: 640; loss: 1.63; acc: 0.48
Batch: 660; loss: 1.57; acc: 0.61
Batch: 680; loss: 1.44; acc: 0.62
Batch: 700; loss: 1.59; acc: 0.55
Batch: 720; loss: 1.52; acc: 0.61
Batch: 740; loss: 1.59; acc: 0.48
Batch: 760; loss: 1.62; acc: 0.55
Batch: 780; loss: 1.52; acc: 0.53
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.439758287044242e-05
8.792174776317552e-05
Batch: 0; loss: 1.47; acc: 0.58
Batch: 20; loss: 1.71; acc: 0.5
Batch: 40; loss: 1.34; acc: 0.62
Batch: 60; loss: 1.4; acc: 0.61
Batch: 80; loss: 1.41; acc: 0.66
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.58; acc: 0.53
Batch: 140; loss: 1.46; acc: 0.7
Val Epoch over. val_loss: 1.5569077168300653; val_accuracy: 0.5456807324840764 

The current subspace-distance is: 8.792174776317552e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.44
Batch: 20; loss: 1.46; acc: 0.67
Batch: 40; loss: 1.63; acc: 0.48
Batch: 60; loss: 1.56; acc: 0.52
Batch: 80; loss: 1.5; acc: 0.53
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.47; acc: 0.61
Batch: 140; loss: 1.45; acc: 0.62
Batch: 160; loss: 1.63; acc: 0.52
Batch: 180; loss: 1.82; acc: 0.44
Batch: 200; loss: 1.69; acc: 0.5
Batch: 220; loss: 1.54; acc: 0.55
Batch: 240; loss: 1.63; acc: 0.47
Batch: 260; loss: 1.61; acc: 0.55
Batch: 280; loss: 1.6; acc: 0.56
Batch: 300; loss: 1.47; acc: 0.59
Batch: 320; loss: 1.61; acc: 0.48
Batch: 340; loss: 1.52; acc: 0.62
Batch: 360; loss: 1.65; acc: 0.52
Batch: 380; loss: 1.58; acc: 0.56
Batch: 400; loss: 1.51; acc: 0.56
Batch: 420; loss: 1.63; acc: 0.52
Batch: 440; loss: 1.65; acc: 0.48
Batch: 460; loss: 1.53; acc: 0.53
Batch: 480; loss: 1.43; acc: 0.64
Batch: 500; loss: 1.56; acc: 0.5
Batch: 520; loss: 1.52; acc: 0.56
Batch: 540; loss: 1.46; acc: 0.58
Batch: 560; loss: 1.46; acc: 0.62
Batch: 580; loss: 1.55; acc: 0.5
Batch: 600; loss: 1.6; acc: 0.56
Batch: 620; loss: 1.73; acc: 0.44
Batch: 640; loss: 1.55; acc: 0.55
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.54; acc: 0.56
Batch: 700; loss: 1.55; acc: 0.58
Batch: 720; loss: 1.62; acc: 0.56
Batch: 740; loss: 1.72; acc: 0.42
Batch: 760; loss: 1.54; acc: 0.48
Batch: 780; loss: 1.71; acc: 0.44
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.465203766012564e-05
8.768110274104401e-05
Batch: 0; loss: 1.49; acc: 0.58
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.35; acc: 0.62
Batch: 60; loss: 1.41; acc: 0.61
Batch: 80; loss: 1.43; acc: 0.66
Batch: 100; loss: 1.64; acc: 0.5
Batch: 120; loss: 1.6; acc: 0.53
Batch: 140; loss: 1.48; acc: 0.7
Val Epoch over. val_loss: 1.5659072254873385; val_accuracy: 0.5499601910828026 

The current subspace-distance is: 8.768110274104401e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.55; acc: 0.55
Batch: 20; loss: 1.57; acc: 0.5
Batch: 40; loss: 1.55; acc: 0.56
Batch: 60; loss: 1.8; acc: 0.39
Batch: 80; loss: 1.68; acc: 0.45
Batch: 100; loss: 1.7; acc: 0.45
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.56; acc: 0.59
Batch: 160; loss: 1.58; acc: 0.52
Batch: 180; loss: 1.53; acc: 0.52
Batch: 200; loss: 1.4; acc: 0.64
Batch: 220; loss: 1.51; acc: 0.53
Batch: 240; loss: 1.53; acc: 0.58
Batch: 260; loss: 1.74; acc: 0.52
Batch: 280; loss: 1.68; acc: 0.5
Batch: 300; loss: 1.6; acc: 0.55
Batch: 320; loss: 1.63; acc: 0.47
Batch: 340; loss: 1.45; acc: 0.61
Batch: 360; loss: 1.48; acc: 0.56
Batch: 380; loss: 1.61; acc: 0.52
Batch: 400; loss: 1.65; acc: 0.53
Batch: 420; loss: 1.62; acc: 0.53
Batch: 440; loss: 1.63; acc: 0.52
Batch: 460; loss: 1.59; acc: 0.53
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.5; acc: 0.59
Batch: 520; loss: 1.59; acc: 0.42
Batch: 540; loss: 1.49; acc: 0.61
Batch: 560; loss: 1.56; acc: 0.53
Batch: 580; loss: 1.63; acc: 0.5
Batch: 600; loss: 1.66; acc: 0.5
Batch: 620; loss: 1.5; acc: 0.61
Batch: 640; loss: 1.5; acc: 0.56
Batch: 660; loss: 1.56; acc: 0.59
Batch: 680; loss: 1.58; acc: 0.55
Batch: 700; loss: 1.58; acc: 0.5
Batch: 720; loss: 1.57; acc: 0.55
Batch: 740; loss: 1.54; acc: 0.56
Batch: 760; loss: 1.53; acc: 0.56
Batch: 780; loss: 1.63; acc: 0.47
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.4395931228064e-05
8.717243326827884e-05
Batch: 0; loss: 1.47; acc: 0.62
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.34; acc: 0.59
Batch: 60; loss: 1.38; acc: 0.61
Batch: 80; loss: 1.41; acc: 0.67
Batch: 100; loss: 1.62; acc: 0.47
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.45; acc: 0.67
Val Epoch over. val_loss: 1.551512421316402; val_accuracy: 0.5455812101910829 

The current subspace-distance is: 8.717243326827884e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.71; acc: 0.44
Batch: 20; loss: 1.38; acc: 0.64
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.63; acc: 0.42
Batch: 80; loss: 1.88; acc: 0.42
Batch: 100; loss: 1.61; acc: 0.45
Batch: 120; loss: 1.69; acc: 0.53
Batch: 140; loss: 1.56; acc: 0.45
Batch: 160; loss: 1.69; acc: 0.48
Batch: 180; loss: 1.62; acc: 0.45
Batch: 200; loss: 1.5; acc: 0.55
Batch: 220; loss: 1.53; acc: 0.56
Batch: 240; loss: 1.45; acc: 0.67
Batch: 260; loss: 1.86; acc: 0.42
Batch: 280; loss: 1.64; acc: 0.52
Batch: 300; loss: 1.54; acc: 0.52
Batch: 320; loss: 1.59; acc: 0.45
Batch: 340; loss: 1.66; acc: 0.42
Batch: 360; loss: 1.52; acc: 0.55
Batch: 380; loss: 1.64; acc: 0.44
Batch: 400; loss: 1.57; acc: 0.58
Batch: 420; loss: 1.47; acc: 0.53
Batch: 440; loss: 1.6; acc: 0.52
Batch: 460; loss: 1.67; acc: 0.48
Batch: 480; loss: 1.79; acc: 0.41
Batch: 500; loss: 1.55; acc: 0.55
Batch: 520; loss: 1.59; acc: 0.47
Batch: 540; loss: 1.47; acc: 0.48
Batch: 560; loss: 1.57; acc: 0.53
Batch: 580; loss: 1.6; acc: 0.53
Batch: 600; loss: 1.49; acc: 0.59
Batch: 620; loss: 1.72; acc: 0.42
Batch: 640; loss: 1.59; acc: 0.53
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.55; acc: 0.56
Batch: 700; loss: 1.76; acc: 0.44
Batch: 720; loss: 1.74; acc: 0.45
Batch: 740; loss: 1.52; acc: 0.55
Batch: 760; loss: 1.71; acc: 0.5
Batch: 780; loss: 1.61; acc: 0.55
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.35709904297255e-05
8.782292570685968e-05
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.62
Batch: 60; loss: 1.38; acc: 0.64
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.62; acc: 0.5
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.44; acc: 0.7
Val Epoch over. val_loss: 1.5493776092104092; val_accuracy: 0.5489649681528662 

The current subspace-distance is: 8.782292570685968e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.56
Batch: 40; loss: 1.68; acc: 0.5
Batch: 60; loss: 1.65; acc: 0.45
Batch: 80; loss: 1.6; acc: 0.55
Batch: 100; loss: 1.53; acc: 0.52
Batch: 120; loss: 1.72; acc: 0.53
Batch: 140; loss: 1.57; acc: 0.58
Batch: 160; loss: 1.61; acc: 0.5
Batch: 180; loss: 1.57; acc: 0.58
Batch: 200; loss: 1.58; acc: 0.55
Batch: 220; loss: 1.53; acc: 0.5
Batch: 240; loss: 1.55; acc: 0.5
Batch: 260; loss: 1.63; acc: 0.53
Batch: 280; loss: 1.76; acc: 0.48
Batch: 300; loss: 1.57; acc: 0.56
Batch: 320; loss: 1.58; acc: 0.56
Batch: 340; loss: 1.69; acc: 0.45
Batch: 360; loss: 1.61; acc: 0.56
Batch: 380; loss: 1.58; acc: 0.53
Batch: 400; loss: 1.41; acc: 0.58
Batch: 420; loss: 1.69; acc: 0.41
Batch: 440; loss: 1.62; acc: 0.5
Batch: 460; loss: 1.78; acc: 0.42
Batch: 480; loss: 1.51; acc: 0.64
Batch: 500; loss: 1.5; acc: 0.59
Batch: 520; loss: 1.6; acc: 0.5
Batch: 540; loss: 1.46; acc: 0.61
Batch: 560; loss: 1.63; acc: 0.58
Batch: 580; loss: 1.46; acc: 0.56
Batch: 600; loss: 1.55; acc: 0.58
Batch: 620; loss: 1.49; acc: 0.55
Batch: 640; loss: 1.54; acc: 0.5
Batch: 660; loss: 1.55; acc: 0.59
Batch: 680; loss: 1.66; acc: 0.45
Batch: 700; loss: 1.53; acc: 0.62
Batch: 720; loss: 1.45; acc: 0.55
Batch: 740; loss: 1.6; acc: 0.52
Batch: 760; loss: 1.58; acc: 0.5
Batch: 780; loss: 1.53; acc: 0.55
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.437516564503312e-05
8.82943204487674e-05
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.71; acc: 0.52
Batch: 40; loss: 1.34; acc: 0.62
Batch: 60; loss: 1.41; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.69
Batch: 100; loss: 1.63; acc: 0.48
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.46; acc: 0.72
Val Epoch over. val_loss: 1.557883142665693; val_accuracy: 0.5528463375796179 

The current subspace-distance is: 8.82943204487674e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.41
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.52; acc: 0.56
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.53; acc: 0.61
Batch: 100; loss: 1.49; acc: 0.58
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.58
Batch: 160; loss: 1.77; acc: 0.55
Batch: 180; loss: 1.53; acc: 0.52
Batch: 200; loss: 1.68; acc: 0.44
Batch: 220; loss: 1.53; acc: 0.55
Batch: 240; loss: 1.52; acc: 0.53
Batch: 260; loss: 1.64; acc: 0.55
Batch: 280; loss: 1.54; acc: 0.45
Batch: 300; loss: 1.38; acc: 0.66
Batch: 320; loss: 1.54; acc: 0.58
Batch: 340; loss: 1.59; acc: 0.56
Batch: 360; loss: 1.7; acc: 0.45
Batch: 380; loss: 1.71; acc: 0.47
Batch: 400; loss: 1.45; acc: 0.59
Batch: 420; loss: 1.84; acc: 0.38
Batch: 440; loss: 1.59; acc: 0.55
Batch: 460; loss: 1.53; acc: 0.53
Batch: 480; loss: 1.59; acc: 0.53
Batch: 500; loss: 1.71; acc: 0.48
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.58; acc: 0.55
Batch: 560; loss: 1.41; acc: 0.56
Batch: 580; loss: 1.68; acc: 0.53
Batch: 600; loss: 1.72; acc: 0.41
Batch: 620; loss: 1.48; acc: 0.58
Batch: 640; loss: 1.67; acc: 0.53
Batch: 660; loss: 1.54; acc: 0.58
Batch: 680; loss: 1.64; acc: 0.47
Batch: 700; loss: 1.54; acc: 0.55
Batch: 720; loss: 1.64; acc: 0.47
Batch: 740; loss: 1.56; acc: 0.58
Batch: 760; loss: 1.57; acc: 0.55
Batch: 780; loss: 1.51; acc: 0.53
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.495188714936376e-05
8.783358498476446e-05
Batch: 0; loss: 1.49; acc: 0.59
Batch: 20; loss: 1.73; acc: 0.45
Batch: 40; loss: 1.32; acc: 0.67
Batch: 60; loss: 1.39; acc: 0.62
Batch: 80; loss: 1.4; acc: 0.67
Batch: 100; loss: 1.65; acc: 0.47
Batch: 120; loss: 1.61; acc: 0.52
Batch: 140; loss: 1.44; acc: 0.72
Val Epoch over. val_loss: 1.5564404566576526; val_accuracy: 0.5527468152866242 

The current subspace-distance is: 8.783358498476446e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.42; acc: 0.58
Batch: 20; loss: 1.57; acc: 0.58
Batch: 40; loss: 1.46; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.53
Batch: 80; loss: 1.57; acc: 0.52
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.55; acc: 0.52
Batch: 160; loss: 1.77; acc: 0.45
Batch: 180; loss: 1.66; acc: 0.45
Batch: 200; loss: 1.78; acc: 0.42
Batch: 220; loss: 1.66; acc: 0.53
Batch: 240; loss: 1.44; acc: 0.56
Batch: 260; loss: 1.68; acc: 0.41
Batch: 280; loss: 1.61; acc: 0.53
Batch: 300; loss: 1.71; acc: 0.47
Batch: 320; loss: 1.69; acc: 0.44
Batch: 340; loss: 1.61; acc: 0.5
Batch: 360; loss: 1.46; acc: 0.59
Batch: 380; loss: 1.42; acc: 0.64
Batch: 400; loss: 1.75; acc: 0.48
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.68; acc: 0.52
Batch: 460; loss: 1.66; acc: 0.52
Batch: 480; loss: 1.49; acc: 0.59
Batch: 500; loss: 1.66; acc: 0.5
Batch: 520; loss: 1.5; acc: 0.53
Batch: 540; loss: 1.56; acc: 0.56
Batch: 560; loss: 1.51; acc: 0.58
Batch: 580; loss: 1.47; acc: 0.59
Batch: 600; loss: 1.81; acc: 0.44
Batch: 620; loss: 1.57; acc: 0.53
Batch: 640; loss: 1.53; acc: 0.55
Batch: 660; loss: 1.59; acc: 0.47
Batch: 680; loss: 1.56; acc: 0.62
Batch: 700; loss: 1.45; acc: 0.62
Batch: 720; loss: 1.64; acc: 0.5
Batch: 740; loss: 1.59; acc: 0.52
Batch: 760; loss: 1.54; acc: 0.61
Batch: 780; loss: 1.58; acc: 0.58
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.564125502947718e-05
8.87656060513109e-05
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.4; acc: 0.62
Batch: 80; loss: 1.4; acc: 0.69
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.46; acc: 0.69
Val Epoch over. val_loss: 1.5588603179166272; val_accuracy: 0.548765923566879 

The current subspace-distance is: 8.87656060513109e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.47; acc: 0.59
Batch: 40; loss: 1.5; acc: 0.56
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.52; acc: 0.55
Batch: 100; loss: 1.61; acc: 0.5
Batch: 120; loss: 1.56; acc: 0.53
Batch: 140; loss: 1.63; acc: 0.5
Batch: 160; loss: 1.72; acc: 0.47
Batch: 180; loss: 1.58; acc: 0.55
Batch: 200; loss: 1.6; acc: 0.55
Batch: 220; loss: 1.55; acc: 0.58
Batch: 240; loss: 1.64; acc: 0.48
Batch: 260; loss: 1.58; acc: 0.47
Batch: 280; loss: 1.66; acc: 0.5
Batch: 300; loss: 1.42; acc: 0.62
Batch: 320; loss: 1.57; acc: 0.52
Batch: 340; loss: 1.55; acc: 0.55
Batch: 360; loss: 1.65; acc: 0.44
Batch: 380; loss: 1.62; acc: 0.53
Batch: 400; loss: 1.51; acc: 0.56
Batch: 420; loss: 1.67; acc: 0.48
Batch: 440; loss: 1.59; acc: 0.42
Batch: 460; loss: 1.64; acc: 0.52
Batch: 480; loss: 1.71; acc: 0.44
Batch: 500; loss: 1.65; acc: 0.44
Batch: 520; loss: 1.67; acc: 0.52
Batch: 540; loss: 1.47; acc: 0.61
Batch: 560; loss: 1.65; acc: 0.5
Batch: 580; loss: 1.51; acc: 0.56
Batch: 600; loss: 1.69; acc: 0.52
Batch: 620; loss: 1.45; acc: 0.59
Batch: 640; loss: 1.67; acc: 0.48
Batch: 660; loss: 1.52; acc: 0.56
Batch: 680; loss: 1.51; acc: 0.52
Batch: 700; loss: 1.74; acc: 0.39
Batch: 720; loss: 1.43; acc: 0.61
Batch: 740; loss: 1.66; acc: 0.48
Batch: 760; loss: 1.57; acc: 0.45
Batch: 780; loss: 1.56; acc: 0.61
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.51229376369156e-05
8.710125257493928e-05
Batch: 0; loss: 1.45; acc: 0.61
Batch: 20; loss: 1.7; acc: 0.47
Batch: 40; loss: 1.32; acc: 0.62
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.39; acc: 0.72
Batch: 100; loss: 1.61; acc: 0.44
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 1.5535992771197276; val_accuracy: 0.5501592356687898 

The current subspace-distance is: 8.710125257493928e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.45; acc: 0.58
Batch: 40; loss: 1.48; acc: 0.58
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.39; acc: 0.62
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.55; acc: 0.48
Batch: 160; loss: 1.59; acc: 0.52
Batch: 180; loss: 1.67; acc: 0.5
Batch: 200; loss: 1.64; acc: 0.45
Batch: 220; loss: 1.45; acc: 0.56
Batch: 240; loss: 1.63; acc: 0.47
Batch: 260; loss: 1.56; acc: 0.5
Batch: 280; loss: 1.7; acc: 0.42
Batch: 300; loss: 1.41; acc: 0.64
Batch: 320; loss: 1.5; acc: 0.59
Batch: 340; loss: 1.61; acc: 0.53
Batch: 360; loss: 1.52; acc: 0.58
Batch: 380; loss: 1.48; acc: 0.58
Batch: 400; loss: 1.64; acc: 0.53
Batch: 420; loss: 1.62; acc: 0.45
Batch: 440; loss: 1.51; acc: 0.56
Batch: 460; loss: 1.58; acc: 0.56
Batch: 480; loss: 1.68; acc: 0.47
Batch: 500; loss: 1.55; acc: 0.56
Batch: 520; loss: 1.76; acc: 0.41
Batch: 540; loss: 1.48; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.47
Batch: 580; loss: 1.53; acc: 0.56
Batch: 600; loss: 1.63; acc: 0.47
Batch: 620; loss: 1.73; acc: 0.45
Batch: 640; loss: 1.56; acc: 0.58
Batch: 660; loss: 1.57; acc: 0.53
Batch: 680; loss: 1.77; acc: 0.44
Batch: 700; loss: 1.51; acc: 0.53
Batch: 720; loss: 1.62; acc: 0.5
Batch: 740; loss: 1.62; acc: 0.52
Batch: 760; loss: 1.62; acc: 0.5
Batch: 780; loss: 1.52; acc: 0.59
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.500810847384855e-05
8.660069579491392e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.38; acc: 0.62
Batch: 80; loss: 1.41; acc: 0.7
Batch: 100; loss: 1.62; acc: 0.47
Batch: 120; loss: 1.6; acc: 0.55
Batch: 140; loss: 1.46; acc: 0.69
Val Epoch over. val_loss: 1.554211548179578; val_accuracy: 0.553343949044586 

The current subspace-distance is: 8.660069579491392e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.42
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.61; acc: 0.5
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.61; acc: 0.5
Batch: 160; loss: 1.5; acc: 0.59
Batch: 180; loss: 1.58; acc: 0.47
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.59; acc: 0.48
Batch: 240; loss: 1.76; acc: 0.41
Batch: 260; loss: 1.68; acc: 0.5
Batch: 280; loss: 1.51; acc: 0.55
Batch: 300; loss: 1.7; acc: 0.47
Batch: 320; loss: 1.68; acc: 0.5
Batch: 340; loss: 1.75; acc: 0.45
Batch: 360; loss: 1.46; acc: 0.58
Batch: 380; loss: 1.49; acc: 0.61
Batch: 400; loss: 1.8; acc: 0.42
Batch: 420; loss: 1.68; acc: 0.5
Batch: 440; loss: 1.57; acc: 0.5
Batch: 460; loss: 1.52; acc: 0.61
Batch: 480; loss: 1.65; acc: 0.53
Batch: 500; loss: 1.63; acc: 0.47
Batch: 520; loss: 1.49; acc: 0.59
Batch: 540; loss: 1.55; acc: 0.48
Batch: 560; loss: 1.51; acc: 0.52
Batch: 580; loss: 1.61; acc: 0.5
Batch: 600; loss: 1.64; acc: 0.48
Batch: 620; loss: 1.54; acc: 0.58
Batch: 640; loss: 1.39; acc: 0.61
Batch: 660; loss: 1.72; acc: 0.48
Batch: 680; loss: 1.48; acc: 0.58
Batch: 700; loss: 1.56; acc: 0.58
Batch: 720; loss: 1.56; acc: 0.53
Batch: 740; loss: 1.59; acc: 0.47
Batch: 760; loss: 1.63; acc: 0.47
Batch: 780; loss: 1.59; acc: 0.5
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.698855137685314e-05
8.901383262127638e-05
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.38; acc: 0.61
Batch: 80; loss: 1.4; acc: 0.67
Batch: 100; loss: 1.62; acc: 0.5
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.44; acc: 0.69
Val Epoch over. val_loss: 1.5479702380052798; val_accuracy: 0.5504578025477707 

The current subspace-distance is: 8.901383262127638e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.52
Batch: 20; loss: 1.62; acc: 0.61
Batch: 40; loss: 1.58; acc: 0.52
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.7; acc: 0.45
Batch: 160; loss: 1.6; acc: 0.53
Batch: 180; loss: 1.42; acc: 0.58
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.64; acc: 0.55
Batch: 240; loss: 1.62; acc: 0.48
Batch: 260; loss: 1.68; acc: 0.53
Batch: 280; loss: 1.56; acc: 0.59
Batch: 300; loss: 1.69; acc: 0.47
Batch: 320; loss: 1.65; acc: 0.45
Batch: 340; loss: 1.48; acc: 0.55
Batch: 360; loss: 1.55; acc: 0.55
Batch: 380; loss: 1.49; acc: 0.55
Batch: 400; loss: 1.7; acc: 0.36
Batch: 420; loss: 1.48; acc: 0.62
Batch: 440; loss: 1.75; acc: 0.41
Batch: 460; loss: 1.56; acc: 0.58
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.59; acc: 0.52
Batch: 520; loss: 1.61; acc: 0.53
Batch: 540; loss: 1.62; acc: 0.5
Batch: 560; loss: 1.46; acc: 0.61
Batch: 580; loss: 1.67; acc: 0.42
Batch: 600; loss: 1.84; acc: 0.41
Batch: 620; loss: 1.62; acc: 0.44
Batch: 640; loss: 1.59; acc: 0.59
Batch: 660; loss: 1.8; acc: 0.44
Batch: 680; loss: 1.6; acc: 0.45
Batch: 700; loss: 1.53; acc: 0.55
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.63; acc: 0.48
Batch: 760; loss: 1.76; acc: 0.45
Batch: 780; loss: 1.68; acc: 0.53
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.666147525422275e-05
8.766259998083115e-05
Batch: 0; loss: 1.46; acc: 0.61
Batch: 20; loss: 1.69; acc: 0.52
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.4; acc: 0.67
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.46; acc: 0.69
Val Epoch over. val_loss: 1.5547914687235644; val_accuracy: 0.549562101910828 

The current subspace-distance is: 8.766259998083115e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.73; acc: 0.47
Batch: 40; loss: 1.5; acc: 0.58
Batch: 60; loss: 1.6; acc: 0.56
Batch: 80; loss: 1.73; acc: 0.45
Batch: 100; loss: 1.71; acc: 0.5
Batch: 120; loss: 1.56; acc: 0.55
Batch: 140; loss: 1.52; acc: 0.59
Batch: 160; loss: 1.48; acc: 0.52
Batch: 180; loss: 1.65; acc: 0.55
Batch: 200; loss: 1.52; acc: 0.56
Batch: 220; loss: 1.74; acc: 0.41
Batch: 240; loss: 1.5; acc: 0.55
Batch: 260; loss: 1.6; acc: 0.44
Batch: 280; loss: 1.49; acc: 0.67
Batch: 300; loss: 1.46; acc: 0.67
Batch: 320; loss: 1.52; acc: 0.53
Batch: 340; loss: 1.55; acc: 0.56
Batch: 360; loss: 1.66; acc: 0.48
Batch: 380; loss: 1.59; acc: 0.55
Batch: 400; loss: 1.55; acc: 0.59
Batch: 420; loss: 1.52; acc: 0.58
Batch: 440; loss: 1.54; acc: 0.59
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.35; acc: 0.62
Batch: 500; loss: 1.63; acc: 0.48
Batch: 520; loss: 1.52; acc: 0.55
Batch: 540; loss: 1.73; acc: 0.41
Batch: 560; loss: 1.53; acc: 0.55
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.5; acc: 0.61
Batch: 620; loss: 1.67; acc: 0.47
Batch: 640; loss: 1.61; acc: 0.48
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.71; acc: 0.39
Batch: 700; loss: 1.43; acc: 0.62
Batch: 720; loss: 1.53; acc: 0.48
Batch: 740; loss: 1.78; acc: 0.44
Batch: 760; loss: 1.53; acc: 0.56
Batch: 780; loss: 1.58; acc: 0.5
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.591877460479736e-05
8.837539644446224e-05
Batch: 0; loss: 1.44; acc: 0.62
Batch: 20; loss: 1.69; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.66
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.37; acc: 0.72
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.45; acc: 0.7
Val Epoch over. val_loss: 1.5468472728304044; val_accuracy: 0.5522492038216561 

The current subspace-distance is: 8.837539644446224e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.58; acc: 0.52
Batch: 40; loss: 1.46; acc: 0.61
Batch: 60; loss: 1.51; acc: 0.56
Batch: 80; loss: 1.38; acc: 0.62
Batch: 100; loss: 1.68; acc: 0.47
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.79; acc: 0.39
Batch: 160; loss: 1.53; acc: 0.55
Batch: 180; loss: 1.74; acc: 0.42
Batch: 200; loss: 1.58; acc: 0.45
Batch: 220; loss: 1.58; acc: 0.56
Batch: 240; loss: 1.71; acc: 0.52
Batch: 260; loss: 1.48; acc: 0.53
Batch: 280; loss: 1.43; acc: 0.61
Batch: 300; loss: 1.62; acc: 0.48
Batch: 320; loss: 1.66; acc: 0.41
Batch: 340; loss: 1.62; acc: 0.56
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.55; acc: 0.53
Batch: 400; loss: 1.52; acc: 0.58
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.63; acc: 0.5
Batch: 460; loss: 1.43; acc: 0.64
Batch: 480; loss: 1.51; acc: 0.56
Batch: 500; loss: 1.53; acc: 0.58
Batch: 520; loss: 1.87; acc: 0.36
Batch: 540; loss: 1.8; acc: 0.47
Batch: 560; loss: 1.64; acc: 0.44
Batch: 580; loss: 1.56; acc: 0.56
Batch: 600; loss: 1.57; acc: 0.58
Batch: 620; loss: 1.49; acc: 0.56
Batch: 640; loss: 1.47; acc: 0.61
Batch: 660; loss: 1.58; acc: 0.55
Batch: 680; loss: 1.57; acc: 0.62
Batch: 700; loss: 1.68; acc: 0.48
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.49; acc: 0.56
Batch: 760; loss: 1.55; acc: 0.5
Batch: 780; loss: 1.38; acc: 0.64
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.659877105150372e-05
8.971730130724609e-05
Batch: 0; loss: 1.48; acc: 0.61
Batch: 20; loss: 1.72; acc: 0.48
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.39; acc: 0.62
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.53
Batch: 140; loss: 1.48; acc: 0.62
Val Epoch over. val_loss: 1.5630517909481267; val_accuracy: 0.54796974522293 

The current subspace-distance is: 8.971730130724609e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.44
Batch: 20; loss: 1.57; acc: 0.53
Batch: 40; loss: 1.58; acc: 0.59
Batch: 60; loss: 1.59; acc: 0.42
Batch: 80; loss: 1.82; acc: 0.42
Batch: 100; loss: 1.71; acc: 0.48
Batch: 120; loss: 1.52; acc: 0.53
Batch: 140; loss: 1.58; acc: 0.56
Batch: 160; loss: 1.64; acc: 0.45
Batch: 180; loss: 1.61; acc: 0.48
Batch: 200; loss: 1.54; acc: 0.58
Batch: 220; loss: 1.77; acc: 0.47
Batch: 240; loss: 1.53; acc: 0.56
Batch: 260; loss: 1.56; acc: 0.5
Batch: 280; loss: 1.58; acc: 0.52
Batch: 300; loss: 1.56; acc: 0.53
Batch: 320; loss: 1.57; acc: 0.55
Batch: 340; loss: 1.54; acc: 0.5
Batch: 360; loss: 1.75; acc: 0.44
Batch: 380; loss: 1.66; acc: 0.42
Batch: 400; loss: 1.6; acc: 0.48
Batch: 420; loss: 1.5; acc: 0.55
Batch: 440; loss: 1.56; acc: 0.55
Batch: 460; loss: 1.47; acc: 0.69
Batch: 480; loss: 1.71; acc: 0.45
Batch: 500; loss: 1.5; acc: 0.55
Batch: 520; loss: 1.36; acc: 0.64
Batch: 540; loss: 1.46; acc: 0.59
Batch: 560; loss: 1.62; acc: 0.53
Batch: 580; loss: 1.59; acc: 0.52
Batch: 600; loss: 1.54; acc: 0.66
Batch: 620; loss: 1.72; acc: 0.44
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.51; acc: 0.58
Batch: 680; loss: 1.47; acc: 0.55
Batch: 700; loss: 1.67; acc: 0.47
Batch: 720; loss: 1.4; acc: 0.62
Batch: 740; loss: 1.69; acc: 0.45
Batch: 760; loss: 1.48; acc: 0.62
Batch: 780; loss: 1.49; acc: 0.59
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.621209028409794e-05
8.755301678320393e-05
Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.38; acc: 0.62
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.63; acc: 0.45
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 1.557363304362935; val_accuracy: 0.5564291401273885 

The current subspace-distance is: 8.755301678320393e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.64; acc: 0.48
Batch: 20; loss: 1.58; acc: 0.55
Batch: 40; loss: 1.65; acc: 0.52
Batch: 60; loss: 1.62; acc: 0.52
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.49; acc: 0.55
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.56; acc: 0.47
Batch: 160; loss: 1.41; acc: 0.56
Batch: 180; loss: 1.61; acc: 0.59
Batch: 200; loss: 1.58; acc: 0.48
Batch: 220; loss: 1.55; acc: 0.52
Batch: 240; loss: 1.56; acc: 0.55
Batch: 260; loss: 1.64; acc: 0.48
Batch: 280; loss: 1.55; acc: 0.55
Batch: 300; loss: 1.6; acc: 0.52
Batch: 320; loss: 1.81; acc: 0.42
Batch: 340; loss: 1.65; acc: 0.53
Batch: 360; loss: 1.61; acc: 0.5
Batch: 380; loss: 1.42; acc: 0.62
Batch: 400; loss: 1.46; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.59
Batch: 440; loss: 1.68; acc: 0.52
Batch: 460; loss: 1.45; acc: 0.61
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.76; acc: 0.44
Batch: 520; loss: 1.58; acc: 0.58
Batch: 540; loss: 1.49; acc: 0.55
Batch: 560; loss: 1.71; acc: 0.45
Batch: 580; loss: 1.65; acc: 0.47
Batch: 600; loss: 1.58; acc: 0.52
Batch: 620; loss: 1.55; acc: 0.62
Batch: 640; loss: 1.42; acc: 0.62
Batch: 660; loss: 1.6; acc: 0.5
Batch: 680; loss: 1.55; acc: 0.56
Batch: 700; loss: 1.51; acc: 0.58
Batch: 720; loss: 1.45; acc: 0.55
Batch: 740; loss: 1.57; acc: 0.55
Batch: 760; loss: 1.56; acc: 0.58
Batch: 780; loss: 1.61; acc: 0.55
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.668382699601352e-05
8.979687845567241e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.7; acc: 0.47
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.37; acc: 0.62
Batch: 80; loss: 1.41; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.46; acc: 0.66
Val Epoch over. val_loss: 1.549299969794644; val_accuracy: 0.5538415605095541 

The current subspace-distance is: 8.979687845567241e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.69
Batch: 20; loss: 1.49; acc: 0.53
Batch: 40; loss: 1.59; acc: 0.55
Batch: 60; loss: 1.74; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.63; acc: 0.48
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 1.6; acc: 0.52
Batch: 160; loss: 1.54; acc: 0.53
Batch: 180; loss: 1.78; acc: 0.41
Batch: 200; loss: 1.69; acc: 0.5
Batch: 220; loss: 1.43; acc: 0.61
Batch: 240; loss: 1.61; acc: 0.53
Batch: 260; loss: 1.74; acc: 0.45
Batch: 280; loss: 1.68; acc: 0.48
Batch: 300; loss: 1.53; acc: 0.53
Batch: 320; loss: 1.6; acc: 0.55
Batch: 340; loss: 1.63; acc: 0.44
Batch: 360; loss: 1.7; acc: 0.47
Batch: 380; loss: 1.66; acc: 0.48
Batch: 400; loss: 1.59; acc: 0.56
Batch: 420; loss: 1.57; acc: 0.56
Batch: 440; loss: 1.82; acc: 0.39
Batch: 460; loss: 1.51; acc: 0.58
Batch: 480; loss: 1.6; acc: 0.55
Batch: 500; loss: 1.58; acc: 0.59
Batch: 520; loss: 1.56; acc: 0.59
Batch: 540; loss: 1.62; acc: 0.41
Batch: 560; loss: 1.47; acc: 0.59
Batch: 580; loss: 1.65; acc: 0.42
Batch: 600; loss: 1.5; acc: 0.53
Batch: 620; loss: 1.68; acc: 0.48
Batch: 640; loss: 1.48; acc: 0.61
Batch: 660; loss: 1.68; acc: 0.5
Batch: 680; loss: 1.77; acc: 0.36
Batch: 700; loss: 1.52; acc: 0.52
Batch: 720; loss: 1.78; acc: 0.44
Batch: 740; loss: 1.47; acc: 0.58
Batch: 760; loss: 1.51; acc: 0.52
Batch: 780; loss: 1.59; acc: 0.58
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

9.626478276913986e-05
8.954002259997651e-05
Batch: 0; loss: 1.45; acc: 0.62
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.39; acc: 0.7
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 1.46; acc: 0.62
Val Epoch over. val_loss: 1.5507164358333418; val_accuracy: 0.54796974522293 

The current subspace-distance is: 8.954002259997651e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_1_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 13.5

The number of parameters is: 279504

The number of individual parameters is:

108
432
108
108
162
52488
162
162
324
157464
324
324
64
62208
64
64
4096
64
640
10
64
64

nonzero elements in E: 55900796
elements in E: 55900800
fraction nonzero: 0.9999999284446734
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.37; acc: 0.11
Batch: 20; loss: 2.12; acc: 0.19
Batch: 40; loss: 2.04; acc: 0.33
Batch: 60; loss: 2.06; acc: 0.23
Batch: 80; loss: 1.97; acc: 0.36
Batch: 100; loss: 2.04; acc: 0.33
Batch: 120; loss: 1.92; acc: 0.39
Batch: 140; loss: 1.83; acc: 0.44
Batch: 160; loss: 1.93; acc: 0.39
Batch: 180; loss: 1.87; acc: 0.38
Batch: 200; loss: 1.76; acc: 0.52
Batch: 220; loss: 1.92; acc: 0.38
Batch: 240; loss: 1.82; acc: 0.47
Batch: 260; loss: 1.81; acc: 0.44
Batch: 280; loss: 1.67; acc: 0.55
Batch: 300; loss: 1.87; acc: 0.38
Batch: 320; loss: 1.82; acc: 0.38
Batch: 340; loss: 1.85; acc: 0.44
Batch: 360; loss: 1.83; acc: 0.42
Batch: 380; loss: 1.75; acc: 0.45
Batch: 400; loss: 1.79; acc: 0.5
Batch: 420; loss: 1.6; acc: 0.58
Batch: 440; loss: 1.68; acc: 0.5
Batch: 460; loss: 1.77; acc: 0.48
Batch: 480; loss: 1.78; acc: 0.45
Batch: 500; loss: 1.7; acc: 0.5
Batch: 520; loss: 1.76; acc: 0.48
Batch: 540; loss: 1.77; acc: 0.42
Batch: 560; loss: 1.67; acc: 0.42
Batch: 580; loss: 1.64; acc: 0.47
Batch: 600; loss: 1.68; acc: 0.52
Batch: 620; loss: 1.69; acc: 0.56
Batch: 640; loss: 1.99; acc: 0.31
Batch: 660; loss: 1.62; acc: 0.61
Batch: 680; loss: 1.7; acc: 0.44
Batch: 700; loss: 1.69; acc: 0.48
Batch: 720; loss: 1.73; acc: 0.47
Batch: 740; loss: 1.67; acc: 0.55
Batch: 760; loss: 1.71; acc: 0.52
Batch: 780; loss: 1.64; acc: 0.55
Train Epoch over. train_loss: 1.82; train_accuracy: 0.43 

4.689113484346308e-05
3.7151123251533136e-05
Batch: 0; loss: 1.65; acc: 0.48
Batch: 20; loss: 1.79; acc: 0.45
Batch: 40; loss: 1.44; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.55
Batch: 80; loss: 1.54; acc: 0.59
Batch: 100; loss: 1.71; acc: 0.56
Batch: 120; loss: 1.76; acc: 0.45
Batch: 140; loss: 1.51; acc: 0.69
Val Epoch over. val_loss: 1.66564983623043; val_accuracy: 0.4955214968152866 

The current subspace-distance is: 3.7151123251533136e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.5
Batch: 20; loss: 1.68; acc: 0.5
Batch: 40; loss: 1.86; acc: 0.48
Batch: 60; loss: 1.68; acc: 0.55
Batch: 80; loss: 1.72; acc: 0.48
Batch: 100; loss: 1.69; acc: 0.56
Batch: 120; loss: 1.65; acc: 0.55
Batch: 140; loss: 1.59; acc: 0.58
Batch: 160; loss: 1.72; acc: 0.59
Batch: 180; loss: 1.58; acc: 0.61
Batch: 200; loss: 1.63; acc: 0.55
Batch: 220; loss: 1.64; acc: 0.52
Batch: 240; loss: 1.53; acc: 0.64
Batch: 260; loss: 1.6; acc: 0.58
Batch: 280; loss: 1.62; acc: 0.5
Batch: 300; loss: 1.54; acc: 0.56
Batch: 320; loss: 1.53; acc: 0.61
Batch: 340; loss: 1.64; acc: 0.52
Batch: 360; loss: 1.72; acc: 0.47
Batch: 380; loss: 1.7; acc: 0.47
Batch: 400; loss: 1.5; acc: 0.59
Batch: 420; loss: 1.56; acc: 0.59
Batch: 440; loss: 1.6; acc: 0.56
Batch: 460; loss: 1.6; acc: 0.52
Batch: 480; loss: 1.43; acc: 0.67
Batch: 500; loss: 1.57; acc: 0.61
Batch: 520; loss: 1.68; acc: 0.52
Batch: 540; loss: 1.61; acc: 0.56
Batch: 560; loss: 1.61; acc: 0.58
Batch: 580; loss: 1.77; acc: 0.39
Batch: 600; loss: 1.58; acc: 0.55
Batch: 620; loss: 1.57; acc: 0.53
Batch: 640; loss: 1.75; acc: 0.48
Batch: 660; loss: 1.56; acc: 0.58
Batch: 680; loss: 1.47; acc: 0.59
Batch: 700; loss: 1.67; acc: 0.5
Batch: 720; loss: 1.58; acc: 0.52
Batch: 740; loss: 1.54; acc: 0.55
Batch: 760; loss: 1.49; acc: 0.61
Batch: 780; loss: 1.54; acc: 0.56
Train Epoch over. train_loss: 1.64; train_accuracy: 0.52 

5.8356734371045604e-05
5.133900776854716e-05
Batch: 0; loss: 1.53; acc: 0.56
Batch: 20; loss: 1.76; acc: 0.5
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.61
Batch: 80; loss: 1.54; acc: 0.55
Batch: 100; loss: 1.69; acc: 0.55
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.44; acc: 0.64
Val Epoch over. val_loss: 1.5926993819558697; val_accuracy: 0.5336385350318471 

The current subspace-distance is: 5.133900776854716e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.55
Batch: 20; loss: 1.59; acc: 0.53
Batch: 40; loss: 1.6; acc: 0.62
Batch: 60; loss: 1.48; acc: 0.64
Batch: 80; loss: 1.64; acc: 0.41
Batch: 100; loss: 1.61; acc: 0.53
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.53; acc: 0.56
Batch: 160; loss: 1.62; acc: 0.47
Batch: 180; loss: 1.68; acc: 0.44
Batch: 200; loss: 1.44; acc: 0.59
Batch: 220; loss: 1.66; acc: 0.45
Batch: 240; loss: 1.58; acc: 0.58
Batch: 260; loss: 1.68; acc: 0.5
Batch: 280; loss: 1.6; acc: 0.5
Batch: 300; loss: 1.65; acc: 0.53
Batch: 320; loss: 1.69; acc: 0.45
Batch: 340; loss: 1.41; acc: 0.69
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 1.58; acc: 0.59
Batch: 400; loss: 1.67; acc: 0.53
Batch: 420; loss: 1.56; acc: 0.56
Batch: 440; loss: 1.5; acc: 0.55
Batch: 460; loss: 1.39; acc: 0.67
Batch: 480; loss: 1.56; acc: 0.61
Batch: 500; loss: 1.51; acc: 0.53
Batch: 520; loss: 1.47; acc: 0.59
Batch: 540; loss: 1.51; acc: 0.58
Batch: 560; loss: 1.76; acc: 0.36
Batch: 580; loss: 1.54; acc: 0.53
Batch: 600; loss: 1.36; acc: 0.66
Batch: 620; loss: 1.59; acc: 0.56
Batch: 640; loss: 1.62; acc: 0.52
Batch: 660; loss: 1.4; acc: 0.66
Batch: 680; loss: 1.51; acc: 0.61
Batch: 700; loss: 1.4; acc: 0.64
Batch: 720; loss: 1.51; acc: 0.58
Batch: 740; loss: 1.47; acc: 0.62
Batch: 760; loss: 1.47; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.47
Train Epoch over. train_loss: 1.56; train_accuracy: 0.55 

6.869836215628311e-05
6.111813127063215e-05
Batch: 0; loss: 1.38; acc: 0.66
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.31; acc: 0.58
Batch: 60; loss: 1.52; acc: 0.53
Batch: 80; loss: 1.34; acc: 0.5
Batch: 100; loss: 1.48; acc: 0.67
Batch: 120; loss: 1.56; acc: 0.62
Batch: 140; loss: 1.38; acc: 0.62
Val Epoch over. val_loss: 1.5214950613155487; val_accuracy: 0.5416998407643312 

The current subspace-distance is: 6.111813127063215e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.61
Batch: 20; loss: 1.66; acc: 0.5
Batch: 40; loss: 1.5; acc: 0.61
Batch: 60; loss: 1.5; acc: 0.62
Batch: 80; loss: 1.53; acc: 0.59
Batch: 100; loss: 1.56; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.69; acc: 0.5
Batch: 160; loss: 1.46; acc: 0.59
Batch: 180; loss: 1.45; acc: 0.59
Batch: 200; loss: 1.39; acc: 0.69
Batch: 220; loss: 1.54; acc: 0.52
Batch: 240; loss: 1.47; acc: 0.58
Batch: 260; loss: 1.52; acc: 0.59
Batch: 280; loss: 1.47; acc: 0.64
Batch: 300; loss: 1.43; acc: 0.64
Batch: 320; loss: 1.52; acc: 0.55
Batch: 340; loss: 1.43; acc: 0.61
Batch: 360; loss: 1.5; acc: 0.58
Batch: 380; loss: 1.56; acc: 0.52
Batch: 400; loss: 1.56; acc: 0.48
Batch: 420; loss: 1.65; acc: 0.52
Batch: 440; loss: 1.58; acc: 0.52
Batch: 460; loss: 1.47; acc: 0.58
Batch: 480; loss: 1.67; acc: 0.39
Batch: 500; loss: 1.4; acc: 0.64
Batch: 520; loss: 1.41; acc: 0.61
Batch: 540; loss: 1.42; acc: 0.62
Batch: 560; loss: 1.68; acc: 0.5
Batch: 580; loss: 1.56; acc: 0.52
Batch: 600; loss: 1.78; acc: 0.5
Batch: 620; loss: 1.26; acc: 0.77
Batch: 640; loss: 1.5; acc: 0.58
Batch: 660; loss: 1.64; acc: 0.47
Batch: 680; loss: 1.44; acc: 0.62
Batch: 700; loss: 1.39; acc: 0.58
Batch: 720; loss: 1.58; acc: 0.52
Batch: 740; loss: 1.52; acc: 0.55
Batch: 760; loss: 1.45; acc: 0.67
Batch: 780; loss: 1.49; acc: 0.53
Train Epoch over. train_loss: 1.51; train_accuracy: 0.57 

7.46516088838689e-05
6.817549729021266e-05
Batch: 0; loss: 1.43; acc: 0.58
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.33; acc: 0.58
Batch: 60; loss: 1.38; acc: 0.56
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.51; acc: 0.66
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.4; acc: 0.67
Val Epoch over. val_loss: 1.5054395373459835; val_accuracy: 0.5596138535031847 

The current subspace-distance is: 6.817549729021266e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.58
Batch: 20; loss: 1.46; acc: 0.55
Batch: 40; loss: 1.59; acc: 0.5
Batch: 60; loss: 1.44; acc: 0.64
Batch: 80; loss: 1.47; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.59
Batch: 120; loss: 1.48; acc: 0.59
Batch: 140; loss: 1.46; acc: 0.56
Batch: 160; loss: 1.5; acc: 0.62
Batch: 180; loss: 1.52; acc: 0.58
Batch: 200; loss: 1.57; acc: 0.55
Batch: 220; loss: 1.42; acc: 0.61
Batch: 240; loss: 1.57; acc: 0.5
Batch: 260; loss: 1.56; acc: 0.47
Batch: 280; loss: 1.54; acc: 0.53
Batch: 300; loss: 1.54; acc: 0.61
Batch: 320; loss: 1.31; acc: 0.69
Batch: 340; loss: 1.43; acc: 0.66
Batch: 360; loss: 1.51; acc: 0.56
Batch: 380; loss: 1.41; acc: 0.55
Batch: 400; loss: 1.62; acc: 0.5
Batch: 420; loss: 1.71; acc: 0.5
Batch: 440; loss: 1.57; acc: 0.58
Batch: 460; loss: 1.34; acc: 0.66
Batch: 480; loss: 1.5; acc: 0.56
Batch: 500; loss: 1.62; acc: 0.52
Batch: 520; loss: 1.34; acc: 0.69
Batch: 540; loss: 1.51; acc: 0.55
Batch: 560; loss: 1.39; acc: 0.64
Batch: 580; loss: 1.29; acc: 0.7
Batch: 600; loss: 1.6; acc: 0.52
Batch: 620; loss: 1.43; acc: 0.64
Batch: 640; loss: 1.39; acc: 0.59
Batch: 660; loss: 1.51; acc: 0.58
Batch: 680; loss: 1.47; acc: 0.56
Batch: 700; loss: 1.37; acc: 0.67
Batch: 720; loss: 1.68; acc: 0.45
Batch: 740; loss: 1.36; acc: 0.61
Batch: 760; loss: 1.57; acc: 0.53
Batch: 780; loss: 1.59; acc: 0.5
Train Epoch over. train_loss: 1.47; train_accuracy: 0.59 

7.966443808982149e-05
7.285097672138363e-05
Batch: 0; loss: 1.36; acc: 0.62
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.43; acc: 0.55
Batch: 80; loss: 1.31; acc: 0.67
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.53; acc: 0.5
Batch: 140; loss: 1.28; acc: 0.72
Val Epoch over. val_loss: 1.473516878049085; val_accuracy: 0.5784235668789809 

The current subspace-distance is: 7.285097672138363e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.5; acc: 0.56
Batch: 20; loss: 1.48; acc: 0.62
Batch: 40; loss: 1.47; acc: 0.59
Batch: 60; loss: 1.29; acc: 0.69
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.3; acc: 0.73
Batch: 120; loss: 1.39; acc: 0.62
Batch: 140; loss: 1.36; acc: 0.64
Batch: 160; loss: 1.48; acc: 0.66
Batch: 180; loss: 1.48; acc: 0.55
Batch: 200; loss: 1.33; acc: 0.67
Batch: 220; loss: 1.43; acc: 0.62
Batch: 240; loss: 1.55; acc: 0.61
Batch: 260; loss: 1.29; acc: 0.72
Batch: 280; loss: 1.41; acc: 0.62
Batch: 300; loss: 1.61; acc: 0.52
Batch: 320; loss: 1.51; acc: 0.58
Batch: 340; loss: 1.34; acc: 0.62
Batch: 360; loss: 1.6; acc: 0.45
Batch: 380; loss: 1.38; acc: 0.69
Batch: 400; loss: 1.47; acc: 0.66
Batch: 420; loss: 1.29; acc: 0.66
Batch: 440; loss: 1.48; acc: 0.56
Batch: 460; loss: 1.65; acc: 0.45
Batch: 480; loss: 1.52; acc: 0.62
Batch: 500; loss: 1.44; acc: 0.62
Batch: 520; loss: 1.52; acc: 0.53
Batch: 540; loss: 1.35; acc: 0.64
Batch: 560; loss: 1.37; acc: 0.61
Batch: 580; loss: 1.49; acc: 0.55
Batch: 600; loss: 1.34; acc: 0.62
Batch: 620; loss: 1.42; acc: 0.62
Batch: 640; loss: 1.43; acc: 0.62
Batch: 660; loss: 1.42; acc: 0.64
Batch: 680; loss: 1.44; acc: 0.67
Batch: 700; loss: 1.45; acc: 0.55
Batch: 720; loss: 1.55; acc: 0.58
Batch: 740; loss: 1.29; acc: 0.64
Batch: 760; loss: 1.58; acc: 0.5
Batch: 780; loss: 1.4; acc: 0.67
Train Epoch over. train_loss: 1.45; train_accuracy: 0.6 

8.558385889045894e-05
7.905871461844072e-05
Batch: 0; loss: 1.28; acc: 0.81
Batch: 20; loss: 1.65; acc: 0.5
Batch: 40; loss: 1.21; acc: 0.75
Batch: 60; loss: 1.39; acc: 0.62
Batch: 80; loss: 1.23; acc: 0.73
Batch: 100; loss: 1.46; acc: 0.67
Batch: 120; loss: 1.52; acc: 0.56
Batch: 140; loss: 1.22; acc: 0.72
Val Epoch over. val_loss: 1.4260423783284084; val_accuracy: 0.6226114649681529 

The current subspace-distance is: 7.905871461844072e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.58
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 1.52; acc: 0.56
Batch: 60; loss: 1.37; acc: 0.61
Batch: 80; loss: 1.31; acc: 0.64
Batch: 100; loss: 1.52; acc: 0.62
Batch: 120; loss: 1.44; acc: 0.59
Batch: 140; loss: 1.32; acc: 0.66
Batch: 160; loss: 1.43; acc: 0.58
Batch: 180; loss: 1.34; acc: 0.62
Batch: 200; loss: 1.31; acc: 0.67
Batch: 220; loss: 1.45; acc: 0.52
Batch: 240; loss: 1.55; acc: 0.56
Batch: 260; loss: 1.5; acc: 0.55
Batch: 280; loss: 1.44; acc: 0.61
Batch: 300; loss: 1.44; acc: 0.62
Batch: 320; loss: 1.48; acc: 0.5
Batch: 340; loss: 1.36; acc: 0.7
Batch: 360; loss: 1.52; acc: 0.53
Batch: 380; loss: 1.3; acc: 0.66
Batch: 400; loss: 1.42; acc: 0.64
Batch: 420; loss: 1.5; acc: 0.59
Batch: 440; loss: 1.3; acc: 0.62
Batch: 460; loss: 1.42; acc: 0.62
Batch: 480; loss: 1.6; acc: 0.45
Batch: 500; loss: 1.38; acc: 0.62
Batch: 520; loss: 1.39; acc: 0.67
Batch: 540; loss: 1.46; acc: 0.56
Batch: 560; loss: 1.52; acc: 0.55
Batch: 580; loss: 1.3; acc: 0.66
Batch: 600; loss: 1.46; acc: 0.56
Batch: 620; loss: 1.49; acc: 0.55
Batch: 640; loss: 1.43; acc: 0.59
Batch: 660; loss: 1.33; acc: 0.72
Batch: 680; loss: 1.45; acc: 0.58
Batch: 700; loss: 1.36; acc: 0.64
Batch: 720; loss: 1.51; acc: 0.55
Batch: 740; loss: 1.49; acc: 0.5
Batch: 760; loss: 1.55; acc: 0.61
Batch: 780; loss: 1.38; acc: 0.61
Train Epoch over. train_loss: 1.43; train_accuracy: 0.6 

9.002919250633568e-05
8.331364369951189e-05
Batch: 0; loss: 1.29; acc: 0.73
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.27; acc: 0.59
Batch: 60; loss: 1.41; acc: 0.59
Batch: 80; loss: 1.33; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.64
Batch: 120; loss: 1.52; acc: 0.55
Batch: 140; loss: 1.28; acc: 0.78
Val Epoch over. val_loss: 1.442889202931884; val_accuracy: 0.6005175159235668 

The current subspace-distance is: 8.331364369951189e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.55
Batch: 20; loss: 1.25; acc: 0.73
Batch: 40; loss: 1.52; acc: 0.62
Batch: 60; loss: 1.33; acc: 0.75
Batch: 80; loss: 1.41; acc: 0.58
Batch: 100; loss: 1.29; acc: 0.69
Batch: 120; loss: 1.35; acc: 0.59
Batch: 140; loss: 1.4; acc: 0.64
Batch: 160; loss: 1.5; acc: 0.53
Batch: 180; loss: 1.28; acc: 0.64
Batch: 200; loss: 1.48; acc: 0.52
Batch: 220; loss: 1.35; acc: 0.61
Batch: 240; loss: 1.56; acc: 0.53
Batch: 260; loss: 1.32; acc: 0.61
Batch: 280; loss: 1.6; acc: 0.53
Batch: 300; loss: 1.41; acc: 0.56
Batch: 320; loss: 1.49; acc: 0.53
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 1.25; acc: 0.72
Batch: 380; loss: 1.38; acc: 0.61
Batch: 400; loss: 1.42; acc: 0.61
Batch: 420; loss: 1.14; acc: 0.81
Batch: 440; loss: 1.5; acc: 0.55
Batch: 460; loss: 1.37; acc: 0.73
Batch: 480; loss: 1.36; acc: 0.61
Batch: 500; loss: 1.5; acc: 0.55
Batch: 520; loss: 1.31; acc: 0.59
Batch: 540; loss: 1.36; acc: 0.64
Batch: 560; loss: 1.33; acc: 0.55
Batch: 580; loss: 1.58; acc: 0.52
Batch: 600; loss: 1.39; acc: 0.64
Batch: 620; loss: 1.34; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.62
Batch: 660; loss: 1.43; acc: 0.52
Batch: 680; loss: 1.45; acc: 0.66
Batch: 700; loss: 1.24; acc: 0.67
Batch: 720; loss: 1.42; acc: 0.62
Batch: 740; loss: 1.37; acc: 0.64
Batch: 760; loss: 1.45; acc: 0.52
Batch: 780; loss: 1.37; acc: 0.62
Train Epoch over. train_loss: 1.4; train_accuracy: 0.61 

9.762582340044901e-05
9.170951670967042e-05
Batch: 0; loss: 1.44; acc: 0.56
Batch: 20; loss: 2.09; acc: 0.34
Batch: 40; loss: 1.55; acc: 0.48
Batch: 60; loss: 1.66; acc: 0.44
Batch: 80; loss: 1.48; acc: 0.55
Batch: 100; loss: 1.77; acc: 0.38
Batch: 120; loss: 1.77; acc: 0.47
Batch: 140; loss: 1.66; acc: 0.39
Val Epoch over. val_loss: 1.8361341672338498; val_accuracy: 0.4195859872611465 

The current subspace-distance is: 9.170951670967042e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.59
Batch: 20; loss: 1.34; acc: 0.64
Batch: 40; loss: 1.48; acc: 0.55
Batch: 60; loss: 1.43; acc: 0.53
Batch: 80; loss: 1.42; acc: 0.53
Batch: 100; loss: 1.25; acc: 0.69
Batch: 120; loss: 1.34; acc: 0.66
Batch: 140; loss: 1.48; acc: 0.52
Batch: 160; loss: 1.21; acc: 0.64
Batch: 180; loss: 1.31; acc: 0.7
Batch: 200; loss: 1.2; acc: 0.66
Batch: 220; loss: 1.27; acc: 0.69
Batch: 240; loss: 1.47; acc: 0.53
Batch: 260; loss: 1.44; acc: 0.62
Batch: 280; loss: 1.44; acc: 0.59
Batch: 300; loss: 1.29; acc: 0.62
Batch: 320; loss: 1.54; acc: 0.52
Batch: 340; loss: 1.36; acc: 0.61
Batch: 360; loss: 1.32; acc: 0.66
Batch: 380; loss: 1.33; acc: 0.61
Batch: 400; loss: 1.35; acc: 0.64
Batch: 420; loss: 1.29; acc: 0.64
Batch: 440; loss: 1.34; acc: 0.58
Batch: 460; loss: 1.49; acc: 0.52
Batch: 480; loss: 1.36; acc: 0.64
Batch: 500; loss: 1.47; acc: 0.53
Batch: 520; loss: 1.39; acc: 0.67
Batch: 540; loss: 1.32; acc: 0.64
Batch: 560; loss: 1.35; acc: 0.58
Batch: 580; loss: 1.4; acc: 0.55
Batch: 600; loss: 1.26; acc: 0.7
Batch: 620; loss: 1.49; acc: 0.52
Batch: 640; loss: 1.42; acc: 0.61
Batch: 660; loss: 1.41; acc: 0.59
Batch: 680; loss: 1.42; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.48
Batch: 720; loss: 1.43; acc: 0.52
Batch: 740; loss: 1.33; acc: 0.62
Batch: 760; loss: 1.36; acc: 0.62
Batch: 780; loss: 1.31; acc: 0.67
Train Epoch over. train_loss: 1.37; train_accuracy: 0.61 

0.00010195247159572318
9.608986147213727e-05
Batch: 0; loss: 1.18; acc: 0.75
Batch: 20; loss: 1.47; acc: 0.61
Batch: 40; loss: 1.13; acc: 0.72
Batch: 60; loss: 1.28; acc: 0.66
Batch: 80; loss: 1.16; acc: 0.67
Batch: 100; loss: 1.31; acc: 0.7
Batch: 120; loss: 1.48; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.77
Val Epoch over. val_loss: 1.3243870218847966; val_accuracy: 0.6441082802547771 

The current subspace-distance is: 9.608986147213727e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.62
Batch: 20; loss: 1.39; acc: 0.62
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.43; acc: 0.58
Batch: 80; loss: 1.29; acc: 0.72
Batch: 100; loss: 1.46; acc: 0.52
Batch: 120; loss: 1.33; acc: 0.61
Batch: 140; loss: 1.42; acc: 0.56
Batch: 160; loss: 1.37; acc: 0.61
Batch: 180; loss: 1.5; acc: 0.48
Batch: 200; loss: 1.45; acc: 0.62
Batch: 220; loss: 1.28; acc: 0.69
Batch: 240; loss: 1.46; acc: 0.55
Batch: 260; loss: 1.33; acc: 0.69
Batch: 280; loss: 1.23; acc: 0.7
Batch: 300; loss: 1.36; acc: 0.62
Batch: 320; loss: 1.43; acc: 0.56
Batch: 340; loss: 1.21; acc: 0.72
Batch: 360; loss: 1.34; acc: 0.66
Batch: 380; loss: 1.23; acc: 0.64
Batch: 400; loss: 1.27; acc: 0.64
Batch: 420; loss: 1.55; acc: 0.5
Batch: 440; loss: 1.35; acc: 0.64
Batch: 460; loss: 1.44; acc: 0.56
Batch: 480; loss: 1.47; acc: 0.58
Batch: 500; loss: 1.32; acc: 0.59
Batch: 520; loss: 1.25; acc: 0.64
Batch: 540; loss: 1.33; acc: 0.62
Batch: 560; loss: 1.4; acc: 0.55
Batch: 580; loss: 1.34; acc: 0.64
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.48; acc: 0.58
Batch: 640; loss: 1.42; acc: 0.61
Batch: 660; loss: 1.27; acc: 0.62
Batch: 680; loss: 1.2; acc: 0.7
Batch: 700; loss: 1.32; acc: 0.61
Batch: 720; loss: 1.4; acc: 0.58
Batch: 740; loss: 1.21; acc: 0.67
Batch: 760; loss: 1.3; acc: 0.66
Batch: 780; loss: 1.28; acc: 0.67
Train Epoch over. train_loss: 1.36; train_accuracy: 0.62 

0.00010928093252005056
0.00010314509563613683
Batch: 0; loss: 1.17; acc: 0.77
Batch: 20; loss: 1.49; acc: 0.52
Batch: 40; loss: 1.12; acc: 0.66
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 1.13; acc: 0.64
Batch: 100; loss: 1.34; acc: 0.7
Batch: 120; loss: 1.45; acc: 0.64
Batch: 140; loss: 1.09; acc: 0.78
Val Epoch over. val_loss: 1.3157957754317362; val_accuracy: 0.618531050955414 

The current subspace-distance is: 0.00010314509563613683 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.31; acc: 0.55
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.23; acc: 0.72
Batch: 120; loss: 1.42; acc: 0.62
Batch: 140; loss: 1.31; acc: 0.69
Batch: 160; loss: 1.55; acc: 0.58
Batch: 180; loss: 1.15; acc: 0.75
Batch: 200; loss: 1.1; acc: 0.77
Batch: 220; loss: 1.24; acc: 0.75
Batch: 240; loss: 1.21; acc: 0.66
Batch: 260; loss: 1.35; acc: 0.66
Batch: 280; loss: 1.31; acc: 0.61
Batch: 300; loss: 1.15; acc: 0.69
Batch: 320; loss: 1.43; acc: 0.61
Batch: 340; loss: 1.5; acc: 0.56
Batch: 360; loss: 1.41; acc: 0.56
Batch: 380; loss: 1.38; acc: 0.67
Batch: 400; loss: 1.47; acc: 0.56
Batch: 420; loss: 1.24; acc: 0.62
Batch: 440; loss: 1.35; acc: 0.59
Batch: 460; loss: 1.31; acc: 0.64
Batch: 480; loss: 1.41; acc: 0.55
Batch: 500; loss: 1.41; acc: 0.56
Batch: 520; loss: 1.3; acc: 0.66
Batch: 540; loss: 1.3; acc: 0.69
Batch: 560; loss: 1.26; acc: 0.61
Batch: 580; loss: 1.25; acc: 0.67
Batch: 600; loss: 1.22; acc: 0.64
Batch: 620; loss: 1.25; acc: 0.55
Batch: 640; loss: 1.38; acc: 0.61
Batch: 660; loss: 1.27; acc: 0.62
Batch: 680; loss: 1.24; acc: 0.67
Batch: 700; loss: 1.44; acc: 0.56
Batch: 720; loss: 1.24; acc: 0.72
Batch: 740; loss: 1.18; acc: 0.72
Batch: 760; loss: 1.2; acc: 0.7
Batch: 780; loss: 1.27; acc: 0.67
Train Epoch over. train_loss: 1.34; train_accuracy: 0.62 

0.00011107152386102825
0.00010405885404907167
Batch: 0; loss: 1.13; acc: 0.8
Batch: 20; loss: 1.46; acc: 0.58
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.23; acc: 0.7
Batch: 80; loss: 1.1; acc: 0.77
Batch: 100; loss: 1.33; acc: 0.7
Batch: 120; loss: 1.42; acc: 0.67
Batch: 140; loss: 1.08; acc: 0.8
Val Epoch over. val_loss: 1.2853439765371335; val_accuracy: 0.6544585987261147 

The current subspace-distance is: 0.00010405885404907167 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.32; acc: 0.67
Batch: 20; loss: 1.36; acc: 0.62
Batch: 40; loss: 1.15; acc: 0.75
Batch: 60; loss: 1.37; acc: 0.59
Batch: 80; loss: 1.24; acc: 0.73
Batch: 100; loss: 1.23; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.31; acc: 0.69
Batch: 160; loss: 1.47; acc: 0.5
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.28; acc: 0.62
Batch: 220; loss: 1.49; acc: 0.53
Batch: 240; loss: 1.43; acc: 0.52
Batch: 260; loss: 1.27; acc: 0.66
Batch: 280; loss: 1.39; acc: 0.59
Batch: 300; loss: 1.42; acc: 0.56
Batch: 320; loss: 1.22; acc: 0.73
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.36; acc: 0.56
Batch: 380; loss: 1.13; acc: 0.8
Batch: 400; loss: 1.33; acc: 0.66
Batch: 420; loss: 1.45; acc: 0.58
Batch: 440; loss: 1.25; acc: 0.61
Batch: 460; loss: 1.62; acc: 0.52
Batch: 480; loss: 1.29; acc: 0.69
Batch: 500; loss: 1.14; acc: 0.72
Batch: 520; loss: 1.32; acc: 0.59
Batch: 540; loss: 1.36; acc: 0.59
Batch: 560; loss: 1.34; acc: 0.66
Batch: 580; loss: 1.38; acc: 0.55
Batch: 600; loss: 1.29; acc: 0.66
Batch: 620; loss: 1.26; acc: 0.61
Batch: 640; loss: 1.3; acc: 0.66
Batch: 660; loss: 1.33; acc: 0.62
Batch: 680; loss: 1.33; acc: 0.66
Batch: 700; loss: 1.2; acc: 0.64
Batch: 720; loss: 1.44; acc: 0.58
Batch: 740; loss: 1.34; acc: 0.61
Batch: 760; loss: 1.47; acc: 0.53
Batch: 780; loss: 1.43; acc: 0.59
Train Epoch over. train_loss: 1.33; train_accuracy: 0.63 

0.0001127610812545754
0.0001058278139680624
Batch: 0; loss: 1.13; acc: 0.8
Batch: 20; loss: 1.41; acc: 0.59
Batch: 40; loss: 1.06; acc: 0.75
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.08; acc: 0.77
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.4; acc: 0.66
Batch: 140; loss: 1.08; acc: 0.8
Val Epoch over. val_loss: 1.2762256522846829; val_accuracy: 0.6528662420382165 

The current subspace-distance is: 0.0001058278139680624 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.32; acc: 0.61
Batch: 20; loss: 1.44; acc: 0.53
Batch: 40; loss: 1.19; acc: 0.66
Batch: 60; loss: 1.27; acc: 0.59
Batch: 80; loss: 1.28; acc: 0.61
Batch: 100; loss: 1.31; acc: 0.58
Batch: 120; loss: 1.17; acc: 0.72
Batch: 140; loss: 1.36; acc: 0.56
Batch: 160; loss: 1.28; acc: 0.67
Batch: 180; loss: 1.44; acc: 0.52
Batch: 200; loss: 1.23; acc: 0.69
Batch: 220; loss: 1.24; acc: 0.73
Batch: 240; loss: 1.42; acc: 0.56
Batch: 260; loss: 1.34; acc: 0.62
Batch: 280; loss: 1.47; acc: 0.58
Batch: 300; loss: 1.25; acc: 0.59
Batch: 320; loss: 1.25; acc: 0.64
Batch: 340; loss: 1.21; acc: 0.69
Batch: 360; loss: 1.37; acc: 0.64
Batch: 380; loss: 1.32; acc: 0.7
Batch: 400; loss: 1.49; acc: 0.58
Batch: 420; loss: 1.53; acc: 0.48
Batch: 440; loss: 1.26; acc: 0.64
Batch: 460; loss: 1.48; acc: 0.59
Batch: 480; loss: 1.33; acc: 0.61
Batch: 500; loss: 1.36; acc: 0.58
Batch: 520; loss: 1.14; acc: 0.72
Batch: 540; loss: 1.2; acc: 0.69
Batch: 560; loss: 1.45; acc: 0.58
Batch: 580; loss: 1.21; acc: 0.69
Batch: 600; loss: 1.19; acc: 0.75
Batch: 620; loss: 1.28; acc: 0.67
Batch: 640; loss: 1.24; acc: 0.61
Batch: 660; loss: 1.22; acc: 0.7
Batch: 680; loss: 1.35; acc: 0.62
Batch: 700; loss: 1.24; acc: 0.69
Batch: 720; loss: 1.2; acc: 0.77
Batch: 740; loss: 1.36; acc: 0.56
Batch: 760; loss: 1.41; acc: 0.56
Batch: 780; loss: 1.39; acc: 0.64
Train Epoch over. train_loss: 1.33; train_accuracy: 0.63 

0.00011312031710986048
0.00010785652557387948
Batch: 0; loss: 1.12; acc: 0.78
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 1.05; acc: 0.78
Batch: 60; loss: 1.2; acc: 0.73
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.7
Batch: 120; loss: 1.41; acc: 0.67
Batch: 140; loss: 1.03; acc: 0.8
Val Epoch over. val_loss: 1.2502572361830693; val_accuracy: 0.6676950636942676 

The current subspace-distance is: 0.00010785652557387948 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.7
Batch: 20; loss: 1.43; acc: 0.52
Batch: 40; loss: 1.4; acc: 0.58
Batch: 60; loss: 1.17; acc: 0.75
Batch: 80; loss: 1.37; acc: 0.61
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.24; acc: 0.66
Batch: 160; loss: 1.32; acc: 0.62
Batch: 180; loss: 1.35; acc: 0.59
Batch: 200; loss: 1.38; acc: 0.59
Batch: 220; loss: 1.39; acc: 0.67
Batch: 240; loss: 1.35; acc: 0.59
Batch: 260; loss: 1.33; acc: 0.61
Batch: 280; loss: 1.31; acc: 0.67
Batch: 300; loss: 1.36; acc: 0.64
Batch: 320; loss: 1.32; acc: 0.64
Batch: 340; loss: 1.24; acc: 0.67
Batch: 360; loss: 1.26; acc: 0.66
Batch: 380; loss: 1.44; acc: 0.62
Batch: 400; loss: 1.26; acc: 0.66
Batch: 420; loss: 1.13; acc: 0.73
Batch: 440; loss: 1.21; acc: 0.69
Batch: 460; loss: 1.33; acc: 0.66
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.4; acc: 0.61
Batch: 540; loss: 1.29; acc: 0.62
Batch: 560; loss: 1.3; acc: 0.62
Batch: 580; loss: 1.37; acc: 0.62
Batch: 600; loss: 1.27; acc: 0.62
Batch: 620; loss: 1.39; acc: 0.61
Batch: 640; loss: 1.29; acc: 0.66
Batch: 660; loss: 1.4; acc: 0.61
Batch: 680; loss: 1.36; acc: 0.66
Batch: 700; loss: 1.37; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.7
Batch: 740; loss: 1.32; acc: 0.66
Batch: 760; loss: 1.43; acc: 0.53
Batch: 780; loss: 1.33; acc: 0.61
Train Epoch over. train_loss: 1.32; train_accuracy: 0.63 

0.0001145891728810966
0.00010664219007594511
Batch: 0; loss: 1.11; acc: 0.81
Batch: 20; loss: 1.41; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.73
Batch: 60; loss: 1.23; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.4; acc: 0.66
Batch: 140; loss: 1.06; acc: 0.78
Val Epoch over. val_loss: 1.2596540170110715; val_accuracy: 0.6599323248407644 

The current subspace-distance is: 0.00010664219007594511 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.43; acc: 0.55
Batch: 20; loss: 1.1; acc: 0.78
Batch: 40; loss: 1.36; acc: 0.59
Batch: 60; loss: 1.4; acc: 0.61
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.37; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.7
Batch: 140; loss: 1.31; acc: 0.61
Batch: 160; loss: 1.38; acc: 0.62
Batch: 180; loss: 1.23; acc: 0.69
Batch: 200; loss: 1.54; acc: 0.47
Batch: 220; loss: 1.46; acc: 0.61
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.27; acc: 0.73
Batch: 280; loss: 1.17; acc: 0.77
Batch: 300; loss: 1.24; acc: 0.59
Batch: 320; loss: 1.38; acc: 0.62
Batch: 340; loss: 1.31; acc: 0.62
Batch: 360; loss: 1.28; acc: 0.72
Batch: 380; loss: 1.43; acc: 0.55
Batch: 400; loss: 1.42; acc: 0.59
Batch: 420; loss: 1.41; acc: 0.62
Batch: 440; loss: 1.5; acc: 0.62
Batch: 460; loss: 1.38; acc: 0.61
Batch: 480; loss: 1.26; acc: 0.69
Batch: 500; loss: 1.49; acc: 0.55
Batch: 520; loss: 1.34; acc: 0.61
Batch: 540; loss: 1.28; acc: 0.67
Batch: 560; loss: 1.43; acc: 0.55
Batch: 580; loss: 1.22; acc: 0.61
Batch: 600; loss: 1.26; acc: 0.67
Batch: 620; loss: 1.1; acc: 0.73
Batch: 640; loss: 1.18; acc: 0.62
Batch: 660; loss: 1.05; acc: 0.8
Batch: 680; loss: 1.2; acc: 0.75
Batch: 700; loss: 1.22; acc: 0.69
Batch: 720; loss: 1.3; acc: 0.58
Batch: 740; loss: 1.23; acc: 0.7
Batch: 760; loss: 1.34; acc: 0.64
Batch: 780; loss: 1.4; acc: 0.55
Train Epoch over. train_loss: 1.32; train_accuracy: 0.63 

0.00011577922123251483
0.00011000897211488336
Batch: 0; loss: 1.12; acc: 0.8
Batch: 20; loss: 1.43; acc: 0.58
Batch: 40; loss: 1.07; acc: 0.73
Batch: 60; loss: 1.25; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.75
Batch: 100; loss: 1.3; acc: 0.69
Batch: 120; loss: 1.41; acc: 0.64
Batch: 140; loss: 1.08; acc: 0.81
Val Epoch over. val_loss: 1.2772744552345032; val_accuracy: 0.6588375796178344 

The current subspace-distance is: 0.00011000897211488336 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.61
Batch: 20; loss: 1.25; acc: 0.69
Batch: 40; loss: 1.41; acc: 0.59
Batch: 60; loss: 1.33; acc: 0.69
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.62
Batch: 180; loss: 1.17; acc: 0.67
Batch: 200; loss: 1.31; acc: 0.58
Batch: 220; loss: 1.22; acc: 0.72
Batch: 240; loss: 1.44; acc: 0.58
Batch: 260; loss: 1.5; acc: 0.59
Batch: 280; loss: 1.28; acc: 0.62
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 1.3; acc: 0.61
Batch: 340; loss: 1.37; acc: 0.61
Batch: 360; loss: 1.27; acc: 0.66
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 1.51; acc: 0.55
Batch: 420; loss: 1.39; acc: 0.56
Batch: 440; loss: 1.27; acc: 0.67
Batch: 460; loss: 1.31; acc: 0.61
Batch: 480; loss: 1.26; acc: 0.59
Batch: 500; loss: 1.3; acc: 0.7
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.31; acc: 0.64
Batch: 580; loss: 1.29; acc: 0.67
Batch: 600; loss: 1.38; acc: 0.58
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 1.31; acc: 0.62
Batch: 660; loss: 1.13; acc: 0.69
Batch: 680; loss: 1.26; acc: 0.7
Batch: 700; loss: 1.21; acc: 0.64
Batch: 720; loss: 1.39; acc: 0.61
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 1.1; acc: 0.73
Batch: 780; loss: 1.29; acc: 0.64
Train Epoch over. train_loss: 1.31; train_accuracy: 0.63 

0.00011541171988938004
0.00010962566011585295
Batch: 0; loss: 1.11; acc: 0.78
Batch: 20; loss: 1.45; acc: 0.61
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 1.31; acc: 0.67
Batch: 120; loss: 1.41; acc: 0.66
Batch: 140; loss: 1.1; acc: 0.81
Val Epoch over. val_loss: 1.2728758374596858; val_accuracy: 0.6647093949044586 

The current subspace-distance is: 0.00010962566011585295 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.43; acc: 0.58
Batch: 20; loss: 1.24; acc: 0.64
Batch: 40; loss: 1.35; acc: 0.61
Batch: 60; loss: 1.24; acc: 0.67
Batch: 80; loss: 1.23; acc: 0.62
Batch: 100; loss: 1.42; acc: 0.58
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.32; acc: 0.62
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.37; acc: 0.55
Batch: 220; loss: 1.43; acc: 0.61
Batch: 240; loss: 1.24; acc: 0.67
Batch: 260; loss: 1.21; acc: 0.66
Batch: 280; loss: 1.3; acc: 0.64
Batch: 300; loss: 1.52; acc: 0.48
Batch: 320; loss: 1.45; acc: 0.52
Batch: 340; loss: 1.42; acc: 0.56
Batch: 360; loss: 1.29; acc: 0.64
Batch: 380; loss: 1.3; acc: 0.7
Batch: 400; loss: 1.2; acc: 0.72
Batch: 420; loss: 1.28; acc: 0.64
Batch: 440; loss: 1.31; acc: 0.59
Batch: 460; loss: 1.48; acc: 0.55
Batch: 480; loss: 1.26; acc: 0.61
Batch: 500; loss: 1.3; acc: 0.59
Batch: 520; loss: 1.29; acc: 0.69
Batch: 540; loss: 1.39; acc: 0.62
Batch: 560; loss: 1.28; acc: 0.61
Batch: 580; loss: 1.36; acc: 0.62
Batch: 600; loss: 1.48; acc: 0.53
Batch: 620; loss: 1.41; acc: 0.64
Batch: 640; loss: 1.25; acc: 0.7
Batch: 660; loss: 1.35; acc: 0.62
Batch: 680; loss: 1.41; acc: 0.62
Batch: 700; loss: 1.43; acc: 0.56
Batch: 720; loss: 1.37; acc: 0.64
Batch: 740; loss: 1.29; acc: 0.66
Batch: 760; loss: 1.11; acc: 0.7
Batch: 780; loss: 1.22; acc: 0.7
Train Epoch over. train_loss: 1.31; train_accuracy: 0.63 

0.0001182494088425301
0.00011215255653951317
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.23; acc: 0.62
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.28; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.66
Batch: 140; loss: 1.09; acc: 0.77
Val Epoch over. val_loss: 1.2692229470629601; val_accuracy: 0.652468152866242 

The current subspace-distance is: 0.00011215255653951317 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.8
Batch: 20; loss: 1.28; acc: 0.7
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.26; acc: 0.73
Batch: 80; loss: 1.23; acc: 0.67
Batch: 100; loss: 1.29; acc: 0.62
Batch: 120; loss: 1.38; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.69
Batch: 160; loss: 1.19; acc: 0.7
Batch: 180; loss: 1.4; acc: 0.62
Batch: 200; loss: 1.36; acc: 0.66
Batch: 220; loss: 1.3; acc: 0.59
Batch: 240; loss: 1.16; acc: 0.69
Batch: 260; loss: 1.38; acc: 0.56
Batch: 280; loss: 1.11; acc: 0.7
Batch: 300; loss: 1.43; acc: 0.61
Batch: 320; loss: 1.24; acc: 0.67
Batch: 340; loss: 1.35; acc: 0.56
Batch: 360; loss: 1.27; acc: 0.66
Batch: 380; loss: 1.3; acc: 0.67
Batch: 400; loss: 1.6; acc: 0.48
Batch: 420; loss: 1.24; acc: 0.66
Batch: 440; loss: 1.21; acc: 0.66
Batch: 460; loss: 1.39; acc: 0.62
Batch: 480; loss: 1.38; acc: 0.64
Batch: 500; loss: 1.4; acc: 0.55
Batch: 520; loss: 1.24; acc: 0.62
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.67; acc: 0.42
Batch: 580; loss: 1.1; acc: 0.72
Batch: 600; loss: 1.31; acc: 0.62
Batch: 620; loss: 1.35; acc: 0.61
Batch: 640; loss: 1.37; acc: 0.62
Batch: 660; loss: 1.51; acc: 0.59
Batch: 680; loss: 1.36; acc: 0.59
Batch: 700; loss: 1.32; acc: 0.59
Batch: 720; loss: 1.32; acc: 0.61
Batch: 740; loss: 1.26; acc: 0.7
Batch: 760; loss: 1.19; acc: 0.73
Batch: 780; loss: 1.48; acc: 0.5
Train Epoch over. train_loss: 1.31; train_accuracy: 0.63 

0.00011979832197539508
0.00011338407057337463
Batch: 0; loss: 1.1; acc: 0.84
Batch: 20; loss: 1.43; acc: 0.58
Batch: 40; loss: 1.07; acc: 0.77
Batch: 60; loss: 1.22; acc: 0.72
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.72
Batch: 120; loss: 1.41; acc: 0.67
Batch: 140; loss: 1.07; acc: 0.81
Val Epoch over. val_loss: 1.2581646233607249; val_accuracy: 0.6664012738853503 

The current subspace-distance is: 0.00011338407057337463 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.36; acc: 0.59
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 1.38; acc: 0.59
Batch: 60; loss: 1.37; acc: 0.56
Batch: 80; loss: 1.31; acc: 0.61
Batch: 100; loss: 1.33; acc: 0.56
Batch: 120; loss: 1.35; acc: 0.61
Batch: 140; loss: 1.35; acc: 0.61
Batch: 160; loss: 1.49; acc: 0.53
Batch: 180; loss: 1.25; acc: 0.7
Batch: 200; loss: 1.26; acc: 0.67
Batch: 220; loss: 1.3; acc: 0.66
Batch: 240; loss: 1.43; acc: 0.5
Batch: 260; loss: 1.39; acc: 0.66
Batch: 280; loss: 1.14; acc: 0.75
Batch: 300; loss: 1.47; acc: 0.59
Batch: 320; loss: 1.3; acc: 0.64
Batch: 340; loss: 1.43; acc: 0.5
Batch: 360; loss: 1.14; acc: 0.7
Batch: 380; loss: 1.41; acc: 0.59
Batch: 400; loss: 1.34; acc: 0.62
Batch: 420; loss: 1.27; acc: 0.66
Batch: 440; loss: 1.38; acc: 0.59
Batch: 460; loss: 1.39; acc: 0.64
Batch: 480; loss: 1.25; acc: 0.64
Batch: 500; loss: 1.33; acc: 0.67
Batch: 520; loss: 1.24; acc: 0.72
Batch: 540; loss: 1.37; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.64
Batch: 600; loss: 1.3; acc: 0.66
Batch: 620; loss: 1.39; acc: 0.53
Batch: 640; loss: 1.3; acc: 0.67
Batch: 660; loss: 1.29; acc: 0.69
Batch: 680; loss: 1.39; acc: 0.55
Batch: 700; loss: 1.38; acc: 0.59
Batch: 720; loss: 1.25; acc: 0.66
Batch: 740; loss: 1.32; acc: 0.61
Batch: 760; loss: 1.21; acc: 0.61
Batch: 780; loss: 1.24; acc: 0.64
Train Epoch over. train_loss: 1.31; train_accuracy: 0.63 

0.00012111230898881331
0.00011331574933137745
Batch: 0; loss: 1.12; acc: 0.77
Batch: 20; loss: 1.43; acc: 0.61
Batch: 40; loss: 1.11; acc: 0.75
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 1.32; acc: 0.73
Batch: 120; loss: 1.4; acc: 0.66
Batch: 140; loss: 1.1; acc: 0.8
Val Epoch over. val_loss: 1.271515990898108; val_accuracy: 0.6592356687898089 

The current subspace-distance is: 0.00011331574933137745 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.41; acc: 0.55
Batch: 40; loss: 1.32; acc: 0.61
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.2; acc: 0.69
Batch: 100; loss: 1.34; acc: 0.56
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.69
Batch: 160; loss: 1.33; acc: 0.56
Batch: 180; loss: 1.43; acc: 0.59
Batch: 200; loss: 1.49; acc: 0.58
Batch: 220; loss: 1.29; acc: 0.61
Batch: 240; loss: 1.31; acc: 0.61
Batch: 260; loss: 1.19; acc: 0.7
Batch: 280; loss: 1.25; acc: 0.66
Batch: 300; loss: 1.31; acc: 0.67
Batch: 320; loss: 1.2; acc: 0.73
Batch: 340; loss: 1.44; acc: 0.58
Batch: 360; loss: 1.35; acc: 0.66
Batch: 380; loss: 1.34; acc: 0.66
Batch: 400; loss: 1.29; acc: 0.67
Batch: 420; loss: 1.26; acc: 0.67
Batch: 440; loss: 1.19; acc: 0.72
Batch: 460; loss: 1.39; acc: 0.58
Batch: 480; loss: 1.37; acc: 0.64
Batch: 500; loss: 1.26; acc: 0.64
Batch: 520; loss: 1.5; acc: 0.58
Batch: 540; loss: 1.4; acc: 0.58
Batch: 560; loss: 1.27; acc: 0.66
Batch: 580; loss: 1.26; acc: 0.66
Batch: 600; loss: 1.29; acc: 0.64
Batch: 620; loss: 1.29; acc: 0.59
Batch: 640; loss: 1.33; acc: 0.62
Batch: 660; loss: 1.3; acc: 0.66
Batch: 680; loss: 1.2; acc: 0.72
Batch: 700; loss: 1.3; acc: 0.59
Batch: 720; loss: 1.32; acc: 0.62
Batch: 740; loss: 1.2; acc: 0.67
Batch: 760; loss: 1.43; acc: 0.62
Batch: 780; loss: 1.27; acc: 0.67
Train Epoch over. train_loss: 1.31; train_accuracy: 0.63 

0.00012188073742436245
0.00011483123671496287
Batch: 0; loss: 1.11; acc: 0.78
Batch: 20; loss: 1.43; acc: 0.59
Batch: 40; loss: 1.09; acc: 0.73
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.29; acc: 0.73
Batch: 120; loss: 1.4; acc: 0.64
Batch: 140; loss: 1.14; acc: 0.8
Val Epoch over. val_loss: 1.267692559084315; val_accuracy: 0.6575437898089171 

The current subspace-distance is: 0.00011483123671496287 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.44; acc: 0.64
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 1.37; acc: 0.59
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 1.55; acc: 0.48
Batch: 100; loss: 1.3; acc: 0.58
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.31; acc: 0.59
Batch: 160; loss: 1.22; acc: 0.67
Batch: 180; loss: 1.33; acc: 0.58
Batch: 200; loss: 1.33; acc: 0.58
Batch: 220; loss: 1.48; acc: 0.55
Batch: 240; loss: 1.27; acc: 0.62
Batch: 260; loss: 1.23; acc: 0.67
Batch: 280; loss: 1.46; acc: 0.53
Batch: 300; loss: 1.32; acc: 0.62
Batch: 320; loss: 1.31; acc: 0.66
Batch: 340; loss: 1.27; acc: 0.62
Batch: 360; loss: 1.38; acc: 0.59
Batch: 380; loss: 1.34; acc: 0.69
Batch: 400; loss: 1.24; acc: 0.66
Batch: 420; loss: 1.27; acc: 0.66
Batch: 440; loss: 1.34; acc: 0.64
Batch: 460; loss: 1.4; acc: 0.58
Batch: 480; loss: 1.27; acc: 0.72
Batch: 500; loss: 1.27; acc: 0.66
Batch: 520; loss: 1.27; acc: 0.61
Batch: 540; loss: 1.35; acc: 0.64
Batch: 560; loss: 1.21; acc: 0.69
Batch: 580; loss: 1.26; acc: 0.66
Batch: 600; loss: 1.29; acc: 0.59
Batch: 620; loss: 1.46; acc: 0.55
Batch: 640; loss: 1.22; acc: 0.7
Batch: 660; loss: 1.24; acc: 0.66
Batch: 680; loss: 1.16; acc: 0.67
Batch: 700; loss: 1.13; acc: 0.78
Batch: 720; loss: 1.35; acc: 0.59
Batch: 740; loss: 1.49; acc: 0.53
Batch: 760; loss: 1.21; acc: 0.7
Batch: 780; loss: 1.32; acc: 0.61
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.0001230831549037248
0.00011498087405925617
Batch: 0; loss: 1.08; acc: 0.81
Batch: 20; loss: 1.42; acc: 0.58
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 1.02; acc: 0.73
Batch: 100; loss: 1.27; acc: 0.69
Batch: 120; loss: 1.39; acc: 0.62
Batch: 140; loss: 1.05; acc: 0.78
Val Epoch over. val_loss: 1.2366657659506342; val_accuracy: 0.6659036624203821 

The current subspace-distance is: 0.00011498087405925617 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.37; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.34; acc: 0.61
Batch: 80; loss: 1.28; acc: 0.59
Batch: 100; loss: 1.33; acc: 0.67
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 1.19; acc: 0.75
Batch: 160; loss: 1.2; acc: 0.69
Batch: 180; loss: 1.39; acc: 0.56
Batch: 200; loss: 1.16; acc: 0.67
Batch: 220; loss: 1.35; acc: 0.55
Batch: 240; loss: 1.27; acc: 0.64
Batch: 260; loss: 1.31; acc: 0.59
Batch: 280; loss: 1.29; acc: 0.58
Batch: 300; loss: 1.23; acc: 0.64
Batch: 320; loss: 1.38; acc: 0.55
Batch: 340; loss: 1.45; acc: 0.5
Batch: 360; loss: 1.34; acc: 0.58
Batch: 380; loss: 1.25; acc: 0.75
Batch: 400; loss: 1.33; acc: 0.64
Batch: 420; loss: 1.28; acc: 0.61
Batch: 440; loss: 1.24; acc: 0.62
Batch: 460; loss: 1.35; acc: 0.61
Batch: 480; loss: 1.44; acc: 0.5
Batch: 500; loss: 1.45; acc: 0.55
Batch: 520; loss: 1.14; acc: 0.72
Batch: 540; loss: 1.18; acc: 0.64
Batch: 560; loss: 1.21; acc: 0.67
Batch: 580; loss: 1.37; acc: 0.61
Batch: 600; loss: 1.27; acc: 0.69
Batch: 620; loss: 1.21; acc: 0.75
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 1.32; acc: 0.59
Batch: 680; loss: 1.35; acc: 0.67
Batch: 700; loss: 1.28; acc: 0.66
Batch: 720; loss: 1.34; acc: 0.62
Batch: 740; loss: 1.27; acc: 0.64
Batch: 760; loss: 1.36; acc: 0.59
Batch: 780; loss: 1.29; acc: 0.61
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.00012132733536418527
0.0001143030749517493
Batch: 0; loss: 1.12; acc: 0.77
Batch: 20; loss: 1.43; acc: 0.62
Batch: 40; loss: 1.09; acc: 0.73
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.29; acc: 0.73
Batch: 120; loss: 1.41; acc: 0.62
Batch: 140; loss: 1.1; acc: 0.8
Val Epoch over. val_loss: 1.2575136669881783; val_accuracy: 0.6630175159235668 

The current subspace-distance is: 0.0001143030749517493 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.37; acc: 0.58
Batch: 20; loss: 1.29; acc: 0.66
Batch: 40; loss: 1.38; acc: 0.55
Batch: 60; loss: 1.23; acc: 0.75
Batch: 80; loss: 1.34; acc: 0.56
Batch: 100; loss: 1.14; acc: 0.75
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 1.27; acc: 0.64
Batch: 160; loss: 1.32; acc: 0.59
Batch: 180; loss: 1.24; acc: 0.7
Batch: 200; loss: 1.09; acc: 0.72
Batch: 220; loss: 1.38; acc: 0.64
Batch: 240; loss: 1.26; acc: 0.67
Batch: 260; loss: 1.44; acc: 0.55
Batch: 280; loss: 1.33; acc: 0.62
Batch: 300; loss: 1.24; acc: 0.62
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.27; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.72
Batch: 380; loss: 1.35; acc: 0.59
Batch: 400; loss: 1.3; acc: 0.62
Batch: 420; loss: 1.27; acc: 0.58
Batch: 440; loss: 1.43; acc: 0.53
Batch: 460; loss: 1.32; acc: 0.64
Batch: 480; loss: 1.29; acc: 0.64
Batch: 500; loss: 1.21; acc: 0.7
Batch: 520; loss: 1.45; acc: 0.56
Batch: 540; loss: 1.49; acc: 0.58
Batch: 560; loss: 1.32; acc: 0.69
Batch: 580; loss: 1.38; acc: 0.61
Batch: 600; loss: 1.24; acc: 0.64
Batch: 620; loss: 1.12; acc: 0.75
Batch: 640; loss: 1.34; acc: 0.59
Batch: 660; loss: 1.44; acc: 0.53
Batch: 680; loss: 1.22; acc: 0.7
Batch: 700; loss: 1.3; acc: 0.7
Batch: 720; loss: 1.36; acc: 0.61
Batch: 740; loss: 1.37; acc: 0.56
Batch: 760; loss: 1.35; acc: 0.67
Batch: 780; loss: 1.31; acc: 0.66
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.0001218139732372947
0.00011533744691405445
Batch: 0; loss: 1.08; acc: 0.84
Batch: 20; loss: 1.38; acc: 0.56
Batch: 40; loss: 1.07; acc: 0.73
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.27; acc: 0.7
Batch: 120; loss: 1.38; acc: 0.66
Batch: 140; loss: 1.06; acc: 0.78
Val Epoch over. val_loss: 1.235763425280334; val_accuracy: 0.67078025477707 

The current subspace-distance is: 0.00011533744691405445 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.49; acc: 0.55
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.64
Batch: 120; loss: 1.47; acc: 0.48
Batch: 140; loss: 1.3; acc: 0.61
Batch: 160; loss: 1.36; acc: 0.66
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 1.41; acc: 0.58
Batch: 240; loss: 1.2; acc: 0.69
Batch: 260; loss: 1.27; acc: 0.67
Batch: 280; loss: 1.4; acc: 0.61
Batch: 300; loss: 1.38; acc: 0.55
Batch: 320; loss: 1.32; acc: 0.62
Batch: 340; loss: 1.29; acc: 0.67
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.43; acc: 0.55
Batch: 400; loss: 1.42; acc: 0.56
Batch: 420; loss: 1.27; acc: 0.67
Batch: 440; loss: 1.18; acc: 0.67
Batch: 460; loss: 1.29; acc: 0.62
Batch: 480; loss: 1.37; acc: 0.66
Batch: 500; loss: 1.4; acc: 0.53
Batch: 520; loss: 1.46; acc: 0.56
Batch: 540; loss: 1.4; acc: 0.55
Batch: 560; loss: 1.45; acc: 0.61
Batch: 580; loss: 1.36; acc: 0.56
Batch: 600; loss: 1.28; acc: 0.69
Batch: 620; loss: 1.39; acc: 0.61
Batch: 640; loss: 1.38; acc: 0.61
Batch: 660; loss: 1.34; acc: 0.61
Batch: 680; loss: 1.26; acc: 0.61
Batch: 700; loss: 1.36; acc: 0.62
Batch: 720; loss: 1.23; acc: 0.67
Batch: 740; loss: 1.4; acc: 0.55
Batch: 760; loss: 1.3; acc: 0.59
Batch: 780; loss: 1.24; acc: 0.69
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.00012244543177075684
0.00011646106577245519
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.41; acc: 0.62
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 1.28; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.62
Batch: 140; loss: 1.07; acc: 0.8
Val Epoch over. val_loss: 1.244411570631015; val_accuracy: 0.6666998407643312 

The current subspace-distance is: 0.00011646106577245519 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.7
Batch: 20; loss: 1.35; acc: 0.61
Batch: 40; loss: 1.46; acc: 0.53
Batch: 60; loss: 1.25; acc: 0.64
Batch: 80; loss: 1.34; acc: 0.55
Batch: 100; loss: 1.37; acc: 0.59
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.61
Batch: 160; loss: 1.28; acc: 0.64
Batch: 180; loss: 1.37; acc: 0.61
Batch: 200; loss: 1.26; acc: 0.61
Batch: 220; loss: 1.35; acc: 0.61
Batch: 240; loss: 1.16; acc: 0.73
Batch: 260; loss: 1.3; acc: 0.56
Batch: 280; loss: 1.28; acc: 0.62
Batch: 300; loss: 1.36; acc: 0.64
Batch: 320; loss: 1.35; acc: 0.56
Batch: 340; loss: 1.42; acc: 0.58
Batch: 360; loss: 1.26; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.72
Batch: 400; loss: 1.37; acc: 0.58
Batch: 420; loss: 1.34; acc: 0.61
Batch: 440; loss: 1.25; acc: 0.64
Batch: 460; loss: 1.19; acc: 0.66
Batch: 480; loss: 1.25; acc: 0.7
Batch: 500; loss: 1.32; acc: 0.61
Batch: 520; loss: 1.38; acc: 0.56
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.23; acc: 0.72
Batch: 580; loss: 1.22; acc: 0.7
Batch: 600; loss: 1.33; acc: 0.61
Batch: 620; loss: 1.34; acc: 0.58
Batch: 640; loss: 1.43; acc: 0.56
Batch: 660; loss: 1.45; acc: 0.52
Batch: 680; loss: 1.3; acc: 0.61
Batch: 700; loss: 1.3; acc: 0.67
Batch: 720; loss: 1.45; acc: 0.55
Batch: 740; loss: 1.32; acc: 0.58
Batch: 760; loss: 1.26; acc: 0.69
Batch: 780; loss: 1.4; acc: 0.58
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.0001222662685904652
0.00011605428881011903
Batch: 0; loss: 1.11; acc: 0.78
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 1.08; acc: 0.75
Batch: 60; loss: 1.21; acc: 0.64
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 1.27; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.62
Batch: 140; loss: 1.07; acc: 0.81
Val Epoch over. val_loss: 1.2493714625668373; val_accuracy: 0.6668988853503185 

The current subspace-distance is: 0.00011605428881011903 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.3; acc: 0.66
Batch: 20; loss: 1.22; acc: 0.67
Batch: 40; loss: 1.3; acc: 0.66
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.29; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.61
Batch: 140; loss: 1.21; acc: 0.67
Batch: 160; loss: 1.1; acc: 0.73
Batch: 180; loss: 1.56; acc: 0.47
Batch: 200; loss: 1.42; acc: 0.62
Batch: 220; loss: 1.16; acc: 0.59
Batch: 240; loss: 1.21; acc: 0.59
Batch: 260; loss: 1.2; acc: 0.69
Batch: 280; loss: 1.33; acc: 0.64
Batch: 300; loss: 1.18; acc: 0.7
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.43; acc: 0.58
Batch: 360; loss: 1.29; acc: 0.62
Batch: 380; loss: 1.44; acc: 0.52
Batch: 400; loss: 1.05; acc: 0.75
Batch: 420; loss: 1.29; acc: 0.62
Batch: 440; loss: 1.32; acc: 0.66
Batch: 460; loss: 1.21; acc: 0.62
Batch: 480; loss: 1.39; acc: 0.56
Batch: 500; loss: 1.24; acc: 0.69
Batch: 520; loss: 1.32; acc: 0.56
Batch: 540; loss: 1.4; acc: 0.56
Batch: 560; loss: 1.28; acc: 0.67
Batch: 580; loss: 1.33; acc: 0.61
Batch: 600; loss: 1.28; acc: 0.67
Batch: 620; loss: 1.11; acc: 0.75
Batch: 640; loss: 1.31; acc: 0.64
Batch: 660; loss: 1.38; acc: 0.72
Batch: 680; loss: 1.6; acc: 0.47
Batch: 700; loss: 1.21; acc: 0.67
Batch: 720; loss: 1.33; acc: 0.62
Batch: 740; loss: 1.17; acc: 0.66
Batch: 760; loss: 1.25; acc: 0.69
Batch: 780; loss: 1.26; acc: 0.73
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.00012262740347068757
0.00011551522766239941
Batch: 0; loss: 1.1; acc: 0.83
Batch: 20; loss: 1.42; acc: 0.58
Batch: 40; loss: 1.07; acc: 0.77
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.29; acc: 0.7
Batch: 120; loss: 1.41; acc: 0.66
Batch: 140; loss: 1.06; acc: 0.8
Val Epoch over. val_loss: 1.2526768924324376; val_accuracy: 0.6626194267515924 

The current subspace-distance is: 0.00011551522766239941 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.31; acc: 0.55
Batch: 20; loss: 1.41; acc: 0.59
Batch: 40; loss: 1.31; acc: 0.58
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.19; acc: 0.66
Batch: 100; loss: 1.36; acc: 0.64
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.2; acc: 0.62
Batch: 160; loss: 1.28; acc: 0.67
Batch: 180; loss: 1.21; acc: 0.7
Batch: 200; loss: 1.19; acc: 0.66
Batch: 220; loss: 1.13; acc: 0.77
Batch: 240; loss: 1.15; acc: 0.72
Batch: 260; loss: 1.29; acc: 0.67
Batch: 280; loss: 1.1; acc: 0.7
Batch: 300; loss: 1.31; acc: 0.64
Batch: 320; loss: 1.15; acc: 0.67
Batch: 340; loss: 1.31; acc: 0.56
Batch: 360; loss: 1.35; acc: 0.67
Batch: 380; loss: 1.36; acc: 0.69
Batch: 400; loss: 1.35; acc: 0.64
Batch: 420; loss: 1.2; acc: 0.66
Batch: 440; loss: 1.3; acc: 0.56
Batch: 460; loss: 1.21; acc: 0.72
Batch: 480; loss: 1.37; acc: 0.64
Batch: 500; loss: 1.09; acc: 0.72
Batch: 520; loss: 1.17; acc: 0.69
Batch: 540; loss: 1.21; acc: 0.7
Batch: 560; loss: 1.34; acc: 0.59
Batch: 580; loss: 1.33; acc: 0.69
Batch: 600; loss: 1.38; acc: 0.64
Batch: 620; loss: 1.18; acc: 0.69
Batch: 640; loss: 1.5; acc: 0.55
Batch: 660; loss: 1.38; acc: 0.55
Batch: 680; loss: 1.6; acc: 0.53
Batch: 700; loss: 1.29; acc: 0.61
Batch: 720; loss: 1.36; acc: 0.62
Batch: 740; loss: 1.15; acc: 0.64
Batch: 760; loss: 1.21; acc: 0.73
Batch: 780; loss: 1.27; acc: 0.66
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.00012338160013314337
0.00011709427781170234
Batch: 0; loss: 1.1; acc: 0.78
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.28; acc: 0.69
Batch: 120; loss: 1.4; acc: 0.66
Batch: 140; loss: 1.04; acc: 0.8
Val Epoch over. val_loss: 1.2706494562944788; val_accuracy: 0.6441082802547771 

The current subspace-distance is: 0.00011709427781170234 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.27; acc: 0.67
Batch: 40; loss: 1.3; acc: 0.64
Batch: 60; loss: 1.14; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.31; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.73
Batch: 160; loss: 1.32; acc: 0.59
Batch: 180; loss: 1.23; acc: 0.66
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 1.19; acc: 0.67
Batch: 240; loss: 1.24; acc: 0.67
Batch: 260; loss: 1.34; acc: 0.61
Batch: 280; loss: 1.3; acc: 0.59
Batch: 300; loss: 1.36; acc: 0.58
Batch: 320; loss: 1.05; acc: 0.8
Batch: 340; loss: 1.31; acc: 0.58
Batch: 360; loss: 1.39; acc: 0.58
Batch: 380; loss: 1.22; acc: 0.69
Batch: 400; loss: 1.39; acc: 0.58
Batch: 420; loss: 1.34; acc: 0.59
Batch: 440; loss: 1.19; acc: 0.67
Batch: 460; loss: 1.35; acc: 0.59
Batch: 480; loss: 1.32; acc: 0.64
Batch: 500; loss: 1.37; acc: 0.67
Batch: 520; loss: 1.23; acc: 0.66
Batch: 540; loss: 1.26; acc: 0.66
Batch: 560; loss: 1.43; acc: 0.62
Batch: 580; loss: 1.21; acc: 0.69
Batch: 600; loss: 1.14; acc: 0.72
Batch: 620; loss: 1.37; acc: 0.62
Batch: 640; loss: 1.29; acc: 0.66
Batch: 660; loss: 1.43; acc: 0.67
Batch: 680; loss: 1.27; acc: 0.67
Batch: 700; loss: 1.42; acc: 0.64
Batch: 720; loss: 1.25; acc: 0.62
Batch: 740; loss: 1.45; acc: 0.47
Batch: 760; loss: 1.32; acc: 0.66
Batch: 780; loss: 1.44; acc: 0.59
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.00012409589544404298
0.00011834091128548607
Batch: 0; loss: 1.09; acc: 0.83
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 1.09; acc: 0.77
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.28; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.61
Batch: 140; loss: 1.08; acc: 0.8
Val Epoch over. val_loss: 1.251463998275198; val_accuracy: 0.6643113057324841 

The current subspace-distance is: 0.00011834091128548607 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.34; acc: 0.59
Batch: 40; loss: 1.39; acc: 0.53
Batch: 60; loss: 1.25; acc: 0.62
Batch: 80; loss: 1.14; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.66
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.31; acc: 0.58
Batch: 180; loss: 1.25; acc: 0.67
Batch: 200; loss: 1.4; acc: 0.62
Batch: 220; loss: 1.53; acc: 0.55
Batch: 240; loss: 1.36; acc: 0.61
Batch: 260; loss: 1.18; acc: 0.73
Batch: 280; loss: 1.19; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.67
Batch: 320; loss: 1.31; acc: 0.62
Batch: 340; loss: 1.38; acc: 0.61
Batch: 360; loss: 1.21; acc: 0.66
Batch: 380; loss: 1.37; acc: 0.59
Batch: 400; loss: 1.37; acc: 0.62
Batch: 420; loss: 1.38; acc: 0.56
Batch: 440; loss: 1.3; acc: 0.64
Batch: 460; loss: 1.4; acc: 0.59
Batch: 480; loss: 1.33; acc: 0.67
Batch: 500; loss: 1.4; acc: 0.56
Batch: 520; loss: 1.34; acc: 0.67
Batch: 540; loss: 1.46; acc: 0.58
Batch: 560; loss: 1.33; acc: 0.64
Batch: 580; loss: 1.38; acc: 0.67
Batch: 600; loss: 1.2; acc: 0.66
Batch: 620; loss: 1.34; acc: 0.61
Batch: 640; loss: 1.24; acc: 0.62
Batch: 660; loss: 1.26; acc: 0.62
Batch: 680; loss: 1.35; acc: 0.61
Batch: 700; loss: 1.47; acc: 0.58
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.19; acc: 0.64
Batch: 760; loss: 1.42; acc: 0.62
Batch: 780; loss: 1.21; acc: 0.69
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.0001233721268363297
0.00011596544209169224
Batch: 0; loss: 1.09; acc: 0.8
Batch: 20; loss: 1.4; acc: 0.56
Batch: 40; loss: 1.06; acc: 0.75
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.28; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.62
Batch: 140; loss: 1.04; acc: 0.81
Val Epoch over. val_loss: 1.2401263334189252; val_accuracy: 0.6599323248407644 

The current subspace-distance is: 0.00011596544209169224 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.37; acc: 0.59
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 1.56; acc: 0.48
Batch: 60; loss: 1.29; acc: 0.61
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 1.25; acc: 0.67
Batch: 160; loss: 1.17; acc: 0.7
Batch: 180; loss: 1.24; acc: 0.69
Batch: 200; loss: 1.35; acc: 0.59
Batch: 220; loss: 1.2; acc: 0.66
Batch: 240; loss: 1.34; acc: 0.64
Batch: 260; loss: 1.23; acc: 0.7
Batch: 280; loss: 1.23; acc: 0.64
Batch: 300; loss: 1.33; acc: 0.59
Batch: 320; loss: 1.64; acc: 0.41
Batch: 340; loss: 1.28; acc: 0.59
Batch: 360; loss: 1.39; acc: 0.5
Batch: 380; loss: 1.16; acc: 0.73
Batch: 400; loss: 1.52; acc: 0.58
Batch: 420; loss: 1.23; acc: 0.61
Batch: 440; loss: 1.44; acc: 0.55
Batch: 460; loss: 1.34; acc: 0.58
Batch: 480; loss: 1.27; acc: 0.61
Batch: 500; loss: 1.17; acc: 0.7
Batch: 520; loss: 1.49; acc: 0.52
Batch: 540; loss: 1.16; acc: 0.75
Batch: 560; loss: 1.13; acc: 0.67
Batch: 580; loss: 1.28; acc: 0.61
Batch: 600; loss: 1.29; acc: 0.62
Batch: 620; loss: 1.34; acc: 0.62
Batch: 640; loss: 1.16; acc: 0.67
Batch: 660; loss: 1.42; acc: 0.56
Batch: 680; loss: 1.21; acc: 0.59
Batch: 700; loss: 1.4; acc: 0.59
Batch: 720; loss: 1.37; acc: 0.62
Batch: 740; loss: 1.19; acc: 0.7
Batch: 760; loss: 1.23; acc: 0.66
Batch: 780; loss: 1.32; acc: 0.66
Train Epoch over. train_loss: 1.3; train_accuracy: 0.63 

0.00012448742927517742
0.00011810380237875506
Batch: 0; loss: 1.09; acc: 0.81
Batch: 20; loss: 1.4; acc: 0.59
Batch: 40; loss: 1.08; acc: 0.77
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.28; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.64
Batch: 140; loss: 1.06; acc: 0.81
Val Epoch over. val_loss: 1.2430912373931544; val_accuracy: 0.6663017515923567 

The current subspace-distance is: 0.00011810380237875506 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_1_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 13.5

The number of parameters is: 279504

The number of individual parameters is:

108
432
108
108
162
52488
162
162
324
157464
324
324
64
62208
64
64
4096
64
640
10
64
64

nonzero elements in E: 83851192
elements in E: 83851200
fraction nonzero: 0.9999999045928979
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.48; acc: 0.05
Batch: 20; loss: 2.12; acc: 0.2
Batch: 40; loss: 2.09; acc: 0.2
Batch: 60; loss: 2.04; acc: 0.25
Batch: 80; loss: 1.96; acc: 0.39
Batch: 100; loss: 1.91; acc: 0.34
Batch: 120; loss: 1.97; acc: 0.36
Batch: 140; loss: 1.93; acc: 0.39
Batch: 160; loss: 2.08; acc: 0.23
Batch: 180; loss: 1.64; acc: 0.52
Batch: 200; loss: 1.89; acc: 0.39
Batch: 220; loss: 1.64; acc: 0.56
Batch: 240; loss: 1.8; acc: 0.45
Batch: 260; loss: 1.66; acc: 0.59
Batch: 280; loss: 1.71; acc: 0.45
Batch: 300; loss: 1.64; acc: 0.48
Batch: 320; loss: 1.62; acc: 0.48
Batch: 340; loss: 1.66; acc: 0.58
Batch: 360; loss: 1.64; acc: 0.55
Batch: 380; loss: 1.6; acc: 0.56
Batch: 400; loss: 1.59; acc: 0.48
Batch: 420; loss: 1.74; acc: 0.39
Batch: 440; loss: 1.77; acc: 0.48
Batch: 460; loss: 1.76; acc: 0.36
Batch: 480; loss: 1.55; acc: 0.55
Batch: 500; loss: 1.65; acc: 0.42
Batch: 520; loss: 1.47; acc: 0.59
Batch: 540; loss: 1.53; acc: 0.58
Batch: 560; loss: 1.55; acc: 0.58
Batch: 580; loss: 1.54; acc: 0.56
Batch: 600; loss: 1.67; acc: 0.47
Batch: 620; loss: 1.5; acc: 0.58
Batch: 640; loss: 1.68; acc: 0.5
Batch: 660; loss: 1.68; acc: 0.5
Batch: 680; loss: 1.62; acc: 0.55
Batch: 700; loss: 1.67; acc: 0.47
Batch: 720; loss: 1.47; acc: 0.61
Batch: 740; loss: 1.51; acc: 0.59
Batch: 760; loss: 1.37; acc: 0.64
Batch: 780; loss: 1.46; acc: 0.58
Train Epoch over. train_loss: 1.69; train_accuracy: 0.49 

5.320024138200097e-05
4.537337372312322e-05
Batch: 0; loss: 1.45; acc: 0.55
Batch: 20; loss: 1.73; acc: 0.45
Batch: 40; loss: 1.27; acc: 0.69
Batch: 60; loss: 1.38; acc: 0.59
Batch: 80; loss: 1.52; acc: 0.5
Batch: 100; loss: 1.53; acc: 0.59
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.43; acc: 0.62
Val Epoch over. val_loss: 1.5274188966508124; val_accuracy: 0.5477707006369427 

The current subspace-distance is: 4.537337372312322e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.5; acc: 0.58
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.54; acc: 0.5
Batch: 60; loss: 1.42; acc: 0.62
Batch: 80; loss: 1.46; acc: 0.58
Batch: 100; loss: 1.5; acc: 0.62
Batch: 120; loss: 1.61; acc: 0.53
Batch: 140; loss: 1.38; acc: 0.67
Batch: 160; loss: 1.28; acc: 0.7
Batch: 180; loss: 1.72; acc: 0.41
Batch: 200; loss: 1.44; acc: 0.56
Batch: 220; loss: 1.34; acc: 0.58
Batch: 240; loss: 1.5; acc: 0.56
Batch: 260; loss: 1.41; acc: 0.61
Batch: 280; loss: 1.58; acc: 0.52
Batch: 300; loss: 1.43; acc: 0.64
Batch: 320; loss: 1.4; acc: 0.66
Batch: 340; loss: 1.39; acc: 0.62
Batch: 360; loss: 1.55; acc: 0.56
Batch: 380; loss: 1.53; acc: 0.55
Batch: 400; loss: 1.23; acc: 0.77
Batch: 420; loss: 1.42; acc: 0.59
Batch: 440; loss: 1.34; acc: 0.66
Batch: 460; loss: 1.32; acc: 0.64
Batch: 480; loss: 1.28; acc: 0.67
Batch: 500; loss: 1.5; acc: 0.55
Batch: 520; loss: 1.59; acc: 0.56
Batch: 540; loss: 1.53; acc: 0.53
Batch: 560; loss: 1.36; acc: 0.7
Batch: 580; loss: 1.29; acc: 0.7
Batch: 600; loss: 1.35; acc: 0.64
Batch: 620; loss: 1.29; acc: 0.61
Batch: 640; loss: 1.52; acc: 0.53
Batch: 660; loss: 1.32; acc: 0.62
Batch: 680; loss: 1.31; acc: 0.7
Batch: 700; loss: 1.39; acc: 0.56
Batch: 720; loss: 1.22; acc: 0.75
Batch: 740; loss: 1.45; acc: 0.61
Batch: 760; loss: 1.26; acc: 0.62
Batch: 780; loss: 1.43; acc: 0.61
Train Epoch over. train_loss: 1.42; train_accuracy: 0.6 

7.178810483310372e-05
6.514502456411719e-05
Batch: 0; loss: 1.18; acc: 0.64
Batch: 20; loss: 1.55; acc: 0.53
Batch: 40; loss: 1.09; acc: 0.69
Batch: 60; loss: 1.25; acc: 0.69
Batch: 80; loss: 1.23; acc: 0.59
Batch: 100; loss: 1.39; acc: 0.59
Batch: 120; loss: 1.45; acc: 0.62
Batch: 140; loss: 1.19; acc: 0.7
Val Epoch over. val_loss: 1.3416732382622494; val_accuracy: 0.6161425159235668 

The current subspace-distance is: 6.514502456411719e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.45
Batch: 40; loss: 1.49; acc: 0.61
Batch: 60; loss: 1.41; acc: 0.61
Batch: 80; loss: 1.4; acc: 0.59
Batch: 100; loss: 1.37; acc: 0.56
Batch: 120; loss: 1.45; acc: 0.61
Batch: 140; loss: 1.32; acc: 0.69
Batch: 160; loss: 1.27; acc: 0.69
Batch: 180; loss: 1.43; acc: 0.53
Batch: 200; loss: 1.34; acc: 0.59
Batch: 220; loss: 1.28; acc: 0.66
Batch: 240; loss: 1.3; acc: 0.66
Batch: 260; loss: 1.4; acc: 0.56
Batch: 280; loss: 1.31; acc: 0.61
Batch: 300; loss: 1.24; acc: 0.61
Batch: 320; loss: 1.42; acc: 0.55
Batch: 340; loss: 1.62; acc: 0.48
Batch: 360; loss: 1.57; acc: 0.5
Batch: 380; loss: 1.37; acc: 0.59
Batch: 400; loss: 1.32; acc: 0.67
Batch: 420; loss: 1.24; acc: 0.67
Batch: 440; loss: 1.23; acc: 0.69
Batch: 460; loss: 1.43; acc: 0.56
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.48; acc: 0.56
Batch: 520; loss: 1.43; acc: 0.56
Batch: 540; loss: 1.21; acc: 0.69
Batch: 560; loss: 1.29; acc: 0.69
Batch: 580; loss: 1.25; acc: 0.62
Batch: 600; loss: 1.15; acc: 0.72
Batch: 620; loss: 1.23; acc: 0.7
Batch: 640; loss: 1.21; acc: 0.75
Batch: 660; loss: 1.44; acc: 0.53
Batch: 680; loss: 1.33; acc: 0.62
Batch: 700; loss: 1.24; acc: 0.7
Batch: 720; loss: 1.39; acc: 0.55
Batch: 740; loss: 1.35; acc: 0.62
Batch: 760; loss: 1.39; acc: 0.56
Batch: 780; loss: 1.16; acc: 0.69
Train Epoch over. train_loss: 1.34; train_accuracy: 0.62 

8.202189928852022e-05
7.547349378000945e-05
Batch: 0; loss: 1.13; acc: 0.72
Batch: 20; loss: 1.51; acc: 0.61
Batch: 40; loss: 1.04; acc: 0.75
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.28; acc: 0.69
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 1.12; acc: 0.77
Val Epoch over. val_loss: 1.278946084581363; val_accuracy: 0.6577428343949044 

The current subspace-distance is: 7.547349378000945e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.58
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 1.27; acc: 0.66
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.35; acc: 0.62
Batch: 100; loss: 1.33; acc: 0.67
Batch: 120; loss: 1.24; acc: 0.69
Batch: 140; loss: 1.3; acc: 0.61
Batch: 160; loss: 1.28; acc: 0.69
Batch: 180; loss: 1.26; acc: 0.69
Batch: 200; loss: 1.25; acc: 0.67
Batch: 220; loss: 1.15; acc: 0.7
Batch: 240; loss: 1.12; acc: 0.77
Batch: 260; loss: 1.16; acc: 0.64
Batch: 280; loss: 1.35; acc: 0.59
Batch: 300; loss: 1.38; acc: 0.55
Batch: 320; loss: 1.26; acc: 0.61
Batch: 340; loss: 1.27; acc: 0.67
Batch: 360; loss: 1.46; acc: 0.53
Batch: 380; loss: 1.33; acc: 0.64
Batch: 400; loss: 1.36; acc: 0.61
Batch: 420; loss: 1.43; acc: 0.53
Batch: 440; loss: 1.27; acc: 0.66
Batch: 460; loss: 1.38; acc: 0.62
Batch: 480; loss: 1.26; acc: 0.64
Batch: 500; loss: 1.26; acc: 0.58
Batch: 520; loss: 1.42; acc: 0.61
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.18; acc: 0.72
Batch: 580; loss: 1.24; acc: 0.64
Batch: 600; loss: 1.34; acc: 0.61
Batch: 620; loss: 1.3; acc: 0.59
Batch: 640; loss: 1.31; acc: 0.61
Batch: 660; loss: 1.22; acc: 0.62
Batch: 680; loss: 1.21; acc: 0.66
Batch: 700; loss: 1.34; acc: 0.56
Batch: 720; loss: 1.23; acc: 0.64
Batch: 740; loss: 1.21; acc: 0.73
Batch: 760; loss: 1.3; acc: 0.67
Batch: 780; loss: 1.24; acc: 0.69
Train Epoch over. train_loss: 1.29; train_accuracy: 0.64 

9.149697871180251e-05
8.513477223459631e-05
Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.13; acc: 0.69
Batch: 60; loss: 1.27; acc: 0.61
Batch: 80; loss: 1.18; acc: 0.62
Batch: 100; loss: 1.28; acc: 0.64
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 1.28; acc: 0.69
Val Epoch over. val_loss: 1.384434096372811; val_accuracy: 0.5714570063694268 

The current subspace-distance is: 8.513477223459631e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.39; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.58
Batch: 40; loss: 1.17; acc: 0.7
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.35; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 1.43; acc: 0.61
Batch: 160; loss: 1.21; acc: 0.7
Batch: 180; loss: 1.16; acc: 0.69
Batch: 200; loss: 1.22; acc: 0.72
Batch: 220; loss: 1.29; acc: 0.67
Batch: 240; loss: 1.21; acc: 0.67
Batch: 260; loss: 1.26; acc: 0.67
Batch: 280; loss: 1.25; acc: 0.69
Batch: 300; loss: 1.35; acc: 0.56
Batch: 320; loss: 1.3; acc: 0.61
Batch: 340; loss: 1.42; acc: 0.64
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.15; acc: 0.73
Batch: 400; loss: 1.29; acc: 0.64
Batch: 420; loss: 1.29; acc: 0.66
Batch: 440; loss: 1.16; acc: 0.69
Batch: 460; loss: 1.04; acc: 0.73
Batch: 480; loss: 1.35; acc: 0.64
Batch: 500; loss: 1.11; acc: 0.77
Batch: 520; loss: 1.27; acc: 0.64
Batch: 540; loss: 1.49; acc: 0.47
Batch: 560; loss: 1.47; acc: 0.53
Batch: 580; loss: 1.25; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.73
Batch: 620; loss: 1.12; acc: 0.78
Batch: 640; loss: 1.3; acc: 0.59
Batch: 660; loss: 1.29; acc: 0.62
Batch: 680; loss: 1.28; acc: 0.7
Batch: 700; loss: 1.39; acc: 0.61
Batch: 720; loss: 1.32; acc: 0.67
Batch: 740; loss: 1.25; acc: 0.67
Batch: 760; loss: 1.34; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.75
Train Epoch over. train_loss: 1.27; train_accuracy: 0.65 

9.729397424962372e-05
9.107643563766032e-05
Batch: 0; loss: 1.31; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.55
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.58; acc: 0.52
Batch: 80; loss: 1.51; acc: 0.45
Batch: 100; loss: 1.38; acc: 0.52
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.71; acc: 0.5
Val Epoch over. val_loss: 1.647620991536766; val_accuracy: 0.4794984076433121 

The current subspace-distance is: 9.107643563766032e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.72
Batch: 20; loss: 1.22; acc: 0.7
Batch: 40; loss: 1.11; acc: 0.73
Batch: 60; loss: 1.29; acc: 0.61
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 1.31; acc: 0.61
Batch: 160; loss: 1.05; acc: 0.77
Batch: 180; loss: 1.12; acc: 0.73
Batch: 200; loss: 1.16; acc: 0.73
Batch: 220; loss: 1.27; acc: 0.59
Batch: 240; loss: 1.28; acc: 0.61
Batch: 260; loss: 1.18; acc: 0.66
Batch: 280; loss: 1.27; acc: 0.7
Batch: 300; loss: 1.13; acc: 0.75
Batch: 320; loss: 1.01; acc: 0.75
Batch: 340; loss: 1.22; acc: 0.72
Batch: 360; loss: 1.26; acc: 0.64
Batch: 380; loss: 1.21; acc: 0.69
Batch: 400; loss: 1.41; acc: 0.5
Batch: 420; loss: 1.31; acc: 0.66
Batch: 440; loss: 1.47; acc: 0.52
Batch: 460; loss: 1.25; acc: 0.7
Batch: 480; loss: 1.33; acc: 0.61
Batch: 500; loss: 1.16; acc: 0.7
Batch: 520; loss: 1.39; acc: 0.56
Batch: 540; loss: 1.29; acc: 0.58
Batch: 560; loss: 1.2; acc: 0.7
Batch: 580; loss: 1.32; acc: 0.64
Batch: 600; loss: 1.28; acc: 0.69
Batch: 620; loss: 1.14; acc: 0.66
Batch: 640; loss: 1.16; acc: 0.73
Batch: 660; loss: 1.12; acc: 0.73
Batch: 680; loss: 1.14; acc: 0.78
Batch: 700; loss: 1.22; acc: 0.7
Batch: 720; loss: 1.22; acc: 0.64
Batch: 740; loss: 1.27; acc: 0.7
Batch: 760; loss: 1.3; acc: 0.61
Batch: 780; loss: 1.27; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.00010369131632614881
9.804194269236177e-05
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.47; acc: 0.59
Batch: 40; loss: 0.98; acc: 0.73
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.14; acc: 0.62
Batch: 100; loss: 1.26; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.62
Batch: 140; loss: 1.08; acc: 0.75
Val Epoch over. val_loss: 1.2615430719533545; val_accuracy: 0.6362460191082803 

The current subspace-distance is: 9.804194269236177e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 1.03; acc: 0.78
Batch: 40; loss: 1.31; acc: 0.56
Batch: 60; loss: 1.04; acc: 0.73
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.66
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.34; acc: 0.62
Batch: 160; loss: 1.23; acc: 0.64
Batch: 180; loss: 1.19; acc: 0.67
Batch: 200; loss: 1.31; acc: 0.59
Batch: 220; loss: 1.25; acc: 0.69
Batch: 240; loss: 1.16; acc: 0.7
Batch: 260; loss: 1.4; acc: 0.55
Batch: 280; loss: 1.16; acc: 0.7
Batch: 300; loss: 1.25; acc: 0.67
Batch: 320; loss: 1.28; acc: 0.67
Batch: 340; loss: 1.35; acc: 0.58
Batch: 360; loss: 1.19; acc: 0.67
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 1.15; acc: 0.7
Batch: 420; loss: 1.46; acc: 0.44
Batch: 440; loss: 1.17; acc: 0.64
Batch: 460; loss: 1.28; acc: 0.61
Batch: 480; loss: 1.18; acc: 0.73
Batch: 500; loss: 1.2; acc: 0.67
Batch: 520; loss: 1.03; acc: 0.75
Batch: 540; loss: 1.23; acc: 0.66
Batch: 560; loss: 1.15; acc: 0.72
Batch: 580; loss: 1.51; acc: 0.5
Batch: 600; loss: 1.43; acc: 0.58
Batch: 620; loss: 1.39; acc: 0.56
Batch: 640; loss: 1.04; acc: 0.75
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.18; acc: 0.69
Batch: 700; loss: 1.16; acc: 0.69
Batch: 720; loss: 1.45; acc: 0.58
Batch: 740; loss: 1.2; acc: 0.66
Batch: 760; loss: 1.05; acc: 0.8
Batch: 780; loss: 1.18; acc: 0.69
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00011033113696612418
0.00010378760634921491
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.36; acc: 0.61
Batch: 40; loss: 1.02; acc: 0.72
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 1.18; acc: 0.64
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 1.15; acc: 0.7
Val Epoch over. val_loss: 1.2539451600639684; val_accuracy: 0.6503781847133758 

The current subspace-distance is: 0.00010378760634921491 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 1.12; acc: 0.7
Batch: 60; loss: 1.17; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.69
Batch: 100; loss: 1.17; acc: 0.67
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 1.17; acc: 0.7
Batch: 160; loss: 1.11; acc: 0.77
Batch: 180; loss: 1.28; acc: 0.7
Batch: 200; loss: 1.17; acc: 0.77
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 1.39; acc: 0.58
Batch: 260; loss: 1.18; acc: 0.73
Batch: 280; loss: 1.31; acc: 0.55
Batch: 300; loss: 1.23; acc: 0.62
Batch: 320; loss: 1.04; acc: 0.75
Batch: 340; loss: 1.08; acc: 0.77
Batch: 360; loss: 1.32; acc: 0.7
Batch: 380; loss: 1.45; acc: 0.55
Batch: 400; loss: 1.24; acc: 0.7
Batch: 420; loss: 1.2; acc: 0.67
Batch: 440; loss: 1.4; acc: 0.61
Batch: 460; loss: 1.34; acc: 0.61
Batch: 480; loss: 1.36; acc: 0.62
Batch: 500; loss: 1.21; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.8
Batch: 540; loss: 1.19; acc: 0.72
Batch: 560; loss: 1.2; acc: 0.67
Batch: 580; loss: 1.18; acc: 0.73
Batch: 600; loss: 1.12; acc: 0.67
Batch: 620; loss: 1.33; acc: 0.61
Batch: 640; loss: 1.17; acc: 0.72
Batch: 660; loss: 1.26; acc: 0.58
Batch: 680; loss: 1.24; acc: 0.64
Batch: 700; loss: 1.19; acc: 0.69
Batch: 720; loss: 1.33; acc: 0.64
Batch: 740; loss: 1.13; acc: 0.66
Batch: 760; loss: 1.27; acc: 0.61
Batch: 780; loss: 1.23; acc: 0.72
Train Epoch over. train_loss: 1.21; train_accuracy: 0.66 

0.00011481720866868272
0.00010890026896959171
Batch: 0; loss: 1.2; acc: 0.67
Batch: 20; loss: 1.52; acc: 0.52
Batch: 40; loss: 1.09; acc: 0.75
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.34; acc: 0.62
Batch: 100; loss: 1.22; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.55
Batch: 140; loss: 1.13; acc: 0.75
Val Epoch over. val_loss: 1.3011834405030414; val_accuracy: 0.6278861464968153 

The current subspace-distance is: 0.00010890026896959171 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 1.32; acc: 0.7
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.28; acc: 0.66
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 1.03; acc: 0.8
Batch: 140; loss: 1.17; acc: 0.77
Batch: 160; loss: 1.5; acc: 0.53
Batch: 180; loss: 1.13; acc: 0.67
Batch: 200; loss: 1.25; acc: 0.7
Batch: 220; loss: 1.24; acc: 0.66
Batch: 240; loss: 1.34; acc: 0.58
Batch: 260; loss: 1.19; acc: 0.7
Batch: 280; loss: 1.21; acc: 0.67
Batch: 300; loss: 1.15; acc: 0.7
Batch: 320; loss: 1.09; acc: 0.67
Batch: 340; loss: 1.08; acc: 0.67
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.28; acc: 0.66
Batch: 400; loss: 1.09; acc: 0.72
Batch: 420; loss: 1.25; acc: 0.67
Batch: 440; loss: 0.97; acc: 0.77
Batch: 460; loss: 0.96; acc: 0.77
Batch: 480; loss: 1.34; acc: 0.58
Batch: 500; loss: 1.18; acc: 0.7
Batch: 520; loss: 1.29; acc: 0.56
Batch: 540; loss: 1.36; acc: 0.52
Batch: 560; loss: 1.27; acc: 0.61
Batch: 580; loss: 1.28; acc: 0.61
Batch: 600; loss: 1.08; acc: 0.77
Batch: 620; loss: 1.03; acc: 0.77
Batch: 640; loss: 1.2; acc: 0.66
Batch: 660; loss: 1.17; acc: 0.66
Batch: 680; loss: 1.25; acc: 0.64
Batch: 700; loss: 1.11; acc: 0.67
Batch: 720; loss: 1.25; acc: 0.66
Batch: 740; loss: 1.35; acc: 0.61
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.14; acc: 0.72
Train Epoch over. train_loss: 1.19; train_accuracy: 0.67 

0.00012089993833797053
0.00011518892279127613
Batch: 0; loss: 1.14; acc: 0.59
Batch: 20; loss: 1.51; acc: 0.56
Batch: 40; loss: 1.09; acc: 0.62
Batch: 60; loss: 1.3; acc: 0.62
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 1.34; acc: 0.61
Batch: 120; loss: 1.38; acc: 0.56
Batch: 140; loss: 1.24; acc: 0.66
Val Epoch over. val_loss: 1.3437586863329456; val_accuracy: 0.59375 

The current subspace-distance is: 0.00011518892279127613 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 1.3; acc: 0.64
Batch: 60; loss: 1.09; acc: 0.73
Batch: 80; loss: 1.17; acc: 0.67
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 1.19; acc: 0.62
Batch: 160; loss: 1.3; acc: 0.61
Batch: 180; loss: 1.14; acc: 0.62
Batch: 200; loss: 1.19; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.66
Batch: 240; loss: 1.04; acc: 0.73
Batch: 260; loss: 1.15; acc: 0.66
Batch: 280; loss: 1.15; acc: 0.66
Batch: 300; loss: 1.12; acc: 0.7
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.4; acc: 0.56
Batch: 360; loss: 1.18; acc: 0.7
Batch: 380; loss: 1.1; acc: 0.72
Batch: 400; loss: 1.14; acc: 0.66
Batch: 420; loss: 1.32; acc: 0.59
Batch: 440; loss: 1.37; acc: 0.64
Batch: 460; loss: 0.99; acc: 0.75
Batch: 480; loss: 1.33; acc: 0.56
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 1.3; acc: 0.61
Batch: 540; loss: 1.19; acc: 0.59
Batch: 560; loss: 1.03; acc: 0.72
Batch: 580; loss: 1.01; acc: 0.75
Batch: 600; loss: 1.22; acc: 0.58
Batch: 620; loss: 1.13; acc: 0.75
Batch: 640; loss: 1.2; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.8
Batch: 680; loss: 1.23; acc: 0.67
Batch: 700; loss: 1.1; acc: 0.75
Batch: 720; loss: 1.17; acc: 0.64
Batch: 740; loss: 1.19; acc: 0.69
Batch: 760; loss: 1.23; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.73
Train Epoch over. train_loss: 1.17; train_accuracy: 0.67 

0.0001248628250323236
0.00011930629989365116
Batch: 0; loss: 1.02; acc: 0.72
Batch: 20; loss: 1.31; acc: 0.62
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.96; acc: 0.72
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.06; acc: 0.73
Val Epoch over. val_loss: 1.1716417203283613; val_accuracy: 0.6504777070063694 

The current subspace-distance is: 0.00011930629989365116 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.14; acc: 0.77
Batch: 20; loss: 1.15; acc: 0.72
Batch: 40; loss: 1.06; acc: 0.73
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.73
Batch: 100; loss: 1.06; acc: 0.67
Batch: 120; loss: 1.15; acc: 0.69
Batch: 140; loss: 1.28; acc: 0.7
Batch: 160; loss: 1.3; acc: 0.64
Batch: 180; loss: 1.07; acc: 0.66
Batch: 200; loss: 1.22; acc: 0.61
Batch: 220; loss: 1.27; acc: 0.59
Batch: 240; loss: 1.09; acc: 0.77
Batch: 260; loss: 1.09; acc: 0.77
Batch: 280; loss: 1.36; acc: 0.61
Batch: 300; loss: 1.14; acc: 0.61
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 1.26; acc: 0.66
Batch: 360; loss: 1.04; acc: 0.67
Batch: 380; loss: 1.21; acc: 0.69
Batch: 400; loss: 1.05; acc: 0.73
Batch: 420; loss: 1.22; acc: 0.67
Batch: 440; loss: 1.28; acc: 0.61
Batch: 460; loss: 1.23; acc: 0.72
Batch: 480; loss: 1.08; acc: 0.73
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 1.18; acc: 0.64
Batch: 540; loss: 1.17; acc: 0.67
Batch: 560; loss: 1.24; acc: 0.64
Batch: 580; loss: 1.07; acc: 0.75
Batch: 600; loss: 1.03; acc: 0.8
Batch: 620; loss: 1.18; acc: 0.62
Batch: 640; loss: 1.44; acc: 0.55
Batch: 660; loss: 1.19; acc: 0.72
Batch: 680; loss: 1.25; acc: 0.59
Batch: 700; loss: 1.11; acc: 0.72
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.23; acc: 0.66
Batch: 780; loss: 1.29; acc: 0.62
Train Epoch over. train_loss: 1.16; train_accuracy: 0.67 

0.00012754005729220808
0.00012060595327056944
Batch: 0; loss: 0.94; acc: 0.75
Batch: 20; loss: 1.24; acc: 0.64
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.96; acc: 0.72
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 0.94; acc: 0.83
Val Epoch over. val_loss: 1.0820322909932227; val_accuracy: 0.7056130573248408 

The current subspace-distance is: 0.00012060595327056944 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.73
Batch: 20; loss: 1.27; acc: 0.66
Batch: 40; loss: 0.99; acc: 0.75
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.14; acc: 0.72
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.08; acc: 0.7
Batch: 140; loss: 1.23; acc: 0.62
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 1.18; acc: 0.66
Batch: 200; loss: 1.04; acc: 0.72
Batch: 220; loss: 0.87; acc: 0.84
Batch: 240; loss: 1.34; acc: 0.53
Batch: 260; loss: 0.99; acc: 0.7
Batch: 280; loss: 1.05; acc: 0.72
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 1.06; acc: 0.7
Batch: 340; loss: 1.27; acc: 0.59
Batch: 360; loss: 1.22; acc: 0.61
Batch: 380; loss: 1.09; acc: 0.7
Batch: 400; loss: 1.31; acc: 0.64
Batch: 420; loss: 1.34; acc: 0.61
Batch: 440; loss: 1.13; acc: 0.73
Batch: 460; loss: 1.2; acc: 0.61
Batch: 480; loss: 0.99; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.59
Batch: 520; loss: 1.14; acc: 0.7
Batch: 540; loss: 1.12; acc: 0.72
Batch: 560; loss: 1.08; acc: 0.69
Batch: 580; loss: 1.33; acc: 0.59
Batch: 600; loss: 1.09; acc: 0.75
Batch: 620; loss: 1.12; acc: 0.69
Batch: 640; loss: 1.02; acc: 0.7
Batch: 660; loss: 1.45; acc: 0.58
Batch: 680; loss: 1.06; acc: 0.73
Batch: 700; loss: 1.11; acc: 0.69
Batch: 720; loss: 1.12; acc: 0.62
Batch: 740; loss: 1.16; acc: 0.73
Batch: 760; loss: 1.14; acc: 0.62
Batch: 780; loss: 1.05; acc: 0.75
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

0.00012759288074448705
0.00012110408715670928
Batch: 0; loss: 0.94; acc: 0.75
Batch: 20; loss: 1.24; acc: 0.69
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.78
Val Epoch over. val_loss: 1.1023644292430512; val_accuracy: 0.6995421974522293 

The current subspace-distance is: 0.00012110408715670928 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.33; acc: 0.58
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.27; acc: 0.59
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.18; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.23; acc: 0.7
Batch: 160; loss: 1.06; acc: 0.72
Batch: 180; loss: 1.22; acc: 0.69
Batch: 200; loss: 1.25; acc: 0.61
Batch: 220; loss: 1.13; acc: 0.67
Batch: 240; loss: 1.13; acc: 0.67
Batch: 260; loss: 1.23; acc: 0.64
Batch: 280; loss: 1.18; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.72
Batch: 320; loss: 1.16; acc: 0.7
Batch: 340; loss: 1.04; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.35; acc: 0.59
Batch: 400; loss: 1.12; acc: 0.66
Batch: 420; loss: 1.06; acc: 0.78
Batch: 440; loss: 1.15; acc: 0.67
Batch: 460; loss: 1.18; acc: 0.67
Batch: 480; loss: 1.06; acc: 0.77
Batch: 500; loss: 1.28; acc: 0.55
Batch: 520; loss: 1.16; acc: 0.64
Batch: 540; loss: 1.19; acc: 0.69
Batch: 560; loss: 1.22; acc: 0.64
Batch: 580; loss: 1.12; acc: 0.69
Batch: 600; loss: 0.98; acc: 0.72
Batch: 620; loss: 1.22; acc: 0.61
Batch: 640; loss: 1.18; acc: 0.66
Batch: 660; loss: 1.17; acc: 0.75
Batch: 680; loss: 1.3; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.73
Batch: 720; loss: 1.15; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.72
Batch: 760; loss: 1.16; acc: 0.62
Batch: 780; loss: 1.28; acc: 0.62
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

0.0001284328754991293
0.00012188818800495937
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.73
Batch: 120; loss: 1.16; acc: 0.72
Batch: 140; loss: 0.93; acc: 0.78
Val Epoch over. val_loss: 1.0757797273101322; val_accuracy: 0.7065087579617835 

The current subspace-distance is: 0.00012188818800495937 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.06; acc: 0.77
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 1.29; acc: 0.61
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 1.15; acc: 0.62
Batch: 100; loss: 1.29; acc: 0.59
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 1.2; acc: 0.66
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 1.23; acc: 0.67
Batch: 200; loss: 1.18; acc: 0.64
Batch: 220; loss: 1.13; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.69
Batch: 260; loss: 1.26; acc: 0.58
Batch: 280; loss: 1.28; acc: 0.5
Batch: 300; loss: 1.2; acc: 0.64
Batch: 320; loss: 1.04; acc: 0.77
Batch: 340; loss: 1.3; acc: 0.66
Batch: 360; loss: 1.16; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 1.28; acc: 0.61
Batch: 420; loss: 0.9; acc: 0.86
Batch: 440; loss: 0.99; acc: 0.73
Batch: 460; loss: 1.05; acc: 0.81
Batch: 480; loss: 1.04; acc: 0.72
Batch: 500; loss: 0.98; acc: 0.75
Batch: 520; loss: 1.12; acc: 0.7
Batch: 540; loss: 1.12; acc: 0.61
Batch: 560; loss: 1.18; acc: 0.64
Batch: 580; loss: 1.29; acc: 0.62
Batch: 600; loss: 1.18; acc: 0.67
Batch: 620; loss: 1.18; acc: 0.62
Batch: 640; loss: 1.16; acc: 0.69
Batch: 660; loss: 1.11; acc: 0.67
Batch: 680; loss: 1.08; acc: 0.78
Batch: 700; loss: 1.18; acc: 0.73
Batch: 720; loss: 1.13; acc: 0.72
Batch: 740; loss: 1.15; acc: 0.66
Batch: 760; loss: 1.2; acc: 0.61
Batch: 780; loss: 1.11; acc: 0.66
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00013073786976747215
0.00012350382166914642
Batch: 0; loss: 0.97; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.67
Batch: 40; loss: 0.82; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.0; acc: 0.73
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 1.18; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.0964412172888494; val_accuracy: 0.7019307324840764 

The current subspace-distance is: 0.00012350382166914642 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 1.37; acc: 0.66
Batch: 60; loss: 1.35; acc: 0.55
Batch: 80; loss: 1.12; acc: 0.66
Batch: 100; loss: 1.09; acc: 0.61
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 1.22; acc: 0.59
Batch: 160; loss: 1.08; acc: 0.7
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.06; acc: 0.7
Batch: 220; loss: 1.24; acc: 0.7
Batch: 240; loss: 1.3; acc: 0.55
Batch: 260; loss: 1.17; acc: 0.62
Batch: 280; loss: 0.98; acc: 0.78
Batch: 300; loss: 1.21; acc: 0.69
Batch: 320; loss: 1.32; acc: 0.61
Batch: 340; loss: 1.08; acc: 0.72
Batch: 360; loss: 0.96; acc: 0.78
Batch: 380; loss: 1.09; acc: 0.75
Batch: 400; loss: 1.29; acc: 0.59
Batch: 420; loss: 1.07; acc: 0.66
Batch: 440; loss: 1.09; acc: 0.62
Batch: 460; loss: 1.06; acc: 0.7
Batch: 480; loss: 1.05; acc: 0.72
Batch: 500; loss: 1.34; acc: 0.59
Batch: 520; loss: 1.1; acc: 0.66
Batch: 540; loss: 0.98; acc: 0.73
Batch: 560; loss: 1.14; acc: 0.7
Batch: 580; loss: 1.19; acc: 0.67
Batch: 600; loss: 1.18; acc: 0.66
Batch: 620; loss: 1.1; acc: 0.69
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.14; acc: 0.67
Batch: 680; loss: 1.09; acc: 0.67
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.13; acc: 0.66
Batch: 740; loss: 1.12; acc: 0.61
Batch: 760; loss: 1.2; acc: 0.62
Batch: 780; loss: 1.14; acc: 0.7
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00013066954852547497
0.0001256528339581564
Batch: 0; loss: 0.96; acc: 0.77
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 0.85; acc: 0.81
Batch: 60; loss: 1.04; acc: 0.69
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.78
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.81
Val Epoch over. val_loss: 1.1234163649522575; val_accuracy: 0.6876990445859873 

The current subspace-distance is: 0.0001256528339581564 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.51; acc: 0.53
Batch: 20; loss: 1.04; acc: 0.75
Batch: 40; loss: 0.96; acc: 0.72
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 1.22; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 1.05; acc: 0.75
Batch: 160; loss: 1.02; acc: 0.75
Batch: 180; loss: 1.29; acc: 0.59
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 1.22; acc: 0.67
Batch: 240; loss: 1.09; acc: 0.72
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 1.23; acc: 0.56
Batch: 300; loss: 1.31; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.72
Batch: 340; loss: 1.06; acc: 0.72
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.04; acc: 0.77
Batch: 400; loss: 1.33; acc: 0.59
Batch: 420; loss: 1.16; acc: 0.61
Batch: 440; loss: 1.16; acc: 0.69
Batch: 460; loss: 1.08; acc: 0.78
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 1.17; acc: 0.67
Batch: 520; loss: 0.98; acc: 0.7
Batch: 540; loss: 1.28; acc: 0.66
Batch: 560; loss: 1.07; acc: 0.78
Batch: 580; loss: 1.19; acc: 0.66
Batch: 600; loss: 1.08; acc: 0.72
Batch: 620; loss: 1.06; acc: 0.73
Batch: 640; loss: 1.12; acc: 0.59
Batch: 660; loss: 1.03; acc: 0.75
Batch: 680; loss: 1.22; acc: 0.64
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 0.98; acc: 0.77
Batch: 740; loss: 1.11; acc: 0.64
Batch: 760; loss: 1.13; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00013353554822970182
0.00012617964239325374
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.27; acc: 0.61
Batch: 40; loss: 0.84; acc: 0.78
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.0893932178521613; val_accuracy: 0.7024283439490446 

The current subspace-distance is: 0.00012617964239325374 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.59
Batch: 20; loss: 1.35; acc: 0.66
Batch: 40; loss: 1.07; acc: 0.62
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.78
Batch: 100; loss: 0.97; acc: 0.78
Batch: 120; loss: 1.1; acc: 0.7
Batch: 140; loss: 0.94; acc: 0.75
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 1.1; acc: 0.73
Batch: 200; loss: 0.92; acc: 0.75
Batch: 220; loss: 1.12; acc: 0.69
Batch: 240; loss: 0.94; acc: 0.73
Batch: 260; loss: 1.29; acc: 0.59
Batch: 280; loss: 1.34; acc: 0.61
Batch: 300; loss: 1.12; acc: 0.7
Batch: 320; loss: 1.09; acc: 0.67
Batch: 340; loss: 1.18; acc: 0.64
Batch: 360; loss: 1.09; acc: 0.73
Batch: 380; loss: 1.0; acc: 0.8
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 1.12; acc: 0.73
Batch: 460; loss: 0.99; acc: 0.78
Batch: 480; loss: 1.06; acc: 0.7
Batch: 500; loss: 1.02; acc: 0.72
Batch: 520; loss: 1.14; acc: 0.64
Batch: 540; loss: 1.19; acc: 0.67
Batch: 560; loss: 1.16; acc: 0.77
Batch: 580; loss: 1.21; acc: 0.62
Batch: 600; loss: 1.08; acc: 0.7
Batch: 620; loss: 1.09; acc: 0.66
Batch: 640; loss: 1.0; acc: 0.72
Batch: 660; loss: 1.26; acc: 0.67
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 1.02; acc: 0.73
Batch: 740; loss: 0.89; acc: 0.81
Batch: 760; loss: 1.3; acc: 0.58
Batch: 780; loss: 1.27; acc: 0.62
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.0001342313626082614
0.0001288946659769863
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.23; acc: 0.69
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.05; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.78
Val Epoch over. val_loss: 1.0799837936261656; val_accuracy: 0.7073049363057324 

The current subspace-distance is: 0.0001288946659769863 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.56
Batch: 20; loss: 1.14; acc: 0.69
Batch: 40; loss: 1.11; acc: 0.72
Batch: 60; loss: 1.19; acc: 0.64
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 1.23; acc: 0.62
Batch: 160; loss: 1.17; acc: 0.67
Batch: 180; loss: 1.12; acc: 0.62
Batch: 200; loss: 1.03; acc: 0.72
Batch: 220; loss: 1.07; acc: 0.75
Batch: 240; loss: 1.05; acc: 0.73
Batch: 260; loss: 1.23; acc: 0.67
Batch: 280; loss: 1.01; acc: 0.69
Batch: 300; loss: 1.0; acc: 0.75
Batch: 320; loss: 1.03; acc: 0.7
Batch: 340; loss: 1.0; acc: 0.77
Batch: 360; loss: 1.18; acc: 0.56
Batch: 380; loss: 1.15; acc: 0.77
Batch: 400; loss: 1.2; acc: 0.59
Batch: 420; loss: 1.09; acc: 0.77
Batch: 440; loss: 1.32; acc: 0.56
Batch: 460; loss: 1.13; acc: 0.66
Batch: 480; loss: 0.95; acc: 0.78
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.03; acc: 0.7
Batch: 540; loss: 0.97; acc: 0.77
Batch: 560; loss: 1.08; acc: 0.62
Batch: 580; loss: 1.25; acc: 0.61
Batch: 600; loss: 1.26; acc: 0.55
Batch: 620; loss: 1.24; acc: 0.56
Batch: 640; loss: 1.19; acc: 0.64
Batch: 660; loss: 1.0; acc: 0.69
Batch: 680; loss: 1.0; acc: 0.69
Batch: 700; loss: 1.0; acc: 0.73
Batch: 720; loss: 1.18; acc: 0.67
Batch: 740; loss: 0.93; acc: 0.7
Batch: 760; loss: 1.01; acc: 0.67
Batch: 780; loss: 1.04; acc: 0.67
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00013600710371974856
0.00012996305304113775
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.28; acc: 0.64
Batch: 40; loss: 0.86; acc: 0.77
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 1.02; acc: 0.72
Val Epoch over. val_loss: 1.1178055584051048; val_accuracy: 0.6777468152866242 

The current subspace-distance is: 0.00012996305304113775 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.69
Batch: 40; loss: 1.16; acc: 0.62
Batch: 60; loss: 0.89; acc: 0.77
Batch: 80; loss: 1.22; acc: 0.7
Batch: 100; loss: 1.16; acc: 0.67
Batch: 120; loss: 1.06; acc: 0.75
Batch: 140; loss: 1.09; acc: 0.72
Batch: 160; loss: 1.1; acc: 0.69
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.15; acc: 0.67
Batch: 220; loss: 1.11; acc: 0.73
Batch: 240; loss: 1.09; acc: 0.67
Batch: 260; loss: 0.99; acc: 0.8
Batch: 280; loss: 0.92; acc: 0.78
Batch: 300; loss: 1.11; acc: 0.67
Batch: 320; loss: 1.06; acc: 0.67
Batch: 340; loss: 1.09; acc: 0.72
Batch: 360; loss: 1.04; acc: 0.73
Batch: 380; loss: 1.05; acc: 0.77
Batch: 400; loss: 1.16; acc: 0.7
Batch: 420; loss: 1.11; acc: 0.75
Batch: 440; loss: 0.96; acc: 0.77
Batch: 460; loss: 1.23; acc: 0.64
Batch: 480; loss: 0.94; acc: 0.73
Batch: 500; loss: 1.22; acc: 0.67
Batch: 520; loss: 1.28; acc: 0.56
Batch: 540; loss: 1.07; acc: 0.62
Batch: 560; loss: 1.42; acc: 0.56
Batch: 580; loss: 1.12; acc: 0.62
Batch: 600; loss: 1.14; acc: 0.61
Batch: 620; loss: 1.09; acc: 0.73
Batch: 640; loss: 1.1; acc: 0.72
Batch: 660; loss: 1.01; acc: 0.73
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 1.23; acc: 0.59
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 1.04; acc: 0.73
Batch: 760; loss: 1.29; acc: 0.59
Batch: 780; loss: 1.19; acc: 0.67
Train Epoch over. train_loss: 1.12; train_accuracy: 0.68 

0.0001365794742014259
0.00013021632912568748
Batch: 0; loss: 0.9; acc: 0.81
Batch: 20; loss: 1.23; acc: 0.67
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.73
Batch: 80; loss: 0.96; acc: 0.7
Batch: 100; loss: 1.05; acc: 0.8
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.78
Val Epoch over. val_loss: 1.0670996228600764; val_accuracy: 0.7179538216560509 

The current subspace-distance is: 0.00013021632912568748 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.1; acc: 0.67
Batch: 40; loss: 1.02; acc: 0.72
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 1.02; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.92; acc: 0.8
Batch: 160; loss: 1.1; acc: 0.67
Batch: 180; loss: 1.19; acc: 0.67
Batch: 200; loss: 1.07; acc: 0.7
Batch: 220; loss: 1.36; acc: 0.56
Batch: 240; loss: 1.19; acc: 0.69
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 1.12; acc: 0.72
Batch: 300; loss: 1.16; acc: 0.67
Batch: 320; loss: 0.92; acc: 0.81
Batch: 340; loss: 0.88; acc: 0.84
Batch: 360; loss: 1.07; acc: 0.72
Batch: 380; loss: 1.19; acc: 0.72
Batch: 400; loss: 1.04; acc: 0.73
Batch: 420; loss: 1.14; acc: 0.64
Batch: 440; loss: 1.11; acc: 0.69
Batch: 460; loss: 1.16; acc: 0.58
Batch: 480; loss: 1.02; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.64
Batch: 520; loss: 1.07; acc: 0.72
Batch: 540; loss: 1.3; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 1.21; acc: 0.64
Batch: 600; loss: 1.02; acc: 0.73
Batch: 620; loss: 1.02; acc: 0.8
Batch: 640; loss: 1.15; acc: 0.7
Batch: 660; loss: 1.07; acc: 0.7
Batch: 680; loss: 1.25; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.69
Batch: 720; loss: 1.0; acc: 0.77
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 0.95; acc: 0.78
Batch: 780; loss: 1.36; acc: 0.59
Train Epoch over. train_loss: 1.12; train_accuracy: 0.69 

0.0001391804253216833
0.00013289811613503844
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.95; acc: 0.69
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 1.15; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.78
Val Epoch over. val_loss: 1.0550237066426855; val_accuracy: 0.7057125796178344 

The current subspace-distance is: 0.00013289811613503844 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.75
Batch: 20; loss: 0.99; acc: 0.75
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.13; acc: 0.7
Batch: 80; loss: 1.08; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.07; acc: 0.66
Batch: 140; loss: 1.18; acc: 0.64
Batch: 160; loss: 1.25; acc: 0.58
Batch: 180; loss: 1.16; acc: 0.64
Batch: 200; loss: 1.04; acc: 0.73
Batch: 220; loss: 1.19; acc: 0.7
Batch: 240; loss: 1.08; acc: 0.62
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.15; acc: 0.7
Batch: 300; loss: 1.13; acc: 0.7
Batch: 320; loss: 1.1; acc: 0.69
Batch: 340; loss: 1.14; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.7
Batch: 380; loss: 1.16; acc: 0.66
Batch: 400; loss: 1.12; acc: 0.61
Batch: 420; loss: 1.13; acc: 0.72
Batch: 440; loss: 1.12; acc: 0.67
Batch: 460; loss: 1.33; acc: 0.58
Batch: 480; loss: 1.16; acc: 0.58
Batch: 500; loss: 1.29; acc: 0.62
Batch: 520; loss: 1.04; acc: 0.78
Batch: 540; loss: 1.2; acc: 0.7
Batch: 560; loss: 1.19; acc: 0.67
Batch: 580; loss: 1.14; acc: 0.64
Batch: 600; loss: 1.08; acc: 0.75
Batch: 620; loss: 1.13; acc: 0.72
Batch: 640; loss: 1.13; acc: 0.7
Batch: 660; loss: 0.99; acc: 0.81
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.05; acc: 0.73
Batch: 720; loss: 1.2; acc: 0.64
Batch: 740; loss: 1.11; acc: 0.69
Batch: 760; loss: 0.98; acc: 0.73
Batch: 780; loss: 1.37; acc: 0.52
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00013883163046557456
0.00013206302537582815
Batch: 0; loss: 0.92; acc: 0.77
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.98; acc: 0.72
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.16; acc: 0.64
Batch: 140; loss: 0.91; acc: 0.81
Val Epoch over. val_loss: 1.0567464938588962; val_accuracy: 0.7172571656050956 

The current subspace-distance is: 0.00013206302537582815 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.72
Batch: 20; loss: 1.1; acc: 0.69
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 1.05; acc: 0.69
Batch: 100; loss: 0.93; acc: 0.78
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.69
Batch: 160; loss: 1.1; acc: 0.73
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.0; acc: 0.78
Batch: 220; loss: 1.24; acc: 0.56
Batch: 240; loss: 1.03; acc: 0.7
Batch: 260; loss: 1.09; acc: 0.69
Batch: 280; loss: 1.34; acc: 0.61
Batch: 300; loss: 1.0; acc: 0.75
Batch: 320; loss: 1.18; acc: 0.61
Batch: 340; loss: 0.95; acc: 0.78
Batch: 360; loss: 1.0; acc: 0.7
Batch: 380; loss: 1.03; acc: 0.7
Batch: 400; loss: 1.09; acc: 0.75
Batch: 420; loss: 1.23; acc: 0.64
Batch: 440; loss: 1.13; acc: 0.67
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.09; acc: 0.66
Batch: 500; loss: 1.09; acc: 0.66
Batch: 520; loss: 1.27; acc: 0.62
Batch: 540; loss: 1.1; acc: 0.69
Batch: 560; loss: 1.02; acc: 0.77
Batch: 580; loss: 1.2; acc: 0.66
Batch: 600; loss: 1.12; acc: 0.67
Batch: 620; loss: 1.2; acc: 0.62
Batch: 640; loss: 1.24; acc: 0.61
Batch: 660; loss: 1.3; acc: 0.62
Batch: 680; loss: 1.11; acc: 0.64
Batch: 700; loss: 1.1; acc: 0.69
Batch: 720; loss: 1.18; acc: 0.66
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.15; acc: 0.67
Batch: 780; loss: 1.24; acc: 0.67
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00014089434989728034
0.00013414831482805312
Batch: 0; loss: 0.91; acc: 0.77
Batch: 20; loss: 1.22; acc: 0.67
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.7
Batch: 80; loss: 0.96; acc: 0.72
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 1.15; acc: 0.64
Batch: 140; loss: 0.91; acc: 0.81
Val Epoch over. val_loss: 1.0597376925930095; val_accuracy: 0.7182523885350318 

The current subspace-distance is: 0.00013414831482805312 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.06; acc: 0.66
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 1.25; acc: 0.64
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 1.1; acc: 0.64
Batch: 140; loss: 1.17; acc: 0.72
Batch: 160; loss: 1.06; acc: 0.69
Batch: 180; loss: 1.2; acc: 0.69
Batch: 200; loss: 1.03; acc: 0.69
Batch: 220; loss: 0.9; acc: 0.84
Batch: 240; loss: 1.02; acc: 0.72
Batch: 260; loss: 1.11; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.66
Batch: 300; loss: 1.16; acc: 0.66
Batch: 320; loss: 0.99; acc: 0.77
Batch: 340; loss: 1.06; acc: 0.72
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.16; acc: 0.69
Batch: 400; loss: 1.02; acc: 0.72
Batch: 420; loss: 1.03; acc: 0.73
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 1.04; acc: 0.67
Batch: 480; loss: 1.0; acc: 0.75
Batch: 500; loss: 1.27; acc: 0.59
Batch: 520; loss: 1.03; acc: 0.75
Batch: 540; loss: 1.13; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.78
Batch: 600; loss: 1.11; acc: 0.75
Batch: 620; loss: 1.27; acc: 0.62
Batch: 640; loss: 1.06; acc: 0.73
Batch: 660; loss: 1.02; acc: 0.69
Batch: 680; loss: 1.15; acc: 0.62
Batch: 700; loss: 0.95; acc: 0.73
Batch: 720; loss: 1.21; acc: 0.59
Batch: 740; loss: 1.06; acc: 0.7
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 1.16; acc: 0.62
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00013896434393245727
0.00013256201054900885
Batch: 0; loss: 0.89; acc: 0.78
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.73
Batch: 100; loss: 1.03; acc: 0.78
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.0505750103361289; val_accuracy: 0.7273089171974523 

The current subspace-distance is: 0.00013256201054900885 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.21; acc: 0.59
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 1.03; acc: 0.73
Batch: 60; loss: 1.22; acc: 0.62
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 1.15; acc: 0.7
Batch: 160; loss: 0.96; acc: 0.7
Batch: 180; loss: 1.19; acc: 0.64
Batch: 200; loss: 1.23; acc: 0.62
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.15; acc: 0.7
Batch: 280; loss: 1.16; acc: 0.62
Batch: 300; loss: 1.24; acc: 0.59
Batch: 320; loss: 1.12; acc: 0.7
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.2; acc: 0.66
Batch: 380; loss: 1.0; acc: 0.78
Batch: 400; loss: 1.01; acc: 0.7
Batch: 420; loss: 1.23; acc: 0.59
Batch: 440; loss: 1.09; acc: 0.7
Batch: 460; loss: 1.16; acc: 0.66
Batch: 480; loss: 1.09; acc: 0.72
Batch: 500; loss: 0.95; acc: 0.72
Batch: 520; loss: 1.17; acc: 0.61
Batch: 540; loss: 0.99; acc: 0.75
Batch: 560; loss: 1.24; acc: 0.58
Batch: 580; loss: 1.04; acc: 0.66
Batch: 600; loss: 1.34; acc: 0.55
Batch: 620; loss: 1.01; acc: 0.7
Batch: 640; loss: 1.22; acc: 0.58
Batch: 660; loss: 1.15; acc: 0.61
Batch: 680; loss: 1.06; acc: 0.73
Batch: 700; loss: 1.18; acc: 0.58
Batch: 720; loss: 1.23; acc: 0.62
Batch: 740; loss: 1.0; acc: 0.81
Batch: 760; loss: 1.18; acc: 0.67
Batch: 780; loss: 1.25; acc: 0.58
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00014004057447891682
0.00013329292414709926
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.73
Batch: 100; loss: 1.04; acc: 0.78
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 0.91; acc: 0.77
Val Epoch over. val_loss: 1.0388240385207401; val_accuracy: 0.7266122611464968 

The current subspace-distance is: 0.00013329292414709926 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.72
Batch: 20; loss: 1.08; acc: 0.7
Batch: 40; loss: 1.13; acc: 0.64
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 0.94; acc: 0.83
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.61
Batch: 160; loss: 0.99; acc: 0.73
Batch: 180; loss: 1.23; acc: 0.69
Batch: 200; loss: 1.16; acc: 0.72
Batch: 220; loss: 1.21; acc: 0.59
Batch: 240; loss: 1.0; acc: 0.77
Batch: 260; loss: 1.2; acc: 0.67
Batch: 280; loss: 1.07; acc: 0.67
Batch: 300; loss: 1.23; acc: 0.59
Batch: 320; loss: 0.97; acc: 0.72
Batch: 340; loss: 1.01; acc: 0.78
Batch: 360; loss: 1.06; acc: 0.72
Batch: 380; loss: 1.09; acc: 0.72
Batch: 400; loss: 0.98; acc: 0.83
Batch: 420; loss: 1.08; acc: 0.73
Batch: 440; loss: 1.22; acc: 0.61
Batch: 460; loss: 0.88; acc: 0.8
Batch: 480; loss: 1.07; acc: 0.81
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 1.04; acc: 0.72
Batch: 540; loss: 1.1; acc: 0.66
Batch: 560; loss: 1.05; acc: 0.73
Batch: 580; loss: 1.26; acc: 0.66
Batch: 600; loss: 1.2; acc: 0.62
Batch: 620; loss: 1.14; acc: 0.72
Batch: 640; loss: 1.09; acc: 0.67
Batch: 660; loss: 1.22; acc: 0.53
Batch: 680; loss: 1.25; acc: 0.59
Batch: 700; loss: 1.07; acc: 0.72
Batch: 720; loss: 1.13; acc: 0.69
Batch: 740; loss: 1.02; acc: 0.73
Batch: 760; loss: 1.13; acc: 0.69
Batch: 780; loss: 1.23; acc: 0.59
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00014030036982148886
0.00013357479474507272
Batch: 0; loss: 0.91; acc: 0.78
Batch: 20; loss: 1.22; acc: 0.66
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.96; acc: 0.7
Batch: 100; loss: 1.01; acc: 0.8
Batch: 120; loss: 1.15; acc: 0.66
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.0485124663942178; val_accuracy: 0.7188495222929936 

The current subspace-distance is: 0.00013357479474507272 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.72
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 1.02; acc: 0.8
Batch: 60; loss: 1.12; acc: 0.64
Batch: 80; loss: 1.2; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.05; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.73
Batch: 160; loss: 1.15; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.72
Batch: 200; loss: 1.17; acc: 0.72
Batch: 220; loss: 1.57; acc: 0.52
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.04; acc: 0.75
Batch: 280; loss: 1.13; acc: 0.69
Batch: 300; loss: 1.2; acc: 0.59
Batch: 320; loss: 1.22; acc: 0.67
Batch: 340; loss: 0.91; acc: 0.8
Batch: 360; loss: 1.03; acc: 0.73
Batch: 380; loss: 1.08; acc: 0.69
Batch: 400; loss: 1.12; acc: 0.7
Batch: 420; loss: 1.09; acc: 0.73
Batch: 440; loss: 1.25; acc: 0.69
Batch: 460; loss: 0.95; acc: 0.75
Batch: 480; loss: 1.01; acc: 0.73
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.2; acc: 0.67
Batch: 540; loss: 0.91; acc: 0.8
Batch: 560; loss: 1.27; acc: 0.55
Batch: 580; loss: 1.09; acc: 0.72
Batch: 600; loss: 1.05; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.72
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.12; acc: 0.66
Batch: 680; loss: 1.17; acc: 0.69
Batch: 700; loss: 1.08; acc: 0.66
Batch: 720; loss: 1.0; acc: 0.77
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 0.99; acc: 0.7
Batch: 780; loss: 1.08; acc: 0.73
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00014190016372594982
0.00013397923612501472
Batch: 0; loss: 0.9; acc: 0.78
Batch: 20; loss: 1.2; acc: 0.67
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 1.03; acc: 0.73
Batch: 80; loss: 0.95; acc: 0.69
Batch: 100; loss: 1.02; acc: 0.8
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 0.91; acc: 0.8
Val Epoch over. val_loss: 1.0428485866564854; val_accuracy: 0.7210390127388535 

The current subspace-distance is: 0.00013397923612501472 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.25; acc: 0.66
Batch: 40; loss: 0.94; acc: 0.77
Batch: 60; loss: 1.29; acc: 0.53
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.11; acc: 0.61
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.67
Batch: 160; loss: 0.95; acc: 0.77
Batch: 180; loss: 1.02; acc: 0.72
Batch: 200; loss: 0.99; acc: 0.73
Batch: 220; loss: 1.09; acc: 0.7
Batch: 240; loss: 1.06; acc: 0.73
Batch: 260; loss: 0.97; acc: 0.73
Batch: 280; loss: 1.07; acc: 0.73
Batch: 300; loss: 1.39; acc: 0.59
Batch: 320; loss: 1.13; acc: 0.64
Batch: 340; loss: 1.07; acc: 0.69
Batch: 360; loss: 1.02; acc: 0.75
Batch: 380; loss: 1.11; acc: 0.66
Batch: 400; loss: 1.14; acc: 0.69
Batch: 420; loss: 1.03; acc: 0.72
Batch: 440; loss: 1.13; acc: 0.64
Batch: 460; loss: 1.04; acc: 0.69
Batch: 480; loss: 1.06; acc: 0.72
Batch: 500; loss: 1.12; acc: 0.7
Batch: 520; loss: 0.91; acc: 0.78
Batch: 540; loss: 1.04; acc: 0.73
Batch: 560; loss: 1.07; acc: 0.69
Batch: 580; loss: 1.12; acc: 0.69
Batch: 600; loss: 1.28; acc: 0.64
Batch: 620; loss: 1.18; acc: 0.67
Batch: 640; loss: 1.23; acc: 0.62
Batch: 660; loss: 1.08; acc: 0.73
Batch: 680; loss: 1.02; acc: 0.73
Batch: 700; loss: 1.16; acc: 0.61
Batch: 720; loss: 0.98; acc: 0.67
Batch: 740; loss: 1.06; acc: 0.73
Batch: 760; loss: 1.17; acc: 0.69
Batch: 780; loss: 1.1; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00014213733084034175
0.00013499600754585117
Batch: 0; loss: 0.88; acc: 0.77
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 1.05; acc: 0.73
Batch: 80; loss: 0.93; acc: 0.7
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 0.91; acc: 0.8
Val Epoch over. val_loss: 1.0383109905917174; val_accuracy: 0.7239251592356688 

The current subspace-distance is: 0.00013499600754585117 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.69
Batch: 20; loss: 1.3; acc: 0.56
Batch: 40; loss: 1.19; acc: 0.59
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.61
Batch: 100; loss: 1.08; acc: 0.7
Batch: 120; loss: 0.99; acc: 0.66
Batch: 140; loss: 1.21; acc: 0.62
Batch: 160; loss: 1.17; acc: 0.67
Batch: 180; loss: 1.08; acc: 0.66
Batch: 200; loss: 1.11; acc: 0.7
Batch: 220; loss: 1.16; acc: 0.7
Batch: 240; loss: 0.98; acc: 0.69
Batch: 260; loss: 1.27; acc: 0.55
Batch: 280; loss: 1.13; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.62
Batch: 320; loss: 1.0; acc: 0.72
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.14; acc: 0.66
Batch: 380; loss: 1.16; acc: 0.67
Batch: 400; loss: 1.15; acc: 0.67
Batch: 420; loss: 0.99; acc: 0.83
Batch: 440; loss: 1.07; acc: 0.69
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 1.0; acc: 0.77
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 1.01; acc: 0.75
Batch: 540; loss: 1.22; acc: 0.58
Batch: 560; loss: 1.3; acc: 0.58
Batch: 580; loss: 1.03; acc: 0.8
Batch: 600; loss: 1.07; acc: 0.8
Batch: 620; loss: 1.22; acc: 0.66
Batch: 640; loss: 0.89; acc: 0.77
Batch: 660; loss: 1.19; acc: 0.62
Batch: 680; loss: 1.06; acc: 0.67
Batch: 700; loss: 1.21; acc: 0.7
Batch: 720; loss: 1.11; acc: 0.73
Batch: 740; loss: 1.18; acc: 0.69
Batch: 760; loss: 0.98; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.61
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00014204981562215835
0.00013532119919545949
Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 1.04; acc: 0.72
Batch: 80; loss: 0.95; acc: 0.7
Batch: 100; loss: 1.02; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.91; acc: 0.83
Val Epoch over. val_loss: 1.038807060308517; val_accuracy: 0.7195461783439491 

The current subspace-distance is: 0.00013532119919545949 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 0.98; acc: 0.72
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 1.21; acc: 0.66
Batch: 100; loss: 1.12; acc: 0.62
Batch: 120; loss: 1.13; acc: 0.7
Batch: 140; loss: 1.08; acc: 0.67
Batch: 160; loss: 1.01; acc: 0.73
Batch: 180; loss: 1.07; acc: 0.69
Batch: 200; loss: 0.95; acc: 0.77
Batch: 220; loss: 1.09; acc: 0.66
Batch: 240; loss: 1.14; acc: 0.75
Batch: 260; loss: 1.01; acc: 0.75
Batch: 280; loss: 0.97; acc: 0.83
Batch: 300; loss: 1.04; acc: 0.7
Batch: 320; loss: 0.97; acc: 0.77
Batch: 340; loss: 1.21; acc: 0.64
Batch: 360; loss: 1.16; acc: 0.7
Batch: 380; loss: 1.08; acc: 0.72
Batch: 400; loss: 1.09; acc: 0.7
Batch: 420; loss: 1.26; acc: 0.61
Batch: 440; loss: 1.04; acc: 0.72
Batch: 460; loss: 1.22; acc: 0.64
Batch: 480; loss: 1.17; acc: 0.66
Batch: 500; loss: 1.34; acc: 0.5
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.17; acc: 0.66
Batch: 560; loss: 1.2; acc: 0.62
Batch: 580; loss: 1.08; acc: 0.72
Batch: 600; loss: 1.13; acc: 0.69
Batch: 620; loss: 1.11; acc: 0.72
Batch: 640; loss: 1.0; acc: 0.73
Batch: 660; loss: 1.03; acc: 0.67
Batch: 680; loss: 1.19; acc: 0.66
Batch: 700; loss: 1.03; acc: 0.73
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.3; acc: 0.66
Batch: 760; loss: 1.06; acc: 0.72
Batch: 780; loss: 1.31; acc: 0.53
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00014138272672425956
0.00013564345135819167
Batch: 0; loss: 0.87; acc: 0.8
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.73
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 0.9; acc: 0.8
Val Epoch over. val_loss: 1.032492517665693; val_accuracy: 0.7254179936305732 

The current subspace-distance is: 0.00013564345135819167 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 1.21; acc: 0.62
Batch: 40; loss: 1.1; acc: 0.66
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 1.16; acc: 0.66
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.07; acc: 0.66
Batch: 140; loss: 1.03; acc: 0.72
Batch: 160; loss: 1.06; acc: 0.72
Batch: 180; loss: 1.04; acc: 0.72
Batch: 200; loss: 1.12; acc: 0.59
Batch: 220; loss: 1.03; acc: 0.75
Batch: 240; loss: 1.02; acc: 0.73
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 1.05; acc: 0.69
Batch: 300; loss: 1.21; acc: 0.61
Batch: 320; loss: 1.16; acc: 0.66
Batch: 340; loss: 1.14; acc: 0.64
Batch: 360; loss: 1.06; acc: 0.77
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.05; acc: 0.73
Batch: 420; loss: 1.11; acc: 0.72
Batch: 440; loss: 1.29; acc: 0.61
Batch: 460; loss: 1.11; acc: 0.61
Batch: 480; loss: 1.03; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.66
Batch: 520; loss: 1.14; acc: 0.67
Batch: 540; loss: 1.01; acc: 0.77
Batch: 560; loss: 1.2; acc: 0.64
Batch: 580; loss: 0.95; acc: 0.75
Batch: 600; loss: 1.26; acc: 0.59
Batch: 620; loss: 0.98; acc: 0.78
Batch: 640; loss: 1.13; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.78
Batch: 680; loss: 1.12; acc: 0.67
Batch: 700; loss: 1.12; acc: 0.69
Batch: 720; loss: 1.26; acc: 0.64
Batch: 740; loss: 1.0; acc: 0.67
Batch: 760; loss: 1.07; acc: 0.67
Batch: 780; loss: 1.35; acc: 0.61
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00014207506319507957
0.00013645520084537566
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 1.22; acc: 0.66
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 1.03; acc: 0.78
Batch: 120; loss: 1.13; acc: 0.66
Batch: 140; loss: 0.92; acc: 0.8
Val Epoch over. val_loss: 1.041962225346049; val_accuracy: 0.7230294585987261 

The current subspace-distance is: 0.00013645520084537566 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_1_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 13.5

The number of parameters is: 279504

The number of individual parameters is:

108
432
108
108
162
52488
162
162
324
157464
324
324
64
62208
64
64
4096
64
640
10
64
64

nonzero elements in E: 111801589
elements in E: 111801600
fraction nonzero: 0.9999999016114259
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.12
Batch: 20; loss: 2.02; acc: 0.38
Batch: 40; loss: 1.99; acc: 0.36
Batch: 60; loss: 1.81; acc: 0.41
Batch: 80; loss: 1.74; acc: 0.47
Batch: 100; loss: 1.72; acc: 0.47
Batch: 120; loss: 1.71; acc: 0.47
Batch: 140; loss: 1.7; acc: 0.44
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.52; acc: 0.67
Batch: 200; loss: 1.81; acc: 0.41
Batch: 220; loss: 1.5; acc: 0.61
Batch: 240; loss: 1.65; acc: 0.56
Batch: 260; loss: 1.6; acc: 0.53
Batch: 280; loss: 1.73; acc: 0.45
Batch: 300; loss: 1.59; acc: 0.5
Batch: 320; loss: 1.51; acc: 0.58
Batch: 340; loss: 1.69; acc: 0.5
Batch: 360; loss: 1.56; acc: 0.5
Batch: 380; loss: 1.51; acc: 0.61
Batch: 400; loss: 1.49; acc: 0.58
Batch: 420; loss: 1.52; acc: 0.58
Batch: 440; loss: 1.44; acc: 0.58
Batch: 460; loss: 1.5; acc: 0.62
Batch: 480; loss: 1.53; acc: 0.59
Batch: 500; loss: 1.51; acc: 0.61
Batch: 520; loss: 1.33; acc: 0.67
Batch: 540; loss: 1.46; acc: 0.64
Batch: 560; loss: 1.42; acc: 0.56
Batch: 580; loss: 1.45; acc: 0.61
Batch: 600; loss: 1.38; acc: 0.59
Batch: 620; loss: 1.44; acc: 0.59
Batch: 640; loss: 1.37; acc: 0.69
Batch: 660; loss: 1.25; acc: 0.66
Batch: 680; loss: 1.46; acc: 0.56
Batch: 700; loss: 1.52; acc: 0.5
Batch: 720; loss: 1.41; acc: 0.62
Batch: 740; loss: 1.55; acc: 0.53
Batch: 760; loss: 1.35; acc: 0.69
Batch: 780; loss: 1.47; acc: 0.58
Train Epoch over. train_loss: 1.6; train_accuracy: 0.53 

2.9133416319382377e-05
6.5720728343876544e-06
Batch: 0; loss: 1.45; acc: 0.59
Batch: 20; loss: 1.68; acc: 0.47
Batch: 40; loss: 1.26; acc: 0.7
Batch: 60; loss: 1.31; acc: 0.64
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.58
Batch: 120; loss: 1.5; acc: 0.53
Batch: 140; loss: 1.35; acc: 0.73
Val Epoch over. val_loss: 1.5221838852402512; val_accuracy: 0.5494625796178344 

The current subspace-distance is: 6.5720728343876544e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.39; acc: 0.66
Batch: 20; loss: 1.41; acc: 0.59
Batch: 40; loss: 1.37; acc: 0.62
Batch: 60; loss: 1.24; acc: 0.7
Batch: 80; loss: 1.31; acc: 0.69
Batch: 100; loss: 1.41; acc: 0.59
Batch: 120; loss: 1.29; acc: 0.69
Batch: 140; loss: 1.48; acc: 0.59
Batch: 160; loss: 1.26; acc: 0.61
Batch: 180; loss: 1.54; acc: 0.52
Batch: 200; loss: 1.43; acc: 0.55
Batch: 220; loss: 1.25; acc: 0.67
Batch: 240; loss: 1.48; acc: 0.53
Batch: 260; loss: 1.51; acc: 0.52
Batch: 280; loss: 1.34; acc: 0.62
Batch: 300; loss: 1.44; acc: 0.62
Batch: 320; loss: 1.64; acc: 0.53
Batch: 340; loss: 1.41; acc: 0.59
Batch: 360; loss: 1.33; acc: 0.67
Batch: 380; loss: 1.23; acc: 0.64
Batch: 400; loss: 1.17; acc: 0.73
Batch: 420; loss: 1.22; acc: 0.72
Batch: 440; loss: 1.22; acc: 0.64
Batch: 460; loss: 1.47; acc: 0.58
Batch: 480; loss: 1.29; acc: 0.64
Batch: 500; loss: 1.3; acc: 0.66
Batch: 520; loss: 1.22; acc: 0.66
Batch: 540; loss: 1.27; acc: 0.64
Batch: 560; loss: 1.23; acc: 0.66
Batch: 580; loss: 1.31; acc: 0.64
Batch: 600; loss: 1.16; acc: 0.69
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.2; acc: 0.67
Batch: 660; loss: 1.32; acc: 0.59
Batch: 680; loss: 1.3; acc: 0.62
Batch: 700; loss: 1.37; acc: 0.56
Batch: 720; loss: 1.17; acc: 0.73
Batch: 740; loss: 1.2; acc: 0.67
Batch: 760; loss: 1.24; acc: 0.69
Batch: 780; loss: 1.38; acc: 0.58
Train Epoch over. train_loss: 1.33; train_accuracy: 0.63 

3.1869407393969595e-05
8.456511750409845e-06
Batch: 0; loss: 1.24; acc: 0.5
Batch: 20; loss: 1.93; acc: 0.36
Batch: 40; loss: 1.41; acc: 0.55
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.34; acc: 0.48
Batch: 100; loss: 1.48; acc: 0.53
Batch: 120; loss: 1.56; acc: 0.44
Batch: 140; loss: 1.45; acc: 0.52
Val Epoch over. val_loss: 1.5775222687204933; val_accuracy: 0.47929936305732485 

The current subspace-distance is: 8.456511750409845e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.72
Batch: 20; loss: 1.34; acc: 0.61
Batch: 40; loss: 1.36; acc: 0.66
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.23; acc: 0.7
Batch: 100; loss: 1.26; acc: 0.62
Batch: 120; loss: 1.15; acc: 0.73
Batch: 140; loss: 1.4; acc: 0.59
Batch: 160; loss: 1.14; acc: 0.66
Batch: 180; loss: 1.32; acc: 0.58
Batch: 200; loss: 1.28; acc: 0.69
Batch: 220; loss: 1.1; acc: 0.7
Batch: 240; loss: 1.37; acc: 0.58
Batch: 260; loss: 1.18; acc: 0.69
Batch: 280; loss: 1.36; acc: 0.58
Batch: 300; loss: 1.19; acc: 0.7
Batch: 320; loss: 1.25; acc: 0.62
Batch: 340; loss: 1.13; acc: 0.77
Batch: 360; loss: 1.06; acc: 0.77
Batch: 380; loss: 1.04; acc: 0.8
Batch: 400; loss: 1.24; acc: 0.64
Batch: 420; loss: 1.04; acc: 0.73
Batch: 440; loss: 1.19; acc: 0.64
Batch: 460; loss: 1.42; acc: 0.64
Batch: 480; loss: 1.12; acc: 0.69
Batch: 500; loss: 1.1; acc: 0.72
Batch: 520; loss: 1.29; acc: 0.64
Batch: 540; loss: 1.21; acc: 0.69
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.23; acc: 0.66
Batch: 600; loss: 1.3; acc: 0.61
Batch: 620; loss: 1.59; acc: 0.48
Batch: 640; loss: 1.12; acc: 0.72
Batch: 660; loss: 1.13; acc: 0.69
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 1.04; acc: 0.7
Batch: 720; loss: 1.12; acc: 0.7
Batch: 740; loss: 1.16; acc: 0.73
Batch: 760; loss: 1.3; acc: 0.59
Batch: 780; loss: 1.04; acc: 0.73
Train Epoch over. train_loss: 1.23; train_accuracy: 0.65 

3.4103642974514514e-05
1.0459677469043527e-05
Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 1.66; acc: 0.45
Batch: 40; loss: 1.09; acc: 0.72
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.3; acc: 0.66
Batch: 120; loss: 1.42; acc: 0.53
Batch: 140; loss: 1.21; acc: 0.7
Val Epoch over. val_loss: 1.3212881828569303; val_accuracy: 0.5982285031847133 

The current subspace-distance is: 1.0459677469043527e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 1.15; acc: 0.75
Batch: 100; loss: 1.09; acc: 0.77
Batch: 120; loss: 1.16; acc: 0.66
Batch: 140; loss: 1.33; acc: 0.61
Batch: 160; loss: 1.12; acc: 0.69
Batch: 180; loss: 1.07; acc: 0.73
Batch: 200; loss: 1.21; acc: 0.62
Batch: 220; loss: 1.25; acc: 0.61
Batch: 240; loss: 1.26; acc: 0.64
Batch: 260; loss: 1.08; acc: 0.75
Batch: 280; loss: 1.15; acc: 0.64
Batch: 300; loss: 1.31; acc: 0.61
Batch: 320; loss: 1.09; acc: 0.62
Batch: 340; loss: 1.14; acc: 0.72
Batch: 360; loss: 1.0; acc: 0.83
Batch: 380; loss: 1.14; acc: 0.73
Batch: 400; loss: 0.97; acc: 0.75
Batch: 420; loss: 1.36; acc: 0.62
Batch: 440; loss: 1.13; acc: 0.7
Batch: 460; loss: 1.04; acc: 0.67
Batch: 480; loss: 1.14; acc: 0.7
Batch: 500; loss: 1.38; acc: 0.59
Batch: 520; loss: 1.03; acc: 0.81
Batch: 540; loss: 1.09; acc: 0.75
Batch: 560; loss: 1.32; acc: 0.59
Batch: 580; loss: 1.02; acc: 0.78
Batch: 600; loss: 1.07; acc: 0.73
Batch: 620; loss: 1.09; acc: 0.77
Batch: 640; loss: 1.3; acc: 0.59
Batch: 660; loss: 1.37; acc: 0.66
Batch: 680; loss: 1.1; acc: 0.72
Batch: 700; loss: 1.26; acc: 0.64
Batch: 720; loss: 1.11; acc: 0.77
Batch: 740; loss: 1.16; acc: 0.67
Batch: 760; loss: 1.24; acc: 0.67
Batch: 780; loss: 1.33; acc: 0.62
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

3.580816337489523e-05
1.1216372513445094e-05
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.49; acc: 0.52
Batch: 40; loss: 1.08; acc: 0.69
Batch: 60; loss: 1.05; acc: 0.73
Batch: 80; loss: 1.2; acc: 0.67
Batch: 100; loss: 1.14; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.59
Batch: 140; loss: 1.24; acc: 0.67
Val Epoch over. val_loss: 1.2694333017252053; val_accuracy: 0.6226114649681529 

The current subspace-distance is: 1.1216372513445094e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.09; acc: 0.66
Batch: 40; loss: 1.1; acc: 0.72
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.67
Batch: 100; loss: 0.99; acc: 0.8
Batch: 120; loss: 1.21; acc: 0.59
Batch: 140; loss: 1.29; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.56
Batch: 180; loss: 1.05; acc: 0.72
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 1.1; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.75
Batch: 260; loss: 1.15; acc: 0.69
Batch: 280; loss: 1.11; acc: 0.73
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.07; acc: 0.78
Batch: 340; loss: 1.17; acc: 0.67
Batch: 360; loss: 1.23; acc: 0.62
Batch: 380; loss: 1.0; acc: 0.75
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.36; acc: 0.56
Batch: 440; loss: 1.13; acc: 0.64
Batch: 460; loss: 1.16; acc: 0.62
Batch: 480; loss: 1.27; acc: 0.62
Batch: 500; loss: 1.08; acc: 0.75
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 1.12; acc: 0.75
Batch: 560; loss: 1.11; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.61
Batch: 600; loss: 1.22; acc: 0.7
Batch: 620; loss: 0.96; acc: 0.77
Batch: 640; loss: 1.3; acc: 0.62
Batch: 660; loss: 1.15; acc: 0.62
Batch: 680; loss: 1.3; acc: 0.67
Batch: 700; loss: 1.05; acc: 0.75
Batch: 720; loss: 1.29; acc: 0.61
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 1.25; acc: 0.61
Batch: 780; loss: 1.08; acc: 0.67
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

3.7622539821313694e-05
1.2177555618109182e-05
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 1.46; acc: 0.52
Batch: 40; loss: 0.98; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.73
Batch: 80; loss: 1.16; acc: 0.59
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.37; acc: 0.53
Batch: 140; loss: 0.98; acc: 0.86
Val Epoch over. val_loss: 1.172810782292846; val_accuracy: 0.6732683121019108 

The current subspace-distance is: 1.2177555618109182e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.23; acc: 0.64
Batch: 40; loss: 1.29; acc: 0.59
Batch: 60; loss: 1.07; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.75
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 1.25; acc: 0.58
Batch: 160; loss: 1.12; acc: 0.66
Batch: 180; loss: 1.01; acc: 0.72
Batch: 200; loss: 1.21; acc: 0.64
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 1.19; acc: 0.72
Batch: 260; loss: 1.16; acc: 0.67
Batch: 280; loss: 1.29; acc: 0.56
Batch: 300; loss: 0.98; acc: 0.75
Batch: 320; loss: 1.09; acc: 0.66
Batch: 340; loss: 1.13; acc: 0.64
Batch: 360; loss: 1.19; acc: 0.59
Batch: 380; loss: 1.28; acc: 0.56
Batch: 400; loss: 1.08; acc: 0.66
Batch: 420; loss: 1.17; acc: 0.7
Batch: 440; loss: 1.0; acc: 0.69
Batch: 460; loss: 1.03; acc: 0.75
Batch: 480; loss: 1.13; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.66
Batch: 520; loss: 1.12; acc: 0.75
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 1.11; acc: 0.72
Batch: 580; loss: 1.04; acc: 0.72
Batch: 600; loss: 1.05; acc: 0.7
Batch: 620; loss: 1.13; acc: 0.69
Batch: 640; loss: 0.99; acc: 0.67
Batch: 660; loss: 1.04; acc: 0.72
Batch: 680; loss: 1.15; acc: 0.67
Batch: 700; loss: 1.05; acc: 0.69
Batch: 720; loss: 1.24; acc: 0.61
Batch: 740; loss: 1.11; acc: 0.69
Batch: 760; loss: 1.11; acc: 0.69
Batch: 780; loss: 1.33; acc: 0.58
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

3.88683911296539e-05
1.2837917893193662e-05
Batch: 0; loss: 1.04; acc: 0.67
Batch: 20; loss: 1.3; acc: 0.59
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 0.94; acc: 0.78
Batch: 80; loss: 1.17; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 1.1481781305780836; val_accuracy: 0.6668988853503185 

The current subspace-distance is: 1.2837917893193662e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.7
Batch: 40; loss: 0.99; acc: 0.67
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 1.11; acc: 0.67
Batch: 100; loss: 0.98; acc: 0.78
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 0.94; acc: 0.77
Batch: 160; loss: 1.15; acc: 0.7
Batch: 180; loss: 1.23; acc: 0.62
Batch: 200; loss: 1.02; acc: 0.73
Batch: 220; loss: 1.17; acc: 0.62
Batch: 240; loss: 1.06; acc: 0.73
Batch: 260; loss: 0.99; acc: 0.8
Batch: 280; loss: 1.14; acc: 0.64
Batch: 300; loss: 1.12; acc: 0.7
Batch: 320; loss: 1.08; acc: 0.72
Batch: 340; loss: 1.16; acc: 0.62
Batch: 360; loss: 1.25; acc: 0.67
Batch: 380; loss: 1.22; acc: 0.62
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 0.96; acc: 0.83
Batch: 440; loss: 1.23; acc: 0.62
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 1.17; acc: 0.66
Batch: 500; loss: 1.17; acc: 0.67
Batch: 520; loss: 0.94; acc: 0.78
Batch: 540; loss: 0.88; acc: 0.81
Batch: 560; loss: 1.01; acc: 0.77
Batch: 580; loss: 1.18; acc: 0.64
Batch: 600; loss: 1.0; acc: 0.8
Batch: 620; loss: 1.13; acc: 0.64
Batch: 640; loss: 1.03; acc: 0.72
Batch: 660; loss: 1.08; acc: 0.69
Batch: 680; loss: 1.14; acc: 0.73
Batch: 700; loss: 1.04; acc: 0.72
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.08; acc: 0.67
Batch: 760; loss: 1.07; acc: 0.72
Batch: 780; loss: 1.13; acc: 0.66
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

4.023156361654401e-05
1.4206985724740662e-05
Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 1.6; acc: 0.45
Batch: 40; loss: 0.91; acc: 0.72
Batch: 60; loss: 1.04; acc: 0.7
Batch: 80; loss: 1.14; acc: 0.62
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.33; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.73
Val Epoch over. val_loss: 1.2019930398388274; val_accuracy: 0.6233081210191083 

The current subspace-distance is: 1.4206985724740662e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.78
Batch: 20; loss: 1.04; acc: 0.69
Batch: 40; loss: 1.24; acc: 0.61
Batch: 60; loss: 1.22; acc: 0.61
Batch: 80; loss: 0.99; acc: 0.73
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.69
Batch: 140; loss: 1.02; acc: 0.69
Batch: 160; loss: 1.17; acc: 0.58
Batch: 180; loss: 1.05; acc: 0.62
Batch: 200; loss: 1.14; acc: 0.69
Batch: 220; loss: 1.03; acc: 0.73
Batch: 240; loss: 0.95; acc: 0.73
Batch: 260; loss: 0.95; acc: 0.77
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 1.35; acc: 0.58
Batch: 320; loss: 1.14; acc: 0.62
Batch: 340; loss: 1.1; acc: 0.69
Batch: 360; loss: 1.02; acc: 0.72
Batch: 380; loss: 0.99; acc: 0.72
Batch: 400; loss: 1.01; acc: 0.75
Batch: 420; loss: 0.93; acc: 0.7
Batch: 440; loss: 0.9; acc: 0.75
Batch: 460; loss: 1.05; acc: 0.69
Batch: 480; loss: 1.01; acc: 0.75
Batch: 500; loss: 1.03; acc: 0.72
Batch: 520; loss: 1.02; acc: 0.67
Batch: 540; loss: 1.06; acc: 0.73
Batch: 560; loss: 0.98; acc: 0.75
Batch: 580; loss: 1.22; acc: 0.62
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 0.94; acc: 0.72
Batch: 640; loss: 0.94; acc: 0.72
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.11; acc: 0.7
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 0.96; acc: 0.78
Batch: 740; loss: 1.06; acc: 0.69
Batch: 760; loss: 1.18; acc: 0.62
Batch: 780; loss: 1.1; acc: 0.73
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

4.2018302337965e-05
1.4287548765423708e-05
Batch: 0; loss: 1.15; acc: 0.66
Batch: 20; loss: 1.41; acc: 0.52
Batch: 40; loss: 0.87; acc: 0.72
Batch: 60; loss: 1.02; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.62
Batch: 100; loss: 1.1; acc: 0.62
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 0.89; acc: 0.73
Val Epoch over. val_loss: 1.1605395059676686; val_accuracy: 0.6271894904458599 

The current subspace-distance is: 1.4287548765423708e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.77
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.11; acc: 0.75
Batch: 60; loss: 1.1; acc: 0.66
Batch: 80; loss: 1.19; acc: 0.69
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 0.92; acc: 0.72
Batch: 160; loss: 0.99; acc: 0.7
Batch: 180; loss: 1.11; acc: 0.69
Batch: 200; loss: 0.91; acc: 0.73
Batch: 220; loss: 1.11; acc: 0.67
Batch: 240; loss: 1.09; acc: 0.67
Batch: 260; loss: 1.02; acc: 0.73
Batch: 280; loss: 0.96; acc: 0.7
Batch: 300; loss: 0.92; acc: 0.72
Batch: 320; loss: 1.16; acc: 0.64
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.72
Batch: 380; loss: 1.07; acc: 0.66
Batch: 400; loss: 1.02; acc: 0.72
Batch: 420; loss: 1.08; acc: 0.72
Batch: 440; loss: 1.15; acc: 0.66
Batch: 460; loss: 1.08; acc: 0.72
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 1.05; acc: 0.73
Batch: 520; loss: 1.1; acc: 0.67
Batch: 540; loss: 0.91; acc: 0.75
Batch: 560; loss: 1.05; acc: 0.66
Batch: 580; loss: 1.08; acc: 0.64
Batch: 600; loss: 1.01; acc: 0.69
Batch: 620; loss: 1.1; acc: 0.61
Batch: 640; loss: 1.07; acc: 0.69
Batch: 660; loss: 0.95; acc: 0.7
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 0.98; acc: 0.75
Batch: 720; loss: 1.09; acc: 0.72
Batch: 740; loss: 0.97; acc: 0.69
Batch: 760; loss: 1.13; acc: 0.72
Batch: 780; loss: 0.96; acc: 0.78
Train Epoch over. train_loss: 1.04; train_accuracy: 0.7 

4.244170122547075e-05
1.4920544344931841e-05
Batch: 0; loss: 0.91; acc: 0.78
Batch: 20; loss: 1.3; acc: 0.62
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 1.02; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.66
Batch: 100; loss: 1.01; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.59
Batch: 140; loss: 0.89; acc: 0.8
Val Epoch over. val_loss: 1.0801098756729417; val_accuracy: 0.6753582802547771 

The current subspace-distance is: 1.4920544344931841e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 0.97; acc: 0.72
Batch: 40; loss: 1.03; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 1.01; acc: 0.69
Batch: 160; loss: 1.07; acc: 0.67
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 1.12; acc: 0.66
Batch: 220; loss: 1.0; acc: 0.7
Batch: 240; loss: 0.99; acc: 0.8
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 0.91; acc: 0.77
Batch: 300; loss: 1.06; acc: 0.73
Batch: 320; loss: 1.17; acc: 0.62
Batch: 340; loss: 1.08; acc: 0.69
Batch: 360; loss: 1.09; acc: 0.64
Batch: 380; loss: 1.13; acc: 0.64
Batch: 400; loss: 1.12; acc: 0.64
Batch: 420; loss: 1.0; acc: 0.75
Batch: 440; loss: 0.96; acc: 0.77
Batch: 460; loss: 1.11; acc: 0.64
Batch: 480; loss: 1.14; acc: 0.66
Batch: 500; loss: 1.17; acc: 0.59
Batch: 520; loss: 0.92; acc: 0.81
Batch: 540; loss: 0.97; acc: 0.73
Batch: 560; loss: 1.02; acc: 0.75
Batch: 580; loss: 0.94; acc: 0.73
Batch: 600; loss: 1.0; acc: 0.8
Batch: 620; loss: 0.91; acc: 0.77
Batch: 640; loss: 1.14; acc: 0.66
Batch: 660; loss: 0.85; acc: 0.81
Batch: 680; loss: 1.0; acc: 0.66
Batch: 700; loss: 0.98; acc: 0.69
Batch: 720; loss: 0.9; acc: 0.77
Batch: 740; loss: 0.93; acc: 0.77
Batch: 760; loss: 0.98; acc: 0.72
Batch: 780; loss: 1.06; acc: 0.7
Train Epoch over. train_loss: 1.02; train_accuracy: 0.71 

4.434089714777656e-05
1.581536344019696e-05
Batch: 0; loss: 1.26; acc: 0.56
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.11; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.55
Batch: 80; loss: 1.48; acc: 0.47
Batch: 100; loss: 1.38; acc: 0.53
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.11; acc: 0.66
Val Epoch over. val_loss: 1.3837658352912612; val_accuracy: 0.5537420382165605 

The current subspace-distance is: 1.581536344019696e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.8
Batch: 80; loss: 0.99; acc: 0.7
Batch: 100; loss: 0.97; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 1.24; acc: 0.66
Batch: 160; loss: 0.95; acc: 0.72
Batch: 180; loss: 0.93; acc: 0.75
Batch: 200; loss: 0.95; acc: 0.7
Batch: 220; loss: 0.9; acc: 0.8
Batch: 240; loss: 1.06; acc: 0.7
Batch: 260; loss: 1.12; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.62
Batch: 300; loss: 1.17; acc: 0.62
Batch: 320; loss: 1.1; acc: 0.66
Batch: 340; loss: 1.04; acc: 0.72
Batch: 360; loss: 0.98; acc: 0.73
Batch: 380; loss: 1.09; acc: 0.72
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.99; acc: 0.64
Batch: 440; loss: 0.97; acc: 0.66
Batch: 460; loss: 1.1; acc: 0.61
Batch: 480; loss: 0.83; acc: 0.84
Batch: 500; loss: 0.91; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.69
Batch: 540; loss: 0.79; acc: 0.81
Batch: 560; loss: 1.03; acc: 0.67
Batch: 580; loss: 1.06; acc: 0.64
Batch: 600; loss: 1.07; acc: 0.64
Batch: 620; loss: 0.86; acc: 0.77
Batch: 640; loss: 1.12; acc: 0.64
Batch: 660; loss: 1.02; acc: 0.73
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.87; acc: 0.86
Batch: 720; loss: 0.97; acc: 0.73
Batch: 740; loss: 1.23; acc: 0.58
Batch: 760; loss: 0.93; acc: 0.73
Batch: 780; loss: 1.0; acc: 0.69
Train Epoch over. train_loss: 1.0; train_accuracy: 0.72 

4.480260395212099e-05
1.5927216736599803e-05
Batch: 0; loss: 0.81; acc: 0.81
Batch: 20; loss: 1.19; acc: 0.64
Batch: 40; loss: 0.81; acc: 0.75
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 0.92; acc: 0.73
Batch: 100; loss: 0.92; acc: 0.83
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 0.76; acc: 0.86
Val Epoch over. val_loss: 0.9895596120767532; val_accuracy: 0.7238256369426752 

The current subspace-distance is: 1.5927216736599803e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 1.01; acc: 0.7
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 1.07; acc: 0.73
Batch: 100; loss: 0.98; acc: 0.75
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.97; acc: 0.78
Batch: 160; loss: 0.9; acc: 0.72
Batch: 180; loss: 1.12; acc: 0.64
Batch: 200; loss: 0.94; acc: 0.7
Batch: 220; loss: 1.01; acc: 0.72
Batch: 240; loss: 0.96; acc: 0.69
Batch: 260; loss: 0.98; acc: 0.72
Batch: 280; loss: 0.95; acc: 0.72
Batch: 300; loss: 0.99; acc: 0.67
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.97; acc: 0.73
Batch: 360; loss: 1.06; acc: 0.75
Batch: 380; loss: 1.02; acc: 0.69
Batch: 400; loss: 1.37; acc: 0.55
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 1.02; acc: 0.75
Batch: 460; loss: 0.98; acc: 0.73
Batch: 480; loss: 0.92; acc: 0.8
Batch: 500; loss: 0.89; acc: 0.78
Batch: 520; loss: 0.99; acc: 0.73
Batch: 540; loss: 0.86; acc: 0.8
Batch: 560; loss: 0.99; acc: 0.75
Batch: 580; loss: 1.0; acc: 0.72
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 0.93; acc: 0.78
Batch: 640; loss: 1.13; acc: 0.66
Batch: 660; loss: 1.06; acc: 0.66
Batch: 680; loss: 1.01; acc: 0.72
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 0.94; acc: 0.69
Batch: 740; loss: 0.9; acc: 0.73
Batch: 760; loss: 0.9; acc: 0.77
Batch: 780; loss: 1.1; acc: 0.72
Train Epoch over. train_loss: 0.99; train_accuracy: 0.72 

4.607156733982265e-05
1.8114033082383685e-05
Batch: 0; loss: 0.8; acc: 0.83
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 0.75; acc: 0.86
Val Epoch over. val_loss: 0.9512221536059289; val_accuracy: 0.7325835987261147 

The current subspace-distance is: 1.8114033082383685e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.69
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 1.12; acc: 0.66
Batch: 60; loss: 0.92; acc: 0.83
Batch: 80; loss: 0.81; acc: 0.83
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 1.04; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 1.11; acc: 0.7
Batch: 200; loss: 1.04; acc: 0.72
Batch: 220; loss: 1.01; acc: 0.72
Batch: 240; loss: 1.02; acc: 0.72
Batch: 260; loss: 1.06; acc: 0.69
Batch: 280; loss: 1.02; acc: 0.77
Batch: 300; loss: 0.99; acc: 0.73
Batch: 320; loss: 1.0; acc: 0.69
Batch: 340; loss: 1.22; acc: 0.62
Batch: 360; loss: 1.02; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.7
Batch: 400; loss: 1.03; acc: 0.67
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.0; acc: 0.69
Batch: 480; loss: 0.84; acc: 0.77
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 1.01; acc: 0.73
Batch: 540; loss: 0.91; acc: 0.75
Batch: 560; loss: 1.01; acc: 0.78
Batch: 580; loss: 1.07; acc: 0.66
Batch: 600; loss: 0.97; acc: 0.66
Batch: 620; loss: 1.05; acc: 0.72
Batch: 640; loss: 0.99; acc: 0.7
Batch: 660; loss: 0.96; acc: 0.75
Batch: 680; loss: 0.99; acc: 0.73
Batch: 700; loss: 0.97; acc: 0.77
Batch: 720; loss: 0.96; acc: 0.72
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 0.91; acc: 0.77
Batch: 780; loss: 0.99; acc: 0.72
Train Epoch over. train_loss: 0.99; train_accuracy: 0.72 

4.491575600695796e-05
1.616804911463987e-05
Batch: 0; loss: 0.9; acc: 0.75
Batch: 20; loss: 1.34; acc: 0.58
Batch: 40; loss: 0.9; acc: 0.72
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 0.94; acc: 0.73
Val Epoch over. val_loss: 1.0822492370939558; val_accuracy: 0.6850119426751592 

The current subspace-distance is: 1.616804911463987e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.08; acc: 0.7
Batch: 40; loss: 0.92; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 0.93; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 0.86; acc: 0.78
Batch: 160; loss: 0.92; acc: 0.72
Batch: 180; loss: 0.97; acc: 0.77
Batch: 200; loss: 0.89; acc: 0.8
Batch: 220; loss: 1.07; acc: 0.67
Batch: 240; loss: 1.09; acc: 0.69
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 1.19; acc: 0.62
Batch: 300; loss: 0.99; acc: 0.75
Batch: 320; loss: 0.93; acc: 0.75
Batch: 340; loss: 1.15; acc: 0.66
Batch: 360; loss: 1.1; acc: 0.62
Batch: 380; loss: 0.85; acc: 0.8
Batch: 400; loss: 1.09; acc: 0.72
Batch: 420; loss: 0.89; acc: 0.77
Batch: 440; loss: 0.98; acc: 0.7
Batch: 460; loss: 0.86; acc: 0.77
Batch: 480; loss: 0.97; acc: 0.72
Batch: 500; loss: 1.24; acc: 0.61
Batch: 520; loss: 0.89; acc: 0.7
Batch: 540; loss: 0.99; acc: 0.75
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.96; acc: 0.73
Batch: 600; loss: 1.03; acc: 0.7
Batch: 620; loss: 0.88; acc: 0.83
Batch: 640; loss: 0.82; acc: 0.78
Batch: 660; loss: 0.9; acc: 0.77
Batch: 680; loss: 0.88; acc: 0.73
Batch: 700; loss: 0.9; acc: 0.73
Batch: 720; loss: 1.1; acc: 0.7
Batch: 740; loss: 0.97; acc: 0.73
Batch: 760; loss: 0.99; acc: 0.73
Batch: 780; loss: 1.11; acc: 0.67
Train Epoch over. train_loss: 0.98; train_accuracy: 0.72 

4.642143539967947e-05
1.698019877949264e-05
Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 1.12; acc: 0.7
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.83
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 1.13; acc: 0.66
Batch: 140; loss: 0.74; acc: 0.89
Val Epoch over. val_loss: 0.9784245365744184; val_accuracy: 0.7188495222929936 

The current subspace-distance is: 1.698019877949264e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 0.96; acc: 0.77
Batch: 40; loss: 1.24; acc: 0.61
Batch: 60; loss: 0.92; acc: 0.73
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 1.08; acc: 0.64
Batch: 140; loss: 0.86; acc: 0.81
Batch: 160; loss: 1.01; acc: 0.7
Batch: 180; loss: 0.94; acc: 0.69
Batch: 200; loss: 1.02; acc: 0.67
Batch: 220; loss: 1.03; acc: 0.69
Batch: 240; loss: 1.07; acc: 0.64
Batch: 260; loss: 1.04; acc: 0.66
Batch: 280; loss: 0.88; acc: 0.75
Batch: 300; loss: 1.07; acc: 0.67
Batch: 320; loss: 0.85; acc: 0.78
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.01; acc: 0.72
Batch: 380; loss: 0.99; acc: 0.7
Batch: 400; loss: 0.93; acc: 0.77
Batch: 420; loss: 0.91; acc: 0.77
Batch: 440; loss: 1.01; acc: 0.72
Batch: 460; loss: 0.83; acc: 0.78
Batch: 480; loss: 1.15; acc: 0.64
Batch: 500; loss: 0.89; acc: 0.8
Batch: 520; loss: 1.12; acc: 0.67
Batch: 540; loss: 1.01; acc: 0.8
Batch: 560; loss: 1.1; acc: 0.69
Batch: 580; loss: 1.09; acc: 0.67
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.1; acc: 0.67
Batch: 660; loss: 0.9; acc: 0.77
Batch: 680; loss: 0.86; acc: 0.75
Batch: 700; loss: 0.96; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.67
Batch: 740; loss: 1.08; acc: 0.64
Batch: 760; loss: 0.85; acc: 0.81
Batch: 780; loss: 0.96; acc: 0.73
Train Epoch over. train_loss: 0.98; train_accuracy: 0.72 

4.629979594028555e-05
1.7589547496754676e-05
Batch: 0; loss: 0.73; acc: 0.84
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.99; acc: 0.7
Batch: 80; loss: 0.85; acc: 0.7
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 1.09; acc: 0.69
Batch: 140; loss: 0.74; acc: 0.81
Val Epoch over. val_loss: 0.9854301218014614; val_accuracy: 0.7143710191082803 

The current subspace-distance is: 1.7589547496754676e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.88; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 1.04; acc: 0.69
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.61
Batch: 120; loss: 1.18; acc: 0.64
Batch: 140; loss: 0.96; acc: 0.8
Batch: 160; loss: 1.02; acc: 0.66
Batch: 180; loss: 1.14; acc: 0.64
Batch: 200; loss: 0.96; acc: 0.72
Batch: 220; loss: 1.06; acc: 0.66
Batch: 240; loss: 0.9; acc: 0.77
Batch: 260; loss: 0.92; acc: 0.75
Batch: 280; loss: 0.84; acc: 0.72
Batch: 300; loss: 1.08; acc: 0.62
Batch: 320; loss: 0.73; acc: 0.88
Batch: 340; loss: 1.03; acc: 0.69
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.98; acc: 0.69
Batch: 400; loss: 1.01; acc: 0.64
Batch: 420; loss: 0.82; acc: 0.83
Batch: 440; loss: 0.9; acc: 0.73
Batch: 460; loss: 1.14; acc: 0.64
Batch: 480; loss: 1.04; acc: 0.67
Batch: 500; loss: 0.83; acc: 0.75
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 1.03; acc: 0.72
Batch: 560; loss: 0.98; acc: 0.7
Batch: 580; loss: 1.02; acc: 0.67
Batch: 600; loss: 0.9; acc: 0.77
Batch: 620; loss: 0.89; acc: 0.81
Batch: 640; loss: 0.91; acc: 0.75
Batch: 660; loss: 0.98; acc: 0.67
Batch: 680; loss: 1.0; acc: 0.7
Batch: 700; loss: 0.96; acc: 0.75
Batch: 720; loss: 0.8; acc: 0.88
Batch: 740; loss: 1.0; acc: 0.72
Batch: 760; loss: 0.79; acc: 0.84
Batch: 780; loss: 0.98; acc: 0.73
Train Epoch over. train_loss: 0.98; train_accuracy: 0.72 

4.709829590865411e-05
1.8752993128146045e-05
Batch: 0; loss: 0.73; acc: 0.86
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.85; acc: 0.8
Batch: 80; loss: 0.82; acc: 0.83
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 1.08; acc: 0.7
Batch: 140; loss: 0.71; acc: 0.86
Val Epoch over. val_loss: 0.9245638968838248; val_accuracy: 0.7434315286624203 

The current subspace-distance is: 1.8752993128146045e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.84; acc: 0.78
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 0.94; acc: 0.77
Batch: 80; loss: 1.08; acc: 0.69
Batch: 100; loss: 0.85; acc: 0.78
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.87; acc: 0.73
Batch: 160; loss: 0.96; acc: 0.78
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.86; acc: 0.75
Batch: 220; loss: 0.99; acc: 0.66
Batch: 240; loss: 1.19; acc: 0.61
Batch: 260; loss: 1.07; acc: 0.7
Batch: 280; loss: 0.86; acc: 0.77
Batch: 300; loss: 0.89; acc: 0.75
Batch: 320; loss: 0.99; acc: 0.75
Batch: 340; loss: 1.0; acc: 0.7
Batch: 360; loss: 0.96; acc: 0.69
Batch: 380; loss: 1.06; acc: 0.7
Batch: 400; loss: 0.83; acc: 0.77
Batch: 420; loss: 0.91; acc: 0.7
Batch: 440; loss: 0.87; acc: 0.73
Batch: 460; loss: 0.8; acc: 0.73
Batch: 480; loss: 1.04; acc: 0.66
Batch: 500; loss: 0.83; acc: 0.78
Batch: 520; loss: 1.05; acc: 0.72
Batch: 540; loss: 0.94; acc: 0.67
Batch: 560; loss: 0.89; acc: 0.75
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.99; acc: 0.66
Batch: 620; loss: 0.98; acc: 0.72
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 0.92; acc: 0.77
Batch: 680; loss: 1.16; acc: 0.66
Batch: 700; loss: 0.78; acc: 0.83
Batch: 720; loss: 0.9; acc: 0.73
Batch: 740; loss: 1.08; acc: 0.69
Batch: 760; loss: 0.97; acc: 0.78
Batch: 780; loss: 1.16; acc: 0.66
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

4.738891584565863e-05
1.7861615560832433e-05
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 1.11; acc: 0.69
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.9; acc: 0.77
Batch: 120; loss: 1.1; acc: 0.69
Batch: 140; loss: 0.78; acc: 0.81
Val Epoch over. val_loss: 0.9646013234830966; val_accuracy: 0.7336783439490446 

The current subspace-distance is: 1.7861615560832433e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 0.98; acc: 0.67
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 0.93; acc: 0.81
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.69
Batch: 160; loss: 1.0; acc: 0.69
Batch: 180; loss: 0.77; acc: 0.84
Batch: 200; loss: 0.98; acc: 0.66
Batch: 220; loss: 0.92; acc: 0.7
Batch: 240; loss: 1.01; acc: 0.7
Batch: 260; loss: 0.97; acc: 0.78
Batch: 280; loss: 0.98; acc: 0.72
Batch: 300; loss: 0.89; acc: 0.75
Batch: 320; loss: 1.1; acc: 0.72
Batch: 340; loss: 0.87; acc: 0.72
Batch: 360; loss: 0.86; acc: 0.77
Batch: 380; loss: 0.86; acc: 0.72
Batch: 400; loss: 0.87; acc: 0.78
Batch: 420; loss: 0.86; acc: 0.8
Batch: 440; loss: 0.87; acc: 0.75
Batch: 460; loss: 1.09; acc: 0.67
Batch: 480; loss: 1.05; acc: 0.73
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.84; acc: 0.75
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 1.08; acc: 0.66
Batch: 580; loss: 1.08; acc: 0.64
Batch: 600; loss: 0.92; acc: 0.75
Batch: 620; loss: 1.11; acc: 0.61
Batch: 640; loss: 1.1; acc: 0.67
Batch: 660; loss: 0.86; acc: 0.78
Batch: 680; loss: 0.9; acc: 0.7
Batch: 700; loss: 0.81; acc: 0.81
Batch: 720; loss: 0.87; acc: 0.78
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 1.09; acc: 0.66
Batch: 780; loss: 1.06; acc: 0.67
Train Epoch over. train_loss: 0.97; train_accuracy: 0.72 

4.815544525627047e-05
1.8906162949861027e-05
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 1.14; acc: 0.62
Batch: 140; loss: 0.75; acc: 0.8
Val Epoch over. val_loss: 0.9978084173172143; val_accuracy: 0.7164609872611465 

The current subspace-distance is: 1.8906162949861027e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 0.91; acc: 0.77
Batch: 80; loss: 0.99; acc: 0.64
Batch: 100; loss: 0.9; acc: 0.77
Batch: 120; loss: 1.19; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.75
Batch: 160; loss: 0.97; acc: 0.81
Batch: 180; loss: 1.06; acc: 0.64
Batch: 200; loss: 0.95; acc: 0.72
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 0.93; acc: 0.78
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.93; acc: 0.73
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 0.94; acc: 0.72
Batch: 340; loss: 1.03; acc: 0.67
Batch: 360; loss: 0.85; acc: 0.78
Batch: 380; loss: 1.12; acc: 0.7
Batch: 400; loss: 0.91; acc: 0.77
Batch: 420; loss: 1.05; acc: 0.69
Batch: 440; loss: 0.93; acc: 0.73
Batch: 460; loss: 0.85; acc: 0.77
Batch: 480; loss: 1.14; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.69
Batch: 520; loss: 0.9; acc: 0.78
Batch: 540; loss: 0.95; acc: 0.75
Batch: 560; loss: 0.8; acc: 0.81
Batch: 580; loss: 0.93; acc: 0.78
Batch: 600; loss: 1.29; acc: 0.58
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.9; acc: 0.78
Batch: 660; loss: 1.1; acc: 0.61
Batch: 680; loss: 1.07; acc: 0.7
Batch: 700; loss: 1.02; acc: 0.69
Batch: 720; loss: 0.86; acc: 0.73
Batch: 740; loss: 0.96; acc: 0.7
Batch: 760; loss: 0.85; acc: 0.75
Batch: 780; loss: 0.88; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

4.765685662277974e-05
1.8173970602219924e-05
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.89; acc: 0.8
Batch: 100; loss: 0.89; acc: 0.75
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 0.85; acc: 0.8
Val Epoch over. val_loss: 1.019698454695902; val_accuracy: 0.7053144904458599 

The current subspace-distance is: 1.8173970602219924e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.06; acc: 0.7
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.62
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 1.14; acc: 0.64
Batch: 160; loss: 0.94; acc: 0.75
Batch: 180; loss: 0.87; acc: 0.78
Batch: 200; loss: 1.01; acc: 0.72
Batch: 220; loss: 0.96; acc: 0.81
Batch: 240; loss: 1.11; acc: 0.66
Batch: 260; loss: 0.95; acc: 0.69
Batch: 280; loss: 0.82; acc: 0.81
Batch: 300; loss: 0.92; acc: 0.75
Batch: 320; loss: 0.94; acc: 0.78
Batch: 340; loss: 0.93; acc: 0.75
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.92; acc: 0.72
Batch: 400; loss: 0.88; acc: 0.78
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 1.01; acc: 0.69
Batch: 460; loss: 0.99; acc: 0.7
Batch: 480; loss: 1.08; acc: 0.75
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.9; acc: 0.7
Batch: 540; loss: 1.09; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 0.98; acc: 0.77
Batch: 600; loss: 0.9; acc: 0.73
Batch: 620; loss: 1.12; acc: 0.62
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.05; acc: 0.73
Batch: 680; loss: 0.85; acc: 0.78
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 1.12; acc: 0.69
Batch: 740; loss: 0.85; acc: 0.8
Batch: 760; loss: 0.74; acc: 0.78
Batch: 780; loss: 0.85; acc: 0.8
Train Epoch over. train_loss: 0.96; train_accuracy: 0.73 

4.837448796024546e-05
1.851676825026516e-05
Batch: 0; loss: 0.79; acc: 0.86
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 0.87; acc: 0.73
Batch: 60; loss: 1.09; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 0.95; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.72
Val Epoch over. val_loss: 1.0822346673649588; val_accuracy: 0.6693869426751592 

The current subspace-distance is: 1.851676825026516e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 0.91; acc: 0.69
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.91; acc: 0.73
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 0.75; acc: 0.8
Batch: 160; loss: 1.11; acc: 0.69
Batch: 180; loss: 1.02; acc: 0.7
Batch: 200; loss: 1.13; acc: 0.66
Batch: 220; loss: 0.79; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.9; acc: 0.75
Batch: 280; loss: 0.95; acc: 0.72
Batch: 300; loss: 1.03; acc: 0.62
Batch: 320; loss: 1.0; acc: 0.66
Batch: 340; loss: 0.97; acc: 0.77
Batch: 360; loss: 0.78; acc: 0.81
Batch: 380; loss: 0.93; acc: 0.7
Batch: 400; loss: 1.12; acc: 0.64
Batch: 420; loss: 1.02; acc: 0.69
Batch: 440; loss: 1.01; acc: 0.67
Batch: 460; loss: 0.94; acc: 0.7
Batch: 480; loss: 0.76; acc: 0.83
Batch: 500; loss: 0.87; acc: 0.75
Batch: 520; loss: 0.95; acc: 0.75
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 0.78; acc: 0.78
Batch: 580; loss: 1.01; acc: 0.69
Batch: 600; loss: 0.86; acc: 0.8
Batch: 620; loss: 0.79; acc: 0.8
Batch: 640; loss: 0.94; acc: 0.67
Batch: 660; loss: 1.12; acc: 0.64
Batch: 680; loss: 0.88; acc: 0.72
Batch: 700; loss: 0.9; acc: 0.75
Batch: 720; loss: 0.9; acc: 0.75
Batch: 740; loss: 0.95; acc: 0.75
Batch: 760; loss: 0.89; acc: 0.75
Batch: 780; loss: 0.91; acc: 0.73
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.8519963456783444e-05
1.8186319721280597e-05
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 0.65; acc: 0.88
Val Epoch over. val_loss: 0.8779021334496273; val_accuracy: 0.7589570063694268 

The current subspace-distance is: 1.8186319721280597e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.77
Batch: 40; loss: 0.79; acc: 0.75
Batch: 60; loss: 0.98; acc: 0.66
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 0.94; acc: 0.72
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 0.81; acc: 0.78
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 1.12; acc: 0.64
Batch: 200; loss: 0.81; acc: 0.77
Batch: 220; loss: 0.82; acc: 0.78
Batch: 240; loss: 0.87; acc: 0.8
Batch: 260; loss: 1.04; acc: 0.66
Batch: 280; loss: 0.96; acc: 0.67
Batch: 300; loss: 0.88; acc: 0.75
Batch: 320; loss: 1.06; acc: 0.66
Batch: 340; loss: 0.94; acc: 0.73
Batch: 360; loss: 1.03; acc: 0.64
Batch: 380; loss: 1.14; acc: 0.66
Batch: 400; loss: 0.91; acc: 0.77
Batch: 420; loss: 1.11; acc: 0.64
Batch: 440; loss: 0.99; acc: 0.73
Batch: 460; loss: 0.89; acc: 0.73
Batch: 480; loss: 0.82; acc: 0.81
Batch: 500; loss: 1.02; acc: 0.7
Batch: 520; loss: 0.93; acc: 0.75
Batch: 540; loss: 0.91; acc: 0.78
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 0.9; acc: 0.81
Batch: 600; loss: 0.96; acc: 0.78
Batch: 620; loss: 0.93; acc: 0.72
Batch: 640; loss: 0.99; acc: 0.69
Batch: 660; loss: 0.85; acc: 0.78
Batch: 680; loss: 1.07; acc: 0.66
Batch: 700; loss: 0.98; acc: 0.72
Batch: 720; loss: 0.96; acc: 0.69
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.8; acc: 0.8
Batch: 780; loss: 0.86; acc: 0.8
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.9162012146553025e-05
1.8088134311256e-05
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.79; acc: 0.8
Batch: 100; loss: 0.88; acc: 0.77
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.65; acc: 0.88
Val Epoch over. val_loss: 0.8834341654352321; val_accuracy: 0.7601512738853503 

The current subspace-distance is: 1.8088134311256e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.73
Batch: 20; loss: 0.91; acc: 0.77
Batch: 40; loss: 1.04; acc: 0.7
Batch: 60; loss: 0.9; acc: 0.72
Batch: 80; loss: 0.89; acc: 0.73
Batch: 100; loss: 0.97; acc: 0.67
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.84
Batch: 160; loss: 1.11; acc: 0.62
Batch: 180; loss: 0.81; acc: 0.8
Batch: 200; loss: 1.0; acc: 0.75
Batch: 220; loss: 1.0; acc: 0.69
Batch: 240; loss: 0.93; acc: 0.73
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.91; acc: 0.81
Batch: 300; loss: 0.89; acc: 0.73
Batch: 320; loss: 0.87; acc: 0.77
Batch: 340; loss: 0.73; acc: 0.88
Batch: 360; loss: 1.12; acc: 0.72
Batch: 380; loss: 0.89; acc: 0.75
Batch: 400; loss: 0.99; acc: 0.67
Batch: 420; loss: 1.09; acc: 0.64
Batch: 440; loss: 0.95; acc: 0.78
Batch: 460; loss: 1.19; acc: 0.64
Batch: 480; loss: 0.83; acc: 0.8
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.0; acc: 0.72
Batch: 540; loss: 0.93; acc: 0.72
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 0.95; acc: 0.75
Batch: 600; loss: 1.1; acc: 0.72
Batch: 620; loss: 1.03; acc: 0.72
Batch: 640; loss: 0.9; acc: 0.7
Batch: 660; loss: 0.83; acc: 0.81
Batch: 680; loss: 0.95; acc: 0.73
Batch: 700; loss: 0.87; acc: 0.78
Batch: 720; loss: 0.94; acc: 0.75
Batch: 740; loss: 1.15; acc: 0.66
Batch: 760; loss: 1.0; acc: 0.72
Batch: 780; loss: 0.91; acc: 0.75
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.8650104872649536e-05
1.8120685126632452e-05
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 1.02; acc: 0.7
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.81
Batch: 80; loss: 0.79; acc: 0.8
Batch: 100; loss: 0.85; acc: 0.78
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 0.61; acc: 0.89
Val Epoch over. val_loss: 0.8717330052594471; val_accuracy: 0.7616441082802548 

The current subspace-distance is: 1.8120685126632452e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.94; acc: 0.72
Batch: 20; loss: 1.08; acc: 0.69
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.61
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.87; acc: 0.69
Batch: 160; loss: 0.95; acc: 0.75
Batch: 180; loss: 1.04; acc: 0.75
Batch: 200; loss: 0.96; acc: 0.62
Batch: 220; loss: 0.85; acc: 0.78
Batch: 240; loss: 1.11; acc: 0.64
Batch: 260; loss: 0.85; acc: 0.78
Batch: 280; loss: 0.9; acc: 0.7
Batch: 300; loss: 0.93; acc: 0.62
Batch: 320; loss: 1.02; acc: 0.73
Batch: 340; loss: 0.97; acc: 0.69
Batch: 360; loss: 0.85; acc: 0.81
Batch: 380; loss: 1.02; acc: 0.67
Batch: 400; loss: 0.91; acc: 0.72
Batch: 420; loss: 1.0; acc: 0.69
Batch: 440; loss: 0.79; acc: 0.84
Batch: 460; loss: 1.01; acc: 0.75
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 0.87; acc: 0.78
Batch: 520; loss: 0.89; acc: 0.69
Batch: 540; loss: 0.86; acc: 0.83
Batch: 560; loss: 1.04; acc: 0.72
Batch: 580; loss: 1.05; acc: 0.73
Batch: 600; loss: 0.99; acc: 0.73
Batch: 620; loss: 0.87; acc: 0.77
Batch: 640; loss: 0.97; acc: 0.75
Batch: 660; loss: 1.06; acc: 0.69
Batch: 680; loss: 0.91; acc: 0.77
Batch: 700; loss: 0.84; acc: 0.75
Batch: 720; loss: 0.98; acc: 0.7
Batch: 740; loss: 0.82; acc: 0.81
Batch: 760; loss: 0.87; acc: 0.75
Batch: 780; loss: 0.94; acc: 0.72
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.86353637825232e-05
1.7569946066942066e-05
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 1.05; acc: 0.69
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 1.09; acc: 0.66
Batch: 140; loss: 0.62; acc: 0.86
Val Epoch over. val_loss: 0.8886130744484579; val_accuracy: 0.7521894904458599 

The current subspace-distance is: 1.7569946066942066e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 1.04; acc: 0.64
Batch: 60; loss: 0.9; acc: 0.77
Batch: 80; loss: 1.0; acc: 0.67
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 1.23; acc: 0.58
Batch: 160; loss: 0.92; acc: 0.7
Batch: 180; loss: 1.0; acc: 0.73
Batch: 200; loss: 0.92; acc: 0.81
Batch: 220; loss: 0.86; acc: 0.75
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 0.75; acc: 0.81
Batch: 280; loss: 0.99; acc: 0.7
Batch: 300; loss: 1.16; acc: 0.67
Batch: 320; loss: 1.04; acc: 0.67
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.81; acc: 0.83
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 1.0; acc: 0.64
Batch: 420; loss: 1.24; acc: 0.62
Batch: 440; loss: 0.99; acc: 0.72
Batch: 460; loss: 0.97; acc: 0.7
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 0.89; acc: 0.75
Batch: 520; loss: 0.96; acc: 0.7
Batch: 540; loss: 1.02; acc: 0.73
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 1.02; acc: 0.7
Batch: 600; loss: 0.9; acc: 0.73
Batch: 620; loss: 0.79; acc: 0.78
Batch: 640; loss: 0.99; acc: 0.72
Batch: 660; loss: 0.91; acc: 0.78
Batch: 680; loss: 0.97; acc: 0.73
Batch: 700; loss: 0.89; acc: 0.73
Batch: 720; loss: 0.98; acc: 0.72
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 1.14; acc: 0.66
Batch: 780; loss: 0.89; acc: 0.8
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.9154758016811684e-05
1.924990829138551e-05
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.81; acc: 0.73
Batch: 100; loss: 0.89; acc: 0.75
Batch: 120; loss: 1.1; acc: 0.67
Batch: 140; loss: 0.63; acc: 0.88
Val Epoch over. val_loss: 0.8827643808285901; val_accuracy: 0.7561703821656051 

The current subspace-distance is: 1.924990829138551e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.94; acc: 0.78
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 0.76; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.91; acc: 0.72
Batch: 160; loss: 0.81; acc: 0.83
Batch: 180; loss: 0.99; acc: 0.78
Batch: 200; loss: 0.88; acc: 0.7
Batch: 220; loss: 0.91; acc: 0.73
Batch: 240; loss: 0.83; acc: 0.81
Batch: 260; loss: 1.18; acc: 0.59
Batch: 280; loss: 0.95; acc: 0.73
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 0.93; acc: 0.77
Batch: 340; loss: 0.96; acc: 0.8
Batch: 360; loss: 0.8; acc: 0.84
Batch: 380; loss: 0.96; acc: 0.66
Batch: 400; loss: 0.87; acc: 0.73
Batch: 420; loss: 0.87; acc: 0.81
Batch: 440; loss: 0.95; acc: 0.69
Batch: 460; loss: 0.88; acc: 0.77
Batch: 480; loss: 1.02; acc: 0.69
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 1.01; acc: 0.72
Batch: 540; loss: 1.06; acc: 0.64
Batch: 560; loss: 0.99; acc: 0.75
Batch: 580; loss: 1.12; acc: 0.62
Batch: 600; loss: 0.82; acc: 0.78
Batch: 620; loss: 1.1; acc: 0.69
Batch: 640; loss: 0.96; acc: 0.73
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 0.85; acc: 0.8
Batch: 700; loss: 0.96; acc: 0.73
Batch: 720; loss: 1.08; acc: 0.67
Batch: 740; loss: 0.91; acc: 0.78
Batch: 760; loss: 1.02; acc: 0.69
Batch: 780; loss: 0.97; acc: 0.72
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.924609311274253e-05
1.8623464711708948e-05
Batch: 0; loss: 0.67; acc: 0.89
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.64; acc: 0.86
Val Epoch over. val_loss: 0.8764814783813087; val_accuracy: 0.7575636942675159 

The current subspace-distance is: 1.8623464711708948e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.98; acc: 0.7
Batch: 60; loss: 1.0; acc: 0.64
Batch: 80; loss: 0.81; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 0.76; acc: 0.86
Batch: 140; loss: 1.02; acc: 0.7
Batch: 160; loss: 0.87; acc: 0.81
Batch: 180; loss: 0.8; acc: 0.83
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.91; acc: 0.73
Batch: 240; loss: 1.06; acc: 0.64
Batch: 260; loss: 1.14; acc: 0.69
Batch: 280; loss: 0.94; acc: 0.72
Batch: 300; loss: 0.86; acc: 0.78
Batch: 320; loss: 1.03; acc: 0.66
Batch: 340; loss: 1.09; acc: 0.64
Batch: 360; loss: 1.1; acc: 0.67
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 1.04; acc: 0.75
Batch: 440; loss: 0.94; acc: 0.72
Batch: 460; loss: 0.93; acc: 0.78
Batch: 480; loss: 0.99; acc: 0.69
Batch: 500; loss: 1.04; acc: 0.69
Batch: 520; loss: 0.81; acc: 0.75
Batch: 540; loss: 1.1; acc: 0.69
Batch: 560; loss: 0.89; acc: 0.78
Batch: 580; loss: 0.88; acc: 0.78
Batch: 600; loss: 0.97; acc: 0.75
Batch: 620; loss: 0.9; acc: 0.77
Batch: 640; loss: 0.89; acc: 0.78
Batch: 660; loss: 0.95; acc: 0.69
Batch: 680; loss: 0.86; acc: 0.72
Batch: 700; loss: 0.93; acc: 0.69
Batch: 720; loss: 0.96; acc: 0.7
Batch: 740; loss: 0.87; acc: 0.75
Batch: 760; loss: 0.88; acc: 0.75
Batch: 780; loss: 1.1; acc: 0.69
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.9066344217862934e-05
1.973703365365509e-05
Batch: 0; loss: 0.66; acc: 0.91
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.81
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 0.64; acc: 0.88
Val Epoch over. val_loss: 0.8832689083305894; val_accuracy: 0.7551751592356688 

The current subspace-distance is: 1.973703365365509e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 1.04; acc: 0.7
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 1.21; acc: 0.61
Batch: 100; loss: 0.91; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.72
Batch: 160; loss: 1.02; acc: 0.67
Batch: 180; loss: 0.95; acc: 0.75
Batch: 200; loss: 0.99; acc: 0.73
Batch: 220; loss: 1.05; acc: 0.72
Batch: 240; loss: 0.87; acc: 0.8
Batch: 260; loss: 0.85; acc: 0.81
Batch: 280; loss: 0.94; acc: 0.69
Batch: 300; loss: 0.82; acc: 0.81
Batch: 320; loss: 0.96; acc: 0.7
Batch: 340; loss: 1.26; acc: 0.56
Batch: 360; loss: 0.93; acc: 0.69
Batch: 380; loss: 0.98; acc: 0.7
Batch: 400; loss: 0.83; acc: 0.72
Batch: 420; loss: 1.14; acc: 0.69
Batch: 440; loss: 0.85; acc: 0.77
Batch: 460; loss: 0.87; acc: 0.75
Batch: 480; loss: 0.76; acc: 0.83
Batch: 500; loss: 0.96; acc: 0.69
Batch: 520; loss: 0.8; acc: 0.78
Batch: 540; loss: 0.86; acc: 0.81
Batch: 560; loss: 0.95; acc: 0.75
Batch: 580; loss: 0.86; acc: 0.8
Batch: 600; loss: 1.04; acc: 0.69
Batch: 620; loss: 0.85; acc: 0.73
Batch: 640; loss: 0.82; acc: 0.83
Batch: 660; loss: 0.98; acc: 0.73
Batch: 680; loss: 0.96; acc: 0.75
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 0.81; acc: 0.84
Batch: 740; loss: 1.06; acc: 0.77
Batch: 760; loss: 0.97; acc: 0.7
Batch: 780; loss: 0.86; acc: 0.81
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.891100979875773e-05
1.864925434347242e-05
Batch: 0; loss: 0.68; acc: 0.88
Batch: 20; loss: 1.04; acc: 0.67
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.81; acc: 0.72
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 0.62; acc: 0.88
Val Epoch over. val_loss: 0.8802282639369843; val_accuracy: 0.757265127388535 

The current subspace-distance is: 1.864925434347242e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.88; acc: 0.75
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.76; acc: 0.84
Batch: 100; loss: 1.01; acc: 0.73
Batch: 120; loss: 1.03; acc: 0.69
Batch: 140; loss: 1.03; acc: 0.7
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.91; acc: 0.75
Batch: 200; loss: 0.99; acc: 0.69
Batch: 220; loss: 0.91; acc: 0.73
Batch: 240; loss: 1.06; acc: 0.67
Batch: 260; loss: 0.97; acc: 0.7
Batch: 280; loss: 1.2; acc: 0.72
Batch: 300; loss: 0.83; acc: 0.78
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 1.11; acc: 0.66
Batch: 360; loss: 0.98; acc: 0.72
Batch: 380; loss: 0.79; acc: 0.81
Batch: 400; loss: 0.8; acc: 0.77
Batch: 420; loss: 0.93; acc: 0.73
Batch: 440; loss: 1.02; acc: 0.64
Batch: 460; loss: 1.06; acc: 0.72
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 0.88; acc: 0.77
Batch: 520; loss: 0.83; acc: 0.77
Batch: 540; loss: 1.09; acc: 0.64
Batch: 560; loss: 0.89; acc: 0.75
Batch: 580; loss: 0.78; acc: 0.73
Batch: 600; loss: 0.94; acc: 0.78
Batch: 620; loss: 0.98; acc: 0.73
Batch: 640; loss: 1.05; acc: 0.67
Batch: 660; loss: 0.78; acc: 0.84
Batch: 680; loss: 0.89; acc: 0.81
Batch: 700; loss: 0.85; acc: 0.77
Batch: 720; loss: 0.95; acc: 0.72
Batch: 740; loss: 1.22; acc: 0.72
Batch: 760; loss: 0.97; acc: 0.72
Batch: 780; loss: 0.83; acc: 0.73
Train Epoch over. train_loss: 0.94; train_accuracy: 0.73 

4.997639553039335e-05
1.9925479136873037e-05
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 0.63; acc: 0.86
Val Epoch over. val_loss: 0.8845281460482604; val_accuracy: 0.7490047770700637 

The current subspace-distance is: 1.9925479136873037e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.92; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 1.0; acc: 0.73
Batch: 60; loss: 1.04; acc: 0.66
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 1.12; acc: 0.66
Batch: 160; loss: 0.92; acc: 0.72
Batch: 180; loss: 1.01; acc: 0.7
Batch: 200; loss: 0.97; acc: 0.69
Batch: 220; loss: 0.91; acc: 0.7
Batch: 240; loss: 0.84; acc: 0.78
Batch: 260; loss: 1.08; acc: 0.67
Batch: 280; loss: 1.03; acc: 0.69
Batch: 300; loss: 0.95; acc: 0.7
Batch: 320; loss: 1.05; acc: 0.62
Batch: 340; loss: 1.13; acc: 0.7
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.86; acc: 0.73
Batch: 400; loss: 0.75; acc: 0.88
Batch: 420; loss: 1.05; acc: 0.7
Batch: 440; loss: 0.77; acc: 0.8
Batch: 460; loss: 0.85; acc: 0.77
Batch: 480; loss: 1.01; acc: 0.72
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 1.02; acc: 0.62
Batch: 540; loss: 1.02; acc: 0.7
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 0.99; acc: 0.7
Batch: 600; loss: 0.86; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.81
Batch: 640; loss: 1.0; acc: 0.72
Batch: 660; loss: 1.08; acc: 0.69
Batch: 680; loss: 0.91; acc: 0.72
Batch: 700; loss: 0.93; acc: 0.72
Batch: 720; loss: 0.99; acc: 0.72
Batch: 740; loss: 1.15; acc: 0.67
Batch: 760; loss: 1.0; acc: 0.66
Batch: 780; loss: 0.97; acc: 0.78
Train Epoch over. train_loss: 0.94; train_accuracy: 0.73 

4.958079080097377e-05
1.8982305846293457e-05
Batch: 0; loss: 0.68; acc: 0.88
Batch: 20; loss: 1.03; acc: 0.69
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.8; acc: 0.72
Batch: 100; loss: 0.88; acc: 0.73
Batch: 120; loss: 1.09; acc: 0.67
Batch: 140; loss: 0.62; acc: 0.88
Val Epoch over. val_loss: 0.8813617407895957; val_accuracy: 0.754578025477707 

The current subspace-distance is: 1.8982305846293457e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_1_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 13.5

The number of parameters is: 279504

The number of individual parameters is:

108
432
108
108
162
52488
162
162
324
157464
324
324
64
62208
64
64
4096
64
640
10
64
64

nonzero elements in E: 139751988
elements in E: 139752000
fraction nonzero: 0.9999999141336081
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.41; acc: 0.16
Batch: 20; loss: 1.98; acc: 0.33
Batch: 40; loss: 2.03; acc: 0.28
Batch: 60; loss: 1.89; acc: 0.45
Batch: 80; loss: 1.8; acc: 0.41
Batch: 100; loss: 1.75; acc: 0.53
Batch: 120; loss: 1.7; acc: 0.48
Batch: 140; loss: 1.8; acc: 0.42
Batch: 160; loss: 1.59; acc: 0.64
Batch: 180; loss: 1.56; acc: 0.58
Batch: 200; loss: 1.64; acc: 0.5
Batch: 220; loss: 1.45; acc: 0.59
Batch: 240; loss: 1.39; acc: 0.64
Batch: 260; loss: 1.43; acc: 0.64
Batch: 280; loss: 1.52; acc: 0.53
Batch: 300; loss: 1.38; acc: 0.59
Batch: 320; loss: 1.45; acc: 0.58
Batch: 340; loss: 1.35; acc: 0.69
Batch: 360; loss: 1.33; acc: 0.64
Batch: 380; loss: 1.42; acc: 0.62
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.53; acc: 0.48
Batch: 440; loss: 1.47; acc: 0.55
Batch: 460; loss: 1.52; acc: 0.59
Batch: 480; loss: 1.45; acc: 0.58
Batch: 500; loss: 1.33; acc: 0.64
Batch: 520; loss: 1.5; acc: 0.56
Batch: 540; loss: 1.54; acc: 0.48
Batch: 560; loss: 1.29; acc: 0.64
Batch: 580; loss: 1.48; acc: 0.61
Batch: 600; loss: 1.39; acc: 0.61
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.44; acc: 0.55
Batch: 660; loss: 1.53; acc: 0.55
Batch: 680; loss: 1.18; acc: 0.7
Batch: 700; loss: 1.27; acc: 0.66
Batch: 720; loss: 1.2; acc: 0.69
Batch: 740; loss: 1.44; acc: 0.58
Batch: 760; loss: 1.43; acc: 0.62
Batch: 780; loss: 1.41; acc: 0.56
Train Epoch over. train_loss: 1.51; train_accuracy: 0.56 

2.9212744266260415e-05
7.122814167814795e-06
Batch: 0; loss: 1.24; acc: 0.73
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.17; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.66
Batch: 80; loss: 1.21; acc: 0.67
Batch: 100; loss: 1.37; acc: 0.61
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.24; acc: 0.7
Val Epoch over. val_loss: 1.3937901030680178; val_accuracy: 0.6057921974522293 

The current subspace-distance is: 7.122814167814795e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.75
Batch: 20; loss: 1.28; acc: 0.7
Batch: 40; loss: 1.35; acc: 0.55
Batch: 60; loss: 1.15; acc: 0.64
Batch: 80; loss: 1.41; acc: 0.58
Batch: 100; loss: 1.45; acc: 0.56
Batch: 120; loss: 1.34; acc: 0.67
Batch: 140; loss: 1.33; acc: 0.62
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.3; acc: 0.66
Batch: 200; loss: 1.44; acc: 0.58
Batch: 220; loss: 1.14; acc: 0.7
Batch: 240; loss: 1.23; acc: 0.61
Batch: 260; loss: 1.18; acc: 0.69
Batch: 280; loss: 1.26; acc: 0.64
Batch: 300; loss: 1.19; acc: 0.66
Batch: 320; loss: 1.11; acc: 0.77
Batch: 340; loss: 1.27; acc: 0.66
Batch: 360; loss: 1.13; acc: 0.72
Batch: 380; loss: 1.29; acc: 0.66
Batch: 400; loss: 1.22; acc: 0.66
Batch: 420; loss: 1.23; acc: 0.64
Batch: 440; loss: 1.4; acc: 0.55
Batch: 460; loss: 1.37; acc: 0.62
Batch: 480; loss: 1.32; acc: 0.62
Batch: 500; loss: 1.04; acc: 0.73
Batch: 520; loss: 0.97; acc: 0.75
Batch: 540; loss: 1.23; acc: 0.67
Batch: 560; loss: 1.16; acc: 0.77
Batch: 580; loss: 1.11; acc: 0.66
Batch: 600; loss: 1.12; acc: 0.69
Batch: 620; loss: 1.35; acc: 0.62
Batch: 640; loss: 1.18; acc: 0.66
Batch: 660; loss: 1.06; acc: 0.66
Batch: 680; loss: 1.22; acc: 0.62
Batch: 700; loss: 1.11; acc: 0.66
Batch: 720; loss: 1.12; acc: 0.67
Batch: 740; loss: 1.36; acc: 0.61
Batch: 760; loss: 1.36; acc: 0.66
Batch: 780; loss: 1.14; acc: 0.66
Train Epoch over. train_loss: 1.21; train_accuracy: 0.67 

3.317323717055842e-05
9.617396244721022e-06
Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.59; acc: 0.55
Batch: 40; loss: 1.25; acc: 0.52
Batch: 60; loss: 1.23; acc: 0.61
Batch: 80; loss: 1.4; acc: 0.5
Batch: 100; loss: 1.4; acc: 0.56
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 1.24; acc: 0.66
Val Epoch over. val_loss: 1.4270699464591445; val_accuracy: 0.5451831210191083 

The current subspace-distance is: 9.617396244721022e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.64
Batch: 20; loss: 1.03; acc: 0.75
Batch: 40; loss: 1.04; acc: 0.8
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.18; acc: 0.69
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 1.26; acc: 0.62
Batch: 160; loss: 1.09; acc: 0.77
Batch: 180; loss: 1.12; acc: 0.73
Batch: 200; loss: 1.01; acc: 0.73
Batch: 220; loss: 1.01; acc: 0.73
Batch: 240; loss: 1.21; acc: 0.69
Batch: 260; loss: 1.06; acc: 0.7
Batch: 280; loss: 1.17; acc: 0.67
Batch: 300; loss: 1.13; acc: 0.67
Batch: 320; loss: 1.09; acc: 0.67
Batch: 340; loss: 0.91; acc: 0.8
Batch: 360; loss: 1.13; acc: 0.69
Batch: 380; loss: 0.89; acc: 0.77
Batch: 400; loss: 1.07; acc: 0.72
Batch: 420; loss: 1.24; acc: 0.55
Batch: 440; loss: 0.85; acc: 0.8
Batch: 460; loss: 0.99; acc: 0.7
Batch: 480; loss: 0.97; acc: 0.78
Batch: 500; loss: 1.07; acc: 0.69
Batch: 520; loss: 1.22; acc: 0.66
Batch: 540; loss: 1.17; acc: 0.59
Batch: 560; loss: 1.03; acc: 0.78
Batch: 580; loss: 1.04; acc: 0.78
Batch: 600; loss: 1.1; acc: 0.77
Batch: 620; loss: 1.33; acc: 0.61
Batch: 640; loss: 1.16; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.64
Batch: 680; loss: 0.97; acc: 0.8
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 1.11; acc: 0.67
Batch: 760; loss: 0.9; acc: 0.78
Batch: 780; loss: 1.04; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.7 

3.476283382042311e-05
1.1577289114939049e-05
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.41; acc: 0.55
Batch: 40; loss: 1.18; acc: 0.58
Batch: 60; loss: 1.17; acc: 0.59
Batch: 80; loss: 1.22; acc: 0.52
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 1.19; acc: 0.69
Val Epoch over. val_loss: 1.3125015698420774; val_accuracy: 0.5692675159235668 

The current subspace-distance is: 1.1577289114939049e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.73
Batch: 20; loss: 1.24; acc: 0.66
Batch: 40; loss: 1.16; acc: 0.62
Batch: 60; loss: 0.96; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.69
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.05; acc: 0.72
Batch: 140; loss: 1.0; acc: 0.7
Batch: 160; loss: 1.1; acc: 0.66
Batch: 180; loss: 1.05; acc: 0.69
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 0.92; acc: 0.73
Batch: 240; loss: 0.89; acc: 0.84
Batch: 260; loss: 1.11; acc: 0.67
Batch: 280; loss: 0.98; acc: 0.73
Batch: 300; loss: 1.03; acc: 0.78
Batch: 320; loss: 1.25; acc: 0.58
Batch: 340; loss: 1.23; acc: 0.62
Batch: 360; loss: 0.82; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.84
Batch: 400; loss: 1.06; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 1.13; acc: 0.7
Batch: 460; loss: 1.04; acc: 0.78
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 1.0; acc: 0.77
Batch: 520; loss: 1.13; acc: 0.67
Batch: 540; loss: 1.02; acc: 0.7
Batch: 560; loss: 1.24; acc: 0.67
Batch: 580; loss: 0.79; acc: 0.84
Batch: 600; loss: 0.96; acc: 0.78
Batch: 620; loss: 0.92; acc: 0.72
Batch: 640; loss: 0.96; acc: 0.73
Batch: 660; loss: 0.92; acc: 0.75
Batch: 680; loss: 1.14; acc: 0.61
Batch: 700; loss: 1.02; acc: 0.77
Batch: 720; loss: 1.05; acc: 0.67
Batch: 740; loss: 1.03; acc: 0.69
Batch: 760; loss: 1.07; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.83
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

3.651819861261174e-05
1.2157806850154884e-05
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.12; acc: 0.56
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.12; acc: 0.67
Batch: 100; loss: 1.16; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 1.22; acc: 0.66
Val Epoch over. val_loss: 1.3240627263002336; val_accuracy: 0.5804140127388535 

The current subspace-distance is: 1.2157806850154884e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.72
Batch: 20; loss: 0.97; acc: 0.72
Batch: 40; loss: 0.95; acc: 0.8
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.7
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 1.0; acc: 0.77
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 0.99; acc: 0.67
Batch: 200; loss: 0.96; acc: 0.75
Batch: 220; loss: 1.05; acc: 0.67
Batch: 240; loss: 0.99; acc: 0.75
Batch: 260; loss: 0.84; acc: 0.8
Batch: 280; loss: 1.01; acc: 0.72
Batch: 300; loss: 0.95; acc: 0.72
Batch: 320; loss: 0.94; acc: 0.72
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.99; acc: 0.7
Batch: 380; loss: 0.94; acc: 0.75
Batch: 400; loss: 0.81; acc: 0.8
Batch: 420; loss: 0.98; acc: 0.77
Batch: 440; loss: 0.98; acc: 0.8
Batch: 460; loss: 0.86; acc: 0.8
Batch: 480; loss: 0.93; acc: 0.83
Batch: 500; loss: 1.02; acc: 0.73
Batch: 520; loss: 1.04; acc: 0.69
Batch: 540; loss: 0.89; acc: 0.75
Batch: 560; loss: 0.85; acc: 0.83
Batch: 580; loss: 0.81; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.78
Batch: 620; loss: 0.99; acc: 0.73
Batch: 640; loss: 0.97; acc: 0.81
Batch: 660; loss: 1.06; acc: 0.72
Batch: 680; loss: 0.9; acc: 0.73
Batch: 700; loss: 1.07; acc: 0.69
Batch: 720; loss: 0.82; acc: 0.8
Batch: 740; loss: 0.81; acc: 0.81
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.95; acc: 0.77
Train Epoch over. train_loss: 0.99; train_accuracy: 0.73 

3.8321690226439387e-05
1.3798985492030624e-05
Batch: 0; loss: 1.13; acc: 0.62
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.21; acc: 0.56
Batch: 60; loss: 1.35; acc: 0.56
Batch: 80; loss: 1.52; acc: 0.5
Batch: 100; loss: 1.32; acc: 0.58
Batch: 120; loss: 1.2; acc: 0.56
Batch: 140; loss: 1.27; acc: 0.59
Val Epoch over. val_loss: 1.4441401187781315; val_accuracy: 0.5372213375796179 

The current subspace-distance is: 1.3798985492030624e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.73
Batch: 20; loss: 0.92; acc: 0.81
Batch: 40; loss: 1.06; acc: 0.73
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.95; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.8
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 1.01; acc: 0.72
Batch: 160; loss: 0.96; acc: 0.73
Batch: 180; loss: 0.93; acc: 0.81
Batch: 200; loss: 0.87; acc: 0.73
Batch: 220; loss: 1.09; acc: 0.64
Batch: 240; loss: 1.09; acc: 0.69
Batch: 260; loss: 1.01; acc: 0.64
Batch: 280; loss: 0.89; acc: 0.83
Batch: 300; loss: 1.11; acc: 0.66
Batch: 320; loss: 1.22; acc: 0.61
Batch: 340; loss: 0.98; acc: 0.72
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.77; acc: 0.88
Batch: 400; loss: 0.91; acc: 0.77
Batch: 420; loss: 0.77; acc: 0.78
Batch: 440; loss: 1.05; acc: 0.62
Batch: 460; loss: 1.01; acc: 0.72
Batch: 480; loss: 0.96; acc: 0.7
Batch: 500; loss: 1.18; acc: 0.62
Batch: 520; loss: 0.82; acc: 0.81
Batch: 540; loss: 0.84; acc: 0.81
Batch: 560; loss: 0.91; acc: 0.8
Batch: 580; loss: 1.0; acc: 0.75
Batch: 600; loss: 0.92; acc: 0.77
Batch: 620; loss: 0.82; acc: 0.77
Batch: 640; loss: 1.09; acc: 0.7
Batch: 660; loss: 0.94; acc: 0.78
Batch: 680; loss: 0.87; acc: 0.77
Batch: 700; loss: 0.83; acc: 0.81
Batch: 720; loss: 1.13; acc: 0.62
Batch: 740; loss: 0.91; acc: 0.78
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 1.09; acc: 0.66
Train Epoch over. train_loss: 0.95; train_accuracy: 0.73 

4.036831887788139e-05
1.3235887308837846e-05
Batch: 0; loss: 1.04; acc: 0.67
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.22; acc: 0.59
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.12; acc: 0.59
Batch: 140; loss: 0.94; acc: 0.73
Val Epoch over. val_loss: 1.2879156436130499; val_accuracy: 0.629578025477707 

The current subspace-distance is: 1.3235887308837846e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.7
Batch: 20; loss: 1.03; acc: 0.69
Batch: 40; loss: 0.87; acc: 0.77
Batch: 60; loss: 1.05; acc: 0.66
Batch: 80; loss: 1.0; acc: 0.73
Batch: 100; loss: 0.89; acc: 0.77
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.86; acc: 0.78
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.67
Batch: 200; loss: 0.94; acc: 0.72
Batch: 220; loss: 0.82; acc: 0.78
Batch: 240; loss: 0.9; acc: 0.81
Batch: 260; loss: 0.95; acc: 0.72
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.83
Batch: 320; loss: 0.89; acc: 0.78
Batch: 340; loss: 0.87; acc: 0.8
Batch: 360; loss: 0.88; acc: 0.75
Batch: 380; loss: 0.92; acc: 0.72
Batch: 400; loss: 0.97; acc: 0.7
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.86; acc: 0.8
Batch: 460; loss: 0.96; acc: 0.77
Batch: 480; loss: 0.83; acc: 0.78
Batch: 500; loss: 0.77; acc: 0.81
Batch: 520; loss: 1.01; acc: 0.73
Batch: 540; loss: 0.94; acc: 0.7
Batch: 560; loss: 0.85; acc: 0.83
Batch: 580; loss: 0.93; acc: 0.75
Batch: 600; loss: 0.92; acc: 0.7
Batch: 620; loss: 0.99; acc: 0.75
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.92; acc: 0.72
Batch: 680; loss: 0.97; acc: 0.69
Batch: 700; loss: 0.92; acc: 0.77
Batch: 720; loss: 0.99; acc: 0.7
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.8; acc: 0.77
Batch: 780; loss: 0.85; acc: 0.77
Train Epoch over. train_loss: 0.93; train_accuracy: 0.74 

4.119220466236584e-05
1.5443123629665934e-05
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.69; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.79; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.84
Val Epoch over. val_loss: 0.9098243686803587; val_accuracy: 0.7391520700636943 

The current subspace-distance is: 1.5443123629665934e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.8
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.9; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.58
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.85; acc: 0.8
Batch: 160; loss: 0.99; acc: 0.72
Batch: 180; loss: 1.0; acc: 0.72
Batch: 200; loss: 0.89; acc: 0.75
Batch: 220; loss: 0.92; acc: 0.78
Batch: 240; loss: 0.77; acc: 0.84
Batch: 260; loss: 1.11; acc: 0.67
Batch: 280; loss: 0.84; acc: 0.81
Batch: 300; loss: 0.83; acc: 0.81
Batch: 320; loss: 1.0; acc: 0.72
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.87; acc: 0.75
Batch: 380; loss: 0.98; acc: 0.73
Batch: 400; loss: 0.96; acc: 0.72
Batch: 420; loss: 0.91; acc: 0.75
Batch: 440; loss: 0.83; acc: 0.73
Batch: 460; loss: 0.88; acc: 0.73
Batch: 480; loss: 0.97; acc: 0.72
Batch: 500; loss: 0.9; acc: 0.77
Batch: 520; loss: 1.11; acc: 0.67
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.83; acc: 0.77
Batch: 580; loss: 1.01; acc: 0.66
Batch: 600; loss: 0.82; acc: 0.78
Batch: 620; loss: 0.88; acc: 0.8
Batch: 640; loss: 0.94; acc: 0.7
Batch: 660; loss: 0.96; acc: 0.78
Batch: 680; loss: 0.97; acc: 0.75
Batch: 700; loss: 0.79; acc: 0.8
Batch: 720; loss: 0.8; acc: 0.77
Batch: 740; loss: 1.05; acc: 0.7
Batch: 760; loss: 0.92; acc: 0.77
Batch: 780; loss: 0.93; acc: 0.78
Train Epoch over. train_loss: 0.91; train_accuracy: 0.75 

4.338656799518503e-05
1.757909922162071e-05
Batch: 0; loss: 0.97; acc: 0.69
Batch: 20; loss: 1.45; acc: 0.59
Batch: 40; loss: 1.03; acc: 0.66
Batch: 60; loss: 1.16; acc: 0.62
Batch: 80; loss: 1.22; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 0.94; acc: 0.7
Val Epoch over. val_loss: 1.1634673168704768; val_accuracy: 0.6263933121019108 

The current subspace-distance is: 1.757909922162071e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.66
Batch: 40; loss: 0.75; acc: 0.77
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.92; acc: 0.72
Batch: 160; loss: 0.96; acc: 0.66
Batch: 180; loss: 0.97; acc: 0.69
Batch: 200; loss: 0.94; acc: 0.69
Batch: 220; loss: 0.97; acc: 0.75
Batch: 240; loss: 0.73; acc: 0.77
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.88; acc: 0.77
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.75
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 0.9; acc: 0.73
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 0.73; acc: 0.84
Batch: 420; loss: 0.92; acc: 0.75
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 0.85; acc: 0.72
Batch: 500; loss: 1.02; acc: 0.69
Batch: 520; loss: 0.96; acc: 0.72
Batch: 540; loss: 0.74; acc: 0.83
Batch: 560; loss: 0.84; acc: 0.77
Batch: 580; loss: 0.83; acc: 0.8
Batch: 600; loss: 1.13; acc: 0.67
Batch: 620; loss: 0.89; acc: 0.8
Batch: 640; loss: 1.13; acc: 0.67
Batch: 660; loss: 0.94; acc: 0.7
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.84; acc: 0.75
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 0.93; acc: 0.69
Batch: 760; loss: 0.79; acc: 0.78
Batch: 780; loss: 0.96; acc: 0.73
Train Epoch over. train_loss: 0.89; train_accuracy: 0.75 

4.399899989948608e-05
1.612022242625244e-05
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.67
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.9; acc: 0.69
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.89; acc: 0.66
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.91
Val Epoch over. val_loss: 0.9198009804555565; val_accuracy: 0.7312898089171974 

The current subspace-distance is: 1.612022242625244e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.72
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.84; acc: 0.75
Batch: 60; loss: 0.98; acc: 0.67
Batch: 80; loss: 1.09; acc: 0.7
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.77; acc: 0.78
Batch: 160; loss: 0.88; acc: 0.72
Batch: 180; loss: 1.15; acc: 0.59
Batch: 200; loss: 0.98; acc: 0.73
Batch: 220; loss: 1.1; acc: 0.66
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 0.91; acc: 0.75
Batch: 280; loss: 0.75; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.75
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.85; acc: 0.81
Batch: 360; loss: 0.88; acc: 0.83
Batch: 380; loss: 0.98; acc: 0.7
Batch: 400; loss: 0.9; acc: 0.78
Batch: 420; loss: 0.94; acc: 0.73
Batch: 440; loss: 0.81; acc: 0.77
Batch: 460; loss: 0.79; acc: 0.81
Batch: 480; loss: 0.79; acc: 0.75
Batch: 500; loss: 0.84; acc: 0.69
Batch: 520; loss: 0.79; acc: 0.75
Batch: 540; loss: 1.03; acc: 0.7
Batch: 560; loss: 1.05; acc: 0.7
Batch: 580; loss: 0.8; acc: 0.8
Batch: 600; loss: 1.03; acc: 0.7
Batch: 620; loss: 1.02; acc: 0.67
Batch: 640; loss: 0.95; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.85; acc: 0.77
Batch: 700; loss: 1.01; acc: 0.69
Batch: 720; loss: 0.91; acc: 0.72
Batch: 740; loss: 0.8; acc: 0.75
Batch: 760; loss: 0.88; acc: 0.75
Batch: 780; loss: 1.05; acc: 0.69
Train Epoch over. train_loss: 0.88; train_accuracy: 0.75 

4.5168872020440176e-05
1.716030965326354e-05
Batch: 0; loss: 0.98; acc: 0.66
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 0.98; acc: 0.72
Batch: 60; loss: 1.29; acc: 0.59
Batch: 80; loss: 1.36; acc: 0.52
Batch: 100; loss: 1.17; acc: 0.59
Batch: 120; loss: 1.23; acc: 0.66
Batch: 140; loss: 1.35; acc: 0.5
Val Epoch over. val_loss: 1.349572015795738; val_accuracy: 0.5594148089171974 

The current subspace-distance is: 1.716030965326354e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.92; acc: 0.69
Batch: 20; loss: 0.77; acc: 0.75
Batch: 40; loss: 0.9; acc: 0.73
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 1.18; acc: 0.62
Batch: 100; loss: 0.89; acc: 0.72
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.84; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.84; acc: 0.75
Batch: 200; loss: 0.78; acc: 0.81
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.94; acc: 0.77
Batch: 260; loss: 0.83; acc: 0.73
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.98; acc: 0.72
Batch: 340; loss: 0.82; acc: 0.78
Batch: 360; loss: 0.87; acc: 0.78
Batch: 380; loss: 1.0; acc: 0.69
Batch: 400; loss: 0.82; acc: 0.75
Batch: 420; loss: 0.71; acc: 0.88
Batch: 440; loss: 0.76; acc: 0.84
Batch: 460; loss: 1.09; acc: 0.66
Batch: 480; loss: 0.8; acc: 0.77
Batch: 500; loss: 0.82; acc: 0.8
Batch: 520; loss: 0.82; acc: 0.78
Batch: 540; loss: 0.92; acc: 0.7
Batch: 560; loss: 1.03; acc: 0.7
Batch: 580; loss: 0.65; acc: 0.86
Batch: 600; loss: 0.81; acc: 0.75
Batch: 620; loss: 0.89; acc: 0.8
Batch: 640; loss: 0.88; acc: 0.84
Batch: 660; loss: 0.83; acc: 0.72
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.87; acc: 0.72
Batch: 720; loss: 0.65; acc: 0.89
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 1.0; acc: 0.7
Batch: 780; loss: 0.82; acc: 0.72
Train Epoch over. train_loss: 0.85; train_accuracy: 0.76 

4.627234011422843e-05
1.6941918147495016e-05
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.95; acc: 0.67
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.6; acc: 0.84
Val Epoch over. val_loss: 0.814368617003131; val_accuracy: 0.7794585987261147 

The current subspace-distance is: 1.6941918147495016e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.93; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 1.09; acc: 0.69
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.75
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.77
Batch: 220; loss: 0.91; acc: 0.69
Batch: 240; loss: 0.64; acc: 0.81
Batch: 260; loss: 0.98; acc: 0.7
Batch: 280; loss: 0.8; acc: 0.77
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.88; acc: 0.75
Batch: 340; loss: 0.72; acc: 0.8
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.95; acc: 0.67
Batch: 400; loss: 0.87; acc: 0.73
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.97; acc: 0.73
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.85; acc: 0.8
Batch: 500; loss: 0.93; acc: 0.69
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.84; acc: 0.75
Batch: 580; loss: 1.01; acc: 0.7
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.79; acc: 0.84
Batch: 640; loss: 0.89; acc: 0.77
Batch: 660; loss: 0.8; acc: 0.8
Batch: 680; loss: 0.85; acc: 0.73
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.94; acc: 0.72
Batch: 740; loss: 0.86; acc: 0.7
Batch: 760; loss: 0.89; acc: 0.73
Batch: 780; loss: 0.79; acc: 0.73
Train Epoch over. train_loss: 0.84; train_accuracy: 0.76 

4.638629252440296e-05
1.8270548025611788e-05
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 1.02; acc: 0.69
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.86; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.59; acc: 0.84
Val Epoch over. val_loss: 0.8085664260159632; val_accuracy: 0.7759753184713376 

The current subspace-distance is: 1.8270548025611788e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 1.15; acc: 0.67
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.8
Batch: 160; loss: 1.05; acc: 0.72
Batch: 180; loss: 0.75; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.92; acc: 0.75
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.81; acc: 0.72
Batch: 280; loss: 1.0; acc: 0.69
Batch: 300; loss: 0.91; acc: 0.72
Batch: 320; loss: 0.91; acc: 0.75
Batch: 340; loss: 0.83; acc: 0.84
Batch: 360; loss: 0.88; acc: 0.73
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.85; acc: 0.77
Batch: 420; loss: 0.93; acc: 0.73
Batch: 440; loss: 0.81; acc: 0.8
Batch: 460; loss: 0.95; acc: 0.77
Batch: 480; loss: 0.84; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.78
Batch: 520; loss: 0.81; acc: 0.75
Batch: 540; loss: 0.79; acc: 0.8
Batch: 560; loss: 0.95; acc: 0.72
Batch: 580; loss: 0.98; acc: 0.72
Batch: 600; loss: 0.86; acc: 0.75
Batch: 620; loss: 0.96; acc: 0.7
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.87; acc: 0.73
Batch: 700; loss: 0.76; acc: 0.84
Batch: 720; loss: 1.04; acc: 0.75
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.87; acc: 0.75
Batch: 780; loss: 0.85; acc: 0.69
Train Epoch over. train_loss: 0.84; train_accuracy: 0.76 

4.6594639570685104e-05
1.8356589862378314e-05
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.96; acc: 0.7
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.56; acc: 0.88
Val Epoch over. val_loss: 0.8048287812311938; val_accuracy: 0.78015525477707 

The current subspace-distance is: 1.8356589862378314e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.94; acc: 0.67
Batch: 20; loss: 0.9; acc: 0.67
Batch: 40; loss: 0.79; acc: 0.7
Batch: 60; loss: 0.95; acc: 0.73
Batch: 80; loss: 0.85; acc: 0.69
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.73; acc: 0.77
Batch: 160; loss: 0.62; acc: 0.86
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.75; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.78
Batch: 240; loss: 0.95; acc: 0.72
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.8; acc: 0.77
Batch: 320; loss: 0.85; acc: 0.78
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.87; acc: 0.77
Batch: 400; loss: 0.93; acc: 0.73
Batch: 420; loss: 0.76; acc: 0.77
Batch: 440; loss: 1.05; acc: 0.67
Batch: 460; loss: 0.77; acc: 0.78
Batch: 480; loss: 0.93; acc: 0.77
Batch: 500; loss: 0.84; acc: 0.84
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.91; acc: 0.67
Batch: 600; loss: 0.69; acc: 0.78
Batch: 620; loss: 0.92; acc: 0.69
Batch: 640; loss: 0.98; acc: 0.67
Batch: 660; loss: 0.98; acc: 0.75
Batch: 680; loss: 0.86; acc: 0.78
Batch: 700; loss: 0.86; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.73
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.83; acc: 0.78
Train Epoch over. train_loss: 0.83; train_accuracy: 0.77 

4.725758481072262e-05
1.8454982637194917e-05
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.89; acc: 0.7
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.85; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.86
Val Epoch over. val_loss: 0.8734597529575323; val_accuracy: 0.7487062101910829 

The current subspace-distance is: 1.8454982637194917e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.69
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.98; acc: 0.7
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.91
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.72; acc: 0.86
Batch: 180; loss: 0.9; acc: 0.7
Batch: 200; loss: 0.86; acc: 0.75
Batch: 220; loss: 0.94; acc: 0.73
Batch: 240; loss: 0.76; acc: 0.77
Batch: 260; loss: 0.74; acc: 0.8
Batch: 280; loss: 0.8; acc: 0.78
Batch: 300; loss: 0.95; acc: 0.7
Batch: 320; loss: 0.73; acc: 0.72
Batch: 340; loss: 0.89; acc: 0.8
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.82; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.73; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.89
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.85; acc: 0.7
Batch: 500; loss: 1.17; acc: 0.61
Batch: 520; loss: 0.64; acc: 0.86
Batch: 540; loss: 0.9; acc: 0.72
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.89; acc: 0.69
Batch: 640; loss: 0.88; acc: 0.75
Batch: 660; loss: 0.97; acc: 0.67
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.98; acc: 0.75
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.75; acc: 0.78
Batch: 760; loss: 0.9; acc: 0.77
Batch: 780; loss: 0.81; acc: 0.77
Train Epoch over. train_loss: 0.83; train_accuracy: 0.77 

4.759510557050817e-05
1.9425136997597292e-05
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.95; acc: 0.72
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.8092777165258007; val_accuracy: 0.7719944267515924 

The current subspace-distance is: 1.9425136997597292e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.9; acc: 0.69
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.72
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.81; acc: 0.75
Batch: 180; loss: 0.82; acc: 0.78
Batch: 200; loss: 0.76; acc: 0.75
Batch: 220; loss: 0.77; acc: 0.83
Batch: 240; loss: 0.81; acc: 0.78
Batch: 260; loss: 0.96; acc: 0.64
Batch: 280; loss: 1.19; acc: 0.66
Batch: 300; loss: 0.81; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.72
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.69; acc: 0.83
Batch: 400; loss: 0.83; acc: 0.81
Batch: 420; loss: 0.89; acc: 0.75
Batch: 440; loss: 0.74; acc: 0.81
Batch: 460; loss: 0.81; acc: 0.78
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.8; acc: 0.72
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.92; acc: 0.78
Batch: 580; loss: 0.9; acc: 0.72
Batch: 600; loss: 0.79; acc: 0.8
Batch: 620; loss: 0.76; acc: 0.77
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.93; acc: 0.75
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.77; acc: 0.78
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 0.83; acc: 0.78
Batch: 760; loss: 0.79; acc: 0.77
Batch: 780; loss: 0.89; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.77 

4.830545003642328e-05
1.89967431651894e-05
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.88
Val Epoch over. val_loss: 0.7711512190141495; val_accuracy: 0.7835390127388535 

The current subspace-distance is: 1.89967431651894e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.84
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 1.0; acc: 0.67
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.83; acc: 0.72
Batch: 160; loss: 0.96; acc: 0.69
Batch: 180; loss: 0.82; acc: 0.72
Batch: 200; loss: 0.84; acc: 0.73
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.83; acc: 0.8
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.82; acc: 0.8
Batch: 300; loss: 0.82; acc: 0.7
Batch: 320; loss: 1.0; acc: 0.7
Batch: 340; loss: 0.69; acc: 0.88
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 1.03; acc: 0.69
Batch: 400; loss: 0.83; acc: 0.75
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.85; acc: 0.78
Batch: 460; loss: 0.72; acc: 0.77
Batch: 480; loss: 0.74; acc: 0.77
Batch: 500; loss: 0.89; acc: 0.75
Batch: 520; loss: 1.0; acc: 0.69
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.86; acc: 0.75
Batch: 580; loss: 0.91; acc: 0.7
Batch: 600; loss: 0.66; acc: 0.88
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 0.94; acc: 0.81
Batch: 660; loss: 0.78; acc: 0.8
Batch: 680; loss: 0.65; acc: 0.86
Batch: 700; loss: 0.86; acc: 0.7
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.8
Batch: 760; loss: 0.93; acc: 0.69
Batch: 780; loss: 0.94; acc: 0.67
Train Epoch over. train_loss: 0.82; train_accuracy: 0.77 

4.825805081054568e-05
1.8812099369824864e-05
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.89
Val Epoch over. val_loss: 0.7467584934583895; val_accuracy: 0.7953821656050956 

The current subspace-distance is: 1.8812099369824864e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 1.1; acc: 0.73
Batch: 40; loss: 0.75; acc: 0.75
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 0.87; acc: 0.75
Batch: 180; loss: 0.87; acc: 0.67
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.72
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.83; acc: 0.77
Batch: 300; loss: 0.9; acc: 0.7
Batch: 320; loss: 0.85; acc: 0.7
Batch: 340; loss: 0.78; acc: 0.84
Batch: 360; loss: 0.93; acc: 0.75
Batch: 380; loss: 0.74; acc: 0.75
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.75; acc: 0.78
Batch: 440; loss: 0.92; acc: 0.75
Batch: 460; loss: 0.9; acc: 0.67
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.75; acc: 0.8
Batch: 520; loss: 0.67; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 1.19; acc: 0.64
Batch: 580; loss: 0.77; acc: 0.84
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 0.94; acc: 0.73
Batch: 640; loss: 0.86; acc: 0.77
Batch: 660; loss: 0.9; acc: 0.77
Batch: 680; loss: 0.92; acc: 0.7
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.88; acc: 0.73
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.85; acc: 0.81
Batch: 780; loss: 0.95; acc: 0.69
Train Epoch over. train_loss: 0.81; train_accuracy: 0.77 

4.865101436735131e-05
1.853907451732084e-05
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 0.5; acc: 0.91
Val Epoch over. val_loss: 0.778098019065371; val_accuracy: 0.7809514331210191 

The current subspace-distance is: 1.853907451732084e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.73
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.81; acc: 0.8
Batch: 160; loss: 0.75; acc: 0.75
Batch: 180; loss: 0.78; acc: 0.77
Batch: 200; loss: 0.91; acc: 0.72
Batch: 220; loss: 0.99; acc: 0.75
Batch: 240; loss: 0.81; acc: 0.8
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.93; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.97; acc: 0.7
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.78; acc: 0.73
Batch: 380; loss: 0.9; acc: 0.67
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.87; acc: 0.78
Batch: 440; loss: 0.93; acc: 0.72
Batch: 460; loss: 0.79; acc: 0.81
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.89; acc: 0.77
Batch: 560; loss: 0.89; acc: 0.67
Batch: 580; loss: 0.83; acc: 0.75
Batch: 600; loss: 0.92; acc: 0.8
Batch: 620; loss: 0.78; acc: 0.78
Batch: 640; loss: 0.89; acc: 0.64
Batch: 660; loss: 0.84; acc: 0.81
Batch: 680; loss: 0.9; acc: 0.73
Batch: 700; loss: 0.81; acc: 0.73
Batch: 720; loss: 0.72; acc: 0.86
Batch: 740; loss: 1.03; acc: 0.7
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.81; train_accuracy: 0.77 

4.913984230370261e-05
1.9187858924851753e-05
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.77
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.49; acc: 0.89
Val Epoch over. val_loss: 0.7532632872936832; val_accuracy: 0.7937898089171974 

The current subspace-distance is: 1.9187858924851753e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.88; acc: 0.75
Batch: 20; loss: 0.94; acc: 0.69
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.89; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.67
Batch: 100; loss: 0.9; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.78; acc: 0.77
Batch: 160; loss: 0.75; acc: 0.88
Batch: 180; loss: 0.77; acc: 0.75
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.91; acc: 0.73
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.78
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.85; acc: 0.75
Batch: 340; loss: 0.8; acc: 0.77
Batch: 360; loss: 0.83; acc: 0.77
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.97; acc: 0.73
Batch: 420; loss: 1.15; acc: 0.62
Batch: 440; loss: 0.8; acc: 0.77
Batch: 460; loss: 1.04; acc: 0.69
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.97; acc: 0.69
Batch: 520; loss: 0.86; acc: 0.73
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.82; acc: 0.73
Batch: 580; loss: 0.86; acc: 0.69
Batch: 600; loss: 0.78; acc: 0.81
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.89; acc: 0.73
Batch: 660; loss: 0.85; acc: 0.78
Batch: 680; loss: 0.92; acc: 0.73
Batch: 700; loss: 0.84; acc: 0.78
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.89
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.81; train_accuracy: 0.77 

4.9284826673101634e-05
2.074664553219918e-05
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.73
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.49; acc: 0.89
Val Epoch over. val_loss: 0.7609889237744034; val_accuracy: 0.7881170382165605 

The current subspace-distance is: 2.074664553219918e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.89; acc: 0.73
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 0.88; acc: 0.72
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.8; acc: 0.73
Batch: 160; loss: 0.76; acc: 0.81
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.65; acc: 0.89
Batch: 220; loss: 1.02; acc: 0.64
Batch: 240; loss: 0.91; acc: 0.78
Batch: 260; loss: 0.82; acc: 0.77
Batch: 280; loss: 0.7; acc: 0.8
Batch: 300; loss: 0.87; acc: 0.78
Batch: 320; loss: 0.99; acc: 0.72
Batch: 340; loss: 0.9; acc: 0.72
Batch: 360; loss: 0.81; acc: 0.7
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.74; acc: 0.8
Batch: 420; loss: 0.95; acc: 0.73
Batch: 440; loss: 0.82; acc: 0.78
Batch: 460; loss: 0.76; acc: 0.8
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.87; acc: 0.8
Batch: 520; loss: 0.93; acc: 0.67
Batch: 540; loss: 0.9; acc: 0.8
Batch: 560; loss: 0.91; acc: 0.72
Batch: 580; loss: 0.87; acc: 0.78
Batch: 600; loss: 0.85; acc: 0.73
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.87; acc: 0.77
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 1.05; acc: 0.67
Batch: 760; loss: 0.77; acc: 0.8
Batch: 780; loss: 0.79; acc: 0.8
Train Epoch over. train_loss: 0.8; train_accuracy: 0.77 

4.9672129534883425e-05
2.0293158740969375e-05
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.88
Val Epoch over. val_loss: 0.7231978977182109; val_accuracy: 0.8010549363057324 

The current subspace-distance is: 2.0293158740969375e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.88; acc: 0.8
Batch: 40; loss: 0.91; acc: 0.72
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.84; acc: 0.8
Batch: 160; loss: 0.82; acc: 0.7
Batch: 180; loss: 1.06; acc: 0.66
Batch: 200; loss: 0.82; acc: 0.81
Batch: 220; loss: 0.77; acc: 0.75
Batch: 240; loss: 0.96; acc: 0.7
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.76; acc: 0.77
Batch: 340; loss: 0.81; acc: 0.77
Batch: 360; loss: 0.76; acc: 0.78
Batch: 380; loss: 1.01; acc: 0.66
Batch: 400; loss: 0.84; acc: 0.75
Batch: 420; loss: 0.82; acc: 0.7
Batch: 440; loss: 0.77; acc: 0.81
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.81; acc: 0.72
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.92; acc: 0.7
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.75; acc: 0.81
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.83; acc: 0.77
Batch: 640; loss: 0.92; acc: 0.73
Batch: 660; loss: 0.92; acc: 0.7
Batch: 680; loss: 0.96; acc: 0.7
Batch: 700; loss: 0.85; acc: 0.7
Batch: 720; loss: 0.85; acc: 0.77
Batch: 740; loss: 0.87; acc: 0.7
Batch: 760; loss: 0.76; acc: 0.72
Batch: 780; loss: 0.78; acc: 0.8
Train Epoch over. train_loss: 0.8; train_accuracy: 0.77 

4.905920650344342e-05
1.9500712369335815e-05
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.7281082860983101; val_accuracy: 0.8000597133757962 

The current subspace-distance is: 1.9500712369335815e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.77
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.89; acc: 0.73
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.81; acc: 0.8
Batch: 280; loss: 0.93; acc: 0.7
Batch: 300; loss: 0.89; acc: 0.7
Batch: 320; loss: 0.86; acc: 0.7
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.97; acc: 0.72
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.88; acc: 0.7
Batch: 420; loss: 0.88; acc: 0.7
Batch: 440; loss: 1.04; acc: 0.61
Batch: 460; loss: 0.74; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 0.88; acc: 0.78
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 0.81; acc: 0.77
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.73
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.83; acc: 0.8
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.85; acc: 0.7
Batch: 740; loss: 0.93; acc: 0.72
Batch: 760; loss: 1.01; acc: 0.66
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.8; train_accuracy: 0.77 

4.900802014162764e-05
1.8948119759443216e-05
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.89
Val Epoch over. val_loss: 0.7342841678364261; val_accuracy: 0.7947850318471338 

The current subspace-distance is: 1.8948119759443216e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.91; acc: 0.72
Batch: 60; loss: 0.87; acc: 0.72
Batch: 80; loss: 0.7; acc: 0.75
Batch: 100; loss: 0.83; acc: 0.73
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.87; acc: 0.73
Batch: 160; loss: 0.97; acc: 0.69
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.7; acc: 0.8
Batch: 300; loss: 0.92; acc: 0.73
Batch: 320; loss: 0.85; acc: 0.8
Batch: 340; loss: 0.81; acc: 0.78
Batch: 360; loss: 0.76; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.73
Batch: 400; loss: 0.9; acc: 0.8
Batch: 420; loss: 0.87; acc: 0.69
Batch: 440; loss: 0.84; acc: 0.77
Batch: 460; loss: 0.82; acc: 0.75
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.85; acc: 0.78
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 1.02; acc: 0.72
Batch: 600; loss: 0.92; acc: 0.7
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.67; acc: 0.88
Batch: 700; loss: 0.96; acc: 0.72
Batch: 720; loss: 0.78; acc: 0.81
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.8; train_accuracy: 0.77 

4.9850856157718226e-05
2.0277409930713475e-05
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.88; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.45; acc: 0.91
Val Epoch over. val_loss: 0.7227134013631541; val_accuracy: 0.7998606687898089 

The current subspace-distance is: 2.0277409930713475e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.83
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 0.98; acc: 0.7
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.95; acc: 0.73
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 1.06; acc: 0.69
Batch: 180; loss: 0.74; acc: 0.84
Batch: 200; loss: 0.85; acc: 0.7
Batch: 220; loss: 0.75; acc: 0.78
Batch: 240; loss: 0.85; acc: 0.8
Batch: 260; loss: 0.77; acc: 0.78
Batch: 280; loss: 0.85; acc: 0.72
Batch: 300; loss: 0.65; acc: 0.81
Batch: 320; loss: 0.78; acc: 0.72
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.91; acc: 0.77
Batch: 380; loss: 0.68; acc: 0.86
Batch: 400; loss: 0.78; acc: 0.78
Batch: 420; loss: 0.77; acc: 0.83
Batch: 440; loss: 0.91; acc: 0.7
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.73; acc: 0.8
Batch: 500; loss: 1.05; acc: 0.64
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.73
Batch: 580; loss: 0.76; acc: 0.75
Batch: 600; loss: 0.83; acc: 0.72
Batch: 620; loss: 0.72; acc: 0.83
Batch: 640; loss: 0.97; acc: 0.73
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 0.84; acc: 0.75
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 0.95; acc: 0.7
Batch: 740; loss: 0.68; acc: 0.86
Batch: 760; loss: 0.84; acc: 0.72
Batch: 780; loss: 0.79; acc: 0.73
Train Epoch over. train_loss: 0.8; train_accuracy: 0.77 

5.0447248213458806e-05
2.073575888061896e-05
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.43; acc: 0.91
Val Epoch over. val_loss: 0.7245581683459555; val_accuracy: 0.7998606687898089 

The current subspace-distance is: 2.073575888061896e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.83; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.74; acc: 0.84
Batch: 200; loss: 0.83; acc: 0.77
Batch: 220; loss: 0.83; acc: 0.75
Batch: 240; loss: 0.74; acc: 0.8
Batch: 260; loss: 0.83; acc: 0.77
Batch: 280; loss: 0.93; acc: 0.67
Batch: 300; loss: 0.86; acc: 0.72
Batch: 320; loss: 0.72; acc: 0.84
Batch: 340; loss: 0.82; acc: 0.77
Batch: 360; loss: 0.85; acc: 0.75
Batch: 380; loss: 0.68; acc: 0.89
Batch: 400; loss: 0.73; acc: 0.75
Batch: 420; loss: 0.79; acc: 0.78
Batch: 440; loss: 0.87; acc: 0.77
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.86; acc: 0.78
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.76; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.86
Batch: 560; loss: 0.87; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.73
Batch: 600; loss: 0.83; acc: 0.77
Batch: 620; loss: 0.76; acc: 0.75
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.84; acc: 0.73
Batch: 680; loss: 0.8; acc: 0.69
Batch: 700; loss: 0.89; acc: 0.7
Batch: 720; loss: 0.93; acc: 0.73
Batch: 740; loss: 0.59; acc: 0.91
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.79; train_accuracy: 0.78 

4.975866249878891e-05
1.975436316570267e-05
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.7
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.7252977184808937; val_accuracy: 0.806031050955414 

The current subspace-distance is: 1.975436316570267e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.94; acc: 0.7
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.85; acc: 0.75
Batch: 180; loss: 0.89; acc: 0.8
Batch: 200; loss: 0.68; acc: 0.86
Batch: 220; loss: 0.82; acc: 0.78
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.69; acc: 0.77
Batch: 280; loss: 0.58; acc: 0.91
Batch: 300; loss: 0.84; acc: 0.77
Batch: 320; loss: 0.74; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.74; acc: 0.8
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.83; acc: 0.81
Batch: 440; loss: 0.87; acc: 0.73
Batch: 460; loss: 0.9; acc: 0.67
Batch: 480; loss: 0.83; acc: 0.73
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 0.86; acc: 0.73
Batch: 580; loss: 0.72; acc: 0.84
Batch: 600; loss: 0.89; acc: 0.7
Batch: 620; loss: 0.75; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.78
Batch: 740; loss: 0.9; acc: 0.7
Batch: 760; loss: 0.8; acc: 0.73
Batch: 780; loss: 0.93; acc: 0.73
Train Epoch over. train_loss: 0.79; train_accuracy: 0.77 

4.9721853429218754e-05
2.0503634004853666e-05
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.7124002363271774; val_accuracy: 0.8043391719745223 

The current subspace-distance is: 2.0503634004853666e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.77; acc: 0.73
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.79; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 0.86; acc: 0.7
Batch: 160; loss: 0.77; acc: 0.83
Batch: 180; loss: 0.98; acc: 0.69
Batch: 200; loss: 0.74; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.9; acc: 0.75
Batch: 260; loss: 0.82; acc: 0.83
Batch: 280; loss: 0.8; acc: 0.75
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.66; acc: 0.84
Batch: 360; loss: 0.89; acc: 0.77
Batch: 380; loss: 0.92; acc: 0.69
Batch: 400; loss: 0.82; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.9; acc: 0.72
Batch: 460; loss: 0.68; acc: 0.81
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 0.94; acc: 0.78
Batch: 520; loss: 0.94; acc: 0.69
Batch: 540; loss: 0.9; acc: 0.7
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.85; acc: 0.75
Batch: 640; loss: 0.82; acc: 0.75
Batch: 660; loss: 0.91; acc: 0.72
Batch: 680; loss: 0.84; acc: 0.78
Batch: 700; loss: 0.91; acc: 0.73
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 0.88; acc: 0.72
Batch: 780; loss: 0.82; acc: 0.78
Train Epoch over. train_loss: 0.79; train_accuracy: 0.77 

4.954461473971605e-05
1.9244538634666242e-05
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.46; acc: 0.94
Val Epoch over. val_loss: 0.7317301581619652; val_accuracy: 0.8019506369426752 

The current subspace-distance is: 1.9244538634666242e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.8; acc: 0.75
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.68; acc: 0.88
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.93; acc: 0.69
Batch: 240; loss: 0.8; acc: 0.78
Batch: 260; loss: 0.81; acc: 0.8
Batch: 280; loss: 0.96; acc: 0.73
Batch: 300; loss: 0.74; acc: 0.77
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.91; acc: 0.75
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.76; acc: 0.78
Batch: 420; loss: 0.91; acc: 0.69
Batch: 440; loss: 0.8; acc: 0.77
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.8
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 0.73; acc: 0.75
Batch: 560; loss: 0.8; acc: 0.78
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.81; acc: 0.75
Batch: 620; loss: 0.81; acc: 0.77
Batch: 640; loss: 0.97; acc: 0.73
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.8; acc: 0.83
Batch: 720; loss: 0.95; acc: 0.72
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.87; acc: 0.73
Train Epoch over. train_loss: 0.79; train_accuracy: 0.77 

5.065530422143638e-05
2.037251579167787e-05
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.84; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.45; acc: 0.91
Val Epoch over. val_loss: 0.729787785346341; val_accuracy: 0.7972730891719745 

The current subspace-distance is: 2.037251579167787e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.72
Batch: 40; loss: 0.93; acc: 0.73
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.67
Batch: 120; loss: 0.82; acc: 0.69
Batch: 140; loss: 0.79; acc: 0.75
Batch: 160; loss: 0.76; acc: 0.75
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.82; acc: 0.86
Batch: 260; loss: 0.82; acc: 0.78
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.94; acc: 0.66
Batch: 320; loss: 0.74; acc: 0.77
Batch: 340; loss: 0.84; acc: 0.73
Batch: 360; loss: 0.93; acc: 0.8
Batch: 380; loss: 0.78; acc: 0.8
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 0.85; acc: 0.75
Batch: 440; loss: 0.73; acc: 0.78
Batch: 460; loss: 0.84; acc: 0.83
Batch: 480; loss: 0.9; acc: 0.72
Batch: 500; loss: 0.94; acc: 0.7
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 1.04; acc: 0.64
Batch: 560; loss: 0.75; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.72; acc: 0.81
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 0.78; acc: 0.8
Batch: 680; loss: 0.85; acc: 0.67
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.76; acc: 0.84
Batch: 740; loss: 1.07; acc: 0.69
Batch: 760; loss: 0.77; acc: 0.73
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.79; train_accuracy: 0.77 

4.939948848914355e-05
1.948030876519624e-05
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.89
Val Epoch over. val_loss: 0.7256802730499559; val_accuracy: 0.7967754777070064 

The current subspace-distance is: 1.948030876519624e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_1_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:47/N_1_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
