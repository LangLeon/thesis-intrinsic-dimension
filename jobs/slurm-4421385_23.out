model : table13slim
N : 7
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:58

Channel scaling factor: 1.75

The number of parameters is: 271389

The number of individual parameters is:

14
252
14
14
21
38220
21
21
42
114660
42
42
64
112896
64
64
4096
64
640
10
64
64

nonzero elements in E: 13569449
elements in E: 13569450
fraction nonzero: 0.9999999263050455
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.52; acc: 0.06
Batch: 20; loss: 2.41; acc: 0.09
Batch: 40; loss: 2.35; acc: 0.17
Batch: 60; loss: 2.4; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.16
Batch: 100; loss: 2.17; acc: 0.2
Batch: 120; loss: 2.18; acc: 0.16
Batch: 140; loss: 2.19; acc: 0.22
Batch: 160; loss: 2.23; acc: 0.23
Batch: 180; loss: 2.16; acc: 0.23
Batch: 200; loss: 2.11; acc: 0.22
Batch: 220; loss: 2.05; acc: 0.27
Batch: 240; loss: 2.01; acc: 0.28
Batch: 260; loss: 2.02; acc: 0.33
Batch: 280; loss: 2.04; acc: 0.3
Batch: 300; loss: 1.95; acc: 0.38
Batch: 320; loss: 1.98; acc: 0.41
Batch: 340; loss: 2.06; acc: 0.23
Batch: 360; loss: 2.01; acc: 0.3
Batch: 380; loss: 1.93; acc: 0.28
Batch: 400; loss: 1.96; acc: 0.34
Batch: 420; loss: 1.97; acc: 0.44
Batch: 440; loss: 2.02; acc: 0.33
Batch: 460; loss: 1.97; acc: 0.34
Batch: 480; loss: 1.89; acc: 0.34
Batch: 500; loss: 1.99; acc: 0.34
Batch: 520; loss: 2.01; acc: 0.28
Batch: 540; loss: 1.89; acc: 0.48
Batch: 560; loss: 1.94; acc: 0.38
Batch: 580; loss: 1.95; acc: 0.33
Batch: 600; loss: 1.9; acc: 0.44
Batch: 620; loss: 1.93; acc: 0.33
Batch: 640; loss: 1.96; acc: 0.39
Batch: 660; loss: 1.96; acc: 0.27
Batch: 680; loss: 1.94; acc: 0.39
Batch: 700; loss: 1.87; acc: 0.42
Batch: 720; loss: 1.94; acc: 0.28
Batch: 740; loss: 1.87; acc: 0.45
Batch: 760; loss: 1.96; acc: 0.41
Batch: 780; loss: 1.87; acc: 0.34
Train Epoch over. train_loss: 2.04; train_accuracy: 0.3 

2.4007278625504114e-05
4.51780169896665e-06
Batch: 0; loss: 1.86; acc: 0.47
Batch: 20; loss: 1.99; acc: 0.3
Batch: 40; loss: 1.72; acc: 0.47
Batch: 60; loss: 1.86; acc: 0.41
Batch: 80; loss: 1.86; acc: 0.38
Batch: 100; loss: 1.84; acc: 0.36
Batch: 120; loss: 1.88; acc: 0.41
Batch: 140; loss: 1.87; acc: 0.38
Val Epoch over. val_loss: 1.8636793247453727; val_accuracy: 0.41182324840764334 

The current subspace-distance is: 4.51780169896665e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.87; acc: 0.52
Batch: 20; loss: 1.95; acc: 0.31
Batch: 40; loss: 1.92; acc: 0.38
Batch: 60; loss: 1.82; acc: 0.45
Batch: 80; loss: 1.93; acc: 0.39
Batch: 100; loss: 1.89; acc: 0.42
Batch: 120; loss: 1.91; acc: 0.36
Batch: 140; loss: 1.89; acc: 0.45
Batch: 160; loss: 1.87; acc: 0.42
Batch: 180; loss: 1.74; acc: 0.52
Batch: 200; loss: 1.87; acc: 0.38
Batch: 220; loss: 1.8; acc: 0.48
Batch: 240; loss: 1.84; acc: 0.39
Batch: 260; loss: 1.81; acc: 0.39
Batch: 280; loss: 1.9; acc: 0.44
Batch: 300; loss: 1.86; acc: 0.34
Batch: 320; loss: 1.85; acc: 0.44
Batch: 340; loss: 1.84; acc: 0.38
Batch: 360; loss: 1.78; acc: 0.47
Batch: 380; loss: 1.86; acc: 0.39
Batch: 400; loss: 1.84; acc: 0.39
Batch: 420; loss: 2.02; acc: 0.25
Batch: 440; loss: 1.89; acc: 0.33
Batch: 460; loss: 1.84; acc: 0.53
Batch: 480; loss: 1.89; acc: 0.38
Batch: 500; loss: 1.8; acc: 0.47
Batch: 520; loss: 1.77; acc: 0.48
Batch: 540; loss: 1.74; acc: 0.53
Batch: 560; loss: 1.79; acc: 0.44
Batch: 580; loss: 1.9; acc: 0.3
Batch: 600; loss: 1.91; acc: 0.38
Batch: 620; loss: 1.9; acc: 0.39
Batch: 640; loss: 1.83; acc: 0.41
Batch: 660; loss: 1.85; acc: 0.44
Batch: 680; loss: 1.83; acc: 0.39
Batch: 700; loss: 1.77; acc: 0.38
Batch: 720; loss: 1.79; acc: 0.47
Batch: 740; loss: 1.86; acc: 0.44
Batch: 760; loss: 1.81; acc: 0.44
Batch: 780; loss: 1.86; acc: 0.44
Train Epoch over. train_loss: 1.86; train_accuracy: 0.42 

2.7405038053984754e-05
6.426193067454733e-06
Batch: 0; loss: 1.85; acc: 0.42
Batch: 20; loss: 1.97; acc: 0.33
Batch: 40; loss: 1.67; acc: 0.45
Batch: 60; loss: 1.81; acc: 0.45
Batch: 80; loss: 1.81; acc: 0.41
Batch: 100; loss: 1.84; acc: 0.39
Batch: 120; loss: 1.86; acc: 0.42
Batch: 140; loss: 1.76; acc: 0.41
Val Epoch over. val_loss: 1.8144311153205337; val_accuracy: 0.44307324840764334 

The current subspace-distance is: 6.426193067454733e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.96; acc: 0.3
Batch: 20; loss: 1.82; acc: 0.44
Batch: 40; loss: 1.86; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.44
Batch: 80; loss: 1.79; acc: 0.45
Batch: 100; loss: 1.8; acc: 0.39
Batch: 120; loss: 1.87; acc: 0.36
Batch: 140; loss: 1.93; acc: 0.27
Batch: 160; loss: 1.71; acc: 0.55
Batch: 180; loss: 1.86; acc: 0.38
Batch: 200; loss: 1.81; acc: 0.45
Batch: 220; loss: 1.82; acc: 0.42
Batch: 240; loss: 1.8; acc: 0.45
Batch: 260; loss: 1.79; acc: 0.56
Batch: 280; loss: 1.75; acc: 0.53
Batch: 300; loss: 1.8; acc: 0.5
Batch: 320; loss: 1.65; acc: 0.56
Batch: 340; loss: 1.74; acc: 0.5
Batch: 360; loss: 1.9; acc: 0.34
Batch: 380; loss: 1.79; acc: 0.47
Batch: 400; loss: 1.82; acc: 0.47
Batch: 420; loss: 1.73; acc: 0.47
Batch: 440; loss: 1.8; acc: 0.47
Batch: 460; loss: 1.77; acc: 0.53
Batch: 480; loss: 1.87; acc: 0.39
Batch: 500; loss: 1.84; acc: 0.44
Batch: 520; loss: 1.83; acc: 0.44
Batch: 540; loss: 1.75; acc: 0.41
Batch: 560; loss: 1.75; acc: 0.42
Batch: 580; loss: 1.84; acc: 0.44
Batch: 600; loss: 1.81; acc: 0.42
Batch: 620; loss: 1.86; acc: 0.41
Batch: 640; loss: 1.89; acc: 0.38
Batch: 660; loss: 1.85; acc: 0.44
Batch: 680; loss: 1.75; acc: 0.52
Batch: 700; loss: 1.79; acc: 0.48
Batch: 720; loss: 1.76; acc: 0.48
Batch: 740; loss: 1.76; acc: 0.48
Batch: 760; loss: 1.72; acc: 0.45
Batch: 780; loss: 1.75; acc: 0.52
Train Epoch over. train_loss: 1.81; train_accuracy: 0.45 

2.7940954169025645e-05
6.226151981536532e-06
Batch: 0; loss: 1.83; acc: 0.39
Batch: 20; loss: 1.97; acc: 0.41
Batch: 40; loss: 1.61; acc: 0.56
Batch: 60; loss: 1.74; acc: 0.53
Batch: 80; loss: 1.73; acc: 0.5
Batch: 100; loss: 1.81; acc: 0.38
Batch: 120; loss: 1.85; acc: 0.39
Batch: 140; loss: 1.69; acc: 0.53
Val Epoch over. val_loss: 1.7699019962055669; val_accuracy: 0.4885549363057325 

The current subspace-distance is: 6.226151981536532e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.38
Batch: 40; loss: 1.77; acc: 0.53
Batch: 60; loss: 1.86; acc: 0.42
Batch: 80; loss: 1.77; acc: 0.45
Batch: 100; loss: 1.83; acc: 0.45
Batch: 120; loss: 1.85; acc: 0.34
Batch: 140; loss: 1.7; acc: 0.53
Batch: 160; loss: 1.72; acc: 0.47
Batch: 180; loss: 1.77; acc: 0.45
Batch: 200; loss: 1.72; acc: 0.48
Batch: 220; loss: 1.91; acc: 0.34
Batch: 240; loss: 1.74; acc: 0.59
Batch: 260; loss: 1.83; acc: 0.47
Batch: 280; loss: 1.85; acc: 0.45
Batch: 300; loss: 1.82; acc: 0.42
Batch: 320; loss: 1.82; acc: 0.36
Batch: 340; loss: 1.81; acc: 0.47
Batch: 360; loss: 1.82; acc: 0.45
Batch: 380; loss: 1.76; acc: 0.44
Batch: 400; loss: 1.75; acc: 0.53
Batch: 420; loss: 1.78; acc: 0.52
Batch: 440; loss: 1.79; acc: 0.44
Batch: 460; loss: 1.8; acc: 0.41
Batch: 480; loss: 1.65; acc: 0.55
Batch: 500; loss: 1.66; acc: 0.55
Batch: 520; loss: 1.65; acc: 0.53
Batch: 540; loss: 1.75; acc: 0.44
Batch: 560; loss: 1.76; acc: 0.44
Batch: 580; loss: 1.8; acc: 0.38
Batch: 600; loss: 1.79; acc: 0.45
Batch: 620; loss: 1.87; acc: 0.44
Batch: 640; loss: 1.84; acc: 0.45
Batch: 660; loss: 1.81; acc: 0.48
Batch: 680; loss: 1.81; acc: 0.52
Batch: 700; loss: 1.82; acc: 0.41
Batch: 720; loss: 1.69; acc: 0.59
Batch: 740; loss: 1.86; acc: 0.42
Batch: 760; loss: 1.72; acc: 0.52
Batch: 780; loss: 1.7; acc: 0.44
Train Epoch over. train_loss: 1.78; train_accuracy: 0.47 

2.9953742341604084e-05
9.24456435313914e-06
Batch: 0; loss: 1.84; acc: 0.39
Batch: 20; loss: 1.98; acc: 0.39
Batch: 40; loss: 1.6; acc: 0.64
Batch: 60; loss: 1.69; acc: 0.5
Batch: 80; loss: 1.67; acc: 0.53
Batch: 100; loss: 1.8; acc: 0.42
Batch: 120; loss: 1.87; acc: 0.41
Batch: 140; loss: 1.66; acc: 0.58
Val Epoch over. val_loss: 1.7477490712123311; val_accuracy: 0.5089570063694268 

The current subspace-distance is: 9.24456435313914e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.61
Batch: 20; loss: 1.8; acc: 0.39
Batch: 40; loss: 1.74; acc: 0.58
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.75; acc: 0.53
Batch: 100; loss: 1.83; acc: 0.42
Batch: 120; loss: 1.71; acc: 0.5
Batch: 140; loss: 1.8; acc: 0.52
Batch: 160; loss: 1.85; acc: 0.45
Batch: 180; loss: 1.82; acc: 0.45
Batch: 200; loss: 1.66; acc: 0.5
Batch: 220; loss: 1.92; acc: 0.28
Batch: 240; loss: 1.76; acc: 0.5
Batch: 260; loss: 1.74; acc: 0.47
Batch: 280; loss: 1.79; acc: 0.41
Batch: 300; loss: 1.71; acc: 0.52
Batch: 320; loss: 1.68; acc: 0.53
Batch: 340; loss: 1.71; acc: 0.53
Batch: 360; loss: 1.83; acc: 0.42
Batch: 380; loss: 1.71; acc: 0.53
Batch: 400; loss: 1.71; acc: 0.52
Batch: 420; loss: 1.7; acc: 0.53
Batch: 440; loss: 1.71; acc: 0.56
Batch: 460; loss: 1.73; acc: 0.56
Batch: 480; loss: 1.65; acc: 0.52
Batch: 500; loss: 1.77; acc: 0.53
Batch: 520; loss: 1.76; acc: 0.48
Batch: 540; loss: 1.81; acc: 0.45
Batch: 560; loss: 1.81; acc: 0.41
Batch: 580; loss: 1.8; acc: 0.47
Batch: 600; loss: 1.75; acc: 0.45
Batch: 620; loss: 1.81; acc: 0.45
Batch: 640; loss: 1.76; acc: 0.48
Batch: 660; loss: 1.83; acc: 0.47
Batch: 680; loss: 1.77; acc: 0.48
Batch: 700; loss: 1.74; acc: 0.48
Batch: 720; loss: 1.66; acc: 0.53
Batch: 740; loss: 1.72; acc: 0.53
Batch: 760; loss: 1.79; acc: 0.41
Batch: 780; loss: 1.76; acc: 0.48
Train Epoch over. train_loss: 1.75; train_accuracy: 0.49 

3.150515476590954e-05
8.492158485751133e-06
Batch: 0; loss: 1.81; acc: 0.45
Batch: 20; loss: 1.95; acc: 0.36
Batch: 40; loss: 1.56; acc: 0.67
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.62; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.45
Batch: 120; loss: 1.89; acc: 0.42
Batch: 140; loss: 1.61; acc: 0.62
Val Epoch over. val_loss: 1.701020008439471; val_accuracy: 0.5438893312101911 

The current subspace-distance is: 8.492158485751133e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.45
Batch: 20; loss: 1.7; acc: 0.48
Batch: 40; loss: 1.73; acc: 0.47
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.62; acc: 0.53
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.63; acc: 0.56
Batch: 160; loss: 1.82; acc: 0.41
Batch: 180; loss: 1.68; acc: 0.58
Batch: 200; loss: 1.7; acc: 0.5
Batch: 220; loss: 1.72; acc: 0.52
Batch: 240; loss: 1.75; acc: 0.44
Batch: 260; loss: 1.72; acc: 0.48
Batch: 280; loss: 1.72; acc: 0.42
Batch: 300; loss: 1.68; acc: 0.61
Batch: 320; loss: 1.64; acc: 0.56
Batch: 340; loss: 1.74; acc: 0.52
Batch: 360; loss: 1.71; acc: 0.56
Batch: 380; loss: 1.71; acc: 0.58
Batch: 400; loss: 1.63; acc: 0.5
Batch: 420; loss: 1.78; acc: 0.42
Batch: 440; loss: 1.69; acc: 0.59
Batch: 460; loss: 1.71; acc: 0.53
Batch: 480; loss: 1.73; acc: 0.58
Batch: 500; loss: 1.64; acc: 0.58
Batch: 520; loss: 1.66; acc: 0.55
Batch: 540; loss: 1.55; acc: 0.66
Batch: 560; loss: 1.8; acc: 0.45
Batch: 580; loss: 1.65; acc: 0.55
Batch: 600; loss: 1.69; acc: 0.53
Batch: 620; loss: 1.66; acc: 0.53
Batch: 640; loss: 1.57; acc: 0.62
Batch: 660; loss: 1.75; acc: 0.45
Batch: 680; loss: 1.68; acc: 0.58
Batch: 700; loss: 1.73; acc: 0.47
Batch: 720; loss: 1.82; acc: 0.42
Batch: 740; loss: 1.64; acc: 0.55
Batch: 760; loss: 1.71; acc: 0.53
Batch: 780; loss: 1.59; acc: 0.62
Train Epoch over. train_loss: 1.7; train_accuracy: 0.51 

3.423977250349708e-05
1.4020413800608367e-05
Batch: 0; loss: 1.79; acc: 0.44
Batch: 20; loss: 1.91; acc: 0.39
Batch: 40; loss: 1.52; acc: 0.59
Batch: 60; loss: 1.65; acc: 0.56
Batch: 80; loss: 1.58; acc: 0.56
Batch: 100; loss: 1.73; acc: 0.38
Batch: 120; loss: 1.9; acc: 0.39
Batch: 140; loss: 1.54; acc: 0.62
Val Epoch over. val_loss: 1.660499330538853; val_accuracy: 0.5419984076433121 

The current subspace-distance is: 1.4020413800608367e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.91; acc: 0.28
Batch: 20; loss: 1.61; acc: 0.55
Batch: 40; loss: 1.67; acc: 0.56
Batch: 60; loss: 1.64; acc: 0.47
Batch: 80; loss: 1.75; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.59; acc: 0.59
Batch: 140; loss: 1.66; acc: 0.48
Batch: 160; loss: 1.63; acc: 0.48
Batch: 180; loss: 1.78; acc: 0.45
Batch: 200; loss: 1.57; acc: 0.58
Batch: 220; loss: 1.79; acc: 0.36
Batch: 240; loss: 1.76; acc: 0.47
Batch: 260; loss: 1.69; acc: 0.48
Batch: 280; loss: 1.65; acc: 0.64
Batch: 300; loss: 1.74; acc: 0.5
Batch: 320; loss: 1.6; acc: 0.56
Batch: 340; loss: 1.68; acc: 0.52
Batch: 360; loss: 1.78; acc: 0.38
Batch: 380; loss: 1.64; acc: 0.48
Batch: 400; loss: 1.81; acc: 0.39
Batch: 420; loss: 1.64; acc: 0.53
Batch: 440; loss: 1.68; acc: 0.55
Batch: 460; loss: 1.66; acc: 0.59
Batch: 480; loss: 1.59; acc: 0.56
Batch: 500; loss: 1.67; acc: 0.53
Batch: 520; loss: 1.7; acc: 0.52
Batch: 540; loss: 1.63; acc: 0.5
Batch: 560; loss: 1.59; acc: 0.56
Batch: 580; loss: 1.66; acc: 0.58
Batch: 600; loss: 1.64; acc: 0.58
Batch: 620; loss: 1.71; acc: 0.59
Batch: 640; loss: 1.55; acc: 0.59
Batch: 660; loss: 1.72; acc: 0.47
Batch: 680; loss: 1.63; acc: 0.58
Batch: 700; loss: 1.73; acc: 0.47
Batch: 720; loss: 1.68; acc: 0.48
Batch: 740; loss: 1.63; acc: 0.55
Batch: 760; loss: 1.77; acc: 0.44
Batch: 780; loss: 1.75; acc: 0.41
Train Epoch over. train_loss: 1.68; train_accuracy: 0.52 

3.5711549571715295e-05
1.1718378118530381e-05
Batch: 0; loss: 1.78; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.41
Batch: 40; loss: 1.49; acc: 0.56
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.58; acc: 0.56
Batch: 100; loss: 1.73; acc: 0.34
Batch: 120; loss: 1.93; acc: 0.36
Batch: 140; loss: 1.49; acc: 0.67
Val Epoch over. val_loss: 1.6446985363200972; val_accuracy: 0.5462778662420382 

The current subspace-distance is: 1.1718378118530381e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.55; acc: 0.62
Batch: 20; loss: 1.61; acc: 0.58
Batch: 40; loss: 1.71; acc: 0.47
Batch: 60; loss: 1.71; acc: 0.47
Batch: 80; loss: 1.62; acc: 0.55
Batch: 100; loss: 1.63; acc: 0.56
Batch: 120; loss: 1.63; acc: 0.59
Batch: 140; loss: 1.83; acc: 0.45
Batch: 160; loss: 1.54; acc: 0.62
Batch: 180; loss: 1.72; acc: 0.47
Batch: 200; loss: 1.68; acc: 0.58
Batch: 220; loss: 1.64; acc: 0.58
Batch: 240; loss: 1.75; acc: 0.42
Batch: 260; loss: 1.71; acc: 0.52
Batch: 280; loss: 1.7; acc: 0.56
Batch: 300; loss: 1.63; acc: 0.48
Batch: 320; loss: 1.64; acc: 0.52
Batch: 340; loss: 1.67; acc: 0.53
Batch: 360; loss: 1.7; acc: 0.47
Batch: 380; loss: 1.68; acc: 0.44
Batch: 400; loss: 1.61; acc: 0.58
Batch: 420; loss: 1.63; acc: 0.58
Batch: 440; loss: 1.88; acc: 0.48
Batch: 460; loss: 1.68; acc: 0.53
Batch: 480; loss: 1.69; acc: 0.56
Batch: 500; loss: 1.67; acc: 0.53
Batch: 520; loss: 1.59; acc: 0.58
Batch: 540; loss: 1.83; acc: 0.41
Batch: 560; loss: 1.49; acc: 0.66
Batch: 580; loss: 1.59; acc: 0.52
Batch: 600; loss: 1.62; acc: 0.53
Batch: 620; loss: 1.49; acc: 0.61
Batch: 640; loss: 1.6; acc: 0.58
Batch: 660; loss: 1.72; acc: 0.45
Batch: 680; loss: 1.7; acc: 0.48
Batch: 700; loss: 1.6; acc: 0.5
Batch: 720; loss: 1.62; acc: 0.5
Batch: 740; loss: 1.5; acc: 0.62
Batch: 760; loss: 1.67; acc: 0.47
Batch: 780; loss: 1.79; acc: 0.5
Train Epoch over. train_loss: 1.66; train_accuracy: 0.52 

3.744415153050795e-05
1.2527982107712887e-05
Batch: 0; loss: 1.76; acc: 0.44
Batch: 20; loss: 1.89; acc: 0.39
Batch: 40; loss: 1.47; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.57; acc: 0.55
Batch: 100; loss: 1.7; acc: 0.39
Batch: 120; loss: 1.95; acc: 0.3
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 1.637190013934093; val_accuracy: 0.5333399681528662 

The current subspace-distance is: 1.2527982107712887e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.44
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.7; acc: 0.48
Batch: 60; loss: 1.73; acc: 0.5
Batch: 80; loss: 1.54; acc: 0.62
Batch: 100; loss: 1.56; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.56; acc: 0.59
Batch: 160; loss: 1.74; acc: 0.44
Batch: 180; loss: 1.71; acc: 0.48
Batch: 200; loss: 1.66; acc: 0.56
Batch: 220; loss: 1.69; acc: 0.47
Batch: 240; loss: 1.77; acc: 0.42
Batch: 260; loss: 1.78; acc: 0.41
Batch: 280; loss: 1.51; acc: 0.55
Batch: 300; loss: 1.64; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.5
Batch: 340; loss: 1.59; acc: 0.52
Batch: 360; loss: 1.59; acc: 0.58
Batch: 380; loss: 1.77; acc: 0.47
Batch: 400; loss: 1.69; acc: 0.48
Batch: 420; loss: 1.73; acc: 0.5
Batch: 440; loss: 1.65; acc: 0.61
Batch: 460; loss: 1.65; acc: 0.53
Batch: 480; loss: 1.66; acc: 0.53
Batch: 500; loss: 1.69; acc: 0.52
Batch: 520; loss: 1.67; acc: 0.44
Batch: 540; loss: 1.65; acc: 0.48
Batch: 560; loss: 1.54; acc: 0.61
Batch: 580; loss: 1.63; acc: 0.41
Batch: 600; loss: 1.64; acc: 0.59
Batch: 620; loss: 1.42; acc: 0.62
Batch: 640; loss: 1.58; acc: 0.59
Batch: 660; loss: 1.6; acc: 0.61
Batch: 680; loss: 1.69; acc: 0.5
Batch: 700; loss: 1.77; acc: 0.39
Batch: 720; loss: 1.66; acc: 0.53
Batch: 740; loss: 1.63; acc: 0.5
Batch: 760; loss: 1.59; acc: 0.62
Batch: 780; loss: 1.62; acc: 0.52
Train Epoch over. train_loss: 1.65; train_accuracy: 0.52 

3.89391316275578e-05
1.4776607713429257e-05
Batch: 0; loss: 1.74; acc: 0.47
Batch: 20; loss: 1.86; acc: 0.39
Batch: 40; loss: 1.44; acc: 0.59
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.69; acc: 0.41
Batch: 120; loss: 1.95; acc: 0.3
Batch: 140; loss: 1.44; acc: 0.69
Val Epoch over. val_loss: 1.6157879426980475; val_accuracy: 0.5442874203821656 

The current subspace-distance is: 1.4776607713429257e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.59
Batch: 20; loss: 1.62; acc: 0.53
Batch: 40; loss: 1.54; acc: 0.56
Batch: 60; loss: 1.78; acc: 0.48
Batch: 80; loss: 1.7; acc: 0.47
Batch: 100; loss: 1.72; acc: 0.41
Batch: 120; loss: 1.45; acc: 0.62
Batch: 140; loss: 1.59; acc: 0.56
Batch: 160; loss: 1.68; acc: 0.5
Batch: 180; loss: 1.68; acc: 0.48
Batch: 200; loss: 1.57; acc: 0.56
Batch: 220; loss: 1.63; acc: 0.58
Batch: 240; loss: 1.59; acc: 0.59
Batch: 260; loss: 1.67; acc: 0.52
Batch: 280; loss: 1.6; acc: 0.56
Batch: 300; loss: 1.58; acc: 0.59
Batch: 320; loss: 1.5; acc: 0.61
Batch: 340; loss: 1.68; acc: 0.5
Batch: 360; loss: 1.56; acc: 0.52
Batch: 380; loss: 1.72; acc: 0.47
Batch: 400; loss: 1.59; acc: 0.48
Batch: 420; loss: 1.58; acc: 0.56
Batch: 440; loss: 1.69; acc: 0.47
Batch: 460; loss: 1.82; acc: 0.45
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.68; acc: 0.45
Batch: 520; loss: 1.45; acc: 0.59
Batch: 540; loss: 1.53; acc: 0.61
Batch: 560; loss: 1.68; acc: 0.48
Batch: 580; loss: 1.64; acc: 0.55
Batch: 600; loss: 1.81; acc: 0.44
Batch: 620; loss: 1.62; acc: 0.56
Batch: 640; loss: 1.45; acc: 0.61
Batch: 660; loss: 1.7; acc: 0.52
Batch: 680; loss: 1.72; acc: 0.44
Batch: 700; loss: 1.7; acc: 0.48
Batch: 720; loss: 1.55; acc: 0.56
Batch: 740; loss: 1.52; acc: 0.58
Batch: 760; loss: 1.76; acc: 0.44
Batch: 780; loss: 1.68; acc: 0.47
Train Epoch over. train_loss: 1.64; train_accuracy: 0.52 

3.979855318902992e-05
1.3731466424360406e-05
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.41; acc: 0.59
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.95; acc: 0.3
Batch: 140; loss: 1.44; acc: 0.67
Val Epoch over. val_loss: 1.605217803815368; val_accuracy: 0.5398089171974523 

The current subspace-distance is: 1.3731466424360406e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.54; acc: 0.58
Batch: 40; loss: 1.74; acc: 0.5
Batch: 60; loss: 1.48; acc: 0.62
Batch: 80; loss: 1.76; acc: 0.47
Batch: 100; loss: 1.7; acc: 0.41
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.63; acc: 0.47
Batch: 160; loss: 1.6; acc: 0.5
Batch: 180; loss: 1.65; acc: 0.48
Batch: 200; loss: 1.75; acc: 0.5
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.58; acc: 0.48
Batch: 260; loss: 1.68; acc: 0.5
Batch: 280; loss: 1.62; acc: 0.52
Batch: 300; loss: 1.63; acc: 0.55
Batch: 320; loss: 1.61; acc: 0.56
Batch: 340; loss: 1.55; acc: 0.56
Batch: 360; loss: 1.6; acc: 0.53
Batch: 380; loss: 1.8; acc: 0.5
Batch: 400; loss: 1.59; acc: 0.55
Batch: 420; loss: 1.54; acc: 0.61
Batch: 440; loss: 1.55; acc: 0.56
Batch: 460; loss: 1.55; acc: 0.58
Batch: 480; loss: 1.7; acc: 0.45
Batch: 500; loss: 1.5; acc: 0.61
Batch: 520; loss: 1.81; acc: 0.45
Batch: 540; loss: 1.66; acc: 0.61
Batch: 560; loss: 1.63; acc: 0.55
Batch: 580; loss: 1.65; acc: 0.52
Batch: 600; loss: 1.52; acc: 0.66
Batch: 620; loss: 1.62; acc: 0.5
Batch: 640; loss: 1.74; acc: 0.47
Batch: 660; loss: 1.57; acc: 0.59
Batch: 680; loss: 1.65; acc: 0.47
Batch: 700; loss: 1.67; acc: 0.53
Batch: 720; loss: 1.63; acc: 0.56
Batch: 740; loss: 1.62; acc: 0.52
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.44; acc: 0.58
Train Epoch over. train_loss: 1.63; train_accuracy: 0.52 

4.046788308187388e-05
1.3845337889506482e-05
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.84; acc: 0.34
Batch: 40; loss: 1.4; acc: 0.59
Batch: 60; loss: 1.56; acc: 0.59
Batch: 80; loss: 1.53; acc: 0.61
Batch: 100; loss: 1.67; acc: 0.41
Batch: 120; loss: 1.95; acc: 0.33
Batch: 140; loss: 1.42; acc: 0.67
Val Epoch over. val_loss: 1.5962370968168709; val_accuracy: 0.5451831210191083 

The current subspace-distance is: 1.3845337889506482e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.58
Batch: 20; loss: 1.61; acc: 0.48
Batch: 40; loss: 1.6; acc: 0.47
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.63; acc: 0.45
Batch: 140; loss: 1.64; acc: 0.45
Batch: 160; loss: 1.59; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.48
Batch: 200; loss: 1.51; acc: 0.53
Batch: 220; loss: 1.62; acc: 0.59
Batch: 240; loss: 1.65; acc: 0.48
Batch: 260; loss: 1.66; acc: 0.39
Batch: 280; loss: 1.6; acc: 0.55
Batch: 300; loss: 1.59; acc: 0.47
Batch: 320; loss: 1.66; acc: 0.5
Batch: 340; loss: 1.51; acc: 0.55
Batch: 360; loss: 1.64; acc: 0.42
Batch: 380; loss: 1.67; acc: 0.55
Batch: 400; loss: 1.67; acc: 0.45
Batch: 420; loss: 1.59; acc: 0.5
Batch: 440; loss: 1.63; acc: 0.53
Batch: 460; loss: 1.62; acc: 0.45
Batch: 480; loss: 1.58; acc: 0.59
Batch: 500; loss: 1.81; acc: 0.42
Batch: 520; loss: 1.59; acc: 0.55
Batch: 540; loss: 1.67; acc: 0.52
Batch: 560; loss: 1.66; acc: 0.59
Batch: 580; loss: 1.56; acc: 0.53
Batch: 600; loss: 1.56; acc: 0.56
Batch: 620; loss: 1.46; acc: 0.55
Batch: 640; loss: 1.81; acc: 0.48
Batch: 660; loss: 1.59; acc: 0.53
Batch: 680; loss: 1.63; acc: 0.55
Batch: 700; loss: 1.5; acc: 0.64
Batch: 720; loss: 1.85; acc: 0.47
Batch: 740; loss: 1.55; acc: 0.59
Batch: 760; loss: 1.6; acc: 0.52
Batch: 780; loss: 1.6; acc: 0.53
Train Epoch over. train_loss: 1.63; train_accuracy: 0.52 

4.14910027757287e-05
1.4106714843364898e-05
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.38; acc: 0.59
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.59
Batch: 100; loss: 1.66; acc: 0.42
Batch: 120; loss: 1.96; acc: 0.31
Batch: 140; loss: 1.42; acc: 0.69
Val Epoch over. val_loss: 1.5961141609082556; val_accuracy: 0.5409036624203821 

The current subspace-distance is: 1.4106714843364898e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.53
Batch: 20; loss: 1.56; acc: 0.53
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.59; acc: 0.5
Batch: 100; loss: 1.45; acc: 0.62
Batch: 120; loss: 1.66; acc: 0.55
Batch: 140; loss: 1.46; acc: 0.61
Batch: 160; loss: 1.65; acc: 0.48
Batch: 180; loss: 1.66; acc: 0.52
Batch: 200; loss: 1.48; acc: 0.59
Batch: 220; loss: 1.53; acc: 0.59
Batch: 240; loss: 1.54; acc: 0.62
Batch: 260; loss: 1.55; acc: 0.56
Batch: 280; loss: 1.57; acc: 0.55
Batch: 300; loss: 1.54; acc: 0.64
Batch: 320; loss: 1.56; acc: 0.59
Batch: 340; loss: 1.81; acc: 0.44
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 1.52; acc: 0.55
Batch: 400; loss: 1.68; acc: 0.53
Batch: 420; loss: 1.53; acc: 0.69
Batch: 440; loss: 1.53; acc: 0.66
Batch: 460; loss: 1.66; acc: 0.52
Batch: 480; loss: 1.51; acc: 0.62
Batch: 500; loss: 1.63; acc: 0.5
Batch: 520; loss: 1.73; acc: 0.52
Batch: 540; loss: 1.71; acc: 0.44
Batch: 560; loss: 1.7; acc: 0.55
Batch: 580; loss: 1.66; acc: 0.42
Batch: 600; loss: 1.7; acc: 0.42
Batch: 620; loss: 1.55; acc: 0.5
Batch: 640; loss: 1.44; acc: 0.67
Batch: 660; loss: 1.5; acc: 0.56
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.47; acc: 0.52
Batch: 720; loss: 1.48; acc: 0.56
Batch: 740; loss: 1.57; acc: 0.55
Batch: 760; loss: 1.56; acc: 0.56
Batch: 780; loss: 1.55; acc: 0.5
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.313658064347692e-05
1.6435098586953245e-05
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.54; acc: 0.59
Batch: 100; loss: 1.67; acc: 0.44
Batch: 120; loss: 1.97; acc: 0.33
Batch: 140; loss: 1.41; acc: 0.67
Val Epoch over. val_loss: 1.600622183957677; val_accuracy: 0.540406050955414 

The current subspace-distance is: 1.6435098586953245e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.59
Batch: 20; loss: 1.71; acc: 0.52
Batch: 40; loss: 1.62; acc: 0.55
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.56; acc: 0.48
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.51; acc: 0.62
Batch: 140; loss: 1.59; acc: 0.59
Batch: 160; loss: 1.62; acc: 0.56
Batch: 180; loss: 1.53; acc: 0.62
Batch: 200; loss: 1.54; acc: 0.53
Batch: 220; loss: 1.63; acc: 0.55
Batch: 240; loss: 1.65; acc: 0.53
Batch: 260; loss: 1.7; acc: 0.5
Batch: 280; loss: 1.57; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.56
Batch: 320; loss: 1.55; acc: 0.64
Batch: 340; loss: 1.64; acc: 0.55
Batch: 360; loss: 1.6; acc: 0.53
Batch: 380; loss: 1.72; acc: 0.48
Batch: 400; loss: 1.62; acc: 0.55
Batch: 420; loss: 1.53; acc: 0.58
Batch: 440; loss: 1.6; acc: 0.53
Batch: 460; loss: 1.58; acc: 0.59
Batch: 480; loss: 1.6; acc: 0.59
Batch: 500; loss: 1.64; acc: 0.52
Batch: 520; loss: 1.52; acc: 0.56
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.73; acc: 0.48
Batch: 580; loss: 1.64; acc: 0.5
Batch: 600; loss: 1.79; acc: 0.36
Batch: 620; loss: 1.54; acc: 0.55
Batch: 640; loss: 1.56; acc: 0.56
Batch: 660; loss: 1.43; acc: 0.62
Batch: 680; loss: 1.51; acc: 0.61
Batch: 700; loss: 1.56; acc: 0.5
Batch: 720; loss: 1.6; acc: 0.61
Batch: 740; loss: 1.59; acc: 0.55
Batch: 760; loss: 1.58; acc: 0.5
Batch: 780; loss: 1.52; acc: 0.58
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.198567330604419e-05
1.5346842701546848e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.84; acc: 0.34
Batch: 40; loss: 1.36; acc: 0.59
Batch: 60; loss: 1.53; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.45
Batch: 120; loss: 1.96; acc: 0.33
Batch: 140; loss: 1.4; acc: 0.69
Val Epoch over. val_loss: 1.5863742433535826; val_accuracy: 0.5398089171974523 

The current subspace-distance is: 1.5346842701546848e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.5
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.44; acc: 0.53
Batch: 60; loss: 1.81; acc: 0.42
Batch: 80; loss: 1.56; acc: 0.59
Batch: 100; loss: 1.7; acc: 0.44
Batch: 120; loss: 1.75; acc: 0.44
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.76; acc: 0.5
Batch: 180; loss: 1.5; acc: 0.55
Batch: 200; loss: 1.71; acc: 0.52
Batch: 220; loss: 1.58; acc: 0.48
Batch: 240; loss: 1.52; acc: 0.62
Batch: 260; loss: 1.62; acc: 0.52
Batch: 280; loss: 1.58; acc: 0.53
Batch: 300; loss: 1.58; acc: 0.5
Batch: 320; loss: 1.57; acc: 0.55
Batch: 340; loss: 1.57; acc: 0.58
Batch: 360; loss: 1.62; acc: 0.56
Batch: 380; loss: 1.7; acc: 0.53
Batch: 400; loss: 1.77; acc: 0.44
Batch: 420; loss: 1.59; acc: 0.5
Batch: 440; loss: 1.6; acc: 0.56
Batch: 460; loss: 1.66; acc: 0.48
Batch: 480; loss: 1.71; acc: 0.52
Batch: 500; loss: 1.43; acc: 0.61
Batch: 520; loss: 1.78; acc: 0.47
Batch: 540; loss: 1.65; acc: 0.5
Batch: 560; loss: 1.64; acc: 0.47
Batch: 580; loss: 1.46; acc: 0.56
Batch: 600; loss: 1.68; acc: 0.41
Batch: 620; loss: 1.59; acc: 0.52
Batch: 640; loss: 1.6; acc: 0.47
Batch: 660; loss: 1.75; acc: 0.56
Batch: 680; loss: 1.63; acc: 0.55
Batch: 700; loss: 1.54; acc: 0.59
Batch: 720; loss: 1.81; acc: 0.45
Batch: 740; loss: 1.79; acc: 0.48
Batch: 760; loss: 1.71; acc: 0.52
Batch: 780; loss: 1.64; acc: 0.55
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.2737523472169414e-05
1.3136555935489014e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.83; acc: 0.33
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.67; acc: 0.45
Batch: 120; loss: 1.96; acc: 0.36
Batch: 140; loss: 1.41; acc: 0.69
Val Epoch over. val_loss: 1.5892821079606463; val_accuracy: 0.5416998407643312 

The current subspace-distance is: 1.3136555935489014e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.7; acc: 0.47
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.68; acc: 0.52
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.61; acc: 0.53
Batch: 160; loss: 1.72; acc: 0.44
Batch: 180; loss: 1.52; acc: 0.58
Batch: 200; loss: 1.51; acc: 0.61
Batch: 220; loss: 1.51; acc: 0.53
Batch: 240; loss: 1.69; acc: 0.55
Batch: 260; loss: 1.74; acc: 0.47
Batch: 280; loss: 1.6; acc: 0.5
Batch: 300; loss: 1.52; acc: 0.48
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.56; acc: 0.52
Batch: 360; loss: 1.62; acc: 0.56
Batch: 380; loss: 1.51; acc: 0.53
Batch: 400; loss: 1.72; acc: 0.5
Batch: 420; loss: 1.63; acc: 0.45
Batch: 440; loss: 1.62; acc: 0.56
Batch: 460; loss: 1.57; acc: 0.5
Batch: 480; loss: 1.62; acc: 0.55
Batch: 500; loss: 1.57; acc: 0.52
Batch: 520; loss: 1.59; acc: 0.55
Batch: 540; loss: 1.7; acc: 0.47
Batch: 560; loss: 1.74; acc: 0.47
Batch: 580; loss: 1.55; acc: 0.5
Batch: 600; loss: 1.7; acc: 0.48
Batch: 620; loss: 1.58; acc: 0.52
Batch: 640; loss: 1.51; acc: 0.56
Batch: 660; loss: 1.77; acc: 0.48
Batch: 680; loss: 1.62; acc: 0.58
Batch: 700; loss: 1.61; acc: 0.62
Batch: 720; loss: 1.56; acc: 0.56
Batch: 740; loss: 1.73; acc: 0.45
Batch: 760; loss: 1.66; acc: 0.47
Batch: 780; loss: 1.66; acc: 0.44
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

4.231767525197938e-05
1.1686121069942601e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.82; acc: 0.36
Batch: 40; loss: 1.35; acc: 0.64
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 1.52; acc: 0.56
Batch: 100; loss: 1.65; acc: 0.45
Batch: 120; loss: 1.95; acc: 0.36
Batch: 140; loss: 1.39; acc: 0.72
Val Epoch over. val_loss: 1.5802516618352027; val_accuracy: 0.5427945859872612 

The current subspace-distance is: 1.1686121069942601e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.71; acc: 0.42
Batch: 60; loss: 1.62; acc: 0.58
Batch: 80; loss: 1.57; acc: 0.56
Batch: 100; loss: 1.59; acc: 0.55
Batch: 120; loss: 1.71; acc: 0.42
Batch: 140; loss: 1.68; acc: 0.47
Batch: 160; loss: 1.43; acc: 0.61
Batch: 180; loss: 1.68; acc: 0.45
Batch: 200; loss: 1.57; acc: 0.55
Batch: 220; loss: 1.76; acc: 0.41
Batch: 240; loss: 1.66; acc: 0.48
Batch: 260; loss: 1.57; acc: 0.5
Batch: 280; loss: 1.61; acc: 0.5
Batch: 300; loss: 1.61; acc: 0.58
Batch: 320; loss: 1.66; acc: 0.39
Batch: 340; loss: 1.63; acc: 0.47
Batch: 360; loss: 1.73; acc: 0.41
Batch: 380; loss: 1.5; acc: 0.58
Batch: 400; loss: 1.6; acc: 0.53
Batch: 420; loss: 1.68; acc: 0.53
Batch: 440; loss: 1.67; acc: 0.48
Batch: 460; loss: 1.54; acc: 0.5
Batch: 480; loss: 1.58; acc: 0.5
Batch: 500; loss: 1.62; acc: 0.53
Batch: 520; loss: 1.64; acc: 0.42
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.47; acc: 0.66
Batch: 580; loss: 1.69; acc: 0.5
Batch: 600; loss: 1.75; acc: 0.55
Batch: 620; loss: 1.46; acc: 0.59
Batch: 640; loss: 1.66; acc: 0.45
Batch: 660; loss: 1.77; acc: 0.42
Batch: 680; loss: 1.61; acc: 0.53
Batch: 700; loss: 1.65; acc: 0.56
Batch: 720; loss: 1.58; acc: 0.53
Batch: 740; loss: 1.73; acc: 0.47
Batch: 760; loss: 1.6; acc: 0.48
Batch: 780; loss: 1.48; acc: 0.59
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.3432308302726597e-05
1.7056094293366186e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.83; acc: 0.33
Batch: 40; loss: 1.35; acc: 0.64
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 1.53; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.96; acc: 0.36
Batch: 140; loss: 1.4; acc: 0.67
Val Epoch over. val_loss: 1.5852152329341622; val_accuracy: 0.5371218152866242 

The current subspace-distance is: 1.7056094293366186e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.5
Batch: 20; loss: 1.67; acc: 0.45
Batch: 40; loss: 1.65; acc: 0.48
Batch: 60; loss: 1.61; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.48
Batch: 100; loss: 1.66; acc: 0.42
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.73; acc: 0.45
Batch: 160; loss: 1.61; acc: 0.56
Batch: 180; loss: 1.52; acc: 0.59
Batch: 200; loss: 1.5; acc: 0.56
Batch: 220; loss: 1.66; acc: 0.48
Batch: 240; loss: 1.61; acc: 0.56
Batch: 260; loss: 1.66; acc: 0.48
Batch: 280; loss: 1.61; acc: 0.47
Batch: 300; loss: 1.52; acc: 0.56
Batch: 320; loss: 1.65; acc: 0.5
Batch: 340; loss: 1.58; acc: 0.5
Batch: 360; loss: 1.76; acc: 0.41
Batch: 380; loss: 1.62; acc: 0.58
Batch: 400; loss: 1.59; acc: 0.53
Batch: 420; loss: 1.66; acc: 0.45
Batch: 440; loss: 1.53; acc: 0.55
Batch: 460; loss: 1.7; acc: 0.52
Batch: 480; loss: 1.62; acc: 0.45
Batch: 500; loss: 1.59; acc: 0.52
Batch: 520; loss: 1.68; acc: 0.53
Batch: 540; loss: 1.45; acc: 0.64
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.64; acc: 0.47
Batch: 600; loss: 1.58; acc: 0.53
Batch: 620; loss: 1.59; acc: 0.53
Batch: 640; loss: 1.69; acc: 0.53
Batch: 660; loss: 1.51; acc: 0.56
Batch: 680; loss: 1.66; acc: 0.53
Batch: 700; loss: 1.7; acc: 0.47
Batch: 720; loss: 1.5; acc: 0.56
Batch: 740; loss: 1.61; acc: 0.5
Batch: 760; loss: 1.67; acc: 0.5
Batch: 780; loss: 1.45; acc: 0.64
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.385479769553058e-05
1.5156037989072502e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.82; acc: 0.31
Batch: 40; loss: 1.34; acc: 0.7
Batch: 60; loss: 1.52; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.96; acc: 0.38
Batch: 140; loss: 1.39; acc: 0.69
Val Epoch over. val_loss: 1.5830452123265357; val_accuracy: 0.5389132165605095 

The current subspace-distance is: 1.5156037989072502e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.55
Batch: 20; loss: 1.58; acc: 0.58
Batch: 40; loss: 1.57; acc: 0.53
Batch: 60; loss: 1.6; acc: 0.59
Batch: 80; loss: 1.56; acc: 0.56
Batch: 100; loss: 1.38; acc: 0.67
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.67; acc: 0.48
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.66; acc: 0.47
Batch: 200; loss: 1.62; acc: 0.42
Batch: 220; loss: 1.71; acc: 0.44
Batch: 240; loss: 1.56; acc: 0.53
Batch: 260; loss: 1.51; acc: 0.55
Batch: 280; loss: 1.63; acc: 0.5
Batch: 300; loss: 1.54; acc: 0.5
Batch: 320; loss: 1.67; acc: 0.47
Batch: 340; loss: 1.53; acc: 0.62
Batch: 360; loss: 1.58; acc: 0.56
Batch: 380; loss: 1.67; acc: 0.42
Batch: 400; loss: 1.63; acc: 0.56
Batch: 420; loss: 1.64; acc: 0.47
Batch: 440; loss: 1.62; acc: 0.44
Batch: 460; loss: 1.62; acc: 0.45
Batch: 480; loss: 1.62; acc: 0.48
Batch: 500; loss: 1.48; acc: 0.59
Batch: 520; loss: 1.58; acc: 0.52
Batch: 540; loss: 1.77; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.44
Batch: 580; loss: 1.67; acc: 0.53
Batch: 600; loss: 1.58; acc: 0.47
Batch: 620; loss: 1.56; acc: 0.53
Batch: 640; loss: 1.65; acc: 0.52
Batch: 660; loss: 1.74; acc: 0.39
Batch: 680; loss: 1.64; acc: 0.45
Batch: 700; loss: 1.64; acc: 0.53
Batch: 720; loss: 1.59; acc: 0.53
Batch: 740; loss: 1.6; acc: 0.55
Batch: 760; loss: 1.64; acc: 0.55
Batch: 780; loss: 1.65; acc: 0.53
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.450751657714136e-05
1.541008532512933e-05
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.82; acc: 0.31
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 1.53; acc: 0.58
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.97; acc: 0.36
Batch: 140; loss: 1.39; acc: 0.69
Val Epoch over. val_loss: 1.5828199546048596; val_accuracy: 0.5380175159235668 

The current subspace-distance is: 1.541008532512933e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.55
Batch: 20; loss: 1.62; acc: 0.47
Batch: 40; loss: 1.7; acc: 0.41
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.65; acc: 0.5
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.77; acc: 0.42
Batch: 140; loss: 1.55; acc: 0.53
Batch: 160; loss: 1.62; acc: 0.52
Batch: 180; loss: 1.56; acc: 0.52
Batch: 200; loss: 1.7; acc: 0.53
Batch: 220; loss: 1.53; acc: 0.59
Batch: 240; loss: 1.51; acc: 0.62
Batch: 260; loss: 1.49; acc: 0.53
Batch: 280; loss: 1.54; acc: 0.55
Batch: 300; loss: 1.81; acc: 0.38
Batch: 320; loss: 1.56; acc: 0.52
Batch: 340; loss: 1.49; acc: 0.62
Batch: 360; loss: 1.69; acc: 0.5
Batch: 380; loss: 1.62; acc: 0.52
Batch: 400; loss: 1.61; acc: 0.53
Batch: 420; loss: 1.57; acc: 0.59
Batch: 440; loss: 1.6; acc: 0.56
Batch: 460; loss: 1.79; acc: 0.53
Batch: 480; loss: 1.71; acc: 0.47
Batch: 500; loss: 1.54; acc: 0.55
Batch: 520; loss: 1.46; acc: 0.59
Batch: 540; loss: 1.72; acc: 0.41
Batch: 560; loss: 1.71; acc: 0.39
Batch: 580; loss: 1.73; acc: 0.39
Batch: 600; loss: 1.64; acc: 0.48
Batch: 620; loss: 1.6; acc: 0.5
Batch: 640; loss: 1.52; acc: 0.53
Batch: 660; loss: 1.68; acc: 0.47
Batch: 680; loss: 1.57; acc: 0.52
Batch: 700; loss: 1.48; acc: 0.52
Batch: 720; loss: 1.46; acc: 0.58
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.56; acc: 0.55
Batch: 780; loss: 1.51; acc: 0.55
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.551021629595198e-05
1.7451618987252004e-05
Batch: 0; loss: 1.69; acc: 0.44
Batch: 20; loss: 1.81; acc: 0.34
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.42
Batch: 120; loss: 1.97; acc: 0.36
Batch: 140; loss: 1.36; acc: 0.72
Val Epoch over. val_loss: 1.5737564252440337; val_accuracy: 0.5368232484076433 

The current subspace-distance is: 1.7451618987252004e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.36; acc: 0.62
Batch: 40; loss: 1.7; acc: 0.47
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.7; acc: 0.44
Batch: 100; loss: 1.64; acc: 0.53
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.59
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.56; acc: 0.53
Batch: 240; loss: 1.57; acc: 0.52
Batch: 260; loss: 1.57; acc: 0.52
Batch: 280; loss: 1.61; acc: 0.55
Batch: 300; loss: 1.56; acc: 0.56
Batch: 320; loss: 1.54; acc: 0.53
Batch: 340; loss: 1.87; acc: 0.34
Batch: 360; loss: 1.63; acc: 0.52
Batch: 380; loss: 1.7; acc: 0.5
Batch: 400; loss: 1.57; acc: 0.45
Batch: 420; loss: 1.5; acc: 0.55
Batch: 440; loss: 1.58; acc: 0.55
Batch: 460; loss: 1.76; acc: 0.44
Batch: 480; loss: 1.62; acc: 0.5
Batch: 500; loss: 1.7; acc: 0.48
Batch: 520; loss: 1.64; acc: 0.48
Batch: 540; loss: 1.64; acc: 0.45
Batch: 560; loss: 1.52; acc: 0.55
Batch: 580; loss: 1.47; acc: 0.67
Batch: 600; loss: 1.51; acc: 0.53
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.56; acc: 0.47
Batch: 660; loss: 1.67; acc: 0.44
Batch: 680; loss: 1.59; acc: 0.58
Batch: 700; loss: 1.52; acc: 0.56
Batch: 720; loss: 1.48; acc: 0.62
Batch: 740; loss: 1.43; acc: 0.59
Batch: 760; loss: 1.63; acc: 0.47
Batch: 780; loss: 1.74; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.544183684629388e-05
1.687957592366729e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.83; acc: 0.31
Batch: 40; loss: 1.33; acc: 0.67
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.55
Batch: 100; loss: 1.67; acc: 0.45
Batch: 120; loss: 1.98; acc: 0.33
Batch: 140; loss: 1.37; acc: 0.67
Val Epoch over. val_loss: 1.5816655318448498; val_accuracy: 0.5253781847133758 

The current subspace-distance is: 1.687957592366729e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.53; acc: 0.58
Batch: 40; loss: 1.53; acc: 0.56
Batch: 60; loss: 1.71; acc: 0.47
Batch: 80; loss: 1.69; acc: 0.5
Batch: 100; loss: 1.58; acc: 0.52
Batch: 120; loss: 1.67; acc: 0.48
Batch: 140; loss: 1.59; acc: 0.59
Batch: 160; loss: 1.57; acc: 0.56
Batch: 180; loss: 1.62; acc: 0.47
Batch: 200; loss: 1.68; acc: 0.48
Batch: 220; loss: 1.47; acc: 0.56
Batch: 240; loss: 1.55; acc: 0.52
Batch: 260; loss: 1.73; acc: 0.52
Batch: 280; loss: 1.55; acc: 0.45
Batch: 300; loss: 1.41; acc: 0.62
Batch: 320; loss: 1.49; acc: 0.59
Batch: 340; loss: 1.61; acc: 0.5
Batch: 360; loss: 1.64; acc: 0.44
Batch: 380; loss: 1.53; acc: 0.58
Batch: 400; loss: 1.66; acc: 0.42
Batch: 420; loss: 1.55; acc: 0.58
Batch: 440; loss: 1.55; acc: 0.55
Batch: 460; loss: 1.68; acc: 0.48
Batch: 480; loss: 1.61; acc: 0.56
Batch: 500; loss: 1.45; acc: 0.56
Batch: 520; loss: 1.61; acc: 0.52
Batch: 540; loss: 1.55; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.52
Batch: 580; loss: 1.59; acc: 0.56
Batch: 600; loss: 1.68; acc: 0.45
Batch: 620; loss: 1.62; acc: 0.53
Batch: 640; loss: 1.63; acc: 0.48
Batch: 660; loss: 1.61; acc: 0.52
Batch: 680; loss: 1.47; acc: 0.66
Batch: 700; loss: 1.47; acc: 0.56
Batch: 720; loss: 1.72; acc: 0.5
Batch: 740; loss: 1.61; acc: 0.47
Batch: 760; loss: 1.43; acc: 0.69
Batch: 780; loss: 1.7; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.503261880017817e-05
1.9539431377779692e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.8; acc: 0.36
Batch: 40; loss: 1.32; acc: 0.67
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.42
Batch: 120; loss: 1.99; acc: 0.33
Batch: 140; loss: 1.37; acc: 0.72
Val Epoch over. val_loss: 1.5752252644034708; val_accuracy: 0.5315485668789809 

The current subspace-distance is: 1.9539431377779692e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.5
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.67; acc: 0.5
Batch: 60; loss: 1.52; acc: 0.53
Batch: 80; loss: 1.65; acc: 0.44
Batch: 100; loss: 1.67; acc: 0.44
Batch: 120; loss: 1.61; acc: 0.48
Batch: 140; loss: 1.57; acc: 0.52
Batch: 160; loss: 1.46; acc: 0.53
Batch: 180; loss: 1.67; acc: 0.52
Batch: 200; loss: 1.47; acc: 0.55
Batch: 220; loss: 1.53; acc: 0.48
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.56; acc: 0.52
Batch: 280; loss: 1.55; acc: 0.56
Batch: 300; loss: 1.61; acc: 0.52
Batch: 320; loss: 1.66; acc: 0.44
Batch: 340; loss: 1.52; acc: 0.61
Batch: 360; loss: 1.59; acc: 0.47
Batch: 380; loss: 1.67; acc: 0.48
Batch: 400; loss: 1.61; acc: 0.53
Batch: 420; loss: 1.53; acc: 0.52
Batch: 440; loss: 1.76; acc: 0.39
Batch: 460; loss: 1.4; acc: 0.66
Batch: 480; loss: 1.61; acc: 0.5
Batch: 500; loss: 1.67; acc: 0.53
Batch: 520; loss: 1.7; acc: 0.59
Batch: 540; loss: 1.65; acc: 0.52
Batch: 560; loss: 1.55; acc: 0.53
Batch: 580; loss: 1.58; acc: 0.61
Batch: 600; loss: 1.54; acc: 0.56
Batch: 620; loss: 1.65; acc: 0.52
Batch: 640; loss: 1.59; acc: 0.5
Batch: 660; loss: 1.39; acc: 0.62
Batch: 680; loss: 1.53; acc: 0.53
Batch: 700; loss: 1.58; acc: 0.56
Batch: 720; loss: 1.52; acc: 0.52
Batch: 740; loss: 1.68; acc: 0.44
Batch: 760; loss: 1.73; acc: 0.48
Batch: 780; loss: 1.58; acc: 0.55
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

4.4013628212269396e-05
1.2743458682962228e-05
Batch: 0; loss: 1.66; acc: 0.48
Batch: 20; loss: 1.79; acc: 0.33
Batch: 40; loss: 1.31; acc: 0.69
Batch: 60; loss: 1.5; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.56
Batch: 100; loss: 1.64; acc: 0.45
Batch: 120; loss: 1.97; acc: 0.34
Batch: 140; loss: 1.36; acc: 0.69
Val Epoch over. val_loss: 1.5656207875840982; val_accuracy: 0.541202229299363 

The current subspace-distance is: 1.2743458682962228e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.57; acc: 0.55
Batch: 40; loss: 1.66; acc: 0.52
Batch: 60; loss: 1.52; acc: 0.5
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.65; acc: 0.47
Batch: 140; loss: 1.55; acc: 0.47
Batch: 160; loss: 1.6; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.55
Batch: 200; loss: 1.65; acc: 0.5
Batch: 220; loss: 1.53; acc: 0.55
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.7; acc: 0.5
Batch: 280; loss: 1.6; acc: 0.52
Batch: 300; loss: 1.49; acc: 0.56
Batch: 320; loss: 1.59; acc: 0.55
Batch: 340; loss: 1.74; acc: 0.44
Batch: 360; loss: 1.69; acc: 0.47
Batch: 380; loss: 1.53; acc: 0.61
Batch: 400; loss: 1.54; acc: 0.53
Batch: 420; loss: 1.66; acc: 0.52
Batch: 440; loss: 1.56; acc: 0.56
Batch: 460; loss: 1.69; acc: 0.47
Batch: 480; loss: 1.53; acc: 0.53
Batch: 500; loss: 1.54; acc: 0.53
Batch: 520; loss: 1.56; acc: 0.41
Batch: 540; loss: 1.66; acc: 0.52
Batch: 560; loss: 1.62; acc: 0.52
Batch: 580; loss: 1.63; acc: 0.53
Batch: 600; loss: 1.48; acc: 0.62
Batch: 620; loss: 1.78; acc: 0.47
Batch: 640; loss: 1.42; acc: 0.66
Batch: 660; loss: 1.72; acc: 0.5
Batch: 680; loss: 1.47; acc: 0.56
Batch: 700; loss: 1.68; acc: 0.47
Batch: 720; loss: 1.69; acc: 0.48
Batch: 740; loss: 1.51; acc: 0.53
Batch: 760; loss: 1.63; acc: 0.52
Batch: 780; loss: 1.6; acc: 0.48
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

4.5801465603290126e-05
1.9975803297711536e-05
Batch: 0; loss: 1.67; acc: 0.45
Batch: 20; loss: 1.8; acc: 0.34
Batch: 40; loss: 1.3; acc: 0.67
Batch: 60; loss: 1.5; acc: 0.59
Batch: 80; loss: 1.5; acc: 0.53
Batch: 100; loss: 1.66; acc: 0.45
Batch: 120; loss: 1.97; acc: 0.33
Batch: 140; loss: 1.36; acc: 0.7
Val Epoch over. val_loss: 1.565355338868062; val_accuracy: 0.5380175159235668 

The current subspace-distance is: 1.9975803297711536e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.5
Batch: 20; loss: 1.54; acc: 0.53
Batch: 40; loss: 1.66; acc: 0.5
Batch: 60; loss: 1.68; acc: 0.44
Batch: 80; loss: 1.54; acc: 0.48
Batch: 100; loss: 1.8; acc: 0.39
Batch: 120; loss: 1.55; acc: 0.55
Batch: 140; loss: 1.66; acc: 0.45
Batch: 160; loss: 1.4; acc: 0.7
Batch: 180; loss: 1.61; acc: 0.55
Batch: 200; loss: 1.57; acc: 0.52
Batch: 220; loss: 1.52; acc: 0.62
Batch: 240; loss: 1.75; acc: 0.5
Batch: 260; loss: 1.56; acc: 0.55
Batch: 280; loss: 1.56; acc: 0.45
Batch: 300; loss: 1.53; acc: 0.59
Batch: 320; loss: 1.5; acc: 0.52
Batch: 340; loss: 1.43; acc: 0.59
Batch: 360; loss: 1.66; acc: 0.5
Batch: 380; loss: 1.64; acc: 0.52
Batch: 400; loss: 1.53; acc: 0.58
Batch: 420; loss: 1.59; acc: 0.47
Batch: 440; loss: 1.62; acc: 0.56
Batch: 460; loss: 1.48; acc: 0.55
Batch: 480; loss: 1.74; acc: 0.47
Batch: 500; loss: 1.66; acc: 0.45
Batch: 520; loss: 1.61; acc: 0.47
Batch: 540; loss: 1.6; acc: 0.5
Batch: 560; loss: 1.7; acc: 0.41
Batch: 580; loss: 1.67; acc: 0.48
Batch: 600; loss: 1.63; acc: 0.48
Batch: 620; loss: 1.6; acc: 0.55
Batch: 640; loss: 1.74; acc: 0.42
Batch: 660; loss: 1.65; acc: 0.5
Batch: 680; loss: 1.61; acc: 0.5
Batch: 700; loss: 1.76; acc: 0.42
Batch: 720; loss: 1.66; acc: 0.47
Batch: 740; loss: 1.83; acc: 0.34
Batch: 760; loss: 1.63; acc: 0.5
Batch: 780; loss: 1.74; acc: 0.41
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

4.45892583229579e-05
1.4959842701500747e-05
Batch: 0; loss: 1.68; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.34
Batch: 40; loss: 1.32; acc: 0.67
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.52
Batch: 100; loss: 1.66; acc: 0.45
Batch: 120; loss: 1.97; acc: 0.33
Batch: 140; loss: 1.38; acc: 0.7
Val Epoch over. val_loss: 1.5795263667015513; val_accuracy: 0.5333399681528662 

The current subspace-distance is: 1.4959842701500747e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.55
Batch: 20; loss: 1.55; acc: 0.56
Batch: 40; loss: 1.82; acc: 0.36
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.73; acc: 0.42
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.71; acc: 0.39
Batch: 160; loss: 1.75; acc: 0.47
Batch: 180; loss: 1.59; acc: 0.58
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.57; acc: 0.56
Batch: 240; loss: 1.64; acc: 0.45
Batch: 260; loss: 1.7; acc: 0.41
Batch: 280; loss: 1.54; acc: 0.5
Batch: 300; loss: 1.61; acc: 0.47
Batch: 320; loss: 1.58; acc: 0.58
Batch: 340; loss: 1.37; acc: 0.64
Batch: 360; loss: 1.57; acc: 0.59
Batch: 380; loss: 1.71; acc: 0.48
Batch: 400; loss: 1.53; acc: 0.56
Batch: 420; loss: 1.75; acc: 0.45
Batch: 440; loss: 1.75; acc: 0.52
Batch: 460; loss: 1.69; acc: 0.45
Batch: 480; loss: 1.68; acc: 0.45
Batch: 500; loss: 1.63; acc: 0.45
Batch: 520; loss: 1.44; acc: 0.58
Batch: 540; loss: 1.67; acc: 0.47
Batch: 560; loss: 1.69; acc: 0.47
Batch: 580; loss: 1.7; acc: 0.41
Batch: 600; loss: 1.55; acc: 0.53
Batch: 620; loss: 1.65; acc: 0.47
Batch: 640; loss: 1.54; acc: 0.56
Batch: 660; loss: 1.57; acc: 0.48
Batch: 680; loss: 1.64; acc: 0.48
Batch: 700; loss: 1.73; acc: 0.45
Batch: 720; loss: 1.63; acc: 0.55
Batch: 740; loss: 1.64; acc: 0.48
Batch: 760; loss: 1.68; acc: 0.48
Batch: 780; loss: 1.53; acc: 0.61
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.504719254327938e-05
1.612820960872341e-05
Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.8; acc: 0.33
Batch: 40; loss: 1.32; acc: 0.69
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.53
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.96; acc: 0.33
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.5713914367044048; val_accuracy: 0.5363256369426752 

The current subspace-distance is: 1.612820960872341e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.56
Batch: 20; loss: 1.51; acc: 0.61
Batch: 40; loss: 1.6; acc: 0.53
Batch: 60; loss: 1.69; acc: 0.5
Batch: 80; loss: 1.55; acc: 0.53
Batch: 100; loss: 1.52; acc: 0.58
Batch: 120; loss: 1.69; acc: 0.45
Batch: 140; loss: 1.61; acc: 0.53
Batch: 160; loss: 1.65; acc: 0.55
Batch: 180; loss: 1.58; acc: 0.55
Batch: 200; loss: 1.61; acc: 0.47
Batch: 220; loss: 1.6; acc: 0.5
Batch: 240; loss: 1.72; acc: 0.5
Batch: 260; loss: 1.58; acc: 0.5
Batch: 280; loss: 1.6; acc: 0.47
Batch: 300; loss: 1.65; acc: 0.48
Batch: 320; loss: 1.64; acc: 0.5
Batch: 340; loss: 1.62; acc: 0.52
Batch: 360; loss: 1.43; acc: 0.59
Batch: 380; loss: 1.71; acc: 0.42
Batch: 400; loss: 1.68; acc: 0.44
Batch: 420; loss: 1.59; acc: 0.52
Batch: 440; loss: 1.62; acc: 0.55
Batch: 460; loss: 1.57; acc: 0.56
Batch: 480; loss: 1.59; acc: 0.53
Batch: 500; loss: 1.56; acc: 0.58
Batch: 520; loss: 1.58; acc: 0.56
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.56; acc: 0.48
Batch: 580; loss: 1.61; acc: 0.53
Batch: 600; loss: 1.6; acc: 0.55
Batch: 620; loss: 1.63; acc: 0.52
Batch: 640; loss: 1.6; acc: 0.53
Batch: 660; loss: 1.53; acc: 0.59
Batch: 680; loss: 1.56; acc: 0.56
Batch: 700; loss: 1.53; acc: 0.47
Batch: 720; loss: 1.8; acc: 0.48
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.44; acc: 0.62
Batch: 780; loss: 1.62; acc: 0.5
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

4.597348015522584e-05
1.7859378203866072e-05
Batch: 0; loss: 1.68; acc: 0.44
Batch: 20; loss: 1.8; acc: 0.31
Batch: 40; loss: 1.31; acc: 0.7
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.45
Batch: 120; loss: 1.97; acc: 0.34
Batch: 140; loss: 1.35; acc: 0.72
Val Epoch over. val_loss: 1.5680134805144779; val_accuracy: 0.5367237261146497 

The current subspace-distance is: 1.7859378203866072e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.66; acc: 0.56
Batch: 20; loss: 1.47; acc: 0.61
Batch: 40; loss: 1.68; acc: 0.59
Batch: 60; loss: 1.73; acc: 0.41
Batch: 80; loss: 1.64; acc: 0.44
Batch: 100; loss: 1.54; acc: 0.53
Batch: 120; loss: 1.86; acc: 0.34
Batch: 140; loss: 1.51; acc: 0.59
Batch: 160; loss: 1.71; acc: 0.39
Batch: 180; loss: 1.61; acc: 0.55
Batch: 200; loss: 1.56; acc: 0.53
Batch: 220; loss: 1.65; acc: 0.5
Batch: 240; loss: 1.67; acc: 0.44
Batch: 260; loss: 1.37; acc: 0.66
Batch: 280; loss: 1.69; acc: 0.53
Batch: 300; loss: 1.64; acc: 0.5
Batch: 320; loss: 1.65; acc: 0.53
Batch: 340; loss: 1.55; acc: 0.59
Batch: 360; loss: 1.69; acc: 0.53
Batch: 380; loss: 1.66; acc: 0.47
Batch: 400; loss: 1.67; acc: 0.48
Batch: 420; loss: 1.72; acc: 0.45
Batch: 440; loss: 1.68; acc: 0.47
Batch: 460; loss: 1.73; acc: 0.41
Batch: 480; loss: 1.53; acc: 0.59
Batch: 500; loss: 1.61; acc: 0.55
Batch: 520; loss: 1.55; acc: 0.61
Batch: 540; loss: 1.62; acc: 0.44
Batch: 560; loss: 1.59; acc: 0.58
Batch: 580; loss: 1.5; acc: 0.59
Batch: 600; loss: 1.46; acc: 0.55
Batch: 620; loss: 1.68; acc: 0.5
Batch: 640; loss: 1.55; acc: 0.59
Batch: 660; loss: 1.53; acc: 0.61
Batch: 680; loss: 1.56; acc: 0.62
Batch: 700; loss: 1.65; acc: 0.41
Batch: 720; loss: 1.66; acc: 0.48
Batch: 740; loss: 1.61; acc: 0.47
Batch: 760; loss: 1.71; acc: 0.45
Batch: 780; loss: 1.74; acc: 0.45
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.505378456087783e-05
1.5647432519472204e-05
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.8; acc: 0.33
Batch: 40; loss: 1.31; acc: 0.69
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.55
Batch: 100; loss: 1.65; acc: 0.44
Batch: 120; loss: 1.96; acc: 0.34
Batch: 140; loss: 1.36; acc: 0.73
Val Epoch over. val_loss: 1.568888225373189; val_accuracy: 0.5378184713375797 

The current subspace-distance is: 1.5647432519472204e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.6; acc: 0.53
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.58
Batch: 100; loss: 1.63; acc: 0.44
Batch: 120; loss: 1.57; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.53
Batch: 160; loss: 1.58; acc: 0.47
Batch: 180; loss: 1.64; acc: 0.55
Batch: 200; loss: 1.57; acc: 0.5
Batch: 220; loss: 1.59; acc: 0.5
Batch: 240; loss: 1.64; acc: 0.5
Batch: 260; loss: 1.53; acc: 0.64
Batch: 280; loss: 1.64; acc: 0.55
Batch: 300; loss: 1.42; acc: 0.62
Batch: 320; loss: 1.57; acc: 0.48
Batch: 340; loss: 1.51; acc: 0.58
Batch: 360; loss: 1.66; acc: 0.47
Batch: 380; loss: 1.57; acc: 0.56
Batch: 400; loss: 1.63; acc: 0.5
Batch: 420; loss: 1.43; acc: 0.58
Batch: 440; loss: 1.49; acc: 0.56
Batch: 460; loss: 1.65; acc: 0.45
Batch: 480; loss: 1.62; acc: 0.42
Batch: 500; loss: 1.58; acc: 0.59
Batch: 520; loss: 1.59; acc: 0.48
Batch: 540; loss: 1.62; acc: 0.48
Batch: 560; loss: 1.59; acc: 0.58
Batch: 580; loss: 1.57; acc: 0.55
Batch: 600; loss: 1.5; acc: 0.56
Batch: 620; loss: 1.67; acc: 0.47
Batch: 640; loss: 1.53; acc: 0.59
Batch: 660; loss: 1.68; acc: 0.52
Batch: 680; loss: 1.83; acc: 0.45
Batch: 700; loss: 1.72; acc: 0.45
Batch: 720; loss: 1.61; acc: 0.48
Batch: 740; loss: 1.71; acc: 0.52
Batch: 760; loss: 1.64; acc: 0.47
Batch: 780; loss: 1.56; acc: 0.53
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

4.485753015615046e-05
1.6612455510767177e-05
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.8; acc: 0.31
Batch: 40; loss: 1.3; acc: 0.64
Batch: 60; loss: 1.5; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.52
Batch: 100; loss: 1.65; acc: 0.42
Batch: 120; loss: 1.96; acc: 0.33
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.5680665764839026; val_accuracy: 0.5267714968152867 

The current subspace-distance is: 1.6612455510767177e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.56
Batch: 20; loss: 1.83; acc: 0.45
Batch: 40; loss: 1.6; acc: 0.44
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.66; acc: 0.42
Batch: 100; loss: 1.73; acc: 0.5
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.63; acc: 0.58
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.81; acc: 0.45
Batch: 200; loss: 1.81; acc: 0.39
Batch: 220; loss: 1.62; acc: 0.5
Batch: 240; loss: 1.65; acc: 0.41
Batch: 260; loss: 1.63; acc: 0.45
Batch: 280; loss: 1.47; acc: 0.62
Batch: 300; loss: 1.57; acc: 0.5
Batch: 320; loss: 1.56; acc: 0.58
Batch: 340; loss: 1.58; acc: 0.61
Batch: 360; loss: 1.63; acc: 0.48
Batch: 380; loss: 1.58; acc: 0.5
Batch: 400; loss: 1.55; acc: 0.55
Batch: 420; loss: 1.62; acc: 0.45
Batch: 440; loss: 1.66; acc: 0.47
Batch: 460; loss: 1.68; acc: 0.44
Batch: 480; loss: 1.76; acc: 0.42
Batch: 500; loss: 1.77; acc: 0.39
Batch: 520; loss: 1.48; acc: 0.66
Batch: 540; loss: 1.5; acc: 0.59
Batch: 560; loss: 1.67; acc: 0.45
Batch: 580; loss: 1.62; acc: 0.48
Batch: 600; loss: 1.5; acc: 0.61
Batch: 620; loss: 1.69; acc: 0.48
Batch: 640; loss: 1.63; acc: 0.53
Batch: 660; loss: 1.57; acc: 0.56
Batch: 680; loss: 1.7; acc: 0.45
Batch: 700; loss: 1.65; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.48
Batch: 740; loss: 1.77; acc: 0.47
Batch: 760; loss: 1.51; acc: 0.59
Batch: 780; loss: 1.56; acc: 0.52
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

4.514905594987795e-05
1.4311550330603495e-05
Batch: 0; loss: 1.67; acc: 0.5
Batch: 20; loss: 1.8; acc: 0.36
Batch: 40; loss: 1.31; acc: 0.7
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.5; acc: 0.53
Batch: 100; loss: 1.66; acc: 0.45
Batch: 120; loss: 1.97; acc: 0.33
Batch: 140; loss: 1.36; acc: 0.69
Val Epoch over. val_loss: 1.5659390741093144; val_accuracy: 0.5387141719745223 

The current subspace-distance is: 1.4311550330603495e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_7_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.75

The number of parameters is: 271389

The number of individual parameters is:

14
252
14
14
21
38220
21
21
42
114660
42
42
64
112896
64
64
4096
64
640
10
64
64

nonzero elements in E: 27138897
elements in E: 27138900
fraction nonzero: 0.9999998894575682
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.37; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.19
Batch: 40; loss: 2.18; acc: 0.2
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.15; acc: 0.2
Batch: 100; loss: 2.06; acc: 0.28
Batch: 120; loss: 2.02; acc: 0.34
Batch: 140; loss: 2.05; acc: 0.41
Batch: 160; loss: 2.0; acc: 0.3
Batch: 180; loss: 1.95; acc: 0.38
Batch: 200; loss: 1.98; acc: 0.28
Batch: 220; loss: 1.89; acc: 0.53
Batch: 240; loss: 1.94; acc: 0.41
Batch: 260; loss: 1.86; acc: 0.39
Batch: 280; loss: 1.91; acc: 0.41
Batch: 300; loss: 1.85; acc: 0.48
Batch: 320; loss: 1.71; acc: 0.59
Batch: 340; loss: 1.94; acc: 0.34
Batch: 360; loss: 1.85; acc: 0.47
Batch: 380; loss: 1.68; acc: 0.62
Batch: 400; loss: 1.71; acc: 0.55
Batch: 420; loss: 1.8; acc: 0.39
Batch: 440; loss: 1.69; acc: 0.59
Batch: 460; loss: 1.63; acc: 0.61
Batch: 480; loss: 1.78; acc: 0.5
Batch: 500; loss: 1.71; acc: 0.45
Batch: 520; loss: 1.66; acc: 0.55
Batch: 540; loss: 1.71; acc: 0.52
Batch: 560; loss: 1.67; acc: 0.56
Batch: 580; loss: 1.61; acc: 0.62
Batch: 600; loss: 1.62; acc: 0.62
Batch: 620; loss: 1.69; acc: 0.52
Batch: 640; loss: 1.53; acc: 0.7
Batch: 660; loss: 1.63; acc: 0.58
Batch: 680; loss: 1.59; acc: 0.59
Batch: 700; loss: 1.6; acc: 0.64
Batch: 720; loss: 1.58; acc: 0.64
Batch: 740; loss: 1.51; acc: 0.64
Batch: 760; loss: 1.57; acc: 0.67
Batch: 780; loss: 1.71; acc: 0.47
Train Epoch over. train_loss: 1.84; train_accuracy: 0.46 

5.8205136156175286e-05
5.3214007493807e-05
Batch: 0; loss: 1.67; acc: 0.53
Batch: 20; loss: 1.7; acc: 0.47
Batch: 40; loss: 1.35; acc: 0.75
Batch: 60; loss: 1.64; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.69
Batch: 100; loss: 1.66; acc: 0.59
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.78
Val Epoch over. val_loss: 1.5786353091525425; val_accuracy: 0.6157444267515924 

The current subspace-distance is: 5.3214007493807e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.66
Batch: 20; loss: 1.6; acc: 0.64
Batch: 40; loss: 1.59; acc: 0.53
Batch: 60; loss: 1.51; acc: 0.67
Batch: 80; loss: 1.61; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.61
Batch: 120; loss: 1.56; acc: 0.61
Batch: 140; loss: 1.57; acc: 0.67
Batch: 160; loss: 1.73; acc: 0.5
Batch: 180; loss: 1.53; acc: 0.61
Batch: 200; loss: 1.56; acc: 0.62
Batch: 220; loss: 1.46; acc: 0.69
Batch: 240; loss: 1.6; acc: 0.62
Batch: 260; loss: 1.45; acc: 0.73
Batch: 280; loss: 1.73; acc: 0.53
Batch: 300; loss: 1.52; acc: 0.58
Batch: 320; loss: 1.57; acc: 0.58
Batch: 340; loss: 1.57; acc: 0.62
Batch: 360; loss: 1.6; acc: 0.58
Batch: 380; loss: 1.46; acc: 0.62
Batch: 400; loss: 1.45; acc: 0.66
Batch: 420; loss: 1.6; acc: 0.59
Batch: 440; loss: 1.52; acc: 0.59
Batch: 460; loss: 1.59; acc: 0.55
Batch: 480; loss: 1.48; acc: 0.64
Batch: 500; loss: 1.41; acc: 0.73
Batch: 520; loss: 1.59; acc: 0.58
Batch: 540; loss: 1.61; acc: 0.55
Batch: 560; loss: 1.55; acc: 0.59
Batch: 580; loss: 1.46; acc: 0.7
Batch: 600; loss: 1.49; acc: 0.64
Batch: 620; loss: 1.46; acc: 0.69
Batch: 640; loss: 1.47; acc: 0.64
Batch: 660; loss: 1.54; acc: 0.64
Batch: 680; loss: 1.48; acc: 0.61
Batch: 700; loss: 1.46; acc: 0.64
Batch: 720; loss: 1.51; acc: 0.67
Batch: 740; loss: 1.43; acc: 0.72
Batch: 760; loss: 1.48; acc: 0.7
Batch: 780; loss: 1.47; acc: 0.7
Train Epoch over. train_loss: 1.53; train_accuracy: 0.63 

7.455216109519824e-05
6.839617708465084e-05
Batch: 0; loss: 1.49; acc: 0.64
Batch: 20; loss: 1.54; acc: 0.59
Batch: 40; loss: 1.19; acc: 0.78
Batch: 60; loss: 1.45; acc: 0.69
Batch: 80; loss: 1.43; acc: 0.73
Batch: 100; loss: 1.46; acc: 0.72
Batch: 120; loss: 1.48; acc: 0.64
Batch: 140; loss: 1.36; acc: 0.75
Val Epoch over. val_loss: 1.4449153417234968; val_accuracy: 0.6762539808917197 

The current subspace-distance is: 6.839617708465084e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.37; acc: 0.69
Batch: 20; loss: 1.48; acc: 0.66
Batch: 40; loss: 1.33; acc: 0.8
Batch: 60; loss: 1.42; acc: 0.67
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.49; acc: 0.66
Batch: 120; loss: 1.47; acc: 0.69
Batch: 140; loss: 1.4; acc: 0.75
Batch: 160; loss: 1.45; acc: 0.69
Batch: 180; loss: 1.37; acc: 0.69
Batch: 200; loss: 1.45; acc: 0.69
Batch: 220; loss: 1.47; acc: 0.61
Batch: 240; loss: 1.34; acc: 0.8
Batch: 260; loss: 1.39; acc: 0.56
Batch: 280; loss: 1.39; acc: 0.69
Batch: 300; loss: 1.5; acc: 0.59
Batch: 320; loss: 1.56; acc: 0.62
Batch: 340; loss: 1.41; acc: 0.69
Batch: 360; loss: 1.42; acc: 0.67
Batch: 380; loss: 1.52; acc: 0.58
Batch: 400; loss: 1.47; acc: 0.61
Batch: 420; loss: 1.4; acc: 0.7
Batch: 440; loss: 1.42; acc: 0.62
Batch: 460; loss: 1.51; acc: 0.69
Batch: 480; loss: 1.49; acc: 0.66
Batch: 500; loss: 1.51; acc: 0.62
Batch: 520; loss: 1.38; acc: 0.75
Batch: 540; loss: 1.34; acc: 0.73
Batch: 560; loss: 1.35; acc: 0.72
Batch: 580; loss: 1.43; acc: 0.64
Batch: 600; loss: 1.34; acc: 0.7
Batch: 620; loss: 1.44; acc: 0.66
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.38; acc: 0.69
Batch: 680; loss: 1.4; acc: 0.67
Batch: 700; loss: 1.46; acc: 0.61
Batch: 720; loss: 1.48; acc: 0.66
Batch: 740; loss: 1.4; acc: 0.67
Batch: 760; loss: 1.41; acc: 0.73
Batch: 780; loss: 1.45; acc: 0.64
Train Epoch over. train_loss: 1.45; train_accuracy: 0.65 

8.565489406464621e-05
8.111929491860792e-05
Batch: 0; loss: 1.4; acc: 0.66
Batch: 20; loss: 1.51; acc: 0.58
Batch: 40; loss: 1.1; acc: 0.83
Batch: 60; loss: 1.35; acc: 0.7
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.7
Batch: 120; loss: 1.45; acc: 0.66
Batch: 140; loss: 1.33; acc: 0.7
Val Epoch over. val_loss: 1.3685259629207052; val_accuracy: 0.6882961783439491 

The current subspace-distance is: 8.111929491860792e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.67
Batch: 20; loss: 1.51; acc: 0.62
Batch: 40; loss: 1.52; acc: 0.64
Batch: 60; loss: 1.27; acc: 0.7
Batch: 80; loss: 1.59; acc: 0.55
Batch: 100; loss: 1.4; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.62
Batch: 140; loss: 1.54; acc: 0.64
Batch: 160; loss: 1.38; acc: 0.67
Batch: 180; loss: 1.51; acc: 0.61
Batch: 200; loss: 1.37; acc: 0.66
Batch: 220; loss: 1.45; acc: 0.61
Batch: 240; loss: 1.38; acc: 0.67
Batch: 260; loss: 1.38; acc: 0.66
Batch: 280; loss: 1.46; acc: 0.66
Batch: 300; loss: 1.5; acc: 0.62
Batch: 320; loss: 1.35; acc: 0.67
Batch: 340; loss: 1.5; acc: 0.56
Batch: 360; loss: 1.38; acc: 0.72
Batch: 380; loss: 1.47; acc: 0.62
Batch: 400; loss: 1.42; acc: 0.66
Batch: 420; loss: 1.25; acc: 0.75
Batch: 440; loss: 1.34; acc: 0.8
Batch: 460; loss: 1.38; acc: 0.62
Batch: 480; loss: 1.31; acc: 0.67
Batch: 500; loss: 1.37; acc: 0.64
Batch: 520; loss: 1.53; acc: 0.52
Batch: 540; loss: 1.31; acc: 0.7
Batch: 560; loss: 1.24; acc: 0.75
Batch: 580; loss: 1.45; acc: 0.7
Batch: 600; loss: 1.35; acc: 0.61
Batch: 620; loss: 1.31; acc: 0.66
Batch: 640; loss: 1.32; acc: 0.61
Batch: 660; loss: 1.43; acc: 0.58
Batch: 680; loss: 1.47; acc: 0.62
Batch: 700; loss: 1.39; acc: 0.66
Batch: 720; loss: 1.33; acc: 0.62
Batch: 740; loss: 1.44; acc: 0.64
Batch: 760; loss: 1.29; acc: 0.75
Batch: 780; loss: 1.29; acc: 0.69
Train Epoch over. train_loss: 1.39; train_accuracy: 0.66 

9.621323260944337e-05
9.206943650497124e-05
Batch: 0; loss: 1.38; acc: 0.66
Batch: 20; loss: 1.51; acc: 0.58
Batch: 40; loss: 1.08; acc: 0.86
Batch: 60; loss: 1.31; acc: 0.73
Batch: 80; loss: 1.33; acc: 0.69
Batch: 100; loss: 1.35; acc: 0.73
Batch: 120; loss: 1.41; acc: 0.69
Batch: 140; loss: 1.33; acc: 0.66
Val Epoch over. val_loss: 1.3270587784469507; val_accuracy: 0.7061106687898089 

The current subspace-distance is: 9.206943650497124e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.34; acc: 0.69
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 1.32; acc: 0.73
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.62
Batch: 100; loss: 1.27; acc: 0.73
Batch: 120; loss: 1.35; acc: 0.61
Batch: 140; loss: 1.4; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.7
Batch: 180; loss: 1.24; acc: 0.73
Batch: 200; loss: 1.34; acc: 0.75
Batch: 220; loss: 1.41; acc: 0.62
Batch: 240; loss: 1.36; acc: 0.69
Batch: 260; loss: 1.38; acc: 0.64
Batch: 280; loss: 1.46; acc: 0.59
Batch: 300; loss: 1.28; acc: 0.72
Batch: 320; loss: 1.38; acc: 0.64
Batch: 340; loss: 1.39; acc: 0.67
Batch: 360; loss: 1.32; acc: 0.66
Batch: 380; loss: 1.47; acc: 0.62
Batch: 400; loss: 1.28; acc: 0.72
Batch: 420; loss: 1.28; acc: 0.69
Batch: 440; loss: 1.29; acc: 0.77
Batch: 460; loss: 1.18; acc: 0.78
Batch: 480; loss: 1.43; acc: 0.61
Batch: 500; loss: 1.43; acc: 0.66
Batch: 520; loss: 1.33; acc: 0.64
Batch: 540; loss: 1.22; acc: 0.75
Batch: 560; loss: 1.31; acc: 0.69
Batch: 580; loss: 1.42; acc: 0.59
Batch: 600; loss: 1.35; acc: 0.67
Batch: 620; loss: 1.43; acc: 0.66
Batch: 640; loss: 1.15; acc: 0.78
Batch: 660; loss: 1.27; acc: 0.78
Batch: 680; loss: 1.36; acc: 0.7
Batch: 700; loss: 1.59; acc: 0.52
Batch: 720; loss: 1.18; acc: 0.78
Batch: 740; loss: 1.26; acc: 0.7
Batch: 760; loss: 1.39; acc: 0.7
Batch: 780; loss: 1.36; acc: 0.67
Train Epoch over. train_loss: 1.34; train_accuracy: 0.67 

0.00010640299296937883
0.00010083977394970134
Batch: 0; loss: 1.34; acc: 0.69
Batch: 20; loss: 1.49; acc: 0.59
Batch: 40; loss: 1.03; acc: 0.84
Batch: 60; loss: 1.26; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.69
Batch: 100; loss: 1.29; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.72
Batch: 140; loss: 1.27; acc: 0.66
Val Epoch over. val_loss: 1.2731418875372333; val_accuracy: 0.7215366242038217 

The current subspace-distance is: 0.00010083977394970134 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.69
Batch: 40; loss: 1.37; acc: 0.62
Batch: 60; loss: 1.33; acc: 0.7
Batch: 80; loss: 1.34; acc: 0.7
Batch: 100; loss: 1.31; acc: 0.7
Batch: 120; loss: 1.23; acc: 0.72
Batch: 140; loss: 1.31; acc: 0.66
Batch: 160; loss: 1.31; acc: 0.64
Batch: 180; loss: 1.3; acc: 0.67
Batch: 200; loss: 1.24; acc: 0.75
Batch: 220; loss: 1.19; acc: 0.78
Batch: 240; loss: 1.26; acc: 0.62
Batch: 260; loss: 1.35; acc: 0.59
Batch: 280; loss: 1.33; acc: 0.72
Batch: 300; loss: 1.3; acc: 0.66
Batch: 320; loss: 1.41; acc: 0.59
Batch: 340; loss: 1.26; acc: 0.67
Batch: 360; loss: 1.36; acc: 0.62
Batch: 380; loss: 1.21; acc: 0.75
Batch: 400; loss: 1.28; acc: 0.67
Batch: 420; loss: 1.42; acc: 0.66
Batch: 440; loss: 1.32; acc: 0.66
Batch: 460; loss: 1.31; acc: 0.64
Batch: 480; loss: 1.29; acc: 0.67
Batch: 500; loss: 1.25; acc: 0.72
Batch: 520; loss: 1.34; acc: 0.73
Batch: 540; loss: 1.22; acc: 0.75
Batch: 560; loss: 1.36; acc: 0.61
Batch: 580; loss: 1.39; acc: 0.64
Batch: 600; loss: 1.42; acc: 0.67
Batch: 620; loss: 1.26; acc: 0.62
Batch: 640; loss: 1.23; acc: 0.75
Batch: 660; loss: 1.43; acc: 0.59
Batch: 680; loss: 1.41; acc: 0.61
Batch: 700; loss: 1.32; acc: 0.72
Batch: 720; loss: 1.33; acc: 0.72
Batch: 740; loss: 1.23; acc: 0.75
Batch: 760; loss: 1.2; acc: 0.78
Batch: 780; loss: 1.28; acc: 0.66
Train Epoch over. train_loss: 1.31; train_accuracy: 0.68 

0.00011822347005363554
0.00011194745457032695
Batch: 0; loss: 1.33; acc: 0.64
Batch: 20; loss: 1.46; acc: 0.64
Batch: 40; loss: 1.0; acc: 0.81
Batch: 60; loss: 1.22; acc: 0.73
Batch: 80; loss: 1.23; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.77
Batch: 120; loss: 1.32; acc: 0.7
Batch: 140; loss: 1.2; acc: 0.72
Val Epoch over. val_loss: 1.2322570214605635; val_accuracy: 0.7311902866242038 

The current subspace-distance is: 0.00011194745457032695 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.72
Batch: 20; loss: 1.35; acc: 0.61
Batch: 40; loss: 1.16; acc: 0.77
Batch: 60; loss: 1.38; acc: 0.64
Batch: 80; loss: 1.35; acc: 0.58
Batch: 100; loss: 1.26; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 1.4; acc: 0.64
Batch: 160; loss: 1.48; acc: 0.52
Batch: 180; loss: 1.33; acc: 0.64
Batch: 200; loss: 1.27; acc: 0.7
Batch: 220; loss: 1.27; acc: 0.69
Batch: 240; loss: 1.23; acc: 0.77
Batch: 260; loss: 1.24; acc: 0.7
Batch: 280; loss: 1.2; acc: 0.7
Batch: 300; loss: 1.25; acc: 0.72
Batch: 320; loss: 1.25; acc: 0.67
Batch: 340; loss: 1.38; acc: 0.58
Batch: 360; loss: 1.49; acc: 0.53
Batch: 380; loss: 1.25; acc: 0.64
Batch: 400; loss: 1.26; acc: 0.77
Batch: 420; loss: 1.28; acc: 0.67
Batch: 440; loss: 1.2; acc: 0.69
Batch: 460; loss: 1.4; acc: 0.64
Batch: 480; loss: 1.28; acc: 0.72
Batch: 500; loss: 1.33; acc: 0.67
Batch: 520; loss: 1.28; acc: 0.72
Batch: 540; loss: 1.28; acc: 0.7
Batch: 560; loss: 1.2; acc: 0.7
Batch: 580; loss: 1.28; acc: 0.69
Batch: 600; loss: 1.28; acc: 0.67
Batch: 620; loss: 1.22; acc: 0.72
Batch: 640; loss: 1.25; acc: 0.66
Batch: 660; loss: 1.25; acc: 0.67
Batch: 680; loss: 1.19; acc: 0.73
Batch: 700; loss: 1.29; acc: 0.69
Batch: 720; loss: 1.21; acc: 0.73
Batch: 740; loss: 1.29; acc: 0.67
Batch: 760; loss: 1.34; acc: 0.62
Batch: 780; loss: 1.3; acc: 0.72
Train Epoch over. train_loss: 1.27; train_accuracy: 0.69 

0.00012710345617961138
0.00011866341083077714
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.44; acc: 0.64
Batch: 40; loss: 0.97; acc: 0.83
Batch: 60; loss: 1.18; acc: 0.75
Batch: 80; loss: 1.2; acc: 0.77
Batch: 100; loss: 1.16; acc: 0.77
Batch: 120; loss: 1.31; acc: 0.7
Batch: 140; loss: 1.14; acc: 0.75
Val Epoch over. val_loss: 1.2007582817867304; val_accuracy: 0.7356687898089171 

The current subspace-distance is: 0.00011866341083077714 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.8
Batch: 20; loss: 1.23; acc: 0.73
Batch: 40; loss: 1.32; acc: 0.7
Batch: 60; loss: 1.36; acc: 0.64
Batch: 80; loss: 1.31; acc: 0.7
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 1.2; acc: 0.73
Batch: 160; loss: 1.31; acc: 0.67
Batch: 180; loss: 1.18; acc: 0.66
Batch: 200; loss: 1.22; acc: 0.67
Batch: 220; loss: 1.36; acc: 0.62
Batch: 240; loss: 1.22; acc: 0.66
Batch: 260; loss: 1.15; acc: 0.77
Batch: 280; loss: 1.24; acc: 0.67
Batch: 300; loss: 1.14; acc: 0.75
Batch: 320; loss: 1.31; acc: 0.66
Batch: 340; loss: 1.28; acc: 0.7
Batch: 360; loss: 1.27; acc: 0.66
Batch: 380; loss: 1.3; acc: 0.66
Batch: 400; loss: 1.36; acc: 0.66
Batch: 420; loss: 1.2; acc: 0.67
Batch: 440; loss: 1.22; acc: 0.73
Batch: 460; loss: 1.17; acc: 0.7
Batch: 480; loss: 1.4; acc: 0.58
Batch: 500; loss: 1.29; acc: 0.64
Batch: 520; loss: 1.26; acc: 0.72
Batch: 540; loss: 1.19; acc: 0.69
Batch: 560; loss: 1.31; acc: 0.61
Batch: 580; loss: 1.19; acc: 0.77
Batch: 600; loss: 1.17; acc: 0.78
Batch: 620; loss: 1.35; acc: 0.58
Batch: 640; loss: 1.23; acc: 0.8
Batch: 660; loss: 1.4; acc: 0.61
Batch: 680; loss: 1.22; acc: 0.69
Batch: 700; loss: 1.22; acc: 0.72
Batch: 720; loss: 1.27; acc: 0.72
Batch: 740; loss: 1.19; acc: 0.72
Batch: 760; loss: 1.33; acc: 0.58
Batch: 780; loss: 1.21; acc: 0.72
Train Epoch over. train_loss: 1.24; train_accuracy: 0.7 

0.00013520799984689802
0.000128392202896066
Batch: 0; loss: 1.25; acc: 0.67
Batch: 20; loss: 1.38; acc: 0.66
Batch: 40; loss: 0.95; acc: 0.86
Batch: 60; loss: 1.14; acc: 0.77
Batch: 80; loss: 1.16; acc: 0.78
Batch: 100; loss: 1.12; acc: 0.77
Batch: 120; loss: 1.3; acc: 0.72
Batch: 140; loss: 1.09; acc: 0.77
Val Epoch over. val_loss: 1.1686639641500582; val_accuracy: 0.7477109872611465 

The current subspace-distance is: 0.000128392202896066 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.77
Batch: 20; loss: 1.42; acc: 0.61
Batch: 40; loss: 1.21; acc: 0.75
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.33; acc: 0.69
Batch: 100; loss: 1.25; acc: 0.67
Batch: 120; loss: 1.34; acc: 0.66
Batch: 140; loss: 1.17; acc: 0.66
Batch: 160; loss: 1.25; acc: 0.72
Batch: 180; loss: 1.17; acc: 0.67
Batch: 200; loss: 1.19; acc: 0.72
Batch: 220; loss: 1.09; acc: 0.73
Batch: 240; loss: 1.37; acc: 0.56
Batch: 260; loss: 1.2; acc: 0.72
Batch: 280; loss: 1.13; acc: 0.7
Batch: 300; loss: 1.22; acc: 0.7
Batch: 320; loss: 1.38; acc: 0.53
Batch: 340; loss: 1.24; acc: 0.7
Batch: 360; loss: 1.27; acc: 0.67
Batch: 380; loss: 1.21; acc: 0.66
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 1.17; acc: 0.72
Batch: 440; loss: 1.11; acc: 0.73
Batch: 460; loss: 1.22; acc: 0.7
Batch: 480; loss: 1.24; acc: 0.69
Batch: 500; loss: 1.12; acc: 0.75
Batch: 520; loss: 1.0; acc: 0.81
Batch: 540; loss: 1.43; acc: 0.62
Batch: 560; loss: 1.22; acc: 0.72
Batch: 580; loss: 1.0; acc: 0.8
Batch: 600; loss: 1.28; acc: 0.66
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 1.17; acc: 0.7
Batch: 660; loss: 1.2; acc: 0.7
Batch: 680; loss: 1.19; acc: 0.66
Batch: 700; loss: 1.08; acc: 0.8
Batch: 720; loss: 1.22; acc: 0.72
Batch: 740; loss: 1.32; acc: 0.64
Batch: 760; loss: 1.32; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.64
Train Epoch over. train_loss: 1.22; train_accuracy: 0.71 

0.00014258487499319017
0.00013777954154647887
Batch: 0; loss: 1.2; acc: 0.7
Batch: 20; loss: 1.35; acc: 0.64
Batch: 40; loss: 0.92; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.77
Batch: 80; loss: 1.12; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.3; acc: 0.7
Batch: 140; loss: 1.08; acc: 0.78
Val Epoch over. val_loss: 1.147576362843726; val_accuracy: 0.7476114649681529 

The current subspace-distance is: 0.00013777954154647887 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.24; acc: 0.73
Batch: 40; loss: 1.16; acc: 0.8
Batch: 60; loss: 1.28; acc: 0.7
Batch: 80; loss: 1.27; acc: 0.73
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 0.99; acc: 0.81
Batch: 160; loss: 1.12; acc: 0.81
Batch: 180; loss: 1.1; acc: 0.78
Batch: 200; loss: 1.31; acc: 0.64
Batch: 220; loss: 1.2; acc: 0.75
Batch: 240; loss: 1.12; acc: 0.75
Batch: 260; loss: 1.32; acc: 0.59
Batch: 280; loss: 1.24; acc: 0.69
Batch: 300; loss: 1.34; acc: 0.56
Batch: 320; loss: 0.99; acc: 0.83
Batch: 340; loss: 1.25; acc: 0.66
Batch: 360; loss: 1.3; acc: 0.67
Batch: 380; loss: 1.13; acc: 0.78
Batch: 400; loss: 1.2; acc: 0.66
Batch: 420; loss: 1.19; acc: 0.72
Batch: 440; loss: 1.37; acc: 0.69
Batch: 460; loss: 1.15; acc: 0.72
Batch: 480; loss: 1.26; acc: 0.66
Batch: 500; loss: 1.15; acc: 0.77
Batch: 520; loss: 1.08; acc: 0.77
Batch: 540; loss: 1.16; acc: 0.7
Batch: 560; loss: 1.16; acc: 0.72
Batch: 580; loss: 1.12; acc: 0.75
Batch: 600; loss: 1.3; acc: 0.61
Batch: 620; loss: 1.19; acc: 0.7
Batch: 640; loss: 1.45; acc: 0.48
Batch: 660; loss: 1.15; acc: 0.83
Batch: 680; loss: 1.18; acc: 0.72
Batch: 700; loss: 1.16; acc: 0.72
Batch: 720; loss: 1.29; acc: 0.64
Batch: 740; loss: 1.24; acc: 0.7
Batch: 760; loss: 1.24; acc: 0.67
Batch: 780; loss: 1.18; acc: 0.7
Train Epoch over. train_loss: 1.2; train_accuracy: 0.71 

0.00014966633170843124
0.00014422164531424642
Batch: 0; loss: 1.17; acc: 0.72
Batch: 20; loss: 1.31; acc: 0.66
Batch: 40; loss: 0.9; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.77
Batch: 80; loss: 1.09; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.77
Batch: 120; loss: 1.28; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.8
Val Epoch over. val_loss: 1.1259416835323262; val_accuracy: 0.7515923566878981 

The current subspace-distance is: 0.00014422164531424642 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 1.23; acc: 0.61
Batch: 60; loss: 1.16; acc: 0.77
Batch: 80; loss: 1.26; acc: 0.67
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 1.37; acc: 0.66
Batch: 160; loss: 1.11; acc: 0.77
Batch: 180; loss: 1.36; acc: 0.59
Batch: 200; loss: 1.21; acc: 0.66
Batch: 220; loss: 1.2; acc: 0.69
Batch: 240; loss: 1.19; acc: 0.66
Batch: 260; loss: 1.09; acc: 0.77
Batch: 280; loss: 1.09; acc: 0.8
Batch: 300; loss: 1.16; acc: 0.73
Batch: 320; loss: 1.36; acc: 0.55
Batch: 340; loss: 1.2; acc: 0.64
Batch: 360; loss: 1.32; acc: 0.61
Batch: 380; loss: 1.18; acc: 0.69
Batch: 400; loss: 1.29; acc: 0.66
Batch: 420; loss: 1.21; acc: 0.72
Batch: 440; loss: 1.14; acc: 0.75
Batch: 460; loss: 1.13; acc: 0.77
Batch: 480; loss: 1.2; acc: 0.64
Batch: 500; loss: 1.25; acc: 0.69
Batch: 520; loss: 1.01; acc: 0.8
Batch: 540; loss: 1.14; acc: 0.75
Batch: 560; loss: 1.2; acc: 0.7
Batch: 580; loss: 1.23; acc: 0.69
Batch: 600; loss: 1.24; acc: 0.69
Batch: 620; loss: 1.05; acc: 0.81
Batch: 640; loss: 1.32; acc: 0.62
Batch: 660; loss: 1.05; acc: 0.78
Batch: 680; loss: 1.1; acc: 0.77
Batch: 700; loss: 1.13; acc: 0.77
Batch: 720; loss: 1.13; acc: 0.75
Batch: 740; loss: 1.29; acc: 0.59
Batch: 760; loss: 1.03; acc: 0.8
Batch: 780; loss: 1.15; acc: 0.77
Train Epoch over. train_loss: 1.18; train_accuracy: 0.71 

0.0001529529836261645
0.0001472748990636319
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.75
Batch: 80; loss: 1.08; acc: 0.8
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.28; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.78
Val Epoch over. val_loss: 1.1180966819167897; val_accuracy: 0.7503980891719745 

The current subspace-distance is: 0.0001472748990636319 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.17; acc: 0.72
Batch: 40; loss: 1.17; acc: 0.73
Batch: 60; loss: 1.08; acc: 0.75
Batch: 80; loss: 1.1; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.04; acc: 0.8
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.22; acc: 0.67
Batch: 180; loss: 1.24; acc: 0.67
Batch: 200; loss: 1.18; acc: 0.73
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 1.16; acc: 0.7
Batch: 260; loss: 1.33; acc: 0.66
Batch: 280; loss: 1.21; acc: 0.7
Batch: 300; loss: 1.2; acc: 0.73
Batch: 320; loss: 1.29; acc: 0.61
Batch: 340; loss: 1.12; acc: 0.72
Batch: 360; loss: 1.22; acc: 0.69
Batch: 380; loss: 1.25; acc: 0.72
Batch: 400; loss: 1.29; acc: 0.72
Batch: 420; loss: 1.01; acc: 0.83
Batch: 440; loss: 1.11; acc: 0.78
Batch: 460; loss: 1.12; acc: 0.72
Batch: 480; loss: 1.4; acc: 0.59
Batch: 500; loss: 1.28; acc: 0.66
Batch: 520; loss: 1.2; acc: 0.75
Batch: 540; loss: 1.15; acc: 0.8
Batch: 560; loss: 1.13; acc: 0.69
Batch: 580; loss: 1.2; acc: 0.7
Batch: 600; loss: 1.1; acc: 0.77
Batch: 620; loss: 0.95; acc: 0.88
Batch: 640; loss: 1.16; acc: 0.73
Batch: 660; loss: 1.1; acc: 0.7
Batch: 680; loss: 1.04; acc: 0.77
Batch: 700; loss: 1.39; acc: 0.62
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.14; acc: 0.75
Batch: 760; loss: 1.23; acc: 0.69
Batch: 780; loss: 1.19; acc: 0.75
Train Epoch over. train_loss: 1.18; train_accuracy: 0.71 

0.0001564069534651935
0.00015013161464594305
Batch: 0; loss: 1.16; acc: 0.78
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.78
Batch: 80; loss: 1.08; acc: 0.81
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.77
Val Epoch over. val_loss: 1.122053755696412; val_accuracy: 0.7456210191082803 

The current subspace-distance is: 0.00015013161464594305 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 1.01; acc: 0.78
Batch: 60; loss: 1.25; acc: 0.72
Batch: 80; loss: 1.33; acc: 0.61
Batch: 100; loss: 1.27; acc: 0.66
Batch: 120; loss: 1.11; acc: 0.77
Batch: 140; loss: 1.11; acc: 0.78
Batch: 160; loss: 1.18; acc: 0.73
Batch: 180; loss: 1.24; acc: 0.59
Batch: 200; loss: 1.05; acc: 0.75
Batch: 220; loss: 1.38; acc: 0.58
Batch: 240; loss: 1.14; acc: 0.72
Batch: 260; loss: 1.15; acc: 0.73
Batch: 280; loss: 1.02; acc: 0.83
Batch: 300; loss: 1.04; acc: 0.81
Batch: 320; loss: 1.19; acc: 0.72
Batch: 340; loss: 1.28; acc: 0.69
Batch: 360; loss: 1.2; acc: 0.72
Batch: 380; loss: 1.08; acc: 0.73
Batch: 400; loss: 1.22; acc: 0.7
Batch: 420; loss: 1.09; acc: 0.83
Batch: 440; loss: 0.93; acc: 0.84
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.3; acc: 0.66
Batch: 520; loss: 1.1; acc: 0.75
Batch: 540; loss: 1.11; acc: 0.75
Batch: 560; loss: 1.25; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.73
Batch: 600; loss: 0.99; acc: 0.8
Batch: 620; loss: 1.06; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.72
Batch: 660; loss: 1.03; acc: 0.86
Batch: 680; loss: 1.27; acc: 0.66
Batch: 700; loss: 1.15; acc: 0.77
Batch: 720; loss: 1.14; acc: 0.7
Batch: 740; loss: 1.06; acc: 0.73
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 1.23; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.71 

0.00015719309158157557
0.00014958652900531888
Batch: 0; loss: 1.15; acc: 0.77
Batch: 20; loss: 1.32; acc: 0.64
Batch: 40; loss: 0.87; acc: 0.84
Batch: 60; loss: 1.1; acc: 0.78
Batch: 80; loss: 1.08; acc: 0.81
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.77
Val Epoch over. val_loss: 1.1139876052832147; val_accuracy: 0.7437300955414012 

The current subspace-distance is: 0.00014958652900531888 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.62
Batch: 20; loss: 1.45; acc: 0.58
Batch: 40; loss: 1.11; acc: 0.72
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.21; acc: 0.66
Batch: 100; loss: 1.24; acc: 0.64
Batch: 120; loss: 1.12; acc: 0.73
Batch: 140; loss: 1.29; acc: 0.59
Batch: 160; loss: 1.15; acc: 0.67
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 1.1; acc: 0.77
Batch: 220; loss: 1.07; acc: 0.83
Batch: 240; loss: 1.3; acc: 0.58
Batch: 260; loss: 1.19; acc: 0.72
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.18; acc: 0.73
Batch: 320; loss: 1.26; acc: 0.62
Batch: 340; loss: 1.3; acc: 0.66
Batch: 360; loss: 1.07; acc: 0.78
Batch: 380; loss: 1.26; acc: 0.66
Batch: 400; loss: 1.26; acc: 0.69
Batch: 420; loss: 1.36; acc: 0.55
Batch: 440; loss: 1.22; acc: 0.67
Batch: 460; loss: 1.15; acc: 0.73
Batch: 480; loss: 1.13; acc: 0.69
Batch: 500; loss: 1.15; acc: 0.75
Batch: 520; loss: 1.16; acc: 0.73
Batch: 540; loss: 1.03; acc: 0.8
Batch: 560; loss: 1.18; acc: 0.7
Batch: 580; loss: 1.33; acc: 0.59
Batch: 600; loss: 1.06; acc: 0.8
Batch: 620; loss: 1.08; acc: 0.78
Batch: 640; loss: 1.01; acc: 0.75
Batch: 660; loss: 1.22; acc: 0.77
Batch: 680; loss: 1.23; acc: 0.61
Batch: 700; loss: 1.05; acc: 0.73
Batch: 720; loss: 1.11; acc: 0.73
Batch: 740; loss: 1.16; acc: 0.78
Batch: 760; loss: 0.96; acc: 0.84
Batch: 780; loss: 1.17; acc: 0.72
Train Epoch over. train_loss: 1.17; train_accuracy: 0.71 

0.00015996856382116675
0.00015338719822466373
Batch: 0; loss: 1.14; acc: 0.78
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 0.86; acc: 0.84
Batch: 60; loss: 1.1; acc: 0.81
Batch: 80; loss: 1.07; acc: 0.81
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.78
Val Epoch over. val_loss: 1.1118369402399488; val_accuracy: 0.7464171974522293 

The current subspace-distance is: 0.00015338719822466373 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.21; acc: 0.72
Batch: 20; loss: 1.24; acc: 0.72
Batch: 40; loss: 1.31; acc: 0.64
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.84
Batch: 100; loss: 1.18; acc: 0.7
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.67
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 1.28; acc: 0.66
Batch: 200; loss: 1.18; acc: 0.62
Batch: 220; loss: 1.15; acc: 0.72
Batch: 240; loss: 1.13; acc: 0.75
Batch: 260; loss: 1.1; acc: 0.72
Batch: 280; loss: 1.2; acc: 0.7
Batch: 300; loss: 1.18; acc: 0.67
Batch: 320; loss: 1.22; acc: 0.67
Batch: 340; loss: 1.26; acc: 0.66
Batch: 360; loss: 1.14; acc: 0.75
Batch: 380; loss: 1.25; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.72
Batch: 420; loss: 1.21; acc: 0.72
Batch: 440; loss: 1.17; acc: 0.77
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.22; acc: 0.64
Batch: 500; loss: 1.17; acc: 0.75
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 1.17; acc: 0.77
Batch: 560; loss: 1.14; acc: 0.67
Batch: 580; loss: 1.1; acc: 0.8
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.17; acc: 0.67
Batch: 640; loss: 1.31; acc: 0.62
Batch: 660; loss: 1.07; acc: 0.77
Batch: 680; loss: 1.1; acc: 0.72
Batch: 700; loss: 1.15; acc: 0.73
Batch: 720; loss: 1.08; acc: 0.75
Batch: 740; loss: 1.1; acc: 0.75
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 1.04; acc: 0.78
Train Epoch over. train_loss: 1.17; train_accuracy: 0.71 

0.00016140022489707917
0.00015559839084744453
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.31; acc: 0.66
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.8
Batch: 80; loss: 1.05; acc: 0.78
Batch: 100; loss: 1.03; acc: 0.78
Batch: 120; loss: 1.26; acc: 0.7
Batch: 140; loss: 1.04; acc: 0.75
Val Epoch over. val_loss: 1.101909929400037; val_accuracy: 0.7476114649681529 

The current subspace-distance is: 0.00015559839084744453 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.25; acc: 0.72
Batch: 20; loss: 1.18; acc: 0.73
Batch: 40; loss: 1.17; acc: 0.69
Batch: 60; loss: 1.26; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 1.33; acc: 0.58
Batch: 120; loss: 1.24; acc: 0.72
Batch: 140; loss: 1.26; acc: 0.66
Batch: 160; loss: 1.24; acc: 0.64
Batch: 180; loss: 1.24; acc: 0.69
Batch: 200; loss: 1.12; acc: 0.73
Batch: 220; loss: 1.1; acc: 0.7
Batch: 240; loss: 1.33; acc: 0.59
Batch: 260; loss: 1.39; acc: 0.61
Batch: 280; loss: 1.06; acc: 0.8
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.17; acc: 0.72
Batch: 340; loss: 1.2; acc: 0.67
Batch: 360; loss: 1.16; acc: 0.75
Batch: 380; loss: 1.1; acc: 0.7
Batch: 400; loss: 1.12; acc: 0.69
Batch: 420; loss: 1.2; acc: 0.67
Batch: 440; loss: 1.15; acc: 0.7
Batch: 460; loss: 1.19; acc: 0.75
Batch: 480; loss: 1.02; acc: 0.8
Batch: 500; loss: 1.34; acc: 0.64
Batch: 520; loss: 1.09; acc: 0.78
Batch: 540; loss: 1.28; acc: 0.66
Batch: 560; loss: 1.09; acc: 0.8
Batch: 580; loss: 1.06; acc: 0.77
Batch: 600; loss: 1.11; acc: 0.73
Batch: 620; loss: 1.1; acc: 0.78
Batch: 640; loss: 1.11; acc: 0.75
Batch: 660; loss: 1.19; acc: 0.67
Batch: 680; loss: 1.09; acc: 0.7
Batch: 700; loss: 1.18; acc: 0.75
Batch: 720; loss: 1.13; acc: 0.78
Batch: 740; loss: 1.13; acc: 0.72
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.08; acc: 0.8
Train Epoch over. train_loss: 1.16; train_accuracy: 0.71 

0.00016647196025587618
0.00015872086805757135
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.32; acc: 0.64
Batch: 40; loss: 0.86; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.81
Batch: 80; loss: 1.06; acc: 0.8
Batch: 100; loss: 1.02; acc: 0.78
Batch: 120; loss: 1.28; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.77
Val Epoch over. val_loss: 1.1053782454721488; val_accuracy: 0.7449243630573248 

The current subspace-distance is: 0.00015872086805757135 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.73
Batch: 20; loss: 1.02; acc: 0.81
Batch: 40; loss: 1.29; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.77
Batch: 80; loss: 1.18; acc: 0.62
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.06; acc: 0.75
Batch: 140; loss: 1.19; acc: 0.69
Batch: 160; loss: 1.09; acc: 0.77
Batch: 180; loss: 1.19; acc: 0.77
Batch: 200; loss: 1.24; acc: 0.66
Batch: 220; loss: 1.31; acc: 0.69
Batch: 240; loss: 1.22; acc: 0.66
Batch: 260; loss: 1.12; acc: 0.7
Batch: 280; loss: 1.09; acc: 0.8
Batch: 300; loss: 1.06; acc: 0.7
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.12; acc: 0.73
Batch: 360; loss: 1.16; acc: 0.67
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 1.21; acc: 0.75
Batch: 420; loss: 1.18; acc: 0.67
Batch: 440; loss: 1.12; acc: 0.78
Batch: 460; loss: 1.21; acc: 0.67
Batch: 480; loss: 1.06; acc: 0.78
Batch: 500; loss: 1.07; acc: 0.78
Batch: 520; loss: 1.28; acc: 0.69
Batch: 540; loss: 1.09; acc: 0.77
Batch: 560; loss: 1.23; acc: 0.61
Batch: 580; loss: 1.11; acc: 0.72
Batch: 600; loss: 1.01; acc: 0.8
Batch: 620; loss: 1.23; acc: 0.61
Batch: 640; loss: 1.25; acc: 0.7
Batch: 660; loss: 1.13; acc: 0.7
Batch: 680; loss: 1.13; acc: 0.75
Batch: 700; loss: 1.23; acc: 0.67
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 0.99; acc: 0.81
Batch: 780; loss: 1.16; acc: 0.7
Train Epoch over. train_loss: 1.16; train_accuracy: 0.71 

0.00016777653945609927
0.0001600299437996
Batch: 0; loss: 1.14; acc: 0.77
Batch: 20; loss: 1.34; acc: 0.64
Batch: 40; loss: 0.87; acc: 0.86
Batch: 60; loss: 1.08; acc: 0.8
Batch: 80; loss: 1.05; acc: 0.8
Batch: 100; loss: 1.04; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.77
Val Epoch over. val_loss: 1.1064948432005135; val_accuracy: 0.7477109872611465 

The current subspace-distance is: 0.0001600299437996 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.64
Batch: 40; loss: 1.32; acc: 0.67
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 1.18; acc: 0.67
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.13; acc: 0.69
Batch: 200; loss: 1.12; acc: 0.72
Batch: 220; loss: 1.12; acc: 0.75
Batch: 240; loss: 1.14; acc: 0.7
Batch: 260; loss: 1.05; acc: 0.75
Batch: 280; loss: 1.11; acc: 0.75
Batch: 300; loss: 1.34; acc: 0.58
Batch: 320; loss: 1.03; acc: 0.84
Batch: 340; loss: 1.2; acc: 0.69
Batch: 360; loss: 1.03; acc: 0.81
Batch: 380; loss: 1.14; acc: 0.69
Batch: 400; loss: 1.28; acc: 0.61
Batch: 420; loss: 1.26; acc: 0.67
Batch: 440; loss: 1.06; acc: 0.8
Batch: 460; loss: 1.19; acc: 0.61
Batch: 480; loss: 1.14; acc: 0.72
Batch: 500; loss: 1.21; acc: 0.7
Batch: 520; loss: 1.09; acc: 0.77
Batch: 540; loss: 1.06; acc: 0.78
Batch: 560; loss: 1.09; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.09; acc: 0.75
Batch: 620; loss: 1.26; acc: 0.66
Batch: 640; loss: 1.08; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.66
Batch: 680; loss: 1.13; acc: 0.72
Batch: 700; loss: 1.24; acc: 0.66
Batch: 720; loss: 1.14; acc: 0.7
Batch: 740; loss: 1.1; acc: 0.8
Batch: 760; loss: 1.2; acc: 0.73
Batch: 780; loss: 1.3; acc: 0.64
Train Epoch over. train_loss: 1.15; train_accuracy: 0.71 

0.0001654772786423564
0.00016086541290860623
Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.31; acc: 0.66
Batch: 40; loss: 0.85; acc: 0.86
Batch: 60; loss: 1.08; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.83
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.75
Val Epoch over. val_loss: 1.0918067640559688; val_accuracy: 0.7493033439490446 

The current subspace-distance is: 0.00016086541290860623 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.15; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.84
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.81
Batch: 100; loss: 1.25; acc: 0.66
Batch: 120; loss: 0.91; acc: 0.83
Batch: 140; loss: 1.35; acc: 0.64
Batch: 160; loss: 1.18; acc: 0.72
Batch: 180; loss: 1.13; acc: 0.64
Batch: 200; loss: 1.2; acc: 0.69
Batch: 220; loss: 1.11; acc: 0.72
Batch: 240; loss: 1.27; acc: 0.62
Batch: 260; loss: 1.24; acc: 0.69
Batch: 280; loss: 1.18; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.69
Batch: 320; loss: 1.29; acc: 0.59
Batch: 340; loss: 1.29; acc: 0.56
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.17; acc: 0.67
Batch: 400; loss: 1.11; acc: 0.7
Batch: 420; loss: 1.11; acc: 0.75
Batch: 440; loss: 1.08; acc: 0.78
Batch: 460; loss: 1.23; acc: 0.67
Batch: 480; loss: 1.17; acc: 0.69
Batch: 500; loss: 1.08; acc: 0.73
Batch: 520; loss: 1.38; acc: 0.56
Batch: 540; loss: 1.17; acc: 0.75
Batch: 560; loss: 1.09; acc: 0.7
Batch: 580; loss: 1.04; acc: 0.8
Batch: 600; loss: 1.12; acc: 0.77
Batch: 620; loss: 1.03; acc: 0.78
Batch: 640; loss: 1.25; acc: 0.61
Batch: 660; loss: 1.1; acc: 0.7
Batch: 680; loss: 1.23; acc: 0.67
Batch: 700; loss: 1.05; acc: 0.78
Batch: 720; loss: 1.21; acc: 0.75
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 1.25; acc: 0.67
Batch: 780; loss: 1.06; acc: 0.69
Train Epoch over. train_loss: 1.15; train_accuracy: 0.71 

0.00016854798013810068
0.00016172041068784893
Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 0.84; acc: 0.88
Batch: 60; loss: 1.08; acc: 0.8
Batch: 80; loss: 1.02; acc: 0.81
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.04; acc: 0.78
Val Epoch over. val_loss: 1.0859659781121904; val_accuracy: 0.751890923566879 

The current subspace-distance is: 0.00016172041068784893 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.15; acc: 0.78
Batch: 20; loss: 1.14; acc: 0.75
Batch: 40; loss: 1.25; acc: 0.72
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.23; acc: 0.7
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.06; acc: 0.78
Batch: 140; loss: 1.21; acc: 0.66
Batch: 160; loss: 1.05; acc: 0.81
Batch: 180; loss: 1.04; acc: 0.8
Batch: 200; loss: 1.32; acc: 0.61
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 1.05; acc: 0.77
Batch: 260; loss: 1.03; acc: 0.81
Batch: 280; loss: 1.18; acc: 0.69
Batch: 300; loss: 1.11; acc: 0.75
Batch: 320; loss: 1.23; acc: 0.72
Batch: 340; loss: 1.22; acc: 0.66
Batch: 360; loss: 1.25; acc: 0.62
Batch: 380; loss: 1.06; acc: 0.75
Batch: 400; loss: 1.07; acc: 0.77
Batch: 420; loss: 1.29; acc: 0.64
Batch: 440; loss: 1.12; acc: 0.69
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.15; acc: 0.67
Batch: 500; loss: 1.34; acc: 0.59
Batch: 520; loss: 1.14; acc: 0.75
Batch: 540; loss: 1.13; acc: 0.72
Batch: 560; loss: 1.16; acc: 0.72
Batch: 580; loss: 1.06; acc: 0.73
Batch: 600; loss: 1.05; acc: 0.77
Batch: 620; loss: 1.09; acc: 0.72
Batch: 640; loss: 1.34; acc: 0.61
Batch: 660; loss: 1.23; acc: 0.69
Batch: 680; loss: 1.17; acc: 0.69
Batch: 700; loss: 1.25; acc: 0.66
Batch: 720; loss: 1.15; acc: 0.77
Batch: 740; loss: 1.25; acc: 0.67
Batch: 760; loss: 1.18; acc: 0.62
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.15; train_accuracy: 0.71 

0.00016853628039825708
0.0001622803829377517
Batch: 0; loss: 1.12; acc: 0.77
Batch: 20; loss: 1.33; acc: 0.66
Batch: 40; loss: 0.84; acc: 0.86
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.81
Batch: 100; loss: 1.02; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.72
Batch: 140; loss: 1.05; acc: 0.77
Val Epoch over. val_loss: 1.0948351439397046; val_accuracy: 0.7485071656050956 

The current subspace-distance is: 0.0001622803829377517 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 1.32; acc: 0.62
Batch: 60; loss: 1.25; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.72
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.18; acc: 0.69
Batch: 140; loss: 1.14; acc: 0.73
Batch: 160; loss: 1.15; acc: 0.69
Batch: 180; loss: 1.1; acc: 0.72
Batch: 200; loss: 0.98; acc: 0.83
Batch: 220; loss: 1.04; acc: 0.75
Batch: 240; loss: 1.08; acc: 0.75
Batch: 260; loss: 1.05; acc: 0.77
Batch: 280; loss: 1.23; acc: 0.69
Batch: 300; loss: 1.29; acc: 0.64
Batch: 320; loss: 1.12; acc: 0.67
Batch: 340; loss: 1.02; acc: 0.77
Batch: 360; loss: 1.14; acc: 0.69
Batch: 380; loss: 1.25; acc: 0.67
Batch: 400; loss: 1.26; acc: 0.64
Batch: 420; loss: 1.11; acc: 0.69
Batch: 440; loss: 1.12; acc: 0.75
Batch: 460; loss: 0.98; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.73
Batch: 500; loss: 1.17; acc: 0.69
Batch: 520; loss: 1.23; acc: 0.7
Batch: 540; loss: 1.14; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.75
Batch: 580; loss: 1.12; acc: 0.67
Batch: 600; loss: 1.27; acc: 0.62
Batch: 620; loss: 1.11; acc: 0.73
Batch: 640; loss: 1.28; acc: 0.64
Batch: 660; loss: 1.2; acc: 0.64
Batch: 680; loss: 1.21; acc: 0.67
Batch: 700; loss: 1.07; acc: 0.81
Batch: 720; loss: 1.02; acc: 0.77
Batch: 740; loss: 1.19; acc: 0.67
Batch: 760; loss: 1.06; acc: 0.8
Batch: 780; loss: 1.07; acc: 0.72
Train Epoch over. train_loss: 1.15; train_accuracy: 0.71 

0.00016993886674754322
0.0001641654089326039
Batch: 0; loss: 1.12; acc: 0.77
Batch: 20; loss: 1.33; acc: 0.66
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.8
Batch: 100; loss: 1.02; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.73
Val Epoch over. val_loss: 1.101655690153693; val_accuracy: 0.7430334394904459 

The current subspace-distance is: 0.0001641654089326039 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 1.12; acc: 0.7
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.21; acc: 0.66
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 1.15; acc: 0.7
Batch: 180; loss: 1.13; acc: 0.69
Batch: 200; loss: 1.11; acc: 0.72
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.04; acc: 0.78
Batch: 260; loss: 1.28; acc: 0.64
Batch: 280; loss: 1.21; acc: 0.7
Batch: 300; loss: 1.02; acc: 0.7
Batch: 320; loss: 1.15; acc: 0.61
Batch: 340; loss: 1.13; acc: 0.73
Batch: 360; loss: 1.11; acc: 0.75
Batch: 380; loss: 1.03; acc: 0.78
Batch: 400; loss: 1.25; acc: 0.67
Batch: 420; loss: 1.09; acc: 0.67
Batch: 440; loss: 1.35; acc: 0.58
Batch: 460; loss: 1.05; acc: 0.77
Batch: 480; loss: 1.21; acc: 0.66
Batch: 500; loss: 1.19; acc: 0.7
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.11; acc: 0.72
Batch: 560; loss: 1.14; acc: 0.72
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 1.18; acc: 0.66
Batch: 620; loss: 1.12; acc: 0.8
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.13; acc: 0.72
Batch: 680; loss: 1.16; acc: 0.7
Batch: 700; loss: 1.28; acc: 0.66
Batch: 720; loss: 1.2; acc: 0.67
Batch: 740; loss: 1.07; acc: 0.73
Batch: 760; loss: 1.32; acc: 0.64
Batch: 780; loss: 1.03; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.0001713375240797177
0.00016540411161258817
Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 1.34; acc: 0.64
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.81
Batch: 100; loss: 1.01; acc: 0.77
Batch: 120; loss: 1.29; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.75
Val Epoch over. val_loss: 1.0963781369719536; val_accuracy: 0.7456210191082803 

The current subspace-distance is: 0.00016540411161258817 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.26; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.7
Batch: 40; loss: 1.1; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.66
Batch: 80; loss: 1.5; acc: 0.48
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 1.16; acc: 0.7
Batch: 160; loss: 1.08; acc: 0.77
Batch: 180; loss: 1.32; acc: 0.62
Batch: 200; loss: 1.16; acc: 0.75
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.14; acc: 0.72
Batch: 260; loss: 1.15; acc: 0.7
Batch: 280; loss: 1.18; acc: 0.77
Batch: 300; loss: 1.04; acc: 0.77
Batch: 320; loss: 1.1; acc: 0.72
Batch: 340; loss: 1.04; acc: 0.72
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.19; acc: 0.73
Batch: 400; loss: 1.08; acc: 0.75
Batch: 420; loss: 1.17; acc: 0.72
Batch: 440; loss: 1.04; acc: 0.8
Batch: 460; loss: 1.16; acc: 0.69
Batch: 480; loss: 1.17; acc: 0.75
Batch: 500; loss: 1.0; acc: 0.77
Batch: 520; loss: 1.06; acc: 0.73
Batch: 540; loss: 1.05; acc: 0.72
Batch: 560; loss: 1.19; acc: 0.67
Batch: 580; loss: 1.35; acc: 0.59
Batch: 600; loss: 1.19; acc: 0.7
Batch: 620; loss: 1.16; acc: 0.75
Batch: 640; loss: 1.01; acc: 0.75
Batch: 660; loss: 1.24; acc: 0.61
Batch: 680; loss: 1.04; acc: 0.8
Batch: 700; loss: 1.29; acc: 0.58
Batch: 720; loss: 1.01; acc: 0.75
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 1.17; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.0001738321006996557
0.00016706771566532552
Batch: 0; loss: 1.09; acc: 0.75
Batch: 20; loss: 1.32; acc: 0.66
Batch: 40; loss: 0.82; acc: 0.88
Batch: 60; loss: 1.05; acc: 0.8
Batch: 80; loss: 1.01; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.03; acc: 0.77
Val Epoch over. val_loss: 1.075456465885138; val_accuracy: 0.751890923566879 

The current subspace-distance is: 0.00016706771566532552 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.39; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.75
Batch: 40; loss: 1.02; acc: 0.77
Batch: 60; loss: 1.07; acc: 0.73
Batch: 80; loss: 1.19; acc: 0.7
Batch: 100; loss: 1.01; acc: 0.83
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.14; acc: 0.72
Batch: 160; loss: 1.21; acc: 0.67
Batch: 180; loss: 1.14; acc: 0.7
Batch: 200; loss: 1.05; acc: 0.75
Batch: 220; loss: 0.98; acc: 0.86
Batch: 240; loss: 1.02; acc: 0.78
Batch: 260; loss: 1.19; acc: 0.69
Batch: 280; loss: 1.07; acc: 0.77
Batch: 300; loss: 1.22; acc: 0.67
Batch: 320; loss: 1.33; acc: 0.61
Batch: 340; loss: 1.16; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.78
Batch: 380; loss: 1.11; acc: 0.7
Batch: 400; loss: 1.15; acc: 0.77
Batch: 420; loss: 1.12; acc: 0.75
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.01; acc: 0.8
Batch: 500; loss: 0.87; acc: 0.86
Batch: 520; loss: 1.09; acc: 0.75
Batch: 540; loss: 1.04; acc: 0.73
Batch: 560; loss: 1.19; acc: 0.64
Batch: 580; loss: 1.28; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.67
Batch: 620; loss: 1.15; acc: 0.75
Batch: 640; loss: 1.18; acc: 0.7
Batch: 660; loss: 1.21; acc: 0.64
Batch: 680; loss: 1.05; acc: 0.8
Batch: 700; loss: 1.15; acc: 0.67
Batch: 720; loss: 1.02; acc: 0.77
Batch: 740; loss: 1.04; acc: 0.8
Batch: 760; loss: 1.19; acc: 0.66
Batch: 780; loss: 1.21; acc: 0.69
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.0001735695404931903
0.0001661520218476653
Batch: 0; loss: 1.11; acc: 0.77
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 0.85; acc: 0.86
Batch: 60; loss: 1.08; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.81
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.28; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.77
Val Epoch over. val_loss: 1.0963381278287074; val_accuracy: 0.7466162420382165 

The current subspace-distance is: 0.0001661520218476653 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.94; acc: 0.81
Batch: 20; loss: 1.11; acc: 0.75
Batch: 40; loss: 1.15; acc: 0.75
Batch: 60; loss: 1.34; acc: 0.61
Batch: 80; loss: 1.24; acc: 0.62
Batch: 100; loss: 1.09; acc: 0.7
Batch: 120; loss: 1.21; acc: 0.72
Batch: 140; loss: 0.95; acc: 0.86
Batch: 160; loss: 1.16; acc: 0.73
Batch: 180; loss: 1.04; acc: 0.78
Batch: 200; loss: 1.18; acc: 0.67
Batch: 220; loss: 1.16; acc: 0.7
Batch: 240; loss: 1.16; acc: 0.77
Batch: 260; loss: 1.18; acc: 0.77
Batch: 280; loss: 1.02; acc: 0.78
Batch: 300; loss: 1.18; acc: 0.67
Batch: 320; loss: 1.02; acc: 0.78
Batch: 340; loss: 1.21; acc: 0.72
Batch: 360; loss: 1.17; acc: 0.75
Batch: 380; loss: 1.1; acc: 0.72
Batch: 400; loss: 1.4; acc: 0.56
Batch: 420; loss: 1.0; acc: 0.83
Batch: 440; loss: 1.13; acc: 0.7
Batch: 460; loss: 1.08; acc: 0.73
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 1.2; acc: 0.7
Batch: 520; loss: 1.21; acc: 0.66
Batch: 540; loss: 1.1; acc: 0.78
Batch: 560; loss: 1.26; acc: 0.67
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.25; acc: 0.62
Batch: 620; loss: 1.05; acc: 0.78
Batch: 640; loss: 1.07; acc: 0.8
Batch: 660; loss: 1.19; acc: 0.67
Batch: 680; loss: 1.12; acc: 0.7
Batch: 700; loss: 1.26; acc: 0.62
Batch: 720; loss: 1.28; acc: 0.66
Batch: 740; loss: 1.09; acc: 0.78
Batch: 760; loss: 1.12; acc: 0.72
Batch: 780; loss: 1.15; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.0001739923463901505
0.00016761977167334408
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.31; acc: 0.67
Batch: 40; loss: 0.82; acc: 0.89
Batch: 60; loss: 1.06; acc: 0.8
Batch: 80; loss: 1.02; acc: 0.84
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.69
Batch: 140; loss: 1.03; acc: 0.78
Val Epoch over. val_loss: 1.0856493681099764; val_accuracy: 0.747312898089172 

The current subspace-distance is: 0.00016761977167334408 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.26; acc: 0.62
Batch: 20; loss: 1.1; acc: 0.77
Batch: 40; loss: 1.31; acc: 0.64
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 1.11; acc: 0.75
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 1.17; acc: 0.72
Batch: 160; loss: 0.97; acc: 0.83
Batch: 180; loss: 1.14; acc: 0.7
Batch: 200; loss: 1.1; acc: 0.77
Batch: 220; loss: 1.16; acc: 0.72
Batch: 240; loss: 1.23; acc: 0.7
Batch: 260; loss: 1.24; acc: 0.66
Batch: 280; loss: 1.33; acc: 0.58
Batch: 300; loss: 1.17; acc: 0.72
Batch: 320; loss: 1.15; acc: 0.67
Batch: 340; loss: 1.11; acc: 0.77
Batch: 360; loss: 1.16; acc: 0.66
Batch: 380; loss: 1.32; acc: 0.56
Batch: 400; loss: 1.12; acc: 0.7
Batch: 420; loss: 1.08; acc: 0.75
Batch: 440; loss: 1.14; acc: 0.73
Batch: 460; loss: 1.22; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.66
Batch: 500; loss: 1.19; acc: 0.77
Batch: 520; loss: 1.18; acc: 0.64
Batch: 540; loss: 1.16; acc: 0.66
Batch: 560; loss: 1.15; acc: 0.7
Batch: 580; loss: 1.02; acc: 0.8
Batch: 600; loss: 1.07; acc: 0.8
Batch: 620; loss: 1.16; acc: 0.69
Batch: 640; loss: 1.02; acc: 0.75
Batch: 660; loss: 1.05; acc: 0.75
Batch: 680; loss: 1.23; acc: 0.69
Batch: 700; loss: 1.13; acc: 0.67
Batch: 720; loss: 1.24; acc: 0.62
Batch: 740; loss: 1.06; acc: 0.8
Batch: 760; loss: 0.99; acc: 0.77
Batch: 780; loss: 1.12; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.00017427581769879907
0.00016885453078430146
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 0.83; acc: 0.86
Batch: 60; loss: 1.06; acc: 0.8
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.7
Batch: 140; loss: 1.03; acc: 0.78
Val Epoch over. val_loss: 1.083498998052755; val_accuracy: 0.7499004777070064 

The current subspace-distance is: 0.00016885453078430146 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.73
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 1.01; acc: 0.81
Batch: 60; loss: 1.23; acc: 0.62
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.17; acc: 0.75
Batch: 120; loss: 0.89; acc: 0.83
Batch: 140; loss: 1.28; acc: 0.64
Batch: 160; loss: 1.41; acc: 0.55
Batch: 180; loss: 1.16; acc: 0.7
Batch: 200; loss: 1.11; acc: 0.75
Batch: 220; loss: 1.17; acc: 0.69
Batch: 240; loss: 1.05; acc: 0.75
Batch: 260; loss: 1.01; acc: 0.81
Batch: 280; loss: 1.27; acc: 0.59
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 1.25; acc: 0.66
Batch: 340; loss: 0.96; acc: 0.83
Batch: 360; loss: 1.26; acc: 0.69
Batch: 380; loss: 1.15; acc: 0.67
Batch: 400; loss: 1.12; acc: 0.73
Batch: 420; loss: 1.1; acc: 0.73
Batch: 440; loss: 1.13; acc: 0.72
Batch: 460; loss: 1.03; acc: 0.8
Batch: 480; loss: 1.16; acc: 0.69
Batch: 500; loss: 1.11; acc: 0.75
Batch: 520; loss: 1.13; acc: 0.69
Batch: 540; loss: 1.13; acc: 0.73
Batch: 560; loss: 1.11; acc: 0.77
Batch: 580; loss: 1.07; acc: 0.72
Batch: 600; loss: 0.92; acc: 0.84
Batch: 620; loss: 1.15; acc: 0.75
Batch: 640; loss: 1.02; acc: 0.8
Batch: 660; loss: 1.16; acc: 0.69
Batch: 680; loss: 0.96; acc: 0.81
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 1.2; acc: 0.66
Batch: 740; loss: 0.99; acc: 0.81
Batch: 760; loss: 1.35; acc: 0.59
Batch: 780; loss: 1.31; acc: 0.66
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.00017639574070926756
0.00016886286903172731
Batch: 0; loss: 1.11; acc: 0.77
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 0.83; acc: 0.88
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.02; acc: 0.81
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.7
Batch: 140; loss: 1.04; acc: 0.78
Val Epoch over. val_loss: 1.0913911431458345; val_accuracy: 0.743531050955414 

The current subspace-distance is: 0.00016886286903172731 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.37; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.75
Batch: 40; loss: 1.13; acc: 0.77
Batch: 60; loss: 1.05; acc: 0.8
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.14; acc: 0.67
Batch: 200; loss: 1.15; acc: 0.75
Batch: 220; loss: 1.01; acc: 0.83
Batch: 240; loss: 1.08; acc: 0.69
Batch: 260; loss: 1.08; acc: 0.67
Batch: 280; loss: 1.2; acc: 0.64
Batch: 300; loss: 1.02; acc: 0.72
Batch: 320; loss: 1.05; acc: 0.75
Batch: 340; loss: 1.19; acc: 0.64
Batch: 360; loss: 1.14; acc: 0.7
Batch: 380; loss: 1.07; acc: 0.78
Batch: 400; loss: 1.23; acc: 0.66
Batch: 420; loss: 1.12; acc: 0.7
Batch: 440; loss: 1.16; acc: 0.67
Batch: 460; loss: 1.13; acc: 0.72
Batch: 480; loss: 1.15; acc: 0.7
Batch: 500; loss: 1.15; acc: 0.73
Batch: 520; loss: 1.23; acc: 0.62
Batch: 540; loss: 1.01; acc: 0.75
Batch: 560; loss: 1.11; acc: 0.75
Batch: 580; loss: 1.14; acc: 0.7
Batch: 600; loss: 1.14; acc: 0.72
Batch: 620; loss: 1.18; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.81
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.12; acc: 0.73
Batch: 700; loss: 1.25; acc: 0.66
Batch: 720; loss: 1.0; acc: 0.81
Batch: 740; loss: 1.17; acc: 0.66
Batch: 760; loss: 1.08; acc: 0.8
Batch: 780; loss: 1.02; acc: 0.78
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.0001741208543535322
0.000168882412253879
Batch: 0; loss: 1.09; acc: 0.8
Batch: 20; loss: 1.29; acc: 0.66
Batch: 40; loss: 0.83; acc: 0.88
Batch: 60; loss: 1.06; acc: 0.8
Batch: 80; loss: 1.0; acc: 0.84
Batch: 100; loss: 0.99; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.69
Batch: 140; loss: 1.01; acc: 0.81
Val Epoch over. val_loss: 1.0722663095042964; val_accuracy: 0.7534832802547771 

The current subspace-distance is: 0.000168882412253879 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.12; acc: 0.77
Batch: 20; loss: 1.23; acc: 0.67
Batch: 40; loss: 1.13; acc: 0.67
Batch: 60; loss: 1.27; acc: 0.69
Batch: 80; loss: 1.23; acc: 0.7
Batch: 100; loss: 1.18; acc: 0.67
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 1.38; acc: 0.55
Batch: 160; loss: 1.09; acc: 0.73
Batch: 180; loss: 1.11; acc: 0.73
Batch: 200; loss: 1.21; acc: 0.67
Batch: 220; loss: 0.97; acc: 0.81
Batch: 240; loss: 1.26; acc: 0.67
Batch: 260; loss: 1.21; acc: 0.67
Batch: 280; loss: 1.09; acc: 0.73
Batch: 300; loss: 1.09; acc: 0.75
Batch: 320; loss: 1.19; acc: 0.62
Batch: 340; loss: 1.26; acc: 0.61
Batch: 360; loss: 1.34; acc: 0.64
Batch: 380; loss: 1.28; acc: 0.67
Batch: 400; loss: 1.11; acc: 0.66
Batch: 420; loss: 1.07; acc: 0.73
Batch: 440; loss: 1.18; acc: 0.67
Batch: 460; loss: 1.14; acc: 0.7
Batch: 480; loss: 1.13; acc: 0.75
Batch: 500; loss: 1.18; acc: 0.66
Batch: 520; loss: 1.16; acc: 0.72
Batch: 540; loss: 1.13; acc: 0.72
Batch: 560; loss: 1.19; acc: 0.69
Batch: 580; loss: 1.18; acc: 0.72
Batch: 600; loss: 1.14; acc: 0.72
Batch: 620; loss: 1.08; acc: 0.78
Batch: 640; loss: 1.12; acc: 0.69
Batch: 660; loss: 1.13; acc: 0.73
Batch: 680; loss: 1.15; acc: 0.62
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.18; acc: 0.73
Batch: 740; loss: 1.17; acc: 0.72
Batch: 760; loss: 1.16; acc: 0.69
Batch: 780; loss: 1.09; acc: 0.72
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.0001773892145138234
0.00017218405264429748
Batch: 0; loss: 1.09; acc: 0.77
Batch: 20; loss: 1.3; acc: 0.67
Batch: 40; loss: 0.83; acc: 0.88
Batch: 60; loss: 1.06; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.84
Batch: 100; loss: 0.98; acc: 0.8
Batch: 120; loss: 1.27; acc: 0.72
Batch: 140; loss: 1.03; acc: 0.78
Val Epoch over. val_loss: 1.0763447535265782; val_accuracy: 0.7515923566878981 

The current subspace-distance is: 0.00017218405264429748 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.72
Batch: 20; loss: 1.13; acc: 0.73
Batch: 40; loss: 0.96; acc: 0.81
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 1.28; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.73
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 1.12; acc: 0.73
Batch: 160; loss: 1.28; acc: 0.62
Batch: 180; loss: 1.04; acc: 0.75
Batch: 200; loss: 1.27; acc: 0.62
Batch: 220; loss: 1.12; acc: 0.75
Batch: 240; loss: 1.07; acc: 0.75
Batch: 260; loss: 1.17; acc: 0.64
Batch: 280; loss: 0.98; acc: 0.81
Batch: 300; loss: 1.02; acc: 0.75
Batch: 320; loss: 1.08; acc: 0.77
Batch: 340; loss: 1.0; acc: 0.77
Batch: 360; loss: 1.22; acc: 0.69
Batch: 380; loss: 1.09; acc: 0.7
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.21; acc: 0.62
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.1; acc: 0.78
Batch: 480; loss: 1.12; acc: 0.69
Batch: 500; loss: 0.97; acc: 0.8
Batch: 520; loss: 1.15; acc: 0.73
Batch: 540; loss: 1.29; acc: 0.59
Batch: 560; loss: 1.01; acc: 0.84
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.17; acc: 0.67
Batch: 620; loss: 1.07; acc: 0.78
Batch: 640; loss: 1.14; acc: 0.75
Batch: 660; loss: 1.07; acc: 0.73
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.21; acc: 0.7
Batch: 720; loss: 1.01; acc: 0.81
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.14; train_accuracy: 0.71 

0.0001759817241691053
0.00016965904796961695
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.32; acc: 0.64
Batch: 40; loss: 0.82; acc: 0.89
Batch: 60; loss: 1.06; acc: 0.81
Batch: 80; loss: 1.01; acc: 0.81
Batch: 100; loss: 0.99; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.7
Batch: 140; loss: 1.04; acc: 0.78
Val Epoch over. val_loss: 1.0813009537708986; val_accuracy: 0.7483081210191083 

The current subspace-distance is: 0.00016965904796961695 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_7_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.75

The number of parameters is: 271389

The number of individual parameters is:

14
252
14
14
21
38220
21
21
42
114660
42
42
64
112896
64
64
4096
64
640
10
64
64

nonzero elements in E: 54277796
elements in E: 54277800
fraction nonzero: 0.9999999263050455
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.51; acc: 0.09
Batch: 20; loss: 2.21; acc: 0.23
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 1.97; acc: 0.38
Batch: 80; loss: 1.96; acc: 0.39
Batch: 100; loss: 1.75; acc: 0.53
Batch: 120; loss: 1.73; acc: 0.61
Batch: 140; loss: 1.77; acc: 0.47
Batch: 160; loss: 1.63; acc: 0.58
Batch: 180; loss: 1.65; acc: 0.61
Batch: 200; loss: 1.55; acc: 0.72
Batch: 220; loss: 1.52; acc: 0.7
Batch: 240; loss: 1.52; acc: 0.67
Batch: 260; loss: 1.5; acc: 0.72
Batch: 280; loss: 1.55; acc: 0.61
Batch: 300; loss: 1.57; acc: 0.62
Batch: 320; loss: 1.57; acc: 0.66
Batch: 340; loss: 1.43; acc: 0.75
Batch: 360; loss: 1.42; acc: 0.77
Batch: 380; loss: 1.42; acc: 0.69
Batch: 400; loss: 1.44; acc: 0.7
Batch: 420; loss: 1.36; acc: 0.73
Batch: 440; loss: 1.42; acc: 0.7
Batch: 460; loss: 1.4; acc: 0.64
Batch: 480; loss: 1.35; acc: 0.81
Batch: 500; loss: 1.37; acc: 0.75
Batch: 520; loss: 1.38; acc: 0.75
Batch: 540; loss: 1.32; acc: 0.75
Batch: 560; loss: 1.33; acc: 0.72
Batch: 580; loss: 1.31; acc: 0.77
Batch: 600; loss: 1.29; acc: 0.81
Batch: 620; loss: 1.26; acc: 0.84
Batch: 640; loss: 1.26; acc: 0.78
Batch: 660; loss: 1.3; acc: 0.75
Batch: 680; loss: 1.25; acc: 0.75
Batch: 700; loss: 1.24; acc: 0.83
Batch: 720; loss: 1.26; acc: 0.77
Batch: 740; loss: 1.22; acc: 0.81
Batch: 760; loss: 1.3; acc: 0.78
Batch: 780; loss: 1.17; acc: 0.86
Train Epoch over. train_loss: 1.52; train_accuracy: 0.66 

6.292066973401234e-05
5.764984234701842e-05
Batch: 0; loss: 1.35; acc: 0.69
Batch: 20; loss: 1.47; acc: 0.7
Batch: 40; loss: 1.05; acc: 0.91
Batch: 60; loss: 1.22; acc: 0.81
Batch: 80; loss: 1.1; acc: 0.91
Batch: 100; loss: 1.25; acc: 0.83
Batch: 120; loss: 1.4; acc: 0.61
Batch: 140; loss: 1.18; acc: 0.86
Val Epoch over. val_loss: 1.2578151051405888; val_accuracy: 0.7771695859872612 

The current subspace-distance is: 5.764984234701842e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.83
Batch: 20; loss: 1.27; acc: 0.77
Batch: 40; loss: 1.27; acc: 0.83
Batch: 60; loss: 1.27; acc: 0.8
Batch: 80; loss: 1.29; acc: 0.81
Batch: 100; loss: 1.27; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.73
Batch: 140; loss: 1.2; acc: 0.83
Batch: 160; loss: 1.46; acc: 0.69
Batch: 180; loss: 1.15; acc: 0.86
Batch: 200; loss: 1.39; acc: 0.67
Batch: 220; loss: 1.18; acc: 0.81
Batch: 240; loss: 1.2; acc: 0.8
Batch: 260; loss: 1.23; acc: 0.8
Batch: 280; loss: 1.25; acc: 0.72
Batch: 300; loss: 1.19; acc: 0.78
Batch: 320; loss: 1.3; acc: 0.75
Batch: 340; loss: 1.09; acc: 0.83
Batch: 360; loss: 1.26; acc: 0.81
Batch: 380; loss: 1.11; acc: 0.8
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.21; acc: 0.8
Batch: 440; loss: 1.2; acc: 0.8
Batch: 460; loss: 1.09; acc: 0.83
Batch: 480; loss: 1.2; acc: 0.78
Batch: 500; loss: 1.15; acc: 0.77
Batch: 520; loss: 1.11; acc: 0.83
Batch: 540; loss: 1.13; acc: 0.8
Batch: 560; loss: 1.28; acc: 0.69
Batch: 580; loss: 1.19; acc: 0.77
Batch: 600; loss: 1.08; acc: 0.83
Batch: 620; loss: 1.07; acc: 0.86
Batch: 640; loss: 1.06; acc: 0.78
Batch: 660; loss: 1.16; acc: 0.8
Batch: 680; loss: 1.08; acc: 0.75
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.12; acc: 0.84
Batch: 740; loss: 1.17; acc: 0.77
Batch: 760; loss: 1.06; acc: 0.89
Batch: 780; loss: 0.96; acc: 0.86
Train Epoch over. train_loss: 1.2; train_accuracy: 0.77 

8.751722634769976e-05
8.241964678745717e-05
Batch: 0; loss: 1.14; acc: 0.73
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 0.88; acc: 0.94
Batch: 60; loss: 1.07; acc: 0.81
Batch: 80; loss: 0.91; acc: 0.92
Batch: 100; loss: 1.12; acc: 0.78
Batch: 120; loss: 1.32; acc: 0.67
Batch: 140; loss: 1.02; acc: 0.86
Val Epoch over. val_loss: 1.0914297377227977; val_accuracy: 0.8134952229299363 

The current subspace-distance is: 8.241964678745717e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.73
Batch: 20; loss: 1.07; acc: 0.77
Batch: 40; loss: 1.12; acc: 0.8
Batch: 60; loss: 1.12; acc: 0.81
Batch: 80; loss: 1.15; acc: 0.75
Batch: 100; loss: 1.12; acc: 0.84
Batch: 120; loss: 0.99; acc: 0.91
Batch: 140; loss: 1.2; acc: 0.78
Batch: 160; loss: 1.11; acc: 0.84
Batch: 180; loss: 1.04; acc: 0.81
Batch: 200; loss: 1.15; acc: 0.75
Batch: 220; loss: 1.03; acc: 0.81
Batch: 240; loss: 1.05; acc: 0.77
Batch: 260; loss: 1.22; acc: 0.69
Batch: 280; loss: 1.07; acc: 0.86
Batch: 300; loss: 0.97; acc: 0.84
Batch: 320; loss: 1.07; acc: 0.81
Batch: 340; loss: 1.19; acc: 0.73
Batch: 360; loss: 1.19; acc: 0.77
Batch: 380; loss: 1.07; acc: 0.86
Batch: 400; loss: 0.89; acc: 0.88
Batch: 420; loss: 1.04; acc: 0.8
Batch: 440; loss: 1.17; acc: 0.67
Batch: 460; loss: 1.35; acc: 0.75
Batch: 480; loss: 1.11; acc: 0.75
Batch: 500; loss: 1.05; acc: 0.8
Batch: 520; loss: 1.04; acc: 0.8
Batch: 540; loss: 1.0; acc: 0.84
Batch: 560; loss: 1.11; acc: 0.77
Batch: 580; loss: 0.97; acc: 0.84
Batch: 600; loss: 1.1; acc: 0.75
Batch: 620; loss: 0.97; acc: 0.86
Batch: 640; loss: 1.02; acc: 0.84
Batch: 660; loss: 1.03; acc: 0.8
Batch: 680; loss: 1.04; acc: 0.77
Batch: 700; loss: 1.0; acc: 0.84
Batch: 720; loss: 0.98; acc: 0.81
Batch: 740; loss: 1.02; acc: 0.75
Batch: 760; loss: 1.04; acc: 0.88
Batch: 780; loss: 1.03; acc: 0.81
Train Epoch over. train_loss: 1.07; train_accuracy: 0.8 

0.0001087405180442147
0.00010432817362016067
Batch: 0; loss: 0.99; acc: 0.83
Batch: 20; loss: 1.23; acc: 0.72
Batch: 40; loss: 0.73; acc: 0.97
Batch: 60; loss: 0.94; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.92
Batch: 100; loss: 0.95; acc: 0.81
Batch: 120; loss: 1.21; acc: 0.67
Batch: 140; loss: 0.88; acc: 0.88
Val Epoch over. val_loss: 0.9568948787488755; val_accuracy: 0.8387738853503185 

The current subspace-distance is: 0.00010432817362016067 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.8
Batch: 20; loss: 0.95; acc: 0.84
Batch: 40; loss: 0.97; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.86
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 1.0; acc: 0.78
Batch: 140; loss: 0.94; acc: 0.83
Batch: 160; loss: 0.98; acc: 0.84
Batch: 180; loss: 1.05; acc: 0.8
Batch: 200; loss: 0.98; acc: 0.78
Batch: 220; loss: 0.94; acc: 0.88
Batch: 240; loss: 0.95; acc: 0.84
Batch: 260; loss: 0.96; acc: 0.83
Batch: 280; loss: 0.97; acc: 0.81
Batch: 300; loss: 0.97; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.8
Batch: 340; loss: 0.96; acc: 0.78
Batch: 360; loss: 0.96; acc: 0.78
Batch: 380; loss: 0.9; acc: 0.86
Batch: 400; loss: 1.03; acc: 0.78
Batch: 420; loss: 0.89; acc: 0.91
Batch: 440; loss: 1.0; acc: 0.77
Batch: 460; loss: 0.8; acc: 0.84
Batch: 480; loss: 0.94; acc: 0.88
Batch: 500; loss: 1.0; acc: 0.73
Batch: 520; loss: 0.97; acc: 0.77
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 0.81; acc: 0.89
Batch: 580; loss: 0.89; acc: 0.83
Batch: 600; loss: 0.84; acc: 0.89
Batch: 620; loss: 0.91; acc: 0.81
Batch: 640; loss: 0.92; acc: 0.83
Batch: 660; loss: 0.87; acc: 0.86
Batch: 680; loss: 0.94; acc: 0.81
Batch: 700; loss: 0.93; acc: 0.83
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 0.88; acc: 0.88
Batch: 760; loss: 0.91; acc: 0.88
Batch: 780; loss: 0.93; acc: 0.8
Train Epoch over. train_loss: 0.97; train_accuracy: 0.82 

0.00012431209324859083
0.00011911978072021157
Batch: 0; loss: 0.89; acc: 0.83
Batch: 20; loss: 1.15; acc: 0.72
Batch: 40; loss: 0.62; acc: 0.98
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.92
Batch: 100; loss: 0.85; acc: 0.86
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 0.81; acc: 0.86
Val Epoch over. val_loss: 0.870871854435866; val_accuracy: 0.8474323248407644 

The current subspace-distance is: 0.00011911978072021157 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.75
Batch: 20; loss: 0.87; acc: 0.83
Batch: 40; loss: 0.96; acc: 0.81
Batch: 60; loss: 0.87; acc: 0.78
Batch: 80; loss: 0.93; acc: 0.83
Batch: 100; loss: 0.92; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.86
Batch: 140; loss: 0.93; acc: 0.8
Batch: 160; loss: 1.08; acc: 0.73
Batch: 180; loss: 0.92; acc: 0.83
Batch: 200; loss: 0.87; acc: 0.84
Batch: 220; loss: 0.82; acc: 0.88
Batch: 240; loss: 0.88; acc: 0.8
Batch: 260; loss: 0.95; acc: 0.8
Batch: 280; loss: 0.91; acc: 0.8
Batch: 300; loss: 0.93; acc: 0.78
Batch: 320; loss: 0.82; acc: 0.83
Batch: 340; loss: 1.0; acc: 0.78
Batch: 360; loss: 0.89; acc: 0.81
Batch: 380; loss: 0.91; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.88
Batch: 420; loss: 0.86; acc: 0.86
Batch: 440; loss: 0.93; acc: 0.84
Batch: 460; loss: 0.88; acc: 0.84
Batch: 480; loss: 0.94; acc: 0.83
Batch: 500; loss: 0.97; acc: 0.72
Batch: 520; loss: 0.93; acc: 0.81
Batch: 540; loss: 0.97; acc: 0.78
Batch: 560; loss: 0.84; acc: 0.84
Batch: 580; loss: 0.88; acc: 0.81
Batch: 600; loss: 0.94; acc: 0.81
Batch: 620; loss: 0.88; acc: 0.88
Batch: 640; loss: 0.81; acc: 0.83
Batch: 660; loss: 0.96; acc: 0.8
Batch: 680; loss: 0.93; acc: 0.8
Batch: 700; loss: 0.85; acc: 0.84
Batch: 720; loss: 0.9; acc: 0.8
Batch: 740; loss: 0.86; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.89
Batch: 780; loss: 0.75; acc: 0.88
Train Epoch over. train_loss: 0.9; train_accuracy: 0.82 

0.00013784054317511618
0.00013352683163248003
Batch: 0; loss: 0.82; acc: 0.86
Batch: 20; loss: 1.05; acc: 0.73
Batch: 40; loss: 0.55; acc: 0.98
Batch: 60; loss: 0.77; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.92
Batch: 100; loss: 0.79; acc: 0.88
Batch: 120; loss: 1.06; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.86
Val Epoch over. val_loss: 0.8161154419753203; val_accuracy: 0.8513136942675159 

The current subspace-distance is: 0.00013352683163248003 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.75; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.86
Batch: 80; loss: 0.94; acc: 0.83
Batch: 100; loss: 0.88; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.83
Batch: 140; loss: 0.87; acc: 0.81
Batch: 160; loss: 0.94; acc: 0.78
Batch: 180; loss: 0.91; acc: 0.77
Batch: 200; loss: 1.01; acc: 0.73
Batch: 220; loss: 0.87; acc: 0.81
Batch: 240; loss: 0.96; acc: 0.77
Batch: 260; loss: 0.91; acc: 0.81
Batch: 280; loss: 0.86; acc: 0.84
Batch: 300; loss: 0.75; acc: 0.89
Batch: 320; loss: 0.82; acc: 0.86
Batch: 340; loss: 0.91; acc: 0.75
Batch: 360; loss: 0.92; acc: 0.83
Batch: 380; loss: 0.97; acc: 0.81
Batch: 400; loss: 0.76; acc: 0.89
Batch: 420; loss: 0.81; acc: 0.83
Batch: 440; loss: 0.85; acc: 0.78
Batch: 460; loss: 0.83; acc: 0.84
Batch: 480; loss: 0.81; acc: 0.88
Batch: 500; loss: 0.76; acc: 0.88
Batch: 520; loss: 0.86; acc: 0.86
Batch: 540; loss: 0.81; acc: 0.88
Batch: 560; loss: 0.86; acc: 0.81
Batch: 580; loss: 0.94; acc: 0.78
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.84; acc: 0.86
Batch: 640; loss: 0.9; acc: 0.78
Batch: 660; loss: 0.86; acc: 0.83
Batch: 680; loss: 0.87; acc: 0.84
Batch: 700; loss: 0.86; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.86
Batch: 740; loss: 0.83; acc: 0.83
Batch: 760; loss: 0.83; acc: 0.78
Batch: 780; loss: 0.94; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.83 

0.00014928067685104907
0.0001425320515409112
Batch: 0; loss: 0.79; acc: 0.89
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 0.52; acc: 0.98
Batch: 60; loss: 0.75; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.92
Batch: 100; loss: 0.76; acc: 0.88
Batch: 120; loss: 1.03; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.86
Val Epoch over. val_loss: 0.7875359016618911; val_accuracy: 0.8517117834394905 

The current subspace-distance is: 0.0001425320515409112 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.8
Batch: 20; loss: 0.77; acc: 0.86
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.85; acc: 0.83
Batch: 140; loss: 0.74; acc: 0.88
Batch: 160; loss: 0.86; acc: 0.78
Batch: 180; loss: 0.9; acc: 0.77
Batch: 200; loss: 0.85; acc: 0.77
Batch: 220; loss: 0.81; acc: 0.81
Batch: 240; loss: 0.85; acc: 0.83
Batch: 260; loss: 0.87; acc: 0.8
Batch: 280; loss: 0.66; acc: 0.91
Batch: 300; loss: 0.92; acc: 0.7
Batch: 320; loss: 0.85; acc: 0.86
Batch: 340; loss: 0.78; acc: 0.84
Batch: 360; loss: 0.79; acc: 0.86
Batch: 380; loss: 0.86; acc: 0.81
Batch: 400; loss: 0.81; acc: 0.84
Batch: 420; loss: 0.8; acc: 0.84
Batch: 440; loss: 0.8; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.86
Batch: 480; loss: 0.8; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.84
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.9; acc: 0.77
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.91; acc: 0.73
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 0.61; acc: 0.92
Batch: 660; loss: 0.75; acc: 0.86
Batch: 680; loss: 0.77; acc: 0.86
Batch: 700; loss: 0.92; acc: 0.83
Batch: 720; loss: 0.82; acc: 0.86
Batch: 740; loss: 0.73; acc: 0.91
Batch: 760; loss: 0.88; acc: 0.81
Batch: 780; loss: 0.82; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.83 

0.00016124399553518742
0.0001527857530163601
Batch: 0; loss: 0.75; acc: 0.91
Batch: 20; loss: 0.97; acc: 0.75
Batch: 40; loss: 0.47; acc: 0.98
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.92
Batch: 100; loss: 0.73; acc: 0.88
Batch: 120; loss: 1.0; acc: 0.8
Batch: 140; loss: 0.67; acc: 0.86
Val Epoch over. val_loss: 0.7444972061807182; val_accuracy: 0.8500199044585988 

The current subspace-distance is: 0.0001527857530163601 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 0.72; acc: 0.88
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 0.8; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.83
Batch: 140; loss: 0.9; acc: 0.8
Batch: 160; loss: 0.73; acc: 0.84
Batch: 180; loss: 0.73; acc: 0.89
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.81; acc: 0.84
Batch: 240; loss: 0.71; acc: 0.89
Batch: 260; loss: 0.83; acc: 0.8
Batch: 280; loss: 0.78; acc: 0.83
Batch: 300; loss: 0.89; acc: 0.75
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.69; acc: 0.88
Batch: 380; loss: 0.8; acc: 0.8
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.77; acc: 0.83
Batch: 440; loss: 0.92; acc: 0.81
Batch: 460; loss: 0.87; acc: 0.86
Batch: 480; loss: 0.83; acc: 0.84
Batch: 500; loss: 0.79; acc: 0.83
Batch: 520; loss: 0.86; acc: 0.86
Batch: 540; loss: 0.87; acc: 0.8
Batch: 560; loss: 0.82; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.89
Batch: 600; loss: 0.77; acc: 0.88
Batch: 620; loss: 0.84; acc: 0.86
Batch: 640; loss: 0.79; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.92
Batch: 680; loss: 0.79; acc: 0.78
Batch: 700; loss: 0.84; acc: 0.78
Batch: 720; loss: 0.75; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.8
Batch: 760; loss: 0.88; acc: 0.77
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.81; train_accuracy: 0.83 

0.0001676087995292619
0.00016056778258644044
Batch: 0; loss: 0.72; acc: 0.92
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.45; acc: 0.97
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.94
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.96; acc: 0.83
Batch: 140; loss: 0.65; acc: 0.88
Val Epoch over. val_loss: 0.7271621872665016; val_accuracy: 0.8527070063694268 

The current subspace-distance is: 0.00016056778258644044 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.69; acc: 0.86
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 0.93; acc: 0.75
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 0.94; acc: 0.73
Batch: 240; loss: 0.85; acc: 0.83
Batch: 260; loss: 0.77; acc: 0.88
Batch: 280; loss: 0.66; acc: 0.94
Batch: 300; loss: 0.77; acc: 0.84
Batch: 320; loss: 0.82; acc: 0.81
Batch: 340; loss: 0.62; acc: 0.92
Batch: 360; loss: 0.8; acc: 0.84
Batch: 380; loss: 0.76; acc: 0.83
Batch: 400; loss: 0.8; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.88
Batch: 440; loss: 0.78; acc: 0.86
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.76; acc: 0.89
Batch: 500; loss: 0.81; acc: 0.84
Batch: 520; loss: 0.8; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.86
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.9; acc: 0.78
Batch: 600; loss: 0.81; acc: 0.88
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.92; acc: 0.75
Batch: 660; loss: 0.85; acc: 0.8
Batch: 680; loss: 0.67; acc: 0.89
Batch: 700; loss: 0.73; acc: 0.83
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.78; acc: 0.86
Batch: 760; loss: 0.97; acc: 0.75
Batch: 780; loss: 0.91; acc: 0.83
Train Epoch over. train_loss: 0.78; train_accuracy: 0.83 

0.00017806161486078054
0.00017280834435950965
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.97
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.81
Batch: 140; loss: 0.64; acc: 0.88
Val Epoch over. val_loss: 0.7149911615878913; val_accuracy: 0.8528065286624203 

The current subspace-distance is: 0.00017280834435950965 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.91
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.91; acc: 0.81
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.86; acc: 0.77
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.94
Batch: 260; loss: 0.62; acc: 0.91
Batch: 280; loss: 0.71; acc: 0.92
Batch: 300; loss: 0.66; acc: 0.91
Batch: 320; loss: 0.73; acc: 0.83
Batch: 340; loss: 0.95; acc: 0.72
Batch: 360; loss: 0.69; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.92
Batch: 400; loss: 0.83; acc: 0.78
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.79; acc: 0.75
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.88
Batch: 500; loss: 0.75; acc: 0.86
Batch: 520; loss: 0.79; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.94; acc: 0.78
Batch: 580; loss: 0.7; acc: 0.89
Batch: 600; loss: 0.79; acc: 0.78
Batch: 620; loss: 0.8; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.94
Batch: 660; loss: 0.77; acc: 0.81
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.85; acc: 0.8
Batch: 720; loss: 0.81; acc: 0.83
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.83; acc: 0.7
Batch: 780; loss: 0.72; acc: 0.91
Train Epoch over. train_loss: 0.77; train_accuracy: 0.83 

0.00018710803124122322
0.00017941427358891815
Batch: 0; loss: 0.66; acc: 0.91
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.98
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.6926277874002031; val_accuracy: 0.8539012738853503 

The current subspace-distance is: 0.00017941427358891815 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.75; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.86
Batch: 80; loss: 0.81; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.98; acc: 0.73
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.87; acc: 0.81
Batch: 200; loss: 0.8; acc: 0.75
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.91; acc: 0.77
Batch: 280; loss: 0.77; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.86
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 0.7; acc: 0.89
Batch: 380; loss: 0.8; acc: 0.81
Batch: 400; loss: 0.82; acc: 0.84
Batch: 420; loss: 0.74; acc: 0.88
Batch: 440; loss: 0.77; acc: 0.83
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.7; acc: 0.89
Batch: 500; loss: 0.77; acc: 0.81
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 0.78; acc: 0.77
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.73; acc: 0.83
Batch: 600; loss: 0.83; acc: 0.84
Batch: 620; loss: 0.75; acc: 0.84
Batch: 640; loss: 0.76; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.86
Batch: 680; loss: 0.9; acc: 0.75
Batch: 700; loss: 0.82; acc: 0.84
Batch: 720; loss: 0.84; acc: 0.77
Batch: 740; loss: 0.78; acc: 0.8
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.76; train_accuracy: 0.83 

0.0001877999457065016
0.000180377290234901
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.98
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.92; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.89
Val Epoch over. val_loss: 0.6920695714889817; val_accuracy: 0.8539012738853503 

The current subspace-distance is: 0.000180377290234901 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.9; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.89
Batch: 60; loss: 0.83; acc: 0.8
Batch: 80; loss: 0.95; acc: 0.7
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.89
Batch: 220; loss: 0.77; acc: 0.84
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.7; acc: 0.86
Batch: 280; loss: 0.8; acc: 0.88
Batch: 300; loss: 0.82; acc: 0.81
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.67; acc: 0.89
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.76; acc: 0.86
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.87; acc: 0.78
Batch: 460; loss: 0.73; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.91; acc: 0.8
Batch: 540; loss: 0.87; acc: 0.72
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.88
Batch: 640; loss: 0.84; acc: 0.78
Batch: 660; loss: 0.8; acc: 0.83
Batch: 680; loss: 0.74; acc: 0.8
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.7; acc: 0.84
Batch: 740; loss: 0.71; acc: 0.88
Batch: 760; loss: 0.85; acc: 0.73
Batch: 780; loss: 0.67; acc: 0.91
Train Epoch over. train_loss: 0.75; train_accuracy: 0.83 

0.00018850370543077588
0.0001826743973651901
Batch: 0; loss: 0.66; acc: 0.91
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.98
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.94
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.6869600779691319; val_accuracy: 0.8561902866242038 

The current subspace-distance is: 0.0001826743973651901 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.9; acc: 0.75
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.85; acc: 0.83
Batch: 140; loss: 0.81; acc: 0.81
Batch: 160; loss: 0.71; acc: 0.86
Batch: 180; loss: 0.84; acc: 0.81
Batch: 200; loss: 0.73; acc: 0.84
Batch: 220; loss: 0.83; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.84
Batch: 300; loss: 0.7; acc: 0.89
Batch: 320; loss: 0.86; acc: 0.83
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.86
Batch: 380; loss: 0.75; acc: 0.88
Batch: 400; loss: 0.86; acc: 0.72
Batch: 420; loss: 0.81; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.63; acc: 0.91
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.91
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.83; acc: 0.81
Batch: 580; loss: 0.69; acc: 0.88
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.66; acc: 0.88
Batch: 640; loss: 0.74; acc: 0.86
Batch: 660; loss: 0.96; acc: 0.7
Batch: 680; loss: 0.7; acc: 0.88
Batch: 700; loss: 0.82; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.78; acc: 0.81
Batch: 760; loss: 0.82; acc: 0.84
Batch: 780; loss: 0.74; acc: 0.84
Train Epoch over. train_loss: 0.75; train_accuracy: 0.83 

0.00019310045172460377
0.00018649101548362523
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.98
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.95
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.6815002203746966; val_accuracy: 0.8541998407643312 

The current subspace-distance is: 0.00018649101548362523 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.78; acc: 0.78
Batch: 160; loss: 0.84; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.61; acc: 0.91
Batch: 280; loss: 0.8; acc: 0.81
Batch: 300; loss: 0.72; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.91
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.91; acc: 0.73
Batch: 380; loss: 0.82; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.89
Batch: 440; loss: 0.79; acc: 0.81
Batch: 460; loss: 0.64; acc: 0.92
Batch: 480; loss: 0.77; acc: 0.84
Batch: 500; loss: 1.01; acc: 0.69
Batch: 520; loss: 0.73; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.88; acc: 0.73
Batch: 580; loss: 0.62; acc: 0.91
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.93; acc: 0.78
Batch: 640; loss: 0.88; acc: 0.78
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.78; acc: 0.83
Batch: 720; loss: 0.93; acc: 0.78
Batch: 740; loss: 0.74; acc: 0.92
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.68; acc: 0.91
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.00019392784452065825
0.00018619561160448939
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.98
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.6803352401894369; val_accuracy: 0.8557921974522293 

The current subspace-distance is: 0.00018619561160448939 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.73; acc: 0.86
Batch: 180; loss: 0.81; acc: 0.78
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.82; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.84; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.89
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.73; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.7; acc: 0.86
Batch: 420; loss: 0.77; acc: 0.84
Batch: 440; loss: 0.82; acc: 0.78
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.84; acc: 0.75
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.87; acc: 0.77
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.91; acc: 0.77
Batch: 620; loss: 0.88; acc: 0.81
Batch: 640; loss: 0.76; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.83; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.75; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.7; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.00019897993479389697
0.00019259426335338503
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.91; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.98
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.89
Val Epoch over. val_loss: 0.6732119420531449; val_accuracy: 0.856687898089172 

The current subspace-distance is: 0.00019259426335338503 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.8; acc: 0.78
Batch: 160; loss: 0.75; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.63; acc: 0.89
Batch: 220; loss: 0.64; acc: 0.89
Batch: 240; loss: 0.72; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.86; acc: 0.8
Batch: 320; loss: 0.73; acc: 0.89
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.73; acc: 0.83
Batch: 380; loss: 0.65; acc: 0.89
Batch: 400; loss: 0.82; acc: 0.8
Batch: 420; loss: 0.76; acc: 0.81
Batch: 440; loss: 0.47; acc: 0.94
Batch: 460; loss: 0.78; acc: 0.81
Batch: 480; loss: 0.8; acc: 0.8
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.8; acc: 0.83
Batch: 540; loss: 0.86; acc: 0.75
Batch: 560; loss: 0.61; acc: 0.92
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.95; acc: 0.72
Batch: 620; loss: 0.88; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.83; acc: 0.77
Batch: 680; loss: 0.73; acc: 0.78
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.92; acc: 0.73
Batch: 740; loss: 0.79; acc: 0.81
Batch: 760; loss: 0.8; acc: 0.84
Batch: 780; loss: 0.83; acc: 0.72
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.00019619676459114999
0.00019159568182658404
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.98
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.89
Val Epoch over. val_loss: 0.6710501417612574; val_accuracy: 0.8539012738853503 

The current subspace-distance is: 0.00019159568182658404 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.74; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.92
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.97
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.89
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.92
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.92
Batch: 400; loss: 0.83; acc: 0.81
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.62; acc: 0.92
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.6; acc: 0.92
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.79; acc: 0.81
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.63; acc: 0.92
Batch: 620; loss: 0.77; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.94
Batch: 660; loss: 0.79; acc: 0.83
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.83; acc: 0.8
Batch: 720; loss: 0.73; acc: 0.83
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.0001999649975914508
0.00019280135165899992
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.98
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.89
Val Epoch over. val_loss: 0.6661493182182312; val_accuracy: 0.8556926751592356 

The current subspace-distance is: 0.00019280135165899992 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 0.75; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.65; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.88
Batch: 200; loss: 0.81; acc: 0.83
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.94
Batch: 260; loss: 0.73; acc: 0.84
Batch: 280; loss: 0.67; acc: 0.86
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.86
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.63; acc: 0.89
Batch: 400; loss: 0.71; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.83; acc: 0.78
Batch: 460; loss: 0.81; acc: 0.8
Batch: 480; loss: 0.73; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.94; acc: 0.78
Batch: 560; loss: 0.88; acc: 0.75
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.69; acc: 0.89
Batch: 620; loss: 0.76; acc: 0.8
Batch: 640; loss: 0.67; acc: 0.88
Batch: 660; loss: 0.86; acc: 0.73
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.64; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.76; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.00020263702026568353
0.00019632036855909973
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6540461524276976; val_accuracy: 0.8539012738853503 

The current subspace-distance is: 0.00019632036855909973 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.83
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.8; acc: 0.78
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.71; acc: 0.86
Batch: 160; loss: 0.76; acc: 0.83
Batch: 180; loss: 0.84; acc: 0.8
Batch: 200; loss: 0.75; acc: 0.86
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.8
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.62; acc: 0.92
Batch: 340; loss: 0.81; acc: 0.78
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.84
Batch: 460; loss: 0.71; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 0.76; acc: 0.84
Batch: 520; loss: 0.91; acc: 0.77
Batch: 540; loss: 0.63; acc: 0.89
Batch: 560; loss: 0.81; acc: 0.81
Batch: 580; loss: 0.69; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.88
Batch: 660; loss: 0.61; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.88
Batch: 720; loss: 0.71; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.88; acc: 0.78
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.0002050219918601215
0.00019716926908586174
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.89
Val Epoch over. val_loss: 0.6515991987696119; val_accuracy: 0.8550955414012739 

The current subspace-distance is: 0.00019716926908586174 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.86
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.91
Batch: 80; loss: 0.8; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.89
Batch: 160; loss: 0.6; acc: 0.92
Batch: 180; loss: 0.63; acc: 0.84
Batch: 200; loss: 0.77; acc: 0.83
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.69; acc: 0.84
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.79; acc: 0.78
Batch: 320; loss: 0.83; acc: 0.78
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.8; acc: 0.86
Batch: 440; loss: 0.81; acc: 0.75
Batch: 460; loss: 0.78; acc: 0.8
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.73; acc: 0.88
Batch: 520; loss: 0.64; acc: 0.86
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.81
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.8; acc: 0.83
Batch: 620; loss: 0.85; acc: 0.81
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 0.81; acc: 0.77
Batch: 740; loss: 0.79; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.00020603327720891684
0.00019899093604180962
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.92; acc: 0.77
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.95
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.91
Val Epoch over. val_loss: 0.6488182366273965; val_accuracy: 0.8575835987261147 

The current subspace-distance is: 0.00019899093604180962 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.92
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.81
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.89
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.88; acc: 0.78
Batch: 320; loss: 0.66; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.81; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.83
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.76; acc: 0.81
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.7; acc: 0.89
Batch: 500; loss: 0.63; acc: 0.91
Batch: 520; loss: 0.83; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.84
Batch: 580; loss: 0.63; acc: 0.89
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.73; acc: 0.8
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020775730081368238
0.00020174389646854252
Batch: 0; loss: 0.63; acc: 0.92
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6511835267968998; val_accuracy: 0.8556926751592356 

The current subspace-distance is: 0.00020174389646854252 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.77; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.8
Batch: 180; loss: 0.8; acc: 0.75
Batch: 200; loss: 0.71; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.77; acc: 0.78
Batch: 280; loss: 0.55; acc: 0.95
Batch: 300; loss: 0.58; acc: 0.92
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.88
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.67; acc: 0.89
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.77; acc: 0.83
Batch: 540; loss: 0.76; acc: 0.78
Batch: 560; loss: 0.69; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.83
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.76; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.73; acc: 0.83
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.58; acc: 0.94
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.68; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020961072004865855
0.00020195310935378075
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.89
Val Epoch over. val_loss: 0.6474068998151524; val_accuracy: 0.8568869426751592 

The current subspace-distance is: 0.00020195310935378075 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.88
Batch: 160; loss: 0.75; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.91
Batch: 220; loss: 0.63; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.89
Batch: 380; loss: 0.66; acc: 0.89
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 0.75; acc: 0.84
Batch: 440; loss: 0.79; acc: 0.8
Batch: 460; loss: 0.7; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.83
Batch: 500; loss: 0.74; acc: 0.83
Batch: 520; loss: 0.8; acc: 0.75
Batch: 540; loss: 0.77; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.87; acc: 0.77
Batch: 600; loss: 0.85; acc: 0.77
Batch: 620; loss: 0.68; acc: 0.81
Batch: 640; loss: 0.83; acc: 0.84
Batch: 660; loss: 0.75; acc: 0.81
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.6; acc: 0.92
Batch: 760; loss: 0.72; acc: 0.84
Batch: 780; loss: 0.73; acc: 0.8
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020781443163286895
0.00020215108816046268
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6444357067916044; val_accuracy: 0.8571855095541401 

The current subspace-distance is: 0.00020215108816046268 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.85; acc: 0.77
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.89
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.94
Batch: 500; loss: 0.84; acc: 0.84
Batch: 520; loss: 0.79; acc: 0.84
Batch: 540; loss: 0.78; acc: 0.8
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 0.62; acc: 0.88
Batch: 640; loss: 0.73; acc: 0.77
Batch: 660; loss: 0.83; acc: 0.73
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.82; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.92
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.65; acc: 0.91
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020957663946319371
0.00020228840003255755
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6448910255341014; val_accuracy: 0.8562898089171974 

The current subspace-distance is: 0.00020228840003255755 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.76; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.82; acc: 0.77
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.91
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.78; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 0.77; acc: 0.83
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.88
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.78; acc: 0.81
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.8; acc: 0.72
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.8; acc: 0.83
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.84; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.83
Batch: 740; loss: 0.9; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020748966198880225
0.00020118674729019403
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6436341478946103; val_accuracy: 0.8553941082802548 

The current subspace-distance is: 0.00020118674729019403 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.94
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.86
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.89
Batch: 160; loss: 0.83; acc: 0.77
Batch: 180; loss: 0.64; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.8
Batch: 240; loss: 0.6; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.91
Batch: 280; loss: 0.86; acc: 0.75
Batch: 300; loss: 0.83; acc: 0.75
Batch: 320; loss: 0.6; acc: 0.91
Batch: 340; loss: 0.63; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.73
Batch: 380; loss: 0.58; acc: 0.86
Batch: 400; loss: 0.66; acc: 0.89
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.8
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.86; acc: 0.78
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.77; acc: 0.81
Batch: 620; loss: 0.75; acc: 0.77
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.79; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00021105013729538769
0.00020457689242903143
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.81
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6420681613266088; val_accuracy: 0.8567874203821656 

The current subspace-distance is: 0.00020457689242903143 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.93; acc: 0.77
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.59; acc: 0.91
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.72; acc: 0.89
Batch: 220; loss: 0.82; acc: 0.73
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.92
Batch: 300; loss: 0.62; acc: 0.91
Batch: 320; loss: 0.62; acc: 0.91
Batch: 340; loss: 0.77; acc: 0.8
Batch: 360; loss: 0.78; acc: 0.83
Batch: 380; loss: 0.74; acc: 0.83
Batch: 400; loss: 0.88; acc: 0.73
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.7; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.88
Batch: 560; loss: 0.8; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.72; acc: 0.86
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.83
Batch: 680; loss: 0.76; acc: 0.84
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.82; acc: 0.78
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00021327320428099483
0.00020632649830076844
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6374840688933233; val_accuracy: 0.8581807324840764 

The current subspace-distance is: 0.00020632649830076844 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.66; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.66; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.72; acc: 0.86
Batch: 240; loss: 0.84; acc: 0.72
Batch: 260; loss: 0.77; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.74; acc: 0.77
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.73; acc: 0.83
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.73; acc: 0.88
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.79; acc: 0.78
Batch: 460; loss: 0.73; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.75; acc: 0.86
Batch: 600; loss: 0.82; acc: 0.77
Batch: 620; loss: 0.65; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.86
Batch: 660; loss: 0.77; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.76; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.82; acc: 0.77
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00020850503642577678
0.0002021877298830077
Batch: 0; loss: 0.61; acc: 0.92
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.95
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6429234748812998; val_accuracy: 0.8570859872611465 

The current subspace-distance is: 0.0002021877298830077 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.88
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.81; acc: 0.77
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.57; acc: 0.91
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.85; acc: 0.73
Batch: 260; loss: 0.82; acc: 0.78
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.89; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.77; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.75; acc: 0.83
Batch: 480; loss: 0.93; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.89
Batch: 520; loss: 0.71; acc: 0.84
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.81; acc: 0.83
Batch: 620; loss: 0.6; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.84; acc: 0.7
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.7; acc: 0.86
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.76; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00021183757053222507
0.00020535747171379626
Batch: 0; loss: 0.6; acc: 0.92
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.35; acc: 0.97
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6378070518469355; val_accuracy: 0.8561902866242038 

The current subspace-distance is: 0.00020535747171379626 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.79; acc: 0.77
Batch: 220; loss: 0.76; acc: 0.77
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.87; acc: 0.75
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.56; acc: 0.91
Batch: 360; loss: 0.82; acc: 0.75
Batch: 380; loss: 0.7; acc: 0.84
Batch: 400; loss: 0.87; acc: 0.77
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.67; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.65; acc: 0.84
Batch: 500; loss: 0.83; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.84; acc: 0.7
Batch: 640; loss: 0.56; acc: 0.89
Batch: 660; loss: 0.73; acc: 0.81
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.65; acc: 0.86
Batch: 740; loss: 0.78; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002132648223778233
0.00020582017896231264
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6279413056601385; val_accuracy: 0.8584792993630573 

The current subspace-distance is: 0.00020582017896231264 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_7_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.75

The number of parameters is: 271389

The number of individual parameters is:

14
252
14
14
21
38220
21
21
42
114660
42
42
64
112896
64
64
4096
64
640
10
64
64

nonzero elements in E: 81416694
elements in E: 81416700
fraction nonzero: 0.9999999263050455
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.39; acc: 0.08
Batch: 20; loss: 2.1; acc: 0.33
Batch: 40; loss: 1.95; acc: 0.42
Batch: 60; loss: 1.88; acc: 0.44
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.53; acc: 0.69
Batch: 120; loss: 1.59; acc: 0.62
Batch: 140; loss: 1.57; acc: 0.66
Batch: 160; loss: 1.45; acc: 0.72
Batch: 180; loss: 1.61; acc: 0.64
Batch: 200; loss: 1.48; acc: 0.64
Batch: 220; loss: 1.34; acc: 0.73
Batch: 240; loss: 1.42; acc: 0.7
Batch: 260; loss: 1.29; acc: 0.78
Batch: 280; loss: 1.37; acc: 0.73
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.24; acc: 0.78
Batch: 340; loss: 1.4; acc: 0.64
Batch: 360; loss: 1.33; acc: 0.7
Batch: 380; loss: 1.33; acc: 0.75
Batch: 400; loss: 1.19; acc: 0.88
Batch: 420; loss: 1.36; acc: 0.67
Batch: 440; loss: 1.37; acc: 0.7
Batch: 460; loss: 1.23; acc: 0.75
Batch: 480; loss: 1.23; acc: 0.75
Batch: 500; loss: 1.14; acc: 0.81
Batch: 520; loss: 1.13; acc: 0.81
Batch: 540; loss: 1.18; acc: 0.83
Batch: 560; loss: 1.16; acc: 0.83
Batch: 580; loss: 1.21; acc: 0.83
Batch: 600; loss: 1.08; acc: 0.86
Batch: 620; loss: 1.13; acc: 0.81
Batch: 640; loss: 1.23; acc: 0.69
Batch: 660; loss: 1.25; acc: 0.77
Batch: 680; loss: 1.07; acc: 0.86
Batch: 700; loss: 1.06; acc: 0.84
Batch: 720; loss: 1.14; acc: 0.8
Batch: 740; loss: 1.22; acc: 0.77
Batch: 760; loss: 1.1; acc: 0.86
Batch: 780; loss: 1.2; acc: 0.75
Train Epoch over. train_loss: 1.37; train_accuracy: 0.71 

6.250041769817472e-05
5.765141759184189e-05
Batch: 0; loss: 1.22; acc: 0.81
Batch: 20; loss: 1.2; acc: 0.73
Batch: 40; loss: 0.82; acc: 0.92
Batch: 60; loss: 1.03; acc: 0.83
Batch: 80; loss: 0.92; acc: 0.92
Batch: 100; loss: 1.08; acc: 0.83
Batch: 120; loss: 1.26; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.91
Val Epoch over. val_loss: 1.059223250219017; val_accuracy: 0.8309116242038217 

The current subspace-distance is: 5.765141759184189e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.84
Batch: 20; loss: 1.19; acc: 0.75
Batch: 40; loss: 1.12; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.77
Batch: 80; loss: 1.18; acc: 0.78
Batch: 100; loss: 1.09; acc: 0.8
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 1.0; acc: 0.88
Batch: 160; loss: 1.02; acc: 0.83
Batch: 180; loss: 1.11; acc: 0.8
Batch: 200; loss: 0.98; acc: 0.83
Batch: 220; loss: 1.06; acc: 0.81
Batch: 240; loss: 0.98; acc: 0.88
Batch: 260; loss: 0.96; acc: 0.88
Batch: 280; loss: 1.05; acc: 0.83
Batch: 300; loss: 1.05; acc: 0.78
Batch: 320; loss: 0.96; acc: 0.84
Batch: 340; loss: 1.06; acc: 0.77
Batch: 360; loss: 0.97; acc: 0.86
Batch: 380; loss: 0.94; acc: 0.86
Batch: 400; loss: 0.89; acc: 0.89
Batch: 420; loss: 1.03; acc: 0.78
Batch: 440; loss: 1.04; acc: 0.81
Batch: 460; loss: 0.98; acc: 0.84
Batch: 480; loss: 0.93; acc: 0.89
Batch: 500; loss: 0.99; acc: 0.89
Batch: 520; loss: 0.82; acc: 0.89
Batch: 540; loss: 0.98; acc: 0.86
Batch: 560; loss: 0.93; acc: 0.91
Batch: 580; loss: 0.97; acc: 0.78
Batch: 600; loss: 0.9; acc: 0.88
Batch: 620; loss: 0.95; acc: 0.84
Batch: 640; loss: 1.03; acc: 0.77
Batch: 660; loss: 0.89; acc: 0.83
Batch: 680; loss: 0.94; acc: 0.89
Batch: 700; loss: 0.93; acc: 0.86
Batch: 720; loss: 0.91; acc: 0.84
Batch: 740; loss: 0.92; acc: 0.91
Batch: 760; loss: 1.04; acc: 0.8
Batch: 780; loss: 0.95; acc: 0.78
Train Epoch over. train_loss: 1.01; train_accuracy: 0.82 

8.569544297643006e-05
8.066519512794912e-05
Batch: 0; loss: 0.95; acc: 0.84
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.94
Batch: 60; loss: 0.83; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.94
Batch: 100; loss: 0.96; acc: 0.84
Batch: 120; loss: 1.15; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.95
Val Epoch over. val_loss: 0.872803783720466; val_accuracy: 0.859375 

The current subspace-distance is: 8.066519512794912e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.8
Batch: 20; loss: 0.91; acc: 0.86
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.89
Batch: 80; loss: 0.91; acc: 0.86
Batch: 100; loss: 0.86; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.74; acc: 0.92
Batch: 160; loss: 1.02; acc: 0.77
Batch: 180; loss: 0.89; acc: 0.81
Batch: 200; loss: 0.91; acc: 0.81
Batch: 220; loss: 0.87; acc: 0.83
Batch: 240; loss: 0.86; acc: 0.89
Batch: 260; loss: 0.89; acc: 0.81
Batch: 280; loss: 0.88; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.94
Batch: 320; loss: 0.72; acc: 0.94
Batch: 340; loss: 0.96; acc: 0.84
Batch: 360; loss: 0.85; acc: 0.84
Batch: 380; loss: 0.71; acc: 0.88
Batch: 400; loss: 0.83; acc: 0.84
Batch: 420; loss: 1.0; acc: 0.72
Batch: 440; loss: 0.9; acc: 0.84
Batch: 460; loss: 0.86; acc: 0.78
Batch: 480; loss: 0.84; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.92
Batch: 520; loss: 0.92; acc: 0.81
Batch: 540; loss: 0.94; acc: 0.81
Batch: 560; loss: 0.8; acc: 0.88
Batch: 580; loss: 0.9; acc: 0.78
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.85; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.88
Batch: 660; loss: 0.91; acc: 0.83
Batch: 680; loss: 0.75; acc: 0.88
Batch: 700; loss: 0.88; acc: 0.81
Batch: 720; loss: 0.74; acc: 0.91
Batch: 740; loss: 0.84; acc: 0.83
Batch: 760; loss: 0.79; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.88
Train Epoch over. train_loss: 0.86; train_accuracy: 0.84 

0.00010303701128577814
9.800119005376473e-05
Batch: 0; loss: 0.78; acc: 0.92
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.95
Batch: 100; loss: 0.87; acc: 0.84
Batch: 120; loss: 1.04; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.95
Val Epoch over. val_loss: 0.7587832842662836; val_accuracy: 0.8695262738853503 

The current subspace-distance is: 9.800119005376473e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.92
Batch: 60; loss: 0.76; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.91
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.73; acc: 0.88
Batch: 160; loss: 0.78; acc: 0.86
Batch: 180; loss: 0.67; acc: 0.91
Batch: 200; loss: 0.68; acc: 0.92
Batch: 220; loss: 0.89; acc: 0.83
Batch: 240; loss: 0.93; acc: 0.78
Batch: 260; loss: 0.82; acc: 0.91
Batch: 280; loss: 0.74; acc: 0.91
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.82; acc: 0.84
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.83; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.88
Batch: 420; loss: 0.72; acc: 0.91
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.87; acc: 0.73
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 0.79; acc: 0.91
Batch: 560; loss: 0.75; acc: 0.83
Batch: 580; loss: 0.78; acc: 0.86
Batch: 600; loss: 0.72; acc: 0.91
Batch: 620; loss: 0.84; acc: 0.8
Batch: 640; loss: 0.93; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.92
Batch: 680; loss: 0.7; acc: 0.89
Batch: 700; loss: 0.74; acc: 0.86
Batch: 720; loss: 0.72; acc: 0.86
Batch: 740; loss: 0.7; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.76; acc: 0.88
Train Epoch over. train_loss: 0.77; train_accuracy: 0.85 

0.00011890205496456474
0.00011426305718487129
Batch: 0; loss: 0.7; acc: 0.92
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.48; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.94
Batch: 100; loss: 0.79; acc: 0.89
Batch: 120; loss: 0.95; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.95
Val Epoch over. val_loss: 0.6905616398449916; val_accuracy: 0.8765923566878981 

The current subspace-distance is: 0.00011426305718487129 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.74; acc: 0.84
Batch: 160; loss: 0.62; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.89
Batch: 200; loss: 0.71; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.88
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.88
Batch: 340; loss: 0.73; acc: 0.92
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.78; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.92
Batch: 420; loss: 0.68; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.89
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.73; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.91
Batch: 520; loss: 0.72; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.64; acc: 0.92
Batch: 620; loss: 0.69; acc: 0.88
Batch: 640; loss: 0.78; acc: 0.78
Batch: 660; loss: 0.63; acc: 0.89
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.79; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.92
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.86 

0.0001320227311225608
0.00012637340114451945
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.71; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.6243607734039331; val_accuracy: 0.884952229299363 

The current subspace-distance is: 0.00012637340114451945 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.94
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.92
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.91
Batch: 140; loss: 0.74; acc: 0.81
Batch: 160; loss: 0.73; acc: 0.84
Batch: 180; loss: 0.65; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.92
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.69; acc: 0.83
Batch: 260; loss: 0.74; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.76; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.67; acc: 0.86
Batch: 380; loss: 0.65; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.91
Batch: 460; loss: 0.69; acc: 0.86
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.77; acc: 0.83
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.79; acc: 0.86
Batch: 620; loss: 0.8; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.67; acc: 0.88
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.92
Batch: 780; loss: 0.65; acc: 0.92
Train Epoch over. train_loss: 0.66; train_accuracy: 0.87 

0.00014590125647373497
0.00014096283121034503
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.65; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.97
Val Epoch over. val_loss: 0.5770444256864535; val_accuracy: 0.8894307324840764 

The current subspace-distance is: 0.00014096283121034503 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.58; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.79; acc: 0.73
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.78; acc: 0.75
Batch: 320; loss: 0.5; acc: 0.94
Batch: 340; loss: 0.61; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.92
Batch: 400; loss: 0.74; acc: 0.84
Batch: 420; loss: 0.59; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.92
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.64; acc: 0.89
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.94
Batch: 660; loss: 0.69; acc: 0.8
Batch: 680; loss: 0.54; acc: 0.95
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.62; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.61; train_accuracy: 0.87 

0.00015917788550723344
0.00015324109699577093
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.98
Val Epoch over. val_loss: 0.5240691526300588; val_accuracy: 0.9010748407643312 

The current subspace-distance is: 0.00015324109699577093 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.64; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.92
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.92
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.91
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.94
Batch: 480; loss: 0.65; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.56; acc: 0.88
Batch: 640; loss: 0.68; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.63; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

0.00017046710127033293
0.00016316355322487652
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.98
Val Epoch over. val_loss: 0.49072105518192244; val_accuracy: 0.9060509554140127 

The current subspace-distance is: 0.00016316355322487652 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.94
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.95
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.67; acc: 0.84
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.92
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.95
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

0.00018074334366247058
0.0001753969263518229
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.98
Val Epoch over. val_loss: 0.45606453128301416; val_accuracy: 0.9094347133757962 

The current subspace-distance is: 0.0001753969263518229 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.95
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.00019063545914832503
0.00018358956731390208
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.43035013793380394; val_accuracy: 0.9110270700636943 

The current subspace-distance is: 0.00018358956731390208 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.48; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.95
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.0001950640871655196
0.00018780888058245182
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.4277831181219429; val_accuracy: 0.9093351910828026 

The current subspace-distance is: 0.00018780888058245182 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.94
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.47; acc: 0.94
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.68; acc: 0.77
Batch: 600; loss: 0.48; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.91
Batch: 780; loss: 0.6; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00019877620798069984
0.00019205450371373445
Batch: 0; loss: 0.38; acc: 0.98
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.42351112339147334; val_accuracy: 0.9111265923566879 

The current subspace-distance is: 0.00019205450371373445 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.97
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.75; acc: 0.77
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00020011048763990402
0.00019206323486287147
Batch: 0; loss: 0.36; acc: 0.98
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.42047345078295206; val_accuracy: 0.9097332802547771 

The current subspace-distance is: 0.00019206323486287147 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.98
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.55; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.97
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.94
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.42; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.94
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00020181597210466862
0.00019604632689151913
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.40550676832912835; val_accuracy: 0.9115246815286624 

The current subspace-distance is: 0.00019604632689151913 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.98
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.94
Batch: 700; loss: 0.56; acc: 0.78
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

0.0002057617239188403
0.00019719896954484284
Batch: 0; loss: 0.35; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.4066144802198289; val_accuracy: 0.912718949044586 

The current subspace-distance is: 0.00019719896954484284 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.95
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.97
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.63; acc: 0.8
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

0.00020744909124914557
0.0001983710244530812
Batch: 0; loss: 0.35; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.39899038338357473; val_accuracy: 0.9141122611464968 

The current subspace-distance is: 0.0001983710244530812 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.97
Batch: 700; loss: 0.52; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

0.00020941696129739285
0.00020310086256358773
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.399194722342643; val_accuracy: 0.9126194267515924 

The current subspace-distance is: 0.00020310086256358773 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.95
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.61; acc: 0.78
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

0.00021098271827213466
0.00020176765974611044
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.391703353755793; val_accuracy: 0.912718949044586 

The current subspace-distance is: 0.00020176765974611044 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.95
Batch: 180; loss: 0.41; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.97
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

0.00021292141173034906
0.00020421527733560652
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.3866335413638194; val_accuracy: 0.9148089171974523 

The current subspace-distance is: 0.00020421527733560652 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.97
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.95
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

0.0002171613450627774
0.00020944625430274755
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3839167949690181; val_accuracy: 0.9133160828025477 

The current subspace-distance is: 0.00020944625430274755 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.58; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.68; acc: 0.78
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00021488616766873747
0.00020786917593795806
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3875844306816721; val_accuracy: 0.913515127388535 

The current subspace-distance is: 0.00020786917593795806 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.51; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00021357840159907937
0.00020603906887117773
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.38355188574760585; val_accuracy: 0.9138136942675159 

The current subspace-distance is: 0.00020603906887117773 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.95
Batch: 300; loss: 0.65; acc: 0.77
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.0002169668732676655
0.00020829611457884312
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.3878699281033437; val_accuracy: 0.9126194267515924 

The current subspace-distance is: 0.00020829611457884312 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.97
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00021496176486834884
0.00020856426272075623
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.376516468206029; val_accuracy: 0.915406050955414 

The current subspace-distance is: 0.00020856426272075623 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.97
Batch: 300; loss: 0.53; acc: 0.78
Batch: 320; loss: 0.37; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.0002163785684388131
0.00020739343017339706
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.37814657342661717; val_accuracy: 0.9128184713375797 

The current subspace-distance is: 0.00020739343017339706 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.41; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.8
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.97
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.98
Batch: 740; loss: 0.35; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00021882769942749292
0.00020991430210415274
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3841021117890716; val_accuracy: 0.9137141719745223 

The current subspace-distance is: 0.00020991430210415274 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.97
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

0.00021910820214543492
0.00021145165374036878
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.38076012368035167; val_accuracy: 0.9140127388535032 

The current subspace-distance is: 0.00021145165374036878 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.67; acc: 0.77
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 0.65; acc: 0.77
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.81
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.63; acc: 0.84
Batch: 780; loss: 0.69; acc: 0.78
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

0.00022143502428662032
0.00021394394570961595
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.37696435107926657; val_accuracy: 0.914609872611465 

The current subspace-distance is: 0.00021394394570961595 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.97
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

0.00022061896743252873
0.00021288474090397358
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3722398932192736; val_accuracy: 0.9153065286624203 

The current subspace-distance is: 0.00021288474090397358 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.31; acc: 0.97
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.29; acc: 0.97
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.98
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

0.00022170526790432632
0.00021482899319380522
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.3783730085298514; val_accuracy: 0.9147093949044586 

The current subspace-distance is: 0.00021482899319380522 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_7_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.75

The number of parameters is: 271389

The number of individual parameters is:

14
252
14
14
21
38220
21
21
42
114660
42
42
64
112896
64
64
4096
64
640
10
64
64

nonzero elements in E: 108555590
elements in E: 108555600
fraction nonzero: 0.9999999078813069
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.42; acc: 0.11
Batch: 20; loss: 2.18; acc: 0.19
Batch: 40; loss: 1.86; acc: 0.45
Batch: 60; loss: 1.75; acc: 0.42
Batch: 80; loss: 1.6; acc: 0.66
Batch: 100; loss: 1.57; acc: 0.56
Batch: 120; loss: 1.53; acc: 0.64
Batch: 140; loss: 1.36; acc: 0.73
Batch: 160; loss: 1.49; acc: 0.64
Batch: 180; loss: 1.39; acc: 0.64
Batch: 200; loss: 1.25; acc: 0.81
Batch: 220; loss: 1.43; acc: 0.62
Batch: 240; loss: 1.32; acc: 0.81
Batch: 260; loss: 1.3; acc: 0.69
Batch: 280; loss: 1.27; acc: 0.72
Batch: 300; loss: 1.24; acc: 0.75
Batch: 320; loss: 1.32; acc: 0.69
Batch: 340; loss: 1.27; acc: 0.78
Batch: 360; loss: 1.21; acc: 0.81
Batch: 380; loss: 1.13; acc: 0.81
Batch: 400; loss: 1.18; acc: 0.8
Batch: 420; loss: 1.05; acc: 0.86
Batch: 440; loss: 1.12; acc: 0.77
Batch: 460; loss: 1.18; acc: 0.75
Batch: 480; loss: 1.17; acc: 0.84
Batch: 500; loss: 1.04; acc: 0.86
Batch: 520; loss: 1.06; acc: 0.84
Batch: 540; loss: 0.98; acc: 0.84
Batch: 560; loss: 1.01; acc: 0.84
Batch: 580; loss: 1.0; acc: 0.88
Batch: 600; loss: 0.98; acc: 0.88
Batch: 620; loss: 1.18; acc: 0.75
Batch: 640; loss: 1.03; acc: 0.84
Batch: 660; loss: 1.19; acc: 0.73
Batch: 680; loss: 1.12; acc: 0.8
Batch: 700; loss: 1.11; acc: 0.75
Batch: 720; loss: 1.12; acc: 0.84
Batch: 740; loss: 0.99; acc: 0.8
Batch: 760; loss: 0.9; acc: 0.88
Batch: 780; loss: 1.17; acc: 0.75
Train Epoch over. train_loss: 1.28; train_accuracy: 0.73 

2.5445504434173927e-05
7.961636583786458e-06
Batch: 0; loss: 1.09; acc: 0.88
Batch: 20; loss: 1.19; acc: 0.77
Batch: 40; loss: 0.76; acc: 0.92
Batch: 60; loss: 1.03; acc: 0.86
Batch: 80; loss: 0.78; acc: 0.95
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.15; acc: 0.77
Batch: 140; loss: 0.88; acc: 0.95
Val Epoch over. val_loss: 0.9851511842126299; val_accuracy: 0.8386743630573248 

The current subspace-distance is: 7.961636583786458e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.84
Batch: 20; loss: 0.98; acc: 0.84
Batch: 40; loss: 0.99; acc: 0.84
Batch: 60; loss: 1.01; acc: 0.88
Batch: 80; loss: 0.98; acc: 0.84
Batch: 100; loss: 1.02; acc: 0.77
Batch: 120; loss: 0.92; acc: 0.91
Batch: 140; loss: 1.07; acc: 0.78
Batch: 160; loss: 0.95; acc: 0.83
Batch: 180; loss: 0.89; acc: 0.89
Batch: 200; loss: 0.99; acc: 0.83
Batch: 220; loss: 0.95; acc: 0.81
Batch: 240; loss: 1.03; acc: 0.78
Batch: 260; loss: 0.91; acc: 0.88
Batch: 280; loss: 1.05; acc: 0.8
Batch: 300; loss: 1.01; acc: 0.81
Batch: 320; loss: 0.88; acc: 0.8
Batch: 340; loss: 0.87; acc: 0.86
Batch: 360; loss: 1.0; acc: 0.77
Batch: 380; loss: 0.9; acc: 0.86
Batch: 400; loss: 0.83; acc: 0.89
Batch: 420; loss: 0.83; acc: 0.86
Batch: 440; loss: 1.03; acc: 0.81
Batch: 460; loss: 0.91; acc: 0.81
Batch: 480; loss: 0.99; acc: 0.78
Batch: 500; loss: 0.83; acc: 0.94
Batch: 520; loss: 0.85; acc: 0.88
Batch: 540; loss: 0.95; acc: 0.83
Batch: 560; loss: 0.83; acc: 0.86
Batch: 580; loss: 0.87; acc: 0.83
Batch: 600; loss: 0.91; acc: 0.86
Batch: 620; loss: 0.9; acc: 0.89
Batch: 640; loss: 0.89; acc: 0.91
Batch: 660; loss: 0.88; acc: 0.86
Batch: 680; loss: 0.98; acc: 0.8
Batch: 700; loss: 0.88; acc: 0.83
Batch: 720; loss: 0.66; acc: 0.98
Batch: 740; loss: 0.72; acc: 0.92
Batch: 760; loss: 0.81; acc: 0.88
Batch: 780; loss: 0.9; acc: 0.81
Train Epoch over. train_loss: 0.94; train_accuracy: 0.84 

3.0769304430577904e-05
1.1524560250109062e-05
Batch: 0; loss: 0.89; acc: 0.91
Batch: 20; loss: 0.98; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.97
Batch: 60; loss: 0.85; acc: 0.91
Batch: 80; loss: 0.62; acc: 0.94
Batch: 100; loss: 0.82; acc: 0.89
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.64; acc: 0.95
Val Epoch over. val_loss: 0.7998163062296096; val_accuracy: 0.8749004777070064 

The current subspace-distance is: 1.1524560250109062e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.86
Batch: 40; loss: 0.92; acc: 0.81
Batch: 60; loss: 0.91; acc: 0.83
Batch: 80; loss: 0.83; acc: 0.86
Batch: 100; loss: 0.91; acc: 0.81
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 0.78; acc: 0.89
Batch: 160; loss: 0.91; acc: 0.8
Batch: 180; loss: 0.97; acc: 0.83
Batch: 200; loss: 0.75; acc: 0.92
Batch: 220; loss: 1.0; acc: 0.78
Batch: 240; loss: 0.85; acc: 0.83
Batch: 260; loss: 0.79; acc: 0.89
Batch: 280; loss: 0.76; acc: 0.89
Batch: 300; loss: 0.8; acc: 0.92
Batch: 320; loss: 0.82; acc: 0.83
Batch: 340; loss: 0.75; acc: 0.86
Batch: 360; loss: 0.77; acc: 0.91
Batch: 380; loss: 0.76; acc: 0.86
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 0.97; acc: 0.78
Batch: 440; loss: 0.86; acc: 0.83
Batch: 460; loss: 0.78; acc: 0.86
Batch: 480; loss: 0.81; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.73; acc: 0.88
Batch: 560; loss: 0.8; acc: 0.88
Batch: 580; loss: 0.83; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.94
Batch: 620; loss: 0.74; acc: 0.95
Batch: 640; loss: 0.79; acc: 0.86
Batch: 660; loss: 0.75; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.88
Batch: 700; loss: 0.73; acc: 0.89
Batch: 720; loss: 0.72; acc: 0.91
Batch: 740; loss: 0.78; acc: 0.89
Batch: 760; loss: 0.65; acc: 0.92
Batch: 780; loss: 0.74; acc: 0.86
Train Epoch over. train_loss: 0.79; train_accuracy: 0.86 

3.41109189321287e-05
1.2969851923116948e-05
Batch: 0; loss: 0.69; acc: 0.91
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.97
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.97
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.95
Val Epoch over. val_loss: 0.6749696847359845; val_accuracy: 0.8911226114649682 

The current subspace-distance is: 1.2969851923116948e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.89
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.76; acc: 0.84
Batch: 160; loss: 0.75; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.94
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.89
Batch: 240; loss: 0.71; acc: 0.88
Batch: 260; loss: 0.8; acc: 0.81
Batch: 280; loss: 0.79; acc: 0.81
Batch: 300; loss: 0.65; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.92
Batch: 340; loss: 0.64; acc: 0.92
Batch: 360; loss: 0.74; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.91
Batch: 400; loss: 0.68; acc: 0.89
Batch: 420; loss: 0.76; acc: 0.77
Batch: 440; loss: 0.72; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.92
Batch: 480; loss: 0.74; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.95
Batch: 560; loss: 0.65; acc: 0.92
Batch: 580; loss: 0.72; acc: 0.89
Batch: 600; loss: 0.59; acc: 0.91
Batch: 620; loss: 0.77; acc: 0.81
Batch: 640; loss: 0.69; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.94
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.88
Batch: 720; loss: 0.87; acc: 0.78
Batch: 740; loss: 0.66; acc: 0.89
Batch: 760; loss: 0.68; acc: 0.91
Batch: 780; loss: 0.68; acc: 0.92
Train Epoch over. train_loss: 0.69; train_accuracy: 0.87 

3.8451060390798375e-05
1.6546935512451455e-05
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.98
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.598156134034418; val_accuracy: 0.8960987261146497 

The current subspace-distance is: 1.6546935512451455e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.6; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.94
Batch: 80; loss: 0.72; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.92
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.78; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.92
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.69; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.92
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.72; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.95
Batch: 640; loss: 0.69; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.59; acc: 0.91
Batch: 700; loss: 0.67; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.91
Batch: 760; loss: 0.58; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.62; train_accuracy: 0.88 

4.174029527348466e-05
1.7753438442014158e-05
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.97
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5423495619540002; val_accuracy: 0.8994824840764332 

The current subspace-distance is: 1.7753438442014158e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.78; acc: 0.78
Batch: 280; loss: 0.52; acc: 0.95
Batch: 300; loss: 0.56; acc: 0.94
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.77; acc: 0.83
Batch: 540; loss: 0.52; acc: 0.94
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

4.526727934717201e-05
1.895194873213768e-05
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.49241080690341393; val_accuracy: 0.90515525477707 

The current subspace-distance is: 1.895194873213768e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.92
Batch: 140; loss: 0.78; acc: 0.75
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.92
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.92
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.6; acc: 0.92
Batch: 340; loss: 0.75; acc: 0.75
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.95
Batch: 480; loss: 0.54; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.95
Batch: 520; loss: 0.48; acc: 0.94
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.58; acc: 0.77
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

4.806352444575168e-05
2.0528052118606865e-05
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4460432824625331; val_accuracy: 0.9112261146496815 

The current subspace-distance is: 2.0528052118606865e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

5.080852497485466e-05
2.3121749109122902e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4142376679903383; val_accuracy: 0.9151074840764332 

The current subspace-distance is: 2.3121749109122902e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.281555058900267e-05
2.3883838366600685e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.3953899933843856; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.3883838366600685e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.97
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.36; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.97
Batch: 620; loss: 0.37; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

5.484652501763776e-05
2.4735427359701134e-05
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3628470843574803; val_accuracy: 0.9205812101910829 

The current subspace-distance is: 2.4735427359701134e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.72; acc: 0.78
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.98
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.674290514434688e-05
2.4726805349928327e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.359519975959875; val_accuracy: 0.9223726114649682 

The current subspace-distance is: 2.4726805349928327e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.736346065532416e-05
2.5832969186012633e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.36225293576717377; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 2.5832969186012633e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.98
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.95
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.798616257379763e-05
2.6083505872520618e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3548770977812967; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 2.6083505872520618e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.98
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.8653833548305556e-05
2.5550389182171784e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3434053659439087; val_accuracy: 0.9244625796178344 

The current subspace-distance is: 2.5550389182171784e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.98
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.97
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.78
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.854206392541528e-05
2.6116975277545862e-05
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3362586494464024; val_accuracy: 0.9273487261146497 

The current subspace-distance is: 2.6116975277545862e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.9700974816223606e-05
2.652696639415808e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3331343043761648; val_accuracy: 0.9266520700636943 

The current subspace-distance is: 2.652696639415808e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.98
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.97
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.9197114751441404e-05
2.5518869733787142e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3256103301503856; val_accuracy: 0.9281449044585988 

The current subspace-distance is: 2.5518869733787142e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.26; acc: 1.0
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.138500611996278e-05
2.744385892583523e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3244341013917498; val_accuracy: 0.9261544585987261 

The current subspace-distance is: 2.744385892583523e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.98
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.57; acc: 0.78
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.220904469955713e-05
2.9521168471546844e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3222022005327188; val_accuracy: 0.9266520700636943 

The current subspace-distance is: 2.9521168471546844e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.98
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.98
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

6.152965943329036e-05
2.657179720699787e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.31843249850971683; val_accuracy: 0.9269506369426752 

The current subspace-distance is: 2.657179720699787e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.98
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.98
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.322006083792076e-05
2.9253638786030933e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.31754068849952355; val_accuracy: 0.9276472929936306 

The current subspace-distance is: 2.9253638786030933e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.98
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.213178858160973e-05
2.8068370738765225e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3168257146978834; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.8068370738765225e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.97
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.248346471693367e-05
2.6957008230965585e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.31499206004249064; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.6957008230965585e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.98
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.18997000856325e-05
2.6219191568088718e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3132616789287822; val_accuracy: 0.9301353503184714 

The current subspace-distance is: 2.6219191568088718e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.310371827566996e-05
2.8542301151901484e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.31384629490459043; val_accuracy: 0.9290406050955414 

The current subspace-distance is: 2.8542301151901484e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.81
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.235796900000423e-05
2.7122365281684324e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.30921407385616545; val_accuracy: 0.9305334394904459 

The current subspace-distance is: 2.7122365281684324e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.2; acc: 1.0
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.22; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.30180657026358e-05
2.8371872758725658e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3119658396881857; val_accuracy: 0.9300358280254777 

The current subspace-distance is: 2.8371872758725658e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.286821735557169e-05
2.7587191652855836e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.310992287127835; val_accuracy: 0.9289410828025477 

The current subspace-distance is: 2.7587191652855836e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.97
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.97
Batch: 780; loss: 0.54; acc: 0.81
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.329471216304228e-05
2.8696833396679722e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3112528904987748; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.8696833396679722e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.97
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.467539060395211e-05
3.0227945899241604e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.30600703502916227; val_accuracy: 0.929140127388535 

The current subspace-distance is: 3.0227945899241604e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_7_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.75

The number of parameters is: 271389

The number of individual parameters is:

14
252
14
14
21
38220
21
21
42
114660
42
42
64
112896
64
64
4096
64
640
10
64
64

nonzero elements in E: 135694490
elements in E: 135694500
fraction nonzero: 0.9999999263050455
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.54; acc: 0.03
Batch: 20; loss: 2.02; acc: 0.38
Batch: 40; loss: 1.67; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.52
Batch: 80; loss: 1.6; acc: 0.55
Batch: 100; loss: 1.52; acc: 0.64
Batch: 120; loss: 1.48; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.73
Batch: 160; loss: 1.36; acc: 0.77
Batch: 180; loss: 1.39; acc: 0.64
Batch: 200; loss: 1.26; acc: 0.77
Batch: 220; loss: 1.25; acc: 0.86
Batch: 240; loss: 1.14; acc: 0.84
Batch: 260; loss: 1.24; acc: 0.77
Batch: 280; loss: 1.18; acc: 0.77
Batch: 300; loss: 1.11; acc: 0.8
Batch: 320; loss: 1.22; acc: 0.75
Batch: 340; loss: 1.24; acc: 0.8
Batch: 360; loss: 1.27; acc: 0.73
Batch: 380; loss: 1.07; acc: 0.86
Batch: 400; loss: 1.15; acc: 0.77
Batch: 420; loss: 1.08; acc: 0.78
Batch: 440; loss: 1.12; acc: 0.78
Batch: 460; loss: 0.93; acc: 0.91
Batch: 480; loss: 1.04; acc: 0.83
Batch: 500; loss: 0.98; acc: 0.84
Batch: 520; loss: 0.99; acc: 0.88
Batch: 540; loss: 0.98; acc: 0.84
Batch: 560; loss: 0.99; acc: 0.81
Batch: 580; loss: 0.96; acc: 0.84
Batch: 600; loss: 0.92; acc: 0.86
Batch: 620; loss: 0.99; acc: 0.83
Batch: 640; loss: 0.91; acc: 0.84
Batch: 660; loss: 1.0; acc: 0.83
Batch: 680; loss: 0.94; acc: 0.83
Batch: 700; loss: 0.94; acc: 0.84
Batch: 720; loss: 0.94; acc: 0.84
Batch: 740; loss: 0.98; acc: 0.83
Batch: 760; loss: 0.97; acc: 0.83
Batch: 780; loss: 0.94; acc: 0.88
Train Epoch over. train_loss: 1.19; train_accuracy: 0.75 

2.66411680058809e-05
8.837930181471165e-06
Batch: 0; loss: 0.9; acc: 0.86
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 0.61; acc: 0.97
Batch: 60; loss: 0.91; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.9; acc: 0.83
Batch: 120; loss: 1.05; acc: 0.78
Batch: 140; loss: 0.74; acc: 0.92
Val Epoch over. val_loss: 0.8437443675508924; val_accuracy: 0.8632563694267515 

The current subspace-distance is: 8.837930181471165e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.84
Batch: 20; loss: 1.0; acc: 0.8
Batch: 40; loss: 0.9; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.88
Batch: 80; loss: 0.74; acc: 0.92
Batch: 100; loss: 0.95; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 0.91; acc: 0.86
Batch: 160; loss: 1.03; acc: 0.75
Batch: 180; loss: 0.85; acc: 0.86
Batch: 200; loss: 0.91; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.86
Batch: 240; loss: 0.78; acc: 0.84
Batch: 260; loss: 0.79; acc: 0.88
Batch: 280; loss: 0.85; acc: 0.88
Batch: 300; loss: 0.85; acc: 0.81
Batch: 320; loss: 0.71; acc: 0.92
Batch: 340; loss: 0.82; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.92
Batch: 400; loss: 0.85; acc: 0.89
Batch: 420; loss: 0.84; acc: 0.86
Batch: 440; loss: 0.77; acc: 0.86
Batch: 460; loss: 0.76; acc: 0.89
Batch: 480; loss: 0.74; acc: 0.95
Batch: 500; loss: 0.89; acc: 0.83
Batch: 520; loss: 0.72; acc: 0.91
Batch: 540; loss: 0.84; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.86
Batch: 580; loss: 0.79; acc: 0.8
Batch: 600; loss: 0.62; acc: 0.92
Batch: 620; loss: 0.77; acc: 0.83
Batch: 640; loss: 0.79; acc: 0.81
Batch: 660; loss: 0.7; acc: 0.88
Batch: 680; loss: 0.77; acc: 0.86
Batch: 700; loss: 0.78; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.88
Batch: 740; loss: 0.72; acc: 0.88
Batch: 760; loss: 0.73; acc: 0.92
Batch: 780; loss: 0.66; acc: 0.94
Train Epoch over. train_loss: 0.8; train_accuracy: 0.86 

3.184486922691576e-05
1.3096501788822934e-05
Batch: 0; loss: 0.66; acc: 0.91
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.94
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.89; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.95
Val Epoch over. val_loss: 0.6553690129784262; val_accuracy: 0.8919187898089171 

The current subspace-distance is: 1.3096501788822934e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.66; acc: 0.92
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.77; acc: 0.83
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.92
Batch: 280; loss: 0.69; acc: 0.88
Batch: 300; loss: 0.69; acc: 0.86
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.89
Batch: 360; loss: 0.86; acc: 0.77
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.91
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.95
Batch: 460; loss: 0.59; acc: 0.92
Batch: 480; loss: 0.52; acc: 0.94
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.67; acc: 0.88
Batch: 540; loss: 0.72; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.67; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.48; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.98
Batch: 680; loss: 0.52; acc: 0.92
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.52; acc: 0.94
Batch: 760; loss: 0.62; acc: 0.89
Batch: 780; loss: 0.67; acc: 0.94
Train Epoch over. train_loss: 0.66; train_accuracy: 0.88 

3.620607094489969e-05
1.6073034203145653e-05
Batch: 0; loss: 0.5; acc: 0.97
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.95
Val Epoch over. val_loss: 0.5385924013936596; val_accuracy: 0.9090366242038217 

The current subspace-distance is: 1.6073034203145653e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.95
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.91
Batch: 220; loss: 0.66; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.89
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.94
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.66; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.94
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.56; train_accuracy: 0.9 

4.052925214637071e-05
1.717699342407286e-05
Batch: 0; loss: 0.4; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.38; acc: 0.98
Val Epoch over. val_loss: 0.45908599626866115; val_accuracy: 0.9167993630573248 

The current subspace-distance is: 1.717699342407286e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.8
Batch: 220; loss: 0.61; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.97
Batch: 440; loss: 0.31; acc: 1.0
Batch: 460; loss: 0.36; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.95
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.95
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.9 

4.4474934838945046e-05
1.9315579265821725e-05
Batch: 0; loss: 0.35; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.21; acc: 1.0
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.98
Val Epoch over. val_loss: 0.41340898366490747; val_accuracy: 0.9241640127388535 

The current subspace-distance is: 1.9315579265821725e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.97
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.68; acc: 0.78
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.91 

4.765789344673976e-05
2.0403622329467908e-05
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.37547765994907184; val_accuracy: 0.9286425159235668 

The current subspace-distance is: 2.0403622329467908e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.28; acc: 1.0
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.97
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.97
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.3; acc: 0.98
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.92 

5.045628495281562e-05
2.3034963305690326e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.35301328227398504; val_accuracy: 0.929140127388535 

The current subspace-distance is: 2.3034963305690326e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.97
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.92 

5.3135725465836003e-05
2.3733346097287722e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.33109652170330095; val_accuracy: 0.9331210191082803 

The current subspace-distance is: 2.3733346097287722e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.98
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.535441232495941e-05
2.4341996322618797e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.97
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.31482458760024634; val_accuracy: 0.9351114649681529 

The current subspace-distance is: 2.4341996322618797e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.98
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.666667129844427e-05
2.5206005375366658e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.30250377677808143; val_accuracy: 0.934812898089172 

The current subspace-distance is: 2.5206005375366658e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.97
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.7719436881598085e-05
2.5486573576927185e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.29546835074189365; val_accuracy: 0.9370023885350318 

The current subspace-distance is: 2.5486573576927185e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.18; acc: 1.0
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.98
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.8763878769241273e-05
2.6388224796392024e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.29492859409493244; val_accuracy: 0.9366042993630573 

The current subspace-distance is: 2.6388224796392024e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.98
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.8855028328252956e-05
2.664567728061229e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.29568390776017667; val_accuracy: 0.9375995222929936 

The current subspace-distance is: 2.664567728061229e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.97
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.98
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.98
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.883153426111676e-05
2.6167374016949907e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.28733663137551324; val_accuracy: 0.9382961783439491 

The current subspace-distance is: 2.6167374016949907e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.97
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.997539483360015e-05
2.6853891540667973e-05
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.28716466504677085; val_accuracy: 0.93859474522293 

The current subspace-distance is: 2.6853891540667973e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.98
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.98
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.0400194342946634e-05
2.7012574719265103e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.2822683245702914; val_accuracy: 0.9409832802547771 

The current subspace-distance is: 2.7012574719265103e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

6.079936429159716e-05
2.6485704438528046e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.27992525032371474; val_accuracy: 0.9399880573248408 

The current subspace-distance is: 2.6485704438528046e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.97
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.98
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.220742943696678e-05
2.9164348234189674e-05
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.27384995332189427; val_accuracy: 0.9400875796178344 

The current subspace-distance is: 2.9164348234189674e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.173621659399942e-05
2.8579122954397462e-05
