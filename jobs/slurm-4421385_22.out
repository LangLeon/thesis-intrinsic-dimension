model : table13slim
N : 6
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:58

Channel scaling factor: 2.05

The number of parameters is: 275012

The number of individual parameters is:

17
306
17
17
25
39100
25
25
50
115000
50
50
64
115200
64
64
4096
64
640
10
64
64

nonzero elements in E: 13750598
elements in E: 13750600
fraction nonzero: 0.9999998545518014
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.46; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.22; acc: 0.16
Batch: 60; loss: 2.21; acc: 0.17
Batch: 80; loss: 2.18; acc: 0.17
Batch: 100; loss: 2.08; acc: 0.33
Batch: 120; loss: 2.13; acc: 0.2
Batch: 140; loss: 2.18; acc: 0.3
Batch: 160; loss: 2.14; acc: 0.3
Batch: 180; loss: 2.17; acc: 0.2
Batch: 200; loss: 2.03; acc: 0.33
Batch: 220; loss: 2.11; acc: 0.25
Batch: 240; loss: 2.12; acc: 0.3
Batch: 260; loss: 2.07; acc: 0.36
Batch: 280; loss: 2.0; acc: 0.34
Batch: 300; loss: 2.04; acc: 0.39
Batch: 320; loss: 2.0; acc: 0.38
Batch: 340; loss: 1.95; acc: 0.44
Batch: 360; loss: 1.85; acc: 0.53
Batch: 380; loss: 1.93; acc: 0.47
Batch: 400; loss: 1.92; acc: 0.48
Batch: 420; loss: 1.99; acc: 0.42
Batch: 440; loss: 1.86; acc: 0.56
Batch: 460; loss: 1.86; acc: 0.44
Batch: 480; loss: 1.95; acc: 0.41
Batch: 500; loss: 1.96; acc: 0.47
Batch: 520; loss: 1.82; acc: 0.52
Batch: 540; loss: 1.85; acc: 0.45
Batch: 560; loss: 1.94; acc: 0.39
Batch: 580; loss: 1.83; acc: 0.47
Batch: 600; loss: 1.87; acc: 0.36
Batch: 620; loss: 1.9; acc: 0.5
Batch: 640; loss: 1.92; acc: 0.39
Batch: 660; loss: 1.91; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.33
Batch: 700; loss: 1.88; acc: 0.42
Batch: 720; loss: 1.88; acc: 0.45
Batch: 740; loss: 1.84; acc: 0.39
Batch: 760; loss: 1.77; acc: 0.55
Batch: 780; loss: 1.87; acc: 0.42
Train Epoch over. train_loss: 1.99; train_accuracy: 0.38 

2.47099651460303e-05
5.014691851101816e-06
Batch: 0; loss: 1.85; acc: 0.44
Batch: 20; loss: 1.97; acc: 0.33
Batch: 40; loss: 1.66; acc: 0.62
Batch: 60; loss: 1.79; acc: 0.53
Batch: 80; loss: 1.75; acc: 0.5
Batch: 100; loss: 1.87; acc: 0.47
Batch: 120; loss: 1.83; acc: 0.48
Batch: 140; loss: 1.68; acc: 0.52
Val Epoch over. val_loss: 1.808057038647354; val_accuracy: 0.49203821656050956 

The current subspace-distance is: 5.014691851101816e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.95; acc: 0.36
Batch: 20; loss: 1.79; acc: 0.53
Batch: 40; loss: 1.86; acc: 0.52
Batch: 60; loss: 1.88; acc: 0.44
Batch: 80; loss: 1.75; acc: 0.53
Batch: 100; loss: 1.76; acc: 0.48
Batch: 120; loss: 1.77; acc: 0.5
Batch: 140; loss: 1.75; acc: 0.55
Batch: 160; loss: 1.81; acc: 0.47
Batch: 180; loss: 1.89; acc: 0.45
Batch: 200; loss: 1.78; acc: 0.52
Batch: 220; loss: 1.86; acc: 0.47
Batch: 240; loss: 1.79; acc: 0.47
Batch: 260; loss: 1.68; acc: 0.55
Batch: 280; loss: 1.72; acc: 0.56
Batch: 300; loss: 1.88; acc: 0.41
Batch: 320; loss: 1.7; acc: 0.55
Batch: 340; loss: 1.72; acc: 0.47
Batch: 360; loss: 1.79; acc: 0.5
Batch: 380; loss: 1.82; acc: 0.48
Batch: 400; loss: 1.84; acc: 0.47
Batch: 420; loss: 1.89; acc: 0.42
Batch: 440; loss: 1.65; acc: 0.64
Batch: 460; loss: 1.8; acc: 0.5
Batch: 480; loss: 1.87; acc: 0.45
Batch: 500; loss: 1.83; acc: 0.5
Batch: 520; loss: 1.72; acc: 0.47
Batch: 540; loss: 1.67; acc: 0.61
Batch: 560; loss: 1.81; acc: 0.44
Batch: 580; loss: 1.66; acc: 0.59
Batch: 600; loss: 1.84; acc: 0.39
Batch: 620; loss: 1.7; acc: 0.56
Batch: 640; loss: 1.6; acc: 0.67
Batch: 660; loss: 1.88; acc: 0.42
Batch: 680; loss: 1.71; acc: 0.48
Batch: 700; loss: 1.64; acc: 0.62
Batch: 720; loss: 1.8; acc: 0.53
Batch: 740; loss: 1.69; acc: 0.53
Batch: 760; loss: 1.81; acc: 0.47
Batch: 780; loss: 1.8; acc: 0.45
Train Epoch over. train_loss: 1.78; train_accuracy: 0.5 

2.7727428459911607e-05
7.875555638747755e-06
Batch: 0; loss: 1.72; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.38
Batch: 40; loss: 1.55; acc: 0.64
Batch: 60; loss: 1.7; acc: 0.59
Batch: 80; loss: 1.67; acc: 0.56
Batch: 100; loss: 1.72; acc: 0.5
Batch: 120; loss: 1.74; acc: 0.53
Batch: 140; loss: 1.56; acc: 0.66
Val Epoch over. val_loss: 1.7102485659775462; val_accuracy: 0.5302547770700637 

The current subspace-distance is: 7.875555638747755e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.77; acc: 0.55
Batch: 20; loss: 1.73; acc: 0.52
Batch: 40; loss: 1.75; acc: 0.48
Batch: 60; loss: 1.77; acc: 0.45
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.83; acc: 0.5
Batch: 120; loss: 1.73; acc: 0.56
Batch: 140; loss: 1.8; acc: 0.53
Batch: 160; loss: 1.84; acc: 0.38
Batch: 180; loss: 1.64; acc: 0.59
Batch: 200; loss: 1.79; acc: 0.45
Batch: 220; loss: 1.67; acc: 0.45
Batch: 240; loss: 1.73; acc: 0.53
Batch: 260; loss: 1.61; acc: 0.56
Batch: 280; loss: 1.73; acc: 0.52
Batch: 300; loss: 1.8; acc: 0.52
Batch: 320; loss: 1.82; acc: 0.41
Batch: 340; loss: 1.81; acc: 0.53
Batch: 360; loss: 1.63; acc: 0.61
Batch: 380; loss: 1.62; acc: 0.62
Batch: 400; loss: 1.67; acc: 0.61
Batch: 420; loss: 1.71; acc: 0.55
Batch: 440; loss: 1.83; acc: 0.48
Batch: 460; loss: 1.7; acc: 0.52
Batch: 480; loss: 1.78; acc: 0.48
Batch: 500; loss: 1.6; acc: 0.59
Batch: 520; loss: 1.89; acc: 0.38
Batch: 540; loss: 1.64; acc: 0.58
Batch: 560; loss: 1.63; acc: 0.53
Batch: 580; loss: 1.85; acc: 0.48
Batch: 600; loss: 1.82; acc: 0.52
Batch: 620; loss: 1.75; acc: 0.45
Batch: 640; loss: 1.68; acc: 0.47
Batch: 660; loss: 1.77; acc: 0.52
Batch: 680; loss: 1.76; acc: 0.52
Batch: 700; loss: 1.67; acc: 0.45
Batch: 720; loss: 1.82; acc: 0.47
Batch: 740; loss: 1.73; acc: 0.47
Batch: 760; loss: 1.71; acc: 0.5
Batch: 780; loss: 1.69; acc: 0.58
Train Epoch over. train_loss: 1.73; train_accuracy: 0.51 

2.9975462894071825e-05
7.64014203014085e-06
Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.9; acc: 0.39
Batch: 40; loss: 1.52; acc: 0.67
Batch: 60; loss: 1.67; acc: 0.55
Batch: 80; loss: 1.63; acc: 0.55
Batch: 100; loss: 1.69; acc: 0.5
Batch: 120; loss: 1.71; acc: 0.55
Batch: 140; loss: 1.54; acc: 0.59
Val Epoch over. val_loss: 1.674991957700936; val_accuracy: 0.5354299363057324 

The current subspace-distance is: 7.64014203014085e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.62
Batch: 20; loss: 1.73; acc: 0.47
Batch: 40; loss: 1.76; acc: 0.42
Batch: 60; loss: 1.58; acc: 0.62
Batch: 80; loss: 1.77; acc: 0.39
Batch: 100; loss: 1.75; acc: 0.45
Batch: 120; loss: 1.66; acc: 0.58
Batch: 140; loss: 1.62; acc: 0.55
Batch: 160; loss: 1.77; acc: 0.48
Batch: 180; loss: 1.69; acc: 0.48
Batch: 200; loss: 1.74; acc: 0.47
Batch: 220; loss: 1.65; acc: 0.61
Batch: 240; loss: 1.48; acc: 0.75
Batch: 260; loss: 1.6; acc: 0.53
Batch: 280; loss: 1.74; acc: 0.5
Batch: 300; loss: 1.76; acc: 0.53
Batch: 320; loss: 1.56; acc: 0.66
Batch: 340; loss: 1.8; acc: 0.45
Batch: 360; loss: 1.72; acc: 0.59
Batch: 380; loss: 1.69; acc: 0.52
Batch: 400; loss: 1.81; acc: 0.52
Batch: 420; loss: 1.73; acc: 0.52
Batch: 440; loss: 1.6; acc: 0.55
Batch: 460; loss: 1.81; acc: 0.41
Batch: 480; loss: 1.74; acc: 0.47
Batch: 500; loss: 1.49; acc: 0.62
Batch: 520; loss: 1.77; acc: 0.48
Batch: 540; loss: 1.75; acc: 0.5
Batch: 560; loss: 1.68; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.67; acc: 0.53
Batch: 620; loss: 1.59; acc: 0.58
Batch: 640; loss: 1.69; acc: 0.5
Batch: 660; loss: 1.7; acc: 0.52
Batch: 680; loss: 1.73; acc: 0.47
Batch: 700; loss: 1.76; acc: 0.44
Batch: 720; loss: 1.7; acc: 0.52
Batch: 740; loss: 1.74; acc: 0.5
Batch: 760; loss: 1.73; acc: 0.52
Batch: 780; loss: 1.84; acc: 0.39
Train Epoch over. train_loss: 1.71; train_accuracy: 0.51 

3.115961953881197e-05
8.029392120079137e-06
Batch: 0; loss: 1.63; acc: 0.55
Batch: 20; loss: 1.88; acc: 0.41
Batch: 40; loss: 1.52; acc: 0.61
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.63; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.52
Batch: 120; loss: 1.7; acc: 0.52
Batch: 140; loss: 1.55; acc: 0.58
Val Epoch over. val_loss: 1.6607422092158324; val_accuracy: 0.5349323248407644 

The current subspace-distance is: 8.029392120079137e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 1.79; acc: 0.47
Batch: 40; loss: 1.59; acc: 0.55
Batch: 60; loss: 1.7; acc: 0.55
Batch: 80; loss: 1.75; acc: 0.52
Batch: 100; loss: 1.76; acc: 0.48
Batch: 120; loss: 1.8; acc: 0.52
Batch: 140; loss: 1.69; acc: 0.5
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.55; acc: 0.62
Batch: 200; loss: 1.66; acc: 0.56
Batch: 220; loss: 1.8; acc: 0.52
Batch: 240; loss: 1.65; acc: 0.52
Batch: 260; loss: 1.6; acc: 0.52
Batch: 280; loss: 1.7; acc: 0.5
Batch: 300; loss: 1.75; acc: 0.52
Batch: 320; loss: 1.78; acc: 0.48
Batch: 340; loss: 1.74; acc: 0.45
Batch: 360; loss: 1.74; acc: 0.52
Batch: 380; loss: 1.76; acc: 0.48
Batch: 400; loss: 1.67; acc: 0.5
Batch: 420; loss: 1.86; acc: 0.44
Batch: 440; loss: 1.71; acc: 0.48
Batch: 460; loss: 1.68; acc: 0.52
Batch: 480; loss: 1.65; acc: 0.56
Batch: 500; loss: 1.58; acc: 0.64
Batch: 520; loss: 1.79; acc: 0.45
Batch: 540; loss: 1.78; acc: 0.45
Batch: 560; loss: 1.77; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.53
Batch: 600; loss: 1.78; acc: 0.47
Batch: 620; loss: 1.72; acc: 0.48
Batch: 640; loss: 1.68; acc: 0.55
Batch: 660; loss: 1.62; acc: 0.53
Batch: 680; loss: 1.79; acc: 0.39
Batch: 700; loss: 1.67; acc: 0.58
Batch: 720; loss: 1.74; acc: 0.56
Batch: 740; loss: 1.57; acc: 0.56
Batch: 760; loss: 1.83; acc: 0.47
Batch: 780; loss: 1.83; acc: 0.33
Train Epoch over. train_loss: 1.69; train_accuracy: 0.51 

3.169970295857638e-05
9.35051230044337e-06
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.86; acc: 0.39
Batch: 40; loss: 1.49; acc: 0.62
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.6; acc: 0.59
Batch: 100; loss: 1.63; acc: 0.59
Batch: 120; loss: 1.7; acc: 0.5
Batch: 140; loss: 1.55; acc: 0.59
Val Epoch over. val_loss: 1.6324497696700369; val_accuracy: 0.5459792993630573 

The current subspace-distance is: 9.35051230044337e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.47
Batch: 20; loss: 1.68; acc: 0.56
Batch: 40; loss: 1.58; acc: 0.56
Batch: 60; loss: 1.72; acc: 0.52
Batch: 80; loss: 1.61; acc: 0.55
Batch: 100; loss: 1.74; acc: 0.52
Batch: 120; loss: 1.73; acc: 0.48
Batch: 140; loss: 1.76; acc: 0.5
Batch: 160; loss: 1.63; acc: 0.55
Batch: 180; loss: 1.6; acc: 0.53
Batch: 200; loss: 1.66; acc: 0.53
Batch: 220; loss: 1.63; acc: 0.52
Batch: 240; loss: 1.66; acc: 0.53
Batch: 260; loss: 1.66; acc: 0.5
Batch: 280; loss: 1.77; acc: 0.45
Batch: 300; loss: 1.6; acc: 0.56
Batch: 320; loss: 1.66; acc: 0.47
Batch: 340; loss: 1.71; acc: 0.56
Batch: 360; loss: 1.71; acc: 0.48
Batch: 380; loss: 1.61; acc: 0.61
Batch: 400; loss: 1.67; acc: 0.59
Batch: 420; loss: 1.61; acc: 0.61
Batch: 440; loss: 1.71; acc: 0.36
Batch: 460; loss: 1.56; acc: 0.59
Batch: 480; loss: 1.74; acc: 0.56
Batch: 500; loss: 1.92; acc: 0.44
Batch: 520; loss: 1.61; acc: 0.59
Batch: 540; loss: 1.69; acc: 0.48
Batch: 560; loss: 1.75; acc: 0.5
Batch: 580; loss: 1.6; acc: 0.55
Batch: 600; loss: 1.65; acc: 0.58
Batch: 620; loss: 1.82; acc: 0.41
Batch: 640; loss: 1.62; acc: 0.55
Batch: 660; loss: 1.63; acc: 0.61
Batch: 680; loss: 1.71; acc: 0.52
Batch: 700; loss: 1.7; acc: 0.52
Batch: 720; loss: 1.6; acc: 0.62
Batch: 740; loss: 1.69; acc: 0.48
Batch: 760; loss: 1.61; acc: 0.53
Batch: 780; loss: 1.71; acc: 0.55
Train Epoch over. train_loss: 1.68; train_accuracy: 0.52 

3.235514304833487e-05
9.244376087735873e-06
Batch: 0; loss: 1.57; acc: 0.59
Batch: 20; loss: 1.89; acc: 0.39
Batch: 40; loss: 1.49; acc: 0.61
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.65; acc: 0.59
Batch: 120; loss: 1.71; acc: 0.5
Batch: 140; loss: 1.56; acc: 0.56
Val Epoch over. val_loss: 1.6261394942642018; val_accuracy: 0.5440883757961783 

The current subspace-distance is: 9.244376087735873e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.7; acc: 0.52
Batch: 20; loss: 1.6; acc: 0.56
Batch: 40; loss: 1.74; acc: 0.44
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.61; acc: 0.59
Batch: 100; loss: 1.65; acc: 0.45
Batch: 120; loss: 1.84; acc: 0.38
Batch: 140; loss: 1.63; acc: 0.53
Batch: 160; loss: 1.52; acc: 0.61
Batch: 180; loss: 1.63; acc: 0.55
Batch: 200; loss: 1.69; acc: 0.45
Batch: 220; loss: 1.67; acc: 0.55
Batch: 240; loss: 1.63; acc: 0.52
Batch: 260; loss: 1.67; acc: 0.5
Batch: 280; loss: 1.55; acc: 0.52
Batch: 300; loss: 1.67; acc: 0.5
Batch: 320; loss: 1.62; acc: 0.45
Batch: 340; loss: 1.6; acc: 0.58
Batch: 360; loss: 1.56; acc: 0.59
Batch: 380; loss: 1.66; acc: 0.5
Batch: 400; loss: 1.64; acc: 0.52
Batch: 420; loss: 1.63; acc: 0.55
Batch: 440; loss: 1.56; acc: 0.56
Batch: 460; loss: 1.52; acc: 0.59
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.74; acc: 0.5
Batch: 520; loss: 1.61; acc: 0.55
Batch: 540; loss: 1.74; acc: 0.39
Batch: 560; loss: 1.57; acc: 0.58
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.61; acc: 0.47
Batch: 620; loss: 1.72; acc: 0.41
Batch: 640; loss: 1.7; acc: 0.48
Batch: 660; loss: 1.72; acc: 0.52
Batch: 680; loss: 1.69; acc: 0.53
Batch: 700; loss: 1.66; acc: 0.52
Batch: 720; loss: 1.58; acc: 0.64
Batch: 740; loss: 1.69; acc: 0.48
Batch: 760; loss: 1.72; acc: 0.47
Batch: 780; loss: 1.78; acc: 0.34
Train Epoch over. train_loss: 1.66; train_accuracy: 0.52 

3.3025691664079204e-05
9.821826097322628e-06
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.86; acc: 0.39
Batch: 40; loss: 1.46; acc: 0.61
Batch: 60; loss: 1.63; acc: 0.52
Batch: 80; loss: 1.57; acc: 0.58
Batch: 100; loss: 1.62; acc: 0.58
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.54; acc: 0.53
Val Epoch over. val_loss: 1.6063717816286027; val_accuracy: 0.5510549363057324 

The current subspace-distance is: 9.821826097322628e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.7; acc: 0.55
Batch: 20; loss: 1.73; acc: 0.58
Batch: 40; loss: 1.52; acc: 0.59
Batch: 60; loss: 1.53; acc: 0.55
Batch: 80; loss: 1.79; acc: 0.42
Batch: 100; loss: 1.69; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.47
Batch: 140; loss: 1.7; acc: 0.5
Batch: 160; loss: 1.67; acc: 0.52
Batch: 180; loss: 1.78; acc: 0.41
Batch: 200; loss: 1.65; acc: 0.53
Batch: 220; loss: 1.54; acc: 0.67
Batch: 240; loss: 1.75; acc: 0.5
Batch: 260; loss: 1.73; acc: 0.52
Batch: 280; loss: 1.55; acc: 0.59
Batch: 300; loss: 1.61; acc: 0.56
Batch: 320; loss: 1.64; acc: 0.48
Batch: 340; loss: 1.71; acc: 0.48
Batch: 360; loss: 1.73; acc: 0.5
Batch: 380; loss: 1.69; acc: 0.53
Batch: 400; loss: 1.69; acc: 0.45
Batch: 420; loss: 1.62; acc: 0.52
Batch: 440; loss: 1.55; acc: 0.56
Batch: 460; loss: 1.61; acc: 0.56
Batch: 480; loss: 1.62; acc: 0.48
Batch: 500; loss: 1.48; acc: 0.53
Batch: 520; loss: 1.75; acc: 0.48
Batch: 540; loss: 1.66; acc: 0.52
Batch: 560; loss: 1.63; acc: 0.55
Batch: 580; loss: 1.56; acc: 0.56
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.55; acc: 0.5
Batch: 640; loss: 1.51; acc: 0.58
Batch: 660; loss: 1.46; acc: 0.62
Batch: 680; loss: 1.59; acc: 0.62
Batch: 700; loss: 1.58; acc: 0.66
Batch: 720; loss: 1.57; acc: 0.55
Batch: 740; loss: 1.58; acc: 0.53
Batch: 760; loss: 1.73; acc: 0.44
Batch: 780; loss: 1.64; acc: 0.56
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.420780194574036e-05
1.0986498637066688e-05
Batch: 0; loss: 1.52; acc: 0.56
Batch: 20; loss: 1.82; acc: 0.45
Batch: 40; loss: 1.4; acc: 0.61
Batch: 60; loss: 1.61; acc: 0.52
Batch: 80; loss: 1.57; acc: 0.56
Batch: 100; loss: 1.56; acc: 0.62
Batch: 120; loss: 1.64; acc: 0.53
Batch: 140; loss: 1.52; acc: 0.55
Val Epoch over. val_loss: 1.578304255084627; val_accuracy: 0.5537420382165605 

The current subspace-distance is: 1.0986498637066688e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.62
Batch: 20; loss: 1.54; acc: 0.53
Batch: 40; loss: 1.6; acc: 0.62
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.59; acc: 0.58
Batch: 100; loss: 1.74; acc: 0.44
Batch: 120; loss: 1.61; acc: 0.56
Batch: 140; loss: 1.46; acc: 0.62
Batch: 160; loss: 1.55; acc: 0.62
Batch: 180; loss: 1.53; acc: 0.61
Batch: 200; loss: 1.58; acc: 0.48
Batch: 220; loss: 1.65; acc: 0.48
Batch: 240; loss: 1.69; acc: 0.48
Batch: 260; loss: 1.73; acc: 0.41
Batch: 280; loss: 1.59; acc: 0.55
Batch: 300; loss: 1.59; acc: 0.56
Batch: 320; loss: 1.58; acc: 0.52
Batch: 340; loss: 1.77; acc: 0.47
Batch: 360; loss: 1.6; acc: 0.56
Batch: 380; loss: 1.63; acc: 0.52
Batch: 400; loss: 1.62; acc: 0.53
Batch: 420; loss: 1.61; acc: 0.61
Batch: 440; loss: 1.53; acc: 0.59
Batch: 460; loss: 1.61; acc: 0.55
Batch: 480; loss: 1.74; acc: 0.47
Batch: 500; loss: 1.47; acc: 0.58
Batch: 520; loss: 1.61; acc: 0.58
Batch: 540; loss: 1.62; acc: 0.55
Batch: 560; loss: 1.48; acc: 0.59
Batch: 580; loss: 1.68; acc: 0.5
Batch: 600; loss: 1.54; acc: 0.58
Batch: 620; loss: 1.65; acc: 0.52
Batch: 640; loss: 1.64; acc: 0.44
Batch: 660; loss: 1.53; acc: 0.59
Batch: 680; loss: 1.63; acc: 0.47
Batch: 700; loss: 1.53; acc: 0.59
Batch: 720; loss: 1.57; acc: 0.53
Batch: 740; loss: 1.64; acc: 0.52
Batch: 760; loss: 1.7; acc: 0.47
Batch: 780; loss: 1.5; acc: 0.58
Train Epoch over. train_loss: 1.61; train_accuracy: 0.54 

3.586933598853648e-05
1.1698261914716568e-05
Batch: 0; loss: 1.49; acc: 0.62
Batch: 20; loss: 1.8; acc: 0.44
Batch: 40; loss: 1.36; acc: 0.61
Batch: 60; loss: 1.6; acc: 0.52
Batch: 80; loss: 1.54; acc: 0.61
Batch: 100; loss: 1.54; acc: 0.62
Batch: 120; loss: 1.61; acc: 0.59
Batch: 140; loss: 1.5; acc: 0.56
Val Epoch over. val_loss: 1.5530182996373267; val_accuracy: 0.5659832802547771 

The current subspace-distance is: 1.1698261914716568e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.56
Batch: 20; loss: 1.54; acc: 0.61
Batch: 40; loss: 1.47; acc: 0.59
Batch: 60; loss: 1.62; acc: 0.58
Batch: 80; loss: 1.73; acc: 0.47
Batch: 100; loss: 1.61; acc: 0.55
Batch: 120; loss: 1.55; acc: 0.59
Batch: 140; loss: 1.51; acc: 0.58
Batch: 160; loss: 1.47; acc: 0.64
Batch: 180; loss: 1.49; acc: 0.64
Batch: 200; loss: 1.62; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.53
Batch: 240; loss: 1.59; acc: 0.5
Batch: 260; loss: 1.66; acc: 0.48
Batch: 280; loss: 1.62; acc: 0.5
Batch: 300; loss: 1.7; acc: 0.48
Batch: 320; loss: 1.52; acc: 0.56
Batch: 340; loss: 1.73; acc: 0.48
Batch: 360; loss: 1.66; acc: 0.47
Batch: 380; loss: 1.64; acc: 0.5
Batch: 400; loss: 1.5; acc: 0.58
Batch: 420; loss: 1.51; acc: 0.5
Batch: 440; loss: 1.56; acc: 0.56
Batch: 460; loss: 1.64; acc: 0.47
Batch: 480; loss: 1.49; acc: 0.58
Batch: 500; loss: 1.83; acc: 0.48
Batch: 520; loss: 1.52; acc: 0.61
Batch: 540; loss: 1.49; acc: 0.58
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.57; acc: 0.56
Batch: 600; loss: 1.7; acc: 0.45
Batch: 620; loss: 1.5; acc: 0.56
Batch: 640; loss: 1.46; acc: 0.61
Batch: 660; loss: 1.56; acc: 0.52
Batch: 680; loss: 1.57; acc: 0.48
Batch: 700; loss: 1.44; acc: 0.67
Batch: 720; loss: 1.63; acc: 0.58
Batch: 740; loss: 1.49; acc: 0.55
Batch: 760; loss: 1.54; acc: 0.55
Batch: 780; loss: 1.44; acc: 0.62
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

3.687499702209607e-05
1.1974080734944437e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.58
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.49; acc: 0.55
Val Epoch over. val_loss: 1.5424329315780834; val_accuracy: 0.5741441082802548 

The current subspace-distance is: 1.1974080734944437e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.63; acc: 0.48
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.54; acc: 0.53
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.57; acc: 0.56
Batch: 160; loss: 1.49; acc: 0.53
Batch: 180; loss: 1.51; acc: 0.62
Batch: 200; loss: 1.47; acc: 0.66
Batch: 220; loss: 1.66; acc: 0.47
Batch: 240; loss: 1.5; acc: 0.56
Batch: 260; loss: 1.61; acc: 0.58
Batch: 280; loss: 1.66; acc: 0.53
Batch: 300; loss: 1.49; acc: 0.66
Batch: 320; loss: 1.57; acc: 0.5
Batch: 340; loss: 1.69; acc: 0.48
Batch: 360; loss: 1.57; acc: 0.48
Batch: 380; loss: 1.53; acc: 0.61
Batch: 400; loss: 1.56; acc: 0.58
Batch: 420; loss: 1.55; acc: 0.53
Batch: 440; loss: 1.53; acc: 0.59
Batch: 460; loss: 1.58; acc: 0.58
Batch: 480; loss: 1.52; acc: 0.62
Batch: 500; loss: 1.58; acc: 0.52
Batch: 520; loss: 1.71; acc: 0.47
Batch: 540; loss: 1.64; acc: 0.52
Batch: 560; loss: 1.55; acc: 0.56
Batch: 580; loss: 1.44; acc: 0.61
Batch: 600; loss: 1.66; acc: 0.48
Batch: 620; loss: 1.49; acc: 0.61
Batch: 640; loss: 1.64; acc: 0.5
Batch: 660; loss: 1.6; acc: 0.61
Batch: 680; loss: 1.43; acc: 0.62
Batch: 700; loss: 1.64; acc: 0.47
Batch: 720; loss: 1.47; acc: 0.58
Batch: 740; loss: 1.55; acc: 0.56
Batch: 760; loss: 1.55; acc: 0.53
Batch: 780; loss: 1.5; acc: 0.58
Train Epoch over. train_loss: 1.58; train_accuracy: 0.55 

3.777677193284035e-05
1.377853004669305e-05
Batch: 0; loss: 1.46; acc: 0.58
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.33; acc: 0.67
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.64
Batch: 100; loss: 1.47; acc: 0.66
Batch: 120; loss: 1.6; acc: 0.59
Batch: 140; loss: 1.46; acc: 0.55
Val Epoch over. val_loss: 1.5333851568258492; val_accuracy: 0.5747412420382165 

The current subspace-distance is: 1.377853004669305e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.47; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.42
Batch: 40; loss: 1.67; acc: 0.45
Batch: 60; loss: 1.64; acc: 0.48
Batch: 80; loss: 1.45; acc: 0.58
Batch: 100; loss: 1.53; acc: 0.59
Batch: 120; loss: 1.68; acc: 0.47
Batch: 140; loss: 1.62; acc: 0.52
Batch: 160; loss: 1.55; acc: 0.56
Batch: 180; loss: 1.79; acc: 0.38
Batch: 200; loss: 1.73; acc: 0.5
Batch: 220; loss: 1.58; acc: 0.56
Batch: 240; loss: 1.5; acc: 0.59
Batch: 260; loss: 1.47; acc: 0.67
Batch: 280; loss: 1.65; acc: 0.47
Batch: 300; loss: 1.67; acc: 0.53
Batch: 320; loss: 1.49; acc: 0.56
Batch: 340; loss: 1.7; acc: 0.47
Batch: 360; loss: 1.56; acc: 0.52
Batch: 380; loss: 1.61; acc: 0.5
Batch: 400; loss: 1.64; acc: 0.52
Batch: 420; loss: 1.52; acc: 0.64
Batch: 440; loss: 1.53; acc: 0.53
Batch: 460; loss: 1.48; acc: 0.64
Batch: 480; loss: 1.67; acc: 0.45
Batch: 500; loss: 1.68; acc: 0.48
Batch: 520; loss: 1.4; acc: 0.66
Batch: 540; loss: 1.7; acc: 0.55
Batch: 560; loss: 1.62; acc: 0.44
Batch: 580; loss: 1.44; acc: 0.72
Batch: 600; loss: 1.57; acc: 0.53
Batch: 620; loss: 1.61; acc: 0.56
Batch: 640; loss: 1.48; acc: 0.55
Batch: 660; loss: 1.55; acc: 0.48
Batch: 680; loss: 1.57; acc: 0.59
Batch: 700; loss: 1.61; acc: 0.47
Batch: 720; loss: 1.51; acc: 0.55
Batch: 740; loss: 1.67; acc: 0.47
Batch: 760; loss: 1.64; acc: 0.48
Batch: 780; loss: 1.51; acc: 0.55
Train Epoch over. train_loss: 1.57; train_accuracy: 0.54 

3.765022484003566e-05
1.3518856576411054e-05
Batch: 0; loss: 1.45; acc: 0.59
Batch: 20; loss: 1.73; acc: 0.47
Batch: 40; loss: 1.32; acc: 0.67
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.66
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.45; acc: 0.55
Val Epoch over. val_loss: 1.5242426926922645; val_accuracy: 0.5761345541401274 

The current subspace-distance is: 1.3518856576411054e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.53
Batch: 40; loss: 1.57; acc: 0.56
Batch: 60; loss: 1.53; acc: 0.48
Batch: 80; loss: 1.57; acc: 0.55
Batch: 100; loss: 1.54; acc: 0.55
Batch: 120; loss: 1.71; acc: 0.47
Batch: 140; loss: 1.64; acc: 0.48
Batch: 160; loss: 1.56; acc: 0.55
Batch: 180; loss: 1.43; acc: 0.69
Batch: 200; loss: 1.62; acc: 0.48
Batch: 220; loss: 1.64; acc: 0.5
Batch: 240; loss: 1.64; acc: 0.5
Batch: 260; loss: 1.57; acc: 0.53
Batch: 280; loss: 1.61; acc: 0.55
Batch: 300; loss: 1.59; acc: 0.53
Batch: 320; loss: 1.66; acc: 0.55
Batch: 340; loss: 1.56; acc: 0.56
Batch: 360; loss: 1.62; acc: 0.52
Batch: 380; loss: 1.48; acc: 0.62
Batch: 400; loss: 1.59; acc: 0.5
Batch: 420; loss: 1.58; acc: 0.48
Batch: 440; loss: 1.51; acc: 0.66
Batch: 460; loss: 1.5; acc: 0.61
Batch: 480; loss: 1.7; acc: 0.41
Batch: 500; loss: 1.72; acc: 0.5
Batch: 520; loss: 1.5; acc: 0.56
Batch: 540; loss: 1.52; acc: 0.58
Batch: 560; loss: 1.6; acc: 0.45
Batch: 580; loss: 1.52; acc: 0.5
Batch: 600; loss: 1.55; acc: 0.5
Batch: 620; loss: 1.53; acc: 0.48
Batch: 640; loss: 1.74; acc: 0.5
Batch: 660; loss: 1.65; acc: 0.47
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.56
Batch: 720; loss: 1.52; acc: 0.61
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.45; acc: 0.62
Train Epoch over. train_loss: 1.57; train_accuracy: 0.55 

3.8194713852135465e-05
1.4832515262241941e-05
Batch: 0; loss: 1.45; acc: 0.61
Batch: 20; loss: 1.74; acc: 0.44
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.46; acc: 0.66
Batch: 120; loss: 1.61; acc: 0.58
Batch: 140; loss: 1.45; acc: 0.55
Val Epoch over. val_loss: 1.5287502510532451; val_accuracy: 0.5775278662420382 

The current subspace-distance is: 1.4832515262241941e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.61
Batch: 40; loss: 1.54; acc: 0.56
Batch: 60; loss: 1.42; acc: 0.61
Batch: 80; loss: 1.49; acc: 0.55
Batch: 100; loss: 1.47; acc: 0.69
Batch: 120; loss: 1.56; acc: 0.48
Batch: 140; loss: 1.64; acc: 0.5
Batch: 160; loss: 1.67; acc: 0.52
Batch: 180; loss: 1.73; acc: 0.48
Batch: 200; loss: 1.5; acc: 0.58
Batch: 220; loss: 1.62; acc: 0.52
Batch: 240; loss: 1.62; acc: 0.53
Batch: 260; loss: 1.74; acc: 0.38
Batch: 280; loss: 1.56; acc: 0.58
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.48; acc: 0.55
Batch: 340; loss: 1.53; acc: 0.59
Batch: 360; loss: 1.59; acc: 0.56
Batch: 380; loss: 1.5; acc: 0.59
Batch: 400; loss: 1.5; acc: 0.56
Batch: 420; loss: 1.4; acc: 0.64
Batch: 440; loss: 1.46; acc: 0.62
Batch: 460; loss: 1.41; acc: 0.62
Batch: 480; loss: 1.29; acc: 0.67
Batch: 500; loss: 1.57; acc: 0.56
Batch: 520; loss: 1.54; acc: 0.59
Batch: 540; loss: 1.57; acc: 0.52
Batch: 560; loss: 1.74; acc: 0.42
Batch: 580; loss: 1.64; acc: 0.47
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.58; acc: 0.52
Batch: 640; loss: 1.55; acc: 0.5
Batch: 660; loss: 1.53; acc: 0.52
Batch: 680; loss: 1.59; acc: 0.48
Batch: 700; loss: 1.54; acc: 0.61
Batch: 720; loss: 1.61; acc: 0.5
Batch: 740; loss: 1.6; acc: 0.55
Batch: 760; loss: 1.52; acc: 0.53
Batch: 780; loss: 1.58; acc: 0.47
Train Epoch over. train_loss: 1.56; train_accuracy: 0.55 

3.9319736970355734e-05
1.4249565538193565e-05
Batch: 0; loss: 1.46; acc: 0.61
Batch: 20; loss: 1.73; acc: 0.47
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.6; acc: 0.55
Batch: 80; loss: 1.54; acc: 0.59
Batch: 100; loss: 1.46; acc: 0.69
Batch: 120; loss: 1.62; acc: 0.59
Batch: 140; loss: 1.46; acc: 0.53
Val Epoch over. val_loss: 1.534183603183479; val_accuracy: 0.5707603503184714 

The current subspace-distance is: 1.4249565538193565e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.55; acc: 0.58
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.59; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.53
Batch: 80; loss: 1.57; acc: 0.42
Batch: 100; loss: 1.65; acc: 0.53
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 1.49; acc: 0.56
Batch: 160; loss: 1.57; acc: 0.56
Batch: 180; loss: 1.59; acc: 0.48
Batch: 200; loss: 1.57; acc: 0.52
Batch: 220; loss: 1.46; acc: 0.56
Batch: 240; loss: 1.43; acc: 0.67
Batch: 260; loss: 1.58; acc: 0.52
Batch: 280; loss: 1.56; acc: 0.5
Batch: 300; loss: 1.43; acc: 0.61
Batch: 320; loss: 1.53; acc: 0.64
Batch: 340; loss: 1.6; acc: 0.5
Batch: 360; loss: 1.58; acc: 0.52
Batch: 380; loss: 1.62; acc: 0.55
Batch: 400; loss: 1.55; acc: 0.58
Batch: 420; loss: 1.52; acc: 0.58
Batch: 440; loss: 1.55; acc: 0.53
Batch: 460; loss: 1.55; acc: 0.55
Batch: 480; loss: 1.46; acc: 0.58
Batch: 500; loss: 1.6; acc: 0.48
Batch: 520; loss: 1.56; acc: 0.56
Batch: 540; loss: 1.53; acc: 0.5
Batch: 560; loss: 1.62; acc: 0.47
Batch: 580; loss: 1.58; acc: 0.61
Batch: 600; loss: 1.56; acc: 0.56
Batch: 620; loss: 1.53; acc: 0.56
Batch: 640; loss: 1.52; acc: 0.55
Batch: 660; loss: 1.62; acc: 0.52
Batch: 680; loss: 1.5; acc: 0.56
Batch: 700; loss: 1.41; acc: 0.62
Batch: 720; loss: 1.55; acc: 0.56
Batch: 740; loss: 1.53; acc: 0.58
Batch: 760; loss: 1.54; acc: 0.53
Batch: 780; loss: 1.37; acc: 0.64
Train Epoch over. train_loss: 1.56; train_accuracy: 0.55 

3.882718738168478e-05
1.344315296591958e-05
Batch: 0; loss: 1.45; acc: 0.62
Batch: 20; loss: 1.71; acc: 0.45
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.44; acc: 0.66
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.45; acc: 0.56
Val Epoch over. val_loss: 1.521185840770697; val_accuracy: 0.570859872611465 

The current subspace-distance is: 1.344315296591958e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.44
Batch: 40; loss: 1.63; acc: 0.52
Batch: 60; loss: 1.52; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.56
Batch: 100; loss: 1.62; acc: 0.56
Batch: 120; loss: 1.43; acc: 0.67
Batch: 140; loss: 1.39; acc: 0.64
Batch: 160; loss: 1.5; acc: 0.55
Batch: 180; loss: 1.52; acc: 0.52
Batch: 200; loss: 1.55; acc: 0.47
Batch: 220; loss: 1.81; acc: 0.42
Batch: 240; loss: 1.49; acc: 0.58
Batch: 260; loss: 1.52; acc: 0.55
Batch: 280; loss: 1.56; acc: 0.58
Batch: 300; loss: 1.49; acc: 0.59
Batch: 320; loss: 1.53; acc: 0.61
Batch: 340; loss: 1.57; acc: 0.56
Batch: 360; loss: 1.51; acc: 0.56
Batch: 380; loss: 1.63; acc: 0.47
Batch: 400; loss: 1.64; acc: 0.47
Batch: 420; loss: 1.56; acc: 0.52
Batch: 440; loss: 1.59; acc: 0.5
Batch: 460; loss: 1.57; acc: 0.59
Batch: 480; loss: 1.62; acc: 0.52
Batch: 500; loss: 1.6; acc: 0.53
Batch: 520; loss: 1.7; acc: 0.48
Batch: 540; loss: 1.49; acc: 0.64
Batch: 560; loss: 1.52; acc: 0.62
Batch: 580; loss: 1.46; acc: 0.59
Batch: 600; loss: 1.44; acc: 0.61
Batch: 620; loss: 1.48; acc: 0.56
Batch: 640; loss: 1.53; acc: 0.64
Batch: 660; loss: 1.54; acc: 0.53
Batch: 680; loss: 1.65; acc: 0.45
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.49; acc: 0.64
Batch: 740; loss: 1.49; acc: 0.64
Batch: 760; loss: 1.36; acc: 0.69
Batch: 780; loss: 1.62; acc: 0.44
Train Epoch over. train_loss: 1.55; train_accuracy: 0.55 

3.933883272111416e-05
1.393113507219823e-05
Batch: 0; loss: 1.44; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.45
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.44; acc: 0.64
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.56
Val Epoch over. val_loss: 1.516413060722837; val_accuracy: 0.572452229299363 

The current subspace-distance is: 1.393113507219823e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.56
Batch: 20; loss: 1.55; acc: 0.56
Batch: 40; loss: 1.57; acc: 0.56
Batch: 60; loss: 1.42; acc: 0.66
Batch: 80; loss: 1.52; acc: 0.62
Batch: 100; loss: 1.56; acc: 0.53
Batch: 120; loss: 1.51; acc: 0.53
Batch: 140; loss: 1.55; acc: 0.5
Batch: 160; loss: 1.58; acc: 0.55
Batch: 180; loss: 1.49; acc: 0.62
Batch: 200; loss: 1.7; acc: 0.5
Batch: 220; loss: 1.54; acc: 0.56
Batch: 240; loss: 1.5; acc: 0.62
Batch: 260; loss: 1.46; acc: 0.58
Batch: 280; loss: 1.6; acc: 0.59
Batch: 300; loss: 1.45; acc: 0.61
Batch: 320; loss: 1.63; acc: 0.55
Batch: 340; loss: 1.55; acc: 0.56
Batch: 360; loss: 1.48; acc: 0.58
Batch: 380; loss: 1.61; acc: 0.58
Batch: 400; loss: 1.66; acc: 0.39
Batch: 420; loss: 1.57; acc: 0.59
Batch: 440; loss: 1.55; acc: 0.53
Batch: 460; loss: 1.44; acc: 0.58
Batch: 480; loss: 1.69; acc: 0.44
Batch: 500; loss: 1.63; acc: 0.47
Batch: 520; loss: 1.56; acc: 0.53
Batch: 540; loss: 1.51; acc: 0.56
Batch: 560; loss: 1.57; acc: 0.53
Batch: 580; loss: 1.7; acc: 0.48
Batch: 600; loss: 1.38; acc: 0.75
Batch: 620; loss: 1.58; acc: 0.55
Batch: 640; loss: 1.41; acc: 0.66
Batch: 660; loss: 1.7; acc: 0.44
Batch: 680; loss: 1.5; acc: 0.55
Batch: 700; loss: 1.48; acc: 0.61
Batch: 720; loss: 1.4; acc: 0.66
Batch: 740; loss: 1.49; acc: 0.53
Batch: 760; loss: 1.62; acc: 0.47
Batch: 780; loss: 1.59; acc: 0.53
Train Epoch over. train_loss: 1.55; train_accuracy: 0.55 

3.917391222785227e-05
1.2950134987477213e-05
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.7; acc: 0.45
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.52; acc: 0.62
Batch: 100; loss: 1.43; acc: 0.66
Batch: 120; loss: 1.62; acc: 0.59
Batch: 140; loss: 1.43; acc: 0.56
Val Epoch over. val_loss: 1.5145419820858415; val_accuracy: 0.5687699044585988 

The current subspace-distance is: 1.2950134987477213e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.49; acc: 0.58
Batch: 20; loss: 1.57; acc: 0.5
Batch: 40; loss: 1.48; acc: 0.62
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.42; acc: 0.56
Batch: 100; loss: 1.76; acc: 0.42
Batch: 120; loss: 1.8; acc: 0.34
Batch: 140; loss: 1.59; acc: 0.52
Batch: 160; loss: 1.42; acc: 0.64
Batch: 180; loss: 1.47; acc: 0.58
Batch: 200; loss: 1.56; acc: 0.45
Batch: 220; loss: 1.55; acc: 0.53
Batch: 240; loss: 1.65; acc: 0.5
Batch: 260; loss: 1.6; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.53
Batch: 300; loss: 1.55; acc: 0.59
Batch: 320; loss: 1.56; acc: 0.56
Batch: 340; loss: 1.52; acc: 0.53
Batch: 360; loss: 1.68; acc: 0.48
Batch: 380; loss: 1.39; acc: 0.72
Batch: 400; loss: 1.47; acc: 0.64
Batch: 420; loss: 1.61; acc: 0.52
Batch: 440; loss: 1.61; acc: 0.47
Batch: 460; loss: 1.51; acc: 0.55
Batch: 480; loss: 1.59; acc: 0.55
Batch: 500; loss: 1.5; acc: 0.58
Batch: 520; loss: 1.55; acc: 0.5
Batch: 540; loss: 1.69; acc: 0.48
Batch: 560; loss: 1.43; acc: 0.62
Batch: 580; loss: 1.49; acc: 0.53
Batch: 600; loss: 1.62; acc: 0.52
Batch: 620; loss: 1.62; acc: 0.53
Batch: 640; loss: 1.58; acc: 0.52
Batch: 660; loss: 1.6; acc: 0.55
Batch: 680; loss: 1.59; acc: 0.61
Batch: 700; loss: 1.46; acc: 0.58
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.52; acc: 0.55
Batch: 760; loss: 1.52; acc: 0.61
Batch: 780; loss: 1.53; acc: 0.58
Train Epoch over. train_loss: 1.54; train_accuracy: 0.55 

4.004791844636202e-05
1.4808001651545055e-05
Batch: 0; loss: 1.43; acc: 0.58
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.32; acc: 0.64
Batch: 60; loss: 1.57; acc: 0.55
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.42; acc: 0.62
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.59
Val Epoch over. val_loss: 1.5016327359873778; val_accuracy: 0.5676751592356688 

The current subspace-distance is: 1.4808001651545055e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.49; acc: 0.52
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.5; acc: 0.58
Batch: 60; loss: 1.5; acc: 0.52
Batch: 80; loss: 1.5; acc: 0.53
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.51; acc: 0.56
Batch: 140; loss: 1.55; acc: 0.59
Batch: 160; loss: 1.71; acc: 0.47
Batch: 180; loss: 1.54; acc: 0.59
Batch: 200; loss: 1.37; acc: 0.66
Batch: 220; loss: 1.6; acc: 0.45
Batch: 240; loss: 1.5; acc: 0.56
Batch: 260; loss: 1.47; acc: 0.58
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.47; acc: 0.62
Batch: 320; loss: 1.44; acc: 0.59
Batch: 340; loss: 1.61; acc: 0.52
Batch: 360; loss: 1.39; acc: 0.58
Batch: 380; loss: 1.61; acc: 0.58
Batch: 400; loss: 1.46; acc: 0.56
Batch: 420; loss: 1.5; acc: 0.58
Batch: 440; loss: 1.52; acc: 0.58
Batch: 460; loss: 1.6; acc: 0.47
Batch: 480; loss: 1.44; acc: 0.58
Batch: 500; loss: 1.47; acc: 0.62
Batch: 520; loss: 1.53; acc: 0.61
Batch: 540; loss: 1.33; acc: 0.7
Batch: 560; loss: 1.4; acc: 0.59
Batch: 580; loss: 1.55; acc: 0.58
Batch: 600; loss: 1.55; acc: 0.58
Batch: 620; loss: 1.58; acc: 0.44
Batch: 640; loss: 1.48; acc: 0.55
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.57; acc: 0.52
Batch: 700; loss: 1.39; acc: 0.66
Batch: 720; loss: 1.49; acc: 0.59
Batch: 740; loss: 1.44; acc: 0.66
Batch: 760; loss: 1.58; acc: 0.5
Batch: 780; loss: 1.55; acc: 0.61
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.1306579078081995e-05
1.861232340161223e-05
Batch: 0; loss: 1.44; acc: 0.59
Batch: 20; loss: 1.68; acc: 0.45
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.43; acc: 0.66
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.56
Val Epoch over. val_loss: 1.506855128677028; val_accuracy: 0.5692675159235668 

The current subspace-distance is: 1.861232340161223e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.59
Batch: 20; loss: 1.57; acc: 0.52
Batch: 40; loss: 1.57; acc: 0.59
Batch: 60; loss: 1.5; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.48
Batch: 100; loss: 1.47; acc: 0.58
Batch: 120; loss: 1.46; acc: 0.58
Batch: 140; loss: 1.53; acc: 0.58
Batch: 160; loss: 1.45; acc: 0.56
Batch: 180; loss: 1.58; acc: 0.52
Batch: 200; loss: 1.58; acc: 0.45
Batch: 220; loss: 1.49; acc: 0.53
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 1.48; acc: 0.55
Batch: 280; loss: 1.64; acc: 0.5
Batch: 300; loss: 1.55; acc: 0.58
Batch: 320; loss: 1.56; acc: 0.48
Batch: 340; loss: 1.5; acc: 0.59
Batch: 360; loss: 1.47; acc: 0.58
Batch: 380; loss: 1.59; acc: 0.55
Batch: 400; loss: 1.67; acc: 0.48
Batch: 420; loss: 1.56; acc: 0.55
Batch: 440; loss: 1.54; acc: 0.55
Batch: 460; loss: 1.54; acc: 0.55
Batch: 480; loss: 1.58; acc: 0.5
Batch: 500; loss: 1.58; acc: 0.5
Batch: 520; loss: 1.54; acc: 0.56
Batch: 540; loss: 1.46; acc: 0.61
Batch: 560; loss: 1.47; acc: 0.61
Batch: 580; loss: 1.48; acc: 0.64
Batch: 600; loss: 1.43; acc: 0.62
Batch: 620; loss: 1.59; acc: 0.5
Batch: 640; loss: 1.53; acc: 0.59
Batch: 660; loss: 1.61; acc: 0.52
Batch: 680; loss: 1.45; acc: 0.62
Batch: 700; loss: 1.55; acc: 0.53
Batch: 720; loss: 1.53; acc: 0.52
Batch: 740; loss: 1.59; acc: 0.52
Batch: 760; loss: 1.52; acc: 0.56
Batch: 780; loss: 1.46; acc: 0.62
Train Epoch over. train_loss: 1.54; train_accuracy: 0.55 

4.096609336556867e-05
1.6121590306283906e-05
Batch: 0; loss: 1.43; acc: 0.58
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.57; acc: 0.5
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.64
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.61
Val Epoch over. val_loss: 1.5048306337587394; val_accuracy: 0.5657842356687898 

The current subspace-distance is: 1.6121590306283906e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.36; acc: 0.72
Batch: 20; loss: 1.56; acc: 0.5
Batch: 40; loss: 1.52; acc: 0.59
Batch: 60; loss: 1.54; acc: 0.58
Batch: 80; loss: 1.65; acc: 0.47
Batch: 100; loss: 1.51; acc: 0.55
Batch: 120; loss: 1.43; acc: 0.64
Batch: 140; loss: 1.54; acc: 0.59
Batch: 160; loss: 1.4; acc: 0.69
Batch: 180; loss: 1.58; acc: 0.5
Batch: 200; loss: 1.48; acc: 0.59
Batch: 220; loss: 1.54; acc: 0.53
Batch: 240; loss: 1.44; acc: 0.66
Batch: 260; loss: 1.58; acc: 0.5
Batch: 280; loss: 1.58; acc: 0.48
Batch: 300; loss: 1.57; acc: 0.52
Batch: 320; loss: 1.59; acc: 0.53
Batch: 340; loss: 1.55; acc: 0.48
Batch: 360; loss: 1.47; acc: 0.61
Batch: 380; loss: 1.65; acc: 0.44
Batch: 400; loss: 1.52; acc: 0.59
Batch: 420; loss: 1.5; acc: 0.53
Batch: 440; loss: 1.48; acc: 0.61
Batch: 460; loss: 1.51; acc: 0.56
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.63; acc: 0.42
Batch: 520; loss: 1.61; acc: 0.48
Batch: 540; loss: 1.43; acc: 0.59
Batch: 560; loss: 1.5; acc: 0.56
Batch: 580; loss: 1.5; acc: 0.59
Batch: 600; loss: 1.49; acc: 0.53
Batch: 620; loss: 1.59; acc: 0.42
Batch: 640; loss: 1.44; acc: 0.56
Batch: 660; loss: 1.48; acc: 0.55
Batch: 680; loss: 1.52; acc: 0.58
Batch: 700; loss: 1.66; acc: 0.44
Batch: 720; loss: 1.48; acc: 0.56
Batch: 740; loss: 1.69; acc: 0.47
Batch: 760; loss: 1.45; acc: 0.67
Batch: 780; loss: 1.42; acc: 0.62
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.0591865399619564e-05
1.5203097063931637e-05
Batch: 0; loss: 1.43; acc: 0.59
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.5
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.42; acc: 0.64
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.56
Val Epoch over. val_loss: 1.5040175686975954; val_accuracy: 0.5653861464968153 

The current subspace-distance is: 1.5203097063931637e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.5; acc: 0.59
Batch: 60; loss: 1.54; acc: 0.5
Batch: 80; loss: 1.63; acc: 0.44
Batch: 100; loss: 1.75; acc: 0.52
Batch: 120; loss: 1.41; acc: 0.64
Batch: 140; loss: 1.56; acc: 0.52
Batch: 160; loss: 1.55; acc: 0.53
Batch: 180; loss: 1.42; acc: 0.62
Batch: 200; loss: 1.47; acc: 0.55
Batch: 220; loss: 1.53; acc: 0.53
Batch: 240; loss: 1.54; acc: 0.5
Batch: 260; loss: 1.51; acc: 0.56
Batch: 280; loss: 1.64; acc: 0.41
Batch: 300; loss: 1.46; acc: 0.55
Batch: 320; loss: 1.66; acc: 0.42
Batch: 340; loss: 1.51; acc: 0.61
Batch: 360; loss: 1.51; acc: 0.55
Batch: 380; loss: 1.63; acc: 0.52
Batch: 400; loss: 1.46; acc: 0.58
Batch: 420; loss: 1.62; acc: 0.48
Batch: 440; loss: 1.46; acc: 0.62
Batch: 460; loss: 1.51; acc: 0.53
Batch: 480; loss: 1.42; acc: 0.59
Batch: 500; loss: 1.58; acc: 0.48
Batch: 520; loss: 1.53; acc: 0.58
Batch: 540; loss: 1.41; acc: 0.64
Batch: 560; loss: 1.58; acc: 0.59
Batch: 580; loss: 1.63; acc: 0.48
Batch: 600; loss: 1.49; acc: 0.59
Batch: 620; loss: 1.53; acc: 0.52
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.55; acc: 0.47
Batch: 680; loss: 1.51; acc: 0.55
Batch: 700; loss: 1.41; acc: 0.59
Batch: 720; loss: 1.46; acc: 0.64
Batch: 740; loss: 1.56; acc: 0.52
Batch: 760; loss: 1.69; acc: 0.45
Batch: 780; loss: 1.44; acc: 0.58
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.0804356103762984e-05
1.6372488971683197e-05
Batch: 0; loss: 1.44; acc: 0.59
Batch: 20; loss: 1.67; acc: 0.45
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.52
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.67
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.58
Val Epoch over. val_loss: 1.5048151851459672; val_accuracy: 0.5701632165605095 

The current subspace-distance is: 1.6372488971683197e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.66; acc: 0.41
Batch: 20; loss: 1.59; acc: 0.53
Batch: 40; loss: 1.54; acc: 0.5
Batch: 60; loss: 1.45; acc: 0.53
Batch: 80; loss: 1.59; acc: 0.5
Batch: 100; loss: 1.52; acc: 0.56
Batch: 120; loss: 1.62; acc: 0.44
Batch: 140; loss: 1.62; acc: 0.5
Batch: 160; loss: 1.66; acc: 0.44
Batch: 180; loss: 1.57; acc: 0.55
Batch: 200; loss: 1.59; acc: 0.58
Batch: 220; loss: 1.55; acc: 0.5
Batch: 240; loss: 1.68; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.48
Batch: 280; loss: 1.52; acc: 0.58
Batch: 300; loss: 1.47; acc: 0.58
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.69; acc: 0.42
Batch: 360; loss: 1.46; acc: 0.48
Batch: 380; loss: 1.54; acc: 0.56
Batch: 400; loss: 1.47; acc: 0.58
Batch: 420; loss: 1.66; acc: 0.44
Batch: 440; loss: 1.55; acc: 0.52
Batch: 460; loss: 1.58; acc: 0.55
Batch: 480; loss: 1.49; acc: 0.56
Batch: 500; loss: 1.47; acc: 0.58
Batch: 520; loss: 1.54; acc: 0.52
Batch: 540; loss: 1.47; acc: 0.64
Batch: 560; loss: 1.39; acc: 0.62
Batch: 580; loss: 1.52; acc: 0.55
Batch: 600; loss: 1.5; acc: 0.52
Batch: 620; loss: 1.53; acc: 0.52
Batch: 640; loss: 1.47; acc: 0.56
Batch: 660; loss: 1.56; acc: 0.53
Batch: 680; loss: 1.47; acc: 0.59
Batch: 700; loss: 1.47; acc: 0.55
Batch: 720; loss: 1.55; acc: 0.5
Batch: 740; loss: 1.66; acc: 0.55
Batch: 760; loss: 1.58; acc: 0.52
Batch: 780; loss: 1.48; acc: 0.56
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.0081984479911625e-05
1.4791247849643696e-05
Batch: 0; loss: 1.43; acc: 0.61
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.57; acc: 0.55
Batch: 80; loss: 1.5; acc: 0.61
Batch: 100; loss: 1.41; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.59
Batch: 140; loss: 1.41; acc: 0.61
Val Epoch over. val_loss: 1.5023406979384695; val_accuracy: 0.5672770700636943 

The current subspace-distance is: 1.4791247849643696e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.51; acc: 0.5
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.45; acc: 0.53
Batch: 60; loss: 1.37; acc: 0.67
Batch: 80; loss: 1.5; acc: 0.61
Batch: 100; loss: 1.49; acc: 0.56
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.54; acc: 0.52
Batch: 160; loss: 1.52; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.44
Batch: 200; loss: 1.46; acc: 0.61
Batch: 220; loss: 1.45; acc: 0.58
Batch: 240; loss: 1.33; acc: 0.62
Batch: 260; loss: 1.55; acc: 0.56
Batch: 280; loss: 1.38; acc: 0.62
Batch: 300; loss: 1.55; acc: 0.47
Batch: 320; loss: 1.42; acc: 0.56
Batch: 340; loss: 1.72; acc: 0.44
Batch: 360; loss: 1.54; acc: 0.53
Batch: 380; loss: 1.5; acc: 0.53
Batch: 400; loss: 1.69; acc: 0.42
Batch: 420; loss: 1.53; acc: 0.53
Batch: 440; loss: 1.67; acc: 0.5
Batch: 460; loss: 1.47; acc: 0.66
Batch: 480; loss: 1.68; acc: 0.47
Batch: 500; loss: 1.64; acc: 0.42
Batch: 520; loss: 1.59; acc: 0.52
Batch: 540; loss: 1.41; acc: 0.52
Batch: 560; loss: 1.58; acc: 0.5
Batch: 580; loss: 1.52; acc: 0.53
Batch: 600; loss: 1.48; acc: 0.58
Batch: 620; loss: 1.48; acc: 0.58
Batch: 640; loss: 1.53; acc: 0.5
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.6; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.5
Batch: 720; loss: 1.64; acc: 0.5
Batch: 740; loss: 1.48; acc: 0.55
Batch: 760; loss: 1.49; acc: 0.47
Batch: 780; loss: 1.57; acc: 0.58
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.217149762553163e-05
1.8648692275746725e-05
Batch: 0; loss: 1.43; acc: 0.61
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.55
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.42; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.59
Val Epoch over. val_loss: 1.503392247637366; val_accuracy: 0.5633957006369427 

The current subspace-distance is: 1.8648692275746725e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.42; acc: 0.66
Batch: 40; loss: 1.48; acc: 0.61
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.55; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.42
Batch: 120; loss: 1.58; acc: 0.44
Batch: 140; loss: 1.61; acc: 0.48
Batch: 160; loss: 1.46; acc: 0.58
Batch: 180; loss: 1.46; acc: 0.55
Batch: 200; loss: 1.64; acc: 0.47
Batch: 220; loss: 1.67; acc: 0.42
Batch: 240; loss: 1.54; acc: 0.58
Batch: 260; loss: 1.51; acc: 0.61
Batch: 280; loss: 1.48; acc: 0.56
Batch: 300; loss: 1.58; acc: 0.55
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.53; acc: 0.55
Batch: 360; loss: 1.51; acc: 0.55
Batch: 380; loss: 1.49; acc: 0.56
Batch: 400; loss: 1.56; acc: 0.47
Batch: 420; loss: 1.77; acc: 0.45
Batch: 440; loss: 1.55; acc: 0.55
Batch: 460; loss: 1.4; acc: 0.61
Batch: 480; loss: 1.51; acc: 0.53
Batch: 500; loss: 1.58; acc: 0.53
Batch: 520; loss: 1.49; acc: 0.56
Batch: 540; loss: 1.43; acc: 0.62
Batch: 560; loss: 1.65; acc: 0.47
Batch: 580; loss: 1.53; acc: 0.58
Batch: 600; loss: 1.56; acc: 0.48
Batch: 620; loss: 1.48; acc: 0.56
Batch: 640; loss: 1.67; acc: 0.48
Batch: 660; loss: 1.58; acc: 0.5
Batch: 680; loss: 1.42; acc: 0.62
Batch: 700; loss: 1.46; acc: 0.59
Batch: 720; loss: 1.36; acc: 0.66
Batch: 740; loss: 1.52; acc: 0.5
Batch: 760; loss: 1.48; acc: 0.56
Batch: 780; loss: 1.53; acc: 0.52
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.051932774018496e-05
1.5411915228469297e-05
Batch: 0; loss: 1.42; acc: 0.59
Batch: 20; loss: 1.66; acc: 0.45
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.58; acc: 0.52
Batch: 80; loss: 1.51; acc: 0.58
Batch: 100; loss: 1.41; acc: 0.67
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.58
Val Epoch over. val_loss: 1.497257269112168; val_accuracy: 0.5640923566878981 

The current subspace-distance is: 1.5411915228469297e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.47
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 1.5; acc: 0.58
Batch: 60; loss: 1.4; acc: 0.61
Batch: 80; loss: 1.61; acc: 0.52
Batch: 100; loss: 1.61; acc: 0.42
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.63; acc: 0.45
Batch: 160; loss: 1.6; acc: 0.47
Batch: 180; loss: 1.49; acc: 0.61
Batch: 200; loss: 1.53; acc: 0.55
Batch: 220; loss: 1.58; acc: 0.53
Batch: 240; loss: 1.54; acc: 0.5
Batch: 260; loss: 1.54; acc: 0.53
Batch: 280; loss: 1.53; acc: 0.59
Batch: 300; loss: 1.65; acc: 0.47
Batch: 320; loss: 1.37; acc: 0.66
Batch: 340; loss: 1.6; acc: 0.48
Batch: 360; loss: 1.55; acc: 0.56
Batch: 380; loss: 1.49; acc: 0.56
Batch: 400; loss: 1.62; acc: 0.52
Batch: 420; loss: 1.56; acc: 0.52
Batch: 440; loss: 1.6; acc: 0.55
Batch: 460; loss: 1.47; acc: 0.64
Batch: 480; loss: 1.55; acc: 0.52
Batch: 500; loss: 1.55; acc: 0.52
Batch: 520; loss: 1.71; acc: 0.5
Batch: 540; loss: 1.49; acc: 0.58
Batch: 560; loss: 1.61; acc: 0.47
Batch: 580; loss: 1.66; acc: 0.47
Batch: 600; loss: 1.52; acc: 0.62
Batch: 620; loss: 1.43; acc: 0.55
Batch: 640; loss: 1.55; acc: 0.52
Batch: 660; loss: 1.6; acc: 0.52
Batch: 680; loss: 1.55; acc: 0.52
Batch: 700; loss: 1.54; acc: 0.53
Batch: 720; loss: 1.67; acc: 0.48
Batch: 740; loss: 1.45; acc: 0.62
Batch: 760; loss: 1.34; acc: 0.62
Batch: 780; loss: 1.53; acc: 0.59
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.074671960552223e-05
1.1772260222642217e-05
Batch: 0; loss: 1.43; acc: 0.59
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.41; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.61
Val Epoch over. val_loss: 1.500803138799728; val_accuracy: 0.5638933121019108 

The current subspace-distance is: 1.1772260222642217e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.66; acc: 0.5
Batch: 20; loss: 1.46; acc: 0.61
Batch: 40; loss: 1.61; acc: 0.52
Batch: 60; loss: 1.3; acc: 0.7
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.61
Batch: 160; loss: 1.5; acc: 0.58
Batch: 180; loss: 1.66; acc: 0.48
Batch: 200; loss: 1.47; acc: 0.53
Batch: 220; loss: 1.58; acc: 0.52
Batch: 240; loss: 1.51; acc: 0.55
Batch: 260; loss: 1.47; acc: 0.55
Batch: 280; loss: 1.59; acc: 0.42
Batch: 300; loss: 1.58; acc: 0.56
Batch: 320; loss: 1.51; acc: 0.55
Batch: 340; loss: 1.59; acc: 0.48
Batch: 360; loss: 1.55; acc: 0.5
Batch: 380; loss: 1.41; acc: 0.62
Batch: 400; loss: 1.45; acc: 0.59
Batch: 420; loss: 1.52; acc: 0.58
Batch: 440; loss: 1.47; acc: 0.55
Batch: 460; loss: 1.66; acc: 0.45
Batch: 480; loss: 1.65; acc: 0.47
Batch: 500; loss: 1.53; acc: 0.5
Batch: 520; loss: 1.56; acc: 0.52
Batch: 540; loss: 1.54; acc: 0.58
Batch: 560; loss: 1.67; acc: 0.42
Batch: 580; loss: 1.63; acc: 0.42
Batch: 600; loss: 1.63; acc: 0.45
Batch: 620; loss: 1.52; acc: 0.52
Batch: 640; loss: 1.53; acc: 0.52
Batch: 660; loss: 1.45; acc: 0.52
Batch: 680; loss: 1.33; acc: 0.64
Batch: 700; loss: 1.53; acc: 0.5
Batch: 720; loss: 1.42; acc: 0.61
Batch: 740; loss: 1.75; acc: 0.45
Batch: 760; loss: 1.53; acc: 0.5
Batch: 780; loss: 1.63; acc: 0.48
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.142821489949711e-05
1.4007096979185008e-05
Batch: 0; loss: 1.42; acc: 0.61
Batch: 20; loss: 1.65; acc: 0.44
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.5; acc: 0.59
Batch: 100; loss: 1.4; acc: 0.66
Batch: 120; loss: 1.62; acc: 0.59
Batch: 140; loss: 1.4; acc: 0.61
Val Epoch over. val_loss: 1.493572304962547; val_accuracy: 0.5687699044585988 

The current subspace-distance is: 1.4007096979185008e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.52; acc: 0.55
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 1.47; acc: 0.62
Batch: 80; loss: 1.67; acc: 0.5
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.62; acc: 0.53
Batch: 160; loss: 1.56; acc: 0.53
Batch: 180; loss: 1.48; acc: 0.59
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.52; acc: 0.58
Batch: 240; loss: 1.46; acc: 0.52
Batch: 260; loss: 1.64; acc: 0.47
Batch: 280; loss: 1.46; acc: 0.61
Batch: 300; loss: 1.55; acc: 0.55
Batch: 320; loss: 1.49; acc: 0.61
Batch: 340; loss: 1.67; acc: 0.41
Batch: 360; loss: 1.46; acc: 0.55
Batch: 380; loss: 1.49; acc: 0.61
Batch: 400; loss: 1.45; acc: 0.61
Batch: 420; loss: 1.58; acc: 0.52
Batch: 440; loss: 1.44; acc: 0.58
Batch: 460; loss: 1.64; acc: 0.52
Batch: 480; loss: 1.47; acc: 0.58
Batch: 500; loss: 1.52; acc: 0.59
Batch: 520; loss: 1.62; acc: 0.45
Batch: 540; loss: 1.48; acc: 0.67
Batch: 560; loss: 1.4; acc: 0.64
Batch: 580; loss: 1.45; acc: 0.59
Batch: 600; loss: 1.71; acc: 0.45
Batch: 620; loss: 1.42; acc: 0.56
Batch: 640; loss: 1.4; acc: 0.64
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.52; acc: 0.5
Batch: 700; loss: 1.42; acc: 0.56
Batch: 720; loss: 1.59; acc: 0.48
Batch: 740; loss: 1.59; acc: 0.45
Batch: 760; loss: 1.57; acc: 0.53
Batch: 780; loss: 1.5; acc: 0.53
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.2111070797545835e-05
1.8369013560004532e-05
Batch: 0; loss: 1.43; acc: 0.59
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.5; acc: 0.59
Batch: 100; loss: 1.41; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.58
Val Epoch over. val_loss: 1.4999539662318624; val_accuracy: 0.5621019108280255 

The current subspace-distance is: 1.8369013560004532e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.39; acc: 0.59
Batch: 20; loss: 1.51; acc: 0.59
Batch: 40; loss: 1.6; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.47
Batch: 80; loss: 1.57; acc: 0.5
Batch: 100; loss: 1.49; acc: 0.55
Batch: 120; loss: 1.68; acc: 0.42
Batch: 140; loss: 1.45; acc: 0.61
Batch: 160; loss: 1.47; acc: 0.58
Batch: 180; loss: 1.6; acc: 0.58
Batch: 200; loss: 1.57; acc: 0.53
Batch: 220; loss: 1.6; acc: 0.58
Batch: 240; loss: 1.58; acc: 0.45
Batch: 260; loss: 1.65; acc: 0.47
Batch: 280; loss: 1.49; acc: 0.56
Batch: 300; loss: 1.53; acc: 0.55
Batch: 320; loss: 1.3; acc: 0.67
Batch: 340; loss: 1.48; acc: 0.5
Batch: 360; loss: 1.53; acc: 0.53
Batch: 380; loss: 1.42; acc: 0.59
Batch: 400; loss: 1.56; acc: 0.5
Batch: 420; loss: 1.55; acc: 0.53
Batch: 440; loss: 1.52; acc: 0.55
Batch: 460; loss: 1.7; acc: 0.42
Batch: 480; loss: 1.61; acc: 0.52
Batch: 500; loss: 1.56; acc: 0.53
Batch: 520; loss: 1.49; acc: 0.55
Batch: 540; loss: 1.61; acc: 0.5
Batch: 560; loss: 1.47; acc: 0.59
Batch: 580; loss: 1.51; acc: 0.52
Batch: 600; loss: 1.53; acc: 0.5
Batch: 620; loss: 1.7; acc: 0.41
Batch: 640; loss: 1.54; acc: 0.56
Batch: 660; loss: 1.48; acc: 0.59
Batch: 680; loss: 1.58; acc: 0.47
Batch: 700; loss: 1.53; acc: 0.55
Batch: 720; loss: 1.47; acc: 0.64
Batch: 740; loss: 1.47; acc: 0.56
Batch: 760; loss: 1.44; acc: 0.61
Batch: 780; loss: 1.63; acc: 0.5
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.139574593864381e-05
1.6276542737614363e-05
Batch: 0; loss: 1.44; acc: 0.58
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.58; acc: 0.5
Batch: 80; loss: 1.51; acc: 0.56
Batch: 100; loss: 1.43; acc: 0.67
Batch: 120; loss: 1.64; acc: 0.59
Batch: 140; loss: 1.41; acc: 0.58
Val Epoch over. val_loss: 1.5071563682738383; val_accuracy: 0.5625 

The current subspace-distance is: 1.6276542737614363e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.42; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.59
Batch: 40; loss: 1.43; acc: 0.59
Batch: 60; loss: 1.38; acc: 0.59
Batch: 80; loss: 1.64; acc: 0.53
Batch: 100; loss: 1.56; acc: 0.53
Batch: 120; loss: 1.43; acc: 0.59
Batch: 140; loss: 1.47; acc: 0.53
Batch: 160; loss: 1.34; acc: 0.66
Batch: 180; loss: 1.6; acc: 0.55
Batch: 200; loss: 1.57; acc: 0.56
Batch: 220; loss: 1.66; acc: 0.5
Batch: 240; loss: 1.58; acc: 0.48
Batch: 260; loss: 1.55; acc: 0.5
Batch: 280; loss: 1.64; acc: 0.48
Batch: 300; loss: 1.58; acc: 0.5
Batch: 320; loss: 1.65; acc: 0.42
Batch: 340; loss: 1.57; acc: 0.55
Batch: 360; loss: 1.52; acc: 0.55
Batch: 380; loss: 1.57; acc: 0.5
Batch: 400; loss: 1.54; acc: 0.5
Batch: 420; loss: 1.48; acc: 0.58
Batch: 440; loss: 1.51; acc: 0.55
Batch: 460; loss: 1.66; acc: 0.44
Batch: 480; loss: 1.42; acc: 0.56
Batch: 500; loss: 1.56; acc: 0.48
Batch: 520; loss: 1.62; acc: 0.53
Batch: 540; loss: 1.48; acc: 0.53
Batch: 560; loss: 1.52; acc: 0.59
Batch: 580; loss: 1.48; acc: 0.48
Batch: 600; loss: 1.56; acc: 0.53
Batch: 620; loss: 1.46; acc: 0.53
Batch: 640; loss: 1.53; acc: 0.55
Batch: 660; loss: 1.71; acc: 0.38
Batch: 680; loss: 1.61; acc: 0.45
Batch: 700; loss: 1.49; acc: 0.52
Batch: 720; loss: 1.69; acc: 0.45
Batch: 740; loss: 1.46; acc: 0.62
Batch: 760; loss: 1.46; acc: 0.64
Batch: 780; loss: 1.47; acc: 0.58
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.066515975864604e-05
1.515510848548729e-05
Batch: 0; loss: 1.42; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.58; acc: 0.52
Batch: 80; loss: 1.51; acc: 0.56
Batch: 100; loss: 1.41; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.61
Val Epoch over. val_loss: 1.4951953614593312; val_accuracy: 0.564390923566879 

The current subspace-distance is: 1.515510848548729e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_6_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.05

The number of parameters is: 275012

The number of individual parameters is:

17
306
17
17
25
39100
25
25
50
115000
50
50
64
115200
64
64
4096
64
640
10
64
64

nonzero elements in E: 27501197
elements in E: 27501200
fraction nonzero: 0.999999890913851
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.09
Batch: 20; loss: 2.34; acc: 0.09
Batch: 40; loss: 2.13; acc: 0.22
Batch: 60; loss: 2.12; acc: 0.23
Batch: 80; loss: 2.02; acc: 0.27
Batch: 100; loss: 2.03; acc: 0.33
Batch: 120; loss: 1.96; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.33
Batch: 160; loss: 1.94; acc: 0.39
Batch: 180; loss: 1.86; acc: 0.47
Batch: 200; loss: 1.8; acc: 0.53
Batch: 220; loss: 1.88; acc: 0.45
Batch: 240; loss: 1.85; acc: 0.45
Batch: 260; loss: 1.87; acc: 0.45
Batch: 280; loss: 1.89; acc: 0.42
Batch: 300; loss: 1.78; acc: 0.45
Batch: 320; loss: 1.72; acc: 0.59
Batch: 340; loss: 1.76; acc: 0.5
Batch: 360; loss: 1.81; acc: 0.45
Batch: 380; loss: 1.62; acc: 0.67
Batch: 400; loss: 1.77; acc: 0.45
Batch: 420; loss: 1.65; acc: 0.59
Batch: 440; loss: 1.66; acc: 0.61
Batch: 460; loss: 1.69; acc: 0.52
Batch: 480; loss: 1.68; acc: 0.48
Batch: 500; loss: 1.69; acc: 0.58
Batch: 520; loss: 1.67; acc: 0.5
Batch: 540; loss: 1.73; acc: 0.53
Batch: 560; loss: 1.63; acc: 0.61
Batch: 580; loss: 1.6; acc: 0.62
Batch: 600; loss: 1.72; acc: 0.59
Batch: 620; loss: 1.57; acc: 0.61
Batch: 640; loss: 1.68; acc: 0.64
Batch: 660; loss: 1.51; acc: 0.69
Batch: 680; loss: 1.65; acc: 0.53
Batch: 700; loss: 1.72; acc: 0.5
Batch: 720; loss: 1.66; acc: 0.56
Batch: 740; loss: 1.62; acc: 0.56
Batch: 760; loss: 1.63; acc: 0.56
Batch: 780; loss: 1.58; acc: 0.55
Train Epoch over. train_loss: 1.79; train_accuracy: 0.49 

5.340619827620685e-05
4.758388240588829e-05
Batch: 0; loss: 1.57; acc: 0.62
Batch: 20; loss: 1.72; acc: 0.53
Batch: 40; loss: 1.43; acc: 0.72
Batch: 60; loss: 1.52; acc: 0.67
Batch: 80; loss: 1.48; acc: 0.69
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.7; acc: 0.55
Batch: 140; loss: 1.62; acc: 0.56
Val Epoch over. val_loss: 1.5869726253922578; val_accuracy: 0.6200238853503185 

The current subspace-distance is: 4.758388240588829e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.55; acc: 0.61
Batch: 20; loss: 1.54; acc: 0.66
Batch: 40; loss: 1.63; acc: 0.55
Batch: 60; loss: 1.49; acc: 0.64
Batch: 80; loss: 1.57; acc: 0.66
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.53; acc: 0.64
Batch: 140; loss: 1.59; acc: 0.59
Batch: 160; loss: 1.48; acc: 0.69
Batch: 180; loss: 1.75; acc: 0.47
Batch: 200; loss: 1.58; acc: 0.59
Batch: 220; loss: 1.62; acc: 0.53
Batch: 240; loss: 1.49; acc: 0.75
Batch: 260; loss: 1.57; acc: 0.56
Batch: 280; loss: 1.52; acc: 0.66
Batch: 300; loss: 1.45; acc: 0.67
Batch: 320; loss: 1.67; acc: 0.53
Batch: 340; loss: 1.59; acc: 0.62
Batch: 360; loss: 1.51; acc: 0.67
Batch: 380; loss: 1.5; acc: 0.72
Batch: 400; loss: 1.45; acc: 0.66
Batch: 420; loss: 1.52; acc: 0.69
Batch: 440; loss: 1.55; acc: 0.61
Batch: 460; loss: 1.62; acc: 0.56
Batch: 480; loss: 1.49; acc: 0.7
Batch: 500; loss: 1.58; acc: 0.61
Batch: 520; loss: 1.6; acc: 0.56
Batch: 540; loss: 1.57; acc: 0.58
Batch: 560; loss: 1.47; acc: 0.69
Batch: 580; loss: 1.53; acc: 0.62
Batch: 600; loss: 1.41; acc: 0.73
Batch: 620; loss: 1.52; acc: 0.58
Batch: 640; loss: 1.48; acc: 0.72
Batch: 660; loss: 1.57; acc: 0.59
Batch: 680; loss: 1.58; acc: 0.58
Batch: 700; loss: 1.49; acc: 0.67
Batch: 720; loss: 1.61; acc: 0.61
Batch: 740; loss: 1.53; acc: 0.64
Batch: 760; loss: 1.47; acc: 0.72
Batch: 780; loss: 1.63; acc: 0.59
Train Epoch over. train_loss: 1.55; train_accuracy: 0.62 

7.09048326825723e-05
6.529570237034932e-05
Batch: 0; loss: 1.5; acc: 0.58
Batch: 20; loss: 1.6; acc: 0.56
Batch: 40; loss: 1.3; acc: 0.7
Batch: 60; loss: 1.38; acc: 0.75
Batch: 80; loss: 1.33; acc: 0.7
Batch: 100; loss: 1.43; acc: 0.72
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.48; acc: 0.62
Val Epoch over. val_loss: 1.474822683698812; val_accuracy: 0.65734474522293 

The current subspace-distance is: 6.529570237034932e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.81
Batch: 20; loss: 1.47; acc: 0.7
Batch: 40; loss: 1.53; acc: 0.64
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.5; acc: 0.59
Batch: 120; loss: 1.4; acc: 0.69
Batch: 140; loss: 1.42; acc: 0.7
Batch: 160; loss: 1.49; acc: 0.62
Batch: 180; loss: 1.5; acc: 0.61
Batch: 200; loss: 1.52; acc: 0.58
Batch: 220; loss: 1.44; acc: 0.73
Batch: 240; loss: 1.59; acc: 0.64
Batch: 260; loss: 1.54; acc: 0.56
Batch: 280; loss: 1.4; acc: 0.69
Batch: 300; loss: 1.36; acc: 0.73
Batch: 320; loss: 1.42; acc: 0.61
Batch: 340; loss: 1.51; acc: 0.58
Batch: 360; loss: 1.38; acc: 0.75
Batch: 380; loss: 1.58; acc: 0.55
Batch: 400; loss: 1.42; acc: 0.69
Batch: 420; loss: 1.51; acc: 0.66
Batch: 440; loss: 1.41; acc: 0.66
Batch: 460; loss: 1.39; acc: 0.72
Batch: 480; loss: 1.57; acc: 0.58
Batch: 500; loss: 1.46; acc: 0.66
Batch: 520; loss: 1.46; acc: 0.7
Batch: 540; loss: 1.57; acc: 0.53
Batch: 560; loss: 1.63; acc: 0.59
Batch: 580; loss: 1.42; acc: 0.69
Batch: 600; loss: 1.45; acc: 0.62
Batch: 620; loss: 1.35; acc: 0.72
Batch: 640; loss: 1.41; acc: 0.69
Batch: 660; loss: 1.46; acc: 0.67
Batch: 680; loss: 1.28; acc: 0.73
Batch: 700; loss: 1.35; acc: 0.72
Batch: 720; loss: 1.39; acc: 0.7
Batch: 740; loss: 1.52; acc: 0.61
Batch: 760; loss: 1.33; acc: 0.8
Batch: 780; loss: 1.55; acc: 0.59
Train Epoch over. train_loss: 1.46; train_accuracy: 0.66 

8.607473137089983e-05
8.074143261183053e-05
Batch: 0; loss: 1.44; acc: 0.59
Batch: 20; loss: 1.52; acc: 0.56
Batch: 40; loss: 1.25; acc: 0.72
Batch: 60; loss: 1.28; acc: 0.77
Batch: 80; loss: 1.24; acc: 0.75
Batch: 100; loss: 1.32; acc: 0.77
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.4; acc: 0.67
Val Epoch over. val_loss: 1.3949816242145125; val_accuracy: 0.6870023885350318 

The current subspace-distance is: 8.074143261183053e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.64
Batch: 20; loss: 1.44; acc: 0.64
Batch: 40; loss: 1.44; acc: 0.61
Batch: 60; loss: 1.35; acc: 0.75
Batch: 80; loss: 1.35; acc: 0.73
Batch: 100; loss: 1.49; acc: 0.58
Batch: 120; loss: 1.37; acc: 0.7
Batch: 140; loss: 1.41; acc: 0.66
Batch: 160; loss: 1.46; acc: 0.58
Batch: 180; loss: 1.39; acc: 0.75
Batch: 200; loss: 1.49; acc: 0.59
Batch: 220; loss: 1.39; acc: 0.67
Batch: 240; loss: 1.38; acc: 0.73
Batch: 260; loss: 1.39; acc: 0.69
Batch: 280; loss: 1.5; acc: 0.58
Batch: 300; loss: 1.37; acc: 0.72
Batch: 320; loss: 1.46; acc: 0.69
Batch: 340; loss: 1.26; acc: 0.77
Batch: 360; loss: 1.32; acc: 0.8
Batch: 380; loss: 1.48; acc: 0.67
Batch: 400; loss: 1.36; acc: 0.7
Batch: 420; loss: 1.41; acc: 0.61
Batch: 440; loss: 1.41; acc: 0.59
Batch: 460; loss: 1.33; acc: 0.69
Batch: 480; loss: 1.47; acc: 0.53
Batch: 500; loss: 1.41; acc: 0.64
Batch: 520; loss: 1.33; acc: 0.67
Batch: 540; loss: 1.24; acc: 0.8
Batch: 560; loss: 1.22; acc: 0.8
Batch: 580; loss: 1.32; acc: 0.75
Batch: 600; loss: 1.36; acc: 0.69
Batch: 620; loss: 1.35; acc: 0.64
Batch: 640; loss: 1.38; acc: 0.69
Batch: 660; loss: 1.44; acc: 0.62
Batch: 680; loss: 1.51; acc: 0.58
Batch: 700; loss: 1.33; acc: 0.75
Batch: 720; loss: 1.39; acc: 0.67
Batch: 740; loss: 1.57; acc: 0.48
Batch: 760; loss: 1.31; acc: 0.73
Batch: 780; loss: 1.38; acc: 0.75
Train Epoch over. train_loss: 1.38; train_accuracy: 0.68 

0.00010458835458848625
9.876170224742964e-05
Batch: 0; loss: 1.35; acc: 0.61
Batch: 20; loss: 1.46; acc: 0.59
Batch: 40; loss: 1.17; acc: 0.75
Batch: 60; loss: 1.18; acc: 0.81
Batch: 80; loss: 1.18; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.8
Batch: 120; loss: 1.46; acc: 0.55
Batch: 140; loss: 1.33; acc: 0.64
Val Epoch over. val_loss: 1.3104501203366905; val_accuracy: 0.7101910828025477 

The current subspace-distance is: 9.876170224742964e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.64
Batch: 20; loss: 1.3; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.77
Batch: 60; loss: 1.24; acc: 0.75
Batch: 80; loss: 1.29; acc: 0.75
Batch: 100; loss: 1.31; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.58
Batch: 160; loss: 1.27; acc: 0.75
Batch: 180; loss: 1.41; acc: 0.62
Batch: 200; loss: 1.55; acc: 0.5
Batch: 220; loss: 1.38; acc: 0.7
Batch: 240; loss: 1.31; acc: 0.7
Batch: 260; loss: 1.35; acc: 0.69
Batch: 280; loss: 1.23; acc: 0.77
Batch: 300; loss: 1.21; acc: 0.77
Batch: 320; loss: 1.19; acc: 0.8
Batch: 340; loss: 1.43; acc: 0.66
Batch: 360; loss: 1.39; acc: 0.61
Batch: 380; loss: 1.28; acc: 0.7
Batch: 400; loss: 1.37; acc: 0.64
Batch: 420; loss: 1.25; acc: 0.7
Batch: 440; loss: 1.4; acc: 0.62
Batch: 460; loss: 1.22; acc: 0.75
Batch: 480; loss: 1.23; acc: 0.77
Batch: 500; loss: 1.41; acc: 0.64
Batch: 520; loss: 1.24; acc: 0.69
Batch: 540; loss: 1.25; acc: 0.67
Batch: 560; loss: 1.22; acc: 0.77
Batch: 580; loss: 1.31; acc: 0.7
Batch: 600; loss: 1.45; acc: 0.53
Batch: 620; loss: 1.28; acc: 0.67
Batch: 640; loss: 1.41; acc: 0.62
Batch: 660; loss: 1.26; acc: 0.72
Batch: 680; loss: 1.32; acc: 0.67
Batch: 700; loss: 1.29; acc: 0.67
Batch: 720; loss: 1.21; acc: 0.72
Batch: 740; loss: 1.27; acc: 0.69
Batch: 760; loss: 1.29; acc: 0.7
Batch: 780; loss: 1.23; acc: 0.7
Train Epoch over. train_loss: 1.32; train_accuracy: 0.68 

0.00011760562483686954
0.00011166879994561896
Batch: 0; loss: 1.28; acc: 0.66
Batch: 20; loss: 1.4; acc: 0.64
Batch: 40; loss: 1.09; acc: 0.78
Batch: 60; loss: 1.12; acc: 0.83
Batch: 80; loss: 1.14; acc: 0.73
Batch: 100; loss: 1.18; acc: 0.78
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.23; acc: 0.64
Val Epoch over. val_loss: 1.2459370865943327; val_accuracy: 0.7152667197452229 

The current subspace-distance is: 0.00011166879994561896 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.34; acc: 0.62
Batch: 60; loss: 1.12; acc: 0.84
Batch: 80; loss: 1.36; acc: 0.69
Batch: 100; loss: 1.36; acc: 0.67
Batch: 120; loss: 1.37; acc: 0.64
Batch: 140; loss: 1.18; acc: 0.8
Batch: 160; loss: 1.23; acc: 0.73
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.29; acc: 0.67
Batch: 220; loss: 1.31; acc: 0.67
Batch: 240; loss: 1.36; acc: 0.64
Batch: 260; loss: 1.19; acc: 0.75
Batch: 280; loss: 1.3; acc: 0.69
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.35; acc: 0.58
Batch: 340; loss: 1.12; acc: 0.73
Batch: 360; loss: 1.18; acc: 0.75
Batch: 380; loss: 1.13; acc: 0.8
Batch: 400; loss: 1.26; acc: 0.69
Batch: 420; loss: 1.3; acc: 0.69
Batch: 440; loss: 1.3; acc: 0.67
Batch: 460; loss: 1.24; acc: 0.73
Batch: 480; loss: 1.21; acc: 0.69
Batch: 500; loss: 1.2; acc: 0.75
Batch: 520; loss: 1.21; acc: 0.7
Batch: 540; loss: 1.27; acc: 0.69
Batch: 560; loss: 1.27; acc: 0.7
Batch: 580; loss: 1.27; acc: 0.69
Batch: 600; loss: 1.19; acc: 0.8
Batch: 620; loss: 1.34; acc: 0.64
Batch: 640; loss: 1.47; acc: 0.59
Batch: 660; loss: 1.2; acc: 0.73
Batch: 680; loss: 1.26; acc: 0.67
Batch: 700; loss: 1.26; acc: 0.69
Batch: 720; loss: 1.21; acc: 0.78
Batch: 740; loss: 1.34; acc: 0.67
Batch: 760; loss: 1.22; acc: 0.69
Batch: 780; loss: 1.33; acc: 0.72
Train Epoch over. train_loss: 1.27; train_accuracy: 0.69 

0.00012958887964487076
0.00012445091851986945
Batch: 0; loss: 1.24; acc: 0.7
Batch: 20; loss: 1.4; acc: 0.62
Batch: 40; loss: 1.03; acc: 0.88
Batch: 60; loss: 1.09; acc: 0.8
Batch: 80; loss: 1.11; acc: 0.8
Batch: 100; loss: 1.16; acc: 0.73
Batch: 120; loss: 1.35; acc: 0.59
Batch: 140; loss: 1.18; acc: 0.69
Val Epoch over. val_loss: 1.2071524076401048; val_accuracy: 0.7229299363057324 

The current subspace-distance is: 0.00012445091851986945 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.17; acc: 0.73
Batch: 20; loss: 1.35; acc: 0.66
Batch: 40; loss: 1.13; acc: 0.72
Batch: 60; loss: 1.28; acc: 0.66
Batch: 80; loss: 1.23; acc: 0.62
Batch: 100; loss: 1.26; acc: 0.67
Batch: 120; loss: 1.41; acc: 0.58
Batch: 140; loss: 1.36; acc: 0.61
Batch: 160; loss: 1.19; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.64
Batch: 200; loss: 1.37; acc: 0.62
Batch: 220; loss: 1.21; acc: 0.64
Batch: 240; loss: 1.12; acc: 0.75
Batch: 260; loss: 1.38; acc: 0.58
Batch: 280; loss: 1.17; acc: 0.77
Batch: 300; loss: 1.29; acc: 0.66
Batch: 320; loss: 1.16; acc: 0.7
Batch: 340; loss: 1.35; acc: 0.67
Batch: 360; loss: 1.3; acc: 0.62
Batch: 380; loss: 1.45; acc: 0.58
Batch: 400; loss: 1.24; acc: 0.69
Batch: 420; loss: 1.26; acc: 0.69
Batch: 440; loss: 1.12; acc: 0.8
Batch: 460; loss: 1.34; acc: 0.59
Batch: 480; loss: 1.32; acc: 0.64
Batch: 500; loss: 1.24; acc: 0.67
Batch: 520; loss: 1.32; acc: 0.66
Batch: 540; loss: 1.08; acc: 0.8
Batch: 560; loss: 1.11; acc: 0.77
Batch: 580; loss: 1.24; acc: 0.73
Batch: 600; loss: 1.26; acc: 0.7
Batch: 620; loss: 1.25; acc: 0.72
Batch: 640; loss: 1.26; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.7
Batch: 680; loss: 1.23; acc: 0.7
Batch: 700; loss: 1.23; acc: 0.72
Batch: 720; loss: 1.13; acc: 0.77
Batch: 740; loss: 1.34; acc: 0.64
Batch: 760; loss: 1.27; acc: 0.7
Batch: 780; loss: 1.28; acc: 0.66
Train Epoch over. train_loss: 1.24; train_accuracy: 0.7 

0.00014113736688159406
0.00013626761210616678
Batch: 0; loss: 1.21; acc: 0.67
Batch: 20; loss: 1.36; acc: 0.59
Batch: 40; loss: 0.97; acc: 0.86
Batch: 60; loss: 1.06; acc: 0.8
Batch: 80; loss: 1.09; acc: 0.78
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.33; acc: 0.62
Batch: 140; loss: 1.15; acc: 0.66
Val Epoch over. val_loss: 1.1711384084573977; val_accuracy: 0.7272093949044586 

The current subspace-distance is: 0.00013626761210616678 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.66
Batch: 20; loss: 1.07; acc: 0.73
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.28; acc: 0.67
Batch: 100; loss: 1.3; acc: 0.62
Batch: 120; loss: 1.12; acc: 0.8
Batch: 140; loss: 1.33; acc: 0.66
Batch: 160; loss: 1.2; acc: 0.72
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.34; acc: 0.64
Batch: 220; loss: 1.16; acc: 0.72
Batch: 240; loss: 1.27; acc: 0.61
Batch: 260; loss: 1.23; acc: 0.72
Batch: 280; loss: 1.25; acc: 0.67
Batch: 300; loss: 1.28; acc: 0.64
Batch: 320; loss: 1.17; acc: 0.73
Batch: 340; loss: 1.14; acc: 0.8
Batch: 360; loss: 1.09; acc: 0.73
Batch: 380; loss: 1.29; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.69
Batch: 420; loss: 1.25; acc: 0.67
Batch: 440; loss: 1.25; acc: 0.69
Batch: 460; loss: 1.17; acc: 0.75
Batch: 480; loss: 1.24; acc: 0.69
Batch: 500; loss: 1.2; acc: 0.72
Batch: 520; loss: 1.11; acc: 0.8
Batch: 540; loss: 1.06; acc: 0.78
Batch: 560; loss: 1.25; acc: 0.67
Batch: 580; loss: 1.08; acc: 0.72
Batch: 600; loss: 1.09; acc: 0.78
Batch: 620; loss: 0.98; acc: 0.83
Batch: 640; loss: 1.13; acc: 0.78
Batch: 660; loss: 1.25; acc: 0.7
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 1.16; acc: 0.73
Batch: 720; loss: 1.17; acc: 0.72
Batch: 740; loss: 1.22; acc: 0.72
Batch: 760; loss: 1.19; acc: 0.67
Batch: 780; loss: 1.14; acc: 0.77
Train Epoch over. train_loss: 1.21; train_accuracy: 0.71 

0.00015144502685870975
0.0001445089146727696
Batch: 0; loss: 1.19; acc: 0.72
Batch: 20; loss: 1.34; acc: 0.64
Batch: 40; loss: 0.93; acc: 0.86
Batch: 60; loss: 1.03; acc: 0.81
Batch: 80; loss: 1.05; acc: 0.78
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 1.11; acc: 0.66
Val Epoch over. val_loss: 1.138535754316172; val_accuracy: 0.7398487261146497 

The current subspace-distance is: 0.0001445089146727696 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.78
Batch: 20; loss: 1.2; acc: 0.77
Batch: 40; loss: 1.26; acc: 0.66
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.81
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 1.09; acc: 0.78
Batch: 160; loss: 1.37; acc: 0.62
Batch: 180; loss: 1.08; acc: 0.78
Batch: 200; loss: 1.2; acc: 0.72
Batch: 220; loss: 1.06; acc: 0.78
Batch: 240; loss: 1.11; acc: 0.72
Batch: 260; loss: 1.2; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.67
Batch: 300; loss: 1.09; acc: 0.78
Batch: 320; loss: 1.29; acc: 0.69
Batch: 340; loss: 1.25; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.75
Batch: 380; loss: 1.2; acc: 0.72
Batch: 400; loss: 1.13; acc: 0.75
Batch: 420; loss: 1.12; acc: 0.72
Batch: 440; loss: 1.2; acc: 0.66
Batch: 460; loss: 1.24; acc: 0.66
Batch: 480; loss: 1.2; acc: 0.73
Batch: 500; loss: 1.2; acc: 0.72
Batch: 520; loss: 1.19; acc: 0.67
Batch: 540; loss: 1.17; acc: 0.77
Batch: 560; loss: 1.09; acc: 0.8
Batch: 580; loss: 1.28; acc: 0.62
Batch: 600; loss: 1.15; acc: 0.72
Batch: 620; loss: 1.31; acc: 0.67
Batch: 640; loss: 1.22; acc: 0.69
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 1.16; acc: 0.7
Batch: 700; loss: 1.17; acc: 0.75
Batch: 720; loss: 1.18; acc: 0.72
Batch: 740; loss: 1.18; acc: 0.69
Batch: 760; loss: 1.18; acc: 0.7
Batch: 780; loss: 1.22; acc: 0.7
Train Epoch over. train_loss: 1.19; train_accuracy: 0.71 

0.00015928276116028428
0.00015249130956362933
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.33; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.83
Batch: 80; loss: 0.99; acc: 0.83
Batch: 100; loss: 1.15; acc: 0.72
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 1.06; acc: 0.73
Val Epoch over. val_loss: 1.1070150714011708; val_accuracy: 0.7514928343949044 

The current subspace-distance is: 0.00015249130956362933 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.77
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 1.02; acc: 0.8
Batch: 60; loss: 1.12; acc: 0.77
Batch: 80; loss: 1.12; acc: 0.78
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 1.14; acc: 0.73
Batch: 140; loss: 1.13; acc: 0.77
Batch: 160; loss: 1.18; acc: 0.7
Batch: 180; loss: 1.2; acc: 0.75
Batch: 200; loss: 1.08; acc: 0.73
Batch: 220; loss: 1.14; acc: 0.75
Batch: 240; loss: 1.2; acc: 0.7
Batch: 260; loss: 1.08; acc: 0.78
Batch: 280; loss: 1.22; acc: 0.66
Batch: 300; loss: 1.26; acc: 0.66
Batch: 320; loss: 1.07; acc: 0.72
Batch: 340; loss: 1.22; acc: 0.67
Batch: 360; loss: 1.1; acc: 0.69
Batch: 380; loss: 1.18; acc: 0.72
Batch: 400; loss: 1.12; acc: 0.75
Batch: 420; loss: 1.25; acc: 0.69
Batch: 440; loss: 1.11; acc: 0.77
Batch: 460; loss: 1.13; acc: 0.72
Batch: 480; loss: 1.25; acc: 0.72
Batch: 500; loss: 1.12; acc: 0.8
Batch: 520; loss: 1.21; acc: 0.72
Batch: 540; loss: 1.18; acc: 0.67
Batch: 560; loss: 1.03; acc: 0.78
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.62
Batch: 620; loss: 1.26; acc: 0.64
Batch: 640; loss: 1.41; acc: 0.61
Batch: 660; loss: 1.31; acc: 0.62
Batch: 680; loss: 1.17; acc: 0.72
Batch: 700; loss: 1.23; acc: 0.64
Batch: 720; loss: 1.1; acc: 0.75
Batch: 740; loss: 1.13; acc: 0.73
Batch: 760; loss: 1.16; acc: 0.75
Batch: 780; loss: 1.12; acc: 0.73
Train Epoch over. train_loss: 1.16; train_accuracy: 0.72 

0.00016574545588809997
0.00015888732741586864
Batch: 0; loss: 1.17; acc: 0.69
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 0.85; acc: 0.88
Batch: 60; loss: 1.0; acc: 0.83
Batch: 80; loss: 0.97; acc: 0.8
Batch: 100; loss: 1.14; acc: 0.67
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.02; acc: 0.77
Val Epoch over. val_loss: 1.09182383224463; val_accuracy: 0.7507961783439491 

The current subspace-distance is: 0.00015888732741586864 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 1.07; acc: 0.75
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.17; acc: 0.72
Batch: 140; loss: 1.18; acc: 0.66
Batch: 160; loss: 1.19; acc: 0.73
Batch: 180; loss: 1.25; acc: 0.69
Batch: 200; loss: 1.09; acc: 0.69
Batch: 220; loss: 1.03; acc: 0.83
Batch: 240; loss: 1.1; acc: 0.78
Batch: 260; loss: 1.19; acc: 0.73
Batch: 280; loss: 1.13; acc: 0.78
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.16; acc: 0.66
Batch: 340; loss: 1.23; acc: 0.66
Batch: 360; loss: 1.31; acc: 0.62
Batch: 380; loss: 1.3; acc: 0.7
Batch: 400; loss: 1.07; acc: 0.8
Batch: 420; loss: 1.14; acc: 0.67
Batch: 440; loss: 1.11; acc: 0.69
Batch: 460; loss: 1.25; acc: 0.72
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.01; acc: 0.81
Batch: 520; loss: 1.05; acc: 0.77
Batch: 540; loss: 0.96; acc: 0.8
Batch: 560; loss: 1.04; acc: 0.78
Batch: 580; loss: 1.26; acc: 0.67
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.2; acc: 0.66
Batch: 640; loss: 1.21; acc: 0.69
Batch: 660; loss: 1.06; acc: 0.77
Batch: 680; loss: 1.01; acc: 0.77
Batch: 700; loss: 1.26; acc: 0.7
Batch: 720; loss: 1.13; acc: 0.73
Batch: 740; loss: 1.1; acc: 0.78
Batch: 760; loss: 1.24; acc: 0.66
Batch: 780; loss: 1.29; acc: 0.67
Train Epoch over. train_loss: 1.15; train_accuracy: 0.72 

0.00017188845959026366
0.00016490468988195062
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.84; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.81
Batch: 80; loss: 0.94; acc: 0.83
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 1.04; acc: 0.78
Val Epoch over. val_loss: 1.086866756913009; val_accuracy: 0.7515923566878981 

The current subspace-distance is: 0.00016490468988195062 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.35; acc: 0.55
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 1.23; acc: 0.75
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.31; acc: 0.64
Batch: 100; loss: 1.25; acc: 0.67
Batch: 120; loss: 1.2; acc: 0.72
Batch: 140; loss: 1.01; acc: 0.81
Batch: 160; loss: 1.29; acc: 0.64
Batch: 180; loss: 1.06; acc: 0.77
Batch: 200; loss: 1.12; acc: 0.75
Batch: 220; loss: 1.22; acc: 0.66
Batch: 240; loss: 1.27; acc: 0.62
Batch: 260; loss: 1.16; acc: 0.7
Batch: 280; loss: 1.1; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.78
Batch: 320; loss: 1.22; acc: 0.72
Batch: 340; loss: 1.03; acc: 0.77
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.14; acc: 0.73
Batch: 400; loss: 1.13; acc: 0.78
Batch: 420; loss: 1.12; acc: 0.77
Batch: 440; loss: 1.1; acc: 0.75
Batch: 460; loss: 1.07; acc: 0.77
Batch: 480; loss: 1.24; acc: 0.64
Batch: 500; loss: 1.2; acc: 0.75
Batch: 520; loss: 1.06; acc: 0.69
Batch: 540; loss: 1.17; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.66
Batch: 580; loss: 1.24; acc: 0.66
Batch: 600; loss: 1.11; acc: 0.69
Batch: 620; loss: 1.01; acc: 0.75
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.17; acc: 0.67
Batch: 680; loss: 1.07; acc: 0.78
Batch: 700; loss: 1.19; acc: 0.66
Batch: 720; loss: 1.06; acc: 0.78
Batch: 740; loss: 1.06; acc: 0.72
Batch: 760; loss: 1.28; acc: 0.67
Batch: 780; loss: 1.05; acc: 0.75
Train Epoch over. train_loss: 1.15; train_accuracy: 0.72 

0.0001753343385644257
0.00016731755749788135
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.61
Batch: 40; loss: 0.84; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.81
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 1.14; acc: 0.64
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.01; acc: 0.78
Val Epoch over. val_loss: 1.0808410659717147; val_accuracy: 0.7560708598726115 

The current subspace-distance is: 0.00016731755749788135 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 1.0; acc: 0.8
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.17; acc: 0.73
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.28; acc: 0.73
Batch: 140; loss: 1.3; acc: 0.67
Batch: 160; loss: 1.06; acc: 0.73
Batch: 180; loss: 1.15; acc: 0.7
Batch: 200; loss: 1.15; acc: 0.69
Batch: 220; loss: 1.05; acc: 0.83
Batch: 240; loss: 1.04; acc: 0.78
Batch: 260; loss: 1.16; acc: 0.62
Batch: 280; loss: 1.09; acc: 0.7
Batch: 300; loss: 1.0; acc: 0.83
Batch: 320; loss: 1.07; acc: 0.78
Batch: 340; loss: 1.23; acc: 0.7
Batch: 360; loss: 1.07; acc: 0.78
Batch: 380; loss: 1.16; acc: 0.69
Batch: 400; loss: 1.1; acc: 0.77
Batch: 420; loss: 1.08; acc: 0.75
Batch: 440; loss: 1.06; acc: 0.73
Batch: 460; loss: 1.19; acc: 0.69
Batch: 480; loss: 1.16; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.73
Batch: 520; loss: 1.18; acc: 0.62
Batch: 540; loss: 1.28; acc: 0.61
Batch: 560; loss: 1.21; acc: 0.66
Batch: 580; loss: 1.01; acc: 0.78
Batch: 600; loss: 1.19; acc: 0.69
Batch: 620; loss: 1.13; acc: 0.72
Batch: 640; loss: 1.19; acc: 0.72
Batch: 660; loss: 1.35; acc: 0.58
Batch: 680; loss: 1.12; acc: 0.75
Batch: 700; loss: 1.1; acc: 0.73
Batch: 720; loss: 1.01; acc: 0.81
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 1.26; acc: 0.66
Batch: 780; loss: 1.1; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.72 

0.00017390368157066405
0.00016795708506833762
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.8
Batch: 80; loss: 0.93; acc: 0.81
Batch: 100; loss: 1.14; acc: 0.67
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 0.99; acc: 0.78
Val Epoch over. val_loss: 1.0714781383040604; val_accuracy: 0.755672770700637 

The current subspace-distance is: 0.00016795708506833762 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.67
Batch: 20; loss: 1.17; acc: 0.7
Batch: 40; loss: 1.16; acc: 0.67
Batch: 60; loss: 1.31; acc: 0.64
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.7
Batch: 140; loss: 1.11; acc: 0.64
Batch: 160; loss: 1.04; acc: 0.81
Batch: 180; loss: 1.33; acc: 0.75
Batch: 200; loss: 1.19; acc: 0.75
Batch: 220; loss: 1.18; acc: 0.75
Batch: 240; loss: 1.02; acc: 0.73
Batch: 260; loss: 1.11; acc: 0.8
Batch: 280; loss: 1.15; acc: 0.67
Batch: 300; loss: 1.08; acc: 0.77
Batch: 320; loss: 1.08; acc: 0.73
Batch: 340; loss: 1.37; acc: 0.58
Batch: 360; loss: 1.13; acc: 0.73
Batch: 380; loss: 1.12; acc: 0.77
Batch: 400; loss: 1.26; acc: 0.73
Batch: 420; loss: 1.16; acc: 0.72
Batch: 440; loss: 1.17; acc: 0.69
Batch: 460; loss: 1.09; acc: 0.75
Batch: 480; loss: 1.11; acc: 0.75
Batch: 500; loss: 1.11; acc: 0.75
Batch: 520; loss: 1.11; acc: 0.7
Batch: 540; loss: 1.19; acc: 0.67
Batch: 560; loss: 1.13; acc: 0.7
Batch: 580; loss: 1.11; acc: 0.73
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 0.96; acc: 0.84
Batch: 640; loss: 1.07; acc: 0.78
Batch: 660; loss: 1.21; acc: 0.59
Batch: 680; loss: 1.14; acc: 0.75
Batch: 700; loss: 0.94; acc: 0.86
Batch: 720; loss: 1.06; acc: 0.75
Batch: 740; loss: 1.03; acc: 0.75
Batch: 760; loss: 1.08; acc: 0.72
Batch: 780; loss: 1.2; acc: 0.72
Train Epoch over. train_loss: 1.13; train_accuracy: 0.72 

0.00017716600268613547
0.0001706684852251783
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.8
Batch: 80; loss: 0.93; acc: 0.81
Batch: 100; loss: 1.15; acc: 0.62
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.98; acc: 0.78
Val Epoch over. val_loss: 1.0673053534167587; val_accuracy: 0.7539808917197452 

The current subspace-distance is: 0.0001706684852251783 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.75
Batch: 20; loss: 1.28; acc: 0.67
Batch: 40; loss: 0.94; acc: 0.81
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.13; acc: 0.77
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.06; acc: 0.75
Batch: 140; loss: 1.01; acc: 0.78
Batch: 160; loss: 1.0; acc: 0.77
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 1.07; acc: 0.77
Batch: 220; loss: 1.16; acc: 0.7
Batch: 240; loss: 1.04; acc: 0.73
Batch: 260; loss: 1.04; acc: 0.77
Batch: 280; loss: 0.96; acc: 0.83
Batch: 300; loss: 1.3; acc: 0.62
Batch: 320; loss: 1.03; acc: 0.78
Batch: 340; loss: 1.08; acc: 0.75
Batch: 360; loss: 1.14; acc: 0.69
Batch: 380; loss: 1.08; acc: 0.7
Batch: 400; loss: 1.22; acc: 0.69
Batch: 420; loss: 1.19; acc: 0.69
Batch: 440; loss: 0.99; acc: 0.81
Batch: 460; loss: 1.23; acc: 0.67
Batch: 480; loss: 1.22; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.75
Batch: 520; loss: 1.05; acc: 0.81
Batch: 540; loss: 1.02; acc: 0.77
Batch: 560; loss: 1.07; acc: 0.8
Batch: 580; loss: 1.17; acc: 0.72
Batch: 600; loss: 0.92; acc: 0.78
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 1.13; acc: 0.67
Batch: 660; loss: 1.05; acc: 0.72
Batch: 680; loss: 1.09; acc: 0.78
Batch: 700; loss: 1.17; acc: 0.69
Batch: 720; loss: 1.11; acc: 0.8
Batch: 740; loss: 1.27; acc: 0.77
Batch: 760; loss: 1.14; acc: 0.69
Batch: 780; loss: 1.05; acc: 0.77
Train Epoch over. train_loss: 1.12; train_accuracy: 0.72 

0.00018280663061887026
0.00017308504902757704
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.82; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.84
Batch: 100; loss: 1.16; acc: 0.66
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.99; acc: 0.78
Val Epoch over. val_loss: 1.0688853370156257; val_accuracy: 0.7584593949044586 

The current subspace-distance is: 0.00017308504902757704 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 1.1; acc: 0.73
Batch: 40; loss: 1.13; acc: 0.72
Batch: 60; loss: 1.01; acc: 0.77
Batch: 80; loss: 1.15; acc: 0.75
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 1.03; acc: 0.77
Batch: 160; loss: 1.03; acc: 0.77
Batch: 180; loss: 1.08; acc: 0.72
Batch: 200; loss: 1.16; acc: 0.72
Batch: 220; loss: 1.19; acc: 0.67
Batch: 240; loss: 1.0; acc: 0.81
Batch: 260; loss: 1.29; acc: 0.62
Batch: 280; loss: 1.15; acc: 0.72
Batch: 300; loss: 1.0; acc: 0.77
Batch: 320; loss: 1.19; acc: 0.64
Batch: 340; loss: 1.15; acc: 0.7
Batch: 360; loss: 1.12; acc: 0.75
Batch: 380; loss: 1.1; acc: 0.66
Batch: 400; loss: 1.09; acc: 0.77
Batch: 420; loss: 1.13; acc: 0.77
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 1.07; acc: 0.81
Batch: 480; loss: 1.15; acc: 0.72
Batch: 500; loss: 1.13; acc: 0.69
Batch: 520; loss: 1.2; acc: 0.66
Batch: 540; loss: 0.99; acc: 0.77
Batch: 560; loss: 1.14; acc: 0.72
Batch: 580; loss: 1.15; acc: 0.77
Batch: 600; loss: 1.04; acc: 0.78
Batch: 620; loss: 1.08; acc: 0.75
Batch: 640; loss: 1.16; acc: 0.72
Batch: 660; loss: 1.11; acc: 0.67
Batch: 680; loss: 1.15; acc: 0.77
Batch: 700; loss: 1.15; acc: 0.73
Batch: 720; loss: 1.07; acc: 0.67
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 1.07; acc: 0.73
Batch: 780; loss: 0.98; acc: 0.75
Train Epoch over. train_loss: 1.12; train_accuracy: 0.72 

0.000187188561540097
0.00017843568639364094
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.84
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.27; acc: 0.69
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.0535386348985563; val_accuracy: 0.7575636942675159 

The current subspace-distance is: 0.00017843568639364094 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.04; acc: 0.77
Batch: 20; loss: 1.09; acc: 0.77
Batch: 40; loss: 1.0; acc: 0.75
Batch: 60; loss: 1.27; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.69
Batch: 120; loss: 1.1; acc: 0.72
Batch: 140; loss: 1.16; acc: 0.7
Batch: 160; loss: 1.21; acc: 0.66
Batch: 180; loss: 1.14; acc: 0.72
Batch: 200; loss: 1.08; acc: 0.67
Batch: 220; loss: 1.04; acc: 0.7
Batch: 240; loss: 1.06; acc: 0.77
Batch: 260; loss: 1.15; acc: 0.72
Batch: 280; loss: 1.17; acc: 0.67
Batch: 300; loss: 1.02; acc: 0.77
Batch: 320; loss: 1.18; acc: 0.7
Batch: 340; loss: 1.29; acc: 0.58
Batch: 360; loss: 1.18; acc: 0.7
Batch: 380; loss: 0.98; acc: 0.78
Batch: 400; loss: 1.07; acc: 0.73
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 1.21; acc: 0.67
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 1.13; acc: 0.72
Batch: 500; loss: 1.07; acc: 0.67
Batch: 520; loss: 1.13; acc: 0.75
Batch: 540; loss: 1.13; acc: 0.78
Batch: 560; loss: 0.99; acc: 0.78
Batch: 580; loss: 1.27; acc: 0.62
Batch: 600; loss: 1.26; acc: 0.62
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 1.02; acc: 0.81
Batch: 660; loss: 1.18; acc: 0.67
Batch: 680; loss: 1.18; acc: 0.69
Batch: 700; loss: 1.01; acc: 0.77
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.09; acc: 0.73
Batch: 760; loss: 1.18; acc: 0.7
Batch: 780; loss: 1.02; acc: 0.72
Train Epoch over. train_loss: 1.11; train_accuracy: 0.72 

0.0001873434812296182
0.0001802189217414707
Batch: 0; loss: 1.15; acc: 0.67
Batch: 20; loss: 1.26; acc: 0.64
Batch: 40; loss: 0.8; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.73
Batch: 80; loss: 0.9; acc: 0.86
Batch: 100; loss: 1.14; acc: 0.67
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.0452885692286644; val_accuracy: 0.7587579617834395 

The current subspace-distance is: 0.0001802189217414707 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 1.08; acc: 0.78
Batch: 60; loss: 0.94; acc: 0.81
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.13; acc: 0.77
Batch: 120; loss: 1.32; acc: 0.66
Batch: 140; loss: 1.03; acc: 0.67
Batch: 160; loss: 1.19; acc: 0.62
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.07; acc: 0.8
Batch: 220; loss: 1.13; acc: 0.77
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.27; acc: 0.59
Batch: 280; loss: 1.13; acc: 0.66
Batch: 300; loss: 1.11; acc: 0.73
Batch: 320; loss: 0.99; acc: 0.84
Batch: 340; loss: 0.99; acc: 0.75
Batch: 360; loss: 1.14; acc: 0.69
Batch: 380; loss: 1.08; acc: 0.73
Batch: 400; loss: 0.95; acc: 0.8
Batch: 420; loss: 0.95; acc: 0.75
Batch: 440; loss: 1.14; acc: 0.73
Batch: 460; loss: 1.08; acc: 0.77
Batch: 480; loss: 1.23; acc: 0.75
Batch: 500; loss: 0.99; acc: 0.81
Batch: 520; loss: 1.07; acc: 0.77
Batch: 540; loss: 1.14; acc: 0.73
Batch: 560; loss: 1.12; acc: 0.75
Batch: 580; loss: 1.11; acc: 0.7
Batch: 600; loss: 1.04; acc: 0.77
Batch: 620; loss: 1.14; acc: 0.69
Batch: 640; loss: 1.02; acc: 0.78
Batch: 660; loss: 1.08; acc: 0.7
Batch: 680; loss: 1.03; acc: 0.77
Batch: 700; loss: 0.99; acc: 0.8
Batch: 720; loss: 1.18; acc: 0.66
Batch: 740; loss: 1.3; acc: 0.69
Batch: 760; loss: 1.01; acc: 0.81
Batch: 780; loss: 1.12; acc: 0.72
Train Epoch over. train_loss: 1.1; train_accuracy: 0.73 

0.00019148520368617028
0.00018343087867833674
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.64
Batch: 40; loss: 0.8; acc: 0.86
Batch: 60; loss: 0.95; acc: 0.75
Batch: 80; loss: 0.89; acc: 0.88
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.25; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.0374069323964938; val_accuracy: 0.7600517515923567 

The current subspace-distance is: 0.00018343087867833674 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.73
Batch: 40; loss: 1.15; acc: 0.7
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 1.04; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.81
Batch: 140; loss: 1.21; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 1.22; acc: 0.69
Batch: 200; loss: 1.33; acc: 0.59
Batch: 220; loss: 1.05; acc: 0.7
Batch: 240; loss: 1.12; acc: 0.77
Batch: 260; loss: 1.16; acc: 0.73
Batch: 280; loss: 0.92; acc: 0.81
Batch: 300; loss: 1.06; acc: 0.81
Batch: 320; loss: 1.02; acc: 0.77
Batch: 340; loss: 1.15; acc: 0.69
Batch: 360; loss: 1.06; acc: 0.77
Batch: 380; loss: 1.24; acc: 0.64
Batch: 400; loss: 0.98; acc: 0.8
Batch: 420; loss: 1.16; acc: 0.77
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 1.04; acc: 0.72
Batch: 480; loss: 1.1; acc: 0.73
Batch: 500; loss: 1.04; acc: 0.73
Batch: 520; loss: 1.23; acc: 0.64
Batch: 540; loss: 1.07; acc: 0.75
Batch: 560; loss: 1.22; acc: 0.67
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.81
Batch: 620; loss: 1.02; acc: 0.8
Batch: 640; loss: 1.0; acc: 0.77
Batch: 660; loss: 1.26; acc: 0.67
Batch: 680; loss: 1.22; acc: 0.67
Batch: 700; loss: 1.2; acc: 0.64
Batch: 720; loss: 1.1; acc: 0.77
Batch: 740; loss: 1.16; acc: 0.67
Batch: 760; loss: 1.09; acc: 0.75
Batch: 780; loss: 1.24; acc: 0.64
Train Epoch over. train_loss: 1.1; train_accuracy: 0.73 

0.00019298295956104994
0.00018529540102463216
Batch: 0; loss: 1.14; acc: 0.66
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.79; acc: 0.86
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.87; acc: 0.88
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.0254087922679391; val_accuracy: 0.762937898089172 

The current subspace-distance is: 0.00018529540102463216 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.93; acc: 0.81
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 1.1; acc: 0.78
Batch: 60; loss: 1.26; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.78
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.23; acc: 0.66
Batch: 140; loss: 1.21; acc: 0.7
Batch: 160; loss: 1.03; acc: 0.75
Batch: 180; loss: 0.99; acc: 0.83
Batch: 200; loss: 1.05; acc: 0.77
Batch: 220; loss: 1.06; acc: 0.75
Batch: 240; loss: 1.0; acc: 0.78
Batch: 260; loss: 1.06; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.7
Batch: 300; loss: 1.05; acc: 0.75
Batch: 320; loss: 0.87; acc: 0.83
Batch: 340; loss: 1.24; acc: 0.7
Batch: 360; loss: 1.2; acc: 0.66
Batch: 380; loss: 0.95; acc: 0.77
Batch: 400; loss: 1.25; acc: 0.61
Batch: 420; loss: 1.04; acc: 0.8
Batch: 440; loss: 1.06; acc: 0.77
Batch: 460; loss: 0.87; acc: 0.88
Batch: 480; loss: 1.15; acc: 0.72
Batch: 500; loss: 1.05; acc: 0.75
Batch: 520; loss: 1.08; acc: 0.69
Batch: 540; loss: 1.06; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.8
Batch: 580; loss: 1.06; acc: 0.77
Batch: 600; loss: 0.97; acc: 0.81
Batch: 620; loss: 1.08; acc: 0.73
Batch: 640; loss: 1.1; acc: 0.73
Batch: 660; loss: 1.21; acc: 0.7
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.08; acc: 0.77
Batch: 720; loss: 1.02; acc: 0.77
Batch: 740; loss: 1.11; acc: 0.69
Batch: 760; loss: 1.07; acc: 0.77
Batch: 780; loss: 1.16; acc: 0.72
Train Epoch over. train_loss: 1.09; train_accuracy: 0.73 

0.000196156557649374
0.00018798491510096937
Batch: 0; loss: 1.14; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.88; acc: 0.86
Batch: 100; loss: 1.12; acc: 0.67
Batch: 120; loss: 1.25; acc: 0.7
Batch: 140; loss: 0.95; acc: 0.81
Val Epoch over. val_loss: 1.0209367597938344; val_accuracy: 0.7624402866242038 

The current subspace-distance is: 0.00018798491510096937 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 0.99; acc: 0.77
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 0.98; acc: 0.78
Batch: 100; loss: 1.26; acc: 0.64
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 1.14; acc: 0.77
Batch: 160; loss: 1.23; acc: 0.67
Batch: 180; loss: 0.99; acc: 0.83
Batch: 200; loss: 0.98; acc: 0.83
Batch: 220; loss: 1.03; acc: 0.77
Batch: 240; loss: 1.22; acc: 0.64
Batch: 260; loss: 1.12; acc: 0.72
Batch: 280; loss: 1.08; acc: 0.72
Batch: 300; loss: 1.1; acc: 0.73
Batch: 320; loss: 1.22; acc: 0.67
Batch: 340; loss: 1.11; acc: 0.67
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.14; acc: 0.72
Batch: 400; loss: 1.01; acc: 0.75
Batch: 420; loss: 1.21; acc: 0.64
Batch: 440; loss: 1.07; acc: 0.75
Batch: 460; loss: 1.21; acc: 0.7
Batch: 480; loss: 1.22; acc: 0.66
Batch: 500; loss: 0.95; acc: 0.73
Batch: 520; loss: 1.18; acc: 0.67
Batch: 540; loss: 1.16; acc: 0.72
Batch: 560; loss: 1.08; acc: 0.7
Batch: 580; loss: 0.99; acc: 0.77
Batch: 600; loss: 1.13; acc: 0.69
Batch: 620; loss: 1.02; acc: 0.8
Batch: 640; loss: 1.05; acc: 0.75
Batch: 660; loss: 1.14; acc: 0.7
Batch: 680; loss: 1.18; acc: 0.66
Batch: 700; loss: 1.04; acc: 0.72
Batch: 720; loss: 1.2; acc: 0.73
Batch: 740; loss: 1.11; acc: 0.7
Batch: 760; loss: 1.0; acc: 0.75
Batch: 780; loss: 0.84; acc: 0.89
Train Epoch over. train_loss: 1.08; train_accuracy: 0.73 

0.00019339723803568631
0.00018494107644073665
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.88; acc: 0.86
Batch: 100; loss: 1.14; acc: 0.64
Batch: 120; loss: 1.26; acc: 0.7
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 1.0264393489831571; val_accuracy: 0.7607484076433121 

The current subspace-distance is: 0.00018494107644073665 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.05; acc: 0.75
Batch: 40; loss: 1.14; acc: 0.69
Batch: 60; loss: 1.06; acc: 0.83
Batch: 80; loss: 0.98; acc: 0.81
Batch: 100; loss: 1.02; acc: 0.83
Batch: 120; loss: 1.09; acc: 0.77
Batch: 140; loss: 1.1; acc: 0.73
Batch: 160; loss: 1.17; acc: 0.69
Batch: 180; loss: 1.01; acc: 0.77
Batch: 200; loss: 1.15; acc: 0.7
Batch: 220; loss: 0.94; acc: 0.81
Batch: 240; loss: 1.15; acc: 0.69
Batch: 260; loss: 1.05; acc: 0.72
Batch: 280; loss: 1.04; acc: 0.75
Batch: 300; loss: 1.11; acc: 0.75
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 1.13; acc: 0.72
Batch: 360; loss: 1.1; acc: 0.77
Batch: 380; loss: 1.17; acc: 0.62
Batch: 400; loss: 1.07; acc: 0.73
Batch: 420; loss: 1.1; acc: 0.72
Batch: 440; loss: 1.17; acc: 0.73
Batch: 460; loss: 1.0; acc: 0.75
Batch: 480; loss: 1.1; acc: 0.69
Batch: 500; loss: 0.98; acc: 0.75
Batch: 520; loss: 1.21; acc: 0.58
Batch: 540; loss: 1.15; acc: 0.72
Batch: 560; loss: 1.0; acc: 0.81
Batch: 580; loss: 1.14; acc: 0.73
Batch: 600; loss: 1.11; acc: 0.7
Batch: 620; loss: 1.04; acc: 0.73
Batch: 640; loss: 1.04; acc: 0.7
Batch: 660; loss: 1.27; acc: 0.62
Batch: 680; loss: 1.18; acc: 0.61
Batch: 700; loss: 1.04; acc: 0.73
Batch: 720; loss: 0.97; acc: 0.8
Batch: 740; loss: 1.06; acc: 0.78
Batch: 760; loss: 1.2; acc: 0.7
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.08; train_accuracy: 0.73 

0.0001951196463778615
0.0001878735638456419
Batch: 0; loss: 1.15; acc: 0.62
Batch: 20; loss: 1.24; acc: 0.59
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 0.94; acc: 0.75
Batch: 80; loss: 0.87; acc: 0.86
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.81
Val Epoch over. val_loss: 1.0190330075610214; val_accuracy: 0.7616441082802548 

The current subspace-distance is: 0.0001878735638456419 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.01; acc: 0.75
Batch: 20; loss: 1.15; acc: 0.73
Batch: 40; loss: 1.0; acc: 0.73
Batch: 60; loss: 1.17; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.72
Batch: 100; loss: 1.05; acc: 0.73
Batch: 120; loss: 1.07; acc: 0.78
Batch: 140; loss: 1.04; acc: 0.78
Batch: 160; loss: 1.15; acc: 0.66
Batch: 180; loss: 1.06; acc: 0.78
Batch: 200; loss: 1.16; acc: 0.69
Batch: 220; loss: 1.16; acc: 0.73
Batch: 240; loss: 0.96; acc: 0.77
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 1.06; acc: 0.72
Batch: 300; loss: 1.04; acc: 0.78
Batch: 320; loss: 1.06; acc: 0.75
Batch: 340; loss: 0.98; acc: 0.8
Batch: 360; loss: 1.07; acc: 0.78
Batch: 380; loss: 0.97; acc: 0.81
Batch: 400; loss: 1.0; acc: 0.8
Batch: 420; loss: 1.32; acc: 0.64
Batch: 440; loss: 1.04; acc: 0.8
Batch: 460; loss: 1.14; acc: 0.67
Batch: 480; loss: 1.08; acc: 0.77
Batch: 500; loss: 1.34; acc: 0.58
Batch: 520; loss: 1.08; acc: 0.73
Batch: 540; loss: 0.85; acc: 0.89
Batch: 560; loss: 0.95; acc: 0.78
Batch: 580; loss: 1.09; acc: 0.72
Batch: 600; loss: 1.01; acc: 0.75
Batch: 620; loss: 1.15; acc: 0.66
Batch: 640; loss: 1.17; acc: 0.66
Batch: 660; loss: 1.06; acc: 0.77
Batch: 680; loss: 1.28; acc: 0.64
Batch: 700; loss: 1.07; acc: 0.7
Batch: 720; loss: 0.98; acc: 0.78
Batch: 740; loss: 1.1; acc: 0.72
Batch: 760; loss: 1.11; acc: 0.72
Batch: 780; loss: 1.09; acc: 0.77
Train Epoch over. train_loss: 1.08; train_accuracy: 0.73 

0.00019717156828846782
0.00019227943266741931
Batch: 0; loss: 1.13; acc: 0.62
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.87; acc: 0.88
Batch: 100; loss: 1.13; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.7
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.01883209700797; val_accuracy: 0.7603503184713376 

The current subspace-distance is: 0.00019227943266741931 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 1.11; acc: 0.72
Batch: 60; loss: 1.22; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.7
Batch: 100; loss: 1.18; acc: 0.66
Batch: 120; loss: 1.03; acc: 0.73
Batch: 140; loss: 0.86; acc: 0.78
Batch: 160; loss: 0.96; acc: 0.8
Batch: 180; loss: 1.06; acc: 0.8
Batch: 200; loss: 1.1; acc: 0.67
Batch: 220; loss: 1.14; acc: 0.69
Batch: 240; loss: 1.15; acc: 0.7
Batch: 260; loss: 1.1; acc: 0.7
Batch: 280; loss: 1.07; acc: 0.77
Batch: 300; loss: 1.09; acc: 0.7
Batch: 320; loss: 1.04; acc: 0.78
Batch: 340; loss: 1.26; acc: 0.72
Batch: 360; loss: 1.16; acc: 0.69
Batch: 380; loss: 1.24; acc: 0.62
Batch: 400; loss: 1.09; acc: 0.67
Batch: 420; loss: 0.97; acc: 0.77
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.96; acc: 0.77
Batch: 480; loss: 1.23; acc: 0.67
Batch: 500; loss: 1.15; acc: 0.7
Batch: 520; loss: 0.99; acc: 0.75
Batch: 540; loss: 1.06; acc: 0.73
Batch: 560; loss: 1.18; acc: 0.62
Batch: 580; loss: 1.26; acc: 0.72
Batch: 600; loss: 1.13; acc: 0.72
Batch: 620; loss: 1.1; acc: 0.75
Batch: 640; loss: 1.02; acc: 0.7
Batch: 660; loss: 0.92; acc: 0.84
Batch: 680; loss: 1.0; acc: 0.75
Batch: 700; loss: 1.21; acc: 0.66
Batch: 720; loss: 1.13; acc: 0.73
Batch: 740; loss: 0.9; acc: 0.81
Batch: 760; loss: 0.93; acc: 0.73
Batch: 780; loss: 0.99; acc: 0.75
Train Epoch over. train_loss: 1.08; train_accuracy: 0.73 

0.00019655805954243988
0.00018761749379336834
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.25; acc: 0.59
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.89
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 1.22; acc: 0.72
Batch: 140; loss: 0.95; acc: 0.81
Val Epoch over. val_loss: 1.0103513822434054; val_accuracy: 0.7652269108280255 

The current subspace-distance is: 0.00018761749379336834 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.8
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.0; acc: 0.72
Batch: 140; loss: 1.01; acc: 0.77
Batch: 160; loss: 1.22; acc: 0.72
Batch: 180; loss: 1.01; acc: 0.73
Batch: 200; loss: 1.06; acc: 0.69
Batch: 220; loss: 1.0; acc: 0.78
Batch: 240; loss: 1.0; acc: 0.8
Batch: 260; loss: 1.14; acc: 0.67
Batch: 280; loss: 0.97; acc: 0.83
Batch: 300; loss: 0.95; acc: 0.81
Batch: 320; loss: 1.12; acc: 0.7
Batch: 340; loss: 1.18; acc: 0.72
Batch: 360; loss: 0.92; acc: 0.78
Batch: 380; loss: 1.0; acc: 0.78
Batch: 400; loss: 1.11; acc: 0.7
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.22; acc: 0.64
Batch: 460; loss: 0.92; acc: 0.84
Batch: 480; loss: 1.05; acc: 0.78
Batch: 500; loss: 1.32; acc: 0.7
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 1.28; acc: 0.64
Batch: 560; loss: 1.07; acc: 0.72
Batch: 580; loss: 1.09; acc: 0.73
Batch: 600; loss: 1.08; acc: 0.75
Batch: 620; loss: 1.02; acc: 0.73
Batch: 640; loss: 1.05; acc: 0.75
Batch: 660; loss: 1.02; acc: 0.73
Batch: 680; loss: 1.05; acc: 0.75
Batch: 700; loss: 0.99; acc: 0.77
Batch: 720; loss: 1.0; acc: 0.78
Batch: 740; loss: 1.01; acc: 0.81
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.13; acc: 0.73
Train Epoch over. train_loss: 1.08; train_accuracy: 0.73 

0.0001974178449017927
0.0001900223142001778
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.89
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.91; acc: 0.81
Val Epoch over. val_loss: 1.0080933878376226; val_accuracy: 0.7651273885350318 

The current subspace-distance is: 0.0001900223142001778 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.25; acc: 0.7
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 1.03; acc: 0.75
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.03; acc: 0.75
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 1.06; acc: 0.66
Batch: 160; loss: 0.86; acc: 0.88
Batch: 180; loss: 1.09; acc: 0.66
Batch: 200; loss: 0.95; acc: 0.8
Batch: 220; loss: 1.08; acc: 0.73
Batch: 240; loss: 1.07; acc: 0.73
Batch: 260; loss: 1.02; acc: 0.77
Batch: 280; loss: 0.98; acc: 0.75
Batch: 300; loss: 1.2; acc: 0.73
Batch: 320; loss: 1.14; acc: 0.69
Batch: 340; loss: 1.03; acc: 0.75
Batch: 360; loss: 1.25; acc: 0.64
Batch: 380; loss: 1.01; acc: 0.72
Batch: 400; loss: 1.17; acc: 0.73
Batch: 420; loss: 1.12; acc: 0.78
Batch: 440; loss: 1.14; acc: 0.7
Batch: 460; loss: 1.02; acc: 0.8
Batch: 480; loss: 1.27; acc: 0.64
Batch: 500; loss: 1.09; acc: 0.75
Batch: 520; loss: 1.12; acc: 0.67
Batch: 540; loss: 1.19; acc: 0.58
Batch: 560; loss: 0.92; acc: 0.83
Batch: 580; loss: 0.96; acc: 0.75
Batch: 600; loss: 1.05; acc: 0.8
Batch: 620; loss: 1.09; acc: 0.77
Batch: 640; loss: 1.04; acc: 0.7
Batch: 660; loss: 1.16; acc: 0.69
Batch: 680; loss: 1.1; acc: 0.72
Batch: 700; loss: 0.92; acc: 0.84
Batch: 720; loss: 1.06; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.73
Batch: 760; loss: 1.25; acc: 0.62
Batch: 780; loss: 1.1; acc: 0.69
Train Epoch over. train_loss: 1.08; train_accuracy: 0.73 

0.00020058502559550107
0.00019115430768579245
Batch: 0; loss: 1.14; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 0.77; acc: 0.88
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.89
Batch: 100; loss: 1.12; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.0034812981156027; val_accuracy: 0.7630374203821656 

The current subspace-distance is: 0.00019115430768579245 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.61
Batch: 40; loss: 1.06; acc: 0.73
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.05; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.69
Batch: 140; loss: 1.17; acc: 0.67
Batch: 160; loss: 1.13; acc: 0.73
Batch: 180; loss: 1.08; acc: 0.69
Batch: 200; loss: 1.11; acc: 0.72
Batch: 220; loss: 1.04; acc: 0.77
Batch: 240; loss: 0.87; acc: 0.83
Batch: 260; loss: 1.07; acc: 0.73
Batch: 280; loss: 1.04; acc: 0.75
Batch: 300; loss: 1.02; acc: 0.75
Batch: 320; loss: 1.21; acc: 0.73
Batch: 340; loss: 1.02; acc: 0.77
Batch: 360; loss: 1.2; acc: 0.69
Batch: 380; loss: 1.03; acc: 0.75
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 1.03; acc: 0.73
Batch: 440; loss: 0.99; acc: 0.73
Batch: 460; loss: 1.02; acc: 0.77
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 0.99; acc: 0.73
Batch: 520; loss: 1.02; acc: 0.78
Batch: 540; loss: 1.2; acc: 0.67
Batch: 560; loss: 1.09; acc: 0.75
Batch: 580; loss: 1.1; acc: 0.75
Batch: 600; loss: 1.27; acc: 0.62
Batch: 620; loss: 1.13; acc: 0.69
Batch: 640; loss: 1.16; acc: 0.72
Batch: 660; loss: 0.92; acc: 0.83
Batch: 680; loss: 1.08; acc: 0.7
Batch: 700; loss: 1.07; acc: 0.75
Batch: 720; loss: 1.11; acc: 0.73
Batch: 740; loss: 1.08; acc: 0.7
Batch: 760; loss: 1.05; acc: 0.7
Batch: 780; loss: 1.21; acc: 0.64
Train Epoch over. train_loss: 1.07; train_accuracy: 0.73 

0.00019897471065633
0.00019147236889693886
Batch: 0; loss: 1.12; acc: 0.67
Batch: 20; loss: 1.24; acc: 0.59
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.75
Batch: 80; loss: 0.88; acc: 0.88
Batch: 100; loss: 1.11; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.0116845733800512; val_accuracy: 0.7601512738853503 

The current subspace-distance is: 0.00019147236889693886 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.12; acc: 0.67
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 1.1; acc: 0.67
Batch: 60; loss: 0.97; acc: 0.78
Batch: 80; loss: 1.05; acc: 0.69
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.12; acc: 0.77
Batch: 140; loss: 1.05; acc: 0.75
Batch: 160; loss: 1.1; acc: 0.75
Batch: 180; loss: 1.1; acc: 0.67
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 0.99; acc: 0.75
Batch: 240; loss: 1.13; acc: 0.73
Batch: 260; loss: 1.07; acc: 0.8
Batch: 280; loss: 0.93; acc: 0.81
Batch: 300; loss: 1.02; acc: 0.77
Batch: 320; loss: 0.87; acc: 0.83
Batch: 340; loss: 1.06; acc: 0.73
Batch: 360; loss: 1.07; acc: 0.78
Batch: 380; loss: 1.11; acc: 0.73
Batch: 400; loss: 1.0; acc: 0.75
Batch: 420; loss: 0.84; acc: 0.84
Batch: 440; loss: 0.91; acc: 0.81
Batch: 460; loss: 0.97; acc: 0.78
Batch: 480; loss: 1.11; acc: 0.73
Batch: 500; loss: 1.18; acc: 0.66
Batch: 520; loss: 1.05; acc: 0.7
Batch: 540; loss: 0.96; acc: 0.73
Batch: 560; loss: 1.07; acc: 0.67
Batch: 580; loss: 1.19; acc: 0.66
Batch: 600; loss: 0.95; acc: 0.8
Batch: 620; loss: 1.23; acc: 0.61
Batch: 640; loss: 1.03; acc: 0.78
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.12; acc: 0.69
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.04; acc: 0.75
Batch: 760; loss: 1.13; acc: 0.67
Batch: 780; loss: 1.05; acc: 0.7
Train Epoch over. train_loss: 1.07; train_accuracy: 0.73 

0.00020219149882905185
0.0001938563509611413
Batch: 0; loss: 1.14; acc: 0.67
Batch: 20; loss: 1.22; acc: 0.64
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.85; acc: 0.88
Batch: 100; loss: 1.13; acc: 0.66
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.0045519915355998; val_accuracy: 0.7634355095541401 

The current subspace-distance is: 0.0001938563509611413 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 1.29; acc: 0.64
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.69
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.67
Batch: 160; loss: 1.07; acc: 0.73
Batch: 180; loss: 1.05; acc: 0.8
Batch: 200; loss: 0.95; acc: 0.78
Batch: 220; loss: 1.04; acc: 0.75
Batch: 240; loss: 1.1; acc: 0.7
Batch: 260; loss: 1.06; acc: 0.73
Batch: 280; loss: 1.13; acc: 0.67
Batch: 300; loss: 1.04; acc: 0.72
Batch: 320; loss: 1.15; acc: 0.69
Batch: 340; loss: 0.94; acc: 0.81
Batch: 360; loss: 1.02; acc: 0.77
Batch: 380; loss: 1.22; acc: 0.64
Batch: 400; loss: 1.06; acc: 0.77
Batch: 420; loss: 1.08; acc: 0.75
Batch: 440; loss: 1.1; acc: 0.67
Batch: 460; loss: 1.09; acc: 0.77
Batch: 480; loss: 1.11; acc: 0.75
Batch: 500; loss: 1.08; acc: 0.75
Batch: 520; loss: 0.98; acc: 0.72
Batch: 540; loss: 1.13; acc: 0.69
Batch: 560; loss: 1.02; acc: 0.84
Batch: 580; loss: 0.95; acc: 0.78
Batch: 600; loss: 1.24; acc: 0.64
Batch: 620; loss: 1.07; acc: 0.73
Batch: 640; loss: 1.14; acc: 0.64
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.12; acc: 0.78
Batch: 700; loss: 1.18; acc: 0.69
Batch: 720; loss: 1.05; acc: 0.73
Batch: 740; loss: 0.98; acc: 0.81
Batch: 760; loss: 1.09; acc: 0.75
Batch: 780; loss: 0.96; acc: 0.78
Train Epoch over. train_loss: 1.07; train_accuracy: 0.73 

0.00020029211009386927
0.00019005939248017967
Batch: 0; loss: 1.13; acc: 0.61
Batch: 20; loss: 1.21; acc: 0.59
Batch: 40; loss: 0.77; acc: 0.88
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.84
Batch: 100; loss: 1.13; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.89; acc: 0.81
Val Epoch over. val_loss: 0.998482093689548; val_accuracy: 0.7617436305732485 

The current subspace-distance is: 0.00019005939248017967 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.72
Batch: 20; loss: 1.08; acc: 0.8
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.78
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 1.09; acc: 0.8
Batch: 160; loss: 1.09; acc: 0.73
Batch: 180; loss: 0.96; acc: 0.8
Batch: 200; loss: 1.1; acc: 0.72
Batch: 220; loss: 1.09; acc: 0.72
Batch: 240; loss: 0.98; acc: 0.81
Batch: 260; loss: 1.08; acc: 0.78
Batch: 280; loss: 1.18; acc: 0.67
Batch: 300; loss: 1.03; acc: 0.75
Batch: 320; loss: 1.02; acc: 0.73
Batch: 340; loss: 1.07; acc: 0.72
Batch: 360; loss: 0.98; acc: 0.78
Batch: 380; loss: 1.06; acc: 0.69
Batch: 400; loss: 1.02; acc: 0.75
Batch: 420; loss: 1.13; acc: 0.7
Batch: 440; loss: 1.0; acc: 0.77
Batch: 460; loss: 1.24; acc: 0.67
Batch: 480; loss: 0.98; acc: 0.75
Batch: 500; loss: 1.15; acc: 0.77
Batch: 520; loss: 0.91; acc: 0.8
Batch: 540; loss: 1.11; acc: 0.72
Batch: 560; loss: 1.02; acc: 0.8
Batch: 580; loss: 1.08; acc: 0.73
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.13; acc: 0.69
Batch: 640; loss: 1.08; acc: 0.72
Batch: 660; loss: 1.18; acc: 0.66
Batch: 680; loss: 1.19; acc: 0.72
Batch: 700; loss: 1.11; acc: 0.77
Batch: 720; loss: 1.08; acc: 0.75
Batch: 740; loss: 1.11; acc: 0.73
Batch: 760; loss: 1.0; acc: 0.72
Batch: 780; loss: 1.02; acc: 0.73
Train Epoch over. train_loss: 1.07; train_accuracy: 0.73 

0.0002009596355492249
0.0001926972036017105
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 0.92; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.89
Batch: 100; loss: 1.11; acc: 0.67
Batch: 120; loss: 1.22; acc: 0.7
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 0.9964310492679571; val_accuracy: 0.7659235668789809 

The current subspace-distance is: 0.0001926972036017105 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_6_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.05

The number of parameters is: 275012

The number of individual parameters is:

17
306
17
17
25
39100
25
25
50
115000
50
50
64
115200
64
64
4096
64
640
10
64
64

nonzero elements in E: 55002395
elements in E: 55002400
fraction nonzero: 0.9999999090948759
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.37; acc: 0.12
Batch: 20; loss: 2.06; acc: 0.3
Batch: 40; loss: 2.11; acc: 0.25
Batch: 60; loss: 1.93; acc: 0.39
Batch: 80; loss: 1.77; acc: 0.5
Batch: 100; loss: 1.72; acc: 0.5
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.74; acc: 0.44
Batch: 160; loss: 1.7; acc: 0.58
Batch: 180; loss: 1.68; acc: 0.53
Batch: 200; loss: 1.57; acc: 0.72
Batch: 220; loss: 1.58; acc: 0.69
Batch: 240; loss: 1.52; acc: 0.64
Batch: 260; loss: 1.54; acc: 0.67
Batch: 280; loss: 1.46; acc: 0.69
Batch: 300; loss: 1.41; acc: 0.78
Batch: 320; loss: 1.44; acc: 0.61
Batch: 340; loss: 1.44; acc: 0.7
Batch: 360; loss: 1.52; acc: 0.66
Batch: 380; loss: 1.43; acc: 0.77
Batch: 400; loss: 1.63; acc: 0.53
Batch: 420; loss: 1.41; acc: 0.66
Batch: 440; loss: 1.39; acc: 0.69
Batch: 460; loss: 1.39; acc: 0.75
Batch: 480; loss: 1.27; acc: 0.77
Batch: 500; loss: 1.3; acc: 0.75
Batch: 520; loss: 1.48; acc: 0.61
Batch: 540; loss: 1.37; acc: 0.69
Batch: 560; loss: 1.3; acc: 0.72
Batch: 580; loss: 1.35; acc: 0.73
Batch: 600; loss: 1.33; acc: 0.7
Batch: 620; loss: 1.32; acc: 0.73
Batch: 640; loss: 1.33; acc: 0.67
Batch: 660; loss: 1.21; acc: 0.77
Batch: 680; loss: 1.27; acc: 0.67
Batch: 700; loss: 1.41; acc: 0.67
Batch: 720; loss: 1.32; acc: 0.67
Batch: 740; loss: 1.36; acc: 0.56
Batch: 760; loss: 1.27; acc: 0.8
Batch: 780; loss: 1.19; acc: 0.78
Train Epoch over. train_loss: 1.49; train_accuracy: 0.63 

6.839640263933688e-05
6.310739263426512e-05
Batch: 0; loss: 1.3; acc: 0.73
Batch: 20; loss: 1.53; acc: 0.47
Batch: 40; loss: 0.99; acc: 0.86
Batch: 60; loss: 1.19; acc: 0.78
Batch: 80; loss: 1.11; acc: 0.81
Batch: 100; loss: 1.24; acc: 0.73
Batch: 120; loss: 1.32; acc: 0.67
Batch: 140; loss: 1.11; acc: 0.81
Val Epoch over. val_loss: 1.221744299314584; val_accuracy: 0.7550756369426752 

The current subspace-distance is: 6.310739263426512e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.77
Batch: 20; loss: 1.21; acc: 0.75
Batch: 40; loss: 1.19; acc: 0.75
Batch: 60; loss: 1.22; acc: 0.77
Batch: 80; loss: 1.08; acc: 0.81
Batch: 100; loss: 1.24; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.75
Batch: 140; loss: 1.15; acc: 0.8
Batch: 160; loss: 1.17; acc: 0.77
Batch: 180; loss: 1.23; acc: 0.78
Batch: 200; loss: 1.23; acc: 0.77
Batch: 220; loss: 1.1; acc: 0.83
Batch: 240; loss: 1.13; acc: 0.78
Batch: 260; loss: 1.28; acc: 0.69
Batch: 280; loss: 1.22; acc: 0.66
Batch: 300; loss: 1.03; acc: 0.83
Batch: 320; loss: 1.14; acc: 0.75
Batch: 340; loss: 1.19; acc: 0.67
Batch: 360; loss: 1.19; acc: 0.69
Batch: 380; loss: 1.18; acc: 0.73
Batch: 400; loss: 1.26; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.66
Batch: 440; loss: 1.22; acc: 0.73
Batch: 460; loss: 1.02; acc: 0.8
Batch: 480; loss: 1.14; acc: 0.77
Batch: 500; loss: 1.12; acc: 0.78
Batch: 520; loss: 1.03; acc: 0.8
Batch: 540; loss: 1.09; acc: 0.8
Batch: 560; loss: 1.08; acc: 0.81
Batch: 580; loss: 1.02; acc: 0.83
Batch: 600; loss: 1.1; acc: 0.75
Batch: 620; loss: 1.14; acc: 0.75
Batch: 640; loss: 1.17; acc: 0.72
Batch: 660; loss: 1.0; acc: 0.78
Batch: 680; loss: 1.02; acc: 0.81
Batch: 700; loss: 1.2; acc: 0.69
Batch: 720; loss: 1.13; acc: 0.77
Batch: 740; loss: 1.11; acc: 0.72
Batch: 760; loss: 1.1; acc: 0.69
Batch: 780; loss: 0.97; acc: 0.8
Train Epoch over. train_loss: 1.15; train_accuracy: 0.75 

9.350873733637854e-05
8.854480256559327e-05
Batch: 0; loss: 1.11; acc: 0.78
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 0.76; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.86
Batch: 100; loss: 1.03; acc: 0.83
Batch: 120; loss: 1.12; acc: 0.78
Batch: 140; loss: 0.9; acc: 0.89
Val Epoch over. val_loss: 1.0270854713051183; val_accuracy: 0.7971735668789809 

The current subspace-distance is: 8.854480256559327e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.73
Batch: 20; loss: 1.07; acc: 0.77
Batch: 40; loss: 1.16; acc: 0.7
Batch: 60; loss: 1.06; acc: 0.8
Batch: 80; loss: 1.11; acc: 0.7
Batch: 100; loss: 0.96; acc: 0.86
Batch: 120; loss: 1.04; acc: 0.77
Batch: 140; loss: 1.04; acc: 0.8
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 1.03; acc: 0.81
Batch: 200; loss: 1.0; acc: 0.77
Batch: 220; loss: 1.07; acc: 0.77
Batch: 240; loss: 1.09; acc: 0.67
Batch: 260; loss: 1.05; acc: 0.75
Batch: 280; loss: 0.97; acc: 0.84
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 1.13; acc: 0.73
Batch: 340; loss: 1.05; acc: 0.8
Batch: 360; loss: 1.02; acc: 0.8
Batch: 380; loss: 0.97; acc: 0.8
Batch: 400; loss: 0.94; acc: 0.86
Batch: 420; loss: 1.11; acc: 0.73
Batch: 440; loss: 0.96; acc: 0.89
Batch: 460; loss: 1.08; acc: 0.77
Batch: 480; loss: 1.03; acc: 0.72
Batch: 500; loss: 1.04; acc: 0.77
Batch: 520; loss: 1.0; acc: 0.8
Batch: 540; loss: 1.1; acc: 0.75
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 1.09; acc: 0.73
Batch: 600; loss: 0.95; acc: 0.81
Batch: 620; loss: 0.89; acc: 0.88
Batch: 640; loss: 0.99; acc: 0.8
Batch: 660; loss: 1.01; acc: 0.8
Batch: 680; loss: 0.99; acc: 0.78
Batch: 700; loss: 0.99; acc: 0.75
Batch: 720; loss: 1.04; acc: 0.78
Batch: 740; loss: 0.92; acc: 0.8
Batch: 760; loss: 0.84; acc: 0.86
Batch: 780; loss: 1.1; acc: 0.7
Train Epoch over. train_loss: 1.03; train_accuracy: 0.78 

0.00011207799252588302
0.00010610609024297446
Batch: 0; loss: 1.04; acc: 0.78
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.71; acc: 0.88
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.85; acc: 0.86
Batch: 100; loss: 0.96; acc: 0.89
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 0.88; acc: 0.88
Val Epoch over. val_loss: 0.9663741721469126; val_accuracy: 0.8085191082802548 

The current subspace-distance is: 0.00010610609024297446 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.88
Batch: 20; loss: 0.84; acc: 0.89
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 1.07; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.78
Batch: 140; loss: 0.93; acc: 0.8
Batch: 160; loss: 0.95; acc: 0.77
Batch: 180; loss: 0.89; acc: 0.83
Batch: 200; loss: 0.96; acc: 0.78
Batch: 220; loss: 1.04; acc: 0.8
Batch: 240; loss: 0.96; acc: 0.78
Batch: 260; loss: 0.97; acc: 0.83
Batch: 280; loss: 1.05; acc: 0.8
Batch: 300; loss: 0.91; acc: 0.84
Batch: 320; loss: 0.89; acc: 0.88
Batch: 340; loss: 0.98; acc: 0.8
Batch: 360; loss: 0.92; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.88
Batch: 400; loss: 0.92; acc: 0.78
Batch: 420; loss: 0.98; acc: 0.75
Batch: 440; loss: 1.07; acc: 0.78
Batch: 460; loss: 0.87; acc: 0.86
Batch: 480; loss: 0.81; acc: 0.84
Batch: 500; loss: 0.82; acc: 0.88
Batch: 520; loss: 1.0; acc: 0.81
Batch: 540; loss: 0.98; acc: 0.81
Batch: 560; loss: 1.06; acc: 0.77
Batch: 580; loss: 0.93; acc: 0.83
Batch: 600; loss: 1.15; acc: 0.7
Batch: 620; loss: 1.01; acc: 0.81
Batch: 640; loss: 0.96; acc: 0.83
Batch: 660; loss: 0.88; acc: 0.84
Batch: 680; loss: 1.02; acc: 0.75
Batch: 700; loss: 1.07; acc: 0.72
Batch: 720; loss: 1.08; acc: 0.69
Batch: 740; loss: 1.03; acc: 0.78
Batch: 760; loss: 1.1; acc: 0.67
Batch: 780; loss: 0.99; acc: 0.8
Train Epoch over. train_loss: 0.98; train_accuracy: 0.79 

0.0001261195429833606
0.00012075149425072595
Batch: 0; loss: 1.0; acc: 0.78
Batch: 20; loss: 1.13; acc: 0.66
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.9; acc: 0.77
Batch: 80; loss: 0.8; acc: 0.86
Batch: 100; loss: 0.91; acc: 0.86
Batch: 120; loss: 1.02; acc: 0.77
Batch: 140; loss: 0.83; acc: 0.91
Val Epoch over. val_loss: 0.9141145253637034; val_accuracy: 0.818172770700637 

The current subspace-distance is: 0.00012075149425072595 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.88
Batch: 40; loss: 1.01; acc: 0.78
Batch: 60; loss: 0.84; acc: 0.88
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 1.04; acc: 0.7
Batch: 160; loss: 0.74; acc: 0.91
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 0.99; acc: 0.72
Batch: 220; loss: 0.93; acc: 0.81
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.03; acc: 0.77
Batch: 280; loss: 0.99; acc: 0.77
Batch: 300; loss: 0.89; acc: 0.77
Batch: 320; loss: 0.83; acc: 0.88
Batch: 340; loss: 0.99; acc: 0.81
Batch: 360; loss: 0.88; acc: 0.88
Batch: 380; loss: 0.98; acc: 0.78
Batch: 400; loss: 0.88; acc: 0.81
Batch: 420; loss: 0.96; acc: 0.84
Batch: 440; loss: 0.98; acc: 0.78
Batch: 460; loss: 1.01; acc: 0.78
Batch: 480; loss: 0.98; acc: 0.78
Batch: 500; loss: 1.04; acc: 0.73
Batch: 520; loss: 0.91; acc: 0.86
Batch: 540; loss: 0.94; acc: 0.77
Batch: 560; loss: 0.92; acc: 0.8
Batch: 580; loss: 0.84; acc: 0.77
Batch: 600; loss: 0.99; acc: 0.8
Batch: 620; loss: 0.9; acc: 0.8
Batch: 640; loss: 0.95; acc: 0.81
Batch: 660; loss: 0.98; acc: 0.8
Batch: 680; loss: 0.86; acc: 0.86
Batch: 700; loss: 1.05; acc: 0.7
Batch: 720; loss: 0.82; acc: 0.83
Batch: 740; loss: 0.83; acc: 0.81
Batch: 760; loss: 0.93; acc: 0.81
Batch: 780; loss: 0.96; acc: 0.86
Train Epoch over. train_loss: 0.93; train_accuracy: 0.8 

0.0001392969861626625
0.00013388613297138363
Batch: 0; loss: 0.92; acc: 0.88
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 0.58; acc: 0.91
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.74; acc: 0.94
Batch: 100; loss: 0.84; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 0.75; acc: 0.91
Val Epoch over. val_loss: 0.8549685185881937; val_accuracy: 0.8336982484076433 

The current subspace-distance is: 0.00013388613297138363 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.84
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 1.03; acc: 0.73
Batch: 60; loss: 0.79; acc: 0.86
Batch: 80; loss: 1.15; acc: 0.64
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 0.99; acc: 0.81
Batch: 140; loss: 0.95; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.88
Batch: 180; loss: 0.79; acc: 0.88
Batch: 200; loss: 0.76; acc: 0.88
Batch: 220; loss: 0.97; acc: 0.83
Batch: 240; loss: 0.88; acc: 0.78
Batch: 260; loss: 0.82; acc: 0.86
Batch: 280; loss: 0.99; acc: 0.8
Batch: 300; loss: 0.85; acc: 0.86
Batch: 320; loss: 0.8; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.89
Batch: 360; loss: 0.81; acc: 0.89
Batch: 380; loss: 0.93; acc: 0.8
Batch: 400; loss: 0.83; acc: 0.83
Batch: 420; loss: 0.93; acc: 0.81
Batch: 440; loss: 0.91; acc: 0.77
Batch: 460; loss: 0.81; acc: 0.84
Batch: 480; loss: 0.86; acc: 0.83
Batch: 500; loss: 0.9; acc: 0.81
Batch: 520; loss: 0.8; acc: 0.84
Batch: 540; loss: 0.87; acc: 0.83
Batch: 560; loss: 0.82; acc: 0.84
Batch: 580; loss: 1.08; acc: 0.73
Batch: 600; loss: 0.82; acc: 0.86
Batch: 620; loss: 0.93; acc: 0.78
Batch: 640; loss: 0.97; acc: 0.78
Batch: 660; loss: 0.77; acc: 0.81
Batch: 680; loss: 0.83; acc: 0.84
Batch: 700; loss: 0.76; acc: 0.89
Batch: 720; loss: 0.83; acc: 0.86
Batch: 740; loss: 0.73; acc: 0.89
Batch: 760; loss: 0.84; acc: 0.81
Batch: 780; loss: 0.82; acc: 0.83
Train Epoch over. train_loss: 0.88; train_accuracy: 0.81 

0.00015340511163230985
0.00015004313900135458
Batch: 0; loss: 0.86; acc: 0.83
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.8
Batch: 140; loss: 0.67; acc: 0.91
Val Epoch over. val_loss: 0.7997338164384198; val_accuracy: 0.8388734076433121 

The current subspace-distance is: 0.00015004313900135458 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.91
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.83; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.86
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 0.86; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.92; acc: 0.81
Batch: 160; loss: 0.92; acc: 0.72
Batch: 180; loss: 0.7; acc: 0.94
Batch: 200; loss: 0.93; acc: 0.78
Batch: 220; loss: 0.89; acc: 0.75
Batch: 240; loss: 0.73; acc: 0.89
Batch: 260; loss: 0.86; acc: 0.81
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 0.82; acc: 0.84
Batch: 320; loss: 0.83; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.86
Batch: 360; loss: 0.88; acc: 0.8
Batch: 380; loss: 0.85; acc: 0.81
Batch: 400; loss: 0.85; acc: 0.83
Batch: 420; loss: 0.81; acc: 0.83
Batch: 440; loss: 0.71; acc: 0.88
Batch: 460; loss: 0.99; acc: 0.8
Batch: 480; loss: 0.87; acc: 0.8
Batch: 500; loss: 0.93; acc: 0.77
Batch: 520; loss: 0.74; acc: 0.86
Batch: 540; loss: 0.82; acc: 0.84
Batch: 560; loss: 0.81; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.81; acc: 0.84
Batch: 620; loss: 0.67; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.91
Batch: 660; loss: 0.88; acc: 0.78
Batch: 680; loss: 0.79; acc: 0.83
Batch: 700; loss: 0.9; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.81
Batch: 740; loss: 0.82; acc: 0.81
Batch: 760; loss: 0.96; acc: 0.77
Batch: 780; loss: 0.83; acc: 0.88
Train Epoch over. train_loss: 0.84; train_accuracy: 0.82 

0.00016494955343659967
0.00015873729716986418
Batch: 0; loss: 0.77; acc: 0.89
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.48; acc: 0.94
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.91
Batch: 100; loss: 0.75; acc: 0.89
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.91
Val Epoch over. val_loss: 0.7493476757578029; val_accuracy: 0.8432523885350318 

The current subspace-distance is: 0.00015873729716986418 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.75
Batch: 20; loss: 0.79; acc: 0.84
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.83
Batch: 80; loss: 0.85; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.88; acc: 0.8
Batch: 180; loss: 0.79; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.91
Batch: 220; loss: 0.72; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.94
Batch: 260; loss: 0.77; acc: 0.8
Batch: 280; loss: 0.84; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.88; acc: 0.86
Batch: 360; loss: 0.69; acc: 0.89
Batch: 380; loss: 0.82; acc: 0.84
Batch: 400; loss: 0.87; acc: 0.81
Batch: 420; loss: 0.84; acc: 0.75
Batch: 440; loss: 0.87; acc: 0.8
Batch: 460; loss: 0.66; acc: 0.91
Batch: 480; loss: 0.88; acc: 0.75
Batch: 500; loss: 0.85; acc: 0.83
Batch: 520; loss: 0.87; acc: 0.75
Batch: 540; loss: 0.73; acc: 0.88
Batch: 560; loss: 0.72; acc: 0.88
Batch: 580; loss: 0.84; acc: 0.78
Batch: 600; loss: 0.73; acc: 0.86
Batch: 620; loss: 0.71; acc: 0.88
Batch: 640; loss: 0.89; acc: 0.78
Batch: 660; loss: 0.73; acc: 0.86
Batch: 680; loss: 0.9; acc: 0.77
Batch: 700; loss: 0.87; acc: 0.75
Batch: 720; loss: 0.81; acc: 0.81
Batch: 740; loss: 0.67; acc: 0.89
Batch: 760; loss: 0.82; acc: 0.81
Batch: 780; loss: 0.82; acc: 0.84
Train Epoch over. train_loss: 0.8; train_accuracy: 0.82 

0.00017823823145590723
0.00017442468379158527
Batch: 0; loss: 0.75; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.94
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.7199900049692506; val_accuracy: 0.8524084394904459 

The current subspace-distance is: 0.00017442468379158527 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.92
Batch: 80; loss: 0.78; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.81; acc: 0.8
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.77; acc: 0.83
Batch: 200; loss: 0.69; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.94
Batch: 240; loss: 0.74; acc: 0.86
Batch: 260; loss: 0.78; acc: 0.84
Batch: 280; loss: 0.81; acc: 0.86
Batch: 300; loss: 0.83; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.84
Batch: 340; loss: 0.75; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.89; acc: 0.77
Batch: 400; loss: 0.82; acc: 0.84
Batch: 420; loss: 0.94; acc: 0.77
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.82; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.79; acc: 0.83
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.88
Batch: 640; loss: 0.69; acc: 0.86
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.71; acc: 0.89
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.54; acc: 0.97
Batch: 740; loss: 0.86; acc: 0.72
Batch: 760; loss: 0.72; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.83
Train Epoch over. train_loss: 0.77; train_accuracy: 0.83 

0.0001893655426101759
0.00018487365741748363
Batch: 0; loss: 0.71; acc: 0.91
Batch: 20; loss: 0.76; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.91
Val Epoch over. val_loss: 0.6823553313874895; val_accuracy: 0.8564888535031847 

The current subspace-distance is: 0.00018487365741748363 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.84; acc: 0.81
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.88
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.8; acc: 0.81
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.78; acc: 0.88
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.92
Batch: 400; loss: 0.64; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.73; acc: 0.86
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.87; acc: 0.73
Batch: 580; loss: 0.77; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.9; acc: 0.69
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.81; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.77
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.73; acc: 0.83
Batch: 740; loss: 0.88; acc: 0.77
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.0001983480469789356
0.00019249880278948694
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.94
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6658031796194186; val_accuracy: 0.8580812101910829 

The current subspace-distance is: 0.00019249880278948694 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.84
Batch: 40; loss: 0.71; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.72; acc: 0.81
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.88; acc: 0.72
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.81; acc: 0.75
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.66; acc: 0.89
Batch: 320; loss: 0.71; acc: 0.81
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.93; acc: 0.7
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.81; acc: 0.83
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.69; acc: 0.89
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.81
Batch: 620; loss: 0.88; acc: 0.73
Batch: 640; loss: 0.82; acc: 0.75
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.72; acc: 0.78
Batch: 700; loss: 0.71; acc: 0.83
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.67; acc: 0.88
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.00020033528562635183
0.0001939507492352277
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.94
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6578278334657098; val_accuracy: 0.8611664012738853 

The current subspace-distance is: 0.0001939507492352277 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.88
Batch: 160; loss: 0.7; acc: 0.86
Batch: 180; loss: 0.68; acc: 0.84
Batch: 200; loss: 0.82; acc: 0.77
Batch: 220; loss: 0.92; acc: 0.72
Batch: 240; loss: 0.9; acc: 0.8
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.8; acc: 0.75
Batch: 300; loss: 0.74; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.89
Batch: 340; loss: 0.86; acc: 0.78
Batch: 360; loss: 0.75; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.86
Batch: 480; loss: 0.87; acc: 0.8
Batch: 500; loss: 0.77; acc: 0.86
Batch: 520; loss: 0.61; acc: 0.88
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.85; acc: 0.73
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.7; acc: 0.91
Batch: 620; loss: 0.82; acc: 0.7
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.7; acc: 0.86
Batch: 680; loss: 0.87; acc: 0.72
Batch: 700; loss: 0.58; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020330832921899855
0.000199575224542059
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.94
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.6378361214498046; val_accuracy: 0.8637539808917197 

The current subspace-distance is: 0.000199575224542059 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.91
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 0.72; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.68; acc: 0.89
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.77; acc: 0.8
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.85; acc: 0.78
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.71; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.68; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.88
Batch: 420; loss: 0.73; acc: 0.84
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 1.0; acc: 0.7
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.8; acc: 0.8
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.66; acc: 0.88
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.86
Batch: 660; loss: 0.78; acc: 0.83
Batch: 680; loss: 0.79; acc: 0.78
Batch: 700; loss: 0.77; acc: 0.8
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.75; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.88
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00020818946359213442
0.00020192623196635395
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.47; acc: 0.98
Val Epoch over. val_loss: 0.6418536882491628; val_accuracy: 0.862062101910828 

The current subspace-distance is: 0.00020192623196635395 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.9; acc: 0.72
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.96; acc: 0.75
Batch: 120; loss: 0.59; acc: 0.92
Batch: 140; loss: 0.84; acc: 0.7
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.79; acc: 0.81
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.88; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.89
Batch: 260; loss: 0.78; acc: 0.73
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.78
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.75; acc: 0.84
Batch: 520; loss: 0.72; acc: 0.83
Batch: 540; loss: 0.81; acc: 0.75
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.75; acc: 0.83
Batch: 760; loss: 0.98; acc: 0.69
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00021120754536241293
0.0002035497163888067
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.6219954572285816; val_accuracy: 0.8657444267515924 

The current subspace-distance is: 0.0002035497163888067 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.94; acc: 0.78
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.67; acc: 0.88
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.62; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.78; acc: 0.86
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.79; acc: 0.84
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.73; acc: 0.77
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.87; acc: 0.8
Batch: 720; loss: 0.57; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00021241564536467195
0.00020380517526064068
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.94
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.617689425208766; val_accuracy: 0.8641520700636943 

The current subspace-distance is: 0.00020380517526064068 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.92
Batch: 180; loss: 0.81; acc: 0.75
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.74; acc: 0.73
Batch: 280; loss: 0.79; acc: 0.72
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.86; acc: 0.75
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.83; acc: 0.78
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.73; acc: 0.81
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.77; acc: 0.8
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.88
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00021467295300681144
0.00020781822968274355
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.95
Val Epoch over. val_loss: 0.6222886052101281; val_accuracy: 0.8643511146496815 

The current subspace-distance is: 0.00020781822968274355 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.6; acc: 0.89
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.94
Batch: 220; loss: 0.74; acc: 0.77
Batch: 240; loss: 0.56; acc: 0.91
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.74; acc: 0.83
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.74; acc: 0.81
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.75; acc: 0.72
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.78
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.81; acc: 0.78
Batch: 740; loss: 0.55; acc: 0.91
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.73; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00021665860549546778
0.00021198608737904578
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.98
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.94
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.6148169314026073; val_accuracy: 0.866640127388535 

The current subspace-distance is: 0.00021198608737904578 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.95; acc: 0.75
Batch: 160; loss: 0.65; acc: 0.89
Batch: 180; loss: 0.81; acc: 0.77
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.87; acc: 0.72
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.8
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.81; acc: 0.83
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.74; acc: 0.86
Batch: 420; loss: 0.74; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.76; acc: 0.83
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.86; acc: 0.72
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.64; acc: 0.89
Batch: 620; loss: 0.67; acc: 0.81
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.89
Batch: 720; loss: 0.77; acc: 0.8
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00021748911240138113
0.0002122025762218982
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.94
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.95
Val Epoch over. val_loss: 0.6005261092428948; val_accuracy: 0.8657444267515924 

The current subspace-distance is: 0.0002122025762218982 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.85; acc: 0.73
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.76; acc: 0.84
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 0.66; acc: 0.89
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.73; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.85; acc: 0.75
Batch: 320; loss: 0.65; acc: 0.84
Batch: 340; loss: 0.81; acc: 0.72
Batch: 360; loss: 0.87; acc: 0.77
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.67; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.55; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00022221387189347297
0.00021568893862422556
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.95
Val Epoch over. val_loss: 0.5915070656378558; val_accuracy: 0.8660429936305732 

The current subspace-distance is: 0.00021568893862422556 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.77
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.81; acc: 0.77
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.77; acc: 0.8
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.78
Batch: 300; loss: 0.56; acc: 0.92
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.81
Batch: 400; loss: 0.74; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.76; acc: 0.77
Batch: 460; loss: 0.53; acc: 0.94
Batch: 480; loss: 0.68; acc: 0.78
Batch: 500; loss: 0.67; acc: 0.8
Batch: 520; loss: 0.83; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.69; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.89; acc: 0.73
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.77; acc: 0.77
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00022289779735729098
0.0002161501324735582
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.94
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.98
Val Epoch over. val_loss: 0.5820904937899036; val_accuracy: 0.8681329617834395 

The current subspace-distance is: 0.0002161501324735582 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.89; acc: 0.73
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.7; acc: 0.77
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.95
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.86
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.82; acc: 0.81
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.72; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.8
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.73; acc: 0.78
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.72; acc: 0.78
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.83; acc: 0.69
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.0002263141650473699
0.0002184876793762669
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.98
Val Epoch over. val_loss: 0.5936966631442878; val_accuracy: 0.863953025477707 

The current subspace-distance is: 0.0002184876793762669 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.8; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.53; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.67; acc: 0.81
Batch: 320; loss: 0.82; acc: 0.73
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.83; acc: 0.73
Batch: 440; loss: 0.49; acc: 0.92
Batch: 460; loss: 0.79; acc: 0.78
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.92
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.71; acc: 0.86
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.0002257551677757874
0.00021831828053109348
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.5883722068018215; val_accuracy: 0.8654458598726115 

The current subspace-distance is: 0.00021831828053109348 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.91
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.74; acc: 0.78
Batch: 260; loss: 0.67; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.87; acc: 0.72
Batch: 420; loss: 0.62; acc: 0.75
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.91
Batch: 500; loss: 0.7; acc: 0.8
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.73; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.57; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00022925225493963808
0.0002207939833169803
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.58546187220865; val_accuracy: 0.863156847133758 

The current subspace-distance is: 0.0002207939833169803 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.55; acc: 0.94
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.55; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.92
Batch: 200; loss: 0.75; acc: 0.8
Batch: 220; loss: 0.84; acc: 0.69
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.8; acc: 0.78
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.74; acc: 0.73
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.72; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.84
Batch: 620; loss: 0.56; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.81
Batch: 680; loss: 0.83; acc: 0.77
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.74; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00023021311790216714
0.00022328420891426504
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.98
Val Epoch over. val_loss: 0.5921807330884751; val_accuracy: 0.8651472929936306 

The current subspace-distance is: 0.00022328420891426504 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.91
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.63; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.92
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.83
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.91
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.74; acc: 0.8
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.7; acc: 0.84
Batch: 700; loss: 0.88; acc: 0.8
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.75; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00022771884687244892
0.00022030445688869804
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.3; acc: 1.0
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.94
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.5775471371450241; val_accuracy: 0.8665406050955414 

The current subspace-distance is: 0.00022030445688869804 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.95
Batch: 220; loss: 0.84; acc: 0.77
Batch: 240; loss: 0.79; acc: 0.77
Batch: 260; loss: 0.73; acc: 0.72
Batch: 280; loss: 0.51; acc: 0.92
Batch: 300; loss: 0.63; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.7; acc: 0.78
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.76; acc: 0.75
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.95
Batch: 520; loss: 0.74; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.83
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.74; acc: 0.83
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00023050433082971722
0.00022161203378345817
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.98
Val Epoch over. val_loss: 0.584908359749302; val_accuracy: 0.8652468152866242 

The current subspace-distance is: 0.00022161203378345817 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.7; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.76; acc: 0.8
Batch: 480; loss: 0.82; acc: 0.75
Batch: 500; loss: 0.7; acc: 0.78
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.92
Batch: 560; loss: 0.66; acc: 0.88
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.83
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.0002301511849509552
0.00022394473489839584
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.29; acc: 1.0
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.4; acc: 0.98
Val Epoch over. val_loss: 0.5674416513959314; val_accuracy: 0.8689291401273885 

The current subspace-distance is: 0.00022394473489839584 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.85; acc: 0.75
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.54; acc: 0.92
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.66; acc: 0.88
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.72; acc: 0.75
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.92
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00023049776791594923
0.00022219610400497913
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.3; acc: 1.0
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.5791844104885295; val_accuracy: 0.8664410828025477 

The current subspace-distance is: 0.00022219610400497913 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.69; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.81; acc: 0.75
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.7; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.92; acc: 0.69
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.74; acc: 0.78
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00023094638891052455
0.0002215482818428427
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.4; acc: 0.97
Val Epoch over. val_loss: 0.5695954631468293; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 0.0002215482818428427 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.91
Batch: 160; loss: 0.65; acc: 0.78
Batch: 180; loss: 0.83; acc: 0.77
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.67; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.65; acc: 0.77
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.73; acc: 0.84
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.67; acc: 0.89
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.81
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.74; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00023389997659251094
0.00022578808420803398
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.95
Val Epoch over. val_loss: 0.5728015265647014; val_accuracy: 0.8652468152866242 

The current subspace-distance is: 0.00022578808420803398 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_6_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.05

The number of parameters is: 275012

The number of individual parameters is:

17
306
17
17
25
39100
25
25
50
115000
50
50
64
115200
64
64
4096
64
640
10
64
64

nonzero elements in E: 82503593
elements in E: 82503600
fraction nonzero: 0.9999999151552175
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.36; acc: 0.14
Batch: 20; loss: 2.14; acc: 0.2
Batch: 40; loss: 1.96; acc: 0.45
Batch: 60; loss: 1.73; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.5
Batch: 100; loss: 1.58; acc: 0.67
Batch: 120; loss: 1.64; acc: 0.59
Batch: 140; loss: 1.55; acc: 0.58
Batch: 160; loss: 1.59; acc: 0.67
Batch: 180; loss: 1.57; acc: 0.61
Batch: 200; loss: 1.35; acc: 0.8
Batch: 220; loss: 1.43; acc: 0.75
Batch: 240; loss: 1.45; acc: 0.69
Batch: 260; loss: 1.32; acc: 0.78
Batch: 280; loss: 1.32; acc: 0.72
Batch: 300; loss: 1.29; acc: 0.81
Batch: 320; loss: 1.4; acc: 0.69
Batch: 340; loss: 1.38; acc: 0.73
Batch: 360; loss: 1.47; acc: 0.62
Batch: 380; loss: 1.32; acc: 0.7
Batch: 400; loss: 1.26; acc: 0.7
Batch: 420; loss: 1.24; acc: 0.72
Batch: 440; loss: 1.31; acc: 0.7
Batch: 460; loss: 1.36; acc: 0.7
Batch: 480; loss: 1.21; acc: 0.75
Batch: 500; loss: 1.2; acc: 0.84
Batch: 520; loss: 1.2; acc: 0.8
Batch: 540; loss: 1.15; acc: 0.83
Batch: 560; loss: 1.18; acc: 0.72
Batch: 580; loss: 1.23; acc: 0.77
Batch: 600; loss: 1.21; acc: 0.73
Batch: 620; loss: 1.12; acc: 0.83
Batch: 640; loss: 1.11; acc: 0.81
Batch: 660; loss: 1.21; acc: 0.7
Batch: 680; loss: 1.09; acc: 0.8
Batch: 700; loss: 1.15; acc: 0.73
Batch: 720; loss: 1.03; acc: 0.86
Batch: 740; loss: 1.33; acc: 0.61
Batch: 760; loss: 1.08; acc: 0.8
Batch: 780; loss: 1.08; acc: 0.77
Train Epoch over. train_loss: 1.37; train_accuracy: 0.69 

6.552712147822604e-05
6.018587373546325e-05
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.24; acc: 0.67
Batch: 40; loss: 0.83; acc: 0.95
Batch: 60; loss: 1.0; acc: 0.86
Batch: 80; loss: 0.96; acc: 0.83
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.89; acc: 0.92
Val Epoch over. val_loss: 1.04630780182067; val_accuracy: 0.8078224522292994 

The current subspace-distance is: 6.018587373546325e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.3; acc: 0.72
Batch: 40; loss: 0.95; acc: 0.84
Batch: 60; loss: 0.94; acc: 0.89
Batch: 80; loss: 0.99; acc: 0.84
Batch: 100; loss: 0.97; acc: 0.86
Batch: 120; loss: 1.03; acc: 0.8
Batch: 140; loss: 1.08; acc: 0.78
Batch: 160; loss: 0.99; acc: 0.86
Batch: 180; loss: 1.07; acc: 0.78
Batch: 200; loss: 0.97; acc: 0.83
Batch: 220; loss: 0.93; acc: 0.84
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 0.97; acc: 0.86
Batch: 280; loss: 0.98; acc: 0.81
Batch: 300; loss: 0.84; acc: 0.95
Batch: 320; loss: 1.06; acc: 0.78
Batch: 340; loss: 0.83; acc: 0.86
Batch: 360; loss: 0.9; acc: 0.86
Batch: 380; loss: 1.01; acc: 0.83
Batch: 400; loss: 0.93; acc: 0.89
Batch: 420; loss: 0.87; acc: 0.8
Batch: 440; loss: 0.98; acc: 0.83
Batch: 460; loss: 1.03; acc: 0.83
Batch: 480; loss: 0.87; acc: 0.86
Batch: 500; loss: 1.07; acc: 0.72
Batch: 520; loss: 0.84; acc: 0.91
Batch: 540; loss: 1.11; acc: 0.73
Batch: 560; loss: 0.93; acc: 0.84
Batch: 580; loss: 0.94; acc: 0.83
Batch: 600; loss: 0.94; acc: 0.83
Batch: 620; loss: 1.11; acc: 0.69
Batch: 640; loss: 0.86; acc: 0.89
Batch: 660; loss: 1.05; acc: 0.7
Batch: 680; loss: 0.8; acc: 0.91
Batch: 700; loss: 0.88; acc: 0.84
Batch: 720; loss: 1.08; acc: 0.73
Batch: 740; loss: 0.94; acc: 0.78
Batch: 760; loss: 0.8; acc: 0.89
Batch: 780; loss: 0.83; acc: 0.88
Train Epoch over. train_loss: 0.98; train_accuracy: 0.81 

8.989315392682329e-05
8.531265484634787e-05
Batch: 0; loss: 0.88; acc: 0.86
Batch: 20; loss: 1.04; acc: 0.7
Batch: 40; loss: 0.61; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.91
Batch: 80; loss: 0.74; acc: 0.91
Batch: 100; loss: 0.88; acc: 0.84
Batch: 120; loss: 1.04; acc: 0.7
Batch: 140; loss: 0.64; acc: 0.97
Val Epoch over. val_loss: 0.8343319315819224; val_accuracy: 0.8540007961783439 

The current subspace-distance is: 8.531265484634787e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.77
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.87; acc: 0.86
Batch: 60; loss: 0.97; acc: 0.81
Batch: 80; loss: 1.0; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.83
Batch: 140; loss: 0.99; acc: 0.77
Batch: 160; loss: 0.86; acc: 0.83
Batch: 180; loss: 0.88; acc: 0.8
Batch: 200; loss: 0.88; acc: 0.89
Batch: 220; loss: 0.87; acc: 0.8
Batch: 240; loss: 0.82; acc: 0.83
Batch: 260; loss: 0.87; acc: 0.86
Batch: 280; loss: 0.9; acc: 0.81
Batch: 300; loss: 1.0; acc: 0.77
Batch: 320; loss: 0.86; acc: 0.8
Batch: 340; loss: 0.9; acc: 0.8
Batch: 360; loss: 0.78; acc: 0.81
Batch: 380; loss: 0.73; acc: 0.89
Batch: 400; loss: 0.81; acc: 0.86
Batch: 420; loss: 0.8; acc: 0.84
Batch: 440; loss: 0.89; acc: 0.78
Batch: 460; loss: 0.84; acc: 0.89
Batch: 480; loss: 0.83; acc: 0.83
Batch: 500; loss: 0.78; acc: 0.88
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 0.79; acc: 0.86
Batch: 560; loss: 0.8; acc: 0.91
Batch: 580; loss: 0.81; acc: 0.83
Batch: 600; loss: 0.73; acc: 0.89
Batch: 620; loss: 0.91; acc: 0.78
Batch: 640; loss: 0.91; acc: 0.8
Batch: 660; loss: 0.91; acc: 0.78
Batch: 680; loss: 0.84; acc: 0.83
Batch: 700; loss: 0.76; acc: 0.86
Batch: 720; loss: 0.79; acc: 0.89
Batch: 740; loss: 0.83; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.91
Batch: 780; loss: 0.96; acc: 0.73
Train Epoch over. train_loss: 0.84; train_accuracy: 0.83 

0.00010801975440699607
0.00010286540054949
Batch: 0; loss: 0.79; acc: 0.83
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 0.53; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.92
Batch: 80; loss: 0.61; acc: 0.92
Batch: 100; loss: 0.78; acc: 0.86
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.97
Val Epoch over. val_loss: 0.7263667222800528; val_accuracy: 0.870421974522293 

The current subspace-distance is: 0.00010286540054949 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.89
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.8
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.81
Batch: 140; loss: 0.81; acc: 0.81
Batch: 160; loss: 0.71; acc: 0.89
Batch: 180; loss: 0.87; acc: 0.77
Batch: 200; loss: 0.71; acc: 0.83
Batch: 220; loss: 0.82; acc: 0.81
Batch: 240; loss: 0.81; acc: 0.83
Batch: 260; loss: 0.77; acc: 0.81
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.75; acc: 0.83
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.86; acc: 0.81
Batch: 360; loss: 0.87; acc: 0.8
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.78
Batch: 440; loss: 0.73; acc: 0.89
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.86; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.74; acc: 0.83
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.84; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.89
Batch: 620; loss: 0.82; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.81
Batch: 660; loss: 0.85; acc: 0.8
Batch: 680; loss: 0.73; acc: 0.91
Batch: 700; loss: 0.65; acc: 0.89
Batch: 720; loss: 0.73; acc: 0.84
Batch: 740; loss: 0.71; acc: 0.91
Batch: 760; loss: 0.71; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.85 

0.0001237163960468024
0.00011739620822481811
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.91
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.97
Val Epoch over. val_loss: 0.6614382238524734; val_accuracy: 0.8769904458598726 

The current subspace-distance is: 0.00011739620822481811 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.67; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.88
Batch: 80; loss: 0.8; acc: 0.8
Batch: 100; loss: 0.77; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.89
Batch: 180; loss: 0.76; acc: 0.88
Batch: 200; loss: 0.75; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.94
Batch: 280; loss: 0.71; acc: 0.88
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.89
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.81; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.88
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.95
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.74; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.94
Batch: 600; loss: 0.66; acc: 0.88
Batch: 620; loss: 0.69; acc: 0.88
Batch: 640; loss: 0.77; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.98
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.74; acc: 0.88
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.7; train_accuracy: 0.86 

0.00013607318396680057
0.00013078706979285926
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.6188039777764849; val_accuracy: 0.8859474522292994 

The current subspace-distance is: 0.00013078706979285926 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.94
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.88
Batch: 140; loss: 0.72; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.89
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.65; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.94
Batch: 240; loss: 0.67; acc: 0.86
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.94
Batch: 320; loss: 0.65; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.65; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.97
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.77; acc: 0.83
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.74; acc: 0.8
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.94
Batch: 560; loss: 0.62; acc: 0.89
Batch: 580; loss: 0.66; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.89
Batch: 660; loss: 0.73; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.77
Batch: 700; loss: 0.73; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.89
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.86 

0.0001485515240347013
0.00014196182019077241
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.97
Val Epoch over. val_loss: 0.5719345327775189; val_accuracy: 0.8921178343949044 

The current subspace-distance is: 0.00014196182019077241 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.63; acc: 0.92
Batch: 40; loss: 0.64; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.91
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.92
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.57; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.64; acc: 0.89
Batch: 400; loss: 0.75; acc: 0.77
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.8; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.64; acc: 0.92
Batch: 680; loss: 0.62; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.48; acc: 0.94
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.87 

0.0001597358495928347
0.00015297000936698169
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.5451184833884999; val_accuracy: 0.8958996815286624 

The current subspace-distance is: 0.00015297000936698169 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.94
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.61; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.97
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.94
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.59; train_accuracy: 0.87 

0.0001711438671918586
0.0001636270753806457
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.5152229885007166; val_accuracy: 0.8993829617834395 

The current subspace-distance is: 0.0001636270753806457 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.95
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.91
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.94
Batch: 660; loss: 0.55; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.91
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.56; train_accuracy: 0.88 

0.00017927135922946036
0.00017307531379628927
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.4974922839623348; val_accuracy: 0.8990843949044586 

The current subspace-distance is: 0.00017307531379628927 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.92
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.92
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 0.37; acc: 0.95
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.92
Batch: 400; loss: 0.63; acc: 0.78
Batch: 420; loss: 0.42; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.73; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.95
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.00018946117779705673
0.00018340323003940284
Batch: 0; loss: 0.42; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.46240308728947; val_accuracy: 0.9025676751592356 

The current subspace-distance is: 0.00018340323003940284 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.92
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.98
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.56; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0001923244126373902
0.00018620578339323401
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.46554824282788926; val_accuracy: 0.8998805732484076 

The current subspace-distance is: 0.00018620578339323401 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.98
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.8
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.94
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.41; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.95
Batch: 680; loss: 0.55; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0001935791951837018
0.00018823456775862724
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4623745798495165; val_accuracy: 0.9052547770700637 

The current subspace-distance is: 0.00018823456775862724 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.46; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.32; acc: 1.0
Batch: 460; loss: 0.59; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.97
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00019579235231503844
0.00018818274838849902
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4608692228794098; val_accuracy: 0.9028662420382165 

The current subspace-distance is: 0.00018818274838849902 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.55; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.95
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.95
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00019961078942287713
0.00019162251555826515
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4547254914880558; val_accuracy: 0.9022691082802548 

The current subspace-distance is: 0.00019162251555826515 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.77
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020155745733063668
0.00019335541583132
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.4482798462460755; val_accuracy: 0.9047571656050956 

The current subspace-distance is: 0.00019335541583132 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.38; acc: 0.95
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.65; acc: 0.8
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.77
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.92
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

0.00020278965530451387
0.00019550060096662492
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.44521188223437896; val_accuracy: 0.904359076433121 

The current subspace-distance is: 0.00019550060096662492 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.36; acc: 0.95
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00020873089670203626
0.0002028211165452376
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.4412115542751968; val_accuracy: 0.904359076433121 

The current subspace-distance is: 0.0002028211165452376 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.97
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.98
Batch: 780; loss: 0.43; acc: 0.94
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00020909964223392308
0.00020102047710679471
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.43710307321351044; val_accuracy: 0.9042595541401274 

The current subspace-distance is: 0.00020102047710679471 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.95
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

0.00021209401893429458
0.00020364858210086823
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.43338206220584313; val_accuracy: 0.90515525477707 

The current subspace-distance is: 0.00020364858210086823 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021099275909364223
0.00020488652808126062
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4274706486493919; val_accuracy: 0.9067476114649682 

The current subspace-distance is: 0.00020488652808126062 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.92
Batch: 380; loss: 0.58; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.61; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021381706756073982
0.00020576498354785144
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4253899585099737; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 0.00020576498354785144 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.8
Batch: 160; loss: 0.34; acc: 0.97
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.81
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021653564181178808
0.00020873018365819007
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.42486058839946794; val_accuracy: 0.9063495222929936 

The current subspace-distance is: 0.00020873018365819007 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.97
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021702703088521957
0.00021011923672631383
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4276333549979386; val_accuracy: 0.90625 

The current subspace-distance is: 0.00021011923672631383 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.95
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

0.00021751850727014244
0.00020902276446577162
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.43036871351254213; val_accuracy: 0.9061504777070064 

The current subspace-distance is: 0.00020902276446577162 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.97
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00021754928457085043
0.0002098060940625146
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4247425868632687; val_accuracy: 0.90734474522293 

The current subspace-distance is: 0.0002098060940625146 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.95
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.97
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.97
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0002184167824452743
0.00021200008632149547
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.42423828430236526; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 0.00021200008632149547 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.3; acc: 1.0
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.98
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.98
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.78
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.000217494074604474
0.00021141183970030397
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4209882051322111; val_accuracy: 0.9079418789808917 

The current subspace-distance is: 0.00021141183970030397 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.97
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.54; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.95
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00022142301895655692
0.00021207080862950534
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4229436714178438; val_accuracy: 0.908140923566879 

The current subspace-distance is: 0.00021207080862950534 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.78
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.95
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00021929549984633923
0.00021092299721203744
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4179129750485633; val_accuracy: 0.9084394904458599 

The current subspace-distance is: 0.00021092299721203744 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.97
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.66; acc: 0.81
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0002215699350927025
0.0002133718371624127
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4234888984519205; val_accuracy: 0.9078423566878981 

The current subspace-distance is: 0.0002133718371624127 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_6_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.05

The number of parameters is: 275012

The number of individual parameters is:

17
306
17
17
25
39100
25
25
50
115000
50
50
64
115200
64
64
4096
64
640
10
64
64

nonzero elements in E: 110004791
elements in E: 110004800
fraction nonzero: 0.9999999181853882
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 2.05; acc: 0.34
Batch: 40; loss: 1.71; acc: 0.61
Batch: 60; loss: 1.72; acc: 0.53
Batch: 80; loss: 1.49; acc: 0.72
Batch: 100; loss: 1.42; acc: 0.73
Batch: 120; loss: 1.46; acc: 0.67
Batch: 140; loss: 1.39; acc: 0.78
Batch: 160; loss: 1.29; acc: 0.72
Batch: 180; loss: 1.3; acc: 0.8
Batch: 200; loss: 1.28; acc: 0.77
Batch: 220; loss: 1.41; acc: 0.66
Batch: 240; loss: 1.17; acc: 0.84
Batch: 260; loss: 1.26; acc: 0.77
Batch: 280; loss: 1.13; acc: 0.89
Batch: 300; loss: 1.13; acc: 0.86
Batch: 320; loss: 1.16; acc: 0.8
Batch: 340; loss: 1.08; acc: 0.86
Batch: 360; loss: 1.27; acc: 0.75
Batch: 380; loss: 1.06; acc: 0.91
Batch: 400; loss: 1.09; acc: 0.83
Batch: 420; loss: 1.06; acc: 0.83
Batch: 440; loss: 1.18; acc: 0.75
Batch: 460; loss: 1.05; acc: 0.81
Batch: 480; loss: 1.01; acc: 0.89
Batch: 500; loss: 1.12; acc: 0.81
Batch: 520; loss: 1.03; acc: 0.84
Batch: 540; loss: 0.98; acc: 0.91
Batch: 560; loss: 1.01; acc: 0.83
Batch: 580; loss: 1.05; acc: 0.86
Batch: 600; loss: 0.93; acc: 0.86
Batch: 620; loss: 0.95; acc: 0.94
Batch: 640; loss: 1.03; acc: 0.83
Batch: 660; loss: 1.13; acc: 0.81
Batch: 680; loss: 0.95; acc: 0.83
Batch: 700; loss: 0.86; acc: 0.91
Batch: 720; loss: 0.96; acc: 0.89
Batch: 740; loss: 0.88; acc: 0.92
Batch: 760; loss: 1.01; acc: 0.81
Batch: 780; loss: 0.8; acc: 0.95
Train Epoch over. train_loss: 1.19; train_accuracy: 0.78 

2.6621421056916006e-05
8.182769306586124e-06
Batch: 0; loss: 0.9; acc: 0.88
Batch: 20; loss: 0.98; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.95
Batch: 60; loss: 0.95; acc: 0.86
Batch: 80; loss: 0.79; acc: 0.92
Batch: 100; loss: 0.86; acc: 0.91
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.72; acc: 0.98
Val Epoch over. val_loss: 0.8787276061477175; val_accuracy: 0.87390525477707 

The current subspace-distance is: 8.182769306586124e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.89
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.94; acc: 0.84
Batch: 100; loss: 0.98; acc: 0.83
Batch: 120; loss: 0.92; acc: 0.8
Batch: 140; loss: 0.96; acc: 0.84
Batch: 160; loss: 0.81; acc: 0.91
Batch: 180; loss: 0.77; acc: 0.91
Batch: 200; loss: 0.85; acc: 0.89
Batch: 220; loss: 0.84; acc: 0.89
Batch: 240; loss: 0.94; acc: 0.84
Batch: 260; loss: 0.87; acc: 0.84
Batch: 280; loss: 0.9; acc: 0.86
Batch: 300; loss: 0.79; acc: 0.91
Batch: 320; loss: 0.75; acc: 0.94
Batch: 340; loss: 0.77; acc: 0.94
Batch: 360; loss: 0.88; acc: 0.81
Batch: 380; loss: 0.86; acc: 0.86
Batch: 400; loss: 0.71; acc: 0.94
Batch: 420; loss: 0.7; acc: 0.94
Batch: 440; loss: 0.79; acc: 0.91
Batch: 460; loss: 0.85; acc: 0.86
Batch: 480; loss: 0.85; acc: 0.81
Batch: 500; loss: 0.9; acc: 0.8
Batch: 520; loss: 0.76; acc: 0.91
Batch: 540; loss: 0.87; acc: 0.86
Batch: 560; loss: 0.82; acc: 0.86
Batch: 580; loss: 0.8; acc: 0.92
Batch: 600; loss: 0.73; acc: 0.89
Batch: 620; loss: 0.72; acc: 0.89
Batch: 640; loss: 0.75; acc: 0.89
Batch: 660; loss: 0.83; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.94
Batch: 700; loss: 0.74; acc: 0.88
Batch: 720; loss: 0.76; acc: 0.91
Batch: 740; loss: 0.81; acc: 0.91
Batch: 760; loss: 0.8; acc: 0.86
Batch: 780; loss: 0.78; acc: 0.91
Train Epoch over. train_loss: 0.83; train_accuracy: 0.87 

3.153762736474164e-05
1.1455016647232696e-05
Batch: 0; loss: 0.75; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.98
Batch: 60; loss: 0.73; acc: 0.92
Batch: 80; loss: 0.63; acc: 0.92
Batch: 100; loss: 0.7; acc: 0.89
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.97
Val Epoch over. val_loss: 0.6987252058876547; val_accuracy: 0.8971934713375797 

The current subspace-distance is: 1.1455016647232696e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.86
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.91
Batch: 80; loss: 0.75; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.89
Batch: 140; loss: 0.8; acc: 0.88
Batch: 160; loss: 0.72; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.91
Batch: 200; loss: 0.73; acc: 0.89
Batch: 220; loss: 0.78; acc: 0.89
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.75; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.95
Batch: 300; loss: 0.64; acc: 0.94
Batch: 320; loss: 0.65; acc: 0.92
Batch: 340; loss: 0.81; acc: 0.84
Batch: 360; loss: 0.81; acc: 0.83
Batch: 380; loss: 0.75; acc: 0.88
Batch: 400; loss: 0.68; acc: 0.94
Batch: 420; loss: 0.64; acc: 0.89
Batch: 440; loss: 0.68; acc: 0.89
Batch: 460; loss: 0.7; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.94
Batch: 520; loss: 0.67; acc: 0.89
Batch: 540; loss: 0.69; acc: 0.89
Batch: 560; loss: 0.75; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.97
Batch: 600; loss: 0.51; acc: 0.97
Batch: 620; loss: 0.66; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.95
Batch: 660; loss: 0.61; acc: 0.91
Batch: 680; loss: 0.69; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.98
Batch: 720; loss: 0.66; acc: 0.92
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.63; acc: 0.91
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.88 

3.55832526111044e-05
1.4206003470462747e-05
Batch: 0; loss: 0.65; acc: 0.91
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.98
Val Epoch over. val_loss: 0.6106624557713795; val_accuracy: 0.9034633757961783 

The current subspace-distance is: 1.4206003470462747e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.94
Batch: 160; loss: 0.52; acc: 0.94
Batch: 180; loss: 0.66; acc: 0.91
Batch: 200; loss: 0.61; acc: 0.89
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.91
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.94
Batch: 300; loss: 0.7; acc: 0.88
Batch: 320; loss: 0.65; acc: 0.89
Batch: 340; loss: 0.77; acc: 0.78
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.73; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.92
Batch: 440; loss: 0.61; acc: 0.94
Batch: 460; loss: 0.61; acc: 0.91
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.64; acc: 0.88
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.89
Batch: 600; loss: 0.59; acc: 0.94
Batch: 620; loss: 0.77; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.92
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.89
Train Epoch over. train_loss: 0.64; train_accuracy: 0.88 

3.880608710460365e-05
1.582197000971064e-05
Batch: 0; loss: 0.59; acc: 0.95
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.98
Val Epoch over. val_loss: 0.5521516000768941; val_accuracy: 0.9059514331210191 

The current subspace-distance is: 1.582197000971064e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.98
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.75; acc: 0.88
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.92
Batch: 260; loss: 0.6; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.94
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.94
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.64; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.89
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.58; acc: 0.94
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.43; acc: 0.97
Batch: 640; loss: 0.46; acc: 0.95
Batch: 660; loss: 0.63; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.95
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.94
Train Epoch over. train_loss: 0.58; train_accuracy: 0.89 

4.162450568401255e-05
1.7548303731018677e-05
Batch: 0; loss: 0.53; acc: 0.95
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.98
Val Epoch over. val_loss: 0.4991467505883259; val_accuracy: 0.9102308917197452 

The current subspace-distance is: 1.7548303731018677e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.94
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.97
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.95
Batch: 320; loss: 0.55; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.94
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.92
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.94
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.94
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.54; train_accuracy: 0.9 

4.538497159956023e-05
2.0052566469530575e-05
Batch: 0; loss: 0.47; acc: 0.95
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.98
Val Epoch over. val_loss: 0.46564737455860067; val_accuracy: 0.9112261146496815 

The current subspace-distance is: 2.0052566469530575e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.95
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.94
Batch: 620; loss: 0.5; acc: 0.95
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.97
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.95
Train Epoch over. train_loss: 0.5; train_accuracy: 0.9 

4.6816985559416935e-05
1.9961777070420794e-05
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.98
Val Epoch over. val_loss: 0.42319303428291516; val_accuracy: 0.9142117834394905 

The current subspace-distance is: 1.9961777070420794e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.95
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.8
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.97
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.97
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

4.939063001074828e-05
2.094680894515477e-05
Batch: 0; loss: 0.42; acc: 0.97
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.4018956347825421; val_accuracy: 0.9194864649681529 

The current subspace-distance is: 2.094680894515477e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.97
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.64; acc: 0.8
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.97
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.98
Batch: 700; loss: 0.43; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.44; train_accuracy: 0.91 

5.181224332773127e-05
2.4208906324929558e-05
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.37535099561806695; val_accuracy: 0.9208797770700637 

The current subspace-distance is: 2.4208906324929558e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.98
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.98
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.456924918689765e-05
2.3357686586678028e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.36047530544411605; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 2.3357686586678028e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.98
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.98
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.4988795454846695e-05
2.3830667487345636e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.35313179291737307; val_accuracy: 0.9240644904458599 

The current subspace-distance is: 2.3830667487345636e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.98
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.674990461557172e-05
2.711327761062421e-05
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.35173622827241374; val_accuracy: 0.9236664012738853 

The current subspace-distance is: 2.711327761062421e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.6516706536058336e-05
2.3987417080206797e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3452828312945214; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 2.3987417080206797e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.98
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.97
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.715282532037236e-05
2.510029480617959e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.34223025686042324; val_accuracy: 0.9260549363057324 

The current subspace-distance is: 2.510029480617959e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.98
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.7166515034623444e-05
2.5417057258891873e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.33349268508564894; val_accuracy: 0.9260549363057324 

The current subspace-distance is: 2.5417057258891873e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.97
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.865846105734818e-05
2.6263578547514044e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.3376056726571101; val_accuracy: 0.9252587579617835 

The current subspace-distance is: 2.6263578547514044e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.98
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.8762689150171354e-05
2.674849565664772e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.32912462218931526; val_accuracy: 0.9258558917197452 

The current subspace-distance is: 2.674849565664772e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.98
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.9967918787151575e-05
2.802294329740107e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.3316586331766882; val_accuracy: 0.9251592356687898 

The current subspace-distance is: 2.802294329740107e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.97
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.98
Batch: 400; loss: 0.63; acc: 0.77
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.25; acc: 1.0
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.976294414722361e-05
2.7777812647400424e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.32733685632420195; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 2.7777812647400424e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.97
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.97
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.965323362033814e-05
2.5600991648389027e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3204228965340147; val_accuracy: 0.926453025477707 

The current subspace-distance is: 2.5600991648389027e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.98
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.0217724239919335e-05
2.6222909582429565e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.32231467895826715; val_accuracy: 0.9250597133757962 

The current subspace-distance is: 2.6222909582429565e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.98
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.97
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.0451355238910764e-05
2.8002419639960863e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.31613452723071833; val_accuracy: 0.9260549363057324 

The current subspace-distance is: 2.8002419639960863e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.00584244239144e-05
2.7107131245429628e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3197593477311408; val_accuracy: 0.9276472929936306 

The current subspace-distance is: 2.7107131245429628e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.97
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.97
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.97
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.049434523447417e-05
2.7862693968927488e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3200201554473039; val_accuracy: 0.9251592356687898 

The current subspace-distance is: 2.7862693968927488e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.98
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.23; acc: 1.0
Batch: 760; loss: 0.53; acc: 0.8
Batch: 780; loss: 0.36; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.065406705602072e-05
2.6632029403117485e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.31571974239911244; val_accuracy: 0.9263535031847133 

The current subspace-distance is: 2.6632029403117485e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.057497375877574e-05
2.752149521256797e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3180336324842113; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 2.752149521256797e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.98
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.126083462731913e-05
2.8432819817680866e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.31685343014586503; val_accuracy: 0.9252587579617835 

The current subspace-distance is: 2.8432819817680866e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.97
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.97
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.98
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.98
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.81
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.089517773943953e-05
2.6955647626891732e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.31426279692892817; val_accuracy: 0.9271496815286624 

The current subspace-distance is: 2.6955647626891732e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.23; acc: 1.0
Batch: 340; loss: 0.26; acc: 0.98
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.98
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.24; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.98
Batch: 720; loss: 0.38; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.035915430402383e-05
2.666033287823666e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3151514762715929; val_accuracy: 0.9261544585987261 

The current subspace-distance is: 2.666033287823666e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.98
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.217537156771868e-05
2.9061107852612622e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3108977684929113; val_accuracy: 0.9258558917197452 

The current subspace-distance is: 2.9061107852612622e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_6_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.05

The number of parameters is: 275012

The number of individual parameters is:

17
306
17
17
25
39100
25
25
50
115000
50
50
64
115200
64
64
4096
64
640
10
64
64

nonzero elements in E: 137505989
elements in E: 137506000
fraction nonzero: 0.9999999200034908
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.49; acc: 0.08
Batch: 20; loss: 1.96; acc: 0.38
Batch: 40; loss: 1.7; acc: 0.59
Batch: 60; loss: 1.55; acc: 0.7
Batch: 80; loss: 1.4; acc: 0.8
Batch: 100; loss: 1.39; acc: 0.75
Batch: 120; loss: 1.36; acc: 0.75
Batch: 140; loss: 1.3; acc: 0.77
Batch: 160; loss: 1.28; acc: 0.75
Batch: 180; loss: 1.27; acc: 0.78
Batch: 200; loss: 1.18; acc: 0.77
Batch: 220; loss: 1.12; acc: 0.8
Batch: 240; loss: 1.14; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.77
Batch: 280; loss: 1.11; acc: 0.78
Batch: 300; loss: 1.05; acc: 0.84
Batch: 320; loss: 0.93; acc: 0.86
Batch: 340; loss: 1.12; acc: 0.72
Batch: 360; loss: 0.97; acc: 0.88
Batch: 380; loss: 1.04; acc: 0.8
Batch: 400; loss: 1.08; acc: 0.77
Batch: 420; loss: 0.95; acc: 0.8
Batch: 440; loss: 0.91; acc: 0.89
Batch: 460; loss: 0.79; acc: 0.94
Batch: 480; loss: 0.98; acc: 0.83
Batch: 500; loss: 0.81; acc: 0.94
Batch: 520; loss: 0.82; acc: 0.86
Batch: 540; loss: 0.94; acc: 0.81
Batch: 560; loss: 0.8; acc: 0.91
Batch: 580; loss: 0.93; acc: 0.8
Batch: 600; loss: 0.88; acc: 0.83
Batch: 620; loss: 0.87; acc: 0.81
Batch: 640; loss: 0.75; acc: 0.94
Batch: 660; loss: 0.73; acc: 0.89
Batch: 680; loss: 0.73; acc: 0.94
Batch: 700; loss: 0.77; acc: 0.84
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.82; acc: 0.84
Batch: 760; loss: 0.84; acc: 0.83
Batch: 780; loss: 0.68; acc: 0.91
Train Epoch over. train_loss: 1.07; train_accuracy: 0.8 

2.739776209637057e-05
8.976979188446421e-06
Batch: 0; loss: 0.78; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.97
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.94
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.61; acc: 0.92
Val Epoch over. val_loss: 0.7028629624160232; val_accuracy: 0.8990843949044586 

The current subspace-distance is: 8.976979188446421e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.74; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.92
Batch: 100; loss: 0.81; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.75; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.91
Batch: 200; loss: 0.7; acc: 0.91
Batch: 220; loss: 0.81; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.91
Batch: 260; loss: 0.72; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.91
Batch: 300; loss: 0.65; acc: 0.94
Batch: 320; loss: 0.69; acc: 0.88
Batch: 340; loss: 0.75; acc: 0.83
Batch: 360; loss: 0.68; acc: 0.88
Batch: 380; loss: 0.8; acc: 0.83
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.91
Batch: 440; loss: 0.68; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.89
Batch: 480; loss: 0.67; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.67; acc: 0.89
Batch: 540; loss: 0.67; acc: 0.91
Batch: 560; loss: 0.62; acc: 0.89
Batch: 580; loss: 0.61; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.94
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.88
Batch: 680; loss: 0.62; acc: 0.94
Batch: 700; loss: 0.62; acc: 0.94
Batch: 720; loss: 0.66; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.89
Batch: 760; loss: 0.77; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.92
Train Epoch over. train_loss: 0.68; train_accuracy: 0.89 

3.256554919062182e-05
1.1856201126647647e-05
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.7; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.97
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.43; acc: 0.98
Val Epoch over. val_loss: 0.5583426844162546; val_accuracy: 0.9085390127388535 

The current subspace-distance is: 1.1856201126647647e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.89
Batch: 260; loss: 0.62; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.95
Batch: 360; loss: 0.68; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.94
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.92
Batch: 460; loss: 0.66; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.72; acc: 0.81
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.94
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.94
Batch: 740; loss: 0.64; acc: 0.81
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.68; acc: 0.8
Train Epoch over. train_loss: 0.57; train_accuracy: 0.9 

3.6435209040064365e-05
1.436856109648943e-05
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.97
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.97
Val Epoch over. val_loss: 0.4685444473081334; val_accuracy: 0.9182921974522293 

The current subspace-distance is: 1.436856109648943e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.97
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.92
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.48; acc: 0.95
Batch: 240; loss: 0.5; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.97
Batch: 360; loss: 0.5; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.95
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.98
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.98
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.95
Train Epoch over. train_loss: 0.5; train_accuracy: 0.91 

3.9893049688544124e-05
1.6308626072714105e-05
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.98
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4192114299649646; val_accuracy: 0.9259554140127388 

The current subspace-distance is: 1.6308626072714105e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.95
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.97
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.97
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.97
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.91 

4.3013347749365494e-05
1.9234412320656702e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.37597145424906614; val_accuracy: 0.9255573248407644 

The current subspace-distance is: 1.9234412320656702e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.97
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.98
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.95
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.95
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.97
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.97
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.638897007680498e-05
2.126821709680371e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.2; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.34642593657514853; val_accuracy: 0.9331210191082803 

The current subspace-distance is: 2.126821709680371e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.97
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.98
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.27; acc: 1.0
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.92 

4.803387855645269e-05
2.0645064068958163e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3317907328248783; val_accuracy: 0.9314291401273885 

The current subspace-distance is: 2.0645064068958163e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.0021306378766894e-05
2.170941479562316e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3119282814537644; val_accuracy: 0.9334195859872612 

The current subspace-distance is: 2.170941479562316e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.98
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.154406244400889e-05
2.2499407350551337e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.30168306742124495; val_accuracy: 0.933718152866242 

The current subspace-distance is: 2.2499407350551337e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.98
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.312405846780166e-05
2.2992713638814166e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.29179203197075304; val_accuracy: 0.9359076433121019 

The current subspace-distance is: 2.2992713638814166e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.98
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.98
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.425898780231364e-05
2.3467553546652198e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.28268633830319545; val_accuracy: 0.9373009554140127 

The current subspace-distance is: 2.3467553546652198e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.18; acc: 1.0
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.98
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.4672666010446846e-05
2.4068311176961288e-05
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.2768809597013862; val_accuracy: 0.9382961783439491 

The current subspace-distance is: 2.4068311176961288e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.98
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.5459298891946673e-05
2.3745526050333865e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.2757203032731251; val_accuracy: 0.9375 

The current subspace-distance is: 2.3745526050333865e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.2; acc: 0.98
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.5561733461217955e-05
2.4917879272834398e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.27051909201464075; val_accuracy: 0.9394904458598726 

The current subspace-distance is: 2.4917879272834398e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.97
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.22; acc: 0.98
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.98
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.6590724852867424e-05
2.4758754079812206e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.26592256180989515; val_accuracy: 0.9419785031847133 

The current subspace-distance is: 2.4758754079812206e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.98
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.688217788701877e-05
2.4716859115869738e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.2678466990685007; val_accuracy: 0.940187101910828 

The current subspace-distance is: 2.4716859115869738e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.6559565564384684e-05
2.3530752514488995e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.2642468586564064; val_accuracy: 0.9414808917197452 

The current subspace-distance is: 2.3530752514488995e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.98
Batch: 500; loss: 0.5; acc: 0.81
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.8473844546824694e-05
2.5219300368917175e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.2623311639021916; val_accuracy: 0.9411823248407644 

The current subspace-distance is: 2.5219300368917175e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.21; acc: 1.0
Batch: 240; loss: 0.35; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.98
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.98
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.98
Batch: 720; loss: 0.33; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.98
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.765828973380849e-05
2.503091309336014e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2583714484883721; val_accuracy: 0.9427746815286624 

The current subspace-distance is: 2.503091309336014e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.36; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.7854460465023294e-05
2.4836521333782002e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2521507040994942; val_accuracy: 0.9436703821656051 

The current subspace-distance is: 2.4836521333782002e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.98
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.877252988284454e-05
2.511174898245372e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.25529836236861103; val_accuracy: 0.9430732484076433 

The current subspace-distance is: 2.511174898245372e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.16; acc: 1.0
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.98
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.901491749682464e-05
2.6614161470206454e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2520435586761517; val_accuracy: 0.9433718152866242 

The current subspace-distance is: 2.6614161470206454e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.845102350576781e-05
2.5212497348547913e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2503555530005959; val_accuracy: 0.9428742038216561 

The current subspace-distance is: 2.5212497348547913e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.98
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.98
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.98
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.98
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.995178798912093e-05
2.7215768568567e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2522090714258753; val_accuracy: 0.9422770700636943 

The current subspace-distance is: 2.7215768568567e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.969378617010079e-05
2.628445508889854e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2477224019302684; val_accuracy: 0.944765127388535 

The current subspace-distance is: 2.628445508889854e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.98
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.952700666966848e-05
2.513208528398536e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.24736875623084936; val_accuracy: 0.9434713375796179 

The current subspace-distance is: 2.513208528398536e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.98
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.98
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.966001845081337e-05
2.6474388505448587e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2471028367520138; val_accuracy: 0.9451632165605095 

The current subspace-distance is: 2.6474388505448587e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.98
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.98
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.984549716231413e-05
2.6647860067896545e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.98
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.25083913687308124; val_accuracy: 0.9437699044585988 

The current subspace-distance is: 2.6647860067896545e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.19; acc: 1.0
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.98
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.924854121985845e-05
2.6064933990710415e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.98
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.24485657900382; val_accuracy: 0.9448646496815286 

The current subspace-distance is: 2.6064933990710415e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.98
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.036984996171668e-05
2.646401662786957e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.24775897573893238; val_accuracy: 0.9446656050955414 

The current subspace-distance is: 2.646401662786957e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:58/N_6_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:58/N_6_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
