model : table13slim
N : 8
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:57

Channel scaling factor: 1.6

The number of parameters is: 266828

The number of individual parameters is:

13
234
13
13
20
35360
20
20
39
106080
39
39
64
119808
64
64
4096
64
640
10
64
64

nonzero elements in E: 13341399
elements in E: 13341400
fraction nonzero: 0.9999999250453475
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.51; acc: 0.09
Batch: 20; loss: 2.43; acc: 0.06
Batch: 40; loss: 2.34; acc: 0.16
Batch: 60; loss: 2.34; acc: 0.11
Batch: 80; loss: 2.13; acc: 0.2
Batch: 100; loss: 2.34; acc: 0.2
Batch: 120; loss: 2.28; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.27; acc: 0.17
Batch: 180; loss: 2.24; acc: 0.19
Batch: 200; loss: 2.19; acc: 0.2
Batch: 220; loss: 2.23; acc: 0.16
Batch: 240; loss: 2.18; acc: 0.25
Batch: 260; loss: 2.24; acc: 0.14
Batch: 280; loss: 2.2; acc: 0.16
Batch: 300; loss: 2.22; acc: 0.14
Batch: 320; loss: 2.05; acc: 0.33
Batch: 340; loss: 2.16; acc: 0.2
Batch: 360; loss: 2.25; acc: 0.12
Batch: 380; loss: 2.02; acc: 0.36
Batch: 400; loss: 2.18; acc: 0.19
Batch: 420; loss: 2.12; acc: 0.25
Batch: 440; loss: 2.1; acc: 0.27
Batch: 460; loss: 2.02; acc: 0.3
Batch: 480; loss: 2.1; acc: 0.23
Batch: 500; loss: 2.09; acc: 0.31
Batch: 520; loss: 2.01; acc: 0.25
Batch: 540; loss: 1.96; acc: 0.34
Batch: 560; loss: 2.02; acc: 0.36
Batch: 580; loss: 2.05; acc: 0.33
Batch: 600; loss: 2.03; acc: 0.28
Batch: 620; loss: 1.99; acc: 0.36
Batch: 640; loss: 1.99; acc: 0.39
Batch: 660; loss: 1.98; acc: 0.34
Batch: 680; loss: 2.07; acc: 0.3
Batch: 700; loss: 1.91; acc: 0.44
Batch: 720; loss: 1.99; acc: 0.36
Batch: 740; loss: 2.03; acc: 0.36
Batch: 760; loss: 1.91; acc: 0.41
Batch: 780; loss: 2.04; acc: 0.25
Train Epoch over. train_loss: 2.12; train_accuracy: 0.25 

2.3704298655502498e-05
4.936535788147012e-06
Batch: 0; loss: 1.93; acc: 0.45
Batch: 20; loss: 1.93; acc: 0.34
Batch: 40; loss: 1.75; acc: 0.52
Batch: 60; loss: 1.83; acc: 0.5
Batch: 80; loss: 1.88; acc: 0.48
Batch: 100; loss: 1.9; acc: 0.39
Batch: 120; loss: 1.88; acc: 0.42
Batch: 140; loss: 1.85; acc: 0.39
Val Epoch over. val_loss: 1.887060987721583; val_accuracy: 0.4181926751592357 

The current subspace-distance is: 4.936535788147012e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.82; acc: 0.38
Batch: 20; loss: 1.92; acc: 0.42
Batch: 40; loss: 1.97; acc: 0.36
Batch: 60; loss: 1.86; acc: 0.47
Batch: 80; loss: 1.84; acc: 0.39
Batch: 100; loss: 1.83; acc: 0.42
Batch: 120; loss: 1.82; acc: 0.42
Batch: 140; loss: 1.9; acc: 0.41
Batch: 160; loss: 1.83; acc: 0.44
Batch: 180; loss: 1.86; acc: 0.44
Batch: 200; loss: 1.89; acc: 0.44
Batch: 220; loss: 1.89; acc: 0.41
Batch: 240; loss: 1.97; acc: 0.38
Batch: 260; loss: 1.98; acc: 0.28
Batch: 280; loss: 1.91; acc: 0.42
Batch: 300; loss: 1.92; acc: 0.41
Batch: 320; loss: 1.92; acc: 0.36
Batch: 340; loss: 1.87; acc: 0.48
Batch: 360; loss: 1.89; acc: 0.34
Batch: 380; loss: 2.0; acc: 0.3
Batch: 400; loss: 1.8; acc: 0.48
Batch: 420; loss: 1.91; acc: 0.38
Batch: 440; loss: 1.88; acc: 0.38
Batch: 460; loss: 1.87; acc: 0.52
Batch: 480; loss: 1.87; acc: 0.42
Batch: 500; loss: 1.84; acc: 0.45
Batch: 520; loss: 1.82; acc: 0.44
Batch: 540; loss: 1.85; acc: 0.39
Batch: 560; loss: 1.8; acc: 0.47
Batch: 580; loss: 1.84; acc: 0.42
Batch: 600; loss: 1.8; acc: 0.39
Batch: 620; loss: 1.88; acc: 0.42
Batch: 640; loss: 1.88; acc: 0.38
Batch: 660; loss: 1.8; acc: 0.44
Batch: 680; loss: 1.94; acc: 0.38
Batch: 700; loss: 1.97; acc: 0.28
Batch: 720; loss: 1.9; acc: 0.31
Batch: 740; loss: 1.83; acc: 0.34
Batch: 760; loss: 1.83; acc: 0.38
Batch: 780; loss: 1.94; acc: 0.38
Train Epoch over. train_loss: 1.87; train_accuracy: 0.41 

2.7738620701711625e-05
7.102370091160992e-06
Batch: 0; loss: 1.82; acc: 0.36
Batch: 20; loss: 1.9; acc: 0.44
Batch: 40; loss: 1.67; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.61
Batch: 80; loss: 1.74; acc: 0.52
Batch: 100; loss: 1.8; acc: 0.39
Batch: 120; loss: 1.76; acc: 0.53
Batch: 140; loss: 1.73; acc: 0.55
Val Epoch over. val_loss: 1.7895013040797725; val_accuracy: 0.4624800955414013 

The current subspace-distance is: 7.102370091160992e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.42
Batch: 20; loss: 1.73; acc: 0.45
Batch: 40; loss: 1.75; acc: 0.52
Batch: 60; loss: 1.79; acc: 0.34
Batch: 80; loss: 1.83; acc: 0.42
Batch: 100; loss: 1.83; acc: 0.5
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.84; acc: 0.45
Batch: 160; loss: 1.79; acc: 0.45
Batch: 180; loss: 1.8; acc: 0.45
Batch: 200; loss: 1.74; acc: 0.48
Batch: 220; loss: 1.78; acc: 0.48
Batch: 240; loss: 1.81; acc: 0.52
Batch: 260; loss: 1.76; acc: 0.44
Batch: 280; loss: 1.81; acc: 0.47
Batch: 300; loss: 1.84; acc: 0.45
Batch: 320; loss: 1.85; acc: 0.44
Batch: 340; loss: 1.88; acc: 0.34
Batch: 360; loss: 1.83; acc: 0.41
Batch: 380; loss: 1.97; acc: 0.39
Batch: 400; loss: 1.78; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.52
Batch: 440; loss: 1.72; acc: 0.53
Batch: 460; loss: 1.7; acc: 0.58
Batch: 480; loss: 1.83; acc: 0.47
Batch: 500; loss: 1.93; acc: 0.41
Batch: 520; loss: 1.73; acc: 0.59
Batch: 540; loss: 1.77; acc: 0.55
Batch: 560; loss: 1.75; acc: 0.47
Batch: 580; loss: 1.88; acc: 0.45
Batch: 600; loss: 1.69; acc: 0.5
Batch: 620; loss: 1.71; acc: 0.53
Batch: 640; loss: 1.86; acc: 0.45
Batch: 660; loss: 1.76; acc: 0.39
Batch: 680; loss: 1.8; acc: 0.48
Batch: 700; loss: 1.79; acc: 0.5
Batch: 720; loss: 1.91; acc: 0.31
Batch: 740; loss: 1.83; acc: 0.47
Batch: 760; loss: 1.75; acc: 0.52
Batch: 780; loss: 1.8; acc: 0.44
Train Epoch over. train_loss: 1.81; train_accuracy: 0.45 

2.9666443879250437e-05
7.581906629638979e-06
Batch: 0; loss: 1.74; acc: 0.42
Batch: 20; loss: 1.88; acc: 0.42
Batch: 40; loss: 1.63; acc: 0.56
Batch: 60; loss: 1.6; acc: 0.7
Batch: 80; loss: 1.65; acc: 0.58
Batch: 100; loss: 1.73; acc: 0.45
Batch: 120; loss: 1.69; acc: 0.52
Batch: 140; loss: 1.71; acc: 0.45
Val Epoch over. val_loss: 1.7448076214759973; val_accuracy: 0.48706210191082805 

The current subspace-distance is: 7.581906629638979e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.48
Batch: 20; loss: 1.72; acc: 0.48
Batch: 40; loss: 1.78; acc: 0.48
Batch: 60; loss: 1.89; acc: 0.45
Batch: 80; loss: 1.78; acc: 0.53
Batch: 100; loss: 1.82; acc: 0.42
Batch: 120; loss: 1.76; acc: 0.48
Batch: 140; loss: 1.82; acc: 0.44
Batch: 160; loss: 1.85; acc: 0.38
Batch: 180; loss: 1.74; acc: 0.53
Batch: 200; loss: 1.83; acc: 0.44
Batch: 220; loss: 1.83; acc: 0.44
Batch: 240; loss: 1.77; acc: 0.5
Batch: 260; loss: 1.72; acc: 0.52
Batch: 280; loss: 1.72; acc: 0.5
Batch: 300; loss: 1.84; acc: 0.34
Batch: 320; loss: 1.76; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.58
Batch: 360; loss: 1.85; acc: 0.44
Batch: 380; loss: 1.76; acc: 0.42
Batch: 400; loss: 1.79; acc: 0.48
Batch: 420; loss: 1.82; acc: 0.44
Batch: 440; loss: 1.82; acc: 0.48
Batch: 460; loss: 1.65; acc: 0.55
Batch: 480; loss: 1.89; acc: 0.39
Batch: 500; loss: 1.57; acc: 0.67
Batch: 520; loss: 1.76; acc: 0.48
Batch: 540; loss: 1.72; acc: 0.5
Batch: 560; loss: 1.7; acc: 0.56
Batch: 580; loss: 1.78; acc: 0.55
Batch: 600; loss: 1.69; acc: 0.56
Batch: 620; loss: 1.77; acc: 0.58
Batch: 640; loss: 1.64; acc: 0.61
Batch: 660; loss: 1.82; acc: 0.42
Batch: 680; loss: 1.76; acc: 0.45
Batch: 700; loss: 1.69; acc: 0.53
Batch: 720; loss: 1.6; acc: 0.64
Batch: 740; loss: 1.76; acc: 0.52
Batch: 760; loss: 1.77; acc: 0.47
Batch: 780; loss: 1.76; acc: 0.48
Train Epoch over. train_loss: 1.78; train_accuracy: 0.47 

3.2051702874014154e-05
1.2444129424693529e-05
Batch: 0; loss: 1.71; acc: 0.5
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.59; acc: 0.53
Batch: 60; loss: 1.59; acc: 0.72
Batch: 80; loss: 1.64; acc: 0.62
Batch: 100; loss: 1.72; acc: 0.52
Batch: 120; loss: 1.65; acc: 0.59
Batch: 140; loss: 1.69; acc: 0.48
Val Epoch over. val_loss: 1.7298928241061557; val_accuracy: 0.4905453821656051 

The current subspace-distance is: 1.2444129424693529e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.53
Batch: 20; loss: 1.76; acc: 0.48
Batch: 40; loss: 1.84; acc: 0.39
Batch: 60; loss: 1.84; acc: 0.39
Batch: 80; loss: 1.64; acc: 0.55
Batch: 100; loss: 1.71; acc: 0.53
Batch: 120; loss: 1.68; acc: 0.53
Batch: 140; loss: 1.7; acc: 0.48
Batch: 160; loss: 1.84; acc: 0.41
Batch: 180; loss: 1.7; acc: 0.5
Batch: 200; loss: 1.77; acc: 0.45
Batch: 220; loss: 1.6; acc: 0.58
Batch: 240; loss: 1.67; acc: 0.56
Batch: 260; loss: 1.8; acc: 0.44
Batch: 280; loss: 1.82; acc: 0.34
Batch: 300; loss: 1.72; acc: 0.45
Batch: 320; loss: 1.8; acc: 0.42
Batch: 340; loss: 1.79; acc: 0.47
Batch: 360; loss: 1.81; acc: 0.42
Batch: 380; loss: 1.73; acc: 0.44
Batch: 400; loss: 1.7; acc: 0.52
Batch: 420; loss: 1.78; acc: 0.39
Batch: 440; loss: 1.69; acc: 0.47
Batch: 460; loss: 1.86; acc: 0.41
Batch: 480; loss: 1.85; acc: 0.42
Batch: 500; loss: 1.87; acc: 0.33
Batch: 520; loss: 1.74; acc: 0.47
Batch: 540; loss: 1.89; acc: 0.42
Batch: 560; loss: 1.73; acc: 0.47
Batch: 580; loss: 1.87; acc: 0.38
Batch: 600; loss: 1.66; acc: 0.58
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.74; acc: 0.5
Batch: 660; loss: 1.72; acc: 0.48
Batch: 680; loss: 1.79; acc: 0.45
Batch: 700; loss: 1.81; acc: 0.44
Batch: 720; loss: 1.71; acc: 0.5
Batch: 740; loss: 1.67; acc: 0.52
Batch: 760; loss: 1.72; acc: 0.45
Batch: 780; loss: 1.64; acc: 0.53
Train Epoch over. train_loss: 1.76; train_accuracy: 0.48 

3.260594166931696e-05
7.626092610735213e-06
Batch: 0; loss: 1.67; acc: 0.56
Batch: 20; loss: 1.88; acc: 0.47
Batch: 40; loss: 1.52; acc: 0.58
Batch: 60; loss: 1.59; acc: 0.69
Batch: 80; loss: 1.6; acc: 0.64
Batch: 100; loss: 1.7; acc: 0.58
Batch: 120; loss: 1.59; acc: 0.62
Batch: 140; loss: 1.66; acc: 0.5
Val Epoch over. val_loss: 1.7025279080032543; val_accuracy: 0.5184116242038217 

The current subspace-distance is: 7.626092610735213e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.9; acc: 0.41
Batch: 20; loss: 1.79; acc: 0.48
Batch: 40; loss: 1.82; acc: 0.44
Batch: 60; loss: 1.76; acc: 0.44
Batch: 80; loss: 1.77; acc: 0.47
Batch: 100; loss: 1.69; acc: 0.52
Batch: 120; loss: 1.77; acc: 0.44
Batch: 140; loss: 1.68; acc: 0.52
Batch: 160; loss: 1.77; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.52
Batch: 200; loss: 1.78; acc: 0.44
Batch: 220; loss: 1.88; acc: 0.47
Batch: 240; loss: 1.73; acc: 0.47
Batch: 260; loss: 1.74; acc: 0.47
Batch: 280; loss: 1.66; acc: 0.59
Batch: 300; loss: 1.7; acc: 0.55
Batch: 320; loss: 1.7; acc: 0.47
Batch: 340; loss: 1.74; acc: 0.45
Batch: 360; loss: 1.82; acc: 0.53
Batch: 380; loss: 1.84; acc: 0.39
Batch: 400; loss: 1.7; acc: 0.47
Batch: 420; loss: 1.63; acc: 0.58
Batch: 440; loss: 1.78; acc: 0.41
Batch: 460; loss: 1.68; acc: 0.56
Batch: 480; loss: 1.75; acc: 0.5
Batch: 500; loss: 1.66; acc: 0.56
Batch: 520; loss: 1.77; acc: 0.44
Batch: 540; loss: 1.81; acc: 0.41
Batch: 560; loss: 1.83; acc: 0.45
Batch: 580; loss: 1.71; acc: 0.52
Batch: 600; loss: 1.77; acc: 0.5
Batch: 620; loss: 1.69; acc: 0.48
Batch: 640; loss: 1.65; acc: 0.56
Batch: 660; loss: 1.78; acc: 0.52
Batch: 680; loss: 1.75; acc: 0.5
Batch: 700; loss: 1.66; acc: 0.53
Batch: 720; loss: 1.66; acc: 0.52
Batch: 740; loss: 1.75; acc: 0.45
Batch: 760; loss: 1.66; acc: 0.55
Batch: 780; loss: 1.64; acc: 0.59
Train Epoch over. train_loss: 1.74; train_accuracy: 0.49 

3.4214222978334874e-05
8.252602128777653e-06
Batch: 0; loss: 1.63; acc: 0.55
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.47; acc: 0.59
Batch: 60; loss: 1.59; acc: 0.69
Batch: 80; loss: 1.59; acc: 0.62
Batch: 100; loss: 1.67; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.62
Batch: 140; loss: 1.64; acc: 0.53
Val Epoch over. val_loss: 1.6795308840502599; val_accuracy: 0.5244824840764332 

The current subspace-distance is: 8.252602128777653e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.48
Batch: 20; loss: 1.62; acc: 0.56
Batch: 40; loss: 1.75; acc: 0.5
Batch: 60; loss: 1.64; acc: 0.56
Batch: 80; loss: 1.66; acc: 0.56
Batch: 100; loss: 1.73; acc: 0.56
Batch: 120; loss: 1.59; acc: 0.61
Batch: 140; loss: 1.68; acc: 0.56
Batch: 160; loss: 1.75; acc: 0.48
Batch: 180; loss: 1.74; acc: 0.52
Batch: 200; loss: 1.86; acc: 0.38
Batch: 220; loss: 1.72; acc: 0.5
Batch: 240; loss: 1.62; acc: 0.61
Batch: 260; loss: 1.69; acc: 0.53
Batch: 280; loss: 1.72; acc: 0.53
Batch: 300; loss: 1.79; acc: 0.42
Batch: 320; loss: 1.66; acc: 0.58
Batch: 340; loss: 1.73; acc: 0.5
Batch: 360; loss: 1.75; acc: 0.47
Batch: 380; loss: 1.72; acc: 0.5
Batch: 400; loss: 1.72; acc: 0.5
Batch: 420; loss: 1.74; acc: 0.48
Batch: 440; loss: 1.75; acc: 0.48
Batch: 460; loss: 1.55; acc: 0.58
Batch: 480; loss: 1.89; acc: 0.36
Batch: 500; loss: 1.78; acc: 0.41
Batch: 520; loss: 1.81; acc: 0.34
Batch: 540; loss: 1.67; acc: 0.58
Batch: 560; loss: 1.61; acc: 0.48
Batch: 580; loss: 1.68; acc: 0.55
Batch: 600; loss: 1.67; acc: 0.48
Batch: 620; loss: 1.75; acc: 0.44
Batch: 640; loss: 1.59; acc: 0.58
Batch: 660; loss: 1.79; acc: 0.47
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.59; acc: 0.58
Batch: 720; loss: 1.72; acc: 0.45
Batch: 740; loss: 1.63; acc: 0.53
Batch: 760; loss: 1.59; acc: 0.56
Batch: 780; loss: 1.72; acc: 0.48
Train Epoch over. train_loss: 1.7; train_accuracy: 0.5 

3.5575088986661285e-05
9.76929732132703e-06
Batch: 0; loss: 1.57; acc: 0.61
Batch: 20; loss: 1.87; acc: 0.39
Batch: 40; loss: 1.38; acc: 0.72
Batch: 60; loss: 1.56; acc: 0.7
Batch: 80; loss: 1.5; acc: 0.64
Batch: 100; loss: 1.62; acc: 0.59
Batch: 120; loss: 1.52; acc: 0.62
Batch: 140; loss: 1.57; acc: 0.59
Val Epoch over. val_loss: 1.6209577876291457; val_accuracy: 0.5526472929936306 

The current subspace-distance is: 9.76929732132703e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.65; acc: 0.55
Batch: 40; loss: 1.58; acc: 0.58
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.64; acc: 0.47
Batch: 100; loss: 1.44; acc: 0.67
Batch: 120; loss: 1.62; acc: 0.55
Batch: 140; loss: 1.61; acc: 0.59
Batch: 160; loss: 1.65; acc: 0.59
Batch: 180; loss: 1.65; acc: 0.5
Batch: 200; loss: 1.53; acc: 0.64
Batch: 220; loss: 1.86; acc: 0.31
Batch: 240; loss: 1.77; acc: 0.47
Batch: 260; loss: 1.6; acc: 0.55
Batch: 280; loss: 1.7; acc: 0.47
Batch: 300; loss: 1.74; acc: 0.52
Batch: 320; loss: 1.71; acc: 0.52
Batch: 340; loss: 1.54; acc: 0.56
Batch: 360; loss: 1.6; acc: 0.58
Batch: 380; loss: 1.68; acc: 0.52
Batch: 400; loss: 1.75; acc: 0.48
Batch: 420; loss: 1.61; acc: 0.52
Batch: 440; loss: 1.62; acc: 0.59
Batch: 460; loss: 1.6; acc: 0.53
Batch: 480; loss: 1.58; acc: 0.62
Batch: 500; loss: 1.67; acc: 0.47
Batch: 520; loss: 1.68; acc: 0.44
Batch: 540; loss: 1.63; acc: 0.56
Batch: 560; loss: 1.61; acc: 0.55
Batch: 580; loss: 1.73; acc: 0.44
Batch: 600; loss: 1.61; acc: 0.56
Batch: 620; loss: 1.63; acc: 0.52
Batch: 640; loss: 1.59; acc: 0.5
Batch: 660; loss: 1.62; acc: 0.47
Batch: 680; loss: 1.6; acc: 0.47
Batch: 700; loss: 1.6; acc: 0.53
Batch: 720; loss: 1.72; acc: 0.47
Batch: 740; loss: 1.61; acc: 0.55
Batch: 760; loss: 1.57; acc: 0.59
Batch: 780; loss: 1.57; acc: 0.58
Train Epoch over. train_loss: 1.65; train_accuracy: 0.52 

3.927106808987446e-05
1.4324968105938751e-05
Batch: 0; loss: 1.5; acc: 0.61
Batch: 20; loss: 1.83; acc: 0.41
Batch: 40; loss: 1.32; acc: 0.7
Batch: 60; loss: 1.5; acc: 0.69
Batch: 80; loss: 1.45; acc: 0.62
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.48; acc: 0.59
Batch: 140; loss: 1.49; acc: 0.59
Val Epoch over. val_loss: 1.5718905644811643; val_accuracy: 0.572452229299363 

The current subspace-distance is: 1.4324968105938751e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.53
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.58; acc: 0.47
Batch: 60; loss: 1.67; acc: 0.5
Batch: 80; loss: 1.6; acc: 0.52
Batch: 100; loss: 1.58; acc: 0.58
Batch: 120; loss: 1.69; acc: 0.52
Batch: 140; loss: 1.61; acc: 0.59
Batch: 160; loss: 1.77; acc: 0.45
Batch: 180; loss: 1.72; acc: 0.52
Batch: 200; loss: 1.68; acc: 0.5
Batch: 220; loss: 1.45; acc: 0.62
Batch: 240; loss: 1.59; acc: 0.59
Batch: 260; loss: 1.47; acc: 0.67
Batch: 280; loss: 1.69; acc: 0.52
Batch: 300; loss: 1.67; acc: 0.5
Batch: 320; loss: 1.54; acc: 0.55
Batch: 340; loss: 1.5; acc: 0.59
Batch: 360; loss: 1.58; acc: 0.53
Batch: 380; loss: 1.67; acc: 0.5
Batch: 400; loss: 1.55; acc: 0.64
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.66; acc: 0.58
Batch: 460; loss: 1.72; acc: 0.52
Batch: 480; loss: 1.61; acc: 0.52
Batch: 500; loss: 1.73; acc: 0.48
Batch: 520; loss: 1.63; acc: 0.62
Batch: 540; loss: 1.61; acc: 0.53
Batch: 560; loss: 1.63; acc: 0.47
Batch: 580; loss: 1.56; acc: 0.56
Batch: 600; loss: 1.45; acc: 0.66
Batch: 620; loss: 1.6; acc: 0.5
Batch: 640; loss: 1.43; acc: 0.59
Batch: 660; loss: 1.63; acc: 0.48
Batch: 680; loss: 1.63; acc: 0.58
Batch: 700; loss: 1.48; acc: 0.56
Batch: 720; loss: 1.7; acc: 0.47
Batch: 740; loss: 1.69; acc: 0.53
Batch: 760; loss: 1.57; acc: 0.64
Batch: 780; loss: 1.67; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.53 

3.9567537896800786e-05
1.0548019417910837e-05
Batch: 0; loss: 1.43; acc: 0.7
Batch: 20; loss: 1.78; acc: 0.41
Batch: 40; loss: 1.26; acc: 0.7
Batch: 60; loss: 1.43; acc: 0.7
Batch: 80; loss: 1.4; acc: 0.67
Batch: 100; loss: 1.54; acc: 0.62
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.43; acc: 0.62
Val Epoch over. val_loss: 1.5250669953170095; val_accuracy: 0.5803144904458599 

The current subspace-distance is: 1.0548019417910837e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.6; acc: 0.47
Batch: 40; loss: 1.54; acc: 0.56
Batch: 60; loss: 1.59; acc: 0.58
Batch: 80; loss: 1.67; acc: 0.45
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.73; acc: 0.42
Batch: 160; loss: 1.49; acc: 0.62
Batch: 180; loss: 1.55; acc: 0.47
Batch: 200; loss: 1.67; acc: 0.53
Batch: 220; loss: 1.74; acc: 0.45
Batch: 240; loss: 1.56; acc: 0.5
Batch: 260; loss: 1.58; acc: 0.47
Batch: 280; loss: 1.54; acc: 0.59
Batch: 300; loss: 1.67; acc: 0.5
Batch: 320; loss: 1.68; acc: 0.52
Batch: 340; loss: 1.58; acc: 0.56
Batch: 360; loss: 1.59; acc: 0.55
Batch: 380; loss: 1.41; acc: 0.64
Batch: 400; loss: 1.67; acc: 0.53
Batch: 420; loss: 1.69; acc: 0.52
Batch: 440; loss: 1.57; acc: 0.56
Batch: 460; loss: 1.48; acc: 0.62
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.79; acc: 0.41
Batch: 520; loss: 1.55; acc: 0.48
Batch: 540; loss: 1.48; acc: 0.62
Batch: 560; loss: 1.62; acc: 0.5
Batch: 580; loss: 1.69; acc: 0.5
Batch: 600; loss: 1.65; acc: 0.48
Batch: 620; loss: 1.77; acc: 0.44
Batch: 640; loss: 1.59; acc: 0.45
Batch: 660; loss: 1.57; acc: 0.52
Batch: 680; loss: 1.68; acc: 0.47
Batch: 700; loss: 1.58; acc: 0.52
Batch: 720; loss: 1.64; acc: 0.53
Batch: 740; loss: 1.69; acc: 0.45
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.57; acc: 0.56
Train Epoch over. train_loss: 1.58; train_accuracy: 0.54 

4.3371943320380524e-05
1.7586211470188573e-05
Batch: 0; loss: 1.42; acc: 0.7
Batch: 20; loss: 1.76; acc: 0.44
Batch: 40; loss: 1.26; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.75
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.56; acc: 0.59
Batch: 120; loss: 1.48; acc: 0.59
Batch: 140; loss: 1.41; acc: 0.62
Val Epoch over. val_loss: 1.5070118319456745; val_accuracy: 0.5862858280254777 

The current subspace-distance is: 1.7586211470188573e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.51; acc: 0.52
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.72; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.53
Batch: 100; loss: 1.54; acc: 0.52
Batch: 120; loss: 1.64; acc: 0.44
Batch: 140; loss: 1.36; acc: 0.66
Batch: 160; loss: 1.61; acc: 0.5
Batch: 180; loss: 1.38; acc: 0.64
Batch: 200; loss: 1.57; acc: 0.55
Batch: 220; loss: 1.59; acc: 0.5
Batch: 240; loss: 1.67; acc: 0.44
Batch: 260; loss: 1.68; acc: 0.45
Batch: 280; loss: 1.59; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.53
Batch: 320; loss: 1.55; acc: 0.5
Batch: 340; loss: 1.48; acc: 0.59
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.41
Batch: 400; loss: 1.64; acc: 0.48
Batch: 420; loss: 1.57; acc: 0.5
Batch: 440; loss: 1.56; acc: 0.55
Batch: 460; loss: 1.58; acc: 0.66
Batch: 480; loss: 1.42; acc: 0.62
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.6; acc: 0.47
Batch: 540; loss: 1.64; acc: 0.56
Batch: 560; loss: 1.61; acc: 0.53
Batch: 580; loss: 1.49; acc: 0.59
Batch: 600; loss: 1.57; acc: 0.55
Batch: 620; loss: 1.55; acc: 0.53
Batch: 640; loss: 1.48; acc: 0.61
Batch: 660; loss: 1.59; acc: 0.48
Batch: 680; loss: 1.51; acc: 0.56
Batch: 700; loss: 1.62; acc: 0.53
Batch: 720; loss: 1.49; acc: 0.64
Batch: 740; loss: 1.43; acc: 0.66
Batch: 760; loss: 1.59; acc: 0.55
Batch: 780; loss: 1.71; acc: 0.47
Train Epoch over. train_loss: 1.57; train_accuracy: 0.54 

4.3070398533018306e-05
1.1985470337094739e-05
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.74; acc: 0.44
Batch: 40; loss: 1.26; acc: 0.7
Batch: 60; loss: 1.38; acc: 0.72
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.55; acc: 0.53
Batch: 120; loss: 1.47; acc: 0.61
Batch: 140; loss: 1.41; acc: 0.61
Val Epoch over. val_loss: 1.5048505629703497; val_accuracy: 0.5795183121019108 

The current subspace-distance is: 1.1985470337094739e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.52
Batch: 20; loss: 1.56; acc: 0.52
Batch: 40; loss: 1.49; acc: 0.53
Batch: 60; loss: 1.43; acc: 0.62
Batch: 80; loss: 1.63; acc: 0.47
Batch: 100; loss: 1.71; acc: 0.45
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.65; acc: 0.48
Batch: 160; loss: 1.57; acc: 0.5
Batch: 180; loss: 1.49; acc: 0.67
Batch: 200; loss: 1.58; acc: 0.55
Batch: 220; loss: 1.64; acc: 0.53
Batch: 240; loss: 1.64; acc: 0.48
Batch: 260; loss: 1.63; acc: 0.5
Batch: 280; loss: 1.6; acc: 0.42
Batch: 300; loss: 1.34; acc: 0.7
Batch: 320; loss: 1.65; acc: 0.48
Batch: 340; loss: 1.57; acc: 0.53
Batch: 360; loss: 1.53; acc: 0.59
Batch: 380; loss: 1.63; acc: 0.5
Batch: 400; loss: 1.56; acc: 0.55
Batch: 420; loss: 1.71; acc: 0.5
Batch: 440; loss: 1.52; acc: 0.56
Batch: 460; loss: 1.58; acc: 0.42
Batch: 480; loss: 1.68; acc: 0.47
Batch: 500; loss: 1.59; acc: 0.52
Batch: 520; loss: 1.43; acc: 0.58
Batch: 540; loss: 1.47; acc: 0.56
Batch: 560; loss: 1.37; acc: 0.67
Batch: 580; loss: 1.5; acc: 0.53
Batch: 600; loss: 1.58; acc: 0.59
Batch: 620; loss: 1.51; acc: 0.55
Batch: 640; loss: 1.46; acc: 0.56
Batch: 660; loss: 1.46; acc: 0.58
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.46; acc: 0.61
Batch: 720; loss: 1.46; acc: 0.66
Batch: 740; loss: 1.46; acc: 0.59
Batch: 760; loss: 1.54; acc: 0.55
Batch: 780; loss: 1.45; acc: 0.69
Train Epoch over. train_loss: 1.56; train_accuracy: 0.54 

4.473804438021034e-05
1.4427163478103466e-05
Batch: 0; loss: 1.4; acc: 0.69
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.25; acc: 0.73
Batch: 60; loss: 1.37; acc: 0.72
Batch: 80; loss: 1.34; acc: 0.69
Batch: 100; loss: 1.53; acc: 0.59
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.38; acc: 0.67
Val Epoch over. val_loss: 1.4965629608008513; val_accuracy: 0.5861863057324841 

The current subspace-distance is: 1.4427163478103466e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.62
Batch: 20; loss: 1.62; acc: 0.53
Batch: 40; loss: 1.51; acc: 0.58
Batch: 60; loss: 1.45; acc: 0.61
Batch: 80; loss: 1.47; acc: 0.62
Batch: 100; loss: 1.69; acc: 0.47
Batch: 120; loss: 1.58; acc: 0.48
Batch: 140; loss: 1.55; acc: 0.58
Batch: 160; loss: 1.51; acc: 0.58
Batch: 180; loss: 1.65; acc: 0.55
Batch: 200; loss: 1.59; acc: 0.45
Batch: 220; loss: 1.64; acc: 0.55
Batch: 240; loss: 1.55; acc: 0.53
Batch: 260; loss: 1.54; acc: 0.47
Batch: 280; loss: 1.45; acc: 0.55
Batch: 300; loss: 1.61; acc: 0.48
Batch: 320; loss: 1.49; acc: 0.55
Batch: 340; loss: 1.42; acc: 0.59
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.47; acc: 0.61
Batch: 400; loss: 1.64; acc: 0.44
Batch: 420; loss: 1.56; acc: 0.45
Batch: 440; loss: 1.6; acc: 0.52
Batch: 460; loss: 1.43; acc: 0.56
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.61; acc: 0.58
Batch: 520; loss: 1.55; acc: 0.58
Batch: 540; loss: 1.5; acc: 0.61
Batch: 560; loss: 1.65; acc: 0.44
Batch: 580; loss: 1.5; acc: 0.61
Batch: 600; loss: 1.55; acc: 0.58
Batch: 620; loss: 1.55; acc: 0.58
Batch: 640; loss: 1.55; acc: 0.53
Batch: 660; loss: 1.61; acc: 0.5
Batch: 680; loss: 1.62; acc: 0.53
Batch: 700; loss: 1.62; acc: 0.45
Batch: 720; loss: 1.61; acc: 0.58
Batch: 740; loss: 1.5; acc: 0.53
Batch: 760; loss: 1.45; acc: 0.59
Batch: 780; loss: 1.63; acc: 0.55
Train Epoch over. train_loss: 1.56; train_accuracy: 0.54 

4.5913784560980275e-05
1.7800435671233572e-05
Batch: 0; loss: 1.39; acc: 0.7
Batch: 20; loss: 1.7; acc: 0.47
Batch: 40; loss: 1.25; acc: 0.72
Batch: 60; loss: 1.36; acc: 0.72
Batch: 80; loss: 1.34; acc: 0.66
Batch: 100; loss: 1.54; acc: 0.58
Batch: 120; loss: 1.47; acc: 0.56
Batch: 140; loss: 1.4; acc: 0.62
Val Epoch over. val_loss: 1.4909753769066683; val_accuracy: 0.5806130573248408 

The current subspace-distance is: 1.7800435671233572e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.42; acc: 0.66
Batch: 20; loss: 1.58; acc: 0.55
Batch: 40; loss: 1.51; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.59
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.53; acc: 0.53
Batch: 140; loss: 1.63; acc: 0.45
Batch: 160; loss: 1.49; acc: 0.56
Batch: 180; loss: 1.52; acc: 0.56
Batch: 200; loss: 1.67; acc: 0.47
Batch: 220; loss: 1.48; acc: 0.48
Batch: 240; loss: 1.62; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.52
Batch: 280; loss: 1.54; acc: 0.59
Batch: 300; loss: 1.62; acc: 0.58
Batch: 320; loss: 1.62; acc: 0.48
Batch: 340; loss: 1.71; acc: 0.48
Batch: 360; loss: 1.5; acc: 0.55
Batch: 380; loss: 1.67; acc: 0.53
Batch: 400; loss: 1.53; acc: 0.5
Batch: 420; loss: 1.5; acc: 0.48
Batch: 440; loss: 1.63; acc: 0.53
Batch: 460; loss: 1.6; acc: 0.52
Batch: 480; loss: 1.49; acc: 0.58
Batch: 500; loss: 1.57; acc: 0.59
Batch: 520; loss: 1.66; acc: 0.48
Batch: 540; loss: 1.4; acc: 0.61
Batch: 560; loss: 1.49; acc: 0.58
Batch: 580; loss: 1.6; acc: 0.59
Batch: 600; loss: 1.59; acc: 0.56
Batch: 620; loss: 1.73; acc: 0.41
Batch: 640; loss: 1.46; acc: 0.64
Batch: 660; loss: 1.49; acc: 0.59
Batch: 680; loss: 1.64; acc: 0.5
Batch: 700; loss: 1.57; acc: 0.52
Batch: 720; loss: 1.52; acc: 0.55
Batch: 740; loss: 1.55; acc: 0.58
Batch: 760; loss: 1.68; acc: 0.48
Batch: 780; loss: 1.56; acc: 0.48
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.634568904293701e-05
1.595883622940164e-05
Batch: 0; loss: 1.38; acc: 0.7
Batch: 20; loss: 1.69; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.34; acc: 0.69
Batch: 80; loss: 1.31; acc: 0.69
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.46; acc: 0.61
Batch: 140; loss: 1.38; acc: 0.67
Val Epoch over. val_loss: 1.48439256552678; val_accuracy: 0.5833001592356688 

The current subspace-distance is: 1.595883622940164e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.53
Batch: 20; loss: 1.34; acc: 0.7
Batch: 40; loss: 1.65; acc: 0.45
Batch: 60; loss: 1.58; acc: 0.47
Batch: 80; loss: 1.48; acc: 0.52
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.57; acc: 0.53
Batch: 140; loss: 1.58; acc: 0.53
Batch: 160; loss: 1.58; acc: 0.45
Batch: 180; loss: 1.65; acc: 0.47
Batch: 200; loss: 1.6; acc: 0.56
Batch: 220; loss: 1.63; acc: 0.56
Batch: 240; loss: 1.48; acc: 0.59
Batch: 260; loss: 1.51; acc: 0.55
Batch: 280; loss: 1.56; acc: 0.52
Batch: 300; loss: 1.46; acc: 0.61
Batch: 320; loss: 1.68; acc: 0.5
Batch: 340; loss: 1.48; acc: 0.61
Batch: 360; loss: 1.59; acc: 0.52
Batch: 380; loss: 1.59; acc: 0.53
Batch: 400; loss: 1.6; acc: 0.56
Batch: 420; loss: 1.4; acc: 0.55
Batch: 440; loss: 1.57; acc: 0.53
Batch: 460; loss: 1.54; acc: 0.53
Batch: 480; loss: 1.66; acc: 0.47
Batch: 500; loss: 1.58; acc: 0.55
Batch: 520; loss: 1.65; acc: 0.53
Batch: 540; loss: 1.46; acc: 0.62
Batch: 560; loss: 1.54; acc: 0.53
Batch: 580; loss: 1.67; acc: 0.48
Batch: 600; loss: 1.59; acc: 0.5
Batch: 620; loss: 1.61; acc: 0.52
Batch: 640; loss: 1.5; acc: 0.58
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.62; acc: 0.39
Batch: 700; loss: 1.66; acc: 0.48
Batch: 720; loss: 1.52; acc: 0.58
Batch: 740; loss: 1.56; acc: 0.44
Batch: 760; loss: 1.33; acc: 0.66
Batch: 780; loss: 1.5; acc: 0.55
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.717774208984338e-05
1.786039865692146e-05
Batch: 0; loss: 1.4; acc: 0.75
Batch: 20; loss: 1.69; acc: 0.47
Batch: 40; loss: 1.24; acc: 0.72
Batch: 60; loss: 1.35; acc: 0.69
Batch: 80; loss: 1.32; acc: 0.66
Batch: 100; loss: 1.53; acc: 0.59
Batch: 120; loss: 1.48; acc: 0.56
Batch: 140; loss: 1.37; acc: 0.73
Val Epoch over. val_loss: 1.4934906390062563; val_accuracy: 0.5798168789808917 

The current subspace-distance is: 1.786039865692146e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.58; acc: 0.53
Batch: 40; loss: 1.37; acc: 0.66
Batch: 60; loss: 1.45; acc: 0.62
Batch: 80; loss: 1.6; acc: 0.5
Batch: 100; loss: 1.53; acc: 0.53
Batch: 120; loss: 1.52; acc: 0.55
Batch: 140; loss: 1.58; acc: 0.62
Batch: 160; loss: 1.68; acc: 0.41
Batch: 180; loss: 1.71; acc: 0.48
Batch: 200; loss: 1.4; acc: 0.59
Batch: 220; loss: 1.55; acc: 0.55
Batch: 240; loss: 1.65; acc: 0.52
Batch: 260; loss: 1.51; acc: 0.52
Batch: 280; loss: 1.55; acc: 0.44
Batch: 300; loss: 1.51; acc: 0.53
Batch: 320; loss: 1.52; acc: 0.58
Batch: 340; loss: 1.62; acc: 0.44
Batch: 360; loss: 1.45; acc: 0.64
Batch: 380; loss: 1.47; acc: 0.61
Batch: 400; loss: 1.51; acc: 0.53
Batch: 420; loss: 1.63; acc: 0.48
Batch: 440; loss: 1.6; acc: 0.48
Batch: 460; loss: 1.51; acc: 0.55
Batch: 480; loss: 1.62; acc: 0.47
Batch: 500; loss: 1.54; acc: 0.55
Batch: 520; loss: 1.64; acc: 0.55
Batch: 540; loss: 1.53; acc: 0.5
Batch: 560; loss: 1.44; acc: 0.61
Batch: 580; loss: 1.45; acc: 0.53
Batch: 600; loss: 1.62; acc: 0.53
Batch: 620; loss: 1.52; acc: 0.56
Batch: 640; loss: 1.33; acc: 0.64
Batch: 660; loss: 1.61; acc: 0.56
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.62; acc: 0.53
Batch: 720; loss: 1.7; acc: 0.42
Batch: 740; loss: 1.7; acc: 0.48
Batch: 760; loss: 1.56; acc: 0.5
Batch: 780; loss: 1.43; acc: 0.62
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.5982207666384056e-05
1.758048529154621e-05
Batch: 0; loss: 1.36; acc: 0.73
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.7
Batch: 80; loss: 1.29; acc: 0.69
Batch: 100; loss: 1.51; acc: 0.64
Batch: 120; loss: 1.46; acc: 0.61
Batch: 140; loss: 1.37; acc: 0.72
Val Epoch over. val_loss: 1.470198701901041; val_accuracy: 0.5867834394904459 

The current subspace-distance is: 1.758048529154621e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.51; acc: 0.59
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.6; acc: 0.52
Batch: 60; loss: 1.5; acc: 0.56
Batch: 80; loss: 1.42; acc: 0.56
Batch: 100; loss: 1.58; acc: 0.56
Batch: 120; loss: 1.52; acc: 0.59
Batch: 140; loss: 1.54; acc: 0.55
Batch: 160; loss: 1.5; acc: 0.58
Batch: 180; loss: 1.48; acc: 0.61
Batch: 200; loss: 1.61; acc: 0.53
Batch: 220; loss: 1.46; acc: 0.66
Batch: 240; loss: 1.48; acc: 0.55
Batch: 260; loss: 1.64; acc: 0.45
Batch: 280; loss: 1.52; acc: 0.53
Batch: 300; loss: 1.49; acc: 0.59
Batch: 320; loss: 1.59; acc: 0.53
Batch: 340; loss: 1.62; acc: 0.53
Batch: 360; loss: 1.6; acc: 0.5
Batch: 380; loss: 1.48; acc: 0.56
Batch: 400; loss: 1.59; acc: 0.48
Batch: 420; loss: 1.5; acc: 0.56
Batch: 440; loss: 1.47; acc: 0.56
Batch: 460; loss: 1.7; acc: 0.47
Batch: 480; loss: 1.65; acc: 0.45
Batch: 500; loss: 1.57; acc: 0.48
Batch: 520; loss: 1.47; acc: 0.58
Batch: 540; loss: 1.56; acc: 0.59
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.62; acc: 0.53
Batch: 600; loss: 1.6; acc: 0.52
Batch: 620; loss: 1.6; acc: 0.56
Batch: 640; loss: 1.46; acc: 0.55
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.51; acc: 0.58
Batch: 700; loss: 1.52; acc: 0.56
Batch: 720; loss: 1.72; acc: 0.45
Batch: 740; loss: 1.38; acc: 0.62
Batch: 760; loss: 1.72; acc: 0.44
Batch: 780; loss: 1.37; acc: 0.64
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.6703178668394685e-05
1.4091398952587042e-05
Batch: 0; loss: 1.37; acc: 0.73
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 1.33; acc: 0.69
Batch: 80; loss: 1.3; acc: 0.67
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.46; acc: 0.61
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.4782569142663555; val_accuracy: 0.5779259554140127 

The current subspace-distance is: 1.4091398952587042e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.44
Batch: 20; loss: 1.75; acc: 0.34
Batch: 40; loss: 1.37; acc: 0.64
Batch: 60; loss: 1.49; acc: 0.58
Batch: 80; loss: 1.64; acc: 0.44
Batch: 100; loss: 1.39; acc: 0.67
Batch: 120; loss: 1.44; acc: 0.61
Batch: 140; loss: 1.49; acc: 0.62
Batch: 160; loss: 1.49; acc: 0.56
Batch: 180; loss: 1.56; acc: 0.53
Batch: 200; loss: 1.44; acc: 0.62
Batch: 220; loss: 1.56; acc: 0.61
Batch: 240; loss: 1.38; acc: 0.59
Batch: 260; loss: 1.55; acc: 0.53
Batch: 280; loss: 1.69; acc: 0.52
Batch: 300; loss: 1.36; acc: 0.67
Batch: 320; loss: 1.52; acc: 0.52
Batch: 340; loss: 1.44; acc: 0.58
Batch: 360; loss: 1.58; acc: 0.52
Batch: 380; loss: 1.48; acc: 0.53
Batch: 400; loss: 1.74; acc: 0.42
Batch: 420; loss: 1.48; acc: 0.58
Batch: 440; loss: 1.49; acc: 0.56
Batch: 460; loss: 1.74; acc: 0.45
Batch: 480; loss: 1.39; acc: 0.64
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.59; acc: 0.42
Batch: 540; loss: 1.66; acc: 0.48
Batch: 560; loss: 1.43; acc: 0.59
Batch: 580; loss: 1.57; acc: 0.5
Batch: 600; loss: 1.63; acc: 0.48
Batch: 620; loss: 1.6; acc: 0.45
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.44; acc: 0.66
Batch: 680; loss: 1.48; acc: 0.55
Batch: 700; loss: 1.53; acc: 0.55
Batch: 720; loss: 1.59; acc: 0.47
Batch: 740; loss: 1.52; acc: 0.58
Batch: 760; loss: 1.45; acc: 0.64
Batch: 780; loss: 1.51; acc: 0.55
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.698291013482958e-05
1.5973313566064462e-05
Batch: 0; loss: 1.37; acc: 0.73
Batch: 20; loss: 1.64; acc: 0.48
Batch: 40; loss: 1.22; acc: 0.7
Batch: 60; loss: 1.32; acc: 0.69
Batch: 80; loss: 1.29; acc: 0.64
Batch: 100; loss: 1.52; acc: 0.62
Batch: 120; loss: 1.48; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.7
Val Epoch over. val_loss: 1.4709112181025705; val_accuracy: 0.5806130573248408 

The current subspace-distance is: 1.5973313566064462e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.52
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.45; acc: 0.61
Batch: 60; loss: 1.48; acc: 0.55
Batch: 80; loss: 1.62; acc: 0.47
Batch: 100; loss: 1.55; acc: 0.53
Batch: 120; loss: 1.64; acc: 0.48
Batch: 140; loss: 1.63; acc: 0.5
Batch: 160; loss: 1.66; acc: 0.41
Batch: 180; loss: 1.53; acc: 0.53
Batch: 200; loss: 1.59; acc: 0.53
Batch: 220; loss: 1.73; acc: 0.39
Batch: 240; loss: 1.65; acc: 0.47
Batch: 260; loss: 1.71; acc: 0.36
Batch: 280; loss: 1.54; acc: 0.45
Batch: 300; loss: 1.56; acc: 0.56
Batch: 320; loss: 1.66; acc: 0.5
Batch: 340; loss: 1.62; acc: 0.42
Batch: 360; loss: 1.51; acc: 0.56
Batch: 380; loss: 1.63; acc: 0.5
Batch: 400; loss: 1.71; acc: 0.38
Batch: 420; loss: 1.66; acc: 0.47
Batch: 440; loss: 1.52; acc: 0.53
Batch: 460; loss: 1.62; acc: 0.56
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.55; acc: 0.64
Batch: 520; loss: 1.54; acc: 0.52
Batch: 540; loss: 1.41; acc: 0.67
Batch: 560; loss: 1.51; acc: 0.56
Batch: 580; loss: 1.66; acc: 0.53
Batch: 600; loss: 1.63; acc: 0.5
Batch: 620; loss: 1.66; acc: 0.47
Batch: 640; loss: 1.49; acc: 0.62
Batch: 660; loss: 1.56; acc: 0.5
Batch: 680; loss: 1.43; acc: 0.59
Batch: 700; loss: 1.53; acc: 0.58
Batch: 720; loss: 1.65; acc: 0.47
Batch: 740; loss: 1.51; acc: 0.53
Batch: 760; loss: 1.44; acc: 0.53
Batch: 780; loss: 1.41; acc: 0.64
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.796331995748915e-05
1.631732448004186e-05
Batch: 0; loss: 1.36; acc: 0.7
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.69
Batch: 80; loss: 1.28; acc: 0.64
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 1.36; acc: 0.7
Val Epoch over. val_loss: 1.4658467602577938; val_accuracy: 0.580015923566879 

The current subspace-distance is: 1.631732448004186e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.5
Batch: 20; loss: 1.48; acc: 0.53
Batch: 40; loss: 1.42; acc: 0.61
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.47
Batch: 100; loss: 1.46; acc: 0.59
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.6; acc: 0.52
Batch: 160; loss: 1.54; acc: 0.55
Batch: 180; loss: 1.52; acc: 0.61
Batch: 200; loss: 1.6; acc: 0.53
Batch: 220; loss: 1.53; acc: 0.61
Batch: 240; loss: 1.59; acc: 0.53
Batch: 260; loss: 1.63; acc: 0.56
Batch: 280; loss: 1.47; acc: 0.55
Batch: 300; loss: 1.49; acc: 0.47
Batch: 320; loss: 1.37; acc: 0.69
Batch: 340; loss: 1.45; acc: 0.67
Batch: 360; loss: 1.5; acc: 0.52
Batch: 380; loss: 1.54; acc: 0.52
Batch: 400; loss: 1.55; acc: 0.55
Batch: 420; loss: 1.43; acc: 0.64
Batch: 440; loss: 1.48; acc: 0.52
Batch: 460; loss: 1.48; acc: 0.59
Batch: 480; loss: 1.52; acc: 0.5
Batch: 500; loss: 1.63; acc: 0.48
Batch: 520; loss: 1.54; acc: 0.48
Batch: 540; loss: 1.44; acc: 0.53
Batch: 560; loss: 1.52; acc: 0.53
Batch: 580; loss: 1.39; acc: 0.61
Batch: 600; loss: 1.45; acc: 0.62
Batch: 620; loss: 1.32; acc: 0.7
Batch: 640; loss: 1.51; acc: 0.55
Batch: 660; loss: 1.4; acc: 0.62
Batch: 680; loss: 1.41; acc: 0.56
Batch: 700; loss: 1.44; acc: 0.56
Batch: 720; loss: 1.69; acc: 0.39
Batch: 740; loss: 1.49; acc: 0.53
Batch: 760; loss: 1.49; acc: 0.58
Batch: 780; loss: 1.59; acc: 0.58
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.752662425744347e-05
1.5839645129744895e-05
Batch: 0; loss: 1.36; acc: 0.72
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.32; acc: 0.69
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.52; acc: 0.56
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 1.36; acc: 0.73
Val Epoch over. val_loss: 1.4720978334451178; val_accuracy: 0.5765326433121019 

The current subspace-distance is: 1.5839645129744895e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.48
Batch: 20; loss: 1.51; acc: 0.59
Batch: 40; loss: 1.6; acc: 0.48
Batch: 60; loss: 1.59; acc: 0.48
Batch: 80; loss: 1.41; acc: 0.59
Batch: 100; loss: 1.5; acc: 0.52
Batch: 120; loss: 1.65; acc: 0.5
Batch: 140; loss: 1.43; acc: 0.66
Batch: 160; loss: 1.75; acc: 0.55
Batch: 180; loss: 1.55; acc: 0.5
Batch: 200; loss: 1.37; acc: 0.61
Batch: 220; loss: 1.7; acc: 0.41
Batch: 240; loss: 1.44; acc: 0.61
Batch: 260; loss: 1.65; acc: 0.48
Batch: 280; loss: 1.49; acc: 0.52
Batch: 300; loss: 1.54; acc: 0.62
Batch: 320; loss: 1.53; acc: 0.55
Batch: 340; loss: 1.56; acc: 0.52
Batch: 360; loss: 1.65; acc: 0.53
Batch: 380; loss: 1.62; acc: 0.47
Batch: 400; loss: 1.45; acc: 0.59
Batch: 420; loss: 1.46; acc: 0.59
Batch: 440; loss: 1.48; acc: 0.58
Batch: 460; loss: 1.36; acc: 0.64
Batch: 480; loss: 1.48; acc: 0.48
Batch: 500; loss: 1.53; acc: 0.59
Batch: 520; loss: 1.45; acc: 0.58
Batch: 540; loss: 1.29; acc: 0.69
Batch: 560; loss: 1.55; acc: 0.56
Batch: 580; loss: 1.5; acc: 0.59
Batch: 600; loss: 1.46; acc: 0.62
Batch: 620; loss: 1.6; acc: 0.47
Batch: 640; loss: 1.51; acc: 0.59
Batch: 660; loss: 1.54; acc: 0.53
Batch: 680; loss: 1.47; acc: 0.59
Batch: 700; loss: 1.5; acc: 0.64
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.49; acc: 0.56
Batch: 760; loss: 1.42; acc: 0.58
Batch: 780; loss: 1.65; acc: 0.55
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.669599366025068e-05
1.2257849448360503e-05
Batch: 0; loss: 1.35; acc: 0.72
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.3; acc: 0.7
Batch: 80; loss: 1.27; acc: 0.67
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 1.36; acc: 0.67
Val Epoch over. val_loss: 1.463809225969254; val_accuracy: 0.5821058917197452 

The current subspace-distance is: 1.2257849448360503e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.59
Batch: 20; loss: 1.5; acc: 0.56
Batch: 40; loss: 1.52; acc: 0.55
Batch: 60; loss: 1.53; acc: 0.48
Batch: 80; loss: 1.44; acc: 0.61
Batch: 100; loss: 1.65; acc: 0.55
Batch: 120; loss: 1.63; acc: 0.55
Batch: 140; loss: 1.38; acc: 0.66
Batch: 160; loss: 1.57; acc: 0.52
Batch: 180; loss: 1.39; acc: 0.62
Batch: 200; loss: 1.53; acc: 0.55
Batch: 220; loss: 1.62; acc: 0.47
Batch: 240; loss: 1.51; acc: 0.53
Batch: 260; loss: 1.49; acc: 0.53
Batch: 280; loss: 1.77; acc: 0.36
Batch: 300; loss: 1.33; acc: 0.62
Batch: 320; loss: 1.53; acc: 0.53
Batch: 340; loss: 1.63; acc: 0.53
Batch: 360; loss: 1.51; acc: 0.56
Batch: 380; loss: 1.54; acc: 0.5
Batch: 400; loss: 1.41; acc: 0.69
Batch: 420; loss: 1.58; acc: 0.53
Batch: 440; loss: 1.74; acc: 0.41
Batch: 460; loss: 1.76; acc: 0.41
Batch: 480; loss: 1.34; acc: 0.61
Batch: 500; loss: 1.6; acc: 0.52
Batch: 520; loss: 1.57; acc: 0.52
Batch: 540; loss: 1.38; acc: 0.64
Batch: 560; loss: 1.42; acc: 0.48
Batch: 580; loss: 1.56; acc: 0.58
Batch: 600; loss: 1.56; acc: 0.53
Batch: 620; loss: 1.71; acc: 0.45
Batch: 640; loss: 1.58; acc: 0.55
Batch: 660; loss: 1.37; acc: 0.64
Batch: 680; loss: 1.49; acc: 0.55
Batch: 700; loss: 1.51; acc: 0.5
Batch: 720; loss: 1.66; acc: 0.45
Batch: 740; loss: 1.52; acc: 0.62
Batch: 760; loss: 1.46; acc: 0.58
Batch: 780; loss: 1.49; acc: 0.52
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.865726441494189e-05
1.861068631114904e-05
Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.64
Batch: 100; loss: 1.52; acc: 0.58
Batch: 120; loss: 1.46; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.7
Val Epoch over. val_loss: 1.4674368701922667; val_accuracy: 0.5726512738853503 

The current subspace-distance is: 1.861068631114904e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.35; acc: 0.66
Batch: 20; loss: 1.58; acc: 0.53
Batch: 40; loss: 1.45; acc: 0.56
Batch: 60; loss: 1.51; acc: 0.64
Batch: 80; loss: 1.56; acc: 0.5
Batch: 100; loss: 1.46; acc: 0.62
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.64; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.59
Batch: 180; loss: 1.66; acc: 0.45
Batch: 200; loss: 1.41; acc: 0.55
Batch: 220; loss: 1.58; acc: 0.47
Batch: 240; loss: 1.6; acc: 0.52
Batch: 260; loss: 1.43; acc: 0.64
Batch: 280; loss: 1.44; acc: 0.53
Batch: 300; loss: 1.53; acc: 0.53
Batch: 320; loss: 1.33; acc: 0.7
Batch: 340; loss: 1.52; acc: 0.58
Batch: 360; loss: 1.6; acc: 0.55
Batch: 380; loss: 1.52; acc: 0.47
Batch: 400; loss: 1.55; acc: 0.48
Batch: 420; loss: 1.58; acc: 0.52
Batch: 440; loss: 1.43; acc: 0.56
Batch: 460; loss: 1.53; acc: 0.55
Batch: 480; loss: 1.63; acc: 0.48
Batch: 500; loss: 1.48; acc: 0.53
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.46; acc: 0.53
Batch: 560; loss: 1.61; acc: 0.52
Batch: 580; loss: 1.42; acc: 0.59
Batch: 600; loss: 1.55; acc: 0.56
Batch: 620; loss: 1.42; acc: 0.61
Batch: 640; loss: 1.4; acc: 0.69
Batch: 660; loss: 1.5; acc: 0.61
Batch: 680; loss: 1.41; acc: 0.61
Batch: 700; loss: 1.49; acc: 0.55
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.45; acc: 0.59
Batch: 760; loss: 1.49; acc: 0.58
Batch: 780; loss: 1.48; acc: 0.56
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.8448135203216225e-05
1.719593274174258e-05
Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 1.62; acc: 0.5
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.3; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.61
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.47; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.4612163867160772; val_accuracy: 0.5763335987261147 

The current subspace-distance is: 1.719593274174258e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.4; acc: 0.61
Batch: 40; loss: 1.6; acc: 0.5
Batch: 60; loss: 1.52; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.47
Batch: 100; loss: 1.56; acc: 0.52
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.52; acc: 0.52
Batch: 160; loss: 1.48; acc: 0.66
Batch: 180; loss: 1.61; acc: 0.52
Batch: 200; loss: 1.57; acc: 0.47
Batch: 220; loss: 1.42; acc: 0.56
Batch: 240; loss: 1.56; acc: 0.55
Batch: 260; loss: 1.57; acc: 0.45
Batch: 280; loss: 1.6; acc: 0.48
Batch: 300; loss: 1.67; acc: 0.44
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.47; acc: 0.53
Batch: 360; loss: 1.51; acc: 0.58
Batch: 380; loss: 1.45; acc: 0.59
Batch: 400; loss: 1.48; acc: 0.53
Batch: 420; loss: 1.58; acc: 0.55
Batch: 440; loss: 1.57; acc: 0.5
Batch: 460; loss: 1.67; acc: 0.52
Batch: 480; loss: 1.64; acc: 0.55
Batch: 500; loss: 1.45; acc: 0.56
Batch: 520; loss: 1.39; acc: 0.58
Batch: 540; loss: 1.61; acc: 0.48
Batch: 560; loss: 1.57; acc: 0.5
Batch: 580; loss: 1.65; acc: 0.53
Batch: 600; loss: 1.69; acc: 0.44
Batch: 620; loss: 1.62; acc: 0.45
Batch: 640; loss: 1.6; acc: 0.52
Batch: 660; loss: 1.71; acc: 0.47
Batch: 680; loss: 1.66; acc: 0.45
Batch: 700; loss: 1.41; acc: 0.64
Batch: 720; loss: 1.61; acc: 0.52
Batch: 740; loss: 1.63; acc: 0.48
Batch: 760; loss: 1.47; acc: 0.61
Batch: 780; loss: 1.54; acc: 0.53
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.731793160317466e-05
1.5688698113081045e-05
Batch: 0; loss: 1.36; acc: 0.72
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.31; acc: 0.7
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.51; acc: 0.62
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.72
Val Epoch over. val_loss: 1.4655768894086219; val_accuracy: 0.5832006369426752 

The current subspace-distance is: 1.5688698113081045e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.55
Batch: 20; loss: 1.42; acc: 0.62
Batch: 40; loss: 1.49; acc: 0.52
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.78; acc: 0.42
Batch: 100; loss: 1.53; acc: 0.59
Batch: 120; loss: 1.58; acc: 0.5
Batch: 140; loss: 1.44; acc: 0.61
Batch: 160; loss: 1.66; acc: 0.52
Batch: 180; loss: 1.57; acc: 0.59
Batch: 200; loss: 1.63; acc: 0.53
Batch: 220; loss: 1.49; acc: 0.55
Batch: 240; loss: 1.5; acc: 0.58
Batch: 260; loss: 1.51; acc: 0.47
Batch: 280; loss: 1.59; acc: 0.5
Batch: 300; loss: 1.53; acc: 0.64
Batch: 320; loss: 1.52; acc: 0.62
Batch: 340; loss: 1.4; acc: 0.61
Batch: 360; loss: 1.63; acc: 0.5
Batch: 380; loss: 1.54; acc: 0.55
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.45; acc: 0.55
Batch: 440; loss: 1.6; acc: 0.53
Batch: 460; loss: 1.53; acc: 0.53
Batch: 480; loss: 1.44; acc: 0.58
Batch: 500; loss: 1.32; acc: 0.64
Batch: 520; loss: 1.35; acc: 0.62
Batch: 540; loss: 1.64; acc: 0.47
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.43; acc: 0.67
Batch: 600; loss: 1.43; acc: 0.59
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.59; acc: 0.56
Batch: 660; loss: 1.44; acc: 0.56
Batch: 680; loss: 1.53; acc: 0.56
Batch: 700; loss: 1.57; acc: 0.53
Batch: 720; loss: 1.58; acc: 0.55
Batch: 740; loss: 1.44; acc: 0.52
Batch: 760; loss: 1.69; acc: 0.36
Batch: 780; loss: 1.68; acc: 0.5
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.843440547119826e-05
1.4830205145699438e-05
Batch: 0; loss: 1.36; acc: 0.72
Batch: 20; loss: 1.62; acc: 0.47
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.3; acc: 0.69
Batch: 80; loss: 1.28; acc: 0.62
Batch: 100; loss: 1.51; acc: 0.56
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 1.36; acc: 0.67
Val Epoch over. val_loss: 1.4643274226765723; val_accuracy: 0.5753383757961783 

The current subspace-distance is: 1.4830205145699438e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.5
Batch: 20; loss: 1.54; acc: 0.56
Batch: 40; loss: 1.38; acc: 0.66
Batch: 60; loss: 1.4; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.69
Batch: 100; loss: 1.51; acc: 0.59
Batch: 120; loss: 1.61; acc: 0.52
Batch: 140; loss: 1.63; acc: 0.48
Batch: 160; loss: 1.54; acc: 0.47
Batch: 180; loss: 1.63; acc: 0.44
Batch: 200; loss: 1.44; acc: 0.58
Batch: 220; loss: 1.55; acc: 0.52
Batch: 240; loss: 1.61; acc: 0.45
Batch: 260; loss: 1.58; acc: 0.5
Batch: 280; loss: 1.48; acc: 0.53
Batch: 300; loss: 1.64; acc: 0.53
Batch: 320; loss: 1.58; acc: 0.52
Batch: 340; loss: 1.47; acc: 0.58
Batch: 360; loss: 1.46; acc: 0.52
Batch: 380; loss: 1.54; acc: 0.5
Batch: 400; loss: 1.51; acc: 0.48
Batch: 420; loss: 1.65; acc: 0.48
Batch: 440; loss: 1.45; acc: 0.59
Batch: 460; loss: 1.56; acc: 0.55
Batch: 480; loss: 1.5; acc: 0.53
Batch: 500; loss: 1.62; acc: 0.41
Batch: 520; loss: 1.48; acc: 0.55
Batch: 540; loss: 1.6; acc: 0.5
Batch: 560; loss: 1.43; acc: 0.64
Batch: 580; loss: 1.46; acc: 0.56
Batch: 600; loss: 1.65; acc: 0.48
Batch: 620; loss: 1.46; acc: 0.62
Batch: 640; loss: 1.5; acc: 0.53
Batch: 660; loss: 1.61; acc: 0.52
Batch: 680; loss: 1.47; acc: 0.58
Batch: 700; loss: 1.47; acc: 0.52
Batch: 720; loss: 1.57; acc: 0.47
Batch: 740; loss: 1.67; acc: 0.44
Batch: 760; loss: 1.44; acc: 0.58
Batch: 780; loss: 1.66; acc: 0.41
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.735306356451474e-05
1.4703711713082157e-05
Batch: 0; loss: 1.34; acc: 0.73
Batch: 20; loss: 1.62; acc: 0.47
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.3; acc: 0.7
Batch: 80; loss: 1.26; acc: 0.64
Batch: 100; loss: 1.52; acc: 0.58
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.37; acc: 0.67
Val Epoch over. val_loss: 1.4646442862832623; val_accuracy: 0.5730493630573248 

The current subspace-distance is: 1.4703711713082157e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.48
Batch: 20; loss: 1.5; acc: 0.59
Batch: 40; loss: 1.59; acc: 0.53
Batch: 60; loss: 1.47; acc: 0.48
Batch: 80; loss: 1.6; acc: 0.53
Batch: 100; loss: 1.44; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.56
Batch: 140; loss: 1.52; acc: 0.59
Batch: 160; loss: 1.44; acc: 0.58
Batch: 180; loss: 1.49; acc: 0.59
Batch: 200; loss: 1.52; acc: 0.53
Batch: 220; loss: 1.55; acc: 0.56
Batch: 240; loss: 1.51; acc: 0.59
Batch: 260; loss: 1.54; acc: 0.55
Batch: 280; loss: 1.59; acc: 0.44
Batch: 300; loss: 1.61; acc: 0.52
Batch: 320; loss: 1.6; acc: 0.53
Batch: 340; loss: 1.43; acc: 0.55
Batch: 360; loss: 1.52; acc: 0.52
Batch: 380; loss: 1.52; acc: 0.47
Batch: 400; loss: 1.51; acc: 0.48
Batch: 420; loss: 1.48; acc: 0.58
Batch: 440; loss: 1.54; acc: 0.53
Batch: 460; loss: 1.4; acc: 0.58
Batch: 480; loss: 1.53; acc: 0.53
Batch: 500; loss: 1.55; acc: 0.52
Batch: 520; loss: 1.57; acc: 0.52
Batch: 540; loss: 1.62; acc: 0.45
Batch: 560; loss: 1.75; acc: 0.45
Batch: 580; loss: 1.53; acc: 0.45
Batch: 600; loss: 1.62; acc: 0.52
Batch: 620; loss: 1.35; acc: 0.66
Batch: 640; loss: 1.52; acc: 0.59
Batch: 660; loss: 1.48; acc: 0.5
Batch: 680; loss: 1.56; acc: 0.56
Batch: 700; loss: 1.41; acc: 0.62
Batch: 720; loss: 1.45; acc: 0.62
Batch: 740; loss: 1.62; acc: 0.58
Batch: 760; loss: 1.47; acc: 0.59
Batch: 780; loss: 1.48; acc: 0.58
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.827480734093115e-05
1.570938366057817e-05
Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.19; acc: 0.72
Batch: 60; loss: 1.28; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.64
Batch: 100; loss: 1.5; acc: 0.58
Batch: 120; loss: 1.47; acc: 0.56
Batch: 140; loss: 1.35; acc: 0.73
Val Epoch over. val_loss: 1.4540633684510638; val_accuracy: 0.5829020700636943 

The current subspace-distance is: 1.570938366057817e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.55
Batch: 20; loss: 1.56; acc: 0.44
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.55; acc: 0.58
Batch: 80; loss: 1.65; acc: 0.42
Batch: 100; loss: 1.58; acc: 0.42
Batch: 120; loss: 1.38; acc: 0.67
Batch: 140; loss: 1.47; acc: 0.62
Batch: 160; loss: 1.5; acc: 0.56
Batch: 180; loss: 1.56; acc: 0.58
Batch: 200; loss: 1.45; acc: 0.61
Batch: 220; loss: 1.63; acc: 0.58
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.45
Batch: 280; loss: 1.52; acc: 0.59
Batch: 300; loss: 1.57; acc: 0.55
Batch: 320; loss: 1.55; acc: 0.56
Batch: 340; loss: 1.47; acc: 0.52
Batch: 360; loss: 1.77; acc: 0.42
Batch: 380; loss: 1.59; acc: 0.55
Batch: 400; loss: 1.61; acc: 0.45
Batch: 420; loss: 1.68; acc: 0.45
Batch: 440; loss: 1.45; acc: 0.56
Batch: 460; loss: 1.58; acc: 0.5
Batch: 480; loss: 1.55; acc: 0.56
Batch: 500; loss: 1.67; acc: 0.47
Batch: 520; loss: 1.41; acc: 0.59
Batch: 540; loss: 1.54; acc: 0.59
Batch: 560; loss: 1.7; acc: 0.42
Batch: 580; loss: 1.72; acc: 0.52
Batch: 600; loss: 1.37; acc: 0.64
Batch: 620; loss: 1.58; acc: 0.56
Batch: 640; loss: 1.47; acc: 0.56
Batch: 660; loss: 1.64; acc: 0.5
Batch: 680; loss: 1.47; acc: 0.56
Batch: 700; loss: 1.49; acc: 0.56
Batch: 720; loss: 1.42; acc: 0.56
Batch: 740; loss: 1.64; acc: 0.53
Batch: 760; loss: 1.48; acc: 0.45
Batch: 780; loss: 1.4; acc: 0.58
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.876005186815746e-05
1.6609756130492315e-05
Batch: 0; loss: 1.36; acc: 0.7
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.3; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.61
Batch: 100; loss: 1.52; acc: 0.56
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.7
Val Epoch over. val_loss: 1.4624193261383445; val_accuracy: 0.573546974522293 

The current subspace-distance is: 1.6609756130492315e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.47
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 1.43; acc: 0.58
Batch: 60; loss: 1.47; acc: 0.56
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.52; acc: 0.55
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.48; acc: 0.53
Batch: 180; loss: 1.49; acc: 0.58
Batch: 200; loss: 1.54; acc: 0.55
Batch: 220; loss: 1.65; acc: 0.44
Batch: 240; loss: 1.37; acc: 0.53
Batch: 260; loss: 1.55; acc: 0.55
Batch: 280; loss: 1.62; acc: 0.56
Batch: 300; loss: 1.51; acc: 0.56
Batch: 320; loss: 1.68; acc: 0.52
Batch: 340; loss: 1.45; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.55
Batch: 380; loss: 1.5; acc: 0.53
Batch: 400; loss: 1.57; acc: 0.5
Batch: 420; loss: 1.68; acc: 0.41
Batch: 440; loss: 1.55; acc: 0.5
Batch: 460; loss: 1.58; acc: 0.47
Batch: 480; loss: 1.46; acc: 0.55
Batch: 500; loss: 1.36; acc: 0.61
Batch: 520; loss: 1.44; acc: 0.64
Batch: 540; loss: 1.52; acc: 0.58
Batch: 560; loss: 1.48; acc: 0.58
Batch: 580; loss: 1.44; acc: 0.59
Batch: 600; loss: 1.56; acc: 0.56
Batch: 620; loss: 1.66; acc: 0.52
Batch: 640; loss: 1.48; acc: 0.53
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.39; acc: 0.64
Batch: 700; loss: 1.41; acc: 0.64
Batch: 720; loss: 1.67; acc: 0.44
Batch: 740; loss: 1.42; acc: 0.58
Batch: 760; loss: 1.36; acc: 0.66
Batch: 780; loss: 1.51; acc: 0.53
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.9867558118421584e-05
1.9564482499845326e-05
Batch: 0; loss: 1.35; acc: 0.73
Batch: 20; loss: 1.6; acc: 0.48
Batch: 40; loss: 1.19; acc: 0.69
Batch: 60; loss: 1.29; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.61
Batch: 100; loss: 1.51; acc: 0.56
Batch: 120; loss: 1.46; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.7
Val Epoch over. val_loss: 1.460964369166429; val_accuracy: 0.5772292993630573 

The current subspace-distance is: 1.9564482499845326e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.51; acc: 0.45
Batch: 20; loss: 1.37; acc: 0.62
Batch: 40; loss: 1.44; acc: 0.58
Batch: 60; loss: 1.46; acc: 0.62
Batch: 80; loss: 1.51; acc: 0.5
Batch: 100; loss: 1.7; acc: 0.48
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 1.58; acc: 0.47
Batch: 160; loss: 1.48; acc: 0.58
Batch: 180; loss: 1.72; acc: 0.45
Batch: 200; loss: 1.55; acc: 0.52
Batch: 220; loss: 1.35; acc: 0.64
Batch: 240; loss: 1.51; acc: 0.48
Batch: 260; loss: 1.67; acc: 0.48
Batch: 280; loss: 1.71; acc: 0.39
Batch: 300; loss: 1.37; acc: 0.64
Batch: 320; loss: 1.48; acc: 0.56
Batch: 340; loss: 1.62; acc: 0.5
Batch: 360; loss: 1.61; acc: 0.47
Batch: 380; loss: 1.55; acc: 0.5
Batch: 400; loss: 1.5; acc: 0.5
Batch: 420; loss: 1.43; acc: 0.59
Batch: 440; loss: 1.38; acc: 0.61
Batch: 460; loss: 1.53; acc: 0.52
Batch: 480; loss: 1.59; acc: 0.47
Batch: 500; loss: 1.58; acc: 0.55
Batch: 520; loss: 1.41; acc: 0.61
Batch: 540; loss: 1.48; acc: 0.53
Batch: 560; loss: 1.4; acc: 0.62
Batch: 580; loss: 1.59; acc: 0.44
Batch: 600; loss: 1.51; acc: 0.56
Batch: 620; loss: 1.42; acc: 0.66
Batch: 640; loss: 1.5; acc: 0.55
Batch: 660; loss: 1.48; acc: 0.56
Batch: 680; loss: 1.6; acc: 0.59
Batch: 700; loss: 1.48; acc: 0.59
Batch: 720; loss: 1.45; acc: 0.59
Batch: 740; loss: 1.68; acc: 0.44
Batch: 760; loss: 1.57; acc: 0.44
Batch: 780; loss: 1.62; acc: 0.48
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.721934237750247e-05
1.236313437402714e-05
Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 1.6; acc: 0.48
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.28; acc: 0.7
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.5; acc: 0.59
Batch: 120; loss: 1.47; acc: 0.59
Batch: 140; loss: 1.35; acc: 0.66
Val Epoch over. val_loss: 1.4522191984638286; val_accuracy: 0.5774283439490446 

The current subspace-distance is: 1.236313437402714e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_8_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6

The number of parameters is: 266828

The number of individual parameters is:

13
234
13
13
20
35360
20
20
39
106080
39
39
64
119808
64
64
4096
64
640
10
64
64

nonzero elements in E: 26682797
elements in E: 26682800
fraction nonzero: 0.9999998875680214
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.19
Batch: 40; loss: 2.26; acc: 0.19
Batch: 60; loss: 2.18; acc: 0.22
Batch: 80; loss: 2.03; acc: 0.28
Batch: 100; loss: 2.17; acc: 0.23
Batch: 120; loss: 2.0; acc: 0.36
Batch: 140; loss: 2.01; acc: 0.38
Batch: 160; loss: 1.87; acc: 0.47
Batch: 180; loss: 1.89; acc: 0.39
Batch: 200; loss: 1.91; acc: 0.36
Batch: 220; loss: 1.96; acc: 0.41
Batch: 240; loss: 1.87; acc: 0.44
Batch: 260; loss: 1.84; acc: 0.42
Batch: 280; loss: 1.78; acc: 0.53
Batch: 300; loss: 1.8; acc: 0.52
Batch: 320; loss: 1.87; acc: 0.45
Batch: 340; loss: 1.74; acc: 0.53
Batch: 360; loss: 1.76; acc: 0.53
Batch: 380; loss: 1.77; acc: 0.47
Batch: 400; loss: 1.67; acc: 0.58
Batch: 420; loss: 1.74; acc: 0.53
Batch: 440; loss: 1.8; acc: 0.56
Batch: 460; loss: 1.72; acc: 0.55
Batch: 480; loss: 1.76; acc: 0.52
Batch: 500; loss: 1.82; acc: 0.48
Batch: 520; loss: 1.71; acc: 0.53
Batch: 540; loss: 1.64; acc: 0.55
Batch: 560; loss: 1.8; acc: 0.5
Batch: 580; loss: 1.72; acc: 0.55
Batch: 600; loss: 1.7; acc: 0.59
Batch: 620; loss: 1.72; acc: 0.48
Batch: 640; loss: 1.72; acc: 0.48
Batch: 660; loss: 1.68; acc: 0.52
Batch: 680; loss: 1.67; acc: 0.64
Batch: 700; loss: 1.8; acc: 0.42
Batch: 720; loss: 1.69; acc: 0.53
Batch: 740; loss: 1.72; acc: 0.48
Batch: 760; loss: 1.64; acc: 0.62
Batch: 780; loss: 1.64; acc: 0.64
Train Epoch over. train_loss: 1.84; train_accuracy: 0.45 

5.415915802586824e-05
4.903464287053794e-05
Batch: 0; loss: 1.7; acc: 0.5
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.38; acc: 0.75
Batch: 60; loss: 1.62; acc: 0.62
Batch: 80; loss: 1.44; acc: 0.78
Batch: 100; loss: 1.62; acc: 0.62
Batch: 120; loss: 1.76; acc: 0.44
Batch: 140; loss: 1.48; acc: 0.67
Val Epoch over. val_loss: 1.6401217226769513; val_accuracy: 0.5788216560509554 

The current subspace-distance is: 4.903464287053794e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.63; acc: 0.58
Batch: 40; loss: 1.65; acc: 0.56
Batch: 60; loss: 1.71; acc: 0.56
Batch: 80; loss: 1.65; acc: 0.48
Batch: 100; loss: 1.63; acc: 0.61
Batch: 120; loss: 1.6; acc: 0.62
Batch: 140; loss: 1.62; acc: 0.58
Batch: 160; loss: 1.69; acc: 0.56
Batch: 180; loss: 1.64; acc: 0.56
Batch: 200; loss: 1.58; acc: 0.61
Batch: 220; loss: 1.68; acc: 0.47
Batch: 240; loss: 1.58; acc: 0.55
Batch: 260; loss: 1.73; acc: 0.48
Batch: 280; loss: 1.65; acc: 0.58
Batch: 300; loss: 1.69; acc: 0.52
Batch: 320; loss: 1.53; acc: 0.64
Batch: 340; loss: 1.69; acc: 0.52
Batch: 360; loss: 1.65; acc: 0.5
Batch: 380; loss: 1.6; acc: 0.58
Batch: 400; loss: 1.61; acc: 0.53
Batch: 420; loss: 1.71; acc: 0.47
Batch: 440; loss: 1.67; acc: 0.44
Batch: 460; loss: 1.65; acc: 0.56
Batch: 480; loss: 1.71; acc: 0.5
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.57; acc: 0.58
Batch: 540; loss: 1.8; acc: 0.48
Batch: 560; loss: 1.6; acc: 0.58
Batch: 580; loss: 1.49; acc: 0.56
Batch: 600; loss: 1.48; acc: 0.7
Batch: 620; loss: 1.44; acc: 0.64
Batch: 640; loss: 1.77; acc: 0.5
Batch: 660; loss: 1.57; acc: 0.58
Batch: 680; loss: 1.56; acc: 0.56
Batch: 700; loss: 1.49; acc: 0.66
Batch: 720; loss: 1.58; acc: 0.66
Batch: 740; loss: 1.63; acc: 0.58
Batch: 760; loss: 1.47; acc: 0.66
Batch: 780; loss: 1.56; acc: 0.61
Train Epoch over. train_loss: 1.62; train_accuracy: 0.56 

6.776222289772704e-05
6.235906039364636e-05
Batch: 0; loss: 1.56; acc: 0.56
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 1.56; acc: 0.62
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.54; acc: 0.67
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.43; acc: 0.66
Val Epoch over. val_loss: 1.5425845118844586; val_accuracy: 0.5923566878980892 

The current subspace-distance is: 6.235906039364636e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.59; acc: 0.47
Batch: 20; loss: 1.57; acc: 0.59
Batch: 40; loss: 1.86; acc: 0.44
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.4; acc: 0.67
Batch: 100; loss: 1.55; acc: 0.58
Batch: 120; loss: 1.61; acc: 0.59
Batch: 140; loss: 1.54; acc: 0.61
Batch: 160; loss: 1.56; acc: 0.55
Batch: 180; loss: 1.52; acc: 0.66
Batch: 200; loss: 1.47; acc: 0.64
Batch: 220; loss: 1.57; acc: 0.59
Batch: 240; loss: 1.54; acc: 0.66
Batch: 260; loss: 1.49; acc: 0.64
Batch: 280; loss: 1.55; acc: 0.56
Batch: 300; loss: 1.59; acc: 0.59
Batch: 320; loss: 1.48; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.56
Batch: 360; loss: 1.48; acc: 0.61
Batch: 380; loss: 1.61; acc: 0.53
Batch: 400; loss: 1.52; acc: 0.56
Batch: 420; loss: 1.42; acc: 0.64
Batch: 440; loss: 1.54; acc: 0.64
Batch: 460; loss: 1.45; acc: 0.61
Batch: 480; loss: 1.44; acc: 0.67
Batch: 500; loss: 1.43; acc: 0.62
Batch: 520; loss: 1.45; acc: 0.62
Batch: 540; loss: 1.51; acc: 0.5
Batch: 560; loss: 1.62; acc: 0.5
Batch: 580; loss: 1.49; acc: 0.56
Batch: 600; loss: 1.6; acc: 0.56
Batch: 620; loss: 1.56; acc: 0.55
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.63; acc: 0.48
Batch: 680; loss: 1.55; acc: 0.56
Batch: 700; loss: 1.57; acc: 0.56
Batch: 720; loss: 1.59; acc: 0.56
Batch: 740; loss: 1.67; acc: 0.52
Batch: 760; loss: 1.56; acc: 0.53
Batch: 780; loss: 1.39; acc: 0.61
Train Epoch over. train_loss: 1.53; train_accuracy: 0.58 

8.175045513780788e-05
7.656482921447605e-05
Batch: 0; loss: 1.48; acc: 0.62
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.28; acc: 0.73
Batch: 100; loss: 1.47; acc: 0.67
Batch: 120; loss: 1.58; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.62
Val Epoch over. val_loss: 1.4630689499484506; val_accuracy: 0.6256966560509554 

The current subspace-distance is: 7.656482921447605e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.61
Batch: 20; loss: 1.52; acc: 0.56
Batch: 40; loss: 1.52; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.59
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.57; acc: 0.52
Batch: 140; loss: 1.38; acc: 0.62
Batch: 160; loss: 1.48; acc: 0.58
Batch: 180; loss: 1.54; acc: 0.56
Batch: 200; loss: 1.51; acc: 0.59
Batch: 220; loss: 1.53; acc: 0.56
Batch: 240; loss: 1.49; acc: 0.59
Batch: 260; loss: 1.45; acc: 0.62
Batch: 280; loss: 1.48; acc: 0.66
Batch: 300; loss: 1.55; acc: 0.61
Batch: 320; loss: 1.64; acc: 0.56
Batch: 340; loss: 1.45; acc: 0.61
Batch: 360; loss: 1.48; acc: 0.61
Batch: 380; loss: 1.32; acc: 0.72
Batch: 400; loss: 1.47; acc: 0.61
Batch: 420; loss: 1.63; acc: 0.53
Batch: 440; loss: 1.44; acc: 0.58
Batch: 460; loss: 1.43; acc: 0.61
Batch: 480; loss: 1.3; acc: 0.72
Batch: 500; loss: 1.41; acc: 0.59
Batch: 520; loss: 1.53; acc: 0.64
Batch: 540; loss: 1.38; acc: 0.62
Batch: 560; loss: 1.45; acc: 0.56
Batch: 580; loss: 1.39; acc: 0.67
Batch: 600; loss: 1.32; acc: 0.73
Batch: 620; loss: 1.35; acc: 0.7
Batch: 640; loss: 1.35; acc: 0.66
Batch: 660; loss: 1.53; acc: 0.48
Batch: 680; loss: 1.51; acc: 0.61
Batch: 700; loss: 1.53; acc: 0.53
Batch: 720; loss: 1.42; acc: 0.64
Batch: 740; loss: 1.46; acc: 0.64
Batch: 760; loss: 1.38; acc: 0.62
Batch: 780; loss: 1.42; acc: 0.56
Train Epoch over. train_loss: 1.47; train_accuracy: 0.6 

9.375219087814912e-05
8.806097321212292e-05
Batch: 0; loss: 1.42; acc: 0.67
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.27; acc: 0.75
Batch: 60; loss: 1.4; acc: 0.66
Batch: 80; loss: 1.25; acc: 0.69
Batch: 100; loss: 1.43; acc: 0.72
Batch: 120; loss: 1.55; acc: 0.56
Batch: 140; loss: 1.36; acc: 0.64
Val Epoch over. val_loss: 1.41998108329287; val_accuracy: 0.6399283439490446 

The current subspace-distance is: 8.806097321212292e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.5; acc: 0.59
Batch: 20; loss: 1.38; acc: 0.69
Batch: 40; loss: 1.53; acc: 0.5
Batch: 60; loss: 1.5; acc: 0.59
Batch: 80; loss: 1.61; acc: 0.47
Batch: 100; loss: 1.4; acc: 0.62
Batch: 120; loss: 1.56; acc: 0.56
Batch: 140; loss: 1.27; acc: 0.69
Batch: 160; loss: 1.56; acc: 0.55
Batch: 180; loss: 1.45; acc: 0.64
Batch: 200; loss: 1.34; acc: 0.61
Batch: 220; loss: 1.31; acc: 0.7
Batch: 240; loss: 1.46; acc: 0.59
Batch: 260; loss: 1.34; acc: 0.69
Batch: 280; loss: 1.4; acc: 0.64
Batch: 300; loss: 1.46; acc: 0.58
Batch: 320; loss: 1.5; acc: 0.58
Batch: 340; loss: 1.39; acc: 0.64
Batch: 360; loss: 1.53; acc: 0.53
Batch: 380; loss: 1.41; acc: 0.55
Batch: 400; loss: 1.31; acc: 0.69
Batch: 420; loss: 1.45; acc: 0.53
Batch: 440; loss: 1.3; acc: 0.69
Batch: 460; loss: 1.45; acc: 0.56
Batch: 480; loss: 1.41; acc: 0.64
Batch: 500; loss: 1.45; acc: 0.61
Batch: 520; loss: 1.44; acc: 0.62
Batch: 540; loss: 1.53; acc: 0.58
Batch: 560; loss: 1.4; acc: 0.61
Batch: 580; loss: 1.47; acc: 0.55
Batch: 600; loss: 1.37; acc: 0.64
Batch: 620; loss: 1.55; acc: 0.5
Batch: 640; loss: 1.35; acc: 0.66
Batch: 660; loss: 1.34; acc: 0.77
Batch: 680; loss: 1.47; acc: 0.56
Batch: 700; loss: 1.36; acc: 0.64
Batch: 720; loss: 1.4; acc: 0.58
Batch: 740; loss: 1.34; acc: 0.69
Batch: 760; loss: 1.48; acc: 0.56
Batch: 780; loss: 1.41; acc: 0.58
Train Epoch over. train_loss: 1.44; train_accuracy: 0.61 

0.00010301877773599699
9.601956844562665e-05
Batch: 0; loss: 1.38; acc: 0.67
Batch: 20; loss: 1.49; acc: 0.59
Batch: 40; loss: 1.26; acc: 0.73
Batch: 60; loss: 1.37; acc: 0.67
Batch: 80; loss: 1.24; acc: 0.69
Batch: 100; loss: 1.4; acc: 0.62
Batch: 120; loss: 1.51; acc: 0.61
Batch: 140; loss: 1.32; acc: 0.61
Val Epoch over. val_loss: 1.3871220198406535; val_accuracy: 0.6438097133757962 

The current subspace-distance is: 9.601956844562665e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.64
Batch: 40; loss: 1.35; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.69
Batch: 80; loss: 1.26; acc: 0.72
Batch: 100; loss: 1.39; acc: 0.66
Batch: 120; loss: 1.44; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.66
Batch: 160; loss: 1.49; acc: 0.55
Batch: 180; loss: 1.62; acc: 0.47
Batch: 200; loss: 1.42; acc: 0.58
Batch: 220; loss: 1.37; acc: 0.58
Batch: 240; loss: 1.41; acc: 0.64
Batch: 260; loss: 1.45; acc: 0.55
Batch: 280; loss: 1.51; acc: 0.53
Batch: 300; loss: 1.42; acc: 0.59
Batch: 320; loss: 1.4; acc: 0.56
Batch: 340; loss: 1.35; acc: 0.69
Batch: 360; loss: 1.33; acc: 0.64
Batch: 380; loss: 1.4; acc: 0.61
Batch: 400; loss: 1.58; acc: 0.47
Batch: 420; loss: 1.39; acc: 0.56
Batch: 440; loss: 1.32; acc: 0.66
Batch: 460; loss: 1.42; acc: 0.66
Batch: 480; loss: 1.33; acc: 0.67
Batch: 500; loss: 1.53; acc: 0.61
Batch: 520; loss: 1.43; acc: 0.69
Batch: 540; loss: 1.52; acc: 0.53
Batch: 560; loss: 1.48; acc: 0.58
Batch: 580; loss: 1.48; acc: 0.62
Batch: 600; loss: 1.45; acc: 0.55
Batch: 620; loss: 1.41; acc: 0.59
Batch: 640; loss: 1.39; acc: 0.62
Batch: 660; loss: 1.32; acc: 0.69
Batch: 680; loss: 1.53; acc: 0.58
Batch: 700; loss: 1.48; acc: 0.53
Batch: 720; loss: 1.51; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.61
Batch: 760; loss: 1.3; acc: 0.66
Batch: 780; loss: 1.36; acc: 0.61
Train Epoch over. train_loss: 1.41; train_accuracy: 0.61 

0.00011033554619643837
0.00010463741637067869
Batch: 0; loss: 1.36; acc: 0.7
Batch: 20; loss: 1.47; acc: 0.62
Batch: 40; loss: 1.25; acc: 0.7
Batch: 60; loss: 1.34; acc: 0.69
Batch: 80; loss: 1.23; acc: 0.72
Batch: 100; loss: 1.39; acc: 0.56
Batch: 120; loss: 1.51; acc: 0.59
Batch: 140; loss: 1.28; acc: 0.64
Val Epoch over. val_loss: 1.3687221669847038; val_accuracy: 0.6475915605095541 

The current subspace-distance is: 0.00010463741637067869 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.66
Batch: 20; loss: 1.33; acc: 0.64
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.38; acc: 0.61
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.29; acc: 0.67
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 1.35; acc: 0.67
Batch: 160; loss: 1.23; acc: 0.72
Batch: 180; loss: 1.43; acc: 0.55
Batch: 200; loss: 1.37; acc: 0.61
Batch: 220; loss: 1.31; acc: 0.64
Batch: 240; loss: 1.44; acc: 0.58
Batch: 260; loss: 1.4; acc: 0.62
Batch: 280; loss: 1.22; acc: 0.69
Batch: 300; loss: 1.28; acc: 0.69
Batch: 320; loss: 1.43; acc: 0.62
Batch: 340; loss: 1.49; acc: 0.59
Batch: 360; loss: 1.41; acc: 0.61
Batch: 380; loss: 1.38; acc: 0.61
Batch: 400; loss: 1.46; acc: 0.52
Batch: 420; loss: 1.46; acc: 0.56
Batch: 440; loss: 1.42; acc: 0.62
Batch: 460; loss: 1.38; acc: 0.62
Batch: 480; loss: 1.38; acc: 0.64
Batch: 500; loss: 1.3; acc: 0.7
Batch: 520; loss: 1.3; acc: 0.7
Batch: 540; loss: 1.36; acc: 0.58
Batch: 560; loss: 1.42; acc: 0.67
Batch: 580; loss: 1.24; acc: 0.72
Batch: 600; loss: 1.36; acc: 0.66
Batch: 620; loss: 1.39; acc: 0.64
Batch: 640; loss: 1.37; acc: 0.56
Batch: 660; loss: 1.49; acc: 0.58
Batch: 680; loss: 1.29; acc: 0.61
Batch: 700; loss: 1.38; acc: 0.61
Batch: 720; loss: 1.36; acc: 0.67
Batch: 740; loss: 1.35; acc: 0.69
Batch: 760; loss: 1.38; acc: 0.59
Batch: 780; loss: 1.41; acc: 0.58
Train Epoch over. train_loss: 1.39; train_accuracy: 0.62 

0.0001191552946693264
0.00011524763976922259
Batch: 0; loss: 1.32; acc: 0.69
Batch: 20; loss: 1.43; acc: 0.64
Batch: 40; loss: 1.21; acc: 0.77
Batch: 60; loss: 1.3; acc: 0.66
Batch: 80; loss: 1.19; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.55
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.22; acc: 0.67
Val Epoch over. val_loss: 1.3337594665539492; val_accuracy: 0.6534633757961783 

The current subspace-distance is: 0.00011524763976922259 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.62
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.39; acc: 0.64
Batch: 60; loss: 1.31; acc: 0.64
Batch: 80; loss: 1.35; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.69
Batch: 120; loss: 1.25; acc: 0.7
Batch: 140; loss: 1.29; acc: 0.69
Batch: 160; loss: 1.18; acc: 0.75
Batch: 180; loss: 1.31; acc: 0.69
Batch: 200; loss: 1.4; acc: 0.56
Batch: 220; loss: 1.37; acc: 0.69
Batch: 240; loss: 1.3; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.55
Batch: 280; loss: 1.34; acc: 0.66
Batch: 300; loss: 1.39; acc: 0.62
Batch: 320; loss: 1.46; acc: 0.5
Batch: 340; loss: 1.4; acc: 0.62
Batch: 360; loss: 1.21; acc: 0.75
Batch: 380; loss: 1.32; acc: 0.64
Batch: 400; loss: 1.5; acc: 0.52
Batch: 420; loss: 1.38; acc: 0.61
Batch: 440; loss: 1.27; acc: 0.69
Batch: 460; loss: 1.36; acc: 0.62
Batch: 480; loss: 1.49; acc: 0.58
Batch: 500; loss: 1.2; acc: 0.73
Batch: 520; loss: 1.37; acc: 0.62
Batch: 540; loss: 1.34; acc: 0.64
Batch: 560; loss: 1.27; acc: 0.69
Batch: 580; loss: 1.22; acc: 0.67
Batch: 600; loss: 1.28; acc: 0.64
Batch: 620; loss: 1.3; acc: 0.67
Batch: 640; loss: 1.34; acc: 0.58
Batch: 660; loss: 1.29; acc: 0.66
Batch: 680; loss: 1.3; acc: 0.67
Batch: 700; loss: 1.41; acc: 0.58
Batch: 720; loss: 1.1; acc: 0.75
Batch: 740; loss: 1.24; acc: 0.64
Batch: 760; loss: 1.32; acc: 0.67
Batch: 780; loss: 1.25; acc: 0.69
Train Epoch over. train_loss: 1.36; train_accuracy: 0.63 

0.0001311442320002243
0.00012491481902543455
Batch: 0; loss: 1.28; acc: 0.69
Batch: 20; loss: 1.37; acc: 0.64
Batch: 40; loss: 1.14; acc: 0.8
Batch: 60; loss: 1.25; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.34; acc: 0.55
Batch: 120; loss: 1.46; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.72
Val Epoch over. val_loss: 1.2912923682267499; val_accuracy: 0.6624203821656051 

The current subspace-distance is: 0.00012491481902543455 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.59
Batch: 20; loss: 1.46; acc: 0.56
Batch: 40; loss: 1.32; acc: 0.61
Batch: 60; loss: 1.46; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.62
Batch: 100; loss: 1.33; acc: 0.66
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 1.41; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.64
Batch: 180; loss: 1.32; acc: 0.61
Batch: 200; loss: 1.22; acc: 0.72
Batch: 220; loss: 1.19; acc: 0.7
Batch: 240; loss: 1.26; acc: 0.72
Batch: 260; loss: 1.44; acc: 0.61
Batch: 280; loss: 1.39; acc: 0.59
Batch: 300; loss: 1.16; acc: 0.72
Batch: 320; loss: 1.35; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.59
Batch: 360; loss: 1.33; acc: 0.72
Batch: 380; loss: 1.31; acc: 0.61
Batch: 400; loss: 1.21; acc: 0.69
Batch: 420; loss: 1.29; acc: 0.67
Batch: 440; loss: 1.32; acc: 0.64
Batch: 460; loss: 1.34; acc: 0.61
Batch: 480; loss: 1.21; acc: 0.7
Batch: 500; loss: 1.38; acc: 0.59
Batch: 520; loss: 1.31; acc: 0.67
Batch: 540; loss: 1.27; acc: 0.62
Batch: 560; loss: 1.35; acc: 0.62
Batch: 580; loss: 1.28; acc: 0.61
Batch: 600; loss: 1.28; acc: 0.67
Batch: 620; loss: 1.36; acc: 0.64
Batch: 640; loss: 1.37; acc: 0.55
Batch: 660; loss: 1.38; acc: 0.64
Batch: 680; loss: 1.26; acc: 0.66
Batch: 700; loss: 1.24; acc: 0.67
Batch: 720; loss: 1.2; acc: 0.66
Batch: 740; loss: 1.24; acc: 0.69
Batch: 760; loss: 1.42; acc: 0.64
Batch: 780; loss: 1.51; acc: 0.5
Train Epoch over. train_loss: 1.31; train_accuracy: 0.64 

0.00013746321201324463
0.0001335234264843166
Batch: 0; loss: 1.25; acc: 0.75
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.58
Batch: 120; loss: 1.42; acc: 0.62
Batch: 140; loss: 1.09; acc: 0.73
Val Epoch over. val_loss: 1.2520627891941436; val_accuracy: 0.6713773885350318 

The current subspace-distance is: 0.0001335234264843166 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.66
Batch: 20; loss: 1.39; acc: 0.61
Batch: 40; loss: 1.31; acc: 0.67
Batch: 60; loss: 1.22; acc: 0.73
Batch: 80; loss: 1.27; acc: 0.72
Batch: 100; loss: 1.2; acc: 0.61
Batch: 120; loss: 1.29; acc: 0.61
Batch: 140; loss: 1.23; acc: 0.62
Batch: 160; loss: 1.32; acc: 0.59
Batch: 180; loss: 1.2; acc: 0.7
Batch: 200; loss: 1.3; acc: 0.61
Batch: 220; loss: 1.42; acc: 0.58
Batch: 240; loss: 1.33; acc: 0.62
Batch: 260; loss: 1.18; acc: 0.73
Batch: 280; loss: 1.34; acc: 0.61
Batch: 300; loss: 1.28; acc: 0.62
Batch: 320; loss: 1.27; acc: 0.59
Batch: 340; loss: 1.16; acc: 0.78
Batch: 360; loss: 1.25; acc: 0.69
Batch: 380; loss: 1.3; acc: 0.55
Batch: 400; loss: 1.17; acc: 0.73
Batch: 420; loss: 1.22; acc: 0.67
Batch: 440; loss: 1.27; acc: 0.59
Batch: 460; loss: 1.41; acc: 0.56
Batch: 480; loss: 1.29; acc: 0.58
Batch: 500; loss: 1.15; acc: 0.72
Batch: 520; loss: 1.32; acc: 0.58
Batch: 540; loss: 1.33; acc: 0.59
Batch: 560; loss: 1.2; acc: 0.69
Batch: 580; loss: 1.36; acc: 0.56
Batch: 600; loss: 1.23; acc: 0.69
Batch: 620; loss: 1.38; acc: 0.59
Batch: 640; loss: 1.22; acc: 0.62
Batch: 660; loss: 1.26; acc: 0.64
Batch: 680; loss: 1.23; acc: 0.67
Batch: 700; loss: 1.16; acc: 0.72
Batch: 720; loss: 1.27; acc: 0.66
Batch: 740; loss: 1.29; acc: 0.64
Batch: 760; loss: 1.26; acc: 0.62
Batch: 780; loss: 1.46; acc: 0.56
Train Epoch over. train_loss: 1.27; train_accuracy: 0.65 

0.0001522637321613729
0.00014577711408492178
Batch: 0; loss: 1.2; acc: 0.75
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 0.99; acc: 0.8
Batch: 60; loss: 1.13; acc: 0.75
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.23; acc: 0.62
Batch: 120; loss: 1.34; acc: 0.64
Batch: 140; loss: 1.02; acc: 0.78
Val Epoch over. val_loss: 1.195812380237944; val_accuracy: 0.6876990445859873 

The current subspace-distance is: 0.00014577711408492178 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.33; acc: 0.66
Batch: 40; loss: 1.3; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.37; acc: 0.61
Batch: 100; loss: 1.28; acc: 0.64
Batch: 120; loss: 1.35; acc: 0.59
Batch: 140; loss: 1.28; acc: 0.61
Batch: 160; loss: 1.25; acc: 0.67
Batch: 180; loss: 1.29; acc: 0.7
Batch: 200; loss: 1.29; acc: 0.58
Batch: 220; loss: 1.33; acc: 0.56
Batch: 240; loss: 1.2; acc: 0.69
Batch: 260; loss: 1.22; acc: 0.7
Batch: 280; loss: 1.13; acc: 0.77
Batch: 300; loss: 1.19; acc: 0.7
Batch: 320; loss: 1.17; acc: 0.7
Batch: 340; loss: 1.31; acc: 0.62
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.32; acc: 0.56
Batch: 400; loss: 1.22; acc: 0.67
Batch: 420; loss: 1.25; acc: 0.62
Batch: 440; loss: 1.08; acc: 0.77
Batch: 460; loss: 1.17; acc: 0.72
Batch: 480; loss: 1.19; acc: 0.69
Batch: 500; loss: 1.19; acc: 0.62
Batch: 520; loss: 1.4; acc: 0.56
Batch: 540; loss: 1.21; acc: 0.69
Batch: 560; loss: 1.27; acc: 0.62
Batch: 580; loss: 1.19; acc: 0.7
Batch: 600; loss: 1.25; acc: 0.69
Batch: 620; loss: 1.24; acc: 0.7
Batch: 640; loss: 1.25; acc: 0.69
Batch: 660; loss: 1.09; acc: 0.72
Batch: 680; loss: 1.27; acc: 0.59
Batch: 700; loss: 1.14; acc: 0.7
Batch: 720; loss: 1.01; acc: 0.78
Batch: 740; loss: 1.16; acc: 0.66
Batch: 760; loss: 1.07; acc: 0.75
Batch: 780; loss: 1.17; acc: 0.72
Train Epoch over. train_loss: 1.25; train_accuracy: 0.66 

0.000155108209582977
0.00014882785035297275
Batch: 0; loss: 1.22; acc: 0.72
Batch: 20; loss: 1.34; acc: 0.62
Batch: 40; loss: 1.01; acc: 0.8
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 1.25; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.66
Batch: 140; loss: 1.04; acc: 0.73
Val Epoch over. val_loss: 1.209028139235867; val_accuracy: 0.6835191082802548 

The current subspace-distance is: 0.00014882785035297275 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.34; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.77
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.61
Batch: 80; loss: 1.3; acc: 0.64
Batch: 100; loss: 1.3; acc: 0.56
Batch: 120; loss: 1.32; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.69
Batch: 160; loss: 1.35; acc: 0.62
Batch: 180; loss: 1.39; acc: 0.58
Batch: 200; loss: 1.12; acc: 0.73
Batch: 220; loss: 1.32; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.69
Batch: 260; loss: 1.14; acc: 0.72
Batch: 280; loss: 1.23; acc: 0.69
Batch: 300; loss: 1.24; acc: 0.67
Batch: 320; loss: 1.14; acc: 0.67
Batch: 340; loss: 1.37; acc: 0.53
Batch: 360; loss: 1.15; acc: 0.69
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.15; acc: 0.72
Batch: 420; loss: 1.16; acc: 0.66
Batch: 440; loss: 1.42; acc: 0.61
Batch: 460; loss: 1.31; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.66
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 1.23; acc: 0.64
Batch: 540; loss: 1.14; acc: 0.75
Batch: 560; loss: 1.29; acc: 0.59
Batch: 580; loss: 1.34; acc: 0.59
Batch: 600; loss: 1.14; acc: 0.67
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.2; acc: 0.66
Batch: 660; loss: 1.24; acc: 0.64
Batch: 680; loss: 1.25; acc: 0.7
Batch: 700; loss: 1.3; acc: 0.64
Batch: 720; loss: 1.21; acc: 0.64
Batch: 740; loss: 1.11; acc: 0.69
Batch: 760; loss: 1.33; acc: 0.61
Batch: 780; loss: 1.31; acc: 0.67
Train Epoch over. train_loss: 1.24; train_accuracy: 0.66 

0.00015905335021670908
0.00015211805293802172
Batch: 0; loss: 1.19; acc: 0.7
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 0.98; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.81
Batch: 100; loss: 1.21; acc: 0.62
Batch: 120; loss: 1.32; acc: 0.67
Batch: 140; loss: 1.01; acc: 0.8
Val Epoch over. val_loss: 1.1803865766828987; val_accuracy: 0.6897890127388535 

The current subspace-distance is: 0.00015211805293802172 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.38; acc: 0.59
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 1.18; acc: 0.59
Batch: 60; loss: 1.27; acc: 0.69
Batch: 80; loss: 1.15; acc: 0.72
Batch: 100; loss: 1.18; acc: 0.66
Batch: 120; loss: 1.18; acc: 0.7
Batch: 140; loss: 1.16; acc: 0.66
Batch: 160; loss: 1.28; acc: 0.66
Batch: 180; loss: 1.17; acc: 0.66
Batch: 200; loss: 1.5; acc: 0.5
Batch: 220; loss: 1.12; acc: 0.69
Batch: 240; loss: 1.34; acc: 0.58
Batch: 260; loss: 1.29; acc: 0.73
Batch: 280; loss: 1.3; acc: 0.58
Batch: 300; loss: 1.37; acc: 0.56
Batch: 320; loss: 1.27; acc: 0.62
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.62
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.19; acc: 0.67
Batch: 420; loss: 1.21; acc: 0.67
Batch: 440; loss: 1.29; acc: 0.64
Batch: 460; loss: 1.22; acc: 0.66
Batch: 480; loss: 1.1; acc: 0.7
Batch: 500; loss: 1.22; acc: 0.66
Batch: 520; loss: 1.22; acc: 0.7
Batch: 540; loss: 1.13; acc: 0.7
Batch: 560; loss: 1.15; acc: 0.75
Batch: 580; loss: 1.2; acc: 0.62
Batch: 600; loss: 1.25; acc: 0.55
Batch: 620; loss: 1.37; acc: 0.62
Batch: 640; loss: 1.27; acc: 0.62
Batch: 660; loss: 1.23; acc: 0.7
Batch: 680; loss: 1.22; acc: 0.64
Batch: 700; loss: 1.43; acc: 0.53
Batch: 720; loss: 1.16; acc: 0.66
Batch: 740; loss: 1.23; acc: 0.69
Batch: 760; loss: 1.37; acc: 0.61
Batch: 780; loss: 1.22; acc: 0.64
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00016362419410143048
0.000155424524564296
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.98; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.78
Batch: 80; loss: 0.98; acc: 0.83
Batch: 100; loss: 1.22; acc: 0.61
Batch: 120; loss: 1.32; acc: 0.67
Batch: 140; loss: 1.01; acc: 0.77
Val Epoch over. val_loss: 1.1773473681158322; val_accuracy: 0.6937699044585988 

The current subspace-distance is: 0.000155424524564296 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.53
Batch: 20; loss: 1.17; acc: 0.67
Batch: 40; loss: 1.16; acc: 0.64
Batch: 60; loss: 0.93; acc: 0.73
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.16; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.28; acc: 0.64
Batch: 160; loss: 1.19; acc: 0.7
Batch: 180; loss: 1.36; acc: 0.58
Batch: 200; loss: 1.1; acc: 0.7
Batch: 220; loss: 1.22; acc: 0.75
Batch: 240; loss: 1.17; acc: 0.72
Batch: 260; loss: 1.22; acc: 0.56
Batch: 280; loss: 1.27; acc: 0.61
Batch: 300; loss: 1.35; acc: 0.53
Batch: 320; loss: 1.34; acc: 0.66
Batch: 340; loss: 1.21; acc: 0.69
Batch: 360; loss: 1.04; acc: 0.75
Batch: 380; loss: 1.34; acc: 0.66
Batch: 400; loss: 1.26; acc: 0.66
Batch: 420; loss: 1.16; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.75
Batch: 460; loss: 1.16; acc: 0.67
Batch: 480; loss: 1.13; acc: 0.7
Batch: 500; loss: 1.26; acc: 0.61
Batch: 520; loss: 1.11; acc: 0.69
Batch: 540; loss: 1.19; acc: 0.72
Batch: 560; loss: 1.32; acc: 0.56
Batch: 580; loss: 1.09; acc: 0.75
Batch: 600; loss: 1.21; acc: 0.7
Batch: 620; loss: 1.18; acc: 0.67
Batch: 640; loss: 1.09; acc: 0.72
Batch: 660; loss: 1.24; acc: 0.66
Batch: 680; loss: 1.12; acc: 0.77
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.45; acc: 0.5
Batch: 740; loss: 1.14; acc: 0.75
Batch: 760; loss: 1.15; acc: 0.77
Batch: 780; loss: 1.15; acc: 0.69
Train Epoch over. train_loss: 1.22; train_accuracy: 0.66 

0.00016304018208757043
0.00015676909242756665
Batch: 0; loss: 1.19; acc: 0.75
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.96; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.78
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.31; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.75
Val Epoch over. val_loss: 1.1613676551800625; val_accuracy: 0.6995421974522293 

The current subspace-distance is: 0.00015676909242756665 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.28; acc: 0.66
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 1.14; acc: 0.72
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 1.23; acc: 0.69
Batch: 100; loss: 1.24; acc: 0.61
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.15; acc: 0.67
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.1; acc: 0.75
Batch: 220; loss: 1.2; acc: 0.7
Batch: 240; loss: 1.27; acc: 0.61
Batch: 260; loss: 1.19; acc: 0.69
Batch: 280; loss: 1.01; acc: 0.8
Batch: 300; loss: 1.18; acc: 0.69
Batch: 320; loss: 1.28; acc: 0.66
Batch: 340; loss: 1.27; acc: 0.62
Batch: 360; loss: 1.29; acc: 0.58
Batch: 380; loss: 1.26; acc: 0.64
Batch: 400; loss: 1.19; acc: 0.7
Batch: 420; loss: 1.18; acc: 0.72
Batch: 440; loss: 1.11; acc: 0.72
Batch: 460; loss: 1.2; acc: 0.72
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 1.41; acc: 0.59
Batch: 520; loss: 1.37; acc: 0.58
Batch: 540; loss: 1.3; acc: 0.67
Batch: 560; loss: 1.04; acc: 0.77
Batch: 580; loss: 1.23; acc: 0.69
Batch: 600; loss: 1.43; acc: 0.55
Batch: 620; loss: 1.22; acc: 0.59
Batch: 640; loss: 1.19; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.75
Batch: 680; loss: 1.12; acc: 0.73
Batch: 700; loss: 1.16; acc: 0.64
Batch: 720; loss: 1.34; acc: 0.53
Batch: 740; loss: 1.33; acc: 0.69
Batch: 760; loss: 1.32; acc: 0.62
Batch: 780; loss: 1.19; acc: 0.7
Train Epoch over. train_loss: 1.21; train_accuracy: 0.66 

0.00016722870350349694
0.00016005212091840804
Batch: 0; loss: 1.2; acc: 0.75
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 0.97; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.8
Batch: 100; loss: 1.21; acc: 0.61
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 1.01; acc: 0.77
Val Epoch over. val_loss: 1.169297147708334; val_accuracy: 0.6908837579617835 

The current subspace-distance is: 0.00016005212091840804 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.59
Batch: 20; loss: 1.13; acc: 0.75
Batch: 40; loss: 1.11; acc: 0.62
Batch: 60; loss: 1.3; acc: 0.62
Batch: 80; loss: 1.28; acc: 0.66
Batch: 100; loss: 1.18; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.58
Batch: 140; loss: 1.16; acc: 0.69
Batch: 160; loss: 1.32; acc: 0.66
Batch: 180; loss: 1.24; acc: 0.66
Batch: 200; loss: 1.29; acc: 0.62
Batch: 220; loss: 1.29; acc: 0.59
Batch: 240; loss: 1.12; acc: 0.72
Batch: 260; loss: 1.25; acc: 0.61
Batch: 280; loss: 1.1; acc: 0.73
Batch: 300; loss: 1.24; acc: 0.66
Batch: 320; loss: 1.15; acc: 0.66
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 1.14; acc: 0.7
Batch: 380; loss: 0.98; acc: 0.8
Batch: 400; loss: 1.04; acc: 0.67
Batch: 420; loss: 1.02; acc: 0.73
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 1.18; acc: 0.69
Batch: 480; loss: 1.23; acc: 0.72
Batch: 500; loss: 1.22; acc: 0.66
Batch: 520; loss: 1.19; acc: 0.72
Batch: 540; loss: 1.44; acc: 0.52
Batch: 560; loss: 1.1; acc: 0.72
Batch: 580; loss: 1.22; acc: 0.7
Batch: 600; loss: 1.03; acc: 0.75
Batch: 620; loss: 1.29; acc: 0.64
Batch: 640; loss: 1.26; acc: 0.56
Batch: 660; loss: 1.13; acc: 0.73
Batch: 680; loss: 1.39; acc: 0.58
Batch: 700; loss: 1.2; acc: 0.61
Batch: 720; loss: 1.22; acc: 0.62
Batch: 740; loss: 1.29; acc: 0.59
Batch: 760; loss: 1.29; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.73
Train Epoch over. train_loss: 1.2; train_accuracy: 0.67 

0.00017216232663486153
0.00016107737610582262
Batch: 0; loss: 1.18; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.59
Batch: 40; loss: 0.94; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.83
Batch: 100; loss: 1.19; acc: 0.59
Batch: 120; loss: 1.29; acc: 0.67
Batch: 140; loss: 0.98; acc: 0.77
Val Epoch over. val_loss: 1.1370059582078533; val_accuracy: 0.6991441082802548 

The current subspace-distance is: 0.00016107737610582262 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.35; acc: 0.58
Batch: 20; loss: 1.04; acc: 0.72
Batch: 40; loss: 1.16; acc: 0.69
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.72
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 1.14; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.66
Batch: 200; loss: 1.2; acc: 0.7
Batch: 220; loss: 1.3; acc: 0.59
Batch: 240; loss: 1.14; acc: 0.66
Batch: 260; loss: 1.29; acc: 0.61
Batch: 280; loss: 1.28; acc: 0.62
Batch: 300; loss: 1.21; acc: 0.66
Batch: 320; loss: 1.01; acc: 0.77
Batch: 340; loss: 1.12; acc: 0.67
Batch: 360; loss: 1.21; acc: 0.61
Batch: 380; loss: 1.22; acc: 0.7
Batch: 400; loss: 1.19; acc: 0.64
Batch: 420; loss: 1.12; acc: 0.66
Batch: 440; loss: 1.19; acc: 0.69
Batch: 460; loss: 1.16; acc: 0.67
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.26; acc: 0.64
Batch: 520; loss: 1.18; acc: 0.59
Batch: 540; loss: 1.12; acc: 0.73
Batch: 560; loss: 1.14; acc: 0.59
Batch: 580; loss: 1.2; acc: 0.69
Batch: 600; loss: 1.13; acc: 0.72
Batch: 620; loss: 1.1; acc: 0.69
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.73
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 1.13; acc: 0.7
Batch: 720; loss: 1.19; acc: 0.7
Batch: 740; loss: 1.1; acc: 0.72
Batch: 760; loss: 1.29; acc: 0.61
Batch: 780; loss: 1.2; acc: 0.66
Train Epoch over. train_loss: 1.19; train_accuracy: 0.67 

0.00017669795488473028
0.00017168534395750612
Batch: 0; loss: 1.18; acc: 0.75
Batch: 20; loss: 1.3; acc: 0.62
Batch: 40; loss: 0.95; acc: 0.77
Batch: 60; loss: 1.07; acc: 0.73
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 1.18; acc: 0.66
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.01; acc: 0.73
Val Epoch over. val_loss: 1.1391584911164205; val_accuracy: 0.6979498407643312 

The current subspace-distance is: 0.00017168534395750612 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 1.27; acc: 0.62
Batch: 60; loss: 1.13; acc: 0.7
Batch: 80; loss: 1.3; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.19; acc: 0.69
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.75
Batch: 180; loss: 1.1; acc: 0.73
Batch: 200; loss: 1.24; acc: 0.62
Batch: 220; loss: 1.06; acc: 0.73
Batch: 240; loss: 1.34; acc: 0.61
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.17; acc: 0.67
Batch: 300; loss: 1.15; acc: 0.7
Batch: 320; loss: 1.25; acc: 0.62
Batch: 340; loss: 1.23; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.7
Batch: 380; loss: 1.12; acc: 0.67
Batch: 400; loss: 1.13; acc: 0.73
Batch: 420; loss: 1.34; acc: 0.64
Batch: 440; loss: 1.16; acc: 0.66
Batch: 460; loss: 1.29; acc: 0.59
Batch: 480; loss: 1.19; acc: 0.69
Batch: 500; loss: 1.06; acc: 0.7
Batch: 520; loss: 1.11; acc: 0.73
Batch: 540; loss: 1.04; acc: 0.69
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.07; acc: 0.78
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.08; acc: 0.73
Batch: 640; loss: 1.03; acc: 0.77
Batch: 660; loss: 1.14; acc: 0.69
Batch: 680; loss: 1.26; acc: 0.69
Batch: 700; loss: 1.15; acc: 0.7
Batch: 720; loss: 1.23; acc: 0.66
Batch: 740; loss: 1.17; acc: 0.61
Batch: 760; loss: 1.29; acc: 0.61
Batch: 780; loss: 1.22; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

0.00017472720355726779
0.00016830455570016056
Batch: 0; loss: 1.17; acc: 0.73
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 1.05; acc: 0.7
Batch: 80; loss: 0.89; acc: 0.83
Batch: 100; loss: 1.17; acc: 0.64
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 0.99; acc: 0.75
Val Epoch over. val_loss: 1.1195852430003463; val_accuracy: 0.7021297770700637 

The current subspace-distance is: 0.00016830455570016056 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.14; acc: 0.7
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.67
Batch: 80; loss: 1.19; acc: 0.69
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 1.16; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 1.19; acc: 0.67
Batch: 200; loss: 1.3; acc: 0.61
Batch: 220; loss: 1.12; acc: 0.67
Batch: 240; loss: 1.22; acc: 0.67
Batch: 260; loss: 1.0; acc: 0.75
Batch: 280; loss: 1.13; acc: 0.69
Batch: 300; loss: 1.22; acc: 0.66
Batch: 320; loss: 1.23; acc: 0.62
Batch: 340; loss: 1.35; acc: 0.55
Batch: 360; loss: 1.0; acc: 0.78
Batch: 380; loss: 1.17; acc: 0.67
Batch: 400; loss: 1.14; acc: 0.66
Batch: 420; loss: 1.1; acc: 0.77
Batch: 440; loss: 1.27; acc: 0.66
Batch: 460; loss: 1.1; acc: 0.75
Batch: 480; loss: 1.28; acc: 0.56
Batch: 500; loss: 1.25; acc: 0.61
Batch: 520; loss: 1.3; acc: 0.67
Batch: 540; loss: 1.22; acc: 0.7
Batch: 560; loss: 1.15; acc: 0.69
Batch: 580; loss: 1.18; acc: 0.66
Batch: 600; loss: 1.25; acc: 0.66
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.09; acc: 0.73
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 1.04; acc: 0.77
Batch: 700; loss: 1.19; acc: 0.67
Batch: 720; loss: 0.98; acc: 0.84
Batch: 740; loss: 1.17; acc: 0.69
Batch: 760; loss: 1.22; acc: 0.66
Batch: 780; loss: 1.17; acc: 0.73
Train Epoch over. train_loss: 1.17; train_accuracy: 0.67 

0.00017691115499474108
0.00016970701108220965
Batch: 0; loss: 1.16; acc: 0.73
Batch: 20; loss: 1.27; acc: 0.61
Batch: 40; loss: 0.91; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 0.87; acc: 0.8
Batch: 100; loss: 1.14; acc: 0.64
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.1040167216282741; val_accuracy: 0.7071058917197452 

The current subspace-distance is: 0.00016970701108220965 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.29; acc: 0.64
Batch: 40; loss: 1.16; acc: 0.66
Batch: 60; loss: 1.31; acc: 0.62
Batch: 80; loss: 1.28; acc: 0.61
Batch: 100; loss: 0.93; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 1.15; acc: 0.75
Batch: 160; loss: 1.08; acc: 0.75
Batch: 180; loss: 1.18; acc: 0.61
Batch: 200; loss: 1.24; acc: 0.61
Batch: 220; loss: 1.04; acc: 0.72
Batch: 240; loss: 1.13; acc: 0.69
Batch: 260; loss: 1.08; acc: 0.8
Batch: 280; loss: 1.13; acc: 0.7
Batch: 300; loss: 1.13; acc: 0.73
Batch: 320; loss: 1.23; acc: 0.61
Batch: 340; loss: 1.09; acc: 0.7
Batch: 360; loss: 1.14; acc: 0.62
Batch: 380; loss: 1.11; acc: 0.67
Batch: 400; loss: 1.18; acc: 0.64
Batch: 420; loss: 1.06; acc: 0.75
Batch: 440; loss: 1.36; acc: 0.61
Batch: 460; loss: 1.25; acc: 0.69
Batch: 480; loss: 1.3; acc: 0.66
Batch: 500; loss: 1.23; acc: 0.61
Batch: 520; loss: 1.18; acc: 0.67
Batch: 540; loss: 1.12; acc: 0.75
Batch: 560; loss: 1.05; acc: 0.75
Batch: 580; loss: 1.13; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.7
Batch: 620; loss: 1.34; acc: 0.56
Batch: 640; loss: 1.21; acc: 0.61
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.24; acc: 0.61
Batch: 700; loss: 1.26; acc: 0.64
Batch: 720; loss: 1.25; acc: 0.59
Batch: 740; loss: 1.16; acc: 0.69
Batch: 760; loss: 1.17; acc: 0.62
Batch: 780; loss: 0.97; acc: 0.73
Train Epoch over. train_loss: 1.16; train_accuracy: 0.68 

0.0001790243695722893
0.0001745278132148087
Batch: 0; loss: 1.17; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.21; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.095040739341906; val_accuracy: 0.7093949044585988 

The current subspace-distance is: 0.0001745278132148087 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.77
Batch: 40; loss: 1.2; acc: 0.66
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.24; acc: 0.59
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 1.07; acc: 0.66
Batch: 140; loss: 1.18; acc: 0.58
Batch: 160; loss: 1.02; acc: 0.78
Batch: 180; loss: 1.14; acc: 0.67
Batch: 200; loss: 1.23; acc: 0.61
Batch: 220; loss: 1.26; acc: 0.61
Batch: 240; loss: 1.1; acc: 0.72
Batch: 260; loss: 1.16; acc: 0.67
Batch: 280; loss: 1.08; acc: 0.75
Batch: 300; loss: 1.08; acc: 0.66
Batch: 320; loss: 1.13; acc: 0.67
Batch: 340; loss: 0.92; acc: 0.78
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.28; acc: 0.67
Batch: 400; loss: 1.1; acc: 0.72
Batch: 420; loss: 1.12; acc: 0.67
Batch: 440; loss: 0.98; acc: 0.77
Batch: 460; loss: 1.31; acc: 0.55
Batch: 480; loss: 1.02; acc: 0.78
Batch: 500; loss: 1.21; acc: 0.66
Batch: 520; loss: 1.05; acc: 0.73
Batch: 540; loss: 1.25; acc: 0.72
Batch: 560; loss: 0.98; acc: 0.75
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.11; acc: 0.73
Batch: 620; loss: 1.16; acc: 0.72
Batch: 640; loss: 1.09; acc: 0.77
Batch: 660; loss: 1.15; acc: 0.64
Batch: 680; loss: 1.24; acc: 0.69
Batch: 700; loss: 1.11; acc: 0.7
Batch: 720; loss: 1.17; acc: 0.69
Batch: 740; loss: 1.21; acc: 0.7
Batch: 760; loss: 1.11; acc: 0.72
Batch: 780; loss: 1.09; acc: 0.77
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

0.000185435259481892
0.00017732295964378864
Batch: 0; loss: 1.15; acc: 0.75
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.91; acc: 0.78
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 1.16; acc: 0.66
Batch: 120; loss: 1.22; acc: 0.67
Batch: 140; loss: 0.98; acc: 0.72
Val Epoch over. val_loss: 1.0937897657892506; val_accuracy: 0.7009355095541401 

The current subspace-distance is: 0.00017732295964378864 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.69
Batch: 20; loss: 1.27; acc: 0.67
Batch: 40; loss: 1.1; acc: 0.69
Batch: 60; loss: 1.06; acc: 0.66
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 1.19; acc: 0.64
Batch: 120; loss: 1.22; acc: 0.58
Batch: 140; loss: 1.07; acc: 0.77
Batch: 160; loss: 1.06; acc: 0.72
Batch: 180; loss: 1.11; acc: 0.7
Batch: 200; loss: 1.17; acc: 0.64
Batch: 220; loss: 1.21; acc: 0.67
Batch: 240; loss: 1.27; acc: 0.55
Batch: 260; loss: 1.09; acc: 0.67
Batch: 280; loss: 1.2; acc: 0.69
Batch: 300; loss: 1.1; acc: 0.64
Batch: 320; loss: 1.13; acc: 0.67
Batch: 340; loss: 1.07; acc: 0.72
Batch: 360; loss: 1.41; acc: 0.55
Batch: 380; loss: 1.26; acc: 0.67
Batch: 400; loss: 1.21; acc: 0.67
Batch: 420; loss: 1.37; acc: 0.59
Batch: 440; loss: 1.19; acc: 0.64
Batch: 460; loss: 1.17; acc: 0.66
Batch: 480; loss: 1.16; acc: 0.72
Batch: 500; loss: 1.03; acc: 0.75
Batch: 520; loss: 0.99; acc: 0.77
Batch: 540; loss: 1.05; acc: 0.77
Batch: 560; loss: 1.19; acc: 0.66
Batch: 580; loss: 1.22; acc: 0.69
Batch: 600; loss: 1.15; acc: 0.7
Batch: 620; loss: 1.1; acc: 0.73
Batch: 640; loss: 1.19; acc: 0.59
Batch: 660; loss: 1.27; acc: 0.66
Batch: 680; loss: 1.09; acc: 0.77
Batch: 700; loss: 1.16; acc: 0.64
Batch: 720; loss: 1.23; acc: 0.64
Batch: 740; loss: 1.13; acc: 0.64
Batch: 760; loss: 1.16; acc: 0.69
Batch: 780; loss: 1.11; acc: 0.72
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

0.00018252033623866737
0.00017549314361531287
Batch: 0; loss: 1.18; acc: 0.73
Batch: 20; loss: 1.27; acc: 0.58
Batch: 40; loss: 0.92; acc: 0.77
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.87; acc: 0.8
Batch: 100; loss: 1.17; acc: 0.64
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 1.0; acc: 0.7
Val Epoch over. val_loss: 1.1071996525594383; val_accuracy: 0.7005374203821656 

The current subspace-distance is: 0.00017549314361531287 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.12; acc: 0.69
Batch: 40; loss: 1.1; acc: 0.64
Batch: 60; loss: 1.35; acc: 0.66
Batch: 80; loss: 1.31; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.09; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.62
Batch: 160; loss: 1.19; acc: 0.7
Batch: 180; loss: 0.96; acc: 0.77
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 1.16; acc: 0.7
Batch: 240; loss: 1.23; acc: 0.61
Batch: 260; loss: 1.18; acc: 0.67
Batch: 280; loss: 1.26; acc: 0.67
Batch: 300; loss: 1.14; acc: 0.67
Batch: 320; loss: 1.14; acc: 0.7
Batch: 340; loss: 0.97; acc: 0.78
Batch: 360; loss: 1.17; acc: 0.61
Batch: 380; loss: 1.11; acc: 0.69
Batch: 400; loss: 1.08; acc: 0.73
Batch: 420; loss: 1.11; acc: 0.72
Batch: 440; loss: 1.16; acc: 0.64
Batch: 460; loss: 1.24; acc: 0.64
Batch: 480; loss: 1.26; acc: 0.67
Batch: 500; loss: 1.03; acc: 0.7
Batch: 520; loss: 1.15; acc: 0.67
Batch: 540; loss: 1.15; acc: 0.67
Batch: 560; loss: 1.03; acc: 0.78
Batch: 580; loss: 1.17; acc: 0.64
Batch: 600; loss: 1.11; acc: 0.73
Batch: 620; loss: 1.2; acc: 0.66
Batch: 640; loss: 0.96; acc: 0.75
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.22; acc: 0.61
Batch: 700; loss: 1.13; acc: 0.73
Batch: 720; loss: 1.41; acc: 0.55
Batch: 740; loss: 1.17; acc: 0.67
Batch: 760; loss: 1.09; acc: 0.8
Batch: 780; loss: 1.09; acc: 0.7
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00019044212240260094
0.00018172657291870564
Batch: 0; loss: 1.15; acc: 0.69
Batch: 20; loss: 1.26; acc: 0.56
Batch: 40; loss: 0.9; acc: 0.77
Batch: 60; loss: 1.04; acc: 0.72
Batch: 80; loss: 0.84; acc: 0.81
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 0.97; acc: 0.72
Val Epoch over. val_loss: 1.083615828851226; val_accuracy: 0.7110867834394905 

The current subspace-distance is: 0.00018172657291870564 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 1.3; acc: 0.52
Batch: 60; loss: 1.39; acc: 0.53
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.08; acc: 0.7
Batch: 140; loss: 1.07; acc: 0.73
Batch: 160; loss: 1.09; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.78
Batch: 200; loss: 1.21; acc: 0.7
Batch: 220; loss: 1.2; acc: 0.67
Batch: 240; loss: 1.07; acc: 0.72
Batch: 260; loss: 1.35; acc: 0.59
Batch: 280; loss: 1.34; acc: 0.66
Batch: 300; loss: 1.33; acc: 0.52
Batch: 320; loss: 1.1; acc: 0.69
Batch: 340; loss: 0.92; acc: 0.77
Batch: 360; loss: 1.07; acc: 0.73
Batch: 380; loss: 1.22; acc: 0.61
Batch: 400; loss: 1.19; acc: 0.59
Batch: 420; loss: 1.32; acc: 0.67
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.07; acc: 0.72
Batch: 480; loss: 1.22; acc: 0.66
Batch: 500; loss: 1.01; acc: 0.73
Batch: 520; loss: 1.04; acc: 0.8
Batch: 540; loss: 1.05; acc: 0.77
Batch: 560; loss: 1.15; acc: 0.69
Batch: 580; loss: 1.0; acc: 0.77
Batch: 600; loss: 0.98; acc: 0.72
Batch: 620; loss: 1.04; acc: 0.72
Batch: 640; loss: 1.18; acc: 0.67
Batch: 660; loss: 1.1; acc: 0.7
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 0.99; acc: 0.75
Batch: 720; loss: 1.14; acc: 0.64
Batch: 740; loss: 1.22; acc: 0.64
Batch: 760; loss: 1.05; acc: 0.77
Batch: 780; loss: 1.18; acc: 0.64
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00018525046471040696
0.0001778037694748491
Batch: 0; loss: 1.18; acc: 0.72
Batch: 20; loss: 1.26; acc: 0.61
Batch: 40; loss: 0.92; acc: 0.8
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 0.85; acc: 0.83
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 1.24; acc: 0.69
Batch: 140; loss: 0.99; acc: 0.72
Val Epoch over. val_loss: 1.0990576451751077; val_accuracy: 0.7082006369426752 

The current subspace-distance is: 0.0001778037694748491 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.32; acc: 0.61
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 1.12; acc: 0.66
Batch: 60; loss: 1.08; acc: 0.77
Batch: 80; loss: 1.26; acc: 0.58
Batch: 100; loss: 0.94; acc: 0.81
Batch: 120; loss: 1.19; acc: 0.69
Batch: 140; loss: 1.1; acc: 0.7
Batch: 160; loss: 1.13; acc: 0.62
Batch: 180; loss: 1.07; acc: 0.7
Batch: 200; loss: 1.18; acc: 0.64
Batch: 220; loss: 1.04; acc: 0.7
Batch: 240; loss: 1.38; acc: 0.53
Batch: 260; loss: 0.99; acc: 0.78
Batch: 280; loss: 1.08; acc: 0.66
Batch: 300; loss: 1.17; acc: 0.69
Batch: 320; loss: 1.33; acc: 0.56
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 1.2; acc: 0.64
Batch: 380; loss: 1.29; acc: 0.59
Batch: 400; loss: 1.02; acc: 0.77
Batch: 420; loss: 1.15; acc: 0.7
Batch: 440; loss: 1.03; acc: 0.67
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.11; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.88
Batch: 520; loss: 1.06; acc: 0.72
Batch: 540; loss: 1.15; acc: 0.73
Batch: 560; loss: 1.01; acc: 0.75
Batch: 580; loss: 1.41; acc: 0.55
Batch: 600; loss: 1.06; acc: 0.7
Batch: 620; loss: 1.14; acc: 0.69
Batch: 640; loss: 1.06; acc: 0.73
Batch: 660; loss: 1.16; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.67
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.19; acc: 0.7
Batch: 740; loss: 0.97; acc: 0.77
Batch: 760; loss: 1.17; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.0001845682563725859
0.00017772171122487634
Batch: 0; loss: 1.15; acc: 0.72
Batch: 20; loss: 1.26; acc: 0.56
Batch: 40; loss: 0.9; acc: 0.77
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 0.84; acc: 0.81
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 0.97; acc: 0.72
Val Epoch over. val_loss: 1.083207607269287; val_accuracy: 0.7073049363057324 

The current subspace-distance is: 0.00017772171122487634 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.73
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 1.04; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.84
Batch: 80; loss: 1.3; acc: 0.52
Batch: 100; loss: 1.04; acc: 0.67
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.06; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.73
Batch: 180; loss: 1.1; acc: 0.73
Batch: 200; loss: 1.14; acc: 0.64
Batch: 220; loss: 1.12; acc: 0.7
Batch: 240; loss: 1.13; acc: 0.69
Batch: 260; loss: 1.02; acc: 0.7
Batch: 280; loss: 1.02; acc: 0.73
Batch: 300; loss: 1.08; acc: 0.72
Batch: 320; loss: 1.1; acc: 0.75
Batch: 340; loss: 1.1; acc: 0.72
Batch: 360; loss: 1.27; acc: 0.64
Batch: 380; loss: 1.2; acc: 0.69
Batch: 400; loss: 1.25; acc: 0.7
Batch: 420; loss: 1.11; acc: 0.7
Batch: 440; loss: 1.32; acc: 0.56
Batch: 460; loss: 1.23; acc: 0.7
Batch: 480; loss: 1.22; acc: 0.64
Batch: 500; loss: 1.15; acc: 0.73
Batch: 520; loss: 1.13; acc: 0.7
Batch: 540; loss: 1.04; acc: 0.73
Batch: 560; loss: 1.18; acc: 0.67
Batch: 580; loss: 1.09; acc: 0.73
Batch: 600; loss: 1.16; acc: 0.66
Batch: 620; loss: 1.29; acc: 0.56
Batch: 640; loss: 1.06; acc: 0.7
Batch: 660; loss: 1.19; acc: 0.66
Batch: 680; loss: 1.35; acc: 0.59
Batch: 700; loss: 1.21; acc: 0.64
Batch: 720; loss: 1.28; acc: 0.67
Batch: 740; loss: 1.23; acc: 0.64
Batch: 760; loss: 1.18; acc: 0.62
Batch: 780; loss: 1.19; acc: 0.72
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00018879999697674066
0.00018309347797185183
Batch: 0; loss: 1.15; acc: 0.7
Batch: 20; loss: 1.25; acc: 0.58
Batch: 40; loss: 0.89; acc: 0.78
Batch: 60; loss: 1.05; acc: 0.67
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.72
Val Epoch over. val_loss: 1.0772393664736657; val_accuracy: 0.7112858280254777 

The current subspace-distance is: 0.00018309347797185183 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.26; acc: 0.58
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 1.22; acc: 0.61
Batch: 60; loss: 1.1; acc: 0.69
Batch: 80; loss: 1.03; acc: 0.7
Batch: 100; loss: 1.14; acc: 0.7
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.13; acc: 0.69
Batch: 180; loss: 1.33; acc: 0.64
Batch: 200; loss: 1.27; acc: 0.58
Batch: 220; loss: 0.99; acc: 0.83
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.24; acc: 0.62
Batch: 280; loss: 1.19; acc: 0.58
Batch: 300; loss: 1.07; acc: 0.69
Batch: 320; loss: 1.24; acc: 0.62
Batch: 340; loss: 1.19; acc: 0.66
Batch: 360; loss: 1.22; acc: 0.62
Batch: 380; loss: 0.99; acc: 0.75
Batch: 400; loss: 1.37; acc: 0.53
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 1.19; acc: 0.7
Batch: 480; loss: 1.05; acc: 0.77
Batch: 500; loss: 1.23; acc: 0.61
Batch: 520; loss: 1.11; acc: 0.69
Batch: 540; loss: 1.19; acc: 0.64
Batch: 560; loss: 1.11; acc: 0.72
Batch: 580; loss: 1.18; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.73
Batch: 620; loss: 0.98; acc: 0.72
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 0.95; acc: 0.83
Batch: 680; loss: 1.0; acc: 0.77
Batch: 700; loss: 1.02; acc: 0.78
Batch: 720; loss: 1.3; acc: 0.64
Batch: 740; loss: 1.23; acc: 0.61
Batch: 760; loss: 1.12; acc: 0.72
Batch: 780; loss: 1.14; acc: 0.62
Train Epoch over. train_loss: 1.13; train_accuracy: 0.69 

0.00019055022858083248
0.00018077585264109075
Batch: 0; loss: 1.15; acc: 0.69
Batch: 20; loss: 1.27; acc: 0.58
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.98; acc: 0.7
Val Epoch over. val_loss: 1.072499114237014; val_accuracy: 0.7121815286624203 

The current subspace-distance is: 0.00018077585264109075 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.21; acc: 0.64
Batch: 20; loss: 1.18; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.62
Batch: 60; loss: 1.16; acc: 0.66
Batch: 80; loss: 1.46; acc: 0.53
Batch: 100; loss: 1.17; acc: 0.67
Batch: 120; loss: 1.22; acc: 0.7
Batch: 140; loss: 1.07; acc: 0.72
Batch: 160; loss: 1.31; acc: 0.58
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 1.25; acc: 0.62
Batch: 220; loss: 0.97; acc: 0.75
Batch: 240; loss: 1.31; acc: 0.53
Batch: 260; loss: 1.11; acc: 0.73
Batch: 280; loss: 1.05; acc: 0.69
Batch: 300; loss: 1.1; acc: 0.7
Batch: 320; loss: 1.0; acc: 0.72
Batch: 340; loss: 1.02; acc: 0.73
Batch: 360; loss: 0.99; acc: 0.8
Batch: 380; loss: 1.12; acc: 0.73
Batch: 400; loss: 1.16; acc: 0.72
Batch: 420; loss: 1.01; acc: 0.75
Batch: 440; loss: 1.17; acc: 0.64
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 1.07; acc: 0.77
Batch: 500; loss: 0.96; acc: 0.77
Batch: 520; loss: 1.09; acc: 0.64
Batch: 540; loss: 1.4; acc: 0.47
Batch: 560; loss: 0.96; acc: 0.8
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 1.08; acc: 0.64
Batch: 620; loss: 1.18; acc: 0.67
Batch: 640; loss: 1.01; acc: 0.75
Batch: 660; loss: 1.07; acc: 0.72
Batch: 680; loss: 1.04; acc: 0.69
Batch: 700; loss: 1.14; acc: 0.67
Batch: 720; loss: 1.13; acc: 0.62
Batch: 740; loss: 1.07; acc: 0.69
Batch: 760; loss: 1.13; acc: 0.72
Batch: 780; loss: 1.0; acc: 0.73
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00019073164730798453
0.00018687114061322063
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 1.06; acc: 0.7
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 0.97; acc: 0.67
Val Epoch over. val_loss: 1.0791855522781422; val_accuracy: 0.7102906050955414 

The current subspace-distance is: 0.00018687114061322063 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.21; acc: 0.56
Batch: 40; loss: 1.06; acc: 0.72
Batch: 60; loss: 1.19; acc: 0.66
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.02; acc: 0.7
Batch: 140; loss: 1.15; acc: 0.7
Batch: 160; loss: 0.98; acc: 0.75
Batch: 180; loss: 1.07; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.59
Batch: 220; loss: 1.2; acc: 0.61
Batch: 240; loss: 1.13; acc: 0.75
Batch: 260; loss: 1.24; acc: 0.64
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 1.11; acc: 0.67
Batch: 320; loss: 1.15; acc: 0.64
Batch: 340; loss: 1.08; acc: 0.77
Batch: 360; loss: 1.01; acc: 0.72
Batch: 380; loss: 1.07; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.67
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 1.05; acc: 0.73
Batch: 460; loss: 1.09; acc: 0.75
Batch: 480; loss: 0.99; acc: 0.77
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 1.09; acc: 0.66
Batch: 560; loss: 1.24; acc: 0.67
Batch: 580; loss: 1.13; acc: 0.7
Batch: 600; loss: 1.19; acc: 0.59
Batch: 620; loss: 1.24; acc: 0.67
Batch: 640; loss: 1.07; acc: 0.75
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.73
Batch: 700; loss: 1.13; acc: 0.73
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 1.17; acc: 0.66
Batch: 760; loss: 1.17; acc: 0.66
Batch: 780; loss: 1.07; acc: 0.64
Train Epoch over. train_loss: 1.13; train_accuracy: 0.69 

0.0001913127489387989
0.00018142569751944393
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.8
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.19; acc: 0.7
Batch: 140; loss: 0.95; acc: 0.7
Val Epoch over. val_loss: 1.068209323533781; val_accuracy: 0.7192476114649682 

The current subspace-distance is: 0.00018142569751944393 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.06; acc: 0.83
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 1.12; acc: 0.66
Batch: 60; loss: 1.27; acc: 0.58
Batch: 80; loss: 1.23; acc: 0.67
Batch: 100; loss: 1.19; acc: 0.67
Batch: 120; loss: 1.38; acc: 0.55
Batch: 140; loss: 1.15; acc: 0.66
Batch: 160; loss: 1.24; acc: 0.61
Batch: 180; loss: 1.19; acc: 0.67
Batch: 200; loss: 1.0; acc: 0.77
Batch: 220; loss: 1.07; acc: 0.7
Batch: 240; loss: 1.02; acc: 0.73
Batch: 260; loss: 1.04; acc: 0.69
Batch: 280; loss: 1.07; acc: 0.67
Batch: 300; loss: 1.19; acc: 0.67
Batch: 320; loss: 1.22; acc: 0.69
Batch: 340; loss: 1.0; acc: 0.75
Batch: 360; loss: 1.01; acc: 0.75
Batch: 380; loss: 1.26; acc: 0.61
Batch: 400; loss: 1.06; acc: 0.75
Batch: 420; loss: 1.1; acc: 0.66
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.15; acc: 0.72
Batch: 480; loss: 1.06; acc: 0.72
Batch: 500; loss: 0.97; acc: 0.83
Batch: 520; loss: 0.99; acc: 0.75
Batch: 540; loss: 0.87; acc: 0.81
Batch: 560; loss: 1.06; acc: 0.72
Batch: 580; loss: 1.25; acc: 0.66
Batch: 600; loss: 1.08; acc: 0.66
Batch: 620; loss: 1.21; acc: 0.67
Batch: 640; loss: 1.01; acc: 0.69
Batch: 660; loss: 1.16; acc: 0.66
Batch: 680; loss: 1.06; acc: 0.7
Batch: 700; loss: 1.07; acc: 0.72
Batch: 720; loss: 1.2; acc: 0.64
Batch: 740; loss: 1.01; acc: 0.81
Batch: 760; loss: 1.04; acc: 0.72
Batch: 780; loss: 1.07; acc: 0.72
Train Epoch over. train_loss: 1.12; train_accuracy: 0.69 

0.000193550979020074
0.00018544556223787367
Batch: 0; loss: 1.15; acc: 0.7
Batch: 20; loss: 1.27; acc: 0.58
Batch: 40; loss: 0.89; acc: 0.8
Batch: 60; loss: 1.06; acc: 0.67
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.69
Val Epoch over. val_loss: 1.0772990546408732; val_accuracy: 0.7116839171974523 

The current subspace-distance is: 0.00018544556223787367 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_8_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6

The number of parameters is: 266828

The number of individual parameters is:

13
234
13
13
20
35360
20
20
39
106080
39
39
64
119808
64
64
4096
64
640
10
64
64

nonzero elements in E: 53365596
elements in E: 53365600
fraction nonzero: 0.9999999250453475
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.51; acc: 0.09
Batch: 20; loss: 2.2; acc: 0.23
Batch: 40; loss: 2.04; acc: 0.34
Batch: 60; loss: 2.14; acc: 0.31
Batch: 80; loss: 1.97; acc: 0.39
Batch: 100; loss: 1.84; acc: 0.45
Batch: 120; loss: 1.75; acc: 0.52
Batch: 140; loss: 1.72; acc: 0.52
Batch: 160; loss: 1.74; acc: 0.55
Batch: 180; loss: 1.69; acc: 0.52
Batch: 200; loss: 1.68; acc: 0.53
Batch: 220; loss: 1.68; acc: 0.48
Batch: 240; loss: 1.68; acc: 0.55
Batch: 260; loss: 1.57; acc: 0.55
Batch: 280; loss: 1.63; acc: 0.58
Batch: 300; loss: 1.53; acc: 0.64
Batch: 320; loss: 1.52; acc: 0.7
Batch: 340; loss: 1.53; acc: 0.64
Batch: 360; loss: 1.62; acc: 0.53
Batch: 380; loss: 1.5; acc: 0.69
Batch: 400; loss: 1.45; acc: 0.73
Batch: 420; loss: 1.45; acc: 0.75
Batch: 440; loss: 1.54; acc: 0.61
Batch: 460; loss: 1.48; acc: 0.62
Batch: 480; loss: 1.44; acc: 0.66
Batch: 500; loss: 1.47; acc: 0.66
Batch: 520; loss: 1.47; acc: 0.69
Batch: 540; loss: 1.42; acc: 0.73
Batch: 560; loss: 1.48; acc: 0.64
Batch: 580; loss: 1.38; acc: 0.72
Batch: 600; loss: 1.45; acc: 0.64
Batch: 620; loss: 1.46; acc: 0.67
Batch: 640; loss: 1.46; acc: 0.67
Batch: 660; loss: 1.36; acc: 0.72
Batch: 680; loss: 1.34; acc: 0.72
Batch: 700; loss: 1.38; acc: 0.62
Batch: 720; loss: 1.36; acc: 0.73
Batch: 740; loss: 1.4; acc: 0.69
Batch: 760; loss: 1.49; acc: 0.59
Batch: 780; loss: 1.34; acc: 0.7
Train Epoch over. train_loss: 1.6; train_accuracy: 0.57 

6.2203747802414e-05
5.667852019541897e-05
Batch: 0; loss: 1.29; acc: 0.7
Batch: 20; loss: 1.43; acc: 0.66
Batch: 40; loss: 1.03; acc: 0.84
Batch: 60; loss: 1.28; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.77
Batch: 100; loss: 1.3; acc: 0.7
Batch: 120; loss: 1.41; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.78
Val Epoch over. val_loss: 1.295789206103914; val_accuracy: 0.7204418789808917 

The current subspace-distance is: 5.667852019541897e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.77
Batch: 20; loss: 1.29; acc: 0.72
Batch: 40; loss: 1.2; acc: 0.75
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.29; acc: 0.72
Batch: 100; loss: 1.25; acc: 0.78
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.18; acc: 0.81
Batch: 160; loss: 1.35; acc: 0.66
Batch: 180; loss: 1.24; acc: 0.72
Batch: 200; loss: 1.34; acc: 0.64
Batch: 220; loss: 1.28; acc: 0.73
Batch: 240; loss: 1.35; acc: 0.69
Batch: 260; loss: 1.24; acc: 0.72
Batch: 280; loss: 1.24; acc: 0.72
Batch: 300; loss: 1.19; acc: 0.73
Batch: 320; loss: 1.18; acc: 0.72
Batch: 340; loss: 1.26; acc: 0.69
Batch: 360; loss: 1.21; acc: 0.78
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.33; acc: 0.69
Batch: 420; loss: 1.35; acc: 0.66
Batch: 440; loss: 1.21; acc: 0.72
Batch: 460; loss: 1.24; acc: 0.72
Batch: 480; loss: 1.17; acc: 0.73
Batch: 500; loss: 1.16; acc: 0.75
Batch: 520; loss: 1.23; acc: 0.73
Batch: 540; loss: 1.15; acc: 0.77
Batch: 560; loss: 1.15; acc: 0.8
Batch: 580; loss: 1.27; acc: 0.66
Batch: 600; loss: 1.2; acc: 0.75
Batch: 620; loss: 1.22; acc: 0.8
Batch: 640; loss: 1.26; acc: 0.72
Batch: 660; loss: 1.18; acc: 0.66
Batch: 680; loss: 0.99; acc: 0.88
Batch: 700; loss: 1.15; acc: 0.77
Batch: 720; loss: 1.23; acc: 0.69
Batch: 740; loss: 1.17; acc: 0.73
Batch: 760; loss: 1.18; acc: 0.75
Batch: 780; loss: 1.08; acc: 0.81
Train Epoch over. train_loss: 1.25; train_accuracy: 0.71 

8.382452506339177e-05
7.902060315245762e-05
Batch: 0; loss: 1.14; acc: 0.77
Batch: 20; loss: 1.24; acc: 0.78
Batch: 40; loss: 0.88; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.19; acc: 0.73
Batch: 140; loss: 1.06; acc: 0.78
Val Epoch over. val_loss: 1.1254345839190636; val_accuracy: 0.7562699044585988 

The current subspace-distance is: 7.902060315245762e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.72
Batch: 20; loss: 1.13; acc: 0.81
Batch: 40; loss: 1.2; acc: 0.7
Batch: 60; loss: 1.18; acc: 0.7
Batch: 80; loss: 1.17; acc: 0.73
Batch: 100; loss: 1.1; acc: 0.78
Batch: 120; loss: 1.12; acc: 0.75
Batch: 140; loss: 0.99; acc: 0.78
Batch: 160; loss: 1.11; acc: 0.75
Batch: 180; loss: 0.96; acc: 0.83
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 1.0; acc: 0.86
Batch: 240; loss: 1.01; acc: 0.78
Batch: 260; loss: 1.26; acc: 0.62
Batch: 280; loss: 1.23; acc: 0.64
Batch: 300; loss: 1.19; acc: 0.75
Batch: 320; loss: 1.12; acc: 0.7
Batch: 340; loss: 1.17; acc: 0.72
Batch: 360; loss: 1.04; acc: 0.77
Batch: 380; loss: 1.09; acc: 0.77
Batch: 400; loss: 1.23; acc: 0.66
Batch: 420; loss: 1.18; acc: 0.7
Batch: 440; loss: 1.19; acc: 0.69
Batch: 460; loss: 1.11; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.8
Batch: 500; loss: 1.1; acc: 0.75
Batch: 520; loss: 1.09; acc: 0.78
Batch: 540; loss: 1.07; acc: 0.77
Batch: 560; loss: 0.93; acc: 0.86
Batch: 580; loss: 1.09; acc: 0.78
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.04; acc: 0.75
Batch: 640; loss: 1.1; acc: 0.67
Batch: 660; loss: 1.04; acc: 0.73
Batch: 680; loss: 1.13; acc: 0.73
Batch: 700; loss: 1.13; acc: 0.78
Batch: 720; loss: 1.13; acc: 0.77
Batch: 740; loss: 1.04; acc: 0.77
Batch: 760; loss: 1.15; acc: 0.69
Batch: 780; loss: 1.26; acc: 0.66
Train Epoch over. train_loss: 1.13; train_accuracy: 0.73 

0.0001006072125164792
9.605508239474148e-05
Batch: 0; loss: 1.08; acc: 0.77
Batch: 20; loss: 1.15; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.89
Batch: 60; loss: 1.09; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.8
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 0.95; acc: 0.77
Val Epoch over. val_loss: 1.0303066653810489; val_accuracy: 0.7673168789808917 

The current subspace-distance is: 9.605508239474148e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.69
Batch: 20; loss: 1.0; acc: 0.81
Batch: 40; loss: 1.05; acc: 0.7
Batch: 60; loss: 1.06; acc: 0.73
Batch: 80; loss: 1.0; acc: 0.8
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.0; acc: 0.78
Batch: 140; loss: 1.09; acc: 0.73
Batch: 160; loss: 1.15; acc: 0.75
Batch: 180; loss: 1.23; acc: 0.66
Batch: 200; loss: 1.06; acc: 0.77
Batch: 220; loss: 1.09; acc: 0.73
Batch: 240; loss: 1.12; acc: 0.7
Batch: 260; loss: 1.08; acc: 0.75
Batch: 280; loss: 1.03; acc: 0.7
Batch: 300; loss: 1.0; acc: 0.8
Batch: 320; loss: 0.97; acc: 0.83
Batch: 340; loss: 1.16; acc: 0.66
Batch: 360; loss: 0.87; acc: 0.84
Batch: 380; loss: 1.07; acc: 0.72
Batch: 400; loss: 1.04; acc: 0.8
Batch: 420; loss: 0.95; acc: 0.83
Batch: 440; loss: 1.0; acc: 0.8
Batch: 460; loss: 1.08; acc: 0.73
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.99; acc: 0.75
Batch: 520; loss: 0.93; acc: 0.83
Batch: 540; loss: 1.01; acc: 0.81
Batch: 560; loss: 1.06; acc: 0.77
Batch: 580; loss: 0.97; acc: 0.75
Batch: 600; loss: 0.98; acc: 0.83
Batch: 620; loss: 0.97; acc: 0.77
Batch: 640; loss: 0.97; acc: 0.8
Batch: 660; loss: 0.95; acc: 0.8
Batch: 680; loss: 1.03; acc: 0.7
Batch: 700; loss: 1.03; acc: 0.77
Batch: 720; loss: 1.22; acc: 0.69
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 0.98; acc: 0.73
Batch: 780; loss: 1.02; acc: 0.73
Train Epoch over. train_loss: 1.03; train_accuracy: 0.76 

0.00011722469207597896
0.00011067574814660475
Batch: 0; loss: 0.94; acc: 0.78
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.97; acc: 0.78
Batch: 80; loss: 0.81; acc: 0.84
Batch: 100; loss: 0.88; acc: 0.88
Batch: 120; loss: 1.07; acc: 0.7
Batch: 140; loss: 0.87; acc: 0.84
Val Epoch over. val_loss: 0.9270196397593067; val_accuracy: 0.7969745222929936 

The current subspace-distance is: 0.00011067574814660475 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.78
Batch: 20; loss: 0.94; acc: 0.78
Batch: 40; loss: 1.06; acc: 0.75
Batch: 60; loss: 1.05; acc: 0.75
Batch: 80; loss: 1.07; acc: 0.75
Batch: 100; loss: 0.96; acc: 0.77
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.92; acc: 0.8
Batch: 160; loss: 0.9; acc: 0.78
Batch: 180; loss: 1.01; acc: 0.75
Batch: 200; loss: 0.91; acc: 0.81
Batch: 220; loss: 0.99; acc: 0.81
Batch: 240; loss: 1.04; acc: 0.72
Batch: 260; loss: 0.94; acc: 0.8
Batch: 280; loss: 0.97; acc: 0.81
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 0.95; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.81
Batch: 360; loss: 0.98; acc: 0.78
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 0.89; acc: 0.81
Batch: 440; loss: 0.84; acc: 0.86
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.0; acc: 0.75
Batch: 500; loss: 0.86; acc: 0.8
Batch: 520; loss: 0.97; acc: 0.75
Batch: 540; loss: 0.87; acc: 0.81
Batch: 560; loss: 0.86; acc: 0.78
Batch: 580; loss: 0.89; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.83
Batch: 620; loss: 0.77; acc: 0.89
Batch: 640; loss: 0.89; acc: 0.77
Batch: 660; loss: 0.9; acc: 0.75
Batch: 680; loss: 0.93; acc: 0.75
Batch: 700; loss: 0.94; acc: 0.73
Batch: 720; loss: 0.84; acc: 0.8
Batch: 740; loss: 0.77; acc: 0.86
Batch: 760; loss: 0.84; acc: 0.78
Batch: 780; loss: 1.0; acc: 0.72
Train Epoch over. train_loss: 0.94; train_accuracy: 0.78 

0.00013274118828121573
0.00012644972593989223
Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.77
Batch: 40; loss: 0.58; acc: 0.91
Batch: 60; loss: 0.86; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.91
Batch: 100; loss: 0.81; acc: 0.86
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 0.79; acc: 0.83
Val Epoch over. val_loss: 0.8465746747460335; val_accuracy: 0.8153861464968153 

The current subspace-distance is: 0.00012644972593989223 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.96; acc: 0.8
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.78
Batch: 100; loss: 0.86; acc: 0.8
Batch: 120; loss: 1.03; acc: 0.75
Batch: 140; loss: 0.9; acc: 0.83
Batch: 160; loss: 1.03; acc: 0.78
Batch: 180; loss: 0.82; acc: 0.88
Batch: 200; loss: 0.97; acc: 0.8
Batch: 220; loss: 0.86; acc: 0.77
Batch: 240; loss: 0.84; acc: 0.83
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.88; acc: 0.81
Batch: 300; loss: 0.98; acc: 0.78
Batch: 320; loss: 0.86; acc: 0.8
Batch: 340; loss: 0.9; acc: 0.77
Batch: 360; loss: 1.01; acc: 0.72
Batch: 380; loss: 0.84; acc: 0.88
Batch: 400; loss: 0.8; acc: 0.8
Batch: 420; loss: 0.82; acc: 0.81
Batch: 440; loss: 0.92; acc: 0.72
Batch: 460; loss: 0.93; acc: 0.81
Batch: 480; loss: 0.81; acc: 0.83
Batch: 500; loss: 0.75; acc: 0.89
Batch: 520; loss: 0.79; acc: 0.88
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.81; acc: 0.81
Batch: 600; loss: 0.81; acc: 0.8
Batch: 620; loss: 0.81; acc: 0.88
Batch: 640; loss: 0.85; acc: 0.75
Batch: 660; loss: 1.0; acc: 0.8
Batch: 680; loss: 0.87; acc: 0.81
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.81; acc: 0.73
Batch: 740; loss: 0.9; acc: 0.8
Batch: 760; loss: 0.84; acc: 0.77
Batch: 780; loss: 0.86; acc: 0.81
Train Epoch over. train_loss: 0.87; train_accuracy: 0.79 

0.00014310602273326367
0.00013937508629169315
Batch: 0; loss: 0.84; acc: 0.81
Batch: 20; loss: 0.96; acc: 0.8
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.86
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.72; acc: 0.86
Val Epoch over. val_loss: 0.7953313068979105; val_accuracy: 0.8236464968152867 

The current subspace-distance is: 0.00013937508629169315 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.75
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.83; acc: 0.86
Batch: 80; loss: 0.78; acc: 0.83
Batch: 100; loss: 0.87; acc: 0.84
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.7; acc: 0.89
Batch: 160; loss: 0.83; acc: 0.8
Batch: 180; loss: 0.84; acc: 0.88
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.92; acc: 0.69
Batch: 240; loss: 0.92; acc: 0.75
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.78; acc: 0.81
Batch: 300; loss: 0.94; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.84
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.74; acc: 0.84
Batch: 380; loss: 0.81; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.83
Batch: 420; loss: 0.89; acc: 0.77
Batch: 440; loss: 1.11; acc: 0.62
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.86; acc: 0.81
Batch: 500; loss: 1.12; acc: 0.7
Batch: 520; loss: 0.79; acc: 0.78
Batch: 540; loss: 0.86; acc: 0.8
Batch: 560; loss: 0.75; acc: 0.88
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.98; acc: 0.75
Batch: 620; loss: 0.73; acc: 0.81
Batch: 640; loss: 0.83; acc: 0.78
Batch: 660; loss: 0.8; acc: 0.78
Batch: 680; loss: 0.84; acc: 0.81
Batch: 700; loss: 0.91; acc: 0.77
Batch: 720; loss: 0.75; acc: 0.8
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.82; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

0.00015670277934987098
0.00014940681285224855
Batch: 0; loss: 0.79; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.78
Batch: 40; loss: 0.48; acc: 0.94
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.94
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.88
Val Epoch over. val_loss: 0.7454502159243177; val_accuracy: 0.8359872611464968 

The current subspace-distance is: 0.00014940681285224855 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 0.84; acc: 0.8
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.93; acc: 0.7
Batch: 120; loss: 0.66; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.94
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.82; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.76; acc: 0.84
Batch: 300; loss: 0.94; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.86
Batch: 340; loss: 0.99; acc: 0.69
Batch: 360; loss: 0.86; acc: 0.81
Batch: 380; loss: 0.78; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 0.72; acc: 0.86
Batch: 460; loss: 0.72; acc: 0.8
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.76; acc: 0.86
Batch: 580; loss: 0.73; acc: 0.86
Batch: 600; loss: 0.79; acc: 0.81
Batch: 620; loss: 1.02; acc: 0.69
Batch: 640; loss: 0.84; acc: 0.78
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.78; acc: 0.84
Batch: 700; loss: 0.81; acc: 0.78
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.71; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.78; train_accuracy: 0.81 

0.00016446692461613566
0.00015689176507294178
Batch: 0; loss: 0.76; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.7017536478437436; val_accuracy: 0.8428542993630573 

The current subspace-distance is: 0.00015689176507294178 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.87; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.88
Batch: 60; loss: 0.88; acc: 0.78
Batch: 80; loss: 0.82; acc: 0.83
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.8; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.84
Batch: 180; loss: 0.91; acc: 0.77
Batch: 200; loss: 0.56; acc: 0.94
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.73; acc: 0.84
Batch: 260; loss: 0.75; acc: 0.83
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.91
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.67; acc: 0.88
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.77; acc: 0.83
Batch: 480; loss: 0.88; acc: 0.77
Batch: 500; loss: 0.82; acc: 0.78
Batch: 520; loss: 0.76; acc: 0.81
Batch: 540; loss: 0.91; acc: 0.7
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.83; acc: 0.77
Batch: 620; loss: 0.68; acc: 0.81
Batch: 640; loss: 0.67; acc: 0.86
Batch: 660; loss: 0.74; acc: 0.84
Batch: 680; loss: 0.88; acc: 0.75
Batch: 700; loss: 0.9; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.83; acc: 0.77
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.0001752691314322874
0.00016959193453658372
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.54; acc: 0.89
Val Epoch over. val_loss: 0.6757566349901212; val_accuracy: 0.8486265923566879 

The current subspace-distance is: 0.00016959193453658372 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.97; acc: 0.72
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.68; acc: 0.88
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.91
Batch: 260; loss: 0.83; acc: 0.77
Batch: 280; loss: 0.82; acc: 0.83
Batch: 300; loss: 0.7; acc: 0.86
Batch: 320; loss: 0.78; acc: 0.8
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.83; acc: 0.78
Batch: 380; loss: 0.72; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.81; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.8
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.82; acc: 0.72
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.83; acc: 0.81
Batch: 600; loss: 0.8; acc: 0.8
Batch: 620; loss: 0.61; acc: 0.91
Batch: 640; loss: 0.65; acc: 0.91
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.83; acc: 0.78
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.78; acc: 0.78
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.92; acc: 0.62
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.00018513263785280287
0.00017741926421876997
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.94
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.91
Val Epoch over. val_loss: 0.6611991238062549; val_accuracy: 0.8526074840764332 

The current subspace-distance is: 0.00017741926421876997 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.83
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.8; acc: 0.8
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.92
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.78; acc: 0.83
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.79; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.76; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.89
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.8; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.84; acc: 0.77
Batch: 740; loss: 0.79; acc: 0.8
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.78
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00018637195171322674
0.00017784505325835198
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.89
Val Epoch over. val_loss: 0.6416133177128567; val_accuracy: 0.853702229299363 

The current subspace-distance is: 0.00017784505325835198 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.88
Batch: 80; loss: 0.74; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.7; acc: 0.88
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.85; acc: 0.73
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.94; acc: 0.77
Batch: 300; loss: 0.64; acc: 0.92
Batch: 320; loss: 0.89; acc: 0.75
Batch: 340; loss: 0.75; acc: 0.86
Batch: 360; loss: 0.85; acc: 0.72
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.99; acc: 0.69
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.78
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.72; acc: 0.88
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.81; acc: 0.84
Batch: 600; loss: 0.75; acc: 0.83
Batch: 620; loss: 0.62; acc: 0.92
Batch: 640; loss: 0.74; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.86
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.82; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00018952782556880265
0.0001826474763220176
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.95
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.636738035899059; val_accuracy: 0.8572850318471338 

The current subspace-distance is: 0.0001826474763220176 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.74; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.81
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.65; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.75; acc: 0.78
Batch: 380; loss: 0.58; acc: 0.91
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.79; acc: 0.73
Batch: 440; loss: 0.79; acc: 0.81
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.87; acc: 0.72
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 1.0; acc: 0.69
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.81
Batch: 660; loss: 0.8; acc: 0.72
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.77
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00019342004088684916
0.0001866399252321571
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.6362786017785407; val_accuracy: 0.8577826433121019 

The current subspace-distance is: 0.0001866399252321571 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.74; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.73; acc: 0.78
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.8; acc: 0.78
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.7; acc: 0.86
Batch: 480; loss: 0.85; acc: 0.8
Batch: 500; loss: 0.89; acc: 0.77
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.93; acc: 0.73
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.8
Batch: 680; loss: 0.78; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.89
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.0001961116213351488
0.00018914621614385396
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.91
Val Epoch over. val_loss: 0.6323093312561132; val_accuracy: 0.8563893312101911 

The current subspace-distance is: 0.00018914621614385396 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.81
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.88; acc: 0.77
Batch: 240; loss: 0.78; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.78; acc: 0.8
Batch: 400; loss: 0.81; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.88
Batch: 460; loss: 0.71; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.78
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.82; acc: 0.81
Batch: 540; loss: 0.84; acc: 0.83
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.8
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.66; acc: 0.8
Batch: 740; loss: 0.93; acc: 0.78
Batch: 760; loss: 0.56; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00019690573390107602
0.00019018405873794109
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6267840978066632; val_accuracy: 0.856687898089172 

The current subspace-distance is: 0.00019018405873794109 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.65; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.92
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.83; acc: 0.7
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.84
Batch: 280; loss: 0.85; acc: 0.77
Batch: 300; loss: 0.83; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.8
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.81; acc: 0.75
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.74; acc: 0.81
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 0.44; acc: 0.94
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.76; acc: 0.78
Batch: 700; loss: 0.7; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.000201034068595618
0.00019315718964207917
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.6101284190348; val_accuracy: 0.8624601910828026 

The current subspace-distance is: 0.00019315718964207917 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.92
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.79; acc: 0.73
Batch: 160; loss: 0.65; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.7; acc: 0.83
Batch: 300; loss: 0.7; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.91
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.86
Batch: 440; loss: 0.73; acc: 0.78
Batch: 460; loss: 0.82; acc: 0.8
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.87; acc: 0.72
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.86
Batch: 600; loss: 0.75; acc: 0.8
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.74; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.71; acc: 0.84
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.88; acc: 0.78
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00020306701480876654
0.0001960785739356652
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.6084052654588299; val_accuracy: 0.8619625796178344 

The current subspace-distance is: 0.0001960785739356652 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.73
Batch: 20; loss: 0.63; acc: 0.91
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.89
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.83; acc: 0.73
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.8; acc: 0.81
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.94
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.58; acc: 0.89
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.78
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.52; acc: 0.92
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.72; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.87; acc: 0.77
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.00020538782700896263
0.0001962068199645728
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.6058091652241482; val_accuracy: 0.8606687898089171 

The current subspace-distance is: 0.0001962068199645728 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.73; acc: 0.78
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.77
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.78; acc: 0.8
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.49; acc: 0.94
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.71; acc: 0.81
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.88
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.71; acc: 0.84
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.67; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.89
Batch: 740; loss: 0.7; acc: 0.78
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.00020619416318368167
0.00019808537035714835
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.94
Val Epoch over. val_loss: 0.6014559396133301; val_accuracy: 0.8628582802547771 

The current subspace-distance is: 0.00019808537035714835 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.75; acc: 0.8
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.92
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.78
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.89
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.74; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.85; acc: 0.77
Batch: 720; loss: 0.73; acc: 0.78
Batch: 740; loss: 0.68; acc: 0.91
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00020948101882822812
0.00020039634546265006
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6017226831168886; val_accuracy: 0.8594745222929936 

The current subspace-distance is: 0.00020039634546265006 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.85; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.67; acc: 0.78
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.81
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.82; acc: 0.8
Batch: 400; loss: 0.62; acc: 0.88
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 0.78; acc: 0.75
Batch: 460; loss: 0.86; acc: 0.84
Batch: 480; loss: 0.84; acc: 0.77
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.76; acc: 0.73
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.62; acc: 0.8
Batch: 620; loss: 0.63; acc: 0.8
Batch: 640; loss: 0.88; acc: 0.72
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.94
Batch: 740; loss: 0.85; acc: 0.78
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00020614831009879708
0.0002003471745410934
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.5983518077309724; val_accuracy: 0.8586783439490446 

The current subspace-distance is: 0.0002003471745410934 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.91
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.89
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.79; acc: 0.8
Batch: 600; loss: 0.55; acc: 0.91
Batch: 620; loss: 0.89; acc: 0.73
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.92
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.77; acc: 0.75
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.00021046664915047586
0.00020188416237942874
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.5944277944078871; val_accuracy: 0.8621616242038217 

The current subspace-distance is: 0.00020188416237942874 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.89
Batch: 300; loss: 0.73; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.77; acc: 0.83
Batch: 480; loss: 0.65; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.78
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.87; acc: 0.8
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.77
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.78; acc: 0.75
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.8; acc: 0.73
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

0.0002104958111885935
0.00020413250604178756
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6027107646890507; val_accuracy: 0.8613654458598726 

The current subspace-distance is: 0.00020413250604178756 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.73; acc: 0.8
Batch: 300; loss: 0.89; acc: 0.77
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.89; acc: 0.73
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.83; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.68; acc: 0.77
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00021285803813952953
0.00020450045121833682
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.5894800249937993; val_accuracy: 0.8635549363057324 

The current subspace-distance is: 0.00020450045121833682 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.72; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.94
Batch: 200; loss: 0.86; acc: 0.73
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.83; acc: 0.72
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.73; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.82; acc: 0.78
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.88
Batch: 520; loss: 0.82; acc: 0.72
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.76; acc: 0.73
Batch: 620; loss: 0.75; acc: 0.8
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.77; acc: 0.83
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.75
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00020951063197571784
0.00020427361596375704
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.5894074965814117; val_accuracy: 0.862062101910828 

The current subspace-distance is: 0.00020427361596375704 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 0.94; acc: 0.7
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.81; acc: 0.73
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.94
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.89; acc: 0.73
Batch: 520; loss: 0.68; acc: 0.8
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.72; acc: 0.8
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.86; acc: 0.72
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00021187111269682646
0.00020369139383547008
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.5925706000464737; val_accuracy: 0.861265923566879 

The current subspace-distance is: 0.00020369139383547008 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.77
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.67; acc: 0.78
Batch: 160; loss: 0.86; acc: 0.75
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.74; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.71; acc: 0.78
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.7; acc: 0.83
Batch: 460; loss: 0.82; acc: 0.75
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.64; acc: 0.78
Batch: 580; loss: 0.73; acc: 0.81
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.94
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.64; acc: 0.91
Batch: 720; loss: 0.84; acc: 0.77
Batch: 740; loss: 0.76; acc: 0.77
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00021119066514074802
0.0002033631462836638
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.5900643794399918; val_accuracy: 0.8597730891719745 

The current subspace-distance is: 0.0002033631462836638 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.91
Batch: 160; loss: 0.75; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.78; acc: 0.78
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.88; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.79; acc: 0.81
Batch: 480; loss: 0.72; acc: 0.8
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.77; acc: 0.83
Batch: 580; loss: 0.79; acc: 0.8
Batch: 600; loss: 0.58; acc: 0.88
Batch: 620; loss: 0.77; acc: 0.81
Batch: 640; loss: 0.48; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.94
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.89
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00021438184194266796
0.0002039810351561755
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.5901166568895814; val_accuracy: 0.8603702229299363 

The current subspace-distance is: 0.0002039810351561755 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.79; acc: 0.8
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.84; acc: 0.78
Batch: 600; loss: 0.64; acc: 0.84
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.76; acc: 0.77
Batch: 780; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

0.00021450217172969133
0.00020499201491475105
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.92
Val Epoch over. val_loss: 0.5834222466323027; val_accuracy: 0.8623606687898089 

The current subspace-distance is: 0.00020499201491475105 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.88
Batch: 140; loss: 0.78; acc: 0.78
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.94
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.8
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.61; acc: 0.89
Batch: 380; loss: 0.67; acc: 0.86
Batch: 400; loss: 0.82; acc: 0.77
Batch: 420; loss: 0.73; acc: 0.77
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.98; acc: 0.67
Batch: 520; loss: 0.72; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.42; acc: 0.95
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.74; acc: 0.78
Batch: 680; loss: 0.67; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.91
Batch: 740; loss: 1.01; acc: 0.69
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00021443840523716062
0.00020580663112923503
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.5947069155562456; val_accuracy: 0.861265923566879 

The current subspace-distance is: 0.00020580663112923503 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_8_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6

The number of parameters is: 266828

The number of individual parameters is:

13
234
13
13
20
35360
20
20
39
106080
39
39
64
119808
64
64
4096
64
640
10
64
64

nonzero elements in E: 80048394
elements in E: 80048400
fraction nonzero: 0.9999999250453475
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.38; acc: 0.14
Batch: 20; loss: 2.31; acc: 0.11
Batch: 40; loss: 2.04; acc: 0.28
Batch: 60; loss: 1.9; acc: 0.45
Batch: 80; loss: 1.95; acc: 0.36
Batch: 100; loss: 1.77; acc: 0.42
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.57; acc: 0.66
Batch: 160; loss: 1.6; acc: 0.67
Batch: 180; loss: 1.67; acc: 0.58
Batch: 200; loss: 1.67; acc: 0.5
Batch: 220; loss: 1.57; acc: 0.61
Batch: 240; loss: 1.58; acc: 0.48
Batch: 260; loss: 1.53; acc: 0.61
Batch: 280; loss: 1.44; acc: 0.66
Batch: 300; loss: 1.51; acc: 0.64
Batch: 320; loss: 1.55; acc: 0.64
Batch: 340; loss: 1.42; acc: 0.7
Batch: 360; loss: 1.53; acc: 0.64
Batch: 380; loss: 1.5; acc: 0.62
Batch: 400; loss: 1.65; acc: 0.47
Batch: 420; loss: 1.5; acc: 0.61
Batch: 440; loss: 1.45; acc: 0.61
Batch: 460; loss: 1.36; acc: 0.7
Batch: 480; loss: 1.27; acc: 0.69
Batch: 500; loss: 1.39; acc: 0.66
Batch: 520; loss: 1.27; acc: 0.73
Batch: 540; loss: 1.23; acc: 0.8
Batch: 560; loss: 1.31; acc: 0.73
Batch: 580; loss: 1.29; acc: 0.75
Batch: 600; loss: 1.18; acc: 0.8
Batch: 620; loss: 1.31; acc: 0.78
Batch: 640; loss: 1.23; acc: 0.77
Batch: 660; loss: 1.3; acc: 0.75
Batch: 680; loss: 1.2; acc: 0.78
Batch: 700; loss: 1.33; acc: 0.73
Batch: 720; loss: 1.27; acc: 0.77
Batch: 740; loss: 1.19; acc: 0.77
Batch: 760; loss: 1.1; acc: 0.83
Batch: 780; loss: 1.09; acc: 0.8
Train Epoch over. train_loss: 1.49; train_accuracy: 0.63 

6.627002585446462e-05
6.131214468041435e-05
Batch: 0; loss: 1.19; acc: 0.75
Batch: 20; loss: 1.22; acc: 0.7
Batch: 40; loss: 0.91; acc: 0.88
Batch: 60; loss: 1.08; acc: 0.78
Batch: 80; loss: 1.02; acc: 0.83
Batch: 100; loss: 1.13; acc: 0.8
Batch: 120; loss: 1.25; acc: 0.72
Batch: 140; loss: 1.08; acc: 0.83
Val Epoch over. val_loss: 1.1489829504565827; val_accuracy: 0.7906050955414012 

The current subspace-distance is: 6.131214468041435e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.81
Batch: 20; loss: 1.11; acc: 0.83
Batch: 40; loss: 1.22; acc: 0.75
Batch: 60; loss: 1.23; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.78
Batch: 100; loss: 1.18; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.72
Batch: 140; loss: 1.2; acc: 0.69
Batch: 160; loss: 1.29; acc: 0.73
Batch: 180; loss: 1.13; acc: 0.75
Batch: 200; loss: 1.09; acc: 0.8
Batch: 220; loss: 1.05; acc: 0.83
Batch: 240; loss: 1.01; acc: 0.84
Batch: 260; loss: 1.16; acc: 0.77
Batch: 280; loss: 0.97; acc: 0.91
Batch: 300; loss: 1.1; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.73
Batch: 340; loss: 1.12; acc: 0.77
Batch: 360; loss: 0.9; acc: 0.88
Batch: 380; loss: 1.05; acc: 0.83
Batch: 400; loss: 1.04; acc: 0.81
Batch: 420; loss: 1.06; acc: 0.81
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 1.06; acc: 0.75
Batch: 480; loss: 0.99; acc: 0.8
Batch: 500; loss: 1.08; acc: 0.78
Batch: 520; loss: 0.95; acc: 0.86
Batch: 540; loss: 1.05; acc: 0.78
Batch: 560; loss: 1.05; acc: 0.8
Batch: 580; loss: 0.93; acc: 0.8
Batch: 600; loss: 1.03; acc: 0.8
Batch: 620; loss: 1.05; acc: 0.77
Batch: 640; loss: 0.96; acc: 0.88
Batch: 660; loss: 1.02; acc: 0.78
Batch: 680; loss: 0.84; acc: 0.94
Batch: 700; loss: 1.02; acc: 0.78
Batch: 720; loss: 1.1; acc: 0.77
Batch: 740; loss: 0.95; acc: 0.84
Batch: 760; loss: 0.98; acc: 0.8
Batch: 780; loss: 1.07; acc: 0.72
Train Epoch over. train_loss: 1.07; train_accuracy: 0.79 

9.306737774750218e-05
8.911568875191733e-05
Batch: 0; loss: 0.94; acc: 0.8
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 0.65; acc: 0.94
Batch: 60; loss: 0.87; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.89
Batch: 100; loss: 0.85; acc: 0.86
Batch: 120; loss: 1.03; acc: 0.81
Batch: 140; loss: 0.83; acc: 0.88
Val Epoch over. val_loss: 0.9078613796811195; val_accuracy: 0.8341958598726115 

The current subspace-distance is: 8.911568875191733e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.8
Batch: 20; loss: 1.0; acc: 0.81
Batch: 40; loss: 1.02; acc: 0.78
Batch: 60; loss: 0.93; acc: 0.8
Batch: 80; loss: 0.94; acc: 0.8
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.84
Batch: 140; loss: 0.91; acc: 0.83
Batch: 160; loss: 0.98; acc: 0.78
Batch: 180; loss: 0.9; acc: 0.88
Batch: 200; loss: 0.96; acc: 0.78
Batch: 220; loss: 0.87; acc: 0.88
Batch: 240; loss: 0.93; acc: 0.84
Batch: 260; loss: 0.9; acc: 0.86
Batch: 280; loss: 0.84; acc: 0.86
Batch: 300; loss: 1.01; acc: 0.8
Batch: 320; loss: 1.0; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.86
Batch: 360; loss: 0.89; acc: 0.83
Batch: 380; loss: 0.89; acc: 0.83
Batch: 400; loss: 0.85; acc: 0.83
Batch: 420; loss: 0.94; acc: 0.84
Batch: 440; loss: 1.03; acc: 0.7
Batch: 460; loss: 0.91; acc: 0.83
Batch: 480; loss: 0.77; acc: 0.86
Batch: 500; loss: 0.89; acc: 0.78
Batch: 520; loss: 0.76; acc: 0.88
Batch: 540; loss: 0.84; acc: 0.81
Batch: 560; loss: 0.73; acc: 0.88
Batch: 580; loss: 0.85; acc: 0.81
Batch: 600; loss: 0.95; acc: 0.78
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.73; acc: 0.92
Batch: 660; loss: 0.69; acc: 0.88
Batch: 680; loss: 0.81; acc: 0.89
Batch: 700; loss: 0.86; acc: 0.8
Batch: 720; loss: 0.93; acc: 0.78
Batch: 740; loss: 0.74; acc: 0.88
Batch: 760; loss: 0.88; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.88
Train Epoch over. train_loss: 0.88; train_accuracy: 0.82 

0.00011653659021249041
0.00011128240294056013
Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.5; acc: 0.97
Batch: 60; loss: 0.75; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.94
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.92
Val Epoch over. val_loss: 0.7498968854831283; val_accuracy: 0.8572850318471338 

The current subspace-distance is: 0.00011128240294056013 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.94
Batch: 80; loss: 1.0; acc: 0.75
Batch: 100; loss: 0.74; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.92
Batch: 200; loss: 0.63; acc: 0.92
Batch: 220; loss: 0.74; acc: 0.89
Batch: 240; loss: 0.77; acc: 0.88
Batch: 260; loss: 0.82; acc: 0.78
Batch: 280; loss: 0.85; acc: 0.78
Batch: 300; loss: 0.61; acc: 0.92
Batch: 320; loss: 0.85; acc: 0.77
Batch: 340; loss: 0.63; acc: 0.91
Batch: 360; loss: 0.79; acc: 0.86
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.89
Batch: 460; loss: 1.03; acc: 0.72
Batch: 480; loss: 0.84; acc: 0.81
Batch: 500; loss: 0.77; acc: 0.86
Batch: 520; loss: 0.78; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.94
Batch: 560; loss: 0.98; acc: 0.77
Batch: 580; loss: 0.77; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.88
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.72; acc: 0.91
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.88; acc: 0.78
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 0.73; acc: 0.84
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.76; train_accuracy: 0.84 

0.00013440655311569571
0.00012894388055428863
Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.94
Batch: 120; loss: 0.8; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.94
Val Epoch over. val_loss: 0.6607715350807093; val_accuracy: 0.8743033439490446 

The current subspace-distance is: 0.00012894388055428863 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.7; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.91
Batch: 140; loss: 0.68; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.97
Batch: 180; loss: 0.77; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.81; acc: 0.78
Batch: 240; loss: 0.78; acc: 0.81
Batch: 260; loss: 0.68; acc: 0.89
Batch: 280; loss: 0.79; acc: 0.81
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.92
Batch: 340; loss: 0.62; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.72; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.89
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.95
Batch: 480; loss: 0.67; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.6; acc: 0.91
Batch: 540; loss: 0.62; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.75; acc: 0.84
Batch: 620; loss: 0.75; acc: 0.83
Batch: 640; loss: 0.74; acc: 0.81
Batch: 660; loss: 0.84; acc: 0.83
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.97
Batch: 720; loss: 0.63; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.71; acc: 0.78
Batch: 780; loss: 0.86; acc: 0.77
Train Epoch over. train_loss: 0.69; train_accuracy: 0.86 

0.00014865804405417293
0.0001428630785085261
Batch: 0; loss: 0.71; acc: 0.81
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.95
Val Epoch over. val_loss: 0.6025565296980986; val_accuracy: 0.8831608280254777 

The current subspace-distance is: 0.0001428630785085261 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.94
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.91
Batch: 220; loss: 0.8; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.67; acc: 0.95
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.89
Batch: 440; loss: 0.74; acc: 0.88
Batch: 460; loss: 0.86; acc: 0.86
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.71; acc: 0.8
Batch: 640; loss: 0.75; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.92
Batch: 700; loss: 0.69; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.68; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.64; train_accuracy: 0.86 

0.00016416734433732927
0.00015676647308282554
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.5590684461365839; val_accuracy: 0.8869426751592356 

The current subspace-distance is: 0.00015676647308282554 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 0.5; acc: 0.94
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.91
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.94
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.87 

0.0001730458316160366
0.00016649439930915833
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.5371145940130684; val_accuracy: 0.8882364649681529 

The current subspace-distance is: 0.00016649439930915833 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.63; acc: 0.89
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.55; acc: 0.91
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.95
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.94
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.97
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00018482918676454574
0.00017780548660084605
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.4964271639562716; val_accuracy: 0.8926154458598726 

The current subspace-distance is: 0.00017780548660084605 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.91
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.94
Batch: 260; loss: 0.74; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.92
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.94
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.92
Batch: 600; loss: 0.61; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.00019265743321739137
0.00018520951562095433
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.47777331368938375; val_accuracy: 0.8960987261146497 

The current subspace-distance is: 0.00018520951562095433 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.95
Batch: 140; loss: 0.69; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.65; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.65; acc: 0.84
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.75; acc: 0.73
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020081443653907627
0.0001952423481270671
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.4506536256166021; val_accuracy: 0.8996815286624203 

The current subspace-distance is: 0.0001952423481270671 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020473121549002826
0.00019753746164496988
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.4540212590033841; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 0.00019753746164496988 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.97
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.74; acc: 0.73
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.61; acc: 0.78
Batch: 320; loss: 0.66; acc: 0.78
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.61; acc: 0.8
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0002085596788674593
0.00019911257550120354
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.4436940691273683; val_accuracy: 0.9002786624203821 

The current subspace-distance is: 0.00019911257550120354 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.95
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.95
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.97
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0002114269300363958
0.00020369539561215788
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.436829195470567; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 0.00020369539561215788 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.66; acc: 0.77
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.72; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.66; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021118629956617951
0.00020462783868424594
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.4375700471317692; val_accuracy: 0.9009753184713376 

The current subspace-distance is: 0.00020462783868424594 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.95
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.95
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0002119488053722307
0.00020648210193030536
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.4353271594662575; val_accuracy: 0.8999800955414012 

The current subspace-distance is: 0.00020648210193030536 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021837811800651252
0.00020824141392949969
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.4277448553568239; val_accuracy: 0.9004777070063694 

The current subspace-distance is: 0.00020824141392949969 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021813074999954551
0.0002097684919135645
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.4244271570899684; val_accuracy: 0.9026671974522293 

The current subspace-distance is: 0.0002097684919135645 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.95
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.0002208744699601084
0.00021196114539634436
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.41631393817959317; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 0.00021196114539634436 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.95
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022191606694832444
0.0002134570386260748
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.41367167300859076; val_accuracy: 0.9009753184713376 

The current subspace-distance is: 0.0002134570386260748 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.94
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00022532566799782217
0.00021660001948475838
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4132171018867736; val_accuracy: 0.9008757961783439 

The current subspace-distance is: 0.00021660001948475838 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0002255997824249789
0.0002177445567212999
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.41644521721988725; val_accuracy: 0.901671974522293 

The current subspace-distance is: 0.0002177445567212999 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.75; acc: 0.77
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.67; acc: 0.78
Batch: 640; loss: 0.71; acc: 0.77
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00022492143034469336
0.00021650121198035777
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.41183481313240755; val_accuracy: 0.9018710191082803 

The current subspace-distance is: 0.00021650121198035777 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.81
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00022757680562790483
0.0002199532900704071
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.41255429425057333; val_accuracy: 0.900577229299363 

The current subspace-distance is: 0.0002199532900704071 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00022694733343087137
0.0002174405090045184
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4106896607928975; val_accuracy: 0.9021695859872612 

The current subspace-distance is: 0.0002174405090045184 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.62; acc: 0.78
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.98
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.98
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.81
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00022726471070200205
0.0002195644483435899
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.411438846853888; val_accuracy: 0.9015724522292994 

The current subspace-distance is: 0.0002195644483435899 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00022925504890736192
0.00022061521303839982
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4093189264179035; val_accuracy: 0.9026671974522293 

The current subspace-distance is: 0.00022061521303839982 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.95
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.32; acc: 0.98
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00023093377240002155
0.0002234584535472095
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.40391131400302716; val_accuracy: 0.902468152866242 

The current subspace-distance is: 0.0002234584535472095 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00022809994698036462
0.00022227915178518742
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.406672237879911; val_accuracy: 0.9019705414012739 

The current subspace-distance is: 0.00022227915178518742 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.81
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.78
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.00022722849098499864
0.00021986626961734146
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.4060560533195544; val_accuracy: 0.9013734076433121 

The current subspace-distance is: 0.00021986626961734146 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.98
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.0002303778164787218
0.00022106210235506296
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.40385233112581215; val_accuracy: 0.9022691082802548 

The current subspace-distance is: 0.00022106210235506296 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_8_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6

The number of parameters is: 266828

The number of individual parameters is:

13
234
13
13
20
35360
20
20
39
106080
39
39
64
119808
64
64
4096
64
640
10
64
64

nonzero elements in E: 106731190
elements in E: 106731200
fraction nonzero: 0.9999999063066844
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.4; acc: 0.11
Batch: 20; loss: 2.18; acc: 0.2
Batch: 40; loss: 1.94; acc: 0.42
Batch: 60; loss: 1.77; acc: 0.47
Batch: 80; loss: 1.61; acc: 0.64
Batch: 100; loss: 1.64; acc: 0.53
Batch: 120; loss: 1.42; acc: 0.69
Batch: 140; loss: 1.46; acc: 0.67
Batch: 160; loss: 1.54; acc: 0.66
Batch: 180; loss: 1.33; acc: 0.78
Batch: 200; loss: 1.42; acc: 0.72
Batch: 220; loss: 1.29; acc: 0.77
Batch: 240; loss: 1.31; acc: 0.73
Batch: 260; loss: 1.36; acc: 0.75
Batch: 280; loss: 1.26; acc: 0.78
Batch: 300; loss: 1.37; acc: 0.73
Batch: 320; loss: 1.23; acc: 0.8
Batch: 340; loss: 1.26; acc: 0.78
Batch: 360; loss: 1.3; acc: 0.78
Batch: 380; loss: 1.21; acc: 0.77
Batch: 400; loss: 1.22; acc: 0.73
Batch: 420; loss: 1.17; acc: 0.81
Batch: 440; loss: 1.15; acc: 0.81
Batch: 460; loss: 1.17; acc: 0.8
Batch: 480; loss: 1.13; acc: 0.84
Batch: 500; loss: 1.21; acc: 0.72
Batch: 520; loss: 1.11; acc: 0.81
Batch: 540; loss: 1.18; acc: 0.78
Batch: 560; loss: 1.01; acc: 0.84
Batch: 580; loss: 1.05; acc: 0.84
Batch: 600; loss: 0.84; acc: 0.94
Batch: 620; loss: 1.13; acc: 0.77
Batch: 640; loss: 1.06; acc: 0.75
Batch: 660; loss: 0.95; acc: 0.84
Batch: 680; loss: 1.08; acc: 0.81
Batch: 700; loss: 1.09; acc: 0.78
Batch: 720; loss: 1.04; acc: 0.78
Batch: 740; loss: 0.86; acc: 0.95
Batch: 760; loss: 1.17; acc: 0.8
Batch: 780; loss: 0.92; acc: 0.84
Train Epoch over. train_loss: 1.29; train_accuracy: 0.73 

2.6503110348130576e-05
8.331817298312671e-06
Batch: 0; loss: 0.91; acc: 0.88
Batch: 20; loss: 1.09; acc: 0.75
Batch: 40; loss: 0.67; acc: 0.97
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.92
Batch: 100; loss: 0.85; acc: 0.92
Batch: 120; loss: 1.1; acc: 0.75
Batch: 140; loss: 0.77; acc: 0.94
Val Epoch over. val_loss: 0.9292051139151215; val_accuracy: 0.8647492038216561 

The current subspace-distance is: 8.331817298312671e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.05; acc: 0.81
Batch: 20; loss: 0.91; acc: 0.89
Batch: 40; loss: 0.96; acc: 0.83
Batch: 60; loss: 1.02; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.88
Batch: 100; loss: 0.99; acc: 0.81
Batch: 120; loss: 0.85; acc: 0.88
Batch: 140; loss: 0.94; acc: 0.88
Batch: 160; loss: 0.91; acc: 0.84
Batch: 180; loss: 0.87; acc: 0.89
Batch: 200; loss: 0.91; acc: 0.8
Batch: 220; loss: 0.78; acc: 0.91
Batch: 240; loss: 0.84; acc: 0.89
Batch: 260; loss: 0.82; acc: 0.91
Batch: 280; loss: 0.8; acc: 0.88
Batch: 300; loss: 0.9; acc: 0.83
Batch: 320; loss: 0.84; acc: 0.91
Batch: 340; loss: 0.84; acc: 0.84
Batch: 360; loss: 0.8; acc: 0.94
Batch: 380; loss: 0.81; acc: 0.88
Batch: 400; loss: 0.76; acc: 0.91
Batch: 420; loss: 0.88; acc: 0.81
Batch: 440; loss: 0.91; acc: 0.84
Batch: 460; loss: 0.76; acc: 0.88
Batch: 480; loss: 0.89; acc: 0.8
Batch: 500; loss: 0.8; acc: 0.89
Batch: 520; loss: 0.87; acc: 0.83
Batch: 540; loss: 0.92; acc: 0.78
Batch: 560; loss: 0.93; acc: 0.84
Batch: 580; loss: 0.73; acc: 0.92
Batch: 600; loss: 0.86; acc: 0.81
Batch: 620; loss: 0.85; acc: 0.84
Batch: 640; loss: 0.87; acc: 0.88
Batch: 660; loss: 0.77; acc: 0.88
Batch: 680; loss: 0.76; acc: 0.84
Batch: 700; loss: 0.91; acc: 0.77
Batch: 720; loss: 0.69; acc: 0.91
Batch: 740; loss: 0.82; acc: 0.84
Batch: 760; loss: 0.82; acc: 0.78
Batch: 780; loss: 0.85; acc: 0.8
Train Epoch over. train_loss: 0.87; train_accuracy: 0.85 

3.2868625567061827e-05
1.2622146641660947e-05
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.97
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.97
Batch: 100; loss: 0.61; acc: 0.94
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.94
Val Epoch over. val_loss: 0.7004049327343133; val_accuracy: 0.8896297770700637 

The current subspace-distance is: 1.2622146641660947e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.88; acc: 0.84
Batch: 40; loss: 0.73; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.89
Batch: 80; loss: 0.79; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.92
Batch: 140; loss: 0.87; acc: 0.86
Batch: 160; loss: 0.78; acc: 0.8
Batch: 180; loss: 0.82; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.94
Batch: 220; loss: 0.71; acc: 0.89
Batch: 240; loss: 0.75; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.91
Batch: 280; loss: 0.62; acc: 0.95
Batch: 300; loss: 0.62; acc: 0.92
Batch: 320; loss: 0.74; acc: 0.84
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.89
Batch: 400; loss: 0.73; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.92
Batch: 440; loss: 0.78; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.91
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 0.55; acc: 0.98
Batch: 540; loss: 0.71; acc: 0.88
Batch: 560; loss: 0.66; acc: 0.91
Batch: 580; loss: 0.64; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.89
Batch: 620; loss: 0.77; acc: 0.8
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.92
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.92
Batch: 740; loss: 0.64; acc: 0.94
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.92
Train Epoch over. train_loss: 0.7; train_accuracy: 0.87 

3.727419243659824e-05
1.5419051123899408e-05
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.98
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.98
Val Epoch over. val_loss: 0.5803370471972569; val_accuracy: 0.898984872611465 

The current subspace-distance is: 1.5419051123899408e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.94
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.95
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.94
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.89
Batch: 220; loss: 0.63; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.91
Batch: 300; loss: 0.69; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.94
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.55; acc: 0.91
Batch: 400; loss: 0.72; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.5; acc: 0.94
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.63; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.89
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.61; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.91
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

4.1738428990356624e-05
1.8359894966124557e-05
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.97
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.5097214699171151; val_accuracy: 0.9061504777070064 

The current subspace-distance is: 1.8359894966124557e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.58; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.95
Batch: 340; loss: 0.54; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.95
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.91
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.8
Batch: 620; loss: 0.41; acc: 0.95
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.95
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.95
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.521061055129394e-05
1.941122354764957e-05
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.46122484896213384; val_accuracy: 0.9114251592356688 

The current subspace-distance is: 1.941122354764957e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.95
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.91
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.94
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.97
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.78; acc: 0.78
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

4.841325426241383e-05
1.991061435546726e-05
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.42522321451621453; val_accuracy: 0.914609872611465 

The current subspace-distance is: 1.991061435546726e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.95
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

5.1594088290585205e-05
2.400141784164589e-05
Batch: 0; loss: 0.34; acc: 0.97
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3965775318396319; val_accuracy: 0.9183917197452229 

The current subspace-distance is: 2.400141784164589e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.58; acc: 0.78
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.6; acc: 0.8
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.98
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.95
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.3872965509071946e-05
2.47329990088474e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.3754299130219563; val_accuracy: 0.9214769108280255 

The current subspace-distance is: 2.47329990088474e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.95
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

5.689746103598736e-05
2.545632014516741e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.35438333764957014; val_accuracy: 0.9227707006369427 

The current subspace-distance is: 2.545632014516741e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.97
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.33; acc: 1.0
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.7879900850821286e-05
2.5642400942160748e-05
Batch: 0; loss: 0.28; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3428971051790152; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 2.5642400942160748e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.21; acc: 1.0
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.4; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.98
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.97
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.47; acc: 0.81
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.908207094762474e-05
2.5661242034402676e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.33179756980033437; val_accuracy: 0.9260549363057324 

The current subspace-distance is: 2.5661242034402676e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.991009311401285e-05
2.6697825887822546e-05
Batch: 0; loss: 0.28; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3360339012115624; val_accuracy: 0.9252587579617835 

The current subspace-distance is: 2.6697825887822546e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.98
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.1024566093692556e-05
2.8106982426834293e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.32846989126721765; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 2.8106982426834293e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.98
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.1007791373413056e-05
2.7478743504616432e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3231528841765823; val_accuracy: 0.9278463375796179 

The current subspace-distance is: 2.7478743504616432e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.98
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.202397344168276e-05
2.9276759960339405e-05
Batch: 0; loss: 0.27; acc: 0.98
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.32386535135613886; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.9276759960339405e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.226055847946554e-05
2.7536876586964354e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.31784415928421506; val_accuracy: 0.9286425159235668 

The current subspace-distance is: 2.7536876586964354e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.21689687250182e-05
2.8549020498758182e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3138781153377454; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.8549020498758182e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.8
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.224524258868769e-05
2.68855565082049e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.30831891483372187; val_accuracy: 0.9294386942675159 

The current subspace-distance is: 2.68855565082049e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.98
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.98
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.344392022583634e-05
2.8538912374642678e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3080443640233605; val_accuracy: 0.930234872611465 

The current subspace-distance is: 2.8538912374642678e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.407366163330153e-05
2.7970338123850524e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3016915200812042; val_accuracy: 0.9308320063694268 

The current subspace-distance is: 2.7970338123850524e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.97
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.44557730993256e-05
3.103810013271868e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3051196276477188; val_accuracy: 0.931031050955414 

The current subspace-distance is: 3.103810013271868e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.98
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.500930612673983e-05
2.9043714675935917e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.30355225493953486; val_accuracy: 0.931031050955414 

The current subspace-distance is: 2.9043714675935917e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.97
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.98
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.50502770440653e-05
3.106402436969802e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.29787077042897037; val_accuracy: 0.9313296178343949 

The current subspace-distance is: 3.106402436969802e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.400312850018963e-05
2.873725679819472e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3041794728131811; val_accuracy: 0.9307324840764332 

The current subspace-distance is: 2.873725679819472e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.479534204117954e-05
2.8042624762747437e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.29651785447339346; val_accuracy: 0.9316281847133758 

The current subspace-distance is: 2.8042624762747437e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.98
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.98
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.449293141486123e-05
2.9796199669362977e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2989556866276796; val_accuracy: 0.9319267515923567 

The current subspace-distance is: 2.9796199669362977e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.97
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.511150422738865e-05
2.8555987228173763e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.29820481535925225; val_accuracy: 0.9324243630573248 

The current subspace-distance is: 2.8555987228173763e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.98
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.2; acc: 1.0
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.551671685883775e-05
2.9416667530313134e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.2989135695395956; val_accuracy: 0.9322253184713376 

The current subspace-distance is: 2.9416667530313134e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.97
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.98
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.481779564637691e-05
2.990144639625214e-05
Batch: 0; loss: 0.23; acc: 0.98
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.29673150248208624; val_accuracy: 0.9322253184713376 

The current subspace-distance is: 2.990144639625214e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.98
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.22; acc: 1.0
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.513670814456418e-05
2.9727047149208374e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.29586972082686275; val_accuracy: 0.9316281847133758 

The current subspace-distance is: 2.9727047149208374e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_8_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6

The number of parameters is: 266828

The number of individual parameters is:

13
234
13
13
20
35360
20
20
39
106080
39
39
64
119808
64
64
4096
64
640
10
64
64

nonzero elements in E: 133413990
elements in E: 133414000
fraction nonzero: 0.9999999250453475
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.47; acc: 0.12
Batch: 20; loss: 2.09; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.56
Batch: 60; loss: 1.56; acc: 0.67
Batch: 80; loss: 1.46; acc: 0.73
Batch: 100; loss: 1.4; acc: 0.77
Batch: 120; loss: 1.35; acc: 0.8
Batch: 140; loss: 1.33; acc: 0.73
Batch: 160; loss: 1.51; acc: 0.64
Batch: 180; loss: 1.44; acc: 0.64
Batch: 200; loss: 1.29; acc: 0.73
Batch: 220; loss: 1.17; acc: 0.8
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.24; acc: 0.81
Batch: 280; loss: 1.13; acc: 0.81
Batch: 300; loss: 1.16; acc: 0.72
Batch: 320; loss: 1.03; acc: 0.89
Batch: 340; loss: 1.16; acc: 0.72
Batch: 360; loss: 1.07; acc: 0.84
Batch: 380; loss: 0.98; acc: 0.83
Batch: 400; loss: 1.04; acc: 0.8
Batch: 420; loss: 1.09; acc: 0.77
Batch: 440; loss: 1.0; acc: 0.88
Batch: 460; loss: 1.04; acc: 0.8
Batch: 480; loss: 1.06; acc: 0.88
Batch: 500; loss: 0.93; acc: 0.84
Batch: 520; loss: 0.94; acc: 0.88
Batch: 540; loss: 1.0; acc: 0.78
Batch: 560; loss: 0.9; acc: 0.86
Batch: 580; loss: 0.86; acc: 0.86
Batch: 600; loss: 0.8; acc: 0.89
Batch: 620; loss: 0.85; acc: 0.88
Batch: 640; loss: 0.89; acc: 0.83
Batch: 660; loss: 0.95; acc: 0.84
Batch: 680; loss: 0.79; acc: 0.89
Batch: 700; loss: 0.82; acc: 0.86
Batch: 720; loss: 0.98; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.91
Batch: 760; loss: 0.82; acc: 0.91
Batch: 780; loss: 0.87; acc: 0.89
Train Epoch over. train_loss: 1.14; train_accuracy: 0.78 

2.7154514100402594e-05
9.330014108854812e-06
Batch: 0; loss: 0.78; acc: 0.91
Batch: 20; loss: 0.96; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.95
Batch: 60; loss: 0.81; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.94
Batch: 100; loss: 0.79; acc: 0.91
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 0.61; acc: 0.94
Val Epoch over. val_loss: 0.7713689075154104; val_accuracy: 0.8902269108280255 

The current subspace-distance is: 9.330014108854812e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.92
Batch: 40; loss: 0.74; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.89
Batch: 80; loss: 0.77; acc: 0.94
Batch: 100; loss: 0.69; acc: 0.94
Batch: 120; loss: 0.85; acc: 0.88
Batch: 140; loss: 0.82; acc: 0.91
Batch: 160; loss: 0.81; acc: 0.92
Batch: 180; loss: 0.87; acc: 0.83
Batch: 200; loss: 0.69; acc: 0.89
Batch: 220; loss: 0.68; acc: 0.91
Batch: 240; loss: 0.71; acc: 0.92
Batch: 260; loss: 0.7; acc: 0.88
Batch: 280; loss: 0.75; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.95
Batch: 340; loss: 0.71; acc: 0.88
Batch: 360; loss: 0.93; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.91
Batch: 440; loss: 0.79; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.91
Batch: 480; loss: 0.77; acc: 0.83
Batch: 500; loss: 0.68; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.94
Batch: 540; loss: 0.68; acc: 0.92
Batch: 560; loss: 0.58; acc: 0.97
Batch: 580; loss: 0.64; acc: 0.91
Batch: 600; loss: 0.7; acc: 0.89
Batch: 620; loss: 0.64; acc: 0.95
Batch: 640; loss: 1.0; acc: 0.72
Batch: 660; loss: 0.74; acc: 0.86
Batch: 680; loss: 0.76; acc: 0.88
Batch: 700; loss: 0.77; acc: 0.86
Batch: 720; loss: 0.6; acc: 1.0
Batch: 740; loss: 0.72; acc: 0.86
Batch: 760; loss: 0.62; acc: 0.92
Batch: 780; loss: 0.73; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.88 

3.21516054100357e-05
1.1884988452948164e-05
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.63; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.43; acc: 0.97
Val Epoch over. val_loss: 0.6120902225849735; val_accuracy: 0.9045581210191083 

The current subspace-distance is: 1.1884988452948164e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.98
Batch: 20; loss: 0.56; acc: 0.97
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.97
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.95
Batch: 160; loss: 0.68; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.78; acc: 0.91
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.74; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.73; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.86
Batch: 340; loss: 0.69; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.98
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.61; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.92
Batch: 480; loss: 0.59; acc: 0.94
Batch: 500; loss: 0.55; acc: 0.92
Batch: 520; loss: 0.71; acc: 0.89
Batch: 540; loss: 0.78; acc: 0.83
Batch: 560; loss: 0.52; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.91
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.91
Batch: 680; loss: 0.61; acc: 0.92
Batch: 700; loss: 0.65; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.92
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.63; train_accuracy: 0.89 

3.7489706301130354e-05
1.551658715470694e-05
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5228191494562064; val_accuracy: 0.9152070063694268 

The current subspace-distance is: 1.551658715470694e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.97
Batch: 20; loss: 0.57; acc: 0.94
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.89
Batch: 380; loss: 0.58; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.94
Batch: 420; loss: 0.61; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.94
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.94
Batch: 720; loss: 0.58; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.9 

4.0049348172033206e-05
1.6458587197121233e-05
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.4469746952034106; val_accuracy: 0.9234673566878981 

The current subspace-distance is: 1.6458587197121233e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.95
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.95
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.97
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.95
Batch: 260; loss: 0.52; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.36; acc: 1.0
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.97
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.91 

4.3317140807630494e-05
1.775825148797594e-05
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.4063524769939435; val_accuracy: 0.9258558917197452 

The current subspace-distance is: 1.775825148797594e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.48; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.97
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.97
Batch: 680; loss: 0.36; acc: 0.95
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.97
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.98
Train Epoch over. train_loss: 0.45; train_accuracy: 0.91 

4.5984972530277446e-05
1.961306406883523e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.37692905516381475; val_accuracy: 0.9296377388535032 

The current subspace-distance is: 1.961306406883523e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.98
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.98
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.780510062118992e-05
2.089868394250516e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3555479666609673; val_accuracy: 0.9298367834394905 

The current subspace-distance is: 2.089868394250516e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.98
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.98
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.97
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.35; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.92 

5.044386489316821e-05
2.0681112800957635e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3311203413518371; val_accuracy: 0.9334195859872612 

The current subspace-distance is: 2.0681112800957635e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.97
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.309765037964098e-05
2.320954627066385e-05
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3225109536841417; val_accuracy: 0.9316281847133758 

The current subspace-distance is: 2.320954627066385e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.98
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.445936039905064e-05
2.3727365260128863e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.30210780442520313; val_accuracy: 0.9352109872611465 

The current subspace-distance is: 2.3727365260128863e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.25; acc: 0.97
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.6125150877051055e-05
2.3747070372337475e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3040559254823976; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.3747070372337475e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.98
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.98
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.98
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.599736687145196e-05
2.5415074560442008e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.29763908138510525; val_accuracy: 0.9339171974522293 

The current subspace-distance is: 2.5415074560442008e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.98
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.22; acc: 0.98
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.98
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.722699279431254e-05
2.5504990844638087e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.29632210593884156; val_accuracy: 0.9353105095541401 

The current subspace-distance is: 2.5504990844638087e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.98
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.98
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.70837473787833e-05
2.447749830025714e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.29112046098063704; val_accuracy: 0.934812898089172 

The current subspace-distance is: 2.447749830025714e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.18; acc: 1.0
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.97
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.761103966506198e-05
2.4913015295169316e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2842340745553849; val_accuracy: 0.9353105095541401 

The current subspace-distance is: 2.4913015295169316e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.17; acc: 1.0
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.7837296481011435e-05
2.5036164515768178e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2834498365498652; val_accuracy: 0.9357085987261147 

The current subspace-distance is: 2.5036164515768178e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.24; acc: 1.0
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.47; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.97
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.98
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.899069219594821e-05
2.6930005333269946e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2836669118730885; val_accuracy: 0.9370023885350318 

The current subspace-distance is: 2.6930005333269946e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.24; acc: 0.98
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.98
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.93 

5.884715938009322e-05
2.561794281064067e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2799244474643355; val_accuracy: 0.9374004777070064 

The current subspace-distance is: 2.561794281064067e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.98
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.15; acc: 1.0
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.954390871920623e-05
2.67160648945719e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.27492426231408573; val_accuracy: 0.9370023885350318 

The current subspace-distance is: 2.67160648945719e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.982658694847487e-05
2.6310226530767977e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.27160730250891607; val_accuracy: 0.9381966560509554 

The current subspace-distance is: 2.6310226530767977e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.98
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.98
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.980547939543612e-05
2.6132278435397893e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2762559167803473; val_accuracy: 0.9386942675159236 

The current subspace-distance is: 2.6132278435397893e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.98
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.98
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.17; acc: 1.0
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.956673339824192e-05
2.670702087925747e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.269863432712236; val_accuracy: 0.9374004777070064 

The current subspace-distance is: 2.670702087925747e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.98
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.0362002841429785e-05
2.791830411297269e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.27424129829475075; val_accuracy: 0.9375 

The current subspace-distance is: 2.791830411297269e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.98
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.97
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.98
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.024168760632165e-05
2.6926316422759555e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.26942180244216496; val_accuracy: 0.9379976114649682 

The current subspace-distance is: 2.6926316422759555e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.98
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.18; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.0824972024420276e-05
2.8522690627141856e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.2738380225221063; val_accuracy: 0.9381966560509554 

The current subspace-distance is: 2.8522690627141856e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.074848715797998e-05
2.684761057025753e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.27043702804548725; val_accuracy: 0.9370023885350318 

The current subspace-distance is: 2.684761057025753e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.101750477682799e-05
2.7411148039391264e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2700561080958433; val_accuracy: 0.9379976114649682 

The current subspace-distance is: 2.7411148039391264e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.97
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.136328011052683e-05
2.7686723115039058e-05
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.26525139514428037; val_accuracy: 0.9387937898089171 

The current subspace-distance is: 2.7686723115039058e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.98
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.98
Batch: 280; loss: 0.22; acc: 0.98
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.148864486021921e-05
2.8174181352369487e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.26473420606866765; val_accuracy: 0.9384952229299363 

The current subspace-distance is: 2.8174181352369487e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.22; acc: 1.0
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.58; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.1003913288004696e-05
2.7575742933549918e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.26535275706629846; val_accuracy: 0.9387937898089171 

The current subspace-distance is: 2.7575742933549918e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:57/N_8_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:57/N_8_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
