model : table13slim
N : 16
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:47

Channel scaling factor: 1.51657508881031

The number of parameters is: 267435

The number of individual parameters is:

13
234
13
13
19
38532
19
19
37
109668
37
37
64
113664
64
64
4096
64
640
10
64
64

nonzero elements in E: 13371748
elements in E: 13371750
fraction nonzero: 0.9999998504309459
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.49; acc: 0.03
Batch: 20; loss: 2.38; acc: 0.09
Batch: 40; loss: 2.21; acc: 0.28
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.21; acc: 0.22
Batch: 100; loss: 2.15; acc: 0.28
Batch: 120; loss: 2.16; acc: 0.22
Batch: 140; loss: 2.08; acc: 0.27
Batch: 160; loss: 2.05; acc: 0.31
Batch: 180; loss: 1.98; acc: 0.34
Batch: 200; loss: 2.1; acc: 0.28
Batch: 220; loss: 2.0; acc: 0.36
Batch: 240; loss: 2.09; acc: 0.31
Batch: 260; loss: 2.1; acc: 0.2
Batch: 280; loss: 2.06; acc: 0.39
Batch: 300; loss: 1.98; acc: 0.39
Batch: 320; loss: 1.97; acc: 0.39
Batch: 340; loss: 2.01; acc: 0.33
Batch: 360; loss: 2.09; acc: 0.34
Batch: 380; loss: 2.09; acc: 0.23
Batch: 400; loss: 1.95; acc: 0.44
Batch: 420; loss: 1.96; acc: 0.47
Batch: 440; loss: 1.92; acc: 0.47
Batch: 460; loss: 1.91; acc: 0.47
Batch: 480; loss: 2.09; acc: 0.34
Batch: 500; loss: 1.99; acc: 0.41
Batch: 520; loss: 1.87; acc: 0.47
Batch: 540; loss: 1.97; acc: 0.36
Batch: 560; loss: 1.9; acc: 0.47
Batch: 580; loss: 1.83; acc: 0.58
Batch: 600; loss: 1.92; acc: 0.47
Batch: 620; loss: 1.95; acc: 0.31
Batch: 640; loss: 2.03; acc: 0.3
Batch: 660; loss: 1.93; acc: 0.41
Batch: 680; loss: 2.09; acc: 0.27
Batch: 700; loss: 1.9; acc: 0.44
Batch: 720; loss: 1.96; acc: 0.42
Batch: 740; loss: 1.88; acc: 0.48
Batch: 760; loss: 1.9; acc: 0.41
Batch: 780; loss: 1.89; acc: 0.39
Train Epoch over. train_loss: 2.04; train_accuracy: 0.33 

2.3069967937772162e-05
4.090037236892385e-06
Batch: 0; loss: 2.02; acc: 0.22
Batch: 20; loss: 2.05; acc: 0.36
Batch: 40; loss: 1.82; acc: 0.55
Batch: 60; loss: 1.95; acc: 0.39
Batch: 80; loss: 1.85; acc: 0.48
Batch: 100; loss: 1.93; acc: 0.48
Batch: 120; loss: 2.08; acc: 0.34
Batch: 140; loss: 1.79; acc: 0.64
Val Epoch over. val_loss: 1.910588897717227; val_accuracy: 0.43799761146496813 

The current subspace-distance is: 4.090037236892385e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.98; acc: 0.36
Batch: 20; loss: 1.84; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.9; acc: 0.44
Batch: 80; loss: 1.98; acc: 0.39
Batch: 100; loss: 1.83; acc: 0.47
Batch: 120; loss: 1.9; acc: 0.5
Batch: 140; loss: 1.93; acc: 0.47
Batch: 160; loss: 1.97; acc: 0.39
Batch: 180; loss: 1.84; acc: 0.47
Batch: 200; loss: 1.93; acc: 0.38
Batch: 220; loss: 1.95; acc: 0.42
Batch: 240; loss: 1.87; acc: 0.5
Batch: 260; loss: 1.79; acc: 0.48
Batch: 280; loss: 1.92; acc: 0.38
Batch: 300; loss: 1.94; acc: 0.41
Batch: 320; loss: 1.91; acc: 0.39
Batch: 340; loss: 1.9; acc: 0.45
Batch: 360; loss: 1.91; acc: 0.41
Batch: 380; loss: 1.83; acc: 0.39
Batch: 400; loss: 1.9; acc: 0.5
Batch: 420; loss: 1.96; acc: 0.36
Batch: 440; loss: 1.88; acc: 0.42
Batch: 460; loss: 1.8; acc: 0.53
Batch: 480; loss: 1.82; acc: 0.42
Batch: 500; loss: 1.91; acc: 0.36
Batch: 520; loss: 1.85; acc: 0.42
Batch: 540; loss: 1.83; acc: 0.48
Batch: 560; loss: 1.85; acc: 0.45
Batch: 580; loss: 1.92; acc: 0.41
Batch: 600; loss: 1.87; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.55
Batch: 640; loss: 1.88; acc: 0.44
Batch: 660; loss: 1.78; acc: 0.5
Batch: 680; loss: 1.81; acc: 0.44
Batch: 700; loss: 1.93; acc: 0.41
Batch: 720; loss: 1.87; acc: 0.47
Batch: 740; loss: 1.88; acc: 0.41
Batch: 760; loss: 1.89; acc: 0.41
Batch: 780; loss: 1.78; acc: 0.55
Train Epoch over. train_loss: 1.88; train_accuracy: 0.44 

2.5727962565724738e-05
7.46448358768248e-06
Batch: 0; loss: 1.85; acc: 0.39
Batch: 20; loss: 2.02; acc: 0.28
Batch: 40; loss: 1.69; acc: 0.61
Batch: 60; loss: 1.8; acc: 0.5
Batch: 80; loss: 1.73; acc: 0.48
Batch: 100; loss: 1.81; acc: 0.52
Batch: 120; loss: 2.01; acc: 0.33
Batch: 140; loss: 1.7; acc: 0.59
Val Epoch over. val_loss: 1.7884079119202438; val_accuracy: 0.5005971337579618 

The current subspace-distance is: 7.46448358768248e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.5
Batch: 20; loss: 1.86; acc: 0.39
Batch: 40; loss: 1.85; acc: 0.41
Batch: 60; loss: 1.71; acc: 0.53
Batch: 80; loss: 1.81; acc: 0.47
Batch: 100; loss: 1.78; acc: 0.5
Batch: 120; loss: 1.83; acc: 0.47
Batch: 140; loss: 2.03; acc: 0.36
Batch: 160; loss: 1.84; acc: 0.45
Batch: 180; loss: 1.78; acc: 0.48
Batch: 200; loss: 1.8; acc: 0.45
Batch: 220; loss: 1.84; acc: 0.42
Batch: 240; loss: 1.87; acc: 0.39
Batch: 260; loss: 1.91; acc: 0.39
Batch: 280; loss: 1.73; acc: 0.42
Batch: 300; loss: 1.78; acc: 0.52
Batch: 320; loss: 1.88; acc: 0.38
Batch: 340; loss: 1.79; acc: 0.5
Batch: 360; loss: 1.77; acc: 0.47
Batch: 380; loss: 1.65; acc: 0.56
Batch: 400; loss: 1.7; acc: 0.45
Batch: 420; loss: 1.76; acc: 0.38
Batch: 440; loss: 1.69; acc: 0.56
Batch: 460; loss: 1.69; acc: 0.56
Batch: 480; loss: 1.76; acc: 0.5
Batch: 500; loss: 1.76; acc: 0.53
Batch: 520; loss: 1.73; acc: 0.52
Batch: 540; loss: 1.93; acc: 0.38
Batch: 560; loss: 1.78; acc: 0.47
Batch: 580; loss: 1.85; acc: 0.38
Batch: 600; loss: 1.63; acc: 0.59
Batch: 620; loss: 1.86; acc: 0.45
Batch: 640; loss: 1.71; acc: 0.44
Batch: 660; loss: 1.69; acc: 0.48
Batch: 680; loss: 1.83; acc: 0.36
Batch: 700; loss: 1.68; acc: 0.52
Batch: 720; loss: 1.73; acc: 0.5
Batch: 740; loss: 1.7; acc: 0.5
Batch: 760; loss: 1.68; acc: 0.47
Batch: 780; loss: 1.73; acc: 0.5
Train Epoch over. train_loss: 1.76; train_accuracy: 0.48 

2.781531475193333e-05
5.838080141984392e-06
Batch: 0; loss: 1.68; acc: 0.58
Batch: 20; loss: 1.9; acc: 0.34
Batch: 40; loss: 1.54; acc: 0.67
Batch: 60; loss: 1.7; acc: 0.62
Batch: 80; loss: 1.63; acc: 0.5
Batch: 100; loss: 1.73; acc: 0.55
Batch: 120; loss: 1.87; acc: 0.44
Batch: 140; loss: 1.66; acc: 0.59
Val Epoch over. val_loss: 1.6864470790146262; val_accuracy: 0.5335390127388535 

The current subspace-distance is: 5.838080141984392e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.55
Batch: 20; loss: 1.63; acc: 0.62
Batch: 40; loss: 1.73; acc: 0.53
Batch: 60; loss: 1.69; acc: 0.45
Batch: 80; loss: 1.74; acc: 0.5
Batch: 100; loss: 1.87; acc: 0.44
Batch: 120; loss: 1.84; acc: 0.48
Batch: 140; loss: 1.65; acc: 0.52
Batch: 160; loss: 1.64; acc: 0.53
Batch: 180; loss: 1.7; acc: 0.5
Batch: 200; loss: 1.71; acc: 0.45
Batch: 220; loss: 1.66; acc: 0.52
Batch: 240; loss: 1.81; acc: 0.44
Batch: 260; loss: 1.69; acc: 0.48
Batch: 280; loss: 1.71; acc: 0.5
Batch: 300; loss: 1.6; acc: 0.58
Batch: 320; loss: 1.66; acc: 0.52
Batch: 340; loss: 1.67; acc: 0.58
Batch: 360; loss: 1.67; acc: 0.5
Batch: 380; loss: 1.62; acc: 0.58
Batch: 400; loss: 1.87; acc: 0.44
Batch: 420; loss: 1.77; acc: 0.5
Batch: 440; loss: 1.62; acc: 0.58
Batch: 460; loss: 1.69; acc: 0.53
Batch: 480; loss: 1.55; acc: 0.52
Batch: 500; loss: 1.77; acc: 0.5
Batch: 520; loss: 1.69; acc: 0.53
Batch: 540; loss: 1.66; acc: 0.45
Batch: 560; loss: 1.57; acc: 0.58
Batch: 580; loss: 1.66; acc: 0.47
Batch: 600; loss: 1.78; acc: 0.44
Batch: 620; loss: 1.71; acc: 0.5
Batch: 640; loss: 1.69; acc: 0.45
Batch: 660; loss: 1.63; acc: 0.5
Batch: 680; loss: 1.74; acc: 0.47
Batch: 700; loss: 1.78; acc: 0.42
Batch: 720; loss: 1.7; acc: 0.53
Batch: 740; loss: 1.62; acc: 0.55
Batch: 760; loss: 1.59; acc: 0.58
Batch: 780; loss: 1.73; acc: 0.44
Train Epoch over. train_loss: 1.68; train_accuracy: 0.51 

3.083394403802231e-05
7.77861805545399e-06
Batch: 0; loss: 1.61; acc: 0.62
Batch: 20; loss: 1.85; acc: 0.39
Batch: 40; loss: 1.45; acc: 0.67
Batch: 60; loss: 1.63; acc: 0.62
Batch: 80; loss: 1.57; acc: 0.5
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.76; acc: 0.47
Batch: 140; loss: 1.61; acc: 0.59
Val Epoch over. val_loss: 1.62462200860309; val_accuracy: 0.5338375796178344 

The current subspace-distance is: 7.77861805545399e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.52
Batch: 20; loss: 1.74; acc: 0.45
Batch: 40; loss: 1.72; acc: 0.48
Batch: 60; loss: 1.67; acc: 0.47
Batch: 80; loss: 1.79; acc: 0.45
Batch: 100; loss: 1.6; acc: 0.58
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.6; acc: 0.55
Batch: 160; loss: 1.55; acc: 0.55
Batch: 180; loss: 1.61; acc: 0.55
Batch: 200; loss: 1.61; acc: 0.55
Batch: 220; loss: 1.75; acc: 0.42
Batch: 240; loss: 1.7; acc: 0.5
Batch: 260; loss: 1.66; acc: 0.56
Batch: 280; loss: 1.74; acc: 0.48
Batch: 300; loss: 1.62; acc: 0.56
Batch: 320; loss: 1.67; acc: 0.47
Batch: 340; loss: 1.6; acc: 0.53
Batch: 360; loss: 1.61; acc: 0.5
Batch: 380; loss: 1.6; acc: 0.55
Batch: 400; loss: 1.64; acc: 0.47
Batch: 420; loss: 1.54; acc: 0.62
Batch: 440; loss: 1.74; acc: 0.48
Batch: 460; loss: 1.77; acc: 0.36
Batch: 480; loss: 1.65; acc: 0.48
Batch: 500; loss: 1.53; acc: 0.66
Batch: 520; loss: 1.68; acc: 0.5
Batch: 540; loss: 1.66; acc: 0.59
Batch: 560; loss: 1.63; acc: 0.55
Batch: 580; loss: 1.62; acc: 0.44
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.51; acc: 0.64
Batch: 640; loss: 1.72; acc: 0.53
Batch: 660; loss: 1.73; acc: 0.33
Batch: 680; loss: 1.56; acc: 0.62
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.58; acc: 0.48
Batch: 740; loss: 1.76; acc: 0.45
Batch: 760; loss: 1.53; acc: 0.56
Batch: 780; loss: 1.55; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.52 

3.3489843190182e-05
9.558265446685255e-06
Batch: 0; loss: 1.56; acc: 0.61
Batch: 20; loss: 1.78; acc: 0.48
Batch: 40; loss: 1.38; acc: 0.66
Batch: 60; loss: 1.59; acc: 0.61
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.57; acc: 0.61
Val Epoch over. val_loss: 1.5878710959367692; val_accuracy: 0.5478702229299363 

The current subspace-distance is: 9.558265446685255e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.59
Batch: 20; loss: 1.78; acc: 0.42
Batch: 40; loss: 1.47; acc: 0.56
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.58; acc: 0.52
Batch: 100; loss: 1.73; acc: 0.41
Batch: 120; loss: 1.65; acc: 0.45
Batch: 140; loss: 1.49; acc: 0.56
Batch: 160; loss: 1.71; acc: 0.48
Batch: 180; loss: 1.68; acc: 0.44
Batch: 200; loss: 1.65; acc: 0.55
Batch: 220; loss: 1.62; acc: 0.53
Batch: 240; loss: 1.6; acc: 0.53
Batch: 260; loss: 1.59; acc: 0.58
Batch: 280; loss: 1.67; acc: 0.5
Batch: 300; loss: 1.58; acc: 0.45
Batch: 320; loss: 1.7; acc: 0.45
Batch: 340; loss: 1.73; acc: 0.45
Batch: 360; loss: 1.51; acc: 0.59
Batch: 380; loss: 1.67; acc: 0.52
Batch: 400; loss: 1.66; acc: 0.47
Batch: 420; loss: 1.61; acc: 0.55
Batch: 440; loss: 1.63; acc: 0.56
Batch: 460; loss: 1.61; acc: 0.59
Batch: 480; loss: 1.68; acc: 0.53
Batch: 500; loss: 1.6; acc: 0.52
Batch: 520; loss: 1.67; acc: 0.47
Batch: 540; loss: 1.5; acc: 0.59
Batch: 560; loss: 1.71; acc: 0.41
Batch: 580; loss: 1.81; acc: 0.38
Batch: 600; loss: 1.59; acc: 0.52
Batch: 620; loss: 1.67; acc: 0.45
Batch: 640; loss: 1.54; acc: 0.59
Batch: 660; loss: 1.66; acc: 0.45
Batch: 680; loss: 1.49; acc: 0.59
Batch: 700; loss: 1.59; acc: 0.41
Batch: 720; loss: 1.62; acc: 0.5
Batch: 740; loss: 1.6; acc: 0.52
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.5; acc: 0.59
Train Epoch over. train_loss: 1.62; train_accuracy: 0.52 

3.4915086871478707e-05
9.610611414245795e-06
Batch: 0; loss: 1.54; acc: 0.62
Batch: 20; loss: 1.73; acc: 0.5
Batch: 40; loss: 1.36; acc: 0.69
Batch: 60; loss: 1.55; acc: 0.61
Batch: 80; loss: 1.54; acc: 0.61
Batch: 100; loss: 1.63; acc: 0.53
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.56; acc: 0.58
Val Epoch over. val_loss: 1.5710717553545714; val_accuracy: 0.5550358280254777 

The current subspace-distance is: 9.610611414245795e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.71; acc: 0.48
Batch: 40; loss: 1.68; acc: 0.45
Batch: 60; loss: 1.68; acc: 0.42
Batch: 80; loss: 1.63; acc: 0.47
Batch: 100; loss: 1.62; acc: 0.5
Batch: 120; loss: 1.55; acc: 0.61
Batch: 140; loss: 1.56; acc: 0.58
Batch: 160; loss: 1.68; acc: 0.47
Batch: 180; loss: 1.64; acc: 0.45
Batch: 200; loss: 1.7; acc: 0.47
Batch: 220; loss: 1.63; acc: 0.53
Batch: 240; loss: 1.51; acc: 0.56
Batch: 260; loss: 1.8; acc: 0.44
Batch: 280; loss: 1.68; acc: 0.41
Batch: 300; loss: 1.59; acc: 0.52
Batch: 320; loss: 1.69; acc: 0.48
Batch: 340; loss: 1.66; acc: 0.48
Batch: 360; loss: 1.72; acc: 0.48
Batch: 380; loss: 1.42; acc: 0.67
Batch: 400; loss: 1.48; acc: 0.58
Batch: 420; loss: 1.59; acc: 0.53
Batch: 440; loss: 1.47; acc: 0.62
Batch: 460; loss: 1.51; acc: 0.59
Batch: 480; loss: 1.6; acc: 0.55
Batch: 500; loss: 1.56; acc: 0.58
Batch: 520; loss: 1.54; acc: 0.61
Batch: 540; loss: 1.55; acc: 0.5
Batch: 560; loss: 1.73; acc: 0.42
Batch: 580; loss: 1.59; acc: 0.48
Batch: 600; loss: 1.58; acc: 0.58
Batch: 620; loss: 1.49; acc: 0.61
Batch: 640; loss: 1.58; acc: 0.47
Batch: 660; loss: 1.68; acc: 0.48
Batch: 680; loss: 1.47; acc: 0.58
Batch: 700; loss: 1.59; acc: 0.52
Batch: 720; loss: 1.66; acc: 0.44
Batch: 740; loss: 1.67; acc: 0.52
Batch: 760; loss: 1.56; acc: 0.52
Batch: 780; loss: 1.64; acc: 0.55
Train Epoch over. train_loss: 1.59; train_accuracy: 0.53 

3.809352347161621e-05
1.3339458746486343e-05
Batch: 0; loss: 1.51; acc: 0.64
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.33; acc: 0.72
Batch: 60; loss: 1.51; acc: 0.61
Batch: 80; loss: 1.52; acc: 0.52
Batch: 100; loss: 1.57; acc: 0.56
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.55; acc: 0.59
Val Epoch over. val_loss: 1.5437237859531572; val_accuracy: 0.5571257961783439 

The current subspace-distance is: 1.3339458746486343e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.5; acc: 0.56
Batch: 20; loss: 1.78; acc: 0.39
Batch: 40; loss: 1.54; acc: 0.56
Batch: 60; loss: 1.61; acc: 0.47
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.69; acc: 0.48
Batch: 120; loss: 1.51; acc: 0.53
Batch: 140; loss: 1.7; acc: 0.48
Batch: 160; loss: 1.55; acc: 0.56
Batch: 180; loss: 1.61; acc: 0.45
Batch: 200; loss: 1.65; acc: 0.48
Batch: 220; loss: 1.48; acc: 0.58
Batch: 240; loss: 1.53; acc: 0.55
Batch: 260; loss: 1.62; acc: 0.47
Batch: 280; loss: 1.58; acc: 0.5
Batch: 300; loss: 1.59; acc: 0.55
Batch: 320; loss: 1.39; acc: 0.62
Batch: 340; loss: 1.58; acc: 0.5
Batch: 360; loss: 1.59; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.52
Batch: 400; loss: 1.52; acc: 0.55
Batch: 420; loss: 1.54; acc: 0.53
Batch: 440; loss: 1.61; acc: 0.48
Batch: 460; loss: 1.53; acc: 0.56
Batch: 480; loss: 1.36; acc: 0.72
Batch: 500; loss: 1.48; acc: 0.62
Batch: 520; loss: 1.55; acc: 0.55
Batch: 540; loss: 1.49; acc: 0.66
Batch: 560; loss: 1.68; acc: 0.5
Batch: 580; loss: 1.47; acc: 0.67
Batch: 600; loss: 1.5; acc: 0.59
Batch: 620; loss: 1.58; acc: 0.61
Batch: 640; loss: 1.67; acc: 0.55
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.52; acc: 0.62
Batch: 700; loss: 1.66; acc: 0.5
Batch: 720; loss: 1.56; acc: 0.55
Batch: 740; loss: 1.49; acc: 0.69
Batch: 760; loss: 1.68; acc: 0.47
Batch: 780; loss: 1.7; acc: 0.45
Train Epoch over. train_loss: 1.57; train_accuracy: 0.53 

3.890192965627648e-05
1.1615113180596381e-05
Batch: 0; loss: 1.48; acc: 0.67
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.29; acc: 0.7
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.49; acc: 0.53
Batch: 100; loss: 1.54; acc: 0.61
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.51; acc: 0.59
Val Epoch over. val_loss: 1.5213076506450678; val_accuracy: 0.5657842356687898 

The current subspace-distance is: 1.1615113180596381e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.48; acc: 0.61
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.51; acc: 0.56
Batch: 60; loss: 1.73; acc: 0.42
Batch: 80; loss: 1.54; acc: 0.5
Batch: 100; loss: 1.64; acc: 0.42
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.49; acc: 0.56
Batch: 160; loss: 1.49; acc: 0.58
Batch: 180; loss: 1.57; acc: 0.5
Batch: 200; loss: 1.52; acc: 0.58
Batch: 220; loss: 1.57; acc: 0.56
Batch: 240; loss: 1.65; acc: 0.45
Batch: 260; loss: 1.61; acc: 0.52
Batch: 280; loss: 1.69; acc: 0.38
Batch: 300; loss: 1.6; acc: 0.55
Batch: 320; loss: 1.55; acc: 0.48
Batch: 340; loss: 1.62; acc: 0.47
Batch: 360; loss: 1.72; acc: 0.45
Batch: 380; loss: 1.48; acc: 0.58
Batch: 400; loss: 1.42; acc: 0.7
Batch: 420; loss: 1.56; acc: 0.53
Batch: 440; loss: 1.51; acc: 0.56
Batch: 460; loss: 1.43; acc: 0.64
Batch: 480; loss: 1.5; acc: 0.58
Batch: 500; loss: 1.66; acc: 0.44
Batch: 520; loss: 1.57; acc: 0.56
Batch: 540; loss: 1.63; acc: 0.5
Batch: 560; loss: 1.51; acc: 0.5
Batch: 580; loss: 1.42; acc: 0.62
Batch: 600; loss: 1.43; acc: 0.59
Batch: 620; loss: 1.61; acc: 0.53
Batch: 640; loss: 1.52; acc: 0.52
Batch: 660; loss: 1.47; acc: 0.59
Batch: 680; loss: 1.56; acc: 0.52
Batch: 700; loss: 1.61; acc: 0.52
Batch: 720; loss: 1.51; acc: 0.56
Batch: 740; loss: 1.49; acc: 0.59
Batch: 760; loss: 1.56; acc: 0.52
Batch: 780; loss: 1.51; acc: 0.53
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.170654574409127e-05
1.4942368579795584e-05
Batch: 0; loss: 1.47; acc: 0.62
Batch: 20; loss: 1.62; acc: 0.55
Batch: 40; loss: 1.28; acc: 0.69
Batch: 60; loss: 1.43; acc: 0.64
Batch: 80; loss: 1.49; acc: 0.52
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.57; acc: 0.52
Batch: 140; loss: 1.48; acc: 0.66
Val Epoch over. val_loss: 1.5112188910223117; val_accuracy: 0.5637937898089171 

The current subspace-distance is: 1.4942368579795584e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.52
Batch: 20; loss: 1.53; acc: 0.56
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 1.54; acc: 0.55
Batch: 80; loss: 1.71; acc: 0.48
Batch: 100; loss: 1.66; acc: 0.42
Batch: 120; loss: 1.4; acc: 0.56
Batch: 140; loss: 1.55; acc: 0.64
Batch: 160; loss: 1.53; acc: 0.48
Batch: 180; loss: 1.4; acc: 0.59
Batch: 200; loss: 1.6; acc: 0.52
Batch: 220; loss: 1.65; acc: 0.55
Batch: 240; loss: 1.57; acc: 0.45
Batch: 260; loss: 1.52; acc: 0.56
Batch: 280; loss: 1.5; acc: 0.58
Batch: 300; loss: 1.58; acc: 0.52
Batch: 320; loss: 1.79; acc: 0.34
Batch: 340; loss: 1.51; acc: 0.48
Batch: 360; loss: 1.65; acc: 0.55
Batch: 380; loss: 1.55; acc: 0.56
Batch: 400; loss: 1.54; acc: 0.55
Batch: 420; loss: 1.53; acc: 0.56
Batch: 440; loss: 1.53; acc: 0.55
Batch: 460; loss: 1.58; acc: 0.5
Batch: 480; loss: 1.58; acc: 0.5
Batch: 500; loss: 1.64; acc: 0.42
Batch: 520; loss: 1.48; acc: 0.62
Batch: 540; loss: 1.46; acc: 0.56
Batch: 560; loss: 1.41; acc: 0.59
Batch: 580; loss: 1.4; acc: 0.59
Batch: 600; loss: 1.52; acc: 0.61
Batch: 620; loss: 1.37; acc: 0.64
Batch: 640; loss: 1.66; acc: 0.48
Batch: 660; loss: 1.5; acc: 0.48
Batch: 680; loss: 1.47; acc: 0.55
Batch: 700; loss: 1.44; acc: 0.58
Batch: 720; loss: 1.46; acc: 0.58
Batch: 740; loss: 1.63; acc: 0.47
Batch: 760; loss: 1.48; acc: 0.61
Batch: 780; loss: 1.47; acc: 0.58
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.3278523662593216e-05
1.6086407413240522e-05
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.6; acc: 0.5
Batch: 40; loss: 1.26; acc: 0.7
Batch: 60; loss: 1.4; acc: 0.66
Batch: 80; loss: 1.47; acc: 0.5
Batch: 100; loss: 1.48; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.46; acc: 0.62
Val Epoch over. val_loss: 1.4944050972628746; val_accuracy: 0.5638933121019108 

The current subspace-distance is: 1.6086407413240522e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.55
Batch: 20; loss: 1.42; acc: 0.66
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.59; acc: 0.45
Batch: 80; loss: 1.55; acc: 0.5
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 1.56; acc: 0.45
Batch: 160; loss: 1.42; acc: 0.56
Batch: 180; loss: 1.63; acc: 0.52
Batch: 200; loss: 1.67; acc: 0.44
Batch: 220; loss: 1.65; acc: 0.42
Batch: 240; loss: 1.63; acc: 0.55
Batch: 260; loss: 1.74; acc: 0.41
Batch: 280; loss: 1.46; acc: 0.53
Batch: 300; loss: 1.39; acc: 0.59
Batch: 320; loss: 1.62; acc: 0.42
Batch: 340; loss: 1.52; acc: 0.53
Batch: 360; loss: 1.57; acc: 0.47
Batch: 380; loss: 1.78; acc: 0.38
Batch: 400; loss: 1.5; acc: 0.61
Batch: 420; loss: 1.53; acc: 0.55
Batch: 440; loss: 1.6; acc: 0.53
Batch: 460; loss: 1.56; acc: 0.53
Batch: 480; loss: 1.5; acc: 0.58
Batch: 500; loss: 1.45; acc: 0.59
Batch: 520; loss: 1.48; acc: 0.53
Batch: 540; loss: 1.58; acc: 0.45
Batch: 560; loss: 1.51; acc: 0.56
Batch: 580; loss: 1.48; acc: 0.55
Batch: 600; loss: 1.59; acc: 0.55
Batch: 620; loss: 1.49; acc: 0.55
Batch: 640; loss: 1.53; acc: 0.58
Batch: 660; loss: 1.64; acc: 0.48
Batch: 680; loss: 1.63; acc: 0.56
Batch: 700; loss: 1.42; acc: 0.67
Batch: 720; loss: 1.58; acc: 0.45
Batch: 740; loss: 1.51; acc: 0.62
Batch: 760; loss: 1.42; acc: 0.64
Batch: 780; loss: 1.62; acc: 0.53
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.453001747606322e-05
1.5057074961077888e-05
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.25; acc: 0.7
Batch: 60; loss: 1.4; acc: 0.66
Batch: 80; loss: 1.45; acc: 0.5
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.45; acc: 0.64
Val Epoch over. val_loss: 1.4923664627561144; val_accuracy: 0.56140525477707 

The current subspace-distance is: 1.5057074961077888e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.42; acc: 0.64
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.56
Batch: 160; loss: 1.6; acc: 0.52
Batch: 180; loss: 1.53; acc: 0.47
Batch: 200; loss: 1.57; acc: 0.5
Batch: 220; loss: 1.62; acc: 0.47
Batch: 240; loss: 1.63; acc: 0.45
Batch: 260; loss: 1.43; acc: 0.59
Batch: 280; loss: 1.49; acc: 0.62
Batch: 300; loss: 1.53; acc: 0.52
Batch: 320; loss: 1.56; acc: 0.55
Batch: 340; loss: 1.37; acc: 0.64
Batch: 360; loss: 1.45; acc: 0.66
Batch: 380; loss: 1.49; acc: 0.61
Batch: 400; loss: 1.66; acc: 0.41
Batch: 420; loss: 1.57; acc: 0.45
Batch: 440; loss: 1.48; acc: 0.64
Batch: 460; loss: 1.6; acc: 0.47
Batch: 480; loss: 1.43; acc: 0.58
Batch: 500; loss: 1.47; acc: 0.56
Batch: 520; loss: 1.36; acc: 0.69
Batch: 540; loss: 1.54; acc: 0.53
Batch: 560; loss: 1.46; acc: 0.55
Batch: 580; loss: 1.55; acc: 0.56
Batch: 600; loss: 1.43; acc: 0.53
Batch: 620; loss: 1.46; acc: 0.64
Batch: 640; loss: 1.45; acc: 0.62
Batch: 660; loss: 1.62; acc: 0.53
Batch: 680; loss: 1.57; acc: 0.52
Batch: 700; loss: 1.52; acc: 0.53
Batch: 720; loss: 1.61; acc: 0.47
Batch: 740; loss: 1.66; acc: 0.42
Batch: 760; loss: 1.27; acc: 0.8
Batch: 780; loss: 1.49; acc: 0.62
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.436701783561148e-05
1.5263771274476312e-05
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.26; acc: 0.69
Batch: 60; loss: 1.41; acc: 0.66
Batch: 80; loss: 1.45; acc: 0.52
Batch: 100; loss: 1.48; acc: 0.61
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.62
Val Epoch over. val_loss: 1.493244911455045; val_accuracy: 0.5596138535031847 

The current subspace-distance is: 1.5263771274476312e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.5
Batch: 20; loss: 1.51; acc: 0.55
Batch: 40; loss: 1.55; acc: 0.53
Batch: 60; loss: 1.45; acc: 0.5
Batch: 80; loss: 1.47; acc: 0.59
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.41; acc: 0.66
Batch: 140; loss: 1.55; acc: 0.47
Batch: 160; loss: 1.6; acc: 0.53
Batch: 180; loss: 1.51; acc: 0.55
Batch: 200; loss: 1.66; acc: 0.48
Batch: 220; loss: 1.7; acc: 0.44
Batch: 240; loss: 1.65; acc: 0.45
Batch: 260; loss: 1.56; acc: 0.5
Batch: 280; loss: 1.54; acc: 0.5
Batch: 300; loss: 1.48; acc: 0.53
Batch: 320; loss: 1.56; acc: 0.58
Batch: 340; loss: 1.6; acc: 0.47
Batch: 360; loss: 1.54; acc: 0.59
Batch: 380; loss: 1.53; acc: 0.52
Batch: 400; loss: 1.41; acc: 0.61
Batch: 420; loss: 1.57; acc: 0.56
Batch: 440; loss: 1.53; acc: 0.58
Batch: 460; loss: 1.49; acc: 0.56
Batch: 480; loss: 1.35; acc: 0.59
Batch: 500; loss: 1.43; acc: 0.59
Batch: 520; loss: 1.69; acc: 0.41
Batch: 540; loss: 1.53; acc: 0.56
Batch: 560; loss: 1.5; acc: 0.53
Batch: 580; loss: 1.63; acc: 0.48
Batch: 600; loss: 1.59; acc: 0.5
Batch: 620; loss: 1.56; acc: 0.55
Batch: 640; loss: 1.5; acc: 0.58
Batch: 660; loss: 1.46; acc: 0.52
Batch: 680; loss: 1.56; acc: 0.59
Batch: 700; loss: 1.67; acc: 0.44
Batch: 720; loss: 1.56; acc: 0.5
Batch: 740; loss: 1.43; acc: 0.67
Batch: 760; loss: 1.63; acc: 0.45
Batch: 780; loss: 1.58; acc: 0.39
Train Epoch over. train_loss: 1.53; train_accuracy: 0.54 

4.522061863099225e-05
1.535404589958489e-05
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.61; acc: 0.5
Batch: 40; loss: 1.26; acc: 0.69
Batch: 60; loss: 1.4; acc: 0.66
Batch: 80; loss: 1.47; acc: 0.53
Batch: 100; loss: 1.48; acc: 0.59
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.46; acc: 0.62
Val Epoch over. val_loss: 1.4945893454703556; val_accuracy: 0.5666799363057324 

The current subspace-distance is: 1.535404589958489e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.45
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 1.44; acc: 0.59
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.57; acc: 0.53
Batch: 100; loss: 1.57; acc: 0.48
Batch: 120; loss: 1.57; acc: 0.48
Batch: 140; loss: 1.55; acc: 0.55
Batch: 160; loss: 1.58; acc: 0.48
Batch: 180; loss: 1.63; acc: 0.48
Batch: 200; loss: 1.56; acc: 0.47
Batch: 220; loss: 1.47; acc: 0.62
Batch: 240; loss: 1.61; acc: 0.5
Batch: 260; loss: 1.64; acc: 0.52
Batch: 280; loss: 1.56; acc: 0.5
Batch: 300; loss: 1.43; acc: 0.61
Batch: 320; loss: 1.59; acc: 0.45
Batch: 340; loss: 1.57; acc: 0.56
Batch: 360; loss: 1.45; acc: 0.56
Batch: 380; loss: 1.54; acc: 0.53
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.49; acc: 0.56
Batch: 440; loss: 1.45; acc: 0.62
Batch: 460; loss: 1.59; acc: 0.41
Batch: 480; loss: 1.54; acc: 0.45
Batch: 500; loss: 1.58; acc: 0.55
Batch: 520; loss: 1.53; acc: 0.56
Batch: 540; loss: 1.51; acc: 0.58
Batch: 560; loss: 1.58; acc: 0.61
Batch: 580; loss: 1.45; acc: 0.58
Batch: 600; loss: 1.57; acc: 0.52
Batch: 620; loss: 1.5; acc: 0.53
Batch: 640; loss: 1.6; acc: 0.53
Batch: 660; loss: 1.44; acc: 0.58
Batch: 680; loss: 1.38; acc: 0.64
Batch: 700; loss: 1.4; acc: 0.58
Batch: 720; loss: 1.55; acc: 0.52
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.49; acc: 0.56
Batch: 780; loss: 1.59; acc: 0.53
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.43201424786821e-05
1.2905382391181774e-05
Batch: 0; loss: 1.46; acc: 0.61
Batch: 20; loss: 1.6; acc: 0.47
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.45; acc: 0.5
Batch: 100; loss: 1.46; acc: 0.59
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.45; acc: 0.64
Val Epoch over. val_loss: 1.4845949601215922; val_accuracy: 0.5641918789808917 

The current subspace-distance is: 1.2905382391181774e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.45; acc: 0.56
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.57; acc: 0.48
Batch: 60; loss: 1.62; acc: 0.45
Batch: 80; loss: 1.62; acc: 0.44
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.46; acc: 0.61
Batch: 140; loss: 1.34; acc: 0.62
Batch: 160; loss: 1.4; acc: 0.59
Batch: 180; loss: 1.49; acc: 0.55
Batch: 200; loss: 1.5; acc: 0.61
Batch: 220; loss: 1.48; acc: 0.52
Batch: 240; loss: 1.5; acc: 0.53
Batch: 260; loss: 1.49; acc: 0.53
Batch: 280; loss: 1.38; acc: 0.64
Batch: 300; loss: 1.54; acc: 0.61
Batch: 320; loss: 1.45; acc: 0.59
Batch: 340; loss: 1.59; acc: 0.53
Batch: 360; loss: 1.48; acc: 0.56
Batch: 380; loss: 1.37; acc: 0.56
Batch: 400; loss: 1.59; acc: 0.5
Batch: 420; loss: 1.48; acc: 0.52
Batch: 440; loss: 1.54; acc: 0.59
Batch: 460; loss: 1.64; acc: 0.48
Batch: 480; loss: 1.39; acc: 0.66
Batch: 500; loss: 1.74; acc: 0.45
Batch: 520; loss: 1.63; acc: 0.48
Batch: 540; loss: 1.53; acc: 0.5
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.46; acc: 0.56
Batch: 600; loss: 1.45; acc: 0.64
Batch: 620; loss: 1.51; acc: 0.47
Batch: 640; loss: 1.54; acc: 0.53
Batch: 660; loss: 1.87; acc: 0.36
Batch: 680; loss: 1.47; acc: 0.58
Batch: 700; loss: 1.53; acc: 0.52
Batch: 720; loss: 1.39; acc: 0.59
Batch: 740; loss: 1.69; acc: 0.47
Batch: 760; loss: 1.49; acc: 0.59
Batch: 780; loss: 1.46; acc: 0.58
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.5025783037999645e-05
1.6424772184109315e-05
Batch: 0; loss: 1.46; acc: 0.62
Batch: 20; loss: 1.6; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.52
Batch: 140; loss: 1.44; acc: 0.64
Val Epoch over. val_loss: 1.4823957051441168; val_accuracy: 0.5645899681528662 

The current subspace-distance is: 1.6424772184109315e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.56; acc: 0.58
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.57; acc: 0.53
Batch: 100; loss: 1.5; acc: 0.62
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 1.53; acc: 0.55
Batch: 160; loss: 1.4; acc: 0.64
Batch: 180; loss: 1.49; acc: 0.58
Batch: 200; loss: 1.48; acc: 0.62
Batch: 220; loss: 1.64; acc: 0.5
Batch: 240; loss: 1.44; acc: 0.56
Batch: 260; loss: 1.57; acc: 0.42
Batch: 280; loss: 1.44; acc: 0.58
Batch: 300; loss: 1.42; acc: 0.62
Batch: 320; loss: 1.53; acc: 0.52
Batch: 340; loss: 1.59; acc: 0.52
Batch: 360; loss: 1.45; acc: 0.59
Batch: 380; loss: 1.47; acc: 0.56
Batch: 400; loss: 1.42; acc: 0.62
Batch: 420; loss: 1.47; acc: 0.55
Batch: 440; loss: 1.56; acc: 0.47
Batch: 460; loss: 1.54; acc: 0.61
Batch: 480; loss: 1.61; acc: 0.53
Batch: 500; loss: 1.6; acc: 0.45
Batch: 520; loss: 1.46; acc: 0.52
Batch: 540; loss: 1.6; acc: 0.44
Batch: 560; loss: 1.4; acc: 0.66
Batch: 580; loss: 1.45; acc: 0.56
Batch: 600; loss: 1.56; acc: 0.55
Batch: 620; loss: 1.67; acc: 0.5
Batch: 640; loss: 1.56; acc: 0.55
Batch: 660; loss: 1.53; acc: 0.53
Batch: 680; loss: 1.52; acc: 0.5
Batch: 700; loss: 1.53; acc: 0.61
Batch: 720; loss: 1.59; acc: 0.53
Batch: 740; loss: 1.75; acc: 0.42
Batch: 760; loss: 1.67; acc: 0.45
Batch: 780; loss: 1.47; acc: 0.56
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.534804247668944e-05
1.521429749118397e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.61; acc: 0.44
Batch: 40; loss: 1.24; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.47; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.46; acc: 0.64
Val Epoch over. val_loss: 1.4856950150933235; val_accuracy: 0.5634952229299363 

The current subspace-distance is: 1.521429749118397e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.52; acc: 0.52
Batch: 20; loss: 1.41; acc: 0.5
Batch: 40; loss: 1.57; acc: 0.47
Batch: 60; loss: 1.57; acc: 0.5
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.47
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 1.57; acc: 0.58
Batch: 160; loss: 1.48; acc: 0.58
Batch: 180; loss: 1.44; acc: 0.58
Batch: 200; loss: 1.53; acc: 0.48
Batch: 220; loss: 1.54; acc: 0.48
Batch: 240; loss: 1.45; acc: 0.61
Batch: 260; loss: 1.56; acc: 0.52
Batch: 280; loss: 1.65; acc: 0.47
Batch: 300; loss: 1.52; acc: 0.55
Batch: 320; loss: 1.59; acc: 0.45
Batch: 340; loss: 1.7; acc: 0.44
Batch: 360; loss: 1.59; acc: 0.45
Batch: 380; loss: 1.5; acc: 0.55
Batch: 400; loss: 1.59; acc: 0.53
Batch: 420; loss: 1.73; acc: 0.44
Batch: 440; loss: 1.45; acc: 0.56
Batch: 460; loss: 1.47; acc: 0.58
Batch: 480; loss: 1.6; acc: 0.53
Batch: 500; loss: 1.45; acc: 0.66
Batch: 520; loss: 1.42; acc: 0.62
Batch: 540; loss: 1.68; acc: 0.5
Batch: 560; loss: 1.52; acc: 0.56
Batch: 580; loss: 1.36; acc: 0.7
Batch: 600; loss: 1.54; acc: 0.52
Batch: 620; loss: 1.35; acc: 0.62
Batch: 640; loss: 1.6; acc: 0.52
Batch: 660; loss: 1.47; acc: 0.56
Batch: 680; loss: 1.42; acc: 0.64
Batch: 700; loss: 1.52; acc: 0.53
Batch: 720; loss: 1.38; acc: 0.56
Batch: 740; loss: 1.52; acc: 0.48
Batch: 760; loss: 1.53; acc: 0.5
Batch: 780; loss: 1.5; acc: 0.61
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.5530785428127274e-05
1.4934301361790858e-05
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.6; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.45; acc: 0.52
Batch: 100; loss: 1.46; acc: 0.58
Batch: 120; loss: 1.53; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.66
Val Epoch over. val_loss: 1.4835500246400286; val_accuracy: 0.5661823248407644 

The current subspace-distance is: 1.4934301361790858e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.44; acc: 0.56
Batch: 40; loss: 1.41; acc: 0.58
Batch: 60; loss: 1.58; acc: 0.44
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.37; acc: 0.62
Batch: 120; loss: 1.71; acc: 0.42
Batch: 140; loss: 1.42; acc: 0.58
Batch: 160; loss: 1.46; acc: 0.59
Batch: 180; loss: 1.61; acc: 0.52
Batch: 200; loss: 1.46; acc: 0.56
Batch: 220; loss: 1.49; acc: 0.55
Batch: 240; loss: 1.41; acc: 0.61
Batch: 260; loss: 1.71; acc: 0.48
Batch: 280; loss: 1.42; acc: 0.58
Batch: 300; loss: 1.5; acc: 0.53
Batch: 320; loss: 1.61; acc: 0.55
Batch: 340; loss: 1.52; acc: 0.56
Batch: 360; loss: 1.42; acc: 0.62
Batch: 380; loss: 1.65; acc: 0.45
Batch: 400; loss: 1.54; acc: 0.47
Batch: 420; loss: 1.54; acc: 0.45
Batch: 440; loss: 1.64; acc: 0.42
Batch: 460; loss: 1.44; acc: 0.66
Batch: 480; loss: 1.61; acc: 0.48
Batch: 500; loss: 1.56; acc: 0.53
Batch: 520; loss: 1.57; acc: 0.47
Batch: 540; loss: 1.6; acc: 0.5
Batch: 560; loss: 1.54; acc: 0.56
Batch: 580; loss: 1.56; acc: 0.58
Batch: 600; loss: 1.48; acc: 0.56
Batch: 620; loss: 1.47; acc: 0.56
Batch: 640; loss: 1.44; acc: 0.61
Batch: 660; loss: 1.56; acc: 0.5
Batch: 680; loss: 1.47; acc: 0.53
Batch: 700; loss: 1.62; acc: 0.5
Batch: 720; loss: 1.46; acc: 0.56
Batch: 740; loss: 1.44; acc: 0.56
Batch: 760; loss: 1.47; acc: 0.53
Batch: 780; loss: 1.58; acc: 0.53
Train Epoch over. train_loss: 1.52; train_accuracy: 0.54 

4.6061792090767995e-05
1.3471848433255218e-05
Batch: 0; loss: 1.46; acc: 0.61
Batch: 20; loss: 1.59; acc: 0.48
Batch: 40; loss: 1.22; acc: 0.67
Batch: 60; loss: 1.38; acc: 0.66
Batch: 80; loss: 1.43; acc: 0.52
Batch: 100; loss: 1.44; acc: 0.56
Batch: 120; loss: 1.53; acc: 0.53
Batch: 140; loss: 1.44; acc: 0.64
Val Epoch over. val_loss: 1.4712392737151712; val_accuracy: 0.5645899681528662 

The current subspace-distance is: 1.3471848433255218e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.61; acc: 0.55
Batch: 20; loss: 1.46; acc: 0.52
Batch: 40; loss: 1.46; acc: 0.53
Batch: 60; loss: 1.5; acc: 0.53
Batch: 80; loss: 1.51; acc: 0.55
Batch: 100; loss: 1.5; acc: 0.53
Batch: 120; loss: 1.54; acc: 0.56
Batch: 140; loss: 1.41; acc: 0.58
Batch: 160; loss: 1.56; acc: 0.53
Batch: 180; loss: 1.45; acc: 0.55
Batch: 200; loss: 1.5; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.53
Batch: 240; loss: 1.55; acc: 0.55
Batch: 260; loss: 1.56; acc: 0.53
Batch: 280; loss: 1.35; acc: 0.67
Batch: 300; loss: 1.35; acc: 0.59
Batch: 320; loss: 1.4; acc: 0.55
Batch: 340; loss: 1.38; acc: 0.62
Batch: 360; loss: 1.5; acc: 0.59
Batch: 380; loss: 1.48; acc: 0.52
Batch: 400; loss: 1.45; acc: 0.53
Batch: 420; loss: 1.61; acc: 0.47
Batch: 440; loss: 1.48; acc: 0.55
Batch: 460; loss: 1.53; acc: 0.53
Batch: 480; loss: 1.65; acc: 0.41
Batch: 500; loss: 1.65; acc: 0.41
Batch: 520; loss: 1.37; acc: 0.67
Batch: 540; loss: 1.53; acc: 0.58
Batch: 560; loss: 1.38; acc: 0.64
Batch: 580; loss: 1.61; acc: 0.53
Batch: 600; loss: 1.39; acc: 0.62
Batch: 620; loss: 1.62; acc: 0.45
Batch: 640; loss: 1.42; acc: 0.59
Batch: 660; loss: 1.57; acc: 0.53
Batch: 680; loss: 1.46; acc: 0.56
Batch: 700; loss: 1.56; acc: 0.55
Batch: 720; loss: 1.33; acc: 0.64
Batch: 740; loss: 1.57; acc: 0.47
Batch: 760; loss: 1.63; acc: 0.39
Batch: 780; loss: 1.41; acc: 0.61
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.608670133166015e-05
1.4889889826008584e-05
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.61; acc: 0.48
Batch: 40; loss: 1.23; acc: 0.7
Batch: 60; loss: 1.4; acc: 0.66
Batch: 80; loss: 1.45; acc: 0.52
Batch: 100; loss: 1.47; acc: 0.56
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.62
Val Epoch over. val_loss: 1.485747199149648; val_accuracy: 0.5637937898089171 

The current subspace-distance is: 1.4889889826008584e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.44
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.62; acc: 0.53
Batch: 60; loss: 1.61; acc: 0.41
Batch: 80; loss: 1.58; acc: 0.42
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.59
Batch: 140; loss: 1.51; acc: 0.55
Batch: 160; loss: 1.53; acc: 0.5
Batch: 180; loss: 1.54; acc: 0.58
Batch: 200; loss: 1.38; acc: 0.59
Batch: 220; loss: 1.47; acc: 0.53
Batch: 240; loss: 1.45; acc: 0.53
Batch: 260; loss: 1.54; acc: 0.48
Batch: 280; loss: 1.59; acc: 0.56
Batch: 300; loss: 1.52; acc: 0.47
Batch: 320; loss: 1.55; acc: 0.52
Batch: 340; loss: 1.42; acc: 0.59
Batch: 360; loss: 1.52; acc: 0.5
Batch: 380; loss: 1.56; acc: 0.59
Batch: 400; loss: 1.54; acc: 0.55
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.41; acc: 0.59
Batch: 460; loss: 1.55; acc: 0.5
Batch: 480; loss: 1.58; acc: 0.53
Batch: 500; loss: 1.6; acc: 0.55
Batch: 520; loss: 1.48; acc: 0.64
Batch: 540; loss: 1.43; acc: 0.62
Batch: 560; loss: 1.42; acc: 0.61
Batch: 580; loss: 1.49; acc: 0.53
Batch: 600; loss: 1.68; acc: 0.44
Batch: 620; loss: 1.55; acc: 0.56
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.53; acc: 0.62
Batch: 680; loss: 1.62; acc: 0.41
Batch: 700; loss: 1.55; acc: 0.56
Batch: 720; loss: 1.55; acc: 0.52
Batch: 740; loss: 1.51; acc: 0.56
Batch: 760; loss: 1.44; acc: 0.61
Batch: 780; loss: 1.5; acc: 0.56
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.5609653170686215e-05
1.278010313399136e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.43; acc: 0.52
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 1.46; acc: 0.62
Val Epoch over. val_loss: 1.4734176556775525; val_accuracy: 0.5641918789808917 

The current subspace-distance is: 1.278010313399136e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.52
Batch: 20; loss: 1.49; acc: 0.62
Batch: 40; loss: 1.69; acc: 0.48
Batch: 60; loss: 1.54; acc: 0.5
Batch: 80; loss: 1.63; acc: 0.52
Batch: 100; loss: 1.32; acc: 0.61
Batch: 120; loss: 1.57; acc: 0.55
Batch: 140; loss: 1.5; acc: 0.59
Batch: 160; loss: 1.48; acc: 0.55
Batch: 180; loss: 1.45; acc: 0.56
Batch: 200; loss: 1.46; acc: 0.55
Batch: 220; loss: 1.38; acc: 0.66
Batch: 240; loss: 1.49; acc: 0.53
Batch: 260; loss: 1.51; acc: 0.61
Batch: 280; loss: 1.63; acc: 0.48
Batch: 300; loss: 1.39; acc: 0.62
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.45; acc: 0.55
Batch: 360; loss: 1.64; acc: 0.38
Batch: 380; loss: 1.28; acc: 0.69
Batch: 400; loss: 1.47; acc: 0.59
Batch: 420; loss: 1.65; acc: 0.52
Batch: 440; loss: 1.37; acc: 0.58
Batch: 460; loss: 1.45; acc: 0.53
Batch: 480; loss: 1.57; acc: 0.48
Batch: 500; loss: 1.54; acc: 0.47
Batch: 520; loss: 1.45; acc: 0.59
Batch: 540; loss: 1.52; acc: 0.55
Batch: 560; loss: 1.57; acc: 0.52
Batch: 580; loss: 1.67; acc: 0.44
Batch: 600; loss: 1.54; acc: 0.48
Batch: 620; loss: 1.45; acc: 0.56
Batch: 640; loss: 1.67; acc: 0.39
Batch: 660; loss: 1.5; acc: 0.5
Batch: 680; loss: 1.54; acc: 0.53
Batch: 700; loss: 1.4; acc: 0.59
Batch: 720; loss: 1.49; acc: 0.56
Batch: 740; loss: 1.51; acc: 0.52
Batch: 760; loss: 1.41; acc: 0.53
Batch: 780; loss: 1.76; acc: 0.41
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.685230669565499e-05
1.631531813472975e-05
Batch: 0; loss: 1.46; acc: 0.58
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.22; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.44; acc: 0.5
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.58
Val Epoch over. val_loss: 1.4815944091529603; val_accuracy: 0.5597133757961783 

The current subspace-distance is: 1.631531813472975e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.41
Batch: 20; loss: 1.56; acc: 0.53
Batch: 40; loss: 1.6; acc: 0.55
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.57; acc: 0.52
Batch: 100; loss: 1.6; acc: 0.58
Batch: 120; loss: 1.48; acc: 0.56
Batch: 140; loss: 1.43; acc: 0.5
Batch: 160; loss: 1.52; acc: 0.53
Batch: 180; loss: 1.48; acc: 0.53
Batch: 200; loss: 1.6; acc: 0.47
Batch: 220; loss: 1.49; acc: 0.55
Batch: 240; loss: 1.72; acc: 0.42
Batch: 260; loss: 1.48; acc: 0.64
Batch: 280; loss: 1.45; acc: 0.58
Batch: 300; loss: 1.49; acc: 0.52
Batch: 320; loss: 1.6; acc: 0.5
Batch: 340; loss: 1.38; acc: 0.58
Batch: 360; loss: 1.59; acc: 0.55
Batch: 380; loss: 1.57; acc: 0.44
Batch: 400; loss: 1.42; acc: 0.59
Batch: 420; loss: 1.49; acc: 0.53
Batch: 440; loss: 1.45; acc: 0.58
Batch: 460; loss: 1.62; acc: 0.55
Batch: 480; loss: 1.26; acc: 0.7
Batch: 500; loss: 1.63; acc: 0.52
Batch: 520; loss: 1.52; acc: 0.58
Batch: 540; loss: 1.46; acc: 0.61
Batch: 560; loss: 1.45; acc: 0.55
Batch: 580; loss: 1.44; acc: 0.62
Batch: 600; loss: 1.4; acc: 0.55
Batch: 620; loss: 1.57; acc: 0.42
Batch: 640; loss: 1.4; acc: 0.66
Batch: 660; loss: 1.61; acc: 0.42
Batch: 680; loss: 1.63; acc: 0.47
Batch: 700; loss: 1.57; acc: 0.47
Batch: 720; loss: 1.52; acc: 0.5
Batch: 740; loss: 1.44; acc: 0.58
Batch: 760; loss: 1.5; acc: 0.5
Batch: 780; loss: 1.6; acc: 0.52
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.83019721286837e-05
1.9600109226303175e-05
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.47
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.44; acc: 0.48
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.54; acc: 0.52
Batch: 140; loss: 1.46; acc: 0.64
Val Epoch over. val_loss: 1.4737135710989593; val_accuracy: 0.5620023885350318 

The current subspace-distance is: 1.9600109226303175e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.53
Batch: 20; loss: 1.41; acc: 0.56
Batch: 40; loss: 1.5; acc: 0.58
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.53
Batch: 100; loss: 1.47; acc: 0.56
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 1.54; acc: 0.61
Batch: 160; loss: 1.59; acc: 0.59
Batch: 180; loss: 1.61; acc: 0.53
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.39; acc: 0.64
Batch: 260; loss: 1.64; acc: 0.48
Batch: 280; loss: 1.79; acc: 0.38
Batch: 300; loss: 1.45; acc: 0.56
Batch: 320; loss: 1.32; acc: 0.72
Batch: 340; loss: 1.46; acc: 0.59
Batch: 360; loss: 1.64; acc: 0.53
Batch: 380; loss: 1.54; acc: 0.56
Batch: 400; loss: 1.63; acc: 0.47
Batch: 420; loss: 1.68; acc: 0.45
Batch: 440; loss: 1.52; acc: 0.55
Batch: 460; loss: 1.71; acc: 0.5
Batch: 480; loss: 1.53; acc: 0.55
Batch: 500; loss: 1.4; acc: 0.61
Batch: 520; loss: 1.4; acc: 0.58
Batch: 540; loss: 1.38; acc: 0.58
Batch: 560; loss: 1.53; acc: 0.52
Batch: 580; loss: 1.47; acc: 0.58
Batch: 600; loss: 1.61; acc: 0.53
Batch: 620; loss: 1.6; acc: 0.47
Batch: 640; loss: 1.51; acc: 0.59
Batch: 660; loss: 1.49; acc: 0.56
Batch: 680; loss: 1.53; acc: 0.56
Batch: 700; loss: 1.49; acc: 0.56
Batch: 720; loss: 1.38; acc: 0.67
Batch: 740; loss: 1.54; acc: 0.62
Batch: 760; loss: 1.45; acc: 0.61
Batch: 780; loss: 1.66; acc: 0.39
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.8736266762716696e-05
1.888262886495795e-05
Batch: 0; loss: 1.47; acc: 0.58
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.43; acc: 0.52
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.53; acc: 0.52
Batch: 140; loss: 1.46; acc: 0.56
Val Epoch over. val_loss: 1.479422660390283; val_accuracy: 0.559812898089172 

The current subspace-distance is: 1.888262886495795e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.66; acc: 0.5
Batch: 20; loss: 1.51; acc: 0.56
Batch: 40; loss: 1.57; acc: 0.52
Batch: 60; loss: 1.46; acc: 0.58
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.55; acc: 0.53
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.26; acc: 0.66
Batch: 160; loss: 1.54; acc: 0.48
Batch: 180; loss: 1.61; acc: 0.48
Batch: 200; loss: 1.58; acc: 0.5
Batch: 220; loss: 1.73; acc: 0.48
Batch: 240; loss: 1.44; acc: 0.61
Batch: 260; loss: 1.66; acc: 0.5
Batch: 280; loss: 1.47; acc: 0.53
Batch: 300; loss: 1.55; acc: 0.52
Batch: 320; loss: 1.51; acc: 0.5
Batch: 340; loss: 1.56; acc: 0.42
Batch: 360; loss: 1.46; acc: 0.55
Batch: 380; loss: 1.58; acc: 0.48
Batch: 400; loss: 1.39; acc: 0.58
Batch: 420; loss: 1.59; acc: 0.5
Batch: 440; loss: 1.33; acc: 0.61
Batch: 460; loss: 1.54; acc: 0.56
Batch: 480; loss: 1.51; acc: 0.53
Batch: 500; loss: 1.68; acc: 0.45
Batch: 520; loss: 1.49; acc: 0.53
Batch: 540; loss: 1.5; acc: 0.61
Batch: 560; loss: 1.49; acc: 0.56
Batch: 580; loss: 1.46; acc: 0.64
Batch: 600; loss: 1.58; acc: 0.55
Batch: 620; loss: 1.66; acc: 0.42
Batch: 640; loss: 1.35; acc: 0.64
Batch: 660; loss: 1.5; acc: 0.55
Batch: 680; loss: 1.55; acc: 0.55
Batch: 700; loss: 1.63; acc: 0.52
Batch: 720; loss: 1.53; acc: 0.52
Batch: 740; loss: 1.39; acc: 0.67
Batch: 760; loss: 1.49; acc: 0.59
Batch: 780; loss: 1.46; acc: 0.61
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.631163028534502e-05
1.5224874005070888e-05
Batch: 0; loss: 1.47; acc: 0.59
Batch: 20; loss: 1.61; acc: 0.47
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.44; acc: 0.52
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.61
Val Epoch over. val_loss: 1.4781713189592787; val_accuracy: 0.5622014331210191 

The current subspace-distance is: 1.5224874005070888e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.44; acc: 0.58
Batch: 20; loss: 1.64; acc: 0.44
Batch: 40; loss: 1.46; acc: 0.5
Batch: 60; loss: 1.59; acc: 0.53
Batch: 80; loss: 1.46; acc: 0.56
Batch: 100; loss: 1.47; acc: 0.53
Batch: 120; loss: 1.57; acc: 0.53
Batch: 140; loss: 1.66; acc: 0.48
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.51; acc: 0.52
Batch: 200; loss: 1.5; acc: 0.5
Batch: 220; loss: 1.59; acc: 0.47
Batch: 240; loss: 1.53; acc: 0.52
Batch: 260; loss: 1.43; acc: 0.5
Batch: 280; loss: 1.69; acc: 0.5
Batch: 300; loss: 1.81; acc: 0.52
Batch: 320; loss: 1.46; acc: 0.64
Batch: 340; loss: 1.62; acc: 0.48
Batch: 360; loss: 1.51; acc: 0.44
Batch: 380; loss: 1.45; acc: 0.5
Batch: 400; loss: 1.52; acc: 0.5
Batch: 420; loss: 1.57; acc: 0.5
Batch: 440; loss: 1.62; acc: 0.48
Batch: 460; loss: 1.68; acc: 0.5
Batch: 480; loss: 1.5; acc: 0.66
Batch: 500; loss: 1.42; acc: 0.64
Batch: 520; loss: 1.52; acc: 0.56
Batch: 540; loss: 1.59; acc: 0.55
Batch: 560; loss: 1.51; acc: 0.62
Batch: 580; loss: 1.66; acc: 0.53
Batch: 600; loss: 1.37; acc: 0.53
Batch: 620; loss: 1.73; acc: 0.41
Batch: 640; loss: 1.56; acc: 0.53
Batch: 660; loss: 1.6; acc: 0.5
Batch: 680; loss: 1.58; acc: 0.48
Batch: 700; loss: 1.6; acc: 0.5
Batch: 720; loss: 1.66; acc: 0.5
Batch: 740; loss: 1.52; acc: 0.59
Batch: 760; loss: 1.48; acc: 0.53
Batch: 780; loss: 1.44; acc: 0.56
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.825078940484673e-05
1.8373088096268475e-05
Batch: 0; loss: 1.46; acc: 0.58
Batch: 20; loss: 1.62; acc: 0.47
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.43; acc: 0.52
Batch: 100; loss: 1.47; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.59
Val Epoch over. val_loss: 1.477926263383999; val_accuracy: 0.564390923566879 

The current subspace-distance is: 1.8373088096268475e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.42; acc: 0.64
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.53; acc: 0.45
Batch: 60; loss: 1.6; acc: 0.52
Batch: 80; loss: 1.53; acc: 0.48
Batch: 100; loss: 1.48; acc: 0.56
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.45; acc: 0.64
Batch: 160; loss: 1.47; acc: 0.47
Batch: 180; loss: 1.69; acc: 0.38
Batch: 200; loss: 1.36; acc: 0.62
Batch: 220; loss: 1.66; acc: 0.44
Batch: 240; loss: 1.39; acc: 0.61
Batch: 260; loss: 1.59; acc: 0.52
Batch: 280; loss: 1.43; acc: 0.64
Batch: 300; loss: 1.71; acc: 0.45
Batch: 320; loss: 1.55; acc: 0.44
Batch: 340; loss: 1.46; acc: 0.62
Batch: 360; loss: 1.54; acc: 0.53
Batch: 380; loss: 1.66; acc: 0.45
Batch: 400; loss: 1.59; acc: 0.53
Batch: 420; loss: 1.51; acc: 0.53
Batch: 440; loss: 1.56; acc: 0.55
Batch: 460; loss: 1.6; acc: 0.55
Batch: 480; loss: 1.41; acc: 0.53
Batch: 500; loss: 1.41; acc: 0.58
Batch: 520; loss: 1.5; acc: 0.55
Batch: 540; loss: 1.42; acc: 0.56
Batch: 560; loss: 1.47; acc: 0.55
Batch: 580; loss: 1.46; acc: 0.55
Batch: 600; loss: 1.4; acc: 0.58
Batch: 620; loss: 1.45; acc: 0.59
Batch: 640; loss: 1.55; acc: 0.52
Batch: 660; loss: 1.46; acc: 0.52
Batch: 680; loss: 1.47; acc: 0.62
Batch: 700; loss: 1.53; acc: 0.58
Batch: 720; loss: 1.3; acc: 0.64
Batch: 740; loss: 1.43; acc: 0.55
Batch: 760; loss: 1.53; acc: 0.53
Batch: 780; loss: 1.7; acc: 0.55
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.696598625741899e-05
1.58997190737864e-05
Batch: 0; loss: 1.46; acc: 0.61
Batch: 20; loss: 1.61; acc: 0.45
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.43; acc: 0.5
Batch: 100; loss: 1.45; acc: 0.56
Batch: 120; loss: 1.53; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.62
Val Epoch over. val_loss: 1.4720987180236038; val_accuracy: 0.5622014331210191 

The current subspace-distance is: 1.58997190737864e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.55
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.47; acc: 0.53
Batch: 60; loss: 1.57; acc: 0.56
Batch: 80; loss: 1.52; acc: 0.53
Batch: 100; loss: 1.47; acc: 0.58
Batch: 120; loss: 1.57; acc: 0.5
Batch: 140; loss: 1.42; acc: 0.69
Batch: 160; loss: 1.51; acc: 0.5
Batch: 180; loss: 1.55; acc: 0.52
Batch: 200; loss: 1.54; acc: 0.5
Batch: 220; loss: 1.41; acc: 0.62
Batch: 240; loss: 1.5; acc: 0.52
Batch: 260; loss: 1.59; acc: 0.47
Batch: 280; loss: 1.36; acc: 0.58
Batch: 300; loss: 1.45; acc: 0.53
Batch: 320; loss: 1.47; acc: 0.56
Batch: 340; loss: 1.51; acc: 0.58
Batch: 360; loss: 1.44; acc: 0.58
Batch: 380; loss: 1.58; acc: 0.48
Batch: 400; loss: 1.53; acc: 0.56
Batch: 420; loss: 1.55; acc: 0.47
Batch: 440; loss: 1.49; acc: 0.56
Batch: 460; loss: 1.53; acc: 0.5
Batch: 480; loss: 1.49; acc: 0.56
Batch: 500; loss: 1.52; acc: 0.59
Batch: 520; loss: 1.56; acc: 0.52
Batch: 540; loss: 1.44; acc: 0.58
Batch: 560; loss: 1.4; acc: 0.64
Batch: 580; loss: 1.62; acc: 0.47
Batch: 600; loss: 1.32; acc: 0.73
Batch: 620; loss: 1.57; acc: 0.55
Batch: 640; loss: 1.47; acc: 0.5
Batch: 660; loss: 1.53; acc: 0.52
Batch: 680; loss: 1.47; acc: 0.59
Batch: 700; loss: 1.57; acc: 0.52
Batch: 720; loss: 1.58; acc: 0.5
Batch: 740; loss: 1.68; acc: 0.44
Batch: 760; loss: 1.33; acc: 0.64
Batch: 780; loss: 1.57; acc: 0.55
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.775375418830663e-05
1.6851630789460614e-05
Batch: 0; loss: 1.48; acc: 0.56
Batch: 20; loss: 1.62; acc: 0.47
Batch: 40; loss: 1.21; acc: 0.67
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.43; acc: 0.5
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.61
Val Epoch over. val_loss: 1.4783012829009134; val_accuracy: 0.5602109872611465 

The current subspace-distance is: 1.6851630789460614e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.31; acc: 0.61
Batch: 20; loss: 1.38; acc: 0.7
Batch: 40; loss: 1.7; acc: 0.45
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.44; acc: 0.55
Batch: 100; loss: 1.53; acc: 0.48
Batch: 120; loss: 1.49; acc: 0.61
Batch: 140; loss: 1.53; acc: 0.48
Batch: 160; loss: 1.67; acc: 0.42
Batch: 180; loss: 1.5; acc: 0.56
Batch: 200; loss: 1.57; acc: 0.53
Batch: 220; loss: 1.33; acc: 0.64
Batch: 240; loss: 1.7; acc: 0.45
Batch: 260; loss: 1.55; acc: 0.53
Batch: 280; loss: 1.57; acc: 0.47
Batch: 300; loss: 1.41; acc: 0.56
Batch: 320; loss: 1.38; acc: 0.67
Batch: 340; loss: 1.49; acc: 0.52
Batch: 360; loss: 1.61; acc: 0.47
Batch: 380; loss: 1.25; acc: 0.73
Batch: 400; loss: 1.51; acc: 0.55
Batch: 420; loss: 1.66; acc: 0.47
Batch: 440; loss: 1.68; acc: 0.44
Batch: 460; loss: 1.73; acc: 0.44
Batch: 480; loss: 1.41; acc: 0.62
Batch: 500; loss: 1.6; acc: 0.5
Batch: 520; loss: 1.52; acc: 0.5
Batch: 540; loss: 1.66; acc: 0.5
Batch: 560; loss: 1.43; acc: 0.67
Batch: 580; loss: 1.56; acc: 0.53
Batch: 600; loss: 1.43; acc: 0.52
Batch: 620; loss: 1.8; acc: 0.42
Batch: 640; loss: 1.44; acc: 0.66
Batch: 660; loss: 1.6; acc: 0.47
Batch: 680; loss: 1.51; acc: 0.53
Batch: 700; loss: 1.52; acc: 0.45
Batch: 720; loss: 1.49; acc: 0.59
Batch: 740; loss: 1.41; acc: 0.62
Batch: 760; loss: 1.52; acc: 0.48
Batch: 780; loss: 1.49; acc: 0.56
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.6963101340224966e-05
1.4409770301426761e-05
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.6; acc: 0.47
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.38; acc: 0.64
Batch: 80; loss: 1.43; acc: 0.52
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.61
Val Epoch over. val_loss: 1.470365216018288; val_accuracy: 0.5647890127388535 

The current subspace-distance is: 1.4409770301426761e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.52
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.45; acc: 0.58
Batch: 60; loss: 1.44; acc: 0.56
Batch: 80; loss: 1.5; acc: 0.48
Batch: 100; loss: 1.41; acc: 0.59
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.55; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.52
Batch: 180; loss: 1.37; acc: 0.61
Batch: 200; loss: 1.35; acc: 0.7
Batch: 220; loss: 1.65; acc: 0.47
Batch: 240; loss: 1.56; acc: 0.53
Batch: 260; loss: 1.38; acc: 0.64
Batch: 280; loss: 1.47; acc: 0.53
Batch: 300; loss: 1.53; acc: 0.56
Batch: 320; loss: 1.46; acc: 0.59
Batch: 340; loss: 1.34; acc: 0.67
Batch: 360; loss: 1.58; acc: 0.45
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.48; acc: 0.62
Batch: 420; loss: 1.54; acc: 0.53
Batch: 440; loss: 1.5; acc: 0.48
Batch: 460; loss: 1.55; acc: 0.52
Batch: 480; loss: 1.63; acc: 0.44
Batch: 500; loss: 1.42; acc: 0.64
Batch: 520; loss: 1.51; acc: 0.56
Batch: 540; loss: 1.66; acc: 0.44
Batch: 560; loss: 1.51; acc: 0.59
Batch: 580; loss: 1.51; acc: 0.45
Batch: 600; loss: 1.49; acc: 0.59
Batch: 620; loss: 1.34; acc: 0.58
Batch: 640; loss: 1.43; acc: 0.58
Batch: 660; loss: 1.51; acc: 0.69
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.49; acc: 0.62
Batch: 720; loss: 1.55; acc: 0.5
Batch: 740; loss: 1.34; acc: 0.67
Batch: 760; loss: 1.6; acc: 0.48
Batch: 780; loss: 1.45; acc: 0.58
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.750494917971082e-05
1.8887956684920937e-05
Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.62; acc: 0.5
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.62
Batch: 80; loss: 1.43; acc: 0.55
Batch: 100; loss: 1.46; acc: 0.58
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.61
Val Epoch over. val_loss: 1.4749780908511703; val_accuracy: 0.5654856687898089 

The current subspace-distance is: 1.8887956684920937e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.43; acc: 0.58
Batch: 20; loss: 1.57; acc: 0.5
Batch: 40; loss: 1.67; acc: 0.5
Batch: 60; loss: 1.37; acc: 0.5
Batch: 80; loss: 1.45; acc: 0.48
Batch: 100; loss: 1.38; acc: 0.61
Batch: 120; loss: 1.4; acc: 0.61
Batch: 140; loss: 1.57; acc: 0.48
Batch: 160; loss: 1.53; acc: 0.53
Batch: 180; loss: 1.71; acc: 0.41
Batch: 200; loss: 1.4; acc: 0.59
Batch: 220; loss: 1.49; acc: 0.56
Batch: 240; loss: 1.5; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.53
Batch: 280; loss: 1.65; acc: 0.5
Batch: 300; loss: 1.38; acc: 0.66
Batch: 320; loss: 1.49; acc: 0.52
Batch: 340; loss: 1.59; acc: 0.47
Batch: 360; loss: 1.39; acc: 0.64
Batch: 380; loss: 1.44; acc: 0.56
Batch: 400; loss: 1.48; acc: 0.62
Batch: 420; loss: 1.34; acc: 0.56
Batch: 440; loss: 1.51; acc: 0.58
Batch: 460; loss: 1.63; acc: 0.48
Batch: 480; loss: 1.53; acc: 0.58
Batch: 500; loss: 1.33; acc: 0.67
Batch: 520; loss: 1.55; acc: 0.55
Batch: 540; loss: 1.51; acc: 0.5
Batch: 560; loss: 1.47; acc: 0.56
Batch: 580; loss: 1.55; acc: 0.5
Batch: 600; loss: 1.49; acc: 0.56
Batch: 620; loss: 1.49; acc: 0.59
Batch: 640; loss: 1.62; acc: 0.48
Batch: 660; loss: 1.49; acc: 0.53
Batch: 680; loss: 1.52; acc: 0.59
Batch: 700; loss: 1.51; acc: 0.58
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.41; acc: 0.56
Batch: 760; loss: 1.38; acc: 0.61
Batch: 780; loss: 1.4; acc: 0.61
Train Epoch over. train_loss: 1.51; train_accuracy: 0.54 

4.6723042032681406e-05
1.4225424820324406e-05
Batch: 0; loss: 1.48; acc: 0.55
Batch: 20; loss: 1.62; acc: 0.47
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.42; acc: 0.5
Batch: 100; loss: 1.47; acc: 0.56
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.47; acc: 0.59
Val Epoch over. val_loss: 1.4783550607171028; val_accuracy: 0.5604100318471338 

The current subspace-distance is: 1.4225424820324406e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_16_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.51657508881031

The number of parameters is: 267435

The number of individual parameters is:

13
234
13
13
19
38532
19
19
37
109668
37
37
64
113664
64
64
4096
64
640
10
64
64

nonzero elements in E: 26743497
elements in E: 26743500
fraction nonzero: 0.9999998878232094
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.45; acc: 0.05
Batch: 20; loss: 2.38; acc: 0.05
Batch: 40; loss: 2.32; acc: 0.16
Batch: 60; loss: 2.23; acc: 0.14
Batch: 80; loss: 2.16; acc: 0.2
Batch: 100; loss: 2.07; acc: 0.36
Batch: 120; loss: 2.02; acc: 0.2
Batch: 140; loss: 2.07; acc: 0.31
Batch: 160; loss: 2.0; acc: 0.36
Batch: 180; loss: 1.93; acc: 0.45
Batch: 200; loss: 1.92; acc: 0.44
Batch: 220; loss: 1.92; acc: 0.39
Batch: 240; loss: 1.89; acc: 0.45
Batch: 260; loss: 1.87; acc: 0.47
Batch: 280; loss: 1.96; acc: 0.36
Batch: 300; loss: 1.88; acc: 0.45
Batch: 320; loss: 1.73; acc: 0.59
Batch: 340; loss: 1.94; acc: 0.39
Batch: 360; loss: 1.8; acc: 0.47
Batch: 380; loss: 1.76; acc: 0.47
Batch: 400; loss: 1.72; acc: 0.58
Batch: 420; loss: 1.66; acc: 0.55
Batch: 440; loss: 1.72; acc: 0.48
Batch: 460; loss: 1.87; acc: 0.44
Batch: 480; loss: 1.82; acc: 0.5
Batch: 500; loss: 1.66; acc: 0.59
Batch: 520; loss: 1.65; acc: 0.58
Batch: 540; loss: 1.79; acc: 0.53
Batch: 560; loss: 1.72; acc: 0.58
Batch: 580; loss: 1.73; acc: 0.53
Batch: 600; loss: 1.62; acc: 0.62
Batch: 620; loss: 1.74; acc: 0.5
Batch: 640; loss: 1.71; acc: 0.53
Batch: 660; loss: 1.66; acc: 0.5
Batch: 680; loss: 1.57; acc: 0.66
Batch: 700; loss: 1.6; acc: 0.53
Batch: 720; loss: 1.66; acc: 0.52
Batch: 740; loss: 1.56; acc: 0.69
Batch: 760; loss: 1.62; acc: 0.53
Batch: 780; loss: 1.59; acc: 0.64
Train Epoch over. train_loss: 1.85; train_accuracy: 0.44 

5.422906542662531e-05
4.947981142322533e-05
Batch: 0; loss: 1.67; acc: 0.5
Batch: 20; loss: 1.81; acc: 0.42
Batch: 40; loss: 1.49; acc: 0.61
Batch: 60; loss: 1.55; acc: 0.56
Batch: 80; loss: 1.56; acc: 0.67
Batch: 100; loss: 1.59; acc: 0.59
Batch: 120; loss: 1.73; acc: 0.61
Batch: 140; loss: 1.67; acc: 0.55
Val Epoch over. val_loss: 1.6232744402186885; val_accuracy: 0.5715565286624203 

The current subspace-distance is: 4.947981142322533e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.56
Batch: 20; loss: 1.73; acc: 0.45
Batch: 40; loss: 1.72; acc: 0.53
Batch: 60; loss: 1.72; acc: 0.56
Batch: 80; loss: 1.67; acc: 0.5
Batch: 100; loss: 1.7; acc: 0.5
Batch: 120; loss: 1.76; acc: 0.47
Batch: 140; loss: 1.6; acc: 0.67
Batch: 160; loss: 1.61; acc: 0.53
Batch: 180; loss: 1.58; acc: 0.56
Batch: 200; loss: 1.51; acc: 0.59
Batch: 220; loss: 1.51; acc: 0.61
Batch: 240; loss: 1.64; acc: 0.55
Batch: 260; loss: 1.63; acc: 0.61
Batch: 280; loss: 1.52; acc: 0.55
Batch: 300; loss: 1.6; acc: 0.55
Batch: 320; loss: 1.63; acc: 0.52
Batch: 340; loss: 1.46; acc: 0.61
Batch: 360; loss: 1.6; acc: 0.5
Batch: 380; loss: 1.6; acc: 0.55
Batch: 400; loss: 1.57; acc: 0.62
Batch: 420; loss: 1.53; acc: 0.58
Batch: 440; loss: 1.58; acc: 0.55
Batch: 460; loss: 1.69; acc: 0.39
Batch: 480; loss: 1.52; acc: 0.56
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.48; acc: 0.64
Batch: 540; loss: 1.54; acc: 0.64
Batch: 560; loss: 1.61; acc: 0.48
Batch: 580; loss: 1.52; acc: 0.58
Batch: 600; loss: 1.46; acc: 0.72
Batch: 620; loss: 1.56; acc: 0.59
Batch: 640; loss: 1.58; acc: 0.53
Batch: 660; loss: 1.59; acc: 0.56
Batch: 680; loss: 1.74; acc: 0.45
Batch: 700; loss: 1.57; acc: 0.55
Batch: 720; loss: 1.52; acc: 0.7
Batch: 740; loss: 1.61; acc: 0.55
Batch: 760; loss: 1.51; acc: 0.61
Batch: 780; loss: 1.62; acc: 0.59
Train Epoch over. train_loss: 1.6; train_accuracy: 0.56 

6.967155786696821e-05
6.447426858358085e-05
Batch: 0; loss: 1.57; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.47
Batch: 40; loss: 1.37; acc: 0.72
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.47; acc: 0.61
Batch: 100; loss: 1.49; acc: 0.66
Batch: 120; loss: 1.63; acc: 0.55
Batch: 140; loss: 1.52; acc: 0.56
Val Epoch over. val_loss: 1.5221113756204108; val_accuracy: 0.5877786624203821 

The current subspace-distance is: 6.447426858358085e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.52
Batch: 20; loss: 1.47; acc: 0.61
Batch: 40; loss: 1.56; acc: 0.58
Batch: 60; loss: 1.62; acc: 0.53
Batch: 80; loss: 1.39; acc: 0.67
Batch: 100; loss: 1.47; acc: 0.62
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.61; acc: 0.45
Batch: 160; loss: 1.42; acc: 0.64
Batch: 180; loss: 1.64; acc: 0.45
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.5; acc: 0.58
Batch: 240; loss: 1.48; acc: 0.62
Batch: 260; loss: 1.51; acc: 0.59
Batch: 280; loss: 1.42; acc: 0.67
Batch: 300; loss: 1.53; acc: 0.58
Batch: 320; loss: 1.46; acc: 0.62
Batch: 340; loss: 1.57; acc: 0.5
Batch: 360; loss: 1.52; acc: 0.58
Batch: 380; loss: 1.58; acc: 0.52
Batch: 400; loss: 1.58; acc: 0.55
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.48; acc: 0.56
Batch: 460; loss: 1.64; acc: 0.45
Batch: 480; loss: 1.55; acc: 0.59
Batch: 500; loss: 1.56; acc: 0.55
Batch: 520; loss: 1.6; acc: 0.47
Batch: 540; loss: 1.44; acc: 0.66
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.73; acc: 0.45
Batch: 600; loss: 1.49; acc: 0.56
Batch: 620; loss: 1.61; acc: 0.55
Batch: 640; loss: 1.37; acc: 0.7
Batch: 660; loss: 1.73; acc: 0.44
Batch: 680; loss: 1.58; acc: 0.59
Batch: 700; loss: 1.51; acc: 0.56
Batch: 720; loss: 1.59; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.55
Batch: 760; loss: 1.31; acc: 0.72
Batch: 780; loss: 1.4; acc: 0.62
Train Epoch over. train_loss: 1.52; train_accuracy: 0.58 

8.247735968325287e-05
7.797213038429618e-05
Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.6; acc: 0.48
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 1.47; acc: 0.58
Batch: 80; loss: 1.39; acc: 0.66
Batch: 100; loss: 1.44; acc: 0.62
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 1.42; acc: 0.59
Val Epoch over. val_loss: 1.446027023017786; val_accuracy: 0.616640127388535 

The current subspace-distance is: 7.797213038429618e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.51; acc: 0.58
Batch: 20; loss: 1.5; acc: 0.56
Batch: 40; loss: 1.6; acc: 0.5
Batch: 60; loss: 1.51; acc: 0.61
Batch: 80; loss: 1.48; acc: 0.61
Batch: 100; loss: 1.5; acc: 0.55
Batch: 120; loss: 1.34; acc: 0.72
Batch: 140; loss: 1.48; acc: 0.53
Batch: 160; loss: 1.34; acc: 0.69
Batch: 180; loss: 1.41; acc: 0.66
Batch: 200; loss: 1.5; acc: 0.55
Batch: 220; loss: 1.37; acc: 0.64
Batch: 240; loss: 1.4; acc: 0.69
Batch: 260; loss: 1.45; acc: 0.59
Batch: 280; loss: 1.56; acc: 0.5
Batch: 300; loss: 1.37; acc: 0.58
Batch: 320; loss: 1.45; acc: 0.61
Batch: 340; loss: 1.42; acc: 0.62
Batch: 360; loss: 1.48; acc: 0.59
Batch: 380; loss: 1.54; acc: 0.59
Batch: 400; loss: 1.37; acc: 0.62
Batch: 420; loss: 1.41; acc: 0.62
Batch: 440; loss: 1.37; acc: 0.67
Batch: 460; loss: 1.51; acc: 0.53
Batch: 480; loss: 1.48; acc: 0.62
Batch: 500; loss: 1.64; acc: 0.5
Batch: 520; loss: 1.37; acc: 0.67
Batch: 540; loss: 1.37; acc: 0.67
Batch: 560; loss: 1.46; acc: 0.66
Batch: 580; loss: 1.35; acc: 0.62
Batch: 600; loss: 1.28; acc: 0.67
Batch: 620; loss: 1.34; acc: 0.66
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.46; acc: 0.59
Batch: 700; loss: 1.41; acc: 0.53
Batch: 720; loss: 1.36; acc: 0.62
Batch: 740; loss: 1.46; acc: 0.62
Batch: 760; loss: 1.44; acc: 0.61
Batch: 780; loss: 1.32; acc: 0.7
Train Epoch over. train_loss: 1.44; train_accuracy: 0.61 

9.65569051913917e-05
9.103485353989527e-05
Batch: 0; loss: 1.38; acc: 0.66
Batch: 20; loss: 1.56; acc: 0.47
Batch: 40; loss: 1.14; acc: 0.81
Batch: 60; loss: 1.38; acc: 0.61
Batch: 80; loss: 1.27; acc: 0.7
Batch: 100; loss: 1.4; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.56
Batch: 140; loss: 1.26; acc: 0.67
Val Epoch over. val_loss: 1.364378224512574; val_accuracy: 0.6385350318471338 

The current subspace-distance is: 9.103485353989527e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.62
Batch: 20; loss: 1.47; acc: 0.59
Batch: 40; loss: 1.47; acc: 0.61
Batch: 60; loss: 1.33; acc: 0.66
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.38; acc: 0.62
Batch: 120; loss: 1.44; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.72
Batch: 160; loss: 1.37; acc: 0.61
Batch: 180; loss: 1.44; acc: 0.62
Batch: 200; loss: 1.26; acc: 0.64
Batch: 220; loss: 1.37; acc: 0.66
Batch: 240; loss: 1.39; acc: 0.66
Batch: 260; loss: 1.48; acc: 0.5
Batch: 280; loss: 1.29; acc: 0.67
Batch: 300; loss: 1.4; acc: 0.62
Batch: 320; loss: 1.28; acc: 0.64
Batch: 340; loss: 1.57; acc: 0.55
Batch: 360; loss: 1.35; acc: 0.62
Batch: 380; loss: 1.22; acc: 0.73
Batch: 400; loss: 1.47; acc: 0.55
Batch: 420; loss: 1.3; acc: 0.67
Batch: 440; loss: 1.3; acc: 0.62
Batch: 460; loss: 1.37; acc: 0.62
Batch: 480; loss: 1.29; acc: 0.77
Batch: 500; loss: 1.46; acc: 0.53
Batch: 520; loss: 1.42; acc: 0.56
Batch: 540; loss: 1.29; acc: 0.66
Batch: 560; loss: 1.28; acc: 0.61
Batch: 580; loss: 1.35; acc: 0.52
Batch: 600; loss: 1.32; acc: 0.67
Batch: 620; loss: 1.33; acc: 0.59
Batch: 640; loss: 1.42; acc: 0.62
Batch: 660; loss: 1.33; acc: 0.64
Batch: 680; loss: 1.48; acc: 0.56
Batch: 700; loss: 1.34; acc: 0.61
Batch: 720; loss: 1.31; acc: 0.66
Batch: 740; loss: 1.38; acc: 0.64
Batch: 760; loss: 1.37; acc: 0.58
Batch: 780; loss: 1.31; acc: 0.64
Train Epoch over. train_loss: 1.37; train_accuracy: 0.62 

0.00011049347085645422
0.00010527512495173141
Batch: 0; loss: 1.31; acc: 0.67
Batch: 20; loss: 1.47; acc: 0.52
Batch: 40; loss: 1.04; acc: 0.8
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.31; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.12; acc: 0.77
Val Epoch over. val_loss: 1.2921479963193274; val_accuracy: 0.647890127388535 

The current subspace-distance is: 0.00010527512495173141 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.34; acc: 0.59
Batch: 20; loss: 1.18; acc: 0.75
Batch: 40; loss: 1.37; acc: 0.53
Batch: 60; loss: 1.15; acc: 0.73
Batch: 80; loss: 1.32; acc: 0.7
Batch: 100; loss: 1.43; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.73
Batch: 140; loss: 1.28; acc: 0.61
Batch: 160; loss: 1.35; acc: 0.56
Batch: 180; loss: 1.35; acc: 0.59
Batch: 200; loss: 1.22; acc: 0.67
Batch: 220; loss: 1.4; acc: 0.59
Batch: 240; loss: 1.44; acc: 0.53
Batch: 260; loss: 1.31; acc: 0.62
Batch: 280; loss: 1.41; acc: 0.59
Batch: 300; loss: 1.27; acc: 0.67
Batch: 320; loss: 1.4; acc: 0.61
Batch: 340; loss: 1.33; acc: 0.64
Batch: 360; loss: 1.33; acc: 0.61
Batch: 380; loss: 1.25; acc: 0.66
Batch: 400; loss: 1.3; acc: 0.66
Batch: 420; loss: 1.52; acc: 0.55
Batch: 440; loss: 1.39; acc: 0.56
Batch: 460; loss: 1.21; acc: 0.72
Batch: 480; loss: 1.3; acc: 0.67
Batch: 500; loss: 1.49; acc: 0.48
Batch: 520; loss: 1.18; acc: 0.64
Batch: 540; loss: 1.22; acc: 0.67
Batch: 560; loss: 1.22; acc: 0.67
Batch: 580; loss: 1.52; acc: 0.62
Batch: 600; loss: 1.29; acc: 0.62
Batch: 620; loss: 1.43; acc: 0.58
Batch: 640; loss: 1.19; acc: 0.64
Batch: 660; loss: 1.29; acc: 0.61
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.21; acc: 0.64
Batch: 720; loss: 1.31; acc: 0.59
Batch: 740; loss: 1.33; acc: 0.62
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.3; acc: 0.59
Train Epoch over. train_loss: 1.31; train_accuracy: 0.63 

0.00012066283670719713
0.00011536673264345154
Batch: 0; loss: 1.26; acc: 0.66
Batch: 20; loss: 1.43; acc: 0.55
Batch: 40; loss: 0.99; acc: 0.83
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.13; acc: 0.73
Batch: 100; loss: 1.29; acc: 0.72
Batch: 120; loss: 1.42; acc: 0.59
Batch: 140; loss: 1.07; acc: 0.81
Val Epoch over. val_loss: 1.2521698387565128; val_accuracy: 0.6537619426751592 

The current subspace-distance is: 0.00011536673264345154 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.69
Batch: 20; loss: 1.31; acc: 0.58
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.34; acc: 0.62
Batch: 80; loss: 1.5; acc: 0.56
Batch: 100; loss: 1.35; acc: 0.52
Batch: 120; loss: 1.36; acc: 0.67
Batch: 140; loss: 1.32; acc: 0.61
Batch: 160; loss: 1.22; acc: 0.69
Batch: 180; loss: 1.21; acc: 0.66
Batch: 200; loss: 1.3; acc: 0.64
Batch: 220; loss: 1.28; acc: 0.64
Batch: 240; loss: 1.41; acc: 0.55
Batch: 260; loss: 1.36; acc: 0.58
Batch: 280; loss: 1.48; acc: 0.62
Batch: 300; loss: 1.33; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.66
Batch: 340; loss: 1.38; acc: 0.59
Batch: 360; loss: 1.49; acc: 0.58
Batch: 380; loss: 1.34; acc: 0.61
Batch: 400; loss: 1.33; acc: 0.58
Batch: 420; loss: 1.25; acc: 0.64
Batch: 440; loss: 1.36; acc: 0.52
Batch: 460; loss: 1.46; acc: 0.53
Batch: 480; loss: 1.29; acc: 0.64
Batch: 500; loss: 1.16; acc: 0.7
Batch: 520; loss: 1.26; acc: 0.66
Batch: 540; loss: 1.29; acc: 0.62
Batch: 560; loss: 1.22; acc: 0.67
Batch: 580; loss: 1.23; acc: 0.61
Batch: 600; loss: 1.37; acc: 0.59
Batch: 620; loss: 1.26; acc: 0.67
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.31; acc: 0.62
Batch: 680; loss: 1.16; acc: 0.72
Batch: 700; loss: 1.23; acc: 0.7
Batch: 720; loss: 1.15; acc: 0.73
Batch: 740; loss: 1.02; acc: 0.78
Batch: 760; loss: 1.24; acc: 0.72
Batch: 780; loss: 1.22; acc: 0.67
Train Epoch over. train_loss: 1.27; train_accuracy: 0.64 

0.0001306375488638878
0.00012419374252203852
Batch: 0; loss: 1.21; acc: 0.69
Batch: 20; loss: 1.38; acc: 0.58
Batch: 40; loss: 0.93; acc: 0.78
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 1.23; acc: 0.72
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 1.0; acc: 0.81
Val Epoch over. val_loss: 1.2031797474356973; val_accuracy: 0.6642117834394905 

The current subspace-distance is: 0.00012419374252203852 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.25; acc: 0.58
Batch: 40; loss: 1.12; acc: 0.72
Batch: 60; loss: 1.35; acc: 0.59
Batch: 80; loss: 1.26; acc: 0.66
Batch: 100; loss: 1.43; acc: 0.55
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 1.26; acc: 0.56
Batch: 160; loss: 1.45; acc: 0.55
Batch: 180; loss: 1.29; acc: 0.59
Batch: 200; loss: 1.18; acc: 0.7
Batch: 220; loss: 1.25; acc: 0.73
Batch: 240; loss: 1.29; acc: 0.64
Batch: 260; loss: 1.14; acc: 0.69
Batch: 280; loss: 1.43; acc: 0.61
Batch: 300; loss: 1.36; acc: 0.58
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 1.3; acc: 0.58
Batch: 380; loss: 1.2; acc: 0.67
Batch: 400; loss: 1.31; acc: 0.58
Batch: 420; loss: 1.37; acc: 0.59
Batch: 440; loss: 1.4; acc: 0.56
Batch: 460; loss: 1.1; acc: 0.66
Batch: 480; loss: 1.15; acc: 0.67
Batch: 500; loss: 1.3; acc: 0.62
Batch: 520; loss: 1.33; acc: 0.55
Batch: 540; loss: 1.35; acc: 0.59
Batch: 560; loss: 1.4; acc: 0.55
Batch: 580; loss: 1.2; acc: 0.7
Batch: 600; loss: 1.15; acc: 0.62
Batch: 620; loss: 1.16; acc: 0.64
Batch: 640; loss: 1.19; acc: 0.66
Batch: 660; loss: 1.25; acc: 0.66
Batch: 680; loss: 1.24; acc: 0.64
Batch: 700; loss: 1.16; acc: 0.66
Batch: 720; loss: 1.21; acc: 0.62
Batch: 740; loss: 1.26; acc: 0.58
Batch: 760; loss: 1.14; acc: 0.77
Batch: 780; loss: 1.35; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.64 

0.00014322329661808908
0.00013630714965984225
Batch: 0; loss: 1.18; acc: 0.69
Batch: 20; loss: 1.35; acc: 0.56
Batch: 40; loss: 0.89; acc: 0.78
Batch: 60; loss: 1.21; acc: 0.62
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.18; acc: 0.77
Batch: 120; loss: 1.35; acc: 0.62
Batch: 140; loss: 0.94; acc: 0.83
Val Epoch over. val_loss: 1.170795855628457; val_accuracy: 0.676453025477707 

The current subspace-distance is: 0.00013630714965984225 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.64
Batch: 20; loss: 1.34; acc: 0.53
Batch: 40; loss: 1.2; acc: 0.61
Batch: 60; loss: 1.26; acc: 0.64
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 1.28; acc: 0.66
Batch: 120; loss: 1.14; acc: 0.69
Batch: 140; loss: 1.22; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.66
Batch: 180; loss: 1.15; acc: 0.72
Batch: 200; loss: 1.13; acc: 0.69
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.2; acc: 0.62
Batch: 260; loss: 1.34; acc: 0.56
Batch: 280; loss: 1.28; acc: 0.62
Batch: 300; loss: 1.19; acc: 0.62
Batch: 320; loss: 1.23; acc: 0.64
Batch: 340; loss: 1.21; acc: 0.62
Batch: 360; loss: 1.45; acc: 0.5
Batch: 380; loss: 1.26; acc: 0.66
Batch: 400; loss: 1.3; acc: 0.59
Batch: 420; loss: 1.27; acc: 0.66
Batch: 440; loss: 1.07; acc: 0.69
Batch: 460; loss: 1.31; acc: 0.52
Batch: 480; loss: 1.2; acc: 0.59
Batch: 500; loss: 1.2; acc: 0.56
Batch: 520; loss: 1.32; acc: 0.61
Batch: 540; loss: 1.01; acc: 0.8
Batch: 560; loss: 1.2; acc: 0.61
Batch: 580; loss: 1.13; acc: 0.66
Batch: 600; loss: 1.2; acc: 0.69
Batch: 620; loss: 1.18; acc: 0.7
Batch: 640; loss: 1.28; acc: 0.62
Batch: 660; loss: 1.19; acc: 0.67
Batch: 680; loss: 1.12; acc: 0.69
Batch: 700; loss: 1.12; acc: 0.77
Batch: 720; loss: 1.33; acc: 0.62
Batch: 740; loss: 1.15; acc: 0.66
Batch: 760; loss: 1.12; acc: 0.67
Batch: 780; loss: 1.21; acc: 0.64
Train Epoch over. train_loss: 1.22; train_accuracy: 0.64 

0.00014946756709832698
0.00014281085168477148
Batch: 0; loss: 1.17; acc: 0.66
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.16; acc: 0.78
Batch: 120; loss: 1.33; acc: 0.61
Batch: 140; loss: 0.93; acc: 0.81
Val Epoch over. val_loss: 1.149687393076101; val_accuracy: 0.675656847133758 

The current subspace-distance is: 0.00014281085168477148 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.72
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 1.26; acc: 0.62
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.34; acc: 0.59
Batch: 100; loss: 1.36; acc: 0.59
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.1; acc: 0.66
Batch: 160; loss: 1.18; acc: 0.66
Batch: 180; loss: 1.32; acc: 0.64
Batch: 200; loss: 1.25; acc: 0.66
Batch: 220; loss: 1.19; acc: 0.61
Batch: 240; loss: 1.2; acc: 0.59
Batch: 260; loss: 1.13; acc: 0.69
Batch: 280; loss: 1.12; acc: 0.7
Batch: 300; loss: 1.17; acc: 0.67
Batch: 320; loss: 1.13; acc: 0.69
Batch: 340; loss: 1.3; acc: 0.61
Batch: 360; loss: 1.17; acc: 0.69
Batch: 380; loss: 1.12; acc: 0.73
Batch: 400; loss: 1.24; acc: 0.7
Batch: 420; loss: 1.07; acc: 0.75
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.1; acc: 0.64
Batch: 480; loss: 1.05; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 1.42; acc: 0.5
Batch: 560; loss: 1.2; acc: 0.69
Batch: 580; loss: 1.39; acc: 0.48
Batch: 600; loss: 0.99; acc: 0.75
Batch: 620; loss: 1.13; acc: 0.62
Batch: 640; loss: 1.21; acc: 0.67
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 1.07; acc: 0.69
Batch: 700; loss: 1.16; acc: 0.7
Batch: 720; loss: 1.22; acc: 0.7
Batch: 740; loss: 1.12; acc: 0.7
Batch: 760; loss: 1.38; acc: 0.45
Batch: 780; loss: 1.27; acc: 0.59
Train Epoch over. train_loss: 1.2; train_accuracy: 0.65 

0.00016159068036358804
0.00015486107440665364
Batch: 0; loss: 1.14; acc: 0.67
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.84; acc: 0.77
Batch: 60; loss: 1.18; acc: 0.69
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 1.12; acc: 0.77
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 0.89; acc: 0.83
Val Epoch over. val_loss: 1.1286375412515774; val_accuracy: 0.6796377388535032 

The current subspace-distance is: 0.00015486107440665364 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.34; acc: 0.56
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 1.35; acc: 0.56
Batch: 100; loss: 1.35; acc: 0.66
Batch: 120; loss: 1.07; acc: 0.72
Batch: 140; loss: 1.31; acc: 0.62
Batch: 160; loss: 1.23; acc: 0.67
Batch: 180; loss: 1.33; acc: 0.56
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 1.42; acc: 0.58
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.2; acc: 0.67
Batch: 280; loss: 1.16; acc: 0.67
Batch: 300; loss: 1.5; acc: 0.53
Batch: 320; loss: 1.26; acc: 0.61
Batch: 340; loss: 1.09; acc: 0.67
Batch: 360; loss: 1.22; acc: 0.7
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.32; acc: 0.55
Batch: 420; loss: 1.08; acc: 0.67
Batch: 440; loss: 1.27; acc: 0.56
Batch: 460; loss: 1.32; acc: 0.64
Batch: 480; loss: 1.14; acc: 0.64
Batch: 500; loss: 1.0; acc: 0.81
Batch: 520; loss: 1.13; acc: 0.77
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.14; acc: 0.64
Batch: 580; loss: 1.04; acc: 0.72
Batch: 600; loss: 1.39; acc: 0.59
Batch: 620; loss: 1.09; acc: 0.69
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.35; acc: 0.53
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.16; acc: 0.64
Batch: 720; loss: 1.08; acc: 0.7
Batch: 740; loss: 1.26; acc: 0.62
Batch: 760; loss: 1.17; acc: 0.66
Batch: 780; loss: 1.23; acc: 0.64
Train Epoch over. train_loss: 1.18; train_accuracy: 0.65 

0.00016312235675286502
0.00015497126150876284
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.78
Batch: 120; loss: 1.3; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.81
Val Epoch over. val_loss: 1.1158752733734763; val_accuracy: 0.6872014331210191 

The current subspace-distance is: 0.00015497126150876284 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.06; acc: 0.73
Batch: 40; loss: 1.17; acc: 0.62
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 1.14; acc: 0.62
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.28; acc: 0.58
Batch: 140; loss: 1.24; acc: 0.7
Batch: 160; loss: 1.17; acc: 0.72
Batch: 180; loss: 1.21; acc: 0.73
Batch: 200; loss: 1.08; acc: 0.77
Batch: 220; loss: 1.16; acc: 0.69
Batch: 240; loss: 1.25; acc: 0.58
Batch: 260; loss: 1.27; acc: 0.61
Batch: 280; loss: 1.26; acc: 0.64
Batch: 300; loss: 1.21; acc: 0.67
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.05; acc: 0.75
Batch: 360; loss: 1.14; acc: 0.7
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 1.23; acc: 0.62
Batch: 420; loss: 1.08; acc: 0.7
Batch: 440; loss: 1.09; acc: 0.69
Batch: 460; loss: 1.2; acc: 0.66
Batch: 480; loss: 1.1; acc: 0.7
Batch: 500; loss: 1.11; acc: 0.67
Batch: 520; loss: 1.29; acc: 0.61
Batch: 540; loss: 1.31; acc: 0.64
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 1.15; acc: 0.69
Batch: 600; loss: 1.21; acc: 0.66
Batch: 620; loss: 1.21; acc: 0.64
Batch: 640; loss: 1.17; acc: 0.58
Batch: 660; loss: 1.29; acc: 0.59
Batch: 680; loss: 1.21; acc: 0.59
Batch: 700; loss: 1.06; acc: 0.77
Batch: 720; loss: 1.17; acc: 0.61
Batch: 740; loss: 1.19; acc: 0.66
Batch: 760; loss: 1.31; acc: 0.56
Batch: 780; loss: 1.17; acc: 0.64
Train Epoch over. train_loss: 1.18; train_accuracy: 0.65 

0.00016366210184060037
0.00015940195589791983
Batch: 0; loss: 1.14; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.98; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 0.87; acc: 0.81
Val Epoch over. val_loss: 1.1106619443863062; val_accuracy: 0.6903861464968153 

The current subspace-distance is: 0.00015940195589791983 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.34; acc: 0.56
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.77
Batch: 100; loss: 1.18; acc: 0.61
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 1.12; acc: 0.77
Batch: 160; loss: 1.09; acc: 0.69
Batch: 180; loss: 1.26; acc: 0.61
Batch: 200; loss: 1.1; acc: 0.7
Batch: 220; loss: 1.3; acc: 0.64
Batch: 240; loss: 1.15; acc: 0.66
Batch: 260; loss: 1.11; acc: 0.62
Batch: 280; loss: 1.23; acc: 0.56
Batch: 300; loss: 1.14; acc: 0.69
Batch: 320; loss: 1.21; acc: 0.64
Batch: 340; loss: 1.3; acc: 0.62
Batch: 360; loss: 1.08; acc: 0.69
Batch: 380; loss: 1.29; acc: 0.61
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 1.04; acc: 0.7
Batch: 440; loss: 1.11; acc: 0.59
Batch: 460; loss: 1.1; acc: 0.67
Batch: 480; loss: 1.29; acc: 0.58
Batch: 500; loss: 1.27; acc: 0.62
Batch: 520; loss: 1.1; acc: 0.62
Batch: 540; loss: 1.24; acc: 0.58
Batch: 560; loss: 1.27; acc: 0.61
Batch: 580; loss: 1.23; acc: 0.58
Batch: 600; loss: 1.16; acc: 0.69
Batch: 620; loss: 1.18; acc: 0.69
Batch: 640; loss: 1.1; acc: 0.69
Batch: 660; loss: 1.08; acc: 0.7
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.29; acc: 0.61
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 1.34; acc: 0.58
Batch: 760; loss: 1.07; acc: 0.69
Batch: 780; loss: 1.3; acc: 0.61
Train Epoch over. train_loss: 1.17; train_accuracy: 0.66 

0.00017187598859891295
0.00016431821859441698
Batch: 0; loss: 1.12; acc: 0.7
Batch: 20; loss: 1.24; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 1.11; acc: 0.77
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 0.85; acc: 0.8
Val Epoch over. val_loss: 1.09843996375989; val_accuracy: 0.6922770700636943 

The current subspace-distance is: 0.00016431821859441698 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.64
Batch: 20; loss: 1.26; acc: 0.61
Batch: 40; loss: 1.14; acc: 0.66
Batch: 60; loss: 1.21; acc: 0.59
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.07; acc: 0.62
Batch: 160; loss: 1.33; acc: 0.52
Batch: 180; loss: 1.09; acc: 0.7
Batch: 200; loss: 1.31; acc: 0.59
Batch: 220; loss: 1.11; acc: 0.73
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.15; acc: 0.64
Batch: 280; loss: 1.26; acc: 0.56
Batch: 300; loss: 1.23; acc: 0.59
Batch: 320; loss: 1.03; acc: 0.73
Batch: 340; loss: 1.19; acc: 0.64
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.26; acc: 0.64
Batch: 400; loss: 0.99; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.7
Batch: 440; loss: 1.06; acc: 0.75
Batch: 460; loss: 1.16; acc: 0.67
Batch: 480; loss: 1.1; acc: 0.75
Batch: 500; loss: 1.13; acc: 0.67
Batch: 520; loss: 1.29; acc: 0.64
Batch: 540; loss: 1.08; acc: 0.67
Batch: 560; loss: 1.37; acc: 0.58
Batch: 580; loss: 1.05; acc: 0.72
Batch: 600; loss: 1.26; acc: 0.58
Batch: 620; loss: 1.02; acc: 0.7
Batch: 640; loss: 1.29; acc: 0.62
Batch: 660; loss: 1.07; acc: 0.8
Batch: 680; loss: 1.12; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.23; acc: 0.64
Batch: 760; loss: 1.34; acc: 0.56
Batch: 780; loss: 1.2; acc: 0.64
Train Epoch over. train_loss: 1.16; train_accuracy: 0.66 

0.00017251327517442405
0.00016738622798584402
Batch: 0; loss: 1.13; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 1.14; acc: 0.64
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.75
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 0.85; acc: 0.83
Val Epoch over. val_loss: 1.0949774502189296; val_accuracy: 0.6949641719745223 

The current subspace-distance is: 0.00016738622798584402 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.64
Batch: 20; loss: 1.11; acc: 0.7
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 1.04; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 1.23; acc: 0.62
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 1.12; acc: 0.72
Batch: 160; loss: 1.02; acc: 0.77
Batch: 180; loss: 1.13; acc: 0.66
Batch: 200; loss: 1.34; acc: 0.52
Batch: 220; loss: 1.14; acc: 0.69
Batch: 240; loss: 1.08; acc: 0.61
Batch: 260; loss: 1.26; acc: 0.61
Batch: 280; loss: 1.19; acc: 0.64
Batch: 300; loss: 1.18; acc: 0.62
Batch: 320; loss: 1.11; acc: 0.73
Batch: 340; loss: 1.32; acc: 0.58
Batch: 360; loss: 1.26; acc: 0.53
Batch: 380; loss: 1.1; acc: 0.73
Batch: 400; loss: 1.1; acc: 0.72
Batch: 420; loss: 0.93; acc: 0.78
Batch: 440; loss: 1.1; acc: 0.73
Batch: 460; loss: 1.18; acc: 0.58
Batch: 480; loss: 1.12; acc: 0.66
Batch: 500; loss: 1.12; acc: 0.64
Batch: 520; loss: 1.19; acc: 0.62
Batch: 540; loss: 1.22; acc: 0.61
Batch: 560; loss: 1.14; acc: 0.64
Batch: 580; loss: 1.0; acc: 0.78
Batch: 600; loss: 0.99; acc: 0.8
Batch: 620; loss: 1.03; acc: 0.69
Batch: 640; loss: 1.04; acc: 0.73
Batch: 660; loss: 0.97; acc: 0.8
Batch: 680; loss: 1.11; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.64
Batch: 720; loss: 1.2; acc: 0.7
Batch: 740; loss: 1.15; acc: 0.77
Batch: 760; loss: 1.08; acc: 0.66
Batch: 780; loss: 1.11; acc: 0.75
Train Epoch over. train_loss: 1.15; train_accuracy: 0.66 

0.00017780769849196076
0.00016904514632187784
Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.64
Batch: 40; loss: 0.8; acc: 0.78
Batch: 60; loss: 1.14; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.0858321573324263; val_accuracy: 0.6976512738853503 

The current subspace-distance is: 0.00016904514632187784 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.67
Batch: 60; loss: 1.24; acc: 0.59
Batch: 80; loss: 1.38; acc: 0.55
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.07; acc: 0.73
Batch: 140; loss: 1.21; acc: 0.62
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 1.23; acc: 0.59
Batch: 200; loss: 1.21; acc: 0.7
Batch: 220; loss: 1.11; acc: 0.69
Batch: 240; loss: 1.13; acc: 0.61
Batch: 260; loss: 1.12; acc: 0.73
Batch: 280; loss: 1.1; acc: 0.72
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.34; acc: 0.59
Batch: 360; loss: 1.39; acc: 0.53
Batch: 380; loss: 1.14; acc: 0.59
Batch: 400; loss: 1.2; acc: 0.66
Batch: 420; loss: 1.16; acc: 0.72
Batch: 440; loss: 1.07; acc: 0.66
Batch: 460; loss: 1.2; acc: 0.62
Batch: 480; loss: 1.12; acc: 0.67
Batch: 500; loss: 1.29; acc: 0.56
Batch: 520; loss: 1.25; acc: 0.56
Batch: 540; loss: 1.33; acc: 0.62
Batch: 560; loss: 1.24; acc: 0.58
Batch: 580; loss: 1.15; acc: 0.66
Batch: 600; loss: 1.2; acc: 0.66
Batch: 620; loss: 1.24; acc: 0.59
Batch: 640; loss: 1.11; acc: 0.67
Batch: 660; loss: 1.06; acc: 0.72
Batch: 680; loss: 1.35; acc: 0.56
Batch: 700; loss: 1.19; acc: 0.61
Batch: 720; loss: 1.17; acc: 0.59
Batch: 740; loss: 1.21; acc: 0.58
Batch: 760; loss: 1.16; acc: 0.62
Batch: 780; loss: 1.11; acc: 0.7
Train Epoch over. train_loss: 1.14; train_accuracy: 0.67 

0.00017502123955637217
0.00016600255912635475
Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 0.79; acc: 0.78
Batch: 60; loss: 1.14; acc: 0.62
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.81
Val Epoch over. val_loss: 1.0875834815061776; val_accuracy: 0.6928742038216561 

The current subspace-distance is: 0.00016600255912635475 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.05; acc: 0.72
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 1.15; acc: 0.67
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 1.19; acc: 0.61
Batch: 120; loss: 1.16; acc: 0.72
Batch: 140; loss: 1.12; acc: 0.69
Batch: 160; loss: 1.12; acc: 0.67
Batch: 180; loss: 1.09; acc: 0.75
Batch: 200; loss: 1.18; acc: 0.61
Batch: 220; loss: 1.19; acc: 0.66
Batch: 240; loss: 1.01; acc: 0.72
Batch: 260; loss: 1.11; acc: 0.72
Batch: 280; loss: 1.02; acc: 0.73
Batch: 300; loss: 1.22; acc: 0.62
Batch: 320; loss: 1.16; acc: 0.7
Batch: 340; loss: 1.15; acc: 0.73
Batch: 360; loss: 1.18; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.67
Batch: 400; loss: 1.19; acc: 0.64
Batch: 420; loss: 1.08; acc: 0.66
Batch: 440; loss: 1.29; acc: 0.58
Batch: 460; loss: 1.08; acc: 0.69
Batch: 480; loss: 1.01; acc: 0.67
Batch: 500; loss: 1.08; acc: 0.67
Batch: 520; loss: 1.35; acc: 0.5
Batch: 540; loss: 1.09; acc: 0.66
Batch: 560; loss: 1.27; acc: 0.66
Batch: 580; loss: 1.17; acc: 0.67
Batch: 600; loss: 0.96; acc: 0.8
Batch: 620; loss: 1.09; acc: 0.69
Batch: 640; loss: 1.27; acc: 0.66
Batch: 660; loss: 1.22; acc: 0.67
Batch: 680; loss: 1.13; acc: 0.69
Batch: 700; loss: 1.15; acc: 0.72
Batch: 720; loss: 1.19; acc: 0.62
Batch: 740; loss: 1.05; acc: 0.72
Batch: 760; loss: 1.04; acc: 0.64
Batch: 780; loss: 1.18; acc: 0.69
Train Epoch over. train_loss: 1.14; train_accuracy: 0.67 

0.0001820194156607613
0.00017444555123802274
Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 0.91; acc: 0.75
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.82; acc: 0.8
Val Epoch over. val_loss: 1.0674901202226141; val_accuracy: 0.7025278662420382 

The current subspace-distance is: 0.00017444555123802274 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.07; acc: 0.77
Batch: 40; loss: 1.01; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.67
Batch: 120; loss: 1.12; acc: 0.72
Batch: 140; loss: 1.04; acc: 0.66
Batch: 160; loss: 1.32; acc: 0.58
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 1.15; acc: 0.66
Batch: 220; loss: 1.09; acc: 0.69
Batch: 240; loss: 0.94; acc: 0.73
Batch: 260; loss: 1.15; acc: 0.59
Batch: 280; loss: 1.08; acc: 0.69
Batch: 300; loss: 1.05; acc: 0.7
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.22; acc: 0.66
Batch: 360; loss: 1.15; acc: 0.64
Batch: 380; loss: 1.32; acc: 0.55
Batch: 400; loss: 1.22; acc: 0.58
Batch: 420; loss: 1.21; acc: 0.55
Batch: 440; loss: 1.08; acc: 0.69
Batch: 460; loss: 1.11; acc: 0.66
Batch: 480; loss: 1.14; acc: 0.66
Batch: 500; loss: 1.33; acc: 0.58
Batch: 520; loss: 1.08; acc: 0.7
Batch: 540; loss: 1.09; acc: 0.62
Batch: 560; loss: 1.06; acc: 0.7
Batch: 580; loss: 1.06; acc: 0.69
Batch: 600; loss: 1.06; acc: 0.77
Batch: 620; loss: 1.09; acc: 0.67
Batch: 640; loss: 1.11; acc: 0.62
Batch: 660; loss: 1.09; acc: 0.69
Batch: 680; loss: 1.11; acc: 0.64
Batch: 700; loss: 1.2; acc: 0.7
Batch: 720; loss: 1.31; acc: 0.58
Batch: 740; loss: 1.11; acc: 0.58
Batch: 760; loss: 1.12; acc: 0.75
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 1.13; train_accuracy: 0.67 

0.00018642783106770366
0.0001791465765563771
Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 1.17; acc: 0.67
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 1.11; acc: 0.62
Batch: 80; loss: 0.9; acc: 0.75
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 1.0587813725137407; val_accuracy: 0.7075039808917197 

The current subspace-distance is: 0.0001791465765563771 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.38; acc: 0.58
Batch: 20; loss: 1.22; acc: 0.58
Batch: 40; loss: 1.02; acc: 0.78
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 1.16; acc: 0.67
Batch: 100; loss: 1.15; acc: 0.59
Batch: 120; loss: 1.16; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.64
Batch: 160; loss: 1.27; acc: 0.67
Batch: 180; loss: 0.97; acc: 0.73
Batch: 200; loss: 1.09; acc: 0.72
Batch: 220; loss: 0.96; acc: 0.73
Batch: 240; loss: 1.13; acc: 0.7
Batch: 260; loss: 1.18; acc: 0.67
Batch: 280; loss: 1.12; acc: 0.62
Batch: 300; loss: 1.21; acc: 0.61
Batch: 320; loss: 1.18; acc: 0.69
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 1.22; acc: 0.58
Batch: 380; loss: 1.21; acc: 0.56
Batch: 400; loss: 0.95; acc: 0.78
Batch: 420; loss: 1.11; acc: 0.75
Batch: 440; loss: 0.95; acc: 0.8
Batch: 460; loss: 1.04; acc: 0.67
Batch: 480; loss: 1.34; acc: 0.62
Batch: 500; loss: 1.27; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.7
Batch: 540; loss: 1.15; acc: 0.69
Batch: 560; loss: 1.12; acc: 0.7
Batch: 580; loss: 1.05; acc: 0.61
Batch: 600; loss: 1.12; acc: 0.69
Batch: 620; loss: 1.03; acc: 0.72
Batch: 640; loss: 1.09; acc: 0.7
Batch: 660; loss: 1.3; acc: 0.62
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 1.0; acc: 0.67
Batch: 720; loss: 1.16; acc: 0.58
Batch: 740; loss: 1.03; acc: 0.7
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 1.19; acc: 0.59
Train Epoch over. train_loss: 1.12; train_accuracy: 0.68 

0.0001897061156341806
0.00018481102597434074
Batch: 0; loss: 1.1; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 1.09; acc: 0.67
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 0.79; acc: 0.81
Val Epoch over. val_loss: 1.042997725070662; val_accuracy: 0.713077229299363 

The current subspace-distance is: 0.00018481102597434074 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.28; acc: 0.53
Batch: 40; loss: 1.06; acc: 0.67
Batch: 60; loss: 1.26; acc: 0.64
Batch: 80; loss: 1.11; acc: 0.7
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 1.33; acc: 0.55
Batch: 140; loss: 1.19; acc: 0.67
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 0.97; acc: 0.72
Batch: 220; loss: 1.05; acc: 0.7
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.09; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.72
Batch: 300; loss: 1.18; acc: 0.62
Batch: 320; loss: 1.08; acc: 0.7
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 1.18; acc: 0.59
Batch: 380; loss: 1.36; acc: 0.62
Batch: 400; loss: 1.12; acc: 0.62
Batch: 420; loss: 0.96; acc: 0.78
Batch: 440; loss: 0.93; acc: 0.75
Batch: 460; loss: 0.94; acc: 0.75
Batch: 480; loss: 1.41; acc: 0.59
Batch: 500; loss: 1.23; acc: 0.67
Batch: 520; loss: 0.96; acc: 0.78
Batch: 540; loss: 1.15; acc: 0.73
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 0.97; acc: 0.77
Batch: 600; loss: 1.13; acc: 0.69
Batch: 620; loss: 1.1; acc: 0.66
Batch: 640; loss: 0.97; acc: 0.81
Batch: 660; loss: 1.03; acc: 0.66
Batch: 680; loss: 1.05; acc: 0.72
Batch: 700; loss: 1.2; acc: 0.64
Batch: 720; loss: 1.28; acc: 0.58
Batch: 740; loss: 1.21; acc: 0.59
Batch: 760; loss: 1.14; acc: 0.67
Batch: 780; loss: 1.28; acc: 0.61
Train Epoch over. train_loss: 1.11; train_accuracy: 0.68 

0.00019080466881860048
0.00018379413813818246
Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 1.1; acc: 0.67
Batch: 80; loss: 0.89; acc: 0.73
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 1.0451048783435943; val_accuracy: 0.713077229299363 

The current subspace-distance is: 0.00018379413813818246 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 1.1; acc: 0.69
Batch: 40; loss: 1.24; acc: 0.58
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.03; acc: 0.75
Batch: 140; loss: 1.13; acc: 0.69
Batch: 160; loss: 1.08; acc: 0.75
Batch: 180; loss: 1.03; acc: 0.72
Batch: 200; loss: 1.17; acc: 0.62
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 1.18; acc: 0.66
Batch: 260; loss: 1.16; acc: 0.66
Batch: 280; loss: 1.17; acc: 0.61
Batch: 300; loss: 1.29; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.59
Batch: 340; loss: 1.2; acc: 0.66
Batch: 360; loss: 1.2; acc: 0.61
Batch: 380; loss: 1.25; acc: 0.62
Batch: 400; loss: 1.07; acc: 0.8
Batch: 420; loss: 1.17; acc: 0.67
Batch: 440; loss: 0.91; acc: 0.81
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.47; acc: 0.53
Batch: 500; loss: 1.08; acc: 0.73
Batch: 520; loss: 1.19; acc: 0.59
Batch: 540; loss: 1.1; acc: 0.7
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.09; acc: 0.73
Batch: 600; loss: 1.14; acc: 0.64
Batch: 620; loss: 1.1; acc: 0.66
Batch: 640; loss: 1.2; acc: 0.67
Batch: 660; loss: 1.07; acc: 0.73
Batch: 680; loss: 1.22; acc: 0.58
Batch: 700; loss: 1.06; acc: 0.67
Batch: 720; loss: 1.12; acc: 0.72
Batch: 740; loss: 1.09; acc: 0.69
Batch: 760; loss: 1.09; acc: 0.75
Batch: 780; loss: 1.15; acc: 0.69
Train Epoch over. train_loss: 1.11; train_accuracy: 0.68 

0.0001891607535071671
0.00018184618966188282
Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 1.06; acc: 0.8
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 0.76; acc: 0.84
Val Epoch over. val_loss: 1.034076437828647; val_accuracy: 0.7171576433121019 

The current subspace-distance is: 0.00018184618966188282 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.04; acc: 0.69
Batch: 80; loss: 1.12; acc: 0.66
Batch: 100; loss: 0.98; acc: 0.73
Batch: 120; loss: 1.12; acc: 0.67
Batch: 140; loss: 1.12; acc: 0.69
Batch: 160; loss: 1.04; acc: 0.75
Batch: 180; loss: 1.1; acc: 0.64
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.2; acc: 0.61
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 1.19; acc: 0.7
Batch: 280; loss: 1.16; acc: 0.69
Batch: 300; loss: 1.04; acc: 0.7
Batch: 320; loss: 1.03; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.8
Batch: 360; loss: 1.11; acc: 0.69
Batch: 380; loss: 1.24; acc: 0.62
Batch: 400; loss: 1.12; acc: 0.64
Batch: 420; loss: 1.03; acc: 0.69
Batch: 440; loss: 1.07; acc: 0.7
Batch: 460; loss: 1.03; acc: 0.67
Batch: 480; loss: 1.03; acc: 0.69
Batch: 500; loss: 1.04; acc: 0.69
Batch: 520; loss: 1.15; acc: 0.66
Batch: 540; loss: 1.19; acc: 0.64
Batch: 560; loss: 0.95; acc: 0.77
Batch: 580; loss: 0.97; acc: 0.75
Batch: 600; loss: 1.19; acc: 0.62
Batch: 620; loss: 1.26; acc: 0.61
Batch: 640; loss: 1.28; acc: 0.58
Batch: 660; loss: 1.25; acc: 0.61
Batch: 680; loss: 1.11; acc: 0.72
Batch: 700; loss: 1.02; acc: 0.72
Batch: 720; loss: 1.23; acc: 0.52
Batch: 740; loss: 1.22; acc: 0.58
Batch: 760; loss: 1.28; acc: 0.53
Batch: 780; loss: 1.02; acc: 0.64
Train Epoch over. train_loss: 1.1; train_accuracy: 0.68 

0.0001926115364767611
0.00018523620383348316
Batch: 0; loss: 1.11; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 0.91; acc: 0.78
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 0.78; acc: 0.83
Val Epoch over. val_loss: 1.0399149663888725; val_accuracy: 0.7128781847133758 

The current subspace-distance is: 0.00018523620383348316 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.21; acc: 0.58
Batch: 40; loss: 0.98; acc: 0.75
Batch: 60; loss: 1.17; acc: 0.69
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 1.14; acc: 0.66
Batch: 160; loss: 1.11; acc: 0.77
Batch: 180; loss: 1.1; acc: 0.7
Batch: 200; loss: 1.17; acc: 0.58
Batch: 220; loss: 1.07; acc: 0.62
Batch: 240; loss: 0.99; acc: 0.72
Batch: 260; loss: 1.3; acc: 0.55
Batch: 280; loss: 1.08; acc: 0.62
Batch: 300; loss: 0.92; acc: 0.77
Batch: 320; loss: 1.05; acc: 0.78
Batch: 340; loss: 1.12; acc: 0.67
Batch: 360; loss: 1.04; acc: 0.75
Batch: 380; loss: 0.96; acc: 0.77
Batch: 400; loss: 1.09; acc: 0.72
Batch: 420; loss: 1.12; acc: 0.66
Batch: 440; loss: 1.2; acc: 0.59
Batch: 460; loss: 1.31; acc: 0.56
Batch: 480; loss: 1.1; acc: 0.67
Batch: 500; loss: 1.02; acc: 0.7
Batch: 520; loss: 1.13; acc: 0.7
Batch: 540; loss: 1.02; acc: 0.72
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.61
Batch: 600; loss: 1.18; acc: 0.62
Batch: 620; loss: 1.1; acc: 0.69
Batch: 640; loss: 1.02; acc: 0.73
Batch: 660; loss: 1.22; acc: 0.59
Batch: 680; loss: 1.04; acc: 0.59
Batch: 700; loss: 1.15; acc: 0.66
Batch: 720; loss: 1.05; acc: 0.72
Batch: 740; loss: 1.17; acc: 0.58
Batch: 760; loss: 1.06; acc: 0.72
Batch: 780; loss: 1.12; acc: 0.69
Train Epoch over. train_loss: 1.1; train_accuracy: 0.68 

0.0001915886386996135
0.00018284772522747517
Batch: 0; loss: 1.11; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 1.09; acc: 0.67
Batch: 80; loss: 0.88; acc: 0.8
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 0.76; acc: 0.81
Val Epoch over. val_loss: 1.0398691140922012; val_accuracy: 0.7184514331210191 

The current subspace-distance is: 0.00018284772522747517 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.1; acc: 0.67
Batch: 40; loss: 1.19; acc: 0.59
Batch: 60; loss: 1.13; acc: 0.67
Batch: 80; loss: 1.25; acc: 0.61
Batch: 100; loss: 1.17; acc: 0.62
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.07; acc: 0.67
Batch: 180; loss: 1.17; acc: 0.61
Batch: 200; loss: 1.19; acc: 0.61
Batch: 220; loss: 1.02; acc: 0.69
Batch: 240; loss: 1.15; acc: 0.62
Batch: 260; loss: 1.07; acc: 0.66
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 1.0; acc: 0.7
Batch: 320; loss: 1.07; acc: 0.67
Batch: 340; loss: 1.04; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.69
Batch: 380; loss: 1.14; acc: 0.66
Batch: 400; loss: 1.0; acc: 0.73
Batch: 420; loss: 1.06; acc: 0.67
Batch: 440; loss: 1.14; acc: 0.7
Batch: 460; loss: 1.03; acc: 0.7
Batch: 480; loss: 1.06; acc: 0.73
Batch: 500; loss: 1.05; acc: 0.75
Batch: 520; loss: 1.24; acc: 0.56
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 1.03; acc: 0.7
Batch: 580; loss: 1.24; acc: 0.66
Batch: 600; loss: 1.08; acc: 0.66
Batch: 620; loss: 1.13; acc: 0.67
Batch: 640; loss: 0.93; acc: 0.77
Batch: 660; loss: 0.98; acc: 0.73
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 1.09; acc: 0.8
Batch: 720; loss: 1.16; acc: 0.67
Batch: 740; loss: 1.15; acc: 0.58
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 0.9; acc: 0.78
Train Epoch over. train_loss: 1.1; train_accuracy: 0.68 

0.00019374594558030367
0.00018768240988720208
Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.14; acc: 0.69
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.66
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.67
Batch: 140; loss: 0.75; acc: 0.83
Val Epoch over. val_loss: 1.027178084015087; val_accuracy: 0.7204418789808917 

The current subspace-distance is: 0.00018768240988720208 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.69
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 1.21; acc: 0.61
Batch: 80; loss: 1.21; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 1.08; acc: 0.67
Batch: 160; loss: 1.13; acc: 0.64
Batch: 180; loss: 1.03; acc: 0.73
Batch: 200; loss: 1.38; acc: 0.53
Batch: 220; loss: 0.92; acc: 0.81
Batch: 240; loss: 1.21; acc: 0.59
Batch: 260; loss: 1.16; acc: 0.61
Batch: 280; loss: 1.23; acc: 0.53
Batch: 300; loss: 0.95; acc: 0.75
Batch: 320; loss: 0.97; acc: 0.78
Batch: 340; loss: 0.97; acc: 0.73
Batch: 360; loss: 1.12; acc: 0.72
Batch: 380; loss: 1.19; acc: 0.66
Batch: 400; loss: 1.08; acc: 0.7
Batch: 420; loss: 1.1; acc: 0.67
Batch: 440; loss: 0.96; acc: 0.69
Batch: 460; loss: 1.1; acc: 0.66
Batch: 480; loss: 1.05; acc: 0.69
Batch: 500; loss: 0.97; acc: 0.72
Batch: 520; loss: 1.05; acc: 0.69
Batch: 540; loss: 1.21; acc: 0.62
Batch: 560; loss: 1.11; acc: 0.66
Batch: 580; loss: 1.09; acc: 0.66
Batch: 600; loss: 1.06; acc: 0.69
Batch: 620; loss: 1.35; acc: 0.58
Batch: 640; loss: 1.09; acc: 0.67
Batch: 660; loss: 1.13; acc: 0.73
Batch: 680; loss: 1.03; acc: 0.7
Batch: 700; loss: 1.08; acc: 0.69
Batch: 720; loss: 1.14; acc: 0.69
Batch: 740; loss: 1.15; acc: 0.66
Batch: 760; loss: 1.08; acc: 0.69
Batch: 780; loss: 1.13; acc: 0.62
Train Epoch over. train_loss: 1.1; train_accuracy: 0.68 

0.00019462211639620364
0.00018750799063127488
Batch: 0; loss: 1.13; acc: 0.61
Batch: 20; loss: 1.17; acc: 0.69
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 1.09; acc: 0.66
Batch: 80; loss: 0.89; acc: 0.78
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.66
Batch: 140; loss: 0.77; acc: 0.84
Val Epoch over. val_loss: 1.0400090737707297; val_accuracy: 0.7167595541401274 

The current subspace-distance is: 0.00018750799063127488 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.94; acc: 0.75
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 1.17; acc: 0.64
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.17; acc: 0.59
Batch: 140; loss: 1.29; acc: 0.58
Batch: 160; loss: 1.06; acc: 0.69
Batch: 180; loss: 1.0; acc: 0.72
Batch: 200; loss: 1.07; acc: 0.67
Batch: 220; loss: 1.17; acc: 0.66
Batch: 240; loss: 1.07; acc: 0.72
Batch: 260; loss: 0.96; acc: 0.72
Batch: 280; loss: 1.1; acc: 0.7
Batch: 300; loss: 1.02; acc: 0.75
Batch: 320; loss: 1.09; acc: 0.62
Batch: 340; loss: 1.17; acc: 0.69
Batch: 360; loss: 1.09; acc: 0.7
Batch: 380; loss: 1.08; acc: 0.67
Batch: 400; loss: 1.08; acc: 0.64
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 1.12; acc: 0.75
Batch: 460; loss: 1.07; acc: 0.66
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 1.4; acc: 0.58
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.12; acc: 0.66
Batch: 560; loss: 1.04; acc: 0.72
Batch: 580; loss: 1.05; acc: 0.69
Batch: 600; loss: 1.14; acc: 0.72
Batch: 620; loss: 1.03; acc: 0.77
Batch: 640; loss: 1.09; acc: 0.64
Batch: 660; loss: 1.05; acc: 0.73
Batch: 680; loss: 0.99; acc: 0.73
Batch: 700; loss: 1.08; acc: 0.64
Batch: 720; loss: 1.11; acc: 0.66
Batch: 740; loss: 1.13; acc: 0.69
Batch: 760; loss: 1.01; acc: 0.77
Batch: 780; loss: 0.96; acc: 0.73
Train Epoch over. train_loss: 1.09; train_accuracy: 0.68 

0.00019624487322289497
0.00018721696687862277
Batch: 0; loss: 1.13; acc: 0.59
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 1.09; acc: 0.66
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.77
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.77; acc: 0.86
Val Epoch over. val_loss: 1.042518033723163; val_accuracy: 0.7156648089171974 

The current subspace-distance is: 0.00018721696687862277 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 1.15; acc: 0.66
Batch: 40; loss: 1.18; acc: 0.64
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.69
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 1.07; acc: 0.75
Batch: 160; loss: 1.09; acc: 0.62
Batch: 180; loss: 1.09; acc: 0.72
Batch: 200; loss: 1.12; acc: 0.69
Batch: 220; loss: 1.19; acc: 0.67
Batch: 240; loss: 1.05; acc: 0.7
Batch: 260; loss: 0.99; acc: 0.73
Batch: 280; loss: 1.08; acc: 0.72
Batch: 300; loss: 1.22; acc: 0.64
Batch: 320; loss: 1.04; acc: 0.73
Batch: 340; loss: 1.27; acc: 0.55
Batch: 360; loss: 1.05; acc: 0.7
Batch: 380; loss: 1.24; acc: 0.56
Batch: 400; loss: 1.12; acc: 0.58
Batch: 420; loss: 1.01; acc: 0.73
Batch: 440; loss: 0.83; acc: 0.8
Batch: 460; loss: 1.17; acc: 0.64
Batch: 480; loss: 1.09; acc: 0.67
Batch: 500; loss: 1.16; acc: 0.69
Batch: 520; loss: 1.14; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.75
Batch: 560; loss: 1.05; acc: 0.77
Batch: 580; loss: 1.05; acc: 0.77
Batch: 600; loss: 1.24; acc: 0.59
Batch: 620; loss: 1.12; acc: 0.64
Batch: 640; loss: 1.05; acc: 0.72
Batch: 660; loss: 0.99; acc: 0.73
Batch: 680; loss: 1.2; acc: 0.66
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 1.14; acc: 0.64
Batch: 740; loss: 1.18; acc: 0.67
Batch: 760; loss: 1.05; acc: 0.69
Batch: 780; loss: 1.11; acc: 0.73
Train Epoch over. train_loss: 1.09; train_accuracy: 0.68 

0.00019478864851407707
0.00019221851835027337
Batch: 0; loss: 1.1; acc: 0.61
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 0.85; acc: 0.8
Batch: 100; loss: 1.04; acc: 0.78
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 0.75; acc: 0.83
Val Epoch over. val_loss: 1.0222125964559567; val_accuracy: 0.7213375796178344 

The current subspace-distance is: 0.00019221851835027337 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.55
Batch: 40; loss: 1.11; acc: 0.64
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 0.94; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 1.21; acc: 0.64
Batch: 160; loss: 0.94; acc: 0.7
Batch: 180; loss: 1.13; acc: 0.61
Batch: 200; loss: 1.23; acc: 0.62
Batch: 220; loss: 1.17; acc: 0.61
Batch: 240; loss: 1.03; acc: 0.73
Batch: 260; loss: 0.95; acc: 0.78
Batch: 280; loss: 1.14; acc: 0.61
Batch: 300; loss: 0.86; acc: 0.83
Batch: 320; loss: 1.16; acc: 0.67
Batch: 340; loss: 1.17; acc: 0.67
Batch: 360; loss: 1.06; acc: 0.72
Batch: 380; loss: 1.05; acc: 0.73
Batch: 400; loss: 1.15; acc: 0.72
Batch: 420; loss: 1.14; acc: 0.66
Batch: 440; loss: 0.95; acc: 0.78
Batch: 460; loss: 1.16; acc: 0.64
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 0.95; acc: 0.78
Batch: 520; loss: 1.06; acc: 0.67
Batch: 540; loss: 1.03; acc: 0.7
Batch: 560; loss: 1.17; acc: 0.66
Batch: 580; loss: 1.12; acc: 0.69
Batch: 600; loss: 1.07; acc: 0.69
Batch: 620; loss: 1.1; acc: 0.67
Batch: 640; loss: 1.11; acc: 0.7
Batch: 660; loss: 1.16; acc: 0.58
Batch: 680; loss: 1.08; acc: 0.67
Batch: 700; loss: 0.96; acc: 0.72
Batch: 720; loss: 0.9; acc: 0.78
Batch: 740; loss: 1.01; acc: 0.72
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 1.04; acc: 0.72
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00019714684458449483
0.0001910786231746897
Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 0.87; acc: 0.81
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.75; acc: 0.86
Val Epoch over. val_loss: 1.0232338548465898; val_accuracy: 0.724422770700637 

The current subspace-distance is: 0.0001910786231746897 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.26; acc: 0.59
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 1.03; acc: 0.75
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 1.0; acc: 0.75
Batch: 100; loss: 1.13; acc: 0.67
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 0.93; acc: 0.81
Batch: 160; loss: 0.94; acc: 0.81
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 0.99; acc: 0.77
Batch: 220; loss: 0.92; acc: 0.77
Batch: 240; loss: 0.96; acc: 0.73
Batch: 260; loss: 1.03; acc: 0.7
Batch: 280; loss: 1.11; acc: 0.7
Batch: 300; loss: 1.13; acc: 0.64
Batch: 320; loss: 1.08; acc: 0.64
Batch: 340; loss: 1.08; acc: 0.66
Batch: 360; loss: 1.16; acc: 0.64
Batch: 380; loss: 1.25; acc: 0.58
Batch: 400; loss: 1.12; acc: 0.72
Batch: 420; loss: 1.08; acc: 0.66
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 1.3; acc: 0.56
Batch: 480; loss: 1.09; acc: 0.64
Batch: 500; loss: 0.98; acc: 0.72
Batch: 520; loss: 1.1; acc: 0.67
Batch: 540; loss: 0.95; acc: 0.73
Batch: 560; loss: 1.05; acc: 0.75
Batch: 580; loss: 1.12; acc: 0.64
Batch: 600; loss: 1.11; acc: 0.7
Batch: 620; loss: 1.06; acc: 0.69
Batch: 640; loss: 1.23; acc: 0.67
Batch: 660; loss: 1.12; acc: 0.67
Batch: 680; loss: 0.97; acc: 0.8
Batch: 700; loss: 1.17; acc: 0.69
Batch: 720; loss: 1.15; acc: 0.73
Batch: 740; loss: 1.1; acc: 0.69
Batch: 760; loss: 0.95; acc: 0.8
Batch: 780; loss: 1.21; acc: 0.59
Train Epoch over. train_loss: 1.09; train_accuracy: 0.68 

0.00019923117361031473
0.00019215793872717768
Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.66
Batch: 80; loss: 0.89; acc: 0.8
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.75; acc: 0.86
Val Epoch over. val_loss: 1.0302079289582124; val_accuracy: 0.7188495222929936 

The current subspace-distance is: 0.00019215793872717768 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 1.15; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 1.23; acc: 0.59
Batch: 100; loss: 1.14; acc: 0.67
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 1.06; acc: 0.73
Batch: 160; loss: 1.1; acc: 0.7
Batch: 180; loss: 1.08; acc: 0.73
Batch: 200; loss: 1.24; acc: 0.66
Batch: 220; loss: 1.11; acc: 0.67
Batch: 240; loss: 0.93; acc: 0.7
Batch: 260; loss: 1.01; acc: 0.66
Batch: 280; loss: 0.95; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.7
Batch: 320; loss: 1.15; acc: 0.66
Batch: 340; loss: 1.15; acc: 0.66
Batch: 360; loss: 1.11; acc: 0.64
Batch: 380; loss: 1.17; acc: 0.61
Batch: 400; loss: 0.96; acc: 0.8
Batch: 420; loss: 1.16; acc: 0.66
Batch: 440; loss: 1.2; acc: 0.62
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.7
Batch: 500; loss: 1.14; acc: 0.64
Batch: 520; loss: 1.21; acc: 0.67
Batch: 540; loss: 0.88; acc: 0.78
Batch: 560; loss: 1.2; acc: 0.67
Batch: 580; loss: 1.14; acc: 0.7
Batch: 600; loss: 1.13; acc: 0.67
Batch: 620; loss: 0.97; acc: 0.78
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.08; acc: 0.72
Batch: 680; loss: 1.07; acc: 0.66
Batch: 700; loss: 1.04; acc: 0.7
Batch: 720; loss: 1.12; acc: 0.66
Batch: 740; loss: 1.1; acc: 0.72
Batch: 760; loss: 1.02; acc: 0.72
Batch: 780; loss: 1.04; acc: 0.7
Train Epoch over. train_loss: 1.08; train_accuracy: 0.69 

0.00019611483730841428
0.0001881420030258596
Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.87; acc: 0.8
Batch: 100; loss: 1.02; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 0.75; acc: 0.86
Val Epoch over. val_loss: 1.0182850334295042; val_accuracy: 0.7203423566878981 

The current subspace-distance is: 0.0001881420030258596 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_16_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.51657508881031

The number of parameters is: 267435

The number of individual parameters is:

13
234
13
13
19
38532
19
19
37
109668
37
37
64
113664
64
64
4096
64
640
10
64
64

nonzero elements in E: 53486995
elements in E: 53487000
fraction nonzero: 0.9999999065193411
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.5; acc: 0.06
Batch: 20; loss: 2.2; acc: 0.25
Batch: 40; loss: 1.99; acc: 0.41
Batch: 60; loss: 2.05; acc: 0.28
Batch: 80; loss: 1.94; acc: 0.39
Batch: 100; loss: 1.93; acc: 0.53
Batch: 120; loss: 1.87; acc: 0.48
Batch: 140; loss: 1.75; acc: 0.58
Batch: 160; loss: 1.81; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.67
Batch: 200; loss: 1.76; acc: 0.53
Batch: 220; loss: 1.59; acc: 0.58
Batch: 240; loss: 1.84; acc: 0.48
Batch: 260; loss: 1.64; acc: 0.59
Batch: 280; loss: 1.66; acc: 0.61
Batch: 300; loss: 1.54; acc: 0.67
Batch: 320; loss: 1.68; acc: 0.62
Batch: 340; loss: 1.63; acc: 0.61
Batch: 360; loss: 1.49; acc: 0.7
Batch: 380; loss: 1.61; acc: 0.61
Batch: 400; loss: 1.58; acc: 0.53
Batch: 420; loss: 1.59; acc: 0.59
Batch: 440; loss: 1.56; acc: 0.69
Batch: 460; loss: 1.49; acc: 0.7
Batch: 480; loss: 1.54; acc: 0.67
Batch: 500; loss: 1.51; acc: 0.58
Batch: 520; loss: 1.53; acc: 0.67
Batch: 540; loss: 1.39; acc: 0.77
Batch: 560; loss: 1.53; acc: 0.61
Batch: 580; loss: 1.69; acc: 0.55
Batch: 600; loss: 1.41; acc: 0.72
Batch: 620; loss: 1.41; acc: 0.67
Batch: 640; loss: 1.62; acc: 0.52
Batch: 660; loss: 1.45; acc: 0.66
Batch: 680; loss: 1.43; acc: 0.69
Batch: 700; loss: 1.33; acc: 0.72
Batch: 720; loss: 1.37; acc: 0.7
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.43; acc: 0.66
Batch: 780; loss: 1.46; acc: 0.59
Train Epoch over. train_loss: 1.64; train_accuracy: 0.57 

6.197417678777128e-05
5.78558501729276e-05
Batch: 0; loss: 1.45; acc: 0.67
Batch: 20; loss: 1.58; acc: 0.55
Batch: 40; loss: 1.05; acc: 0.88
Batch: 60; loss: 1.29; acc: 0.8
Batch: 80; loss: 1.29; acc: 0.78
Batch: 100; loss: 1.34; acc: 0.8
Batch: 120; loss: 1.42; acc: 0.67
Batch: 140; loss: 1.29; acc: 0.83
Val Epoch over. val_loss: 1.3720612169071367; val_accuracy: 0.7084992038216561 

The current subspace-distance is: 5.78558501729276e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.34; acc: 0.66
Batch: 20; loss: 1.41; acc: 0.64
Batch: 40; loss: 1.38; acc: 0.72
Batch: 60; loss: 1.3; acc: 0.75
Batch: 80; loss: 1.46; acc: 0.62
Batch: 100; loss: 1.45; acc: 0.66
Batch: 120; loss: 1.39; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.72
Batch: 160; loss: 1.36; acc: 0.7
Batch: 180; loss: 1.28; acc: 0.78
Batch: 200; loss: 1.42; acc: 0.67
Batch: 220; loss: 1.31; acc: 0.72
Batch: 240; loss: 1.35; acc: 0.69
Batch: 260; loss: 1.31; acc: 0.75
Batch: 280; loss: 1.37; acc: 0.7
Batch: 300; loss: 1.26; acc: 0.78
Batch: 320; loss: 1.38; acc: 0.69
Batch: 340; loss: 1.32; acc: 0.69
Batch: 360; loss: 1.34; acc: 0.61
Batch: 380; loss: 1.26; acc: 0.72
Batch: 400; loss: 1.25; acc: 0.7
Batch: 420; loss: 1.32; acc: 0.7
Batch: 440; loss: 1.16; acc: 0.83
Batch: 460; loss: 1.23; acc: 0.73
Batch: 480; loss: 1.34; acc: 0.66
Batch: 500; loss: 1.32; acc: 0.7
Batch: 520; loss: 1.37; acc: 0.7
Batch: 540; loss: 1.19; acc: 0.8
Batch: 560; loss: 1.2; acc: 0.78
Batch: 580; loss: 1.3; acc: 0.64
Batch: 600; loss: 1.33; acc: 0.64
Batch: 620; loss: 1.18; acc: 0.73
Batch: 640; loss: 1.2; acc: 0.8
Batch: 660; loss: 1.31; acc: 0.72
Batch: 680; loss: 1.32; acc: 0.62
Batch: 700; loss: 1.28; acc: 0.66
Batch: 720; loss: 1.08; acc: 0.75
Batch: 740; loss: 1.26; acc: 0.7
Batch: 760; loss: 1.28; acc: 0.7
Batch: 780; loss: 1.14; acc: 0.73
Train Epoch over. train_loss: 1.29; train_accuracy: 0.72 

8.903456182451919e-05
8.415534102823585e-05
Batch: 0; loss: 1.25; acc: 0.69
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 0.86; acc: 0.91
Batch: 60; loss: 1.07; acc: 0.78
Batch: 80; loss: 1.02; acc: 0.88
Batch: 100; loss: 1.16; acc: 0.8
Batch: 120; loss: 1.23; acc: 0.73
Batch: 140; loss: 1.05; acc: 0.88
Val Epoch over. val_loss: 1.1591763841878078; val_accuracy: 0.7661226114649682 

The current subspace-distance is: 8.415534102823585e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.09; acc: 0.81
Batch: 40; loss: 1.17; acc: 0.8
Batch: 60; loss: 1.17; acc: 0.78
Batch: 80; loss: 1.37; acc: 0.64
Batch: 100; loss: 0.99; acc: 0.86
Batch: 120; loss: 1.07; acc: 0.81
Batch: 140; loss: 1.03; acc: 0.81
Batch: 160; loss: 1.1; acc: 0.77
Batch: 180; loss: 1.16; acc: 0.72
Batch: 200; loss: 1.03; acc: 0.81
Batch: 220; loss: 1.22; acc: 0.78
Batch: 240; loss: 1.11; acc: 0.75
Batch: 260; loss: 1.19; acc: 0.73
Batch: 280; loss: 1.17; acc: 0.67
Batch: 300; loss: 1.2; acc: 0.77
Batch: 320; loss: 1.17; acc: 0.72
Batch: 340; loss: 1.07; acc: 0.8
Batch: 360; loss: 1.16; acc: 0.73
Batch: 380; loss: 1.02; acc: 0.84
Batch: 400; loss: 1.16; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.73
Batch: 440; loss: 1.11; acc: 0.73
Batch: 460; loss: 1.18; acc: 0.69
Batch: 480; loss: 1.07; acc: 0.78
Batch: 500; loss: 1.05; acc: 0.81
Batch: 520; loss: 1.07; acc: 0.8
Batch: 540; loss: 1.24; acc: 0.7
Batch: 560; loss: 1.0; acc: 0.81
Batch: 580; loss: 1.19; acc: 0.69
Batch: 600; loss: 1.26; acc: 0.72
Batch: 620; loss: 1.07; acc: 0.77
Batch: 640; loss: 1.06; acc: 0.77
Batch: 660; loss: 1.07; acc: 0.8
Batch: 680; loss: 1.16; acc: 0.73
Batch: 700; loss: 1.13; acc: 0.73
Batch: 720; loss: 1.12; acc: 0.75
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 1.08; acc: 0.77
Batch: 780; loss: 1.07; acc: 0.84
Train Epoch over. train_loss: 1.15; train_accuracy: 0.75 

0.000108718202682212
0.00010356328130001202
Batch: 0; loss: 1.13; acc: 0.8
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.74; acc: 0.92
Batch: 60; loss: 0.95; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.92
Batch: 100; loss: 1.07; acc: 0.78
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 0.93; acc: 0.86
Val Epoch over. val_loss: 1.0419416754109085; val_accuracy: 0.7951831210191083 

The current subspace-distance is: 0.00010356328130001202 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 1.08; acc: 0.77
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.83
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.11; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.83
Batch: 160; loss: 1.11; acc: 0.72
Batch: 180; loss: 1.23; acc: 0.7
Batch: 200; loss: 1.06; acc: 0.77
Batch: 220; loss: 1.3; acc: 0.67
Batch: 240; loss: 1.02; acc: 0.83
Batch: 260; loss: 0.93; acc: 0.81
Batch: 280; loss: 1.14; acc: 0.75
Batch: 300; loss: 0.96; acc: 0.8
Batch: 320; loss: 1.17; acc: 0.69
Batch: 340; loss: 0.96; acc: 0.86
Batch: 360; loss: 1.17; acc: 0.7
Batch: 380; loss: 1.09; acc: 0.69
Batch: 400; loss: 1.14; acc: 0.73
Batch: 420; loss: 0.99; acc: 0.84
Batch: 440; loss: 1.08; acc: 0.8
Batch: 460; loss: 1.24; acc: 0.59
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 1.2; acc: 0.69
Batch: 520; loss: 1.0; acc: 0.83
Batch: 540; loss: 1.0; acc: 0.84
Batch: 560; loss: 1.05; acc: 0.75
Batch: 580; loss: 1.09; acc: 0.78
Batch: 600; loss: 1.02; acc: 0.8
Batch: 620; loss: 1.18; acc: 0.72
Batch: 640; loss: 1.02; acc: 0.84
Batch: 660; loss: 1.04; acc: 0.8
Batch: 680; loss: 0.89; acc: 0.84
Batch: 700; loss: 1.08; acc: 0.77
Batch: 720; loss: 0.98; acc: 0.8
Batch: 740; loss: 1.15; acc: 0.72
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 1.04; acc: 0.81
Train Epoch over. train_loss: 1.07; train_accuracy: 0.77 

0.00012716442870441824
0.00012179154145997018
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 0.68; acc: 0.92
Batch: 60; loss: 0.91; acc: 0.84
Batch: 80; loss: 0.8; acc: 0.92
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.1; acc: 0.72
Batch: 140; loss: 0.87; acc: 0.83
Val Epoch over. val_loss: 0.9814850790485455; val_accuracy: 0.7991640127388535 

The current subspace-distance is: 0.00012179154145997018 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.8
Batch: 20; loss: 0.98; acc: 0.77
Batch: 40; loss: 0.98; acc: 0.8
Batch: 60; loss: 0.88; acc: 0.88
Batch: 80; loss: 0.94; acc: 0.84
Batch: 100; loss: 0.93; acc: 0.86
Batch: 120; loss: 1.17; acc: 0.72
Batch: 140; loss: 1.04; acc: 0.73
Batch: 160; loss: 1.05; acc: 0.8
Batch: 180; loss: 0.95; acc: 0.88
Batch: 200; loss: 1.09; acc: 0.77
Batch: 220; loss: 0.99; acc: 0.83
Batch: 240; loss: 1.0; acc: 0.8
Batch: 260; loss: 1.03; acc: 0.78
Batch: 280; loss: 1.01; acc: 0.72
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 1.14; acc: 0.7
Batch: 340; loss: 1.08; acc: 0.75
Batch: 360; loss: 1.18; acc: 0.62
Batch: 380; loss: 0.97; acc: 0.77
Batch: 400; loss: 1.04; acc: 0.73
Batch: 420; loss: 0.96; acc: 0.77
Batch: 440; loss: 0.93; acc: 0.86
Batch: 460; loss: 1.13; acc: 0.69
Batch: 480; loss: 0.92; acc: 0.83
Batch: 500; loss: 1.01; acc: 0.8
Batch: 520; loss: 0.98; acc: 0.75
Batch: 540; loss: 1.05; acc: 0.8
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 0.94; acc: 0.78
Batch: 600; loss: 1.04; acc: 0.7
Batch: 620; loss: 1.02; acc: 0.77
Batch: 640; loss: 0.87; acc: 0.83
Batch: 660; loss: 0.93; acc: 0.83
Batch: 680; loss: 0.9; acc: 0.81
Batch: 700; loss: 0.91; acc: 0.78
Batch: 720; loss: 1.0; acc: 0.75
Batch: 740; loss: 1.01; acc: 0.83
Batch: 760; loss: 0.94; acc: 0.77
Batch: 780; loss: 1.11; acc: 0.73
Train Epoch over. train_loss: 1.0; train_accuracy: 0.78 

0.00014018913498148322
0.0001351333485217765
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 1.14; acc: 0.67
Batch: 40; loss: 0.62; acc: 0.94
Batch: 60; loss: 0.87; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.91
Batch: 100; loss: 0.94; acc: 0.88
Batch: 120; loss: 1.02; acc: 0.8
Batch: 140; loss: 0.79; acc: 0.83
Val Epoch over. val_loss: 0.9067732965110973; val_accuracy: 0.8184713375796179 

The current subspace-distance is: 0.0001351333485217765 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.84
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 1.09; acc: 0.73
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.96; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.83
Batch: 120; loss: 0.92; acc: 0.84
Batch: 140; loss: 0.99; acc: 0.73
Batch: 160; loss: 1.06; acc: 0.73
Batch: 180; loss: 0.79; acc: 0.89
Batch: 200; loss: 1.03; acc: 0.8
Batch: 220; loss: 0.94; acc: 0.81
Batch: 240; loss: 0.99; acc: 0.8
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 1.01; acc: 0.75
Batch: 320; loss: 0.82; acc: 0.91
Batch: 340; loss: 0.92; acc: 0.8
Batch: 360; loss: 0.95; acc: 0.78
Batch: 380; loss: 0.98; acc: 0.81
Batch: 400; loss: 0.84; acc: 0.84
Batch: 420; loss: 0.94; acc: 0.78
Batch: 440; loss: 1.06; acc: 0.77
Batch: 460; loss: 0.96; acc: 0.77
Batch: 480; loss: 0.91; acc: 0.86
Batch: 500; loss: 0.93; acc: 0.75
Batch: 520; loss: 1.03; acc: 0.75
Batch: 540; loss: 0.93; acc: 0.73
Batch: 560; loss: 0.9; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.84
Batch: 600; loss: 0.96; acc: 0.77
Batch: 620; loss: 1.07; acc: 0.7
Batch: 640; loss: 0.68; acc: 0.89
Batch: 660; loss: 0.9; acc: 0.81
Batch: 680; loss: 0.88; acc: 0.8
Batch: 700; loss: 0.95; acc: 0.72
Batch: 720; loss: 0.89; acc: 0.83
Batch: 740; loss: 0.96; acc: 0.78
Batch: 760; loss: 0.88; acc: 0.86
Batch: 780; loss: 0.88; acc: 0.84
Train Epoch over. train_loss: 0.94; train_accuracy: 0.79 

0.00015593743592035025
0.0001502981613157317
Batch: 0; loss: 0.94; acc: 0.84
Batch: 20; loss: 1.1; acc: 0.69
Batch: 40; loss: 0.57; acc: 0.94
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.91
Batch: 100; loss: 0.87; acc: 0.91
Batch: 120; loss: 0.98; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.91
Val Epoch over. val_loss: 0.8556147107652797; val_accuracy: 0.8304140127388535 

The current subspace-distance is: 0.0001502981613157317 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.92
Batch: 40; loss: 0.92; acc: 0.77
Batch: 60; loss: 0.97; acc: 0.81
Batch: 80; loss: 0.87; acc: 0.86
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.89
Batch: 140; loss: 0.94; acc: 0.81
Batch: 160; loss: 1.06; acc: 0.66
Batch: 180; loss: 0.74; acc: 0.92
Batch: 200; loss: 0.85; acc: 0.78
Batch: 220; loss: 0.94; acc: 0.78
Batch: 240; loss: 0.73; acc: 0.89
Batch: 260; loss: 0.83; acc: 0.84
Batch: 280; loss: 0.9; acc: 0.78
Batch: 300; loss: 0.86; acc: 0.81
Batch: 320; loss: 0.91; acc: 0.84
Batch: 340; loss: 0.91; acc: 0.77
Batch: 360; loss: 0.97; acc: 0.75
Batch: 380; loss: 0.94; acc: 0.81
Batch: 400; loss: 1.05; acc: 0.78
Batch: 420; loss: 0.97; acc: 0.73
Batch: 440; loss: 0.97; acc: 0.77
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 0.97; acc: 0.78
Batch: 500; loss: 0.81; acc: 0.88
Batch: 520; loss: 0.92; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.94
Batch: 580; loss: 0.67; acc: 0.92
Batch: 600; loss: 0.82; acc: 0.83
Batch: 620; loss: 0.9; acc: 0.77
Batch: 640; loss: 0.87; acc: 0.81
Batch: 660; loss: 0.81; acc: 0.81
Batch: 680; loss: 0.9; acc: 0.8
Batch: 700; loss: 0.88; acc: 0.77
Batch: 720; loss: 0.98; acc: 0.73
Batch: 740; loss: 0.89; acc: 0.77
Batch: 760; loss: 1.0; acc: 0.7
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.89; train_accuracy: 0.8 

0.00016835567657835782
0.00016474090807605535
Batch: 0; loss: 0.91; acc: 0.83
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.55; acc: 0.94
Batch: 60; loss: 0.83; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.88
Batch: 100; loss: 0.81; acc: 0.91
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.67; acc: 0.89
Val Epoch over. val_loss: 0.8244621985277553; val_accuracy: 0.8376791401273885 

The current subspace-distance is: 0.00016474090807605535 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.67
Batch: 20; loss: 0.82; acc: 0.88
Batch: 40; loss: 0.92; acc: 0.75
Batch: 60; loss: 0.89; acc: 0.8
Batch: 80; loss: 0.84; acc: 0.81
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.81
Batch: 140; loss: 0.89; acc: 0.81
Batch: 160; loss: 0.84; acc: 0.8
Batch: 180; loss: 0.94; acc: 0.8
Batch: 200; loss: 0.87; acc: 0.83
Batch: 220; loss: 0.9; acc: 0.81
Batch: 240; loss: 0.87; acc: 0.8
Batch: 260; loss: 0.94; acc: 0.8
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 0.87; acc: 0.73
Batch: 320; loss: 0.95; acc: 0.83
Batch: 340; loss: 0.87; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.81
Batch: 380; loss: 0.93; acc: 0.81
Batch: 400; loss: 0.96; acc: 0.73
Batch: 420; loss: 0.92; acc: 0.8
Batch: 440; loss: 0.89; acc: 0.77
Batch: 460; loss: 0.79; acc: 0.84
Batch: 480; loss: 0.8; acc: 0.8
Batch: 500; loss: 0.85; acc: 0.86
Batch: 520; loss: 1.0; acc: 0.78
Batch: 540; loss: 0.67; acc: 0.92
Batch: 560; loss: 0.88; acc: 0.81
Batch: 580; loss: 0.8; acc: 0.83
Batch: 600; loss: 0.9; acc: 0.78
Batch: 620; loss: 0.82; acc: 0.81
Batch: 640; loss: 0.91; acc: 0.78
Batch: 660; loss: 0.89; acc: 0.78
Batch: 680; loss: 0.83; acc: 0.84
Batch: 700; loss: 0.81; acc: 0.84
Batch: 720; loss: 0.76; acc: 0.89
Batch: 740; loss: 0.76; acc: 0.84
Batch: 760; loss: 0.89; acc: 0.8
Batch: 780; loss: 0.74; acc: 0.84
Train Epoch over. train_loss: 0.86; train_accuracy: 0.81 

0.0001806805667001754
0.00017420077347196639
Batch: 0; loss: 0.88; acc: 0.81
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.8; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.92
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 0.7875914220597334; val_accuracy: 0.836484872611465 

The current subspace-distance is: 0.00017420077347196639 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.83
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.84
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 0.94; acc: 0.78
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 0.83; acc: 0.8
Batch: 160; loss: 0.79; acc: 0.86
Batch: 180; loss: 0.84; acc: 0.84
Batch: 200; loss: 0.82; acc: 0.78
Batch: 220; loss: 0.89; acc: 0.81
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.89; acc: 0.81
Batch: 280; loss: 0.83; acc: 0.81
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 0.99; acc: 0.77
Batch: 340; loss: 0.89; acc: 0.81
Batch: 360; loss: 0.91; acc: 0.84
Batch: 380; loss: 0.68; acc: 0.91
Batch: 400; loss: 0.82; acc: 0.78
Batch: 420; loss: 0.9; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.89
Batch: 460; loss: 0.99; acc: 0.77
Batch: 480; loss: 0.72; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.89
Batch: 520; loss: 0.85; acc: 0.75
Batch: 540; loss: 0.93; acc: 0.77
Batch: 560; loss: 0.79; acc: 0.83
Batch: 580; loss: 0.96; acc: 0.77
Batch: 600; loss: 0.8; acc: 0.83
Batch: 620; loss: 0.81; acc: 0.83
Batch: 640; loss: 0.88; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.91
Batch: 680; loss: 0.79; acc: 0.83
Batch: 700; loss: 0.86; acc: 0.86
Batch: 720; loss: 0.72; acc: 0.86
Batch: 740; loss: 0.73; acc: 0.88
Batch: 760; loss: 0.76; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.83
Train Epoch over. train_loss: 0.83; train_accuracy: 0.81 

0.00019423187768552452
0.00018698150233831257
Batch: 0; loss: 0.85; acc: 0.83
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.89
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.92
Val Epoch over. val_loss: 0.7570310538741434; val_accuracy: 0.8430533439490446 

The current subspace-distance is: 0.00018698150233831257 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 0.76; acc: 0.88
Batch: 80; loss: 0.9; acc: 0.83
Batch: 100; loss: 0.89; acc: 0.72
Batch: 120; loss: 0.72; acc: 0.88
Batch: 140; loss: 0.86; acc: 0.78
Batch: 160; loss: 0.87; acc: 0.78
Batch: 180; loss: 0.73; acc: 0.83
Batch: 200; loss: 0.98; acc: 0.75
Batch: 220; loss: 0.82; acc: 0.8
Batch: 240; loss: 0.79; acc: 0.83
Batch: 260; loss: 0.99; acc: 0.72
Batch: 280; loss: 0.82; acc: 0.83
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.89
Batch: 340; loss: 0.9; acc: 0.8
Batch: 360; loss: 0.83; acc: 0.75
Batch: 380; loss: 0.91; acc: 0.78
Batch: 400; loss: 0.94; acc: 0.78
Batch: 420; loss: 0.65; acc: 0.95
Batch: 440; loss: 0.83; acc: 0.88
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 0.66; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.88
Batch: 560; loss: 0.89; acc: 0.78
Batch: 580; loss: 0.8; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.84
Batch: 620; loss: 0.83; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.87; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.91
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.68; acc: 0.86
Batch: 780; loss: 0.81; acc: 0.77
Train Epoch over. train_loss: 0.8; train_accuracy: 0.82 

0.0002008498995564878
0.00019560946384444833
Batch: 0; loss: 0.82; acc: 0.84
Batch: 20; loss: 0.97; acc: 0.75
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.55; acc: 0.92
Val Epoch over. val_loss: 0.7189051413991648; val_accuracy: 0.8452428343949044 

The current subspace-distance is: 0.00019560946384444833 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.71; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.89
Batch: 80; loss: 0.79; acc: 0.83
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 0.78; acc: 0.86
Batch: 160; loss: 0.72; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.86
Batch: 200; loss: 0.79; acc: 0.86
Batch: 220; loss: 0.83; acc: 0.81
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.83; acc: 0.8
Batch: 280; loss: 0.84; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.84; acc: 0.84
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.86; acc: 0.78
Batch: 420; loss: 0.63; acc: 0.88
Batch: 440; loss: 0.79; acc: 0.81
Batch: 460; loss: 0.87; acc: 0.77
Batch: 480; loss: 0.81; acc: 0.77
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.97; acc: 0.73
Batch: 600; loss: 0.77; acc: 0.86
Batch: 620; loss: 0.85; acc: 0.81
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.83; acc: 0.77
Batch: 680; loss: 0.7; acc: 0.84
Batch: 700; loss: 0.81; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.8
Batch: 740; loss: 0.97; acc: 0.77
Batch: 760; loss: 0.75; acc: 0.83
Batch: 780; loss: 0.84; acc: 0.81
Train Epoch over. train_loss: 0.78; train_accuracy: 0.82 

0.00020505336578935385
0.00019819257431663573
Batch: 0; loss: 0.81; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.78
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.54; acc: 0.94
Val Epoch over. val_loss: 0.714909507780318; val_accuracy: 0.8459394904458599 

The current subspace-distance is: 0.00019819257431663573 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.64; acc: 0.88
Batch: 160; loss: 0.77; acc: 0.83
Batch: 180; loss: 0.82; acc: 0.78
Batch: 200; loss: 0.79; acc: 0.8
Batch: 220; loss: 0.71; acc: 0.81
Batch: 240; loss: 0.9; acc: 0.77
Batch: 260; loss: 0.77; acc: 0.8
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.86; acc: 0.8
Batch: 340; loss: 0.68; acc: 0.88
Batch: 360; loss: 0.9; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.84; acc: 0.8
Batch: 460; loss: 0.95; acc: 0.73
Batch: 480; loss: 0.79; acc: 0.84
Batch: 500; loss: 0.82; acc: 0.77
Batch: 520; loss: 0.74; acc: 0.84
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.59; acc: 0.92
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.81
Batch: 620; loss: 0.69; acc: 0.88
Batch: 640; loss: 0.73; acc: 0.89
Batch: 660; loss: 0.77; acc: 0.88
Batch: 680; loss: 0.82; acc: 0.8
Batch: 700; loss: 0.78; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.86
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.91; acc: 0.72
Train Epoch over. train_loss: 0.77; train_accuracy: 0.82 

0.00020832192967645824
0.00020056987705174834
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.94; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.94
Val Epoch over. val_loss: 0.7078021079491658; val_accuracy: 0.8468351910828026 

The current subspace-distance is: 0.00020056987705174834 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.7; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.72; acc: 0.81
Batch: 160; loss: 0.87; acc: 0.73
Batch: 180; loss: 0.68; acc: 0.92
Batch: 200; loss: 0.82; acc: 0.75
Batch: 220; loss: 0.67; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.91; acc: 0.77
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.79; acc: 0.83
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.84; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.86
Batch: 400; loss: 0.8; acc: 0.8
Batch: 420; loss: 0.62; acc: 0.89
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.65; acc: 0.91
Batch: 500; loss: 0.83; acc: 0.78
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.79; acc: 0.8
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.79; acc: 0.83
Batch: 600; loss: 0.64; acc: 0.94
Batch: 620; loss: 0.77; acc: 0.75
Batch: 640; loss: 0.89; acc: 0.67
Batch: 660; loss: 0.8; acc: 0.8
Batch: 680; loss: 0.88; acc: 0.78
Batch: 700; loss: 0.72; acc: 0.88
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 1.15; acc: 0.7
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 0.76; train_accuracy: 0.82 

0.00021179340546950698
0.0002054901560768485
Batch: 0; loss: 0.79; acc: 0.83
Batch: 20; loss: 0.93; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6967034159572261; val_accuracy: 0.8495222929936306 

The current subspace-distance is: 0.0002054901560768485 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.77; acc: 0.84
Batch: 180; loss: 0.78; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.91
Batch: 220; loss: 0.83; acc: 0.81
Batch: 240; loss: 0.78; acc: 0.8
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.88; acc: 0.77
Batch: 320; loss: 0.64; acc: 0.86
Batch: 340; loss: 0.65; acc: 0.92
Batch: 360; loss: 0.71; acc: 0.86
Batch: 380; loss: 0.76; acc: 0.88
Batch: 400; loss: 0.72; acc: 0.84
Batch: 420; loss: 0.73; acc: 0.86
Batch: 440; loss: 0.91; acc: 0.78
Batch: 460; loss: 0.88; acc: 0.7
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.77; acc: 0.78
Batch: 540; loss: 0.62; acc: 0.89
Batch: 560; loss: 0.79; acc: 0.84
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.8
Batch: 620; loss: 0.79; acc: 0.83
Batch: 640; loss: 0.92; acc: 0.75
Batch: 660; loss: 0.77; acc: 0.81
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.75; acc: 0.77
Batch: 720; loss: 0.83; acc: 0.8
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.84
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.0002148663334082812
0.00020922592375427485
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.91; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6874411590160079; val_accuracy: 0.8487261146496815 

The current subspace-distance is: 0.00020922592375427485 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.79; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.86
Batch: 160; loss: 0.75; acc: 0.86
Batch: 180; loss: 0.75; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.77
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.89; acc: 0.73
Batch: 260; loss: 0.74; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.69; acc: 0.89
Batch: 320; loss: 0.9; acc: 0.73
Batch: 340; loss: 0.76; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.89
Batch: 380; loss: 0.65; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.92; acc: 0.78
Batch: 440; loss: 0.88; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.6; acc: 0.92
Batch: 520; loss: 0.7; acc: 0.88
Batch: 540; loss: 0.78; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.87; acc: 0.7
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.91
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.85; acc: 0.75
Batch: 680; loss: 0.69; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.89
Batch: 720; loss: 0.77; acc: 0.8
Batch: 740; loss: 0.81; acc: 0.78
Batch: 760; loss: 0.82; acc: 0.81
Batch: 780; loss: 0.75; acc: 0.8
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00021929258946329355
0.00021250610006973147
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.92; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6850338454838771; val_accuracy: 0.8500199044585988 

The current subspace-distance is: 0.00021250610006973147 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.54; acc: 0.94
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.95; acc: 0.75
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.86; acc: 0.69
Batch: 240; loss: 0.58; acc: 0.91
Batch: 260; loss: 0.8; acc: 0.81
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.78; acc: 0.8
Batch: 320; loss: 0.74; acc: 0.86
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.81; acc: 0.8
Batch: 400; loss: 0.8; acc: 0.81
Batch: 420; loss: 0.7; acc: 0.88
Batch: 440; loss: 0.84; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.89
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.6; acc: 0.91
Batch: 580; loss: 0.85; acc: 0.8
Batch: 600; loss: 0.6; acc: 0.92
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.82; acc: 0.75
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.91; acc: 0.75
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.82; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.00022247000015340745
0.0002147476770915091
Batch: 0; loss: 0.76; acc: 0.84
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6703719916237387; val_accuracy: 0.8498208598726115 

The current subspace-distance is: 0.0002147476770915091 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.66; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.95
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.79; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 0.86; acc: 0.7
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.81
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.95
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.88
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.6; acc: 0.91
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.92
Batch: 440; loss: 0.73; acc: 0.84
Batch: 460; loss: 0.81; acc: 0.73
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.77; acc: 0.83
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.89
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.89
Batch: 620; loss: 0.57; acc: 0.92
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.66; acc: 0.89
Batch: 700; loss: 0.83; acc: 0.81
Batch: 720; loss: 0.85; acc: 0.73
Batch: 740; loss: 0.86; acc: 0.78
Batch: 760; loss: 0.7; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.91
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.0002243968629045412
0.00021744193509221077
Batch: 0; loss: 0.76; acc: 0.84
Batch: 20; loss: 0.9; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6766287356045595; val_accuracy: 0.8516122611464968 

The current subspace-distance is: 0.00021744193509221077 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.82; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.68; acc: 0.91
Batch: 160; loss: 0.81; acc: 0.83
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.64; acc: 0.89
Batch: 260; loss: 0.78; acc: 0.84
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.74; acc: 0.88
Batch: 340; loss: 0.82; acc: 0.77
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.61; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.66; acc: 0.91
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.83; acc: 0.77
Batch: 600; loss: 0.77; acc: 0.81
Batch: 620; loss: 0.91; acc: 0.8
Batch: 640; loss: 0.72; acc: 0.8
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.83; acc: 0.78
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.78; acc: 0.75
Batch: 760; loss: 0.72; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.75
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.00022404527408070862
0.00021688466949854046
Batch: 0; loss: 0.75; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6678169166586202; val_accuracy: 0.8515127388535032 

The current subspace-distance is: 0.00021688466949854046 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.87; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.89
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.76; acc: 0.78
Batch: 260; loss: 0.59; acc: 0.92
Batch: 280; loss: 0.65; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.78
Batch: 320; loss: 0.81; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.91; acc: 0.78
Batch: 400; loss: 0.71; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.74; acc: 0.81
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.77; acc: 0.84
Batch: 500; loss: 0.88; acc: 0.77
Batch: 520; loss: 0.84; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.84
Batch: 580; loss: 0.8; acc: 0.8
Batch: 600; loss: 0.79; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.89
Batch: 640; loss: 0.72; acc: 0.83
Batch: 660; loss: 0.83; acc: 0.8
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.69; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.00022637368238065392
0.0002201408497057855
Batch: 0; loss: 0.74; acc: 0.84
Batch: 20; loss: 0.88; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6609628775696845; val_accuracy: 0.8498208598726115 

The current subspace-distance is: 0.0002201408497057855 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.63; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.91
Batch: 140; loss: 0.85; acc: 0.77
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.84; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.92
Batch: 260; loss: 0.86; acc: 0.72
Batch: 280; loss: 0.77; acc: 0.8
Batch: 300; loss: 0.73; acc: 0.88
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.68; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.91
Batch: 420; loss: 0.76; acc: 0.84
Batch: 440; loss: 0.71; acc: 0.78
Batch: 460; loss: 0.64; acc: 0.86
Batch: 480; loss: 0.98; acc: 0.7
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.81; acc: 0.78
Batch: 540; loss: 0.7; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.75; acc: 0.78
Batch: 620; loss: 0.81; acc: 0.75
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.79; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.88
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.74; acc: 0.88
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.92; acc: 0.77
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00022861336765345186
0.0002247397496830672
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.91
Val Epoch over. val_loss: 0.6478468639076136; val_accuracy: 0.8516122611464968 

The current subspace-distance is: 0.0002247397496830672 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.78
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.75; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 0.85; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.84
Batch: 300; loss: 0.66; acc: 0.84
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.88
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.78
Batch: 440; loss: 0.58; acc: 0.88
Batch: 460; loss: 0.74; acc: 0.89
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.88; acc: 0.75
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.74; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.99; acc: 0.73
Batch: 620; loss: 0.75; acc: 0.83
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.65; acc: 0.84
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.71; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00023397093173116446
0.0002265161310788244
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.6448402488307589; val_accuracy: 0.8522093949044586 

The current subspace-distance is: 0.0002265161310788244 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.7
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.8; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.88
Batch: 200; loss: 0.86; acc: 0.78
Batch: 220; loss: 0.75; acc: 0.84
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.65; acc: 0.89
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.7; acc: 0.86
Batch: 340; loss: 0.77; acc: 0.81
Batch: 360; loss: 0.78; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.88
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.65; acc: 0.86
Batch: 500; loss: 0.82; acc: 0.73
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.96; acc: 0.72
Batch: 620; loss: 0.72; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.83
Batch: 680; loss: 0.71; acc: 0.88
Batch: 700; loss: 0.67; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.62; acc: 0.89
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.77; acc: 0.83
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00023659554426558316
0.00022566920961253345
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6505975799196085; val_accuracy: 0.8512141719745223 

The current subspace-distance is: 0.00022566920961253345 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.67; acc: 0.92
Batch: 160; loss: 0.72; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.83; acc: 0.83
Batch: 240; loss: 0.87; acc: 0.8
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.91
Batch: 320; loss: 0.65; acc: 0.91
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.86
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.7; acc: 0.8
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.92; acc: 0.75
Batch: 600; loss: 0.58; acc: 0.91
Batch: 620; loss: 0.71; acc: 0.89
Batch: 640; loss: 0.82; acc: 0.75
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.81; acc: 0.72
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.79; acc: 0.83
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.88
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00023502553813159466
0.00022681025438942015
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6416712737387154; val_accuracy: 0.851015127388535 

The current subspace-distance is: 0.00022681025438942015 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.92
Batch: 80; loss: 0.81; acc: 0.78
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.89
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.92
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.75; acc: 0.78
Batch: 360; loss: 0.86; acc: 0.81
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.92
Batch: 440; loss: 0.86; acc: 0.8
Batch: 460; loss: 0.85; acc: 0.8
Batch: 480; loss: 0.88; acc: 0.78
Batch: 500; loss: 0.55; acc: 0.91
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.91; acc: 0.73
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.61; acc: 0.94
Batch: 680; loss: 0.74; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.88
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.93; acc: 0.75
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00023347479873336852
0.0002285652153659612
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.646485162958218; val_accuracy: 0.8513136942675159 

The current subspace-distance is: 0.0002285652153659612 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.91; acc: 0.77
Batch: 220; loss: 0.8; acc: 0.83
Batch: 240; loss: 0.76; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.92
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.73; acc: 0.84
Batch: 320; loss: 0.74; acc: 0.78
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.9; acc: 0.72
Batch: 380; loss: 0.89; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.91; acc: 0.72
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.83; acc: 0.77
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.72; acc: 0.83
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.83; acc: 0.77
Batch: 680; loss: 0.64; acc: 0.86
Batch: 700; loss: 0.84; acc: 0.77
Batch: 720; loss: 0.82; acc: 0.75
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002359204227104783
0.0002271139674121514
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.6478884747833203; val_accuracy: 0.8517117834394905 

The current subspace-distance is: 0.0002271139674121514 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.75
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.53; acc: 0.94
Batch: 220; loss: 0.63; acc: 0.88
Batch: 240; loss: 0.72; acc: 0.86
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.79; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.77
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.92
Batch: 380; loss: 0.94; acc: 0.72
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.85; acc: 0.72
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.69; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.77; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.81
Batch: 740; loss: 0.73; acc: 0.81
Batch: 760; loss: 0.64; acc: 0.91
Batch: 780; loss: 0.71; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00023799549671821296
0.00022825643827673048
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.638268587126094; val_accuracy: 0.8528065286624203 

The current subspace-distance is: 0.00022825643827673048 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.78; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.56; acc: 0.92
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.86; acc: 0.75
Batch: 260; loss: 0.65; acc: 0.81
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.66; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.84
Batch: 400; loss: 0.79; acc: 0.83
Batch: 420; loss: 0.79; acc: 0.78
Batch: 440; loss: 0.72; acc: 0.84
Batch: 460; loss: 0.96; acc: 0.77
Batch: 480; loss: 0.71; acc: 0.78
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.73; acc: 0.84
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.76; acc: 0.8
Batch: 640; loss: 0.57; acc: 0.92
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.76; acc: 0.83
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.71; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002366305998293683
0.0002289635012857616
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6490058487008332; val_accuracy: 0.8511146496815286 

The current subspace-distance is: 0.0002289635012857616 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.78
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.86; acc: 0.8
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.77; acc: 0.84
Batch: 240; loss: 0.74; acc: 0.75
Batch: 260; loss: 0.63; acc: 0.88
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.73; acc: 0.81
Batch: 360; loss: 0.84; acc: 0.8
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 1.03; acc: 0.72
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.88
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.79; acc: 0.83
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.77; acc: 0.83
Batch: 640; loss: 0.86; acc: 0.77
Batch: 660; loss: 0.67; acc: 0.8
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.67; acc: 0.89
Batch: 720; loss: 0.73; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.8; acc: 0.75
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00023900732048787177
0.0002309671981493011
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6399156088661996; val_accuracy: 0.8512141719745223 

The current subspace-distance is: 0.0002309671981493011 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.83; acc: 0.77
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.76; acc: 0.78
Batch: 200; loss: 0.74; acc: 0.78
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.86; acc: 0.73
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.74; acc: 0.77
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.5; acc: 0.94
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.73; acc: 0.84
Batch: 560; loss: 0.83; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.89
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.69; acc: 0.81
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.92; acc: 0.73
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00024019397096708417
0.00023241004964802414
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.6403168447458061; val_accuracy: 0.8512141719745223 

The current subspace-distance is: 0.00023241004964802414 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.78; acc: 0.81
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.77; acc: 0.77
Batch: 200; loss: 0.86; acc: 0.77
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.89; acc: 0.7
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.89; acc: 0.69
Batch: 320; loss: 0.81; acc: 0.8
Batch: 340; loss: 0.76; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.8; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.8; acc: 0.81
Batch: 580; loss: 0.68; acc: 0.88
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.71; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.88
Batch: 720; loss: 0.75; acc: 0.83
Batch: 740; loss: 0.89; acc: 0.73
Batch: 760; loss: 0.73; acc: 0.88
Batch: 780; loss: 0.9; acc: 0.77
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00023607054026797414
0.00023181262076832354
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.6333751410815367; val_accuracy: 0.8543988853503185 

The current subspace-distance is: 0.00023181262076832354 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_16_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.51657508881031

The number of parameters is: 267435

The number of individual parameters is:

13
234
13
13
19
38532
19
19
37
109668
37
37
64
113664
64
64
4096
64
640
10
64
64

nonzero elements in E: 80230491
elements in E: 80230500
fraction nonzero: 0.9999998878232094
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.44; acc: 0.08
Batch: 20; loss: 2.14; acc: 0.28
Batch: 40; loss: 1.96; acc: 0.42
Batch: 60; loss: 1.81; acc: 0.47
Batch: 80; loss: 1.85; acc: 0.48
Batch: 100; loss: 1.67; acc: 0.56
Batch: 120; loss: 1.77; acc: 0.5
Batch: 140; loss: 1.65; acc: 0.64
Batch: 160; loss: 1.71; acc: 0.45
Batch: 180; loss: 1.43; acc: 0.77
Batch: 200; loss: 1.69; acc: 0.52
Batch: 220; loss: 1.57; acc: 0.66
Batch: 240; loss: 1.48; acc: 0.69
Batch: 260; loss: 1.5; acc: 0.66
Batch: 280; loss: 1.47; acc: 0.67
Batch: 300; loss: 1.37; acc: 0.78
Batch: 320; loss: 1.28; acc: 0.81
Batch: 340; loss: 1.34; acc: 0.81
Batch: 360; loss: 1.46; acc: 0.7
Batch: 380; loss: 1.42; acc: 0.64
Batch: 400; loss: 1.4; acc: 0.72
Batch: 420; loss: 1.27; acc: 0.73
Batch: 440; loss: 1.33; acc: 0.77
Batch: 460; loss: 1.4; acc: 0.66
Batch: 480; loss: 1.29; acc: 0.73
Batch: 500; loss: 1.48; acc: 0.62
Batch: 520; loss: 1.39; acc: 0.69
Batch: 540; loss: 1.36; acc: 0.7
Batch: 560; loss: 1.27; acc: 0.78
Batch: 580; loss: 1.43; acc: 0.62
Batch: 600; loss: 1.31; acc: 0.69
Batch: 620; loss: 1.16; acc: 0.8
Batch: 640; loss: 1.21; acc: 0.81
Batch: 660; loss: 1.15; acc: 0.86
Batch: 680; loss: 1.23; acc: 0.78
Batch: 700; loss: 1.28; acc: 0.81
Batch: 720; loss: 1.12; acc: 0.83
Batch: 740; loss: 1.22; acc: 0.77
Batch: 760; loss: 1.14; acc: 0.88
Batch: 780; loss: 1.28; acc: 0.7
Train Epoch over. train_loss: 1.45; train_accuracy: 0.67 

6.355407094815746e-05
5.9515179600566626e-05
Batch: 0; loss: 1.27; acc: 0.7
Batch: 20; loss: 1.27; acc: 0.78
Batch: 40; loss: 0.91; acc: 0.91
Batch: 60; loss: 1.14; acc: 0.8
Batch: 80; loss: 1.04; acc: 0.83
Batch: 100; loss: 1.14; acc: 0.84
Batch: 120; loss: 1.25; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.95
Val Epoch over. val_loss: 1.139100529206027; val_accuracy: 0.8031449044585988 

The current subspace-distance is: 5.9515179600566626e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.84
Batch: 20; loss: 1.16; acc: 0.86
Batch: 40; loss: 1.18; acc: 0.8
Batch: 60; loss: 1.24; acc: 0.77
Batch: 80; loss: 1.1; acc: 0.88
Batch: 100; loss: 1.08; acc: 0.81
Batch: 120; loss: 1.02; acc: 0.8
Batch: 140; loss: 1.07; acc: 0.84
Batch: 160; loss: 1.07; acc: 0.83
Batch: 180; loss: 0.99; acc: 0.83
Batch: 200; loss: 1.08; acc: 0.86
Batch: 220; loss: 1.0; acc: 0.8
Batch: 240; loss: 1.23; acc: 0.69
Batch: 260; loss: 1.01; acc: 0.89
Batch: 280; loss: 1.09; acc: 0.78
Batch: 300; loss: 1.09; acc: 0.78
Batch: 320; loss: 1.01; acc: 0.84
Batch: 340; loss: 1.03; acc: 0.83
Batch: 360; loss: 1.09; acc: 0.75
Batch: 380; loss: 1.11; acc: 0.73
Batch: 400; loss: 0.92; acc: 0.86
Batch: 420; loss: 0.94; acc: 0.88
Batch: 440; loss: 0.95; acc: 0.8
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 0.96; acc: 0.83
Batch: 500; loss: 0.98; acc: 0.89
Batch: 520; loss: 1.17; acc: 0.75
Batch: 540; loss: 1.1; acc: 0.72
Batch: 560; loss: 0.99; acc: 0.86
Batch: 580; loss: 0.99; acc: 0.8
Batch: 600; loss: 1.13; acc: 0.77
Batch: 620; loss: 1.02; acc: 0.75
Batch: 640; loss: 0.83; acc: 0.91
Batch: 660; loss: 0.9; acc: 0.83
Batch: 680; loss: 1.01; acc: 0.77
Batch: 700; loss: 0.95; acc: 0.83
Batch: 720; loss: 0.98; acc: 0.83
Batch: 740; loss: 1.06; acc: 0.81
Batch: 760; loss: 0.95; acc: 0.78
Batch: 780; loss: 1.09; acc: 0.77
Train Epoch over. train_loss: 1.06; train_accuracy: 0.8 

9.076639980776235e-05
8.658949809614569e-05
Batch: 0; loss: 1.05; acc: 0.8
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 0.66; acc: 0.95
Batch: 60; loss: 0.95; acc: 0.8
Batch: 80; loss: 0.81; acc: 0.88
Batch: 100; loss: 0.88; acc: 0.86
Batch: 120; loss: 1.01; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.95
Val Epoch over. val_loss: 0.90153236411939; val_accuracy: 0.8460390127388535 

The current subspace-distance is: 8.658949809614569e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 0.91; acc: 0.8
Batch: 40; loss: 0.86; acc: 0.86
Batch: 60; loss: 0.95; acc: 0.8
Batch: 80; loss: 1.02; acc: 0.7
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 0.99; acc: 0.81
Batch: 140; loss: 1.06; acc: 0.73
Batch: 160; loss: 0.91; acc: 0.81
Batch: 180; loss: 1.04; acc: 0.78
Batch: 200; loss: 0.86; acc: 0.84
Batch: 220; loss: 0.96; acc: 0.84
Batch: 240; loss: 0.91; acc: 0.81
Batch: 260; loss: 0.93; acc: 0.83
Batch: 280; loss: 0.91; acc: 0.84
Batch: 300; loss: 0.85; acc: 0.89
Batch: 320; loss: 0.85; acc: 0.83
Batch: 340; loss: 0.82; acc: 0.83
Batch: 360; loss: 1.06; acc: 0.7
Batch: 380; loss: 0.91; acc: 0.8
Batch: 400; loss: 0.91; acc: 0.78
Batch: 420; loss: 0.95; acc: 0.83
Batch: 440; loss: 0.86; acc: 0.84
Batch: 460; loss: 0.81; acc: 0.86
Batch: 480; loss: 1.05; acc: 0.77
Batch: 500; loss: 0.81; acc: 0.89
Batch: 520; loss: 1.03; acc: 0.73
Batch: 540; loss: 0.93; acc: 0.84
Batch: 560; loss: 1.02; acc: 0.78
Batch: 580; loss: 0.87; acc: 0.8
Batch: 600; loss: 0.92; acc: 0.8
Batch: 620; loss: 0.96; acc: 0.77
Batch: 640; loss: 0.9; acc: 0.78
Batch: 660; loss: 1.03; acc: 0.77
Batch: 680; loss: 0.76; acc: 0.91
Batch: 700; loss: 0.91; acc: 0.8
Batch: 720; loss: 0.85; acc: 0.86
Batch: 740; loss: 0.87; acc: 0.84
Batch: 760; loss: 0.84; acc: 0.84
Batch: 780; loss: 0.84; acc: 0.81
Train Epoch over. train_loss: 0.91; train_accuracy: 0.82 

0.00011057120718760416
0.00010617302905302495
Batch: 0; loss: 0.89; acc: 0.78
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.52; acc: 0.92
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.59; acc: 0.97
Val Epoch over. val_loss: 0.7890749852748433; val_accuracy: 0.8564888535031847 

The current subspace-distance is: 0.00010617302905302495 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 0.88; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.77; acc: 0.86
Batch: 100; loss: 0.83; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.88
Batch: 140; loss: 0.84; acc: 0.78
Batch: 160; loss: 0.8; acc: 0.84
Batch: 180; loss: 0.87; acc: 0.8
Batch: 200; loss: 0.82; acc: 0.84
Batch: 220; loss: 0.76; acc: 0.86
Batch: 240; loss: 0.69; acc: 0.89
Batch: 260; loss: 0.78; acc: 0.88
Batch: 280; loss: 0.95; acc: 0.81
Batch: 300; loss: 0.79; acc: 0.81
Batch: 320; loss: 0.86; acc: 0.84
Batch: 340; loss: 0.74; acc: 0.89
Batch: 360; loss: 0.93; acc: 0.78
Batch: 380; loss: 0.75; acc: 0.86
Batch: 400; loss: 0.81; acc: 0.88
Batch: 420; loss: 0.91; acc: 0.77
Batch: 440; loss: 0.72; acc: 0.88
Batch: 460; loss: 0.85; acc: 0.86
Batch: 480; loss: 0.71; acc: 0.89
Batch: 500; loss: 0.85; acc: 0.83
Batch: 520; loss: 0.83; acc: 0.83
Batch: 540; loss: 0.78; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.88
Batch: 580; loss: 0.93; acc: 0.77
Batch: 600; loss: 0.73; acc: 0.91
Batch: 620; loss: 0.77; acc: 0.84
Batch: 640; loss: 0.76; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.83
Batch: 680; loss: 0.9; acc: 0.8
Batch: 700; loss: 0.77; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.91
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.89
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.81; train_accuracy: 0.83 

0.00012776210496667773
0.00012160240294178948
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.94
Val Epoch over. val_loss: 0.7183519451861169; val_accuracy: 0.8576831210191083 

The current subspace-distance is: 0.00012160240294178948 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.81
Batch: 40; loss: 0.88; acc: 0.8
Batch: 60; loss: 0.67; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.85; acc: 0.78
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.74; acc: 0.83
Batch: 160; loss: 0.64; acc: 0.94
Batch: 180; loss: 0.81; acc: 0.77
Batch: 200; loss: 0.87; acc: 0.75
Batch: 220; loss: 0.93; acc: 0.75
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.83; acc: 0.75
Batch: 280; loss: 0.65; acc: 0.89
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.77; acc: 0.83
Batch: 360; loss: 0.8; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.88
Batch: 400; loss: 0.85; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.94
Batch: 480; loss: 0.77; acc: 0.77
Batch: 500; loss: 0.99; acc: 0.77
Batch: 520; loss: 0.64; acc: 0.89
Batch: 540; loss: 0.81; acc: 0.83
Batch: 560; loss: 0.79; acc: 0.75
Batch: 580; loss: 0.88; acc: 0.77
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.88
Batch: 640; loss: 0.85; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.73; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.95
Batch: 740; loss: 0.8; acc: 0.88
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.75; train_accuracy: 0.84 

0.00014102229033596814
0.00013507803669199347
Batch: 0; loss: 0.73; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.6659415290234195; val_accuracy: 0.8610668789808917 

The current subspace-distance is: 0.00013507803669199347 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.92; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.65; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.75
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.91
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.86
Batch: 340; loss: 0.81; acc: 0.8
Batch: 360; loss: 0.83; acc: 0.78
Batch: 380; loss: 0.6; acc: 0.92
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.69; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.94
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.62; acc: 0.92
Batch: 520; loss: 0.74; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.89
Batch: 560; loss: 0.77; acc: 0.86
Batch: 580; loss: 0.66; acc: 0.89
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.77; acc: 0.84
Batch: 640; loss: 0.75; acc: 0.8
Batch: 660; loss: 0.74; acc: 0.8
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.86
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.65; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.84 

0.00015354494098573923
0.00014700736210215837
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6250544393518168; val_accuracy: 0.8668391719745223 

The current subspace-distance is: 0.00014700736210215837 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.72; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.74; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.92
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.92; acc: 0.73
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.92
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.7; acc: 0.83
Batch: 340; loss: 0.75; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.92
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.69; acc: 0.86
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.72; acc: 0.75
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.78; acc: 0.75
Batch: 560; loss: 0.69; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.91
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.95
Batch: 780; loss: 0.61; acc: 0.91
Train Epoch over. train_loss: 0.67; train_accuracy: 0.85 

0.00016265643353108317
0.0001575686619617045
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.95
Val Epoch over. val_loss: 0.5910979279666949; val_accuracy: 0.8740047770700637 

The current subspace-distance is: 0.0001575686619617045 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.73
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.74; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.88; acc: 0.75
Batch: 340; loss: 0.68; acc: 0.81
Batch: 360; loss: 0.69; acc: 0.81
Batch: 380; loss: 0.64; acc: 0.89
Batch: 400; loss: 0.74; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.83
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.79; acc: 0.77
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.0001711297081783414
0.00016536802286282182
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.5753284339692183; val_accuracy: 0.8735071656050956 

The current subspace-distance is: 0.00016536802286282182 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.65; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.91
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.92
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.83
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.88
Batch: 240; loss: 0.79; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.47; acc: 0.94
Batch: 420; loss: 0.62; acc: 0.89
Batch: 440; loss: 0.85; acc: 0.75
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.74; acc: 0.77
Batch: 520; loss: 0.5; acc: 0.92
Batch: 540; loss: 0.61; acc: 0.89
Batch: 560; loss: 0.86; acc: 0.72
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.66; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.77
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.73; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.89
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.62; train_accuracy: 0.86 

0.0001792201801436022
0.00017267261864617467
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.94
Val Epoch over. val_loss: 0.5500291722595312; val_accuracy: 0.8774880573248408 

The current subspace-distance is: 0.00017267261864617467 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.94
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.94
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.72; acc: 0.84
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.56; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.00018582268967293203
0.0001772946707205847
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.5385554823905799; val_accuracy: 0.8779856687898089 

The current subspace-distance is: 0.0001772946707205847 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.94
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.9; acc: 0.72
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.76; acc: 0.75
Batch: 520; loss: 0.85; acc: 0.77
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.84; acc: 0.75
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.91
Batch: 720; loss: 0.75; acc: 0.8
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.0001884246157715097
0.00017979128460865468
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.5346037460740205; val_accuracy: 0.8777866242038217 

The current subspace-distance is: 0.00017979128460865468 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.75; acc: 0.73
Batch: 440; loss: 0.56; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.75
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00019084328960161656
0.00018312000611331314
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5262774998784825; val_accuracy: 0.8804737261146497 

The current subspace-distance is: 0.00018312000611331314 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.94
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.95
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

0.0001939008798217401
0.00018789171008393168
Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.5199531034868994; val_accuracy: 0.8803742038216561 

The current subspace-distance is: 0.00018789171008393168 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.97; acc: 0.72
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.77
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.71; acc: 0.83
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.72; acc: 0.78
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.55; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.61; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

0.00019495388551149517
0.00018904329044744372
Batch: 0; loss: 0.56; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.5234298633921678; val_accuracy: 0.8790804140127388 

The current subspace-distance is: 0.00018904329044744372 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 0.52; acc: 0.92
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.8
Batch: 240; loss: 0.56; acc: 0.94
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.64; acc: 0.78
Batch: 540; loss: 0.66; acc: 0.8
Batch: 560; loss: 0.64; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.77
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.68; acc: 0.8
Train Epoch over. train_loss: 0.57; train_accuracy: 0.86 

0.00019527599215507507
0.00018768830341286957
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.5082126284480855; val_accuracy: 0.8808718152866242 

The current subspace-distance is: 0.00018768830341286957 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.58; acc: 0.78
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.94
Batch: 580; loss: 0.77; acc: 0.77
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.8
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.79; acc: 0.72
Train Epoch over. train_loss: 0.57; train_accuracy: 0.86 

0.00019670845358632505
0.0001912767329486087
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.5090375835918317; val_accuracy: 0.8807722929936306 

The current subspace-distance is: 0.0001912767329486087 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.78
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.76; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.92
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.56; train_accuracy: 0.86 

0.0002015790669247508
0.0001949849392985925
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.5037359405475058; val_accuracy: 0.8830613057324841 

The current subspace-distance is: 0.0001949849392985925 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.94
Batch: 40; loss: 0.57; acc: 0.8
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.46; acc: 0.95
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.62; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.97
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.95
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.56; train_accuracy: 0.86 

0.00020230087102390826
0.00019654932839330286
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4981401600655477; val_accuracy: 0.8852507961783439 

The current subspace-distance is: 0.00019654932839330286 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.78
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.75; acc: 0.75
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.74; acc: 0.77
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.73; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.92
Batch: 500; loss: 0.83; acc: 0.8
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.79; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.86 

0.00020457613572943956
0.00019710618653334677
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4981661513922321; val_accuracy: 0.8832603503184714 

The current subspace-distance is: 0.00019710618653334677 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.73; acc: 0.78
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.8
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.86 

0.0002085873275063932
0.00020386677351780236
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4921544193272378; val_accuracy: 0.8845541401273885 

The current subspace-distance is: 0.00020386677351780236 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.78
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.78
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.8
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.78
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020770943956449628
0.00019979465287178755
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.48441716326270134; val_accuracy: 0.8871417197452229 

The current subspace-distance is: 0.00019979465287178755 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.71; acc: 0.78
Batch: 520; loss: 0.46; acc: 0.94
Batch: 540; loss: 0.81; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.82; acc: 0.77
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020795503223780543
0.00020100634719710797
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.49028454018626244; val_accuracy: 0.8854498407643312 

The current subspace-distance is: 0.00020100634719710797 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.72; acc: 0.8
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.78; acc: 0.77
Batch: 640; loss: 0.37; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.95
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020756552112288773
0.00020124354341533035
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.47649287807334; val_accuracy: 0.8886345541401274 

The current subspace-distance is: 0.00020124354341533035 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.67; acc: 0.78
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.73; acc: 0.84
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.000209102887311019
0.00020366869284771383
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4802960531347117; val_accuracy: 0.8890326433121019 

The current subspace-distance is: 0.00020366869284771383 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.94
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.72; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.94
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.94
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.0002118665142916143
0.00020238797878846526
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4763625063903772; val_accuracy: 0.8871417197452229 

The current subspace-distance is: 0.00020238797878846526 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.66; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00020867041894234717
0.0002022806293098256
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.47582806893594704; val_accuracy: 0.8903264331210191 

The current subspace-distance is: 0.0002022806293098256 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.91
Batch: 160; loss: 0.65; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.89
Batch: 500; loss: 0.72; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.68; acc: 0.75
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00021249306155368686
0.00020288689120206982
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.47149437476115624; val_accuracy: 0.8888335987261147 

The current subspace-distance is: 0.00020288689120206982 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.36; acc: 0.97
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.63; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00021168059902265668
0.00020474896882660687
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4628073374746711; val_accuracy: 0.8898288216560509 

The current subspace-distance is: 0.00020474896882660687 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.95
Batch: 680; loss: 0.63; acc: 0.81
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00021084769105073065
0.0002044507273240015
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4715630107434692; val_accuracy: 0.8892316878980892 

The current subspace-distance is: 0.0002044507273240015 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.7; acc: 0.73
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00021339704107958823
0.0002043604908976704
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4749746886408253; val_accuracy: 0.8902269108280255 

The current subspace-distance is: 0.0002043604908976704 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_16_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.51657508881031

The number of parameters is: 267435

The number of individual parameters is:

13
234
13
13
19
38532
19
19
37
109668
37
37
64
113664
64
64
4096
64
640
10
64
64

nonzero elements in E: 106973989
elements in E: 106974000
fraction nonzero: 0.9999998971712752
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.06
Batch: 20; loss: 2.05; acc: 0.31
Batch: 40; loss: 1.84; acc: 0.48
Batch: 60; loss: 1.77; acc: 0.53
Batch: 80; loss: 1.72; acc: 0.53
Batch: 100; loss: 1.57; acc: 0.62
Batch: 120; loss: 1.56; acc: 0.58
Batch: 140; loss: 1.51; acc: 0.61
Batch: 160; loss: 1.53; acc: 0.58
Batch: 180; loss: 1.6; acc: 0.56
Batch: 200; loss: 1.46; acc: 0.66
Batch: 220; loss: 1.46; acc: 0.67
Batch: 240; loss: 1.32; acc: 0.73
Batch: 260; loss: 1.24; acc: 0.84
Batch: 280; loss: 1.33; acc: 0.73
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.33; acc: 0.77
Batch: 360; loss: 1.29; acc: 0.78
Batch: 380; loss: 1.18; acc: 0.88
Batch: 400; loss: 1.19; acc: 0.83
Batch: 420; loss: 1.27; acc: 0.8
Batch: 440; loss: 1.35; acc: 0.64
Batch: 460; loss: 1.17; acc: 0.84
Batch: 480; loss: 1.27; acc: 0.69
Batch: 500; loss: 1.26; acc: 0.73
Batch: 520; loss: 1.18; acc: 0.77
Batch: 540; loss: 1.22; acc: 0.73
Batch: 560; loss: 1.11; acc: 0.86
Batch: 580; loss: 1.29; acc: 0.77
Batch: 600; loss: 1.13; acc: 0.83
Batch: 620; loss: 1.09; acc: 0.86
Batch: 640; loss: 1.11; acc: 0.81
Batch: 660; loss: 1.11; acc: 0.84
Batch: 680; loss: 1.12; acc: 0.75
Batch: 700; loss: 0.99; acc: 0.83
Batch: 720; loss: 1.18; acc: 0.7
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 1.12; acc: 0.77
Batch: 780; loss: 1.11; acc: 0.8
Train Epoch over. train_loss: 1.34; train_accuracy: 0.7 

2.512410719646141e-05
8.468463420285843e-06
Batch: 0; loss: 1.04; acc: 0.86
Batch: 20; loss: 1.14; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.97
Batch: 60; loss: 1.0; acc: 0.8
Batch: 80; loss: 0.91; acc: 0.91
Batch: 100; loss: 0.99; acc: 0.88
Batch: 120; loss: 1.07; acc: 0.84
Batch: 140; loss: 0.84; acc: 0.94
Val Epoch over. val_loss: 1.003050054334531; val_accuracy: 0.8411624203821656 

The current subspace-distance is: 8.468463420285843e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.78
Batch: 20; loss: 0.91; acc: 0.91
Batch: 40; loss: 1.03; acc: 0.78
Batch: 60; loss: 1.11; acc: 0.8
Batch: 80; loss: 1.12; acc: 0.8
Batch: 100; loss: 1.01; acc: 0.83
Batch: 120; loss: 1.02; acc: 0.83
Batch: 140; loss: 1.13; acc: 0.78
Batch: 160; loss: 1.11; acc: 0.8
Batch: 180; loss: 1.19; acc: 0.69
Batch: 200; loss: 1.02; acc: 0.78
Batch: 220; loss: 1.04; acc: 0.77
Batch: 240; loss: 1.01; acc: 0.81
Batch: 260; loss: 0.96; acc: 0.86
Batch: 280; loss: 0.98; acc: 0.83
Batch: 300; loss: 0.9; acc: 0.88
Batch: 320; loss: 1.05; acc: 0.75
Batch: 340; loss: 0.86; acc: 0.89
Batch: 360; loss: 0.95; acc: 0.83
Batch: 380; loss: 0.93; acc: 0.83
Batch: 400; loss: 0.89; acc: 0.86
Batch: 420; loss: 0.86; acc: 0.92
Batch: 440; loss: 0.92; acc: 0.83
Batch: 460; loss: 0.89; acc: 0.88
Batch: 480; loss: 0.94; acc: 0.84
Batch: 500; loss: 0.89; acc: 0.83
Batch: 520; loss: 0.84; acc: 0.92
Batch: 540; loss: 1.0; acc: 0.86
Batch: 560; loss: 1.05; acc: 0.77
Batch: 580; loss: 0.85; acc: 0.84
Batch: 600; loss: 0.92; acc: 0.83
Batch: 620; loss: 0.93; acc: 0.75
Batch: 640; loss: 1.02; acc: 0.77
Batch: 660; loss: 0.85; acc: 0.89
Batch: 680; loss: 0.84; acc: 0.84
Batch: 700; loss: 0.84; acc: 0.84
Batch: 720; loss: 0.78; acc: 0.91
Batch: 740; loss: 0.92; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.94
Batch: 780; loss: 0.83; acc: 0.88
Train Epoch over. train_loss: 0.94; train_accuracy: 0.84 

3.118196036666632e-05
1.2576305380207486e-05
Batch: 0; loss: 0.83; acc: 0.91
Batch: 20; loss: 0.88; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.97
Batch: 60; loss: 0.8; acc: 0.81
Batch: 80; loss: 0.63; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.97
Val Epoch over. val_loss: 0.7618925867566637; val_accuracy: 0.8818670382165605 

The current subspace-distance is: 1.2576305380207486e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.86
Batch: 40; loss: 0.92; acc: 0.83
Batch: 60; loss: 0.77; acc: 0.83
Batch: 80; loss: 0.64; acc: 0.97
Batch: 100; loss: 0.91; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.83
Batch: 140; loss: 0.86; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.91
Batch: 180; loss: 0.83; acc: 0.81
Batch: 200; loss: 0.69; acc: 0.91
Batch: 220; loss: 0.74; acc: 0.92
Batch: 240; loss: 0.68; acc: 0.92
Batch: 260; loss: 0.75; acc: 0.94
Batch: 280; loss: 0.92; acc: 0.81
Batch: 300; loss: 0.67; acc: 0.92
Batch: 320; loss: 0.77; acc: 0.89
Batch: 340; loss: 0.73; acc: 0.89
Batch: 360; loss: 0.73; acc: 0.89
Batch: 380; loss: 0.79; acc: 0.88
Batch: 400; loss: 0.73; acc: 0.86
Batch: 420; loss: 0.8; acc: 0.91
Batch: 440; loss: 0.76; acc: 0.83
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.98
Batch: 500; loss: 0.76; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.89
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.95
Batch: 600; loss: 0.89; acc: 0.8
Batch: 620; loss: 0.63; acc: 0.91
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.92
Batch: 700; loss: 0.69; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.91
Batch: 740; loss: 0.66; acc: 0.91
Batch: 760; loss: 0.77; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.87 

3.6424939025891945e-05
1.600235736987088e-05
Batch: 0; loss: 0.73; acc: 0.88
Batch: 20; loss: 0.74; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.6271958394794707; val_accuracy: 0.8991839171974523 

The current subspace-distance is: 1.600235736987088e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.89
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.88
Batch: 100; loss: 0.75; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.94
Batch: 160; loss: 0.66; acc: 0.91
Batch: 180; loss: 0.72; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.91
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.81
Batch: 280; loss: 0.71; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.94
Batch: 340; loss: 0.62; acc: 0.95
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.74; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.95
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.65; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.89
Batch: 520; loss: 0.62; acc: 0.91
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.68; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.95
Batch: 620; loss: 0.59; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.94
Batch: 660; loss: 0.66; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.92
Batch: 720; loss: 0.6; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.91
Batch: 760; loss: 0.73; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.88 

4.0361650462727994e-05
1.790157148207072e-05
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.97
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.5433840427049406; val_accuracy: 0.9102308917197452 

The current subspace-distance is: 1.790157148207072e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.89
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.92
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 0.68; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.88
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.64; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.97
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.94
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.94
Batch: 620; loss: 0.6; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.95
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.97
Train Epoch over. train_loss: 0.59; train_accuracy: 0.89 

4.436093149706721e-05
2.040884646703489e-05
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.5010117603715059; val_accuracy: 0.9125199044585988 

The current subspace-distance is: 2.040884646703489e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.92
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.97
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.52; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.97
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.91
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.95
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.7282468585763127e-05
2.1399802790256217e-05
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.97
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4519442160418079; val_accuracy: 0.9177945859872612 

The current subspace-distance is: 2.1399802790256217e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.95
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.94
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.92
Batch: 460; loss: 0.69; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.98
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.97
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.9 

5.073484135209583e-05
2.3514845452154987e-05
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.97
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.43720350001647973; val_accuracy: 0.9134156050955414 

The current subspace-distance is: 2.3514845452154987e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.97
Batch: 140; loss: 0.6; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.91
Batch: 220; loss: 0.55; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.97
Batch: 260; loss: 0.47; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.92
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.66; acc: 0.78
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

5.2522580517688766e-05
2.4591932742623612e-05
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.398110482248531; val_accuracy: 0.9192874203821656 

The current subspace-distance is: 2.4591932742623612e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.578211130341515e-05
2.661953476490453e-05
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3790880129405647; val_accuracy: 0.9223726114649682 

The current subspace-distance is: 2.661953476490453e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.6; acc: 0.8
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.95
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.98
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.97
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.95
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

5.900216638110578e-05
2.9152750357752666e-05
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.36432895776192853; val_accuracy: 0.9255573248407644 

The current subspace-distance is: 2.9152750357752666e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.98
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

5.946613600826822e-05
2.88605169771472e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3598979630857516; val_accuracy: 0.9259554140127388 

The current subspace-distance is: 2.88605169771472e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.95
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.97
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

6.009413846186362e-05
2.6753034035209566e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3614531782497266; val_accuracy: 0.9249601910828026 

The current subspace-distance is: 2.6753034035209566e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

6.17709374637343e-05
3.194682358298451e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.34915069414741673; val_accuracy: 0.9257563694267515 

The current subspace-distance is: 3.194682358298451e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.8
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.97
Batch: 500; loss: 0.66; acc: 0.78
Batch: 520; loss: 0.32; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.11922368989326e-05
2.763438351394143e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3448174836908936; val_accuracy: 0.9267515923566879 

The current subspace-distance is: 2.763438351394143e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.97
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.98
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.166763341752812e-05
2.9980499675730243e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3430324229085521; val_accuracy: 0.9268511146496815 

The current subspace-distance is: 2.9980499675730243e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.221552030183375e-05
2.836110616044607e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3390214715129251; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.836110616044607e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.31; acc: 0.97
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.24; acc: 1.0
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.288989243330434e-05
2.8903226848342456e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3390302186369137; val_accuracy: 0.9284434713375797 

The current subspace-distance is: 2.8903226848342456e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.97
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.439725257223472e-05
3.133426434942521e-05
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3320751901549898; val_accuracy: 0.9280453821656051 

The current subspace-distance is: 3.133426434942521e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.98
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.326925358735025e-05
2.8853864932898432e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3286901644081067; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.8853864932898432e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.411067442968488e-05
2.9161417842260562e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.33031447336172604; val_accuracy: 0.9290406050955414 

The current subspace-distance is: 2.9161417842260562e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.522147305076942e-05
3.0267396141425706e-05
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3263837194461731; val_accuracy: 0.9287420382165605 

The current subspace-distance is: 3.0267396141425706e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.97
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.38; acc: 0.95
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.62969978293404e-05
3.152908175252378e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.31963409905790524; val_accuracy: 0.9293391719745223 

The current subspace-distance is: 3.152908175252378e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.606793613173068e-05
3.1633462640456855e-05
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3265885779052783; val_accuracy: 0.9288415605095541 

The current subspace-distance is: 3.1633462640456855e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.81
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.31; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.8
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.49629655526951e-05
3.0306897315313108e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3249356342823642; val_accuracy: 0.9295382165605095 

The current subspace-distance is: 3.0306897315313108e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.23; acc: 1.0
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.95
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.97
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.578831380465999e-05
3.2065974664874375e-05
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3268709467949381; val_accuracy: 0.9282444267515924 

The current subspace-distance is: 3.2065974664874375e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.58806020510383e-05
3.1939543987391517e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.326470443777218; val_accuracy: 0.9279458598726115 

The current subspace-distance is: 3.1939543987391517e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.24; acc: 1.0
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.579420733032748e-05
3.1378229323308915e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.32115090757038944; val_accuracy: 0.9280453821656051 

The current subspace-distance is: 3.1378229323308915e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.98
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.615932943532243e-05
3.165091766277328e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3247736888421569; val_accuracy: 0.9279458598726115 

The current subspace-distance is: 3.165091766277328e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.611552817048505e-05
3.1745792512083426e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.32492923632169224; val_accuracy: 0.9276472929936306 

The current subspace-distance is: 3.1745792512083426e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.98
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.540053436765447e-05
2.923120882769581e-05
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.32293402892389117; val_accuracy: 0.9267515923566879 

The current subspace-distance is: 2.923120882769581e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_16_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.51657508881031

The number of parameters is: 267435

The number of individual parameters is:

13
234
13
13
19
38532
19
19
37
109668
37
37
64
113664
64
64
4096
64
640
10
64
64

nonzero elements in E: 133717489
elements in E: 133717500
fraction nonzero: 0.9999999177370202
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.5; acc: 0.05
Batch: 20; loss: 2.07; acc: 0.31
Batch: 40; loss: 1.78; acc: 0.61
Batch: 60; loss: 1.75; acc: 0.45
Batch: 80; loss: 1.64; acc: 0.66
Batch: 100; loss: 1.53; acc: 0.72
Batch: 120; loss: 1.58; acc: 0.59
Batch: 140; loss: 1.41; acc: 0.78
Batch: 160; loss: 1.39; acc: 0.73
Batch: 180; loss: 1.41; acc: 0.77
Batch: 200; loss: 1.32; acc: 0.75
Batch: 220; loss: 1.24; acc: 0.86
Batch: 240; loss: 1.26; acc: 0.78
Batch: 260; loss: 1.24; acc: 0.78
Batch: 280; loss: 1.18; acc: 0.8
Batch: 300; loss: 1.19; acc: 0.83
Batch: 320; loss: 1.18; acc: 0.78
Batch: 340; loss: 1.19; acc: 0.73
Batch: 360; loss: 1.16; acc: 0.83
Batch: 380; loss: 1.17; acc: 0.75
Batch: 400; loss: 1.07; acc: 0.78
Batch: 420; loss: 1.07; acc: 0.83
Batch: 440; loss: 1.04; acc: 0.84
Batch: 460; loss: 1.09; acc: 0.78
Batch: 480; loss: 1.07; acc: 0.8
Batch: 500; loss: 1.06; acc: 0.77
Batch: 520; loss: 0.95; acc: 0.86
Batch: 540; loss: 1.05; acc: 0.83
Batch: 560; loss: 1.0; acc: 0.88
Batch: 580; loss: 0.99; acc: 0.83
Batch: 600; loss: 0.91; acc: 0.86
Batch: 620; loss: 0.96; acc: 0.84
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 0.92; acc: 0.81
Batch: 680; loss: 0.97; acc: 0.89
Batch: 700; loss: 0.87; acc: 0.88
Batch: 720; loss: 0.77; acc: 0.94
Batch: 740; loss: 0.97; acc: 0.78
Batch: 760; loss: 0.92; acc: 0.84
Batch: 780; loss: 0.84; acc: 0.89
Train Epoch over. train_loss: 1.2; train_accuracy: 0.77 

2.5597995772841386e-05
9.802276508708019e-06
Batch: 0; loss: 0.79; acc: 0.97
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 0.58; acc: 0.97
Batch: 60; loss: 0.77; acc: 0.88
Batch: 80; loss: 0.71; acc: 0.91
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 0.97; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.89
Val Epoch over. val_loss: 0.8283148009306306; val_accuracy: 0.8747014331210191 

The current subspace-distance is: 9.802276508708019e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.86
Batch: 20; loss: 1.0; acc: 0.78
Batch: 40; loss: 0.89; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.94
Batch: 80; loss: 0.93; acc: 0.84
Batch: 100; loss: 0.84; acc: 0.81
Batch: 120; loss: 0.82; acc: 0.84
Batch: 140; loss: 0.93; acc: 0.8
Batch: 160; loss: 0.95; acc: 0.77
Batch: 180; loss: 0.72; acc: 0.91
Batch: 200; loss: 1.11; acc: 0.75
Batch: 220; loss: 0.78; acc: 0.86
Batch: 240; loss: 0.73; acc: 0.94
Batch: 260; loss: 0.74; acc: 0.89
Batch: 280; loss: 0.78; acc: 0.86
Batch: 300; loss: 0.77; acc: 0.86
Batch: 320; loss: 0.88; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.88
Batch: 360; loss: 0.81; acc: 0.88
Batch: 380; loss: 0.75; acc: 0.84
Batch: 400; loss: 0.67; acc: 0.91
Batch: 420; loss: 0.67; acc: 0.91
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.79; acc: 0.86
Batch: 480; loss: 0.76; acc: 0.78
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.8; acc: 0.88
Batch: 540; loss: 0.57; acc: 0.97
Batch: 560; loss: 0.72; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.91
Batch: 600; loss: 0.67; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.83; acc: 0.78
Batch: 660; loss: 0.61; acc: 0.94
Batch: 680; loss: 0.67; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.94
Batch: 740; loss: 0.71; acc: 0.88
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.91
Train Epoch over. train_loss: 0.75; train_accuracy: 0.86 

3.251455564168282e-05
1.377084026898956e-05
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.34; acc: 1.0
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.6046377532421403; val_accuracy: 0.8992834394904459 

The current subspace-distance is: 1.377084026898956e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.91
Batch: 40; loss: 0.64; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.62; acc: 0.94
Batch: 180; loss: 0.58; acc: 0.94
Batch: 200; loss: 0.59; acc: 0.89
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.47; acc: 0.95
Batch: 260; loss: 0.65; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.98
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.97
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.89
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.95
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.98
Batch: 720; loss: 0.54; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.92
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

3.7023208278696984e-05
1.5866966350586154e-05
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.48767427919776574; val_accuracy: 0.9134156050955414 

The current subspace-distance is: 1.5866966350586154e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.95
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.97
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.95
Batch: 540; loss: 0.5; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.95
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.95
Batch: 780; loss: 0.42; acc: 0.98
Train Epoch over. train_loss: 0.51; train_accuracy: 0.9 

4.063863161718473e-05
1.7314820070168935e-05
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4242372456819389; val_accuracy: 0.9210788216560509 

The current subspace-distance is: 1.7314820070168935e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.58; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.97
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.56; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.36; acc: 0.98
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.91 

4.457154864212498e-05
2.014714846154675e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.38114497056052943; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.014714846154675e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.98
Batch: 400; loss: 0.4; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

4.769176302943379e-05
2.1803154595545493e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3437847424844268; val_accuracy: 0.9315286624203821 

The current subspace-distance is: 2.1803154595545493e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.22; acc: 1.0
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

5.018288720748387e-05
2.271282755827997e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.32254038713160593; val_accuracy: 0.9343152866242038 

The current subspace-distance is: 2.271282755827997e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.3675888921134174e-05
2.490729275450576e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.29555894448688835; val_accuracy: 0.9361066878980892 

The current subspace-distance is: 2.490729275450576e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.98
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.98
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.571141082327813e-05
2.511741331545636e-05
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.2774045406633122; val_accuracy: 0.9407842356687898 

The current subspace-distance is: 2.511741331545636e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.15; acc: 1.0
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.98
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.7063702115556225e-05
2.6441355657880194e-05
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.26619896397089504; val_accuracy: 0.9413813694267515 

The current subspace-distance is: 2.6441355657880194e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.21; acc: 1.0
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.98
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.903879355173558e-05
2.754734487098176e-05
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2587643875532849; val_accuracy: 0.9435708598726115 

The current subspace-distance is: 2.754734487098176e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.98
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.98
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.854736809851602e-05
2.611960553622339e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.25629814581316746; val_accuracy: 0.9424761146496815 

The current subspace-distance is: 2.611960553622339e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.2; acc: 1.0
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.853804395883344e-05
2.6285986677976325e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.25424774707692444; val_accuracy: 0.9443670382165605 

The current subspace-distance is: 2.6285986677976325e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.98
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.19; acc: 1.0
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.953366417088546e-05
2.6632320441422053e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.25105301917168743; val_accuracy: 0.944765127388535 

The current subspace-distance is: 2.6632320441422053e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.98
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.935430817771703e-05
2.5794141038204543e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.24731315876458101; val_accuracy: 0.9442675159235668 

The current subspace-distance is: 2.5794141038204543e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.98
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.98
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.0980561102041975e-05
2.7387843147153035e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.24794988049443362; val_accuracy: 0.9446656050955414 

The current subspace-distance is: 2.7387843147153035e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.101811959524639e-05
2.8549005946842954e-05
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2460454884133521; val_accuracy: 0.9445660828025477 

The current subspace-distance is: 2.8549005946842954e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.98
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.98
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.163872603792697e-05
2.8671020118054003e-05
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.24164692241295127; val_accuracy: 0.9442675159235668 

The current subspace-distance is: 2.8671020118054003e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.19; acc: 1.0
Batch: 360; loss: 0.25; acc: 0.97
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.98
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.21332146693021e-05
2.856492210412398e-05
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2408098604553824; val_accuracy: 0.9457603503184714 

The current subspace-distance is: 2.856492210412398e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.98
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.98
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.248499994399026e-05
2.768658487184439e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.23307353694727467; val_accuracy: 0.9465565286624203 

The current subspace-distance is: 2.768658487184439e-05 

Epoch 21 start
The current lr is: 0.09
