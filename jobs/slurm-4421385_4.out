model : table13slim
N : 4
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 4.170581494228353

The number of parameters is: 276580

The number of individual parameters is:

34
544
34
34
51
48552
51
51
101
144228
101
101
64
77568
64
64
4096
64
640
10
64
64

nonzero elements in E: 13828999
elements in E: 13829000
fraction nonzero: 0.9999999276881915
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.42; acc: 0.11
Batch: 20; loss: 2.34; acc: 0.11
Batch: 40; loss: 2.31; acc: 0.2
Batch: 60; loss: 2.35; acc: 0.14
Batch: 80; loss: 2.25; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.17
Batch: 120; loss: 2.24; acc: 0.2
Batch: 140; loss: 2.17; acc: 0.23
Batch: 160; loss: 2.17; acc: 0.22
Batch: 180; loss: 2.09; acc: 0.3
Batch: 200; loss: 2.06; acc: 0.31
Batch: 220; loss: 1.99; acc: 0.41
Batch: 240; loss: 2.14; acc: 0.3
Batch: 260; loss: 2.18; acc: 0.23
Batch: 280; loss: 2.08; acc: 0.23
Batch: 300; loss: 2.09; acc: 0.27
Batch: 320; loss: 2.03; acc: 0.36
Batch: 340; loss: 1.96; acc: 0.39
Batch: 360; loss: 1.94; acc: 0.39
Batch: 380; loss: 1.88; acc: 0.42
Batch: 400; loss: 2.06; acc: 0.38
Batch: 420; loss: 1.98; acc: 0.41
Batch: 440; loss: 1.89; acc: 0.45
Batch: 460; loss: 2.0; acc: 0.42
Batch: 480; loss: 1.94; acc: 0.45
Batch: 500; loss: 1.9; acc: 0.39
Batch: 520; loss: 2.02; acc: 0.33
Batch: 540; loss: 1.99; acc: 0.36
Batch: 560; loss: 1.95; acc: 0.36
Batch: 580; loss: 2.07; acc: 0.25
Batch: 600; loss: 1.87; acc: 0.47
Batch: 620; loss: 1.91; acc: 0.42
Batch: 640; loss: 1.92; acc: 0.47
Batch: 660; loss: 2.05; acc: 0.28
Batch: 680; loss: 1.83; acc: 0.45
Batch: 700; loss: 1.93; acc: 0.39
Batch: 720; loss: 1.95; acc: 0.3
Batch: 740; loss: 1.83; acc: 0.41
Batch: 760; loss: 1.89; acc: 0.44
Batch: 780; loss: 1.91; acc: 0.44
Train Epoch over. train_loss: 2.03; train_accuracy: 0.33 

2.4743767426116392e-05
4.122047812415985e-06
Batch: 0; loss: 1.93; acc: 0.44
Batch: 20; loss: 1.95; acc: 0.39
Batch: 40; loss: 1.74; acc: 0.53
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.82; acc: 0.47
Batch: 100; loss: 1.94; acc: 0.42
Batch: 120; loss: 1.9; acc: 0.44
Batch: 140; loss: 1.88; acc: 0.41
Val Epoch over. val_loss: 1.8838679319734026; val_accuracy: 0.42177547770700635 

The current subspace-distance is: 4.122047812415985e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.87; acc: 0.42
Batch: 20; loss: 1.89; acc: 0.48
Batch: 40; loss: 1.84; acc: 0.48
Batch: 60; loss: 1.85; acc: 0.39
Batch: 80; loss: 1.84; acc: 0.52
Batch: 100; loss: 1.88; acc: 0.39
Batch: 120; loss: 1.8; acc: 0.45
Batch: 140; loss: 1.79; acc: 0.53
Batch: 160; loss: 1.86; acc: 0.33
Batch: 180; loss: 1.77; acc: 0.48
Batch: 200; loss: 1.94; acc: 0.33
Batch: 220; loss: 1.93; acc: 0.42
Batch: 240; loss: 1.86; acc: 0.39
Batch: 260; loss: 1.92; acc: 0.38
Batch: 280; loss: 1.89; acc: 0.41
Batch: 300; loss: 1.86; acc: 0.42
Batch: 320; loss: 1.86; acc: 0.45
Batch: 340; loss: 1.87; acc: 0.41
Batch: 360; loss: 1.75; acc: 0.5
Batch: 380; loss: 1.83; acc: 0.48
Batch: 400; loss: 1.78; acc: 0.5
Batch: 420; loss: 1.77; acc: 0.47
Batch: 440; loss: 1.93; acc: 0.36
Batch: 460; loss: 1.85; acc: 0.39
Batch: 480; loss: 1.81; acc: 0.48
Batch: 500; loss: 1.97; acc: 0.36
Batch: 520; loss: 1.86; acc: 0.31
Batch: 540; loss: 1.78; acc: 0.42
Batch: 560; loss: 1.79; acc: 0.53
Batch: 580; loss: 1.83; acc: 0.52
Batch: 600; loss: 1.8; acc: 0.45
Batch: 620; loss: 1.84; acc: 0.45
Batch: 640; loss: 1.74; acc: 0.44
Batch: 660; loss: 1.87; acc: 0.36
Batch: 680; loss: 1.93; acc: 0.31
Batch: 700; loss: 1.79; acc: 0.48
Batch: 720; loss: 1.85; acc: 0.45
Batch: 740; loss: 1.76; acc: 0.48
Batch: 760; loss: 1.81; acc: 0.42
Batch: 780; loss: 1.7; acc: 0.52
Train Epoch over. train_loss: 1.84; train_accuracy: 0.44 

2.716884591791313e-05
5.641887582896743e-06
Batch: 0; loss: 1.78; acc: 0.42
Batch: 20; loss: 1.88; acc: 0.39
Batch: 40; loss: 1.58; acc: 0.62
Batch: 60; loss: 1.72; acc: 0.53
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.83; acc: 0.44
Batch: 120; loss: 1.84; acc: 0.45
Batch: 140; loss: 1.71; acc: 0.55
Val Epoch over. val_loss: 1.7751147891305814; val_accuracy: 0.4740246815286624 

The current subspace-distance is: 5.641887582896743e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.82; acc: 0.38
Batch: 20; loss: 1.83; acc: 0.44
Batch: 40; loss: 1.7; acc: 0.52
Batch: 60; loss: 1.94; acc: 0.33
Batch: 80; loss: 1.73; acc: 0.56
Batch: 100; loss: 1.88; acc: 0.38
Batch: 120; loss: 1.77; acc: 0.48
Batch: 140; loss: 1.77; acc: 0.38
Batch: 160; loss: 1.8; acc: 0.44
Batch: 180; loss: 1.82; acc: 0.47
Batch: 200; loss: 1.81; acc: 0.41
Batch: 220; loss: 1.78; acc: 0.45
Batch: 240; loss: 1.73; acc: 0.42
Batch: 260; loss: 1.64; acc: 0.53
Batch: 280; loss: 1.77; acc: 0.47
Batch: 300; loss: 1.65; acc: 0.53
Batch: 320; loss: 1.75; acc: 0.47
Batch: 340; loss: 1.86; acc: 0.42
Batch: 360; loss: 1.75; acc: 0.5
Batch: 380; loss: 1.68; acc: 0.53
Batch: 400; loss: 1.68; acc: 0.55
Batch: 420; loss: 1.76; acc: 0.44
Batch: 440; loss: 1.9; acc: 0.44
Batch: 460; loss: 1.86; acc: 0.31
Batch: 480; loss: 1.81; acc: 0.42
Batch: 500; loss: 1.76; acc: 0.47
Batch: 520; loss: 1.77; acc: 0.45
Batch: 540; loss: 1.72; acc: 0.44
Batch: 560; loss: 1.78; acc: 0.48
Batch: 580; loss: 1.78; acc: 0.47
Batch: 600; loss: 1.7; acc: 0.5
Batch: 620; loss: 1.84; acc: 0.39
Batch: 640; loss: 1.71; acc: 0.5
Batch: 660; loss: 1.73; acc: 0.5
Batch: 680; loss: 1.69; acc: 0.48
Batch: 700; loss: 1.76; acc: 0.41
Batch: 720; loss: 1.66; acc: 0.56
Batch: 740; loss: 1.79; acc: 0.44
Batch: 760; loss: 1.75; acc: 0.36
Batch: 780; loss: 1.78; acc: 0.45
Train Epoch over. train_loss: 1.76; train_accuracy: 0.47 

2.955324453068897e-05
8.289921424875502e-06
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.83; acc: 0.39
Batch: 40; loss: 1.51; acc: 0.64
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.83; acc: 0.42
Batch: 120; loss: 1.8; acc: 0.41
Batch: 140; loss: 1.58; acc: 0.59
Val Epoch over. val_loss: 1.703955527323826; val_accuracy: 0.49781050955414013 

The current subspace-distance is: 8.289921424875502e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.52
Batch: 20; loss: 1.77; acc: 0.39
Batch: 40; loss: 1.64; acc: 0.5
Batch: 60; loss: 1.63; acc: 0.61
Batch: 80; loss: 1.74; acc: 0.47
Batch: 100; loss: 1.65; acc: 0.61
Batch: 120; loss: 1.66; acc: 0.53
Batch: 140; loss: 1.71; acc: 0.53
Batch: 160; loss: 1.67; acc: 0.53
Batch: 180; loss: 1.76; acc: 0.47
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.8; acc: 0.44
Batch: 240; loss: 1.75; acc: 0.48
Batch: 260; loss: 1.89; acc: 0.34
Batch: 280; loss: 1.79; acc: 0.52
Batch: 300; loss: 1.72; acc: 0.44
Batch: 320; loss: 1.67; acc: 0.5
Batch: 340; loss: 1.68; acc: 0.62
Batch: 360; loss: 1.75; acc: 0.39
Batch: 380; loss: 1.73; acc: 0.47
Batch: 400; loss: 1.68; acc: 0.48
Batch: 420; loss: 1.68; acc: 0.47
Batch: 440; loss: 1.71; acc: 0.48
Batch: 460; loss: 1.58; acc: 0.56
Batch: 480; loss: 1.65; acc: 0.48
Batch: 500; loss: 1.8; acc: 0.38
Batch: 520; loss: 1.72; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.53
Batch: 560; loss: 1.72; acc: 0.48
Batch: 580; loss: 1.7; acc: 0.47
Batch: 600; loss: 1.7; acc: 0.47
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.67; acc: 0.45
Batch: 660; loss: 1.73; acc: 0.42
Batch: 680; loss: 1.7; acc: 0.52
Batch: 700; loss: 1.65; acc: 0.48
Batch: 720; loss: 1.57; acc: 0.58
Batch: 740; loss: 1.71; acc: 0.48
Batch: 760; loss: 1.6; acc: 0.64
Batch: 780; loss: 1.67; acc: 0.53
Train Epoch over. train_loss: 1.7; train_accuracy: 0.49 

3.206652763765305e-05
7.702060429437552e-06
Batch: 0; loss: 1.68; acc: 0.53
Batch: 20; loss: 1.77; acc: 0.47
Batch: 40; loss: 1.46; acc: 0.62
Batch: 60; loss: 1.63; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.64
Batch: 100; loss: 1.8; acc: 0.41
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.51; acc: 0.66
Val Epoch over. val_loss: 1.656952063748791; val_accuracy: 0.5198049363057324 

The current subspace-distance is: 7.702060429437552e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.56
Batch: 20; loss: 1.62; acc: 0.58
Batch: 40; loss: 1.73; acc: 0.47
Batch: 60; loss: 1.79; acc: 0.52
Batch: 80; loss: 1.72; acc: 0.45
Batch: 100; loss: 1.74; acc: 0.47
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.64; acc: 0.44
Batch: 160; loss: 1.61; acc: 0.62
Batch: 180; loss: 1.67; acc: 0.5
Batch: 200; loss: 1.6; acc: 0.55
Batch: 220; loss: 1.69; acc: 0.48
Batch: 240; loss: 1.64; acc: 0.53
Batch: 260; loss: 1.66; acc: 0.47
Batch: 280; loss: 1.62; acc: 0.52
Batch: 300; loss: 1.68; acc: 0.52
Batch: 320; loss: 1.62; acc: 0.53
Batch: 340; loss: 1.66; acc: 0.52
Batch: 360; loss: 1.64; acc: 0.5
Batch: 380; loss: 1.75; acc: 0.45
Batch: 400; loss: 1.65; acc: 0.58
Batch: 420; loss: 1.6; acc: 0.58
Batch: 440; loss: 1.58; acc: 0.53
Batch: 460; loss: 1.6; acc: 0.52
Batch: 480; loss: 1.76; acc: 0.45
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.58; acc: 0.53
Batch: 540; loss: 1.72; acc: 0.5
Batch: 560; loss: 1.59; acc: 0.52
Batch: 580; loss: 1.65; acc: 0.47
Batch: 600; loss: 1.55; acc: 0.52
Batch: 620; loss: 1.67; acc: 0.47
Batch: 640; loss: 1.68; acc: 0.55
Batch: 660; loss: 1.61; acc: 0.59
Batch: 680; loss: 1.77; acc: 0.47
Batch: 700; loss: 1.53; acc: 0.67
Batch: 720; loss: 1.66; acc: 0.48
Batch: 740; loss: 1.72; acc: 0.47
Batch: 760; loss: 1.62; acc: 0.5
Batch: 780; loss: 1.75; acc: 0.42
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.232812014175579e-05
8.373821401619352e-06
Batch: 0; loss: 1.67; acc: 0.45
Batch: 20; loss: 1.76; acc: 0.47
Batch: 40; loss: 1.41; acc: 0.7
Batch: 60; loss: 1.6; acc: 0.64
Batch: 80; loss: 1.47; acc: 0.59
Batch: 100; loss: 1.75; acc: 0.39
Batch: 120; loss: 1.7; acc: 0.47
Batch: 140; loss: 1.48; acc: 0.67
Val Epoch over. val_loss: 1.6191636164476917; val_accuracy: 0.533937101910828 

The current subspace-distance is: 8.373821401619352e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.48
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.63; acc: 0.58
Batch: 60; loss: 1.8; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.78; acc: 0.45
Batch: 120; loss: 1.71; acc: 0.41
Batch: 140; loss: 1.63; acc: 0.55
Batch: 160; loss: 1.75; acc: 0.41
Batch: 180; loss: 1.77; acc: 0.45
Batch: 200; loss: 1.63; acc: 0.55
Batch: 220; loss: 1.57; acc: 0.56
Batch: 240; loss: 1.56; acc: 0.55
Batch: 260; loss: 1.67; acc: 0.53
Batch: 280; loss: 1.64; acc: 0.53
Batch: 300; loss: 1.67; acc: 0.56
Batch: 320; loss: 1.72; acc: 0.39
Batch: 340; loss: 1.67; acc: 0.55
Batch: 360; loss: 1.66; acc: 0.47
Batch: 380; loss: 1.72; acc: 0.44
Batch: 400; loss: 1.75; acc: 0.39
Batch: 420; loss: 1.47; acc: 0.59
Batch: 440; loss: 1.59; acc: 0.53
Batch: 460; loss: 1.56; acc: 0.5
Batch: 480; loss: 1.6; acc: 0.5
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.63; acc: 0.59
Batch: 540; loss: 1.65; acc: 0.45
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.51; acc: 0.59
Batch: 600; loss: 1.69; acc: 0.45
Batch: 620; loss: 1.55; acc: 0.55
Batch: 640; loss: 1.64; acc: 0.53
Batch: 660; loss: 1.59; acc: 0.59
Batch: 680; loss: 1.66; acc: 0.47
Batch: 700; loss: 1.55; acc: 0.59
Batch: 720; loss: 1.56; acc: 0.61
Batch: 740; loss: 1.56; acc: 0.62
Batch: 760; loss: 1.63; acc: 0.58
Batch: 780; loss: 1.58; acc: 0.48
Train Epoch over. train_loss: 1.62; train_accuracy: 0.53 

3.4750271879602224e-05
7.945026482047979e-06
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.73; acc: 0.5
Batch: 40; loss: 1.36; acc: 0.73
Batch: 60; loss: 1.56; acc: 0.62
Batch: 80; loss: 1.44; acc: 0.64
Batch: 100; loss: 1.7; acc: 0.39
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.59
Val Epoch over. val_loss: 1.587059066553784; val_accuracy: 0.5470740445859873 

The current subspace-distance is: 7.945026482047979e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.48
Batch: 20; loss: 1.69; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.47
Batch: 60; loss: 1.52; acc: 0.67
Batch: 80; loss: 1.57; acc: 0.5
Batch: 100; loss: 1.54; acc: 0.56
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.55; acc: 0.56
Batch: 160; loss: 1.55; acc: 0.61
Batch: 180; loss: 1.52; acc: 0.56
Batch: 200; loss: 1.65; acc: 0.47
Batch: 220; loss: 1.63; acc: 0.48
Batch: 240; loss: 1.57; acc: 0.53
Batch: 260; loss: 1.57; acc: 0.5
Batch: 280; loss: 1.58; acc: 0.52
Batch: 300; loss: 1.56; acc: 0.61
Batch: 320; loss: 1.57; acc: 0.58
Batch: 340; loss: 1.61; acc: 0.55
Batch: 360; loss: 1.61; acc: 0.5
Batch: 380; loss: 1.75; acc: 0.45
Batch: 400; loss: 1.5; acc: 0.58
Batch: 420; loss: 1.58; acc: 0.58
Batch: 440; loss: 1.64; acc: 0.47
Batch: 460; loss: 1.61; acc: 0.56
Batch: 480; loss: 1.64; acc: 0.59
Batch: 500; loss: 1.6; acc: 0.48
Batch: 520; loss: 1.58; acc: 0.53
Batch: 540; loss: 1.58; acc: 0.62
Batch: 560; loss: 1.66; acc: 0.56
Batch: 580; loss: 1.55; acc: 0.58
Batch: 600; loss: 1.54; acc: 0.52
Batch: 620; loss: 1.61; acc: 0.52
Batch: 640; loss: 1.59; acc: 0.53
Batch: 660; loss: 1.7; acc: 0.42
Batch: 680; loss: 1.54; acc: 0.58
Batch: 700; loss: 1.49; acc: 0.58
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.5; acc: 0.58
Batch: 760; loss: 1.66; acc: 0.47
Batch: 780; loss: 1.61; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.53 

3.611247302615084e-05
1.0409264177724253e-05
Batch: 0; loss: 1.6; acc: 0.44
Batch: 20; loss: 1.74; acc: 0.45
Batch: 40; loss: 1.34; acc: 0.7
Batch: 60; loss: 1.57; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.64
Batch: 100; loss: 1.7; acc: 0.42
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.48; acc: 0.58
Val Epoch over. val_loss: 1.5886927212879156; val_accuracy: 0.5426950636942676 

The current subspace-distance is: 1.0409264177724253e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.6; acc: 0.52
Batch: 20; loss: 1.72; acc: 0.48
Batch: 40; loss: 1.59; acc: 0.59
Batch: 60; loss: 1.58; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.53
Batch: 100; loss: 1.66; acc: 0.5
Batch: 120; loss: 1.72; acc: 0.38
Batch: 140; loss: 1.58; acc: 0.53
Batch: 160; loss: 1.65; acc: 0.5
Batch: 180; loss: 1.51; acc: 0.61
Batch: 200; loss: 1.56; acc: 0.59
Batch: 220; loss: 1.58; acc: 0.53
Batch: 240; loss: 1.7; acc: 0.41
Batch: 260; loss: 1.62; acc: 0.48
Batch: 280; loss: 1.6; acc: 0.52
Batch: 300; loss: 1.54; acc: 0.55
Batch: 320; loss: 1.57; acc: 0.55
Batch: 340; loss: 1.61; acc: 0.62
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.62; acc: 0.53
Batch: 400; loss: 1.6; acc: 0.52
Batch: 420; loss: 1.56; acc: 0.56
Batch: 440; loss: 1.52; acc: 0.56
Batch: 460; loss: 1.7; acc: 0.44
Batch: 480; loss: 1.54; acc: 0.53
Batch: 500; loss: 1.59; acc: 0.56
Batch: 520; loss: 1.63; acc: 0.53
Batch: 540; loss: 1.68; acc: 0.52
Batch: 560; loss: 1.69; acc: 0.5
Batch: 580; loss: 1.49; acc: 0.66
Batch: 600; loss: 1.56; acc: 0.55
Batch: 620; loss: 1.5; acc: 0.56
Batch: 640; loss: 1.46; acc: 0.64
Batch: 660; loss: 1.57; acc: 0.52
Batch: 680; loss: 1.63; acc: 0.52
Batch: 700; loss: 1.48; acc: 0.56
Batch: 720; loss: 1.68; acc: 0.47
Batch: 740; loss: 1.55; acc: 0.53
Batch: 760; loss: 1.54; acc: 0.61
Batch: 780; loss: 1.61; acc: 0.52
Train Epoch over. train_loss: 1.6; train_accuracy: 0.53 

3.80667406716384e-05
1.2093375516997185e-05
Batch: 0; loss: 1.59; acc: 0.44
Batch: 20; loss: 1.72; acc: 0.44
Batch: 40; loss: 1.32; acc: 0.73
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.43; acc: 0.66
Batch: 100; loss: 1.66; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.46; acc: 0.56
Val Epoch over. val_loss: 1.571176543357266; val_accuracy: 0.546875 

The current subspace-distance is: 1.2093375516997185e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.55; acc: 0.48
Batch: 40; loss: 1.58; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.5
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.56; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.67
Batch: 160; loss: 1.66; acc: 0.55
Batch: 180; loss: 1.57; acc: 0.59
Batch: 200; loss: 1.47; acc: 0.56
Batch: 220; loss: 1.5; acc: 0.58
Batch: 240; loss: 1.51; acc: 0.55
Batch: 260; loss: 1.62; acc: 0.58
Batch: 280; loss: 1.67; acc: 0.5
Batch: 300; loss: 1.7; acc: 0.52
Batch: 320; loss: 1.4; acc: 0.64
Batch: 340; loss: 1.55; acc: 0.55
Batch: 360; loss: 1.66; acc: 0.47
Batch: 380; loss: 1.62; acc: 0.42
Batch: 400; loss: 1.58; acc: 0.55
Batch: 420; loss: 1.59; acc: 0.55
Batch: 440; loss: 1.52; acc: 0.52
Batch: 460; loss: 1.49; acc: 0.56
Batch: 480; loss: 1.45; acc: 0.67
Batch: 500; loss: 1.57; acc: 0.53
Batch: 520; loss: 1.56; acc: 0.58
Batch: 540; loss: 1.58; acc: 0.52
Batch: 560; loss: 1.67; acc: 0.42
Batch: 580; loss: 1.49; acc: 0.55
Batch: 600; loss: 1.44; acc: 0.58
Batch: 620; loss: 1.62; acc: 0.48
Batch: 640; loss: 1.61; acc: 0.48
Batch: 660; loss: 1.53; acc: 0.56
Batch: 680; loss: 1.56; acc: 0.64
Batch: 700; loss: 1.49; acc: 0.61
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.61; acc: 0.45
Batch: 760; loss: 1.52; acc: 0.53
Batch: 780; loss: 1.62; acc: 0.52
Train Epoch over. train_loss: 1.59; train_accuracy: 0.53 

3.865686448989436e-05
1.021650132315699e-05
Batch: 0; loss: 1.58; acc: 0.45
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.3; acc: 0.72
Batch: 60; loss: 1.53; acc: 0.62
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.62; acc: 0.45
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.47; acc: 0.53
Val Epoch over. val_loss: 1.5581974178362803; val_accuracy: 0.5554339171974523 

The current subspace-distance is: 1.021650132315699e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.52
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.49; acc: 0.61
Batch: 60; loss: 1.63; acc: 0.59
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.61; acc: 0.62
Batch: 120; loss: 1.47; acc: 0.61
Batch: 140; loss: 1.59; acc: 0.56
Batch: 160; loss: 1.54; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.5
Batch: 200; loss: 1.44; acc: 0.66
Batch: 220; loss: 1.52; acc: 0.56
Batch: 240; loss: 1.78; acc: 0.44
Batch: 260; loss: 1.68; acc: 0.55
Batch: 280; loss: 1.69; acc: 0.48
Batch: 300; loss: 1.48; acc: 0.58
Batch: 320; loss: 1.53; acc: 0.56
Batch: 340; loss: 1.59; acc: 0.58
Batch: 360; loss: 1.6; acc: 0.55
Batch: 380; loss: 1.6; acc: 0.55
Batch: 400; loss: 1.54; acc: 0.56
Batch: 420; loss: 1.61; acc: 0.52
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.56; acc: 0.52
Batch: 480; loss: 1.59; acc: 0.59
Batch: 500; loss: 1.58; acc: 0.58
Batch: 520; loss: 1.56; acc: 0.5
Batch: 540; loss: 1.54; acc: 0.56
Batch: 560; loss: 1.5; acc: 0.59
Batch: 580; loss: 1.5; acc: 0.61
Batch: 600; loss: 1.36; acc: 0.66
Batch: 620; loss: 1.57; acc: 0.58
Batch: 640; loss: 1.58; acc: 0.61
Batch: 660; loss: 1.57; acc: 0.47
Batch: 680; loss: 1.57; acc: 0.48
Batch: 700; loss: 1.45; acc: 0.58
Batch: 720; loss: 1.54; acc: 0.48
Batch: 740; loss: 1.43; acc: 0.61
Batch: 760; loss: 1.49; acc: 0.53
Batch: 780; loss: 1.55; acc: 0.58
Train Epoch over. train_loss: 1.58; train_accuracy: 0.53 

4.0155489841708913e-05
1.0338565516576637e-05
Batch: 0; loss: 1.58; acc: 0.47
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.32; acc: 0.73
Batch: 60; loss: 1.54; acc: 0.59
Batch: 80; loss: 1.43; acc: 0.66
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.49; acc: 0.55
Val Epoch over. val_loss: 1.5552317306494257; val_accuracy: 0.5575238853503185 

The current subspace-distance is: 1.0338565516576637e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.53
Batch: 20; loss: 1.57; acc: 0.5
Batch: 40; loss: 1.5; acc: 0.59
Batch: 60; loss: 1.68; acc: 0.44
Batch: 80; loss: 1.57; acc: 0.55
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.47; acc: 0.59
Batch: 140; loss: 1.4; acc: 0.66
Batch: 160; loss: 1.68; acc: 0.44
Batch: 180; loss: 1.63; acc: 0.45
Batch: 200; loss: 1.46; acc: 0.59
Batch: 220; loss: 1.58; acc: 0.52
Batch: 240; loss: 1.43; acc: 0.56
Batch: 260; loss: 1.52; acc: 0.56
Batch: 280; loss: 1.57; acc: 0.53
Batch: 300; loss: 1.48; acc: 0.61
Batch: 320; loss: 1.72; acc: 0.45
Batch: 340; loss: 1.57; acc: 0.5
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.57; acc: 0.45
Batch: 400; loss: 1.56; acc: 0.52
Batch: 420; loss: 1.59; acc: 0.5
Batch: 440; loss: 1.37; acc: 0.72
Batch: 460; loss: 1.59; acc: 0.55
Batch: 480; loss: 1.54; acc: 0.56
Batch: 500; loss: 1.44; acc: 0.58
Batch: 520; loss: 1.48; acc: 0.62
Batch: 540; loss: 1.54; acc: 0.58
Batch: 560; loss: 1.46; acc: 0.58
Batch: 580; loss: 1.68; acc: 0.52
Batch: 600; loss: 1.55; acc: 0.53
Batch: 620; loss: 1.57; acc: 0.55
Batch: 640; loss: 1.53; acc: 0.58
Batch: 660; loss: 1.51; acc: 0.66
Batch: 680; loss: 1.51; acc: 0.5
Batch: 700; loss: 1.44; acc: 0.58
Batch: 720; loss: 1.55; acc: 0.56
Batch: 740; loss: 1.54; acc: 0.59
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.48; acc: 0.58
Train Epoch over. train_loss: 1.57; train_accuracy: 0.54 

4.088816058356315e-05
1.081312893802533e-05
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.72
Batch: 60; loss: 1.55; acc: 0.62
Batch: 80; loss: 1.43; acc: 0.64
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.63; acc: 0.5
Batch: 140; loss: 1.49; acc: 0.56
Val Epoch over. val_loss: 1.5531219107330225; val_accuracy: 0.5577229299363057 

The current subspace-distance is: 1.081312893802533e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.62; acc: 0.45
Batch: 40; loss: 1.62; acc: 0.5
Batch: 60; loss: 1.43; acc: 0.67
Batch: 80; loss: 1.62; acc: 0.52
Batch: 100; loss: 1.54; acc: 0.58
Batch: 120; loss: 1.59; acc: 0.52
Batch: 140; loss: 1.53; acc: 0.62
Batch: 160; loss: 1.6; acc: 0.55
Batch: 180; loss: 1.68; acc: 0.52
Batch: 200; loss: 1.61; acc: 0.55
Batch: 220; loss: 1.42; acc: 0.55
Batch: 240; loss: 1.58; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.48
Batch: 280; loss: 1.57; acc: 0.55
Batch: 300; loss: 1.45; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.66
Batch: 340; loss: 1.57; acc: 0.61
Batch: 360; loss: 1.59; acc: 0.56
Batch: 380; loss: 1.64; acc: 0.47
Batch: 400; loss: 1.62; acc: 0.52
Batch: 420; loss: 1.49; acc: 0.53
Batch: 440; loss: 1.45; acc: 0.59
Batch: 460; loss: 1.41; acc: 0.59
Batch: 480; loss: 1.51; acc: 0.56
Batch: 500; loss: 1.57; acc: 0.58
Batch: 520; loss: 1.64; acc: 0.52
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.43; acc: 0.64
Batch: 580; loss: 1.48; acc: 0.61
Batch: 600; loss: 1.48; acc: 0.58
Batch: 620; loss: 1.58; acc: 0.55
Batch: 640; loss: 1.74; acc: 0.41
Batch: 660; loss: 1.44; acc: 0.56
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.68; acc: 0.48
Batch: 720; loss: 1.73; acc: 0.42
Batch: 740; loss: 1.61; acc: 0.5
Batch: 760; loss: 1.49; acc: 0.58
Batch: 780; loss: 1.52; acc: 0.62
Train Epoch over. train_loss: 1.56; train_accuracy: 0.54 

4.1620278352638707e-05
1.229452209372539e-05
Batch: 0; loss: 1.58; acc: 0.45
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.32; acc: 0.73
Batch: 60; loss: 1.53; acc: 0.62
Batch: 80; loss: 1.42; acc: 0.7
Batch: 100; loss: 1.57; acc: 0.5
Batch: 120; loss: 1.61; acc: 0.47
Batch: 140; loss: 1.47; acc: 0.53
Val Epoch over. val_loss: 1.5448426198048197; val_accuracy: 0.5594148089171974 

The current subspace-distance is: 1.229452209372539e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.48
Batch: 20; loss: 1.38; acc: 0.66
Batch: 40; loss: 1.45; acc: 0.59
Batch: 60; loss: 1.61; acc: 0.48
Batch: 80; loss: 1.65; acc: 0.53
Batch: 100; loss: 1.68; acc: 0.45
Batch: 120; loss: 1.8; acc: 0.38
Batch: 140; loss: 1.69; acc: 0.42
Batch: 160; loss: 1.5; acc: 0.52
Batch: 180; loss: 1.51; acc: 0.58
Batch: 200; loss: 1.52; acc: 0.53
Batch: 220; loss: 1.68; acc: 0.44
Batch: 240; loss: 1.64; acc: 0.41
Batch: 260; loss: 1.53; acc: 0.59
Batch: 280; loss: 1.6; acc: 0.52
Batch: 300; loss: 1.51; acc: 0.55
Batch: 320; loss: 1.65; acc: 0.53
Batch: 340; loss: 1.49; acc: 0.64
Batch: 360; loss: 1.65; acc: 0.5
Batch: 380; loss: 1.56; acc: 0.55
Batch: 400; loss: 1.63; acc: 0.55
Batch: 420; loss: 1.67; acc: 0.41
Batch: 440; loss: 1.56; acc: 0.59
Batch: 460; loss: 1.54; acc: 0.56
Batch: 480; loss: 1.47; acc: 0.7
Batch: 500; loss: 1.64; acc: 0.52
Batch: 520; loss: 1.53; acc: 0.55
Batch: 540; loss: 1.4; acc: 0.7
Batch: 560; loss: 1.52; acc: 0.59
Batch: 580; loss: 1.48; acc: 0.59
Batch: 600; loss: 1.64; acc: 0.53
Batch: 620; loss: 1.57; acc: 0.58
Batch: 640; loss: 1.49; acc: 0.66
Batch: 660; loss: 1.41; acc: 0.62
Batch: 680; loss: 1.41; acc: 0.62
Batch: 700; loss: 1.66; acc: 0.42
Batch: 720; loss: 1.47; acc: 0.66
Batch: 740; loss: 1.4; acc: 0.67
Batch: 760; loss: 1.54; acc: 0.52
Batch: 780; loss: 1.43; acc: 0.62
Train Epoch over. train_loss: 1.56; train_accuracy: 0.54 

4.218727190163918e-05
1.2277824680495542e-05
Batch: 0; loss: 1.58; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.48
Batch: 40; loss: 1.31; acc: 0.72
Batch: 60; loss: 1.54; acc: 0.58
Batch: 80; loss: 1.44; acc: 0.64
Batch: 100; loss: 1.59; acc: 0.5
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.53
Val Epoch over. val_loss: 1.5457247351385226; val_accuracy: 0.555234872611465 

The current subspace-distance is: 1.2277824680495542e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.62
Batch: 20; loss: 1.47; acc: 0.64
Batch: 40; loss: 1.51; acc: 0.61
Batch: 60; loss: 1.64; acc: 0.53
Batch: 80; loss: 1.46; acc: 0.59
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.53; acc: 0.56
Batch: 140; loss: 1.45; acc: 0.67
Batch: 160; loss: 1.41; acc: 0.66
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.61; acc: 0.55
Batch: 220; loss: 1.47; acc: 0.53
Batch: 240; loss: 1.59; acc: 0.5
Batch: 260; loss: 1.47; acc: 0.55
Batch: 280; loss: 1.52; acc: 0.61
Batch: 300; loss: 1.62; acc: 0.5
Batch: 320; loss: 1.52; acc: 0.55
Batch: 340; loss: 1.47; acc: 0.55
Batch: 360; loss: 1.56; acc: 0.53
Batch: 380; loss: 1.52; acc: 0.52
Batch: 400; loss: 1.62; acc: 0.44
Batch: 420; loss: 1.65; acc: 0.48
Batch: 440; loss: 1.55; acc: 0.5
Batch: 460; loss: 1.55; acc: 0.56
Batch: 480; loss: 1.63; acc: 0.58
Batch: 500; loss: 1.61; acc: 0.58
Batch: 520; loss: 1.59; acc: 0.55
Batch: 540; loss: 1.65; acc: 0.47
Batch: 560; loss: 1.56; acc: 0.56
Batch: 580; loss: 1.47; acc: 0.58
Batch: 600; loss: 1.52; acc: 0.59
Batch: 620; loss: 1.76; acc: 0.44
Batch: 640; loss: 1.48; acc: 0.62
Batch: 660; loss: 1.75; acc: 0.36
Batch: 680; loss: 1.45; acc: 0.62
Batch: 700; loss: 1.5; acc: 0.59
Batch: 720; loss: 1.66; acc: 0.5
Batch: 740; loss: 1.41; acc: 0.61
Batch: 760; loss: 1.59; acc: 0.55
Batch: 780; loss: 1.51; acc: 0.56
Train Epoch over. train_loss: 1.56; train_accuracy: 0.54 

4.2629682866390795e-05
1.307106595049845e-05
Batch: 0; loss: 1.57; acc: 0.45
Batch: 20; loss: 1.67; acc: 0.5
Batch: 40; loss: 1.31; acc: 0.72
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.62
Batch: 100; loss: 1.57; acc: 0.5
Batch: 120; loss: 1.62; acc: 0.5
Batch: 140; loss: 1.48; acc: 0.55
Val Epoch over. val_loss: 1.5343391728249325; val_accuracy: 0.5626990445859873 

The current subspace-distance is: 1.307106595049845e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.55
Batch: 20; loss: 1.47; acc: 0.59
Batch: 40; loss: 1.68; acc: 0.52
Batch: 60; loss: 1.72; acc: 0.47
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.45; acc: 0.61
Batch: 120; loss: 1.48; acc: 0.61
Batch: 140; loss: 1.51; acc: 0.55
Batch: 160; loss: 1.54; acc: 0.61
Batch: 180; loss: 1.57; acc: 0.56
Batch: 200; loss: 1.66; acc: 0.47
Batch: 220; loss: 1.55; acc: 0.5
Batch: 240; loss: 1.48; acc: 0.64
Batch: 260; loss: 1.58; acc: 0.56
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.5; acc: 0.52
Batch: 320; loss: 1.53; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.53
Batch: 360; loss: 1.62; acc: 0.53
Batch: 380; loss: 1.6; acc: 0.44
Batch: 400; loss: 1.49; acc: 0.56
Batch: 420; loss: 1.58; acc: 0.55
Batch: 440; loss: 1.56; acc: 0.58
Batch: 460; loss: 1.59; acc: 0.58
Batch: 480; loss: 1.61; acc: 0.59
Batch: 500; loss: 1.58; acc: 0.52
Batch: 520; loss: 1.55; acc: 0.55
Batch: 540; loss: 1.45; acc: 0.61
Batch: 560; loss: 1.4; acc: 0.69
Batch: 580; loss: 1.54; acc: 0.52
Batch: 600; loss: 1.43; acc: 0.59
Batch: 620; loss: 1.59; acc: 0.52
Batch: 640; loss: 1.71; acc: 0.5
Batch: 660; loss: 1.65; acc: 0.45
Batch: 680; loss: 1.49; acc: 0.56
Batch: 700; loss: 1.69; acc: 0.59
Batch: 720; loss: 1.65; acc: 0.48
Batch: 740; loss: 1.61; acc: 0.44
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.68; acc: 0.55
Train Epoch over. train_loss: 1.55; train_accuracy: 0.55 

4.291207005735487e-05
1.2752045222441666e-05
Batch: 0; loss: 1.58; acc: 0.44
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.32; acc: 0.7
Batch: 60; loss: 1.55; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.56
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.53
Val Epoch over. val_loss: 1.545744387966812; val_accuracy: 0.5537420382165605 

The current subspace-distance is: 1.2752045222441666e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.48; acc: 0.61
Batch: 20; loss: 1.54; acc: 0.58
Batch: 40; loss: 1.47; acc: 0.64
Batch: 60; loss: 1.65; acc: 0.59
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.53; acc: 0.55
Batch: 140; loss: 1.52; acc: 0.48
Batch: 160; loss: 1.36; acc: 0.59
Batch: 180; loss: 1.62; acc: 0.44
Batch: 200; loss: 1.49; acc: 0.59
Batch: 220; loss: 1.42; acc: 0.61
Batch: 240; loss: 1.56; acc: 0.53
Batch: 260; loss: 1.53; acc: 0.55
Batch: 280; loss: 1.55; acc: 0.52
Batch: 300; loss: 1.53; acc: 0.53
Batch: 320; loss: 1.52; acc: 0.5
Batch: 340; loss: 1.57; acc: 0.53
Batch: 360; loss: 1.54; acc: 0.55
Batch: 380; loss: 1.73; acc: 0.42
Batch: 400; loss: 1.64; acc: 0.53
Batch: 420; loss: 1.47; acc: 0.61
Batch: 440; loss: 1.56; acc: 0.58
Batch: 460; loss: 1.48; acc: 0.5
Batch: 480; loss: 1.46; acc: 0.59
Batch: 500; loss: 1.69; acc: 0.44
Batch: 520; loss: 1.54; acc: 0.59
Batch: 540; loss: 1.59; acc: 0.5
Batch: 560; loss: 1.5; acc: 0.64
Batch: 580; loss: 1.44; acc: 0.56
Batch: 600; loss: 1.67; acc: 0.53
Batch: 620; loss: 1.53; acc: 0.62
Batch: 640; loss: 1.55; acc: 0.53
Batch: 660; loss: 1.34; acc: 0.69
Batch: 680; loss: 1.46; acc: 0.53
Batch: 700; loss: 1.62; acc: 0.47
Batch: 720; loss: 1.6; acc: 0.52
Batch: 740; loss: 1.51; acc: 0.55
Batch: 760; loss: 1.59; acc: 0.47
Batch: 780; loss: 1.54; acc: 0.58
Train Epoch over. train_loss: 1.55; train_accuracy: 0.55 

4.3188458221266046e-05
1.2585154763655737e-05
Batch: 0; loss: 1.58; acc: 0.42
Batch: 20; loss: 1.67; acc: 0.53
Batch: 40; loss: 1.32; acc: 0.67
Batch: 60; loss: 1.55; acc: 0.58
Batch: 80; loss: 1.44; acc: 0.59
Batch: 100; loss: 1.56; acc: 0.5
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.49; acc: 0.53
Val Epoch over. val_loss: 1.5403036904183163; val_accuracy: 0.5625995222929936 

The current subspace-distance is: 1.2585154763655737e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.54; acc: 0.53
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.58; acc: 0.55
Batch: 60; loss: 1.62; acc: 0.5
Batch: 80; loss: 1.47; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.6; acc: 0.44
Batch: 140; loss: 1.6; acc: 0.44
Batch: 160; loss: 1.52; acc: 0.52
Batch: 180; loss: 1.55; acc: 0.53
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.48; acc: 0.66
Batch: 240; loss: 1.49; acc: 0.58
Batch: 260; loss: 1.54; acc: 0.56
Batch: 280; loss: 1.58; acc: 0.48
Batch: 300; loss: 1.59; acc: 0.55
Batch: 320; loss: 1.48; acc: 0.59
Batch: 340; loss: 1.34; acc: 0.66
Batch: 360; loss: 1.56; acc: 0.55
Batch: 380; loss: 1.5; acc: 0.55
Batch: 400; loss: 1.5; acc: 0.58
Batch: 420; loss: 1.56; acc: 0.55
Batch: 440; loss: 1.56; acc: 0.61
Batch: 460; loss: 1.67; acc: 0.44
Batch: 480; loss: 1.6; acc: 0.58
Batch: 500; loss: 1.6; acc: 0.48
Batch: 520; loss: 1.6; acc: 0.47
Batch: 540; loss: 1.47; acc: 0.62
Batch: 560; loss: 1.47; acc: 0.58
Batch: 580; loss: 1.49; acc: 0.55
Batch: 600; loss: 1.44; acc: 0.61
Batch: 620; loss: 1.57; acc: 0.55
Batch: 640; loss: 1.55; acc: 0.56
Batch: 660; loss: 1.46; acc: 0.56
Batch: 680; loss: 1.59; acc: 0.55
Batch: 700; loss: 1.47; acc: 0.58
Batch: 720; loss: 1.49; acc: 0.47
Batch: 740; loss: 1.58; acc: 0.53
Batch: 760; loss: 1.57; acc: 0.5
Batch: 780; loss: 1.36; acc: 0.69
Train Epoch over. train_loss: 1.55; train_accuracy: 0.55 

4.341705425758846e-05
1.3122519703756552e-05
Batch: 0; loss: 1.58; acc: 0.45
Batch: 20; loss: 1.67; acc: 0.5
Batch: 40; loss: 1.33; acc: 0.7
Batch: 60; loss: 1.54; acc: 0.55
Batch: 80; loss: 1.44; acc: 0.59
Batch: 100; loss: 1.56; acc: 0.5
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.5; acc: 0.56
Val Epoch over. val_loss: 1.5366455043197438; val_accuracy: 0.5656847133757962 

The current subspace-distance is: 1.3122519703756552e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.44
Batch: 20; loss: 1.73; acc: 0.41
Batch: 40; loss: 1.64; acc: 0.5
Batch: 60; loss: 1.51; acc: 0.58
Batch: 80; loss: 1.48; acc: 0.62
Batch: 100; loss: 1.62; acc: 0.5
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.59; acc: 0.59
Batch: 160; loss: 1.56; acc: 0.52
Batch: 180; loss: 1.64; acc: 0.53
Batch: 200; loss: 1.66; acc: 0.45
Batch: 220; loss: 1.61; acc: 0.59
Batch: 240; loss: 1.48; acc: 0.58
Batch: 260; loss: 1.47; acc: 0.55
Batch: 280; loss: 1.58; acc: 0.58
Batch: 300; loss: 1.51; acc: 0.59
Batch: 320; loss: 1.55; acc: 0.53
Batch: 340; loss: 1.55; acc: 0.44
Batch: 360; loss: 1.53; acc: 0.56
Batch: 380; loss: 1.49; acc: 0.59
Batch: 400; loss: 1.48; acc: 0.62
Batch: 420; loss: 1.53; acc: 0.61
Batch: 440; loss: 1.57; acc: 0.5
Batch: 460; loss: 1.59; acc: 0.53
Batch: 480; loss: 1.58; acc: 0.56
Batch: 500; loss: 1.48; acc: 0.62
Batch: 520; loss: 1.56; acc: 0.56
Batch: 540; loss: 1.61; acc: 0.48
Batch: 560; loss: 1.6; acc: 0.58
Batch: 580; loss: 1.51; acc: 0.55
Batch: 600; loss: 1.68; acc: 0.52
Batch: 620; loss: 1.61; acc: 0.55
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.6; acc: 0.53
Batch: 680; loss: 1.48; acc: 0.5
Batch: 700; loss: 1.56; acc: 0.5
Batch: 720; loss: 1.47; acc: 0.56
Batch: 740; loss: 1.39; acc: 0.64
Batch: 760; loss: 1.58; acc: 0.44
Batch: 780; loss: 1.43; acc: 0.66
Train Epoch over. train_loss: 1.54; train_accuracy: 0.55 

4.3041480239480734e-05
1.115448321797885e-05
Batch: 0; loss: 1.57; acc: 0.45
Batch: 20; loss: 1.66; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.69
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.43; acc: 0.61
Batch: 100; loss: 1.54; acc: 0.5
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.47; acc: 0.59
Val Epoch over. val_loss: 1.5250920657139675; val_accuracy: 0.5719546178343949 

The current subspace-distance is: 1.115448321797885e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.45; acc: 0.59
Batch: 20; loss: 1.5; acc: 0.53
Batch: 40; loss: 1.58; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.54; acc: 0.56
Batch: 100; loss: 1.51; acc: 0.5
Batch: 120; loss: 1.55; acc: 0.58
Batch: 140; loss: 1.58; acc: 0.55
Batch: 160; loss: 1.61; acc: 0.44
Batch: 180; loss: 1.45; acc: 0.62
Batch: 200; loss: 1.46; acc: 0.61
Batch: 220; loss: 1.63; acc: 0.47
Batch: 240; loss: 1.64; acc: 0.5
Batch: 260; loss: 1.48; acc: 0.66
Batch: 280; loss: 1.49; acc: 0.58
Batch: 300; loss: 1.53; acc: 0.47
Batch: 320; loss: 1.53; acc: 0.62
Batch: 340; loss: 1.5; acc: 0.58
Batch: 360; loss: 1.53; acc: 0.58
Batch: 380; loss: 1.45; acc: 0.59
Batch: 400; loss: 1.54; acc: 0.56
Batch: 420; loss: 1.58; acc: 0.52
Batch: 440; loss: 1.48; acc: 0.55
Batch: 460; loss: 1.63; acc: 0.5
Batch: 480; loss: 1.35; acc: 0.69
Batch: 500; loss: 1.48; acc: 0.53
Batch: 520; loss: 1.49; acc: 0.58
Batch: 540; loss: 1.49; acc: 0.61
Batch: 560; loss: 1.54; acc: 0.55
Batch: 580; loss: 1.61; acc: 0.52
Batch: 600; loss: 1.57; acc: 0.55
Batch: 620; loss: 1.56; acc: 0.55
Batch: 640; loss: 1.47; acc: 0.61
Batch: 660; loss: 1.6; acc: 0.52
Batch: 680; loss: 1.49; acc: 0.59
Batch: 700; loss: 1.57; acc: 0.56
Batch: 720; loss: 1.54; acc: 0.52
Batch: 740; loss: 1.63; acc: 0.53
Batch: 760; loss: 1.81; acc: 0.36
Batch: 780; loss: 1.65; acc: 0.45
Train Epoch over. train_loss: 1.54; train_accuracy: 0.55 

4.363044354249723e-05
1.1084549441875424e-05
Batch: 0; loss: 1.58; acc: 0.44
Batch: 20; loss: 1.68; acc: 0.5
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.44; acc: 0.56
Batch: 100; loss: 1.56; acc: 0.48
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.49; acc: 0.56
Val Epoch over. val_loss: 1.53137769213148; val_accuracy: 0.5658837579617835 

The current subspace-distance is: 1.1084549441875424e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.42
Batch: 20; loss: 1.56; acc: 0.53
Batch: 40; loss: 1.36; acc: 0.67
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.54; acc: 0.58
Batch: 100; loss: 1.6; acc: 0.53
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.58; acc: 0.52
Batch: 160; loss: 1.56; acc: 0.52
Batch: 180; loss: 1.69; acc: 0.42
Batch: 200; loss: 1.51; acc: 0.58
Batch: 220; loss: 1.58; acc: 0.55
Batch: 240; loss: 1.71; acc: 0.42
Batch: 260; loss: 1.52; acc: 0.56
Batch: 280; loss: 1.44; acc: 0.62
Batch: 300; loss: 1.49; acc: 0.5
Batch: 320; loss: 1.47; acc: 0.59
Batch: 340; loss: 1.6; acc: 0.5
Batch: 360; loss: 1.73; acc: 0.47
Batch: 380; loss: 1.57; acc: 0.52
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.44; acc: 0.59
Batch: 440; loss: 1.64; acc: 0.52
Batch: 460; loss: 1.61; acc: 0.52
Batch: 480; loss: 1.69; acc: 0.53
Batch: 500; loss: 1.44; acc: 0.66
Batch: 520; loss: 1.51; acc: 0.58
Batch: 540; loss: 1.33; acc: 0.67
Batch: 560; loss: 1.54; acc: 0.5
Batch: 580; loss: 1.53; acc: 0.5
Batch: 600; loss: 1.51; acc: 0.56
Batch: 620; loss: 1.56; acc: 0.5
Batch: 640; loss: 1.47; acc: 0.62
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.73; acc: 0.42
Batch: 700; loss: 1.68; acc: 0.45
Batch: 720; loss: 1.59; acc: 0.53
Batch: 740; loss: 1.59; acc: 0.58
Batch: 760; loss: 1.49; acc: 0.58
Batch: 780; loss: 1.49; acc: 0.59
Train Epoch over. train_loss: 1.54; train_accuracy: 0.55 

4.503012678469531e-05
1.194915148516884e-05
Batch: 0; loss: 1.58; acc: 0.44
Batch: 20; loss: 1.66; acc: 0.55
Batch: 40; loss: 1.33; acc: 0.7
Batch: 60; loss: 1.53; acc: 0.58
Batch: 80; loss: 1.44; acc: 0.61
Batch: 100; loss: 1.55; acc: 0.5
Batch: 120; loss: 1.61; acc: 0.5
Batch: 140; loss: 1.49; acc: 0.56
Val Epoch over. val_loss: 1.5301607528309913; val_accuracy: 0.5709593949044586 

The current subspace-distance is: 1.194915148516884e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.61
Batch: 20; loss: 1.51; acc: 0.52
Batch: 40; loss: 1.61; acc: 0.55
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.63; acc: 0.55
Batch: 100; loss: 1.35; acc: 0.59
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 1.73; acc: 0.39
Batch: 160; loss: 1.58; acc: 0.55
Batch: 180; loss: 1.59; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.55
Batch: 220; loss: 1.52; acc: 0.53
Batch: 240; loss: 1.54; acc: 0.58
Batch: 260; loss: 1.63; acc: 0.55
Batch: 280; loss: 1.6; acc: 0.44
Batch: 300; loss: 1.67; acc: 0.42
Batch: 320; loss: 1.52; acc: 0.55
Batch: 340; loss: 1.49; acc: 0.61
Batch: 360; loss: 1.58; acc: 0.5
Batch: 380; loss: 1.54; acc: 0.52
Batch: 400; loss: 1.76; acc: 0.44
Batch: 420; loss: 1.56; acc: 0.5
Batch: 440; loss: 1.53; acc: 0.56
Batch: 460; loss: 1.64; acc: 0.48
Batch: 480; loss: 1.67; acc: 0.52
Batch: 500; loss: 1.44; acc: 0.59
Batch: 520; loss: 1.59; acc: 0.53
Batch: 540; loss: 1.59; acc: 0.52
Batch: 560; loss: 1.67; acc: 0.45
Batch: 580; loss: 1.47; acc: 0.55
Batch: 600; loss: 1.41; acc: 0.69
Batch: 620; loss: 1.44; acc: 0.59
Batch: 640; loss: 1.61; acc: 0.53
Batch: 660; loss: 1.52; acc: 0.58
Batch: 680; loss: 1.49; acc: 0.59
Batch: 700; loss: 1.77; acc: 0.44
Batch: 720; loss: 1.45; acc: 0.58
Batch: 740; loss: 1.44; acc: 0.61
Batch: 760; loss: 1.37; acc: 0.66
Batch: 780; loss: 1.62; acc: 0.55
Train Epoch over. train_loss: 1.54; train_accuracy: 0.55 

4.45099467469845e-05
1.4082766028877813e-05
Batch: 0; loss: 1.58; acc: 0.42
Batch: 20; loss: 1.66; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.72
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.61
Batch: 100; loss: 1.54; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.48; acc: 0.59
Val Epoch over. val_loss: 1.523794023853958; val_accuracy: 0.572452229299363 

The current subspace-distance is: 1.4082766028877813e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.55
Batch: 20; loss: 1.55; acc: 0.56
Batch: 40; loss: 1.5; acc: 0.48
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.47; acc: 0.61
Batch: 100; loss: 1.48; acc: 0.62
Batch: 120; loss: 1.51; acc: 0.53
Batch: 140; loss: 1.51; acc: 0.58
Batch: 160; loss: 1.46; acc: 0.56
Batch: 180; loss: 1.57; acc: 0.53
Batch: 200; loss: 1.61; acc: 0.55
Batch: 220; loss: 1.7; acc: 0.48
Batch: 240; loss: 1.48; acc: 0.61
Batch: 260; loss: 1.61; acc: 0.59
Batch: 280; loss: 1.49; acc: 0.59
Batch: 300; loss: 1.63; acc: 0.58
Batch: 320; loss: 1.54; acc: 0.55
Batch: 340; loss: 1.53; acc: 0.55
Batch: 360; loss: 1.82; acc: 0.41
Batch: 380; loss: 1.6; acc: 0.45
Batch: 400; loss: 1.42; acc: 0.64
Batch: 420; loss: 1.38; acc: 0.59
Batch: 440; loss: 1.6; acc: 0.45
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.54; acc: 0.53
Batch: 500; loss: 1.38; acc: 0.62
Batch: 520; loss: 1.45; acc: 0.62
Batch: 540; loss: 1.45; acc: 0.64
Batch: 560; loss: 1.57; acc: 0.53
Batch: 580; loss: 1.58; acc: 0.52
Batch: 600; loss: 1.58; acc: 0.56
Batch: 620; loss: 1.5; acc: 0.64
Batch: 640; loss: 1.45; acc: 0.55
Batch: 660; loss: 1.5; acc: 0.56
Batch: 680; loss: 1.51; acc: 0.53
Batch: 700; loss: 1.44; acc: 0.59
Batch: 720; loss: 1.42; acc: 0.61
Batch: 740; loss: 1.5; acc: 0.56
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.57; acc: 0.5
Train Epoch over. train_loss: 1.54; train_accuracy: 0.55 

4.688560147769749e-05
1.923274976434186e-05
Batch: 0; loss: 1.57; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.55
Batch: 40; loss: 1.31; acc: 0.72
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.43; acc: 0.62
Batch: 100; loss: 1.54; acc: 0.47
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.46; acc: 0.59
Val Epoch over. val_loss: 1.5151297134958255; val_accuracy: 0.5742436305732485 

The current subspace-distance is: 1.923274976434186e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.58
Batch: 40; loss: 1.44; acc: 0.62
Batch: 60; loss: 1.42; acc: 0.56
Batch: 80; loss: 1.67; acc: 0.42
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.47
Batch: 140; loss: 1.47; acc: 0.61
Batch: 160; loss: 1.47; acc: 0.59
Batch: 180; loss: 1.54; acc: 0.53
Batch: 200; loss: 1.43; acc: 0.55
Batch: 220; loss: 1.7; acc: 0.5
Batch: 240; loss: 1.6; acc: 0.53
Batch: 260; loss: 1.58; acc: 0.59
Batch: 280; loss: 1.59; acc: 0.52
Batch: 300; loss: 1.61; acc: 0.5
Batch: 320; loss: 1.38; acc: 0.64
Batch: 340; loss: 1.6; acc: 0.61
Batch: 360; loss: 1.67; acc: 0.48
Batch: 380; loss: 1.56; acc: 0.56
Batch: 400; loss: 1.51; acc: 0.58
Batch: 420; loss: 1.72; acc: 0.45
Batch: 440; loss: 1.42; acc: 0.64
Batch: 460; loss: 1.51; acc: 0.66
Batch: 480; loss: 1.7; acc: 0.47
Batch: 500; loss: 1.53; acc: 0.52
Batch: 520; loss: 1.63; acc: 0.5
Batch: 540; loss: 1.64; acc: 0.45
Batch: 560; loss: 1.62; acc: 0.53
Batch: 580; loss: 1.51; acc: 0.59
Batch: 600; loss: 1.56; acc: 0.55
Batch: 620; loss: 1.55; acc: 0.61
Batch: 640; loss: 1.51; acc: 0.56
Batch: 660; loss: 1.52; acc: 0.56
Batch: 680; loss: 1.66; acc: 0.53
Batch: 700; loss: 1.61; acc: 0.5
Batch: 720; loss: 1.47; acc: 0.56
Batch: 740; loss: 1.56; acc: 0.53
Batch: 760; loss: 1.44; acc: 0.62
Batch: 780; loss: 1.57; acc: 0.52
Train Epoch over. train_loss: 1.53; train_accuracy: 0.55 

4.469196210266091e-05
1.3184278941480443e-05
Batch: 0; loss: 1.58; acc: 0.44
Batch: 20; loss: 1.68; acc: 0.53
Batch: 40; loss: 1.32; acc: 0.7
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.59
Batch: 100; loss: 1.54; acc: 0.5
Batch: 120; loss: 1.6; acc: 0.47
Batch: 140; loss: 1.49; acc: 0.56
Val Epoch over. val_loss: 1.5264670203445823; val_accuracy: 0.5693670382165605 

The current subspace-distance is: 1.3184278941480443e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.65; acc: 0.44
Batch: 20; loss: 1.43; acc: 0.66
Batch: 40; loss: 1.42; acc: 0.61
Batch: 60; loss: 1.41; acc: 0.64
Batch: 80; loss: 1.51; acc: 0.45
Batch: 100; loss: 1.61; acc: 0.56
Batch: 120; loss: 1.52; acc: 0.59
Batch: 140; loss: 1.49; acc: 0.5
Batch: 160; loss: 1.56; acc: 0.5
Batch: 180; loss: 1.59; acc: 0.53
Batch: 200; loss: 1.48; acc: 0.55
Batch: 220; loss: 1.47; acc: 0.58
Batch: 240; loss: 1.56; acc: 0.56
Batch: 260; loss: 1.41; acc: 0.55
Batch: 280; loss: 1.58; acc: 0.53
Batch: 300; loss: 1.55; acc: 0.52
Batch: 320; loss: 1.57; acc: 0.5
Batch: 340; loss: 1.63; acc: 0.55
Batch: 360; loss: 1.57; acc: 0.53
Batch: 380; loss: 1.69; acc: 0.45
Batch: 400; loss: 1.53; acc: 0.61
Batch: 420; loss: 1.57; acc: 0.52
Batch: 440; loss: 1.56; acc: 0.47
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.37; acc: 0.69
Batch: 500; loss: 1.45; acc: 0.61
Batch: 520; loss: 1.47; acc: 0.56
Batch: 540; loss: 1.5; acc: 0.53
Batch: 560; loss: 1.48; acc: 0.64
Batch: 580; loss: 1.65; acc: 0.52
Batch: 600; loss: 1.64; acc: 0.44
Batch: 620; loss: 1.51; acc: 0.62
Batch: 640; loss: 1.56; acc: 0.55
Batch: 660; loss: 1.65; acc: 0.5
Batch: 680; loss: 1.64; acc: 0.5
Batch: 700; loss: 1.42; acc: 0.64
Batch: 720; loss: 1.48; acc: 0.66
Batch: 740; loss: 1.56; acc: 0.52
Batch: 760; loss: 1.55; acc: 0.56
Batch: 780; loss: 1.47; acc: 0.61
Train Epoch over. train_loss: 1.53; train_accuracy: 0.55 

4.588309093378484e-05
1.4160232240101323e-05
Batch: 0; loss: 1.58; acc: 0.47
Batch: 20; loss: 1.65; acc: 0.53
Batch: 40; loss: 1.33; acc: 0.7
Batch: 60; loss: 1.52; acc: 0.61
Batch: 80; loss: 1.44; acc: 0.64
Batch: 100; loss: 1.53; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.5
Batch: 140; loss: 1.48; acc: 0.59
Val Epoch over. val_loss: 1.5248479144588398; val_accuracy: 0.5725517515923567 

The current subspace-distance is: 1.4160232240101323e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.53
Batch: 20; loss: 1.55; acc: 0.53
Batch: 40; loss: 1.48; acc: 0.58
Batch: 60; loss: 1.55; acc: 0.56
Batch: 80; loss: 1.49; acc: 0.53
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.52; acc: 0.56
Batch: 140; loss: 1.35; acc: 0.69
Batch: 160; loss: 1.49; acc: 0.59
Batch: 180; loss: 1.56; acc: 0.5
Batch: 200; loss: 1.7; acc: 0.5
Batch: 220; loss: 1.45; acc: 0.67
Batch: 240; loss: 1.45; acc: 0.61
Batch: 260; loss: 1.46; acc: 0.58
Batch: 280; loss: 1.41; acc: 0.61
Batch: 300; loss: 1.55; acc: 0.47
Batch: 320; loss: 1.57; acc: 0.45
Batch: 340; loss: 1.52; acc: 0.58
Batch: 360; loss: 1.46; acc: 0.61
Batch: 380; loss: 1.5; acc: 0.58
Batch: 400; loss: 1.64; acc: 0.41
Batch: 420; loss: 1.34; acc: 0.64
Batch: 440; loss: 1.57; acc: 0.5
Batch: 460; loss: 1.55; acc: 0.58
Batch: 480; loss: 1.46; acc: 0.61
Batch: 500; loss: 1.46; acc: 0.61
Batch: 520; loss: 1.56; acc: 0.59
Batch: 540; loss: 1.49; acc: 0.55
Batch: 560; loss: 1.67; acc: 0.48
Batch: 580; loss: 1.64; acc: 0.55
Batch: 600; loss: 1.56; acc: 0.58
Batch: 620; loss: 1.51; acc: 0.58
Batch: 640; loss: 1.59; acc: 0.52
Batch: 660; loss: 1.55; acc: 0.52
Batch: 680; loss: 1.6; acc: 0.47
Batch: 700; loss: 1.47; acc: 0.56
Batch: 720; loss: 1.53; acc: 0.62
Batch: 740; loss: 1.75; acc: 0.44
Batch: 760; loss: 1.54; acc: 0.56
Batch: 780; loss: 1.45; acc: 0.56
Train Epoch over. train_loss: 1.53; train_accuracy: 0.55 

4.573415208142251e-05
1.2931992387166247e-05
Batch: 0; loss: 1.57; acc: 0.42
Batch: 20; loss: 1.66; acc: 0.53
Batch: 40; loss: 1.33; acc: 0.73
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.62
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.48; acc: 0.58
Val Epoch over. val_loss: 1.5199787495242563; val_accuracy: 0.574343152866242 

The current subspace-distance is: 1.2931992387166247e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.42; acc: 0.61
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.59; acc: 0.47
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.48; acc: 0.58
Batch: 100; loss: 1.56; acc: 0.58
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.64
Batch: 160; loss: 1.63; acc: 0.58
Batch: 180; loss: 1.81; acc: 0.41
Batch: 200; loss: 1.53; acc: 0.58
Batch: 220; loss: 1.52; acc: 0.58
Batch: 240; loss: 1.56; acc: 0.55
Batch: 260; loss: 1.7; acc: 0.47
Batch: 280; loss: 1.64; acc: 0.47
Batch: 300; loss: 1.61; acc: 0.5
Batch: 320; loss: 1.64; acc: 0.42
Batch: 340; loss: 1.55; acc: 0.48
Batch: 360; loss: 1.43; acc: 0.67
Batch: 380; loss: 1.49; acc: 0.56
Batch: 400; loss: 1.45; acc: 0.56
Batch: 420; loss: 1.58; acc: 0.47
Batch: 440; loss: 1.83; acc: 0.41
Batch: 460; loss: 1.58; acc: 0.53
Batch: 480; loss: 1.6; acc: 0.56
Batch: 500; loss: 1.56; acc: 0.52
Batch: 520; loss: 1.52; acc: 0.52
Batch: 540; loss: 1.53; acc: 0.55
Batch: 560; loss: 1.5; acc: 0.55
Batch: 580; loss: 1.72; acc: 0.47
Batch: 600; loss: 1.68; acc: 0.5
Batch: 620; loss: 1.64; acc: 0.52
Batch: 640; loss: 1.48; acc: 0.53
Batch: 660; loss: 1.55; acc: 0.5
Batch: 680; loss: 1.49; acc: 0.55
Batch: 700; loss: 1.54; acc: 0.58
Batch: 720; loss: 1.42; acc: 0.64
Batch: 740; loss: 1.46; acc: 0.56
Batch: 760; loss: 1.4; acc: 0.62
Batch: 780; loss: 1.53; acc: 0.52
Train Epoch over. train_loss: 1.53; train_accuracy: 0.55 

4.497282861848362e-05
1.188819260278251e-05
Batch: 0; loss: 1.57; acc: 0.44
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.31; acc: 0.72
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.53; acc: 0.52
Batch: 120; loss: 1.59; acc: 0.52
Batch: 140; loss: 1.45; acc: 0.58
Val Epoch over. val_loss: 1.5062933590761416; val_accuracy: 0.5795183121019108 

The current subspace-distance is: 1.188819260278251e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.56
Batch: 20; loss: 1.44; acc: 0.66
Batch: 40; loss: 1.51; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.52
Batch: 80; loss: 1.5; acc: 0.58
Batch: 100; loss: 1.53; acc: 0.53
Batch: 120; loss: 1.56; acc: 0.56
Batch: 140; loss: 1.62; acc: 0.52
Batch: 160; loss: 1.42; acc: 0.58
Batch: 180; loss: 1.5; acc: 0.52
Batch: 200; loss: 1.36; acc: 0.59
Batch: 220; loss: 1.64; acc: 0.44
Batch: 240; loss: 1.67; acc: 0.48
Batch: 260; loss: 1.46; acc: 0.62
Batch: 280; loss: 1.67; acc: 0.5
Batch: 300; loss: 1.56; acc: 0.47
Batch: 320; loss: 1.31; acc: 0.67
Batch: 340; loss: 1.6; acc: 0.55
Batch: 360; loss: 1.54; acc: 0.56
Batch: 380; loss: 1.62; acc: 0.44
Batch: 400; loss: 1.43; acc: 0.58
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.64; acc: 0.48
Batch: 460; loss: 1.68; acc: 0.48
Batch: 480; loss: 1.66; acc: 0.52
Batch: 500; loss: 1.55; acc: 0.61
Batch: 520; loss: 1.46; acc: 0.62
Batch: 540; loss: 1.54; acc: 0.58
Batch: 560; loss: 1.55; acc: 0.52
Batch: 580; loss: 1.42; acc: 0.62
Batch: 600; loss: 1.56; acc: 0.61
Batch: 620; loss: 1.56; acc: 0.52
Batch: 640; loss: 1.42; acc: 0.62
Batch: 660; loss: 1.48; acc: 0.59
Batch: 680; loss: 1.36; acc: 0.62
Batch: 700; loss: 1.62; acc: 0.52
Batch: 720; loss: 1.63; acc: 0.56
Batch: 740; loss: 1.54; acc: 0.59
Batch: 760; loss: 1.7; acc: 0.47
Batch: 780; loss: 1.51; acc: 0.59
Train Epoch over. train_loss: 1.53; train_accuracy: 0.55 

4.531621016212739e-05
1.225185133080231e-05
Batch: 0; loss: 1.58; acc: 0.45
Batch: 20; loss: 1.66; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.72
Batch: 60; loss: 1.53; acc: 0.56
Batch: 80; loss: 1.44; acc: 0.64
Batch: 100; loss: 1.54; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.53
Batch: 140; loss: 1.49; acc: 0.55
Val Epoch over. val_loss: 1.5249065988382715; val_accuracy: 0.572452229299363 

The current subspace-distance is: 1.225185133080231e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.62
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.57; acc: 0.53
Batch: 60; loss: 1.45; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.48
Batch: 100; loss: 1.54; acc: 0.56
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 1.6; acc: 0.56
Batch: 160; loss: 1.45; acc: 0.64
Batch: 180; loss: 1.51; acc: 0.59
Batch: 200; loss: 1.38; acc: 0.67
Batch: 220; loss: 1.59; acc: 0.56
Batch: 240; loss: 1.72; acc: 0.45
Batch: 260; loss: 1.61; acc: 0.47
Batch: 280; loss: 1.63; acc: 0.5
Batch: 300; loss: 1.62; acc: 0.5
Batch: 320; loss: 1.68; acc: 0.47
Batch: 340; loss: 1.71; acc: 0.48
Batch: 360; loss: 1.4; acc: 0.69
Batch: 380; loss: 1.45; acc: 0.64
Batch: 400; loss: 1.52; acc: 0.59
Batch: 420; loss: 1.46; acc: 0.5
Batch: 440; loss: 1.52; acc: 0.61
Batch: 460; loss: 1.5; acc: 0.56
Batch: 480; loss: 1.35; acc: 0.67
Batch: 500; loss: 1.64; acc: 0.47
Batch: 520; loss: 1.48; acc: 0.58
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.62; acc: 0.53
Batch: 580; loss: 1.58; acc: 0.53
Batch: 600; loss: 1.54; acc: 0.5
Batch: 620; loss: 1.57; acc: 0.56
Batch: 640; loss: 1.66; acc: 0.48
Batch: 660; loss: 1.65; acc: 0.48
Batch: 680; loss: 1.44; acc: 0.64
Batch: 700; loss: 1.43; acc: 0.62
Batch: 720; loss: 1.59; acc: 0.61
Batch: 740; loss: 1.58; acc: 0.55
Batch: 760; loss: 1.58; acc: 0.5
Batch: 780; loss: 1.48; acc: 0.55
Train Epoch over. train_loss: 1.53; train_accuracy: 0.55 

4.547103890217841e-05
1.2977275218872819e-05
Batch: 0; loss: 1.58; acc: 0.42
Batch: 20; loss: 1.66; acc: 0.52
Batch: 40; loss: 1.33; acc: 0.69
Batch: 60; loss: 1.53; acc: 0.58
Batch: 80; loss: 1.44; acc: 0.64
Batch: 100; loss: 1.55; acc: 0.5
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.49; acc: 0.59
Val Epoch over. val_loss: 1.526601599280242; val_accuracy: 0.5732484076433121 

The current subspace-distance is: 1.2977275218872819e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.53
Batch: 20; loss: 1.52; acc: 0.59
Batch: 40; loss: 1.43; acc: 0.58
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.56; acc: 0.48
Batch: 100; loss: 1.67; acc: 0.41
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.51; acc: 0.61
Batch: 160; loss: 1.47; acc: 0.61
Batch: 180; loss: 1.54; acc: 0.53
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.4; acc: 0.61
Batch: 240; loss: 1.67; acc: 0.42
Batch: 260; loss: 1.53; acc: 0.53
Batch: 280; loss: 1.43; acc: 0.61
Batch: 300; loss: 1.69; acc: 0.52
Batch: 320; loss: 1.57; acc: 0.53
Batch: 340; loss: 1.52; acc: 0.53
Batch: 360; loss: 1.44; acc: 0.59
Batch: 380; loss: 1.62; acc: 0.58
Batch: 400; loss: 1.59; acc: 0.5
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.52; acc: 0.58
Batch: 460; loss: 1.67; acc: 0.39
Batch: 480; loss: 1.6; acc: 0.55
Batch: 500; loss: 1.54; acc: 0.5
Batch: 520; loss: 1.48; acc: 0.58
Batch: 540; loss: 1.47; acc: 0.61
Batch: 560; loss: 1.42; acc: 0.64
Batch: 580; loss: 1.38; acc: 0.58
Batch: 600; loss: 1.75; acc: 0.45
Batch: 620; loss: 1.63; acc: 0.45
Batch: 640; loss: 1.39; acc: 0.59
Batch: 660; loss: 1.64; acc: 0.55
Batch: 680; loss: 1.58; acc: 0.56
Batch: 700; loss: 1.69; acc: 0.45
Batch: 720; loss: 1.46; acc: 0.59
Batch: 740; loss: 1.51; acc: 0.56
Batch: 760; loss: 1.63; acc: 0.5
Batch: 780; loss: 1.47; acc: 0.55
Train Epoch over. train_loss: 1.53; train_accuracy: 0.55 

4.6104032662697136e-05
1.4758688848814927e-05
Batch: 0; loss: 1.58; acc: 0.42
Batch: 20; loss: 1.66; acc: 0.55
Batch: 40; loss: 1.33; acc: 0.69
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.43; acc: 0.62
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.5; acc: 0.58
Val Epoch over. val_loss: 1.5228796559534254; val_accuracy: 0.5693670382165605 

The current subspace-distance is: 1.4758688848814927e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.48
Batch: 20; loss: 1.55; acc: 0.56
Batch: 40; loss: 1.62; acc: 0.5
Batch: 60; loss: 1.45; acc: 0.61
Batch: 80; loss: 1.56; acc: 0.5
Batch: 100; loss: 1.54; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.55
Batch: 140; loss: 1.44; acc: 0.59
Batch: 160; loss: 1.44; acc: 0.58
Batch: 180; loss: 1.66; acc: 0.48
Batch: 200; loss: 1.59; acc: 0.56
Batch: 220; loss: 1.55; acc: 0.53
Batch: 240; loss: 1.54; acc: 0.59
Batch: 260; loss: 1.45; acc: 0.61
Batch: 280; loss: 1.47; acc: 0.53
Batch: 300; loss: 1.58; acc: 0.59
Batch: 320; loss: 1.58; acc: 0.44
Batch: 340; loss: 1.51; acc: 0.55
Batch: 360; loss: 1.47; acc: 0.53
Batch: 380; loss: 1.54; acc: 0.55
Batch: 400; loss: 1.59; acc: 0.53
Batch: 420; loss: 1.53; acc: 0.5
Batch: 440; loss: 1.57; acc: 0.53
Batch: 460; loss: 1.45; acc: 0.64
Batch: 480; loss: 1.59; acc: 0.52
Batch: 500; loss: 1.62; acc: 0.52
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.62; acc: 0.45
Batch: 580; loss: 1.48; acc: 0.61
Batch: 600; loss: 1.52; acc: 0.47
Batch: 620; loss: 1.42; acc: 0.62
Batch: 640; loss: 1.53; acc: 0.59
Batch: 660; loss: 1.52; acc: 0.52
Batch: 680; loss: 1.61; acc: 0.48
Batch: 700; loss: 1.44; acc: 0.59
Batch: 720; loss: 1.51; acc: 0.66
Batch: 740; loss: 1.36; acc: 0.56
Batch: 760; loss: 1.54; acc: 0.52
Batch: 780; loss: 1.64; acc: 0.44
Train Epoch over. train_loss: 1.53; train_accuracy: 0.56 

4.587523653754033e-05
1.2862718904216308e-05
Batch: 0; loss: 1.58; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.55
Batch: 40; loss: 1.32; acc: 0.69
Batch: 60; loss: 1.51; acc: 0.58
Batch: 80; loss: 1.43; acc: 0.62
Batch: 100; loss: 1.54; acc: 0.47
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.48; acc: 0.59
Val Epoch over. val_loss: 1.5188075934246088; val_accuracy: 0.5759355095541401 

The current subspace-distance is: 1.2862718904216308e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_4_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.170581494228353

The number of parameters is: 276580

The number of individual parameters is:

34
544
34
34
51
48552
51
51
101
144228
101
101
64
77568
64
64
4096
64
640
10
64
64

nonzero elements in E: 27657997
elements in E: 27658000
fraction nonzero: 0.9999998915322872
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.36; acc: 0.14
Batch: 40; loss: 2.14; acc: 0.28
Batch: 60; loss: 2.17; acc: 0.19
Batch: 80; loss: 2.11; acc: 0.23
Batch: 100; loss: 2.11; acc: 0.28
Batch: 120; loss: 1.99; acc: 0.3
Batch: 140; loss: 2.07; acc: 0.31
Batch: 160; loss: 1.96; acc: 0.34
Batch: 180; loss: 1.98; acc: 0.38
Batch: 200; loss: 1.87; acc: 0.47
Batch: 220; loss: 1.99; acc: 0.38
Batch: 240; loss: 1.97; acc: 0.3
Batch: 260; loss: 1.88; acc: 0.45
Batch: 280; loss: 1.85; acc: 0.44
Batch: 300; loss: 1.86; acc: 0.42
Batch: 320; loss: 1.86; acc: 0.45
Batch: 340; loss: 1.87; acc: 0.48
Batch: 360; loss: 1.83; acc: 0.59
Batch: 380; loss: 1.81; acc: 0.52
Batch: 400; loss: 1.81; acc: 0.47
Batch: 420; loss: 1.82; acc: 0.47
Batch: 440; loss: 1.87; acc: 0.45
Batch: 460; loss: 1.75; acc: 0.56
Batch: 480; loss: 1.77; acc: 0.5
Batch: 500; loss: 1.79; acc: 0.47
Batch: 520; loss: 1.88; acc: 0.41
Batch: 540; loss: 1.72; acc: 0.56
Batch: 560; loss: 1.81; acc: 0.45
Batch: 580; loss: 1.68; acc: 0.58
Batch: 600; loss: 1.69; acc: 0.55
Batch: 620; loss: 1.64; acc: 0.67
Batch: 640; loss: 1.8; acc: 0.44
Batch: 660; loss: 1.67; acc: 0.53
Batch: 680; loss: 1.67; acc: 0.53
Batch: 700; loss: 1.66; acc: 0.56
Batch: 720; loss: 1.68; acc: 0.62
Batch: 740; loss: 1.69; acc: 0.58
Batch: 760; loss: 1.61; acc: 0.66
Batch: 780; loss: 1.61; acc: 0.62
Train Epoch over. train_loss: 1.87; train_accuracy: 0.44 

5.795644028694369e-05
5.283487553242594e-05
Batch: 0; loss: 1.65; acc: 0.61
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.53; acc: 0.59
Batch: 60; loss: 1.63; acc: 0.62
Batch: 80; loss: 1.68; acc: 0.55
Batch: 100; loss: 1.7; acc: 0.53
Batch: 120; loss: 1.7; acc: 0.53
Batch: 140; loss: 1.61; acc: 0.55
Val Epoch over. val_loss: 1.6461672661410776; val_accuracy: 0.5756369426751592 

The current subspace-distance is: 5.283487553242594e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.53
Batch: 20; loss: 1.65; acc: 0.61
Batch: 40; loss: 1.64; acc: 0.53
Batch: 60; loss: 1.69; acc: 0.56
Batch: 80; loss: 1.65; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.62
Batch: 120; loss: 1.67; acc: 0.55
Batch: 140; loss: 1.56; acc: 0.67
Batch: 160; loss: 1.71; acc: 0.56
Batch: 180; loss: 1.53; acc: 0.58
Batch: 200; loss: 1.59; acc: 0.66
Batch: 220; loss: 1.54; acc: 0.69
Batch: 240; loss: 1.67; acc: 0.58
Batch: 260; loss: 1.6; acc: 0.64
Batch: 280; loss: 1.64; acc: 0.56
Batch: 300; loss: 1.48; acc: 0.7
Batch: 320; loss: 1.55; acc: 0.59
Batch: 340; loss: 1.59; acc: 0.59
Batch: 360; loss: 1.6; acc: 0.58
Batch: 380; loss: 1.53; acc: 0.62
Batch: 400; loss: 1.55; acc: 0.58
Batch: 420; loss: 1.52; acc: 0.66
Batch: 440; loss: 1.57; acc: 0.62
Batch: 460; loss: 1.37; acc: 0.75
Batch: 480; loss: 1.6; acc: 0.59
Batch: 500; loss: 1.54; acc: 0.62
Batch: 520; loss: 1.56; acc: 0.55
Batch: 540; loss: 1.53; acc: 0.67
Batch: 560; loss: 1.42; acc: 0.8
Batch: 580; loss: 1.53; acc: 0.69
Batch: 600; loss: 1.57; acc: 0.55
Batch: 620; loss: 1.55; acc: 0.64
Batch: 640; loss: 1.47; acc: 0.64
Batch: 660; loss: 1.48; acc: 0.69
Batch: 680; loss: 1.58; acc: 0.66
Batch: 700; loss: 1.5; acc: 0.61
Batch: 720; loss: 1.54; acc: 0.62
Batch: 740; loss: 1.58; acc: 0.64
Batch: 760; loss: 1.54; acc: 0.53
Batch: 780; loss: 1.59; acc: 0.56
Train Epoch over. train_loss: 1.59; train_accuracy: 0.59 

7.671043567825109e-05
7.199070387287065e-05
Batch: 0; loss: 1.51; acc: 0.8
Batch: 20; loss: 1.59; acc: 0.58
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 1.5; acc: 0.62
Batch: 80; loss: 1.5; acc: 0.55
Batch: 100; loss: 1.53; acc: 0.67
Batch: 120; loss: 1.51; acc: 0.67
Batch: 140; loss: 1.39; acc: 0.62
Val Epoch over. val_loss: 1.5014797517448475; val_accuracy: 0.6303742038216561 

The current subspace-distance is: 7.199070387287065e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.47; acc: 0.62
Batch: 40; loss: 1.44; acc: 0.67
Batch: 60; loss: 1.55; acc: 0.62
Batch: 80; loss: 1.47; acc: 0.67
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.54; acc: 0.66
Batch: 140; loss: 1.4; acc: 0.69
Batch: 160; loss: 1.65; acc: 0.53
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.48; acc: 0.64
Batch: 220; loss: 1.52; acc: 0.64
Batch: 240; loss: 1.6; acc: 0.5
Batch: 260; loss: 1.51; acc: 0.61
Batch: 280; loss: 1.57; acc: 0.58
Batch: 300; loss: 1.61; acc: 0.64
Batch: 320; loss: 1.38; acc: 0.81
Batch: 340; loss: 1.59; acc: 0.58
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.52; acc: 0.61
Batch: 400; loss: 1.57; acc: 0.58
Batch: 420; loss: 1.51; acc: 0.67
Batch: 440; loss: 1.52; acc: 0.59
Batch: 460; loss: 1.46; acc: 0.61
Batch: 480; loss: 1.4; acc: 0.73
Batch: 500; loss: 1.65; acc: 0.48
Batch: 520; loss: 1.45; acc: 0.56
Batch: 540; loss: 1.44; acc: 0.72
Batch: 560; loss: 1.63; acc: 0.45
Batch: 580; loss: 1.49; acc: 0.66
Batch: 600; loss: 1.48; acc: 0.67
Batch: 620; loss: 1.47; acc: 0.64
Batch: 640; loss: 1.36; acc: 0.66
Batch: 660; loss: 1.6; acc: 0.58
Batch: 680; loss: 1.45; acc: 0.61
Batch: 700; loss: 1.6; acc: 0.5
Batch: 720; loss: 1.39; acc: 0.72
Batch: 740; loss: 1.4; acc: 0.67
Batch: 760; loss: 1.53; acc: 0.64
Batch: 780; loss: 1.55; acc: 0.66
Train Epoch over. train_loss: 1.5; train_accuracy: 0.62 

9.012017108034343e-05
8.625895134173334e-05
Batch: 0; loss: 1.5; acc: 0.66
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.23; acc: 0.77
Batch: 60; loss: 1.41; acc: 0.64
Batch: 80; loss: 1.41; acc: 0.66
Batch: 100; loss: 1.44; acc: 0.73
Batch: 120; loss: 1.44; acc: 0.67
Batch: 140; loss: 1.24; acc: 0.75
Val Epoch over. val_loss: 1.4352686207765226; val_accuracy: 0.6616242038216561 

The current subspace-distance is: 8.625895134173334e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.69
Batch: 20; loss: 1.69; acc: 0.47
Batch: 40; loss: 1.51; acc: 0.64
Batch: 60; loss: 1.55; acc: 0.59
Batch: 80; loss: 1.45; acc: 0.62
Batch: 100; loss: 1.59; acc: 0.55
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.34; acc: 0.66
Batch: 160; loss: 1.52; acc: 0.59
Batch: 180; loss: 1.44; acc: 0.61
Batch: 200; loss: 1.5; acc: 0.62
Batch: 220; loss: 1.41; acc: 0.61
Batch: 240; loss: 1.41; acc: 0.67
Batch: 260; loss: 1.41; acc: 0.72
Batch: 280; loss: 1.54; acc: 0.58
Batch: 300; loss: 1.48; acc: 0.58
Batch: 320; loss: 1.45; acc: 0.73
Batch: 340; loss: 1.45; acc: 0.62
Batch: 360; loss: 1.38; acc: 0.72
Batch: 380; loss: 1.35; acc: 0.66
Batch: 400; loss: 1.5; acc: 0.58
Batch: 420; loss: 1.38; acc: 0.66
Batch: 440; loss: 1.46; acc: 0.58
Batch: 460; loss: 1.39; acc: 0.73
Batch: 480; loss: 1.54; acc: 0.58
Batch: 500; loss: 1.34; acc: 0.7
Batch: 520; loss: 1.4; acc: 0.67
Batch: 540; loss: 1.34; acc: 0.7
Batch: 560; loss: 1.52; acc: 0.58
Batch: 580; loss: 1.39; acc: 0.67
Batch: 600; loss: 1.43; acc: 0.69
Batch: 620; loss: 1.32; acc: 0.7
Batch: 640; loss: 1.33; acc: 0.69
Batch: 660; loss: 1.44; acc: 0.67
Batch: 680; loss: 1.35; acc: 0.69
Batch: 700; loss: 1.37; acc: 0.75
Batch: 720; loss: 1.52; acc: 0.61
Batch: 740; loss: 1.39; acc: 0.7
Batch: 760; loss: 1.34; acc: 0.73
Batch: 780; loss: 1.47; acc: 0.62
Train Epoch over. train_loss: 1.45; train_accuracy: 0.64 

0.00010298054257873446
9.747267904458568e-05
Batch: 0; loss: 1.51; acc: 0.64
Batch: 20; loss: 1.54; acc: 0.53
Batch: 40; loss: 1.19; acc: 0.8
Batch: 60; loss: 1.33; acc: 0.66
Batch: 80; loss: 1.35; acc: 0.72
Batch: 100; loss: 1.38; acc: 0.77
Batch: 120; loss: 1.42; acc: 0.64
Batch: 140; loss: 1.14; acc: 0.88
Val Epoch over. val_loss: 1.3897638677791426; val_accuracy: 0.6753582802547771 

The current subspace-distance is: 9.747267904458568e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.43; acc: 0.61
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.37; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.69
Batch: 80; loss: 1.42; acc: 0.7
Batch: 100; loss: 1.32; acc: 0.72
Batch: 120; loss: 1.46; acc: 0.59
Batch: 140; loss: 1.47; acc: 0.66
Batch: 160; loss: 1.31; acc: 0.7
Batch: 180; loss: 1.59; acc: 0.58
Batch: 200; loss: 1.32; acc: 0.7
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.4; acc: 0.66
Batch: 260; loss: 1.48; acc: 0.66
Batch: 280; loss: 1.61; acc: 0.45
Batch: 300; loss: 1.4; acc: 0.66
Batch: 320; loss: 1.49; acc: 0.64
Batch: 340; loss: 1.48; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.27; acc: 0.77
Batch: 400; loss: 1.4; acc: 0.58
Batch: 420; loss: 1.41; acc: 0.7
Batch: 440; loss: 1.32; acc: 0.73
Batch: 460; loss: 1.46; acc: 0.67
Batch: 480; loss: 1.34; acc: 0.69
Batch: 500; loss: 1.35; acc: 0.73
Batch: 520; loss: 1.38; acc: 0.7
Batch: 540; loss: 1.33; acc: 0.69
Batch: 560; loss: 1.41; acc: 0.64
Batch: 580; loss: 1.42; acc: 0.66
Batch: 600; loss: 1.33; acc: 0.72
Batch: 620; loss: 1.44; acc: 0.59
Batch: 640; loss: 1.45; acc: 0.64
Batch: 660; loss: 1.42; acc: 0.64
Batch: 680; loss: 1.38; acc: 0.7
Batch: 700; loss: 1.37; acc: 0.72
Batch: 720; loss: 1.48; acc: 0.61
Batch: 740; loss: 1.43; acc: 0.67
Batch: 760; loss: 1.35; acc: 0.62
Batch: 780; loss: 1.39; acc: 0.7
Train Epoch over. train_loss: 1.41; train_accuracy: 0.65 

0.00011317550524836406
0.0001083973838831298
Batch: 0; loss: 1.48; acc: 0.66
Batch: 20; loss: 1.52; acc: 0.55
Batch: 40; loss: 1.14; acc: 0.81
Batch: 60; loss: 1.28; acc: 0.73
Batch: 80; loss: 1.28; acc: 0.72
Batch: 100; loss: 1.35; acc: 0.75
Batch: 120; loss: 1.42; acc: 0.64
Batch: 140; loss: 1.06; acc: 0.86
Val Epoch over. val_loss: 1.3432323218910558; val_accuracy: 0.681031050955414 

The current subspace-distance is: 0.0001083973838831298 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.64
Batch: 20; loss: 1.35; acc: 0.62
Batch: 40; loss: 1.41; acc: 0.61
Batch: 60; loss: 1.41; acc: 0.67
Batch: 80; loss: 1.26; acc: 0.73
Batch: 100; loss: 1.39; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.69
Batch: 140; loss: 1.42; acc: 0.62
Batch: 160; loss: 1.33; acc: 0.69
Batch: 180; loss: 1.48; acc: 0.55
Batch: 200; loss: 1.4; acc: 0.59
Batch: 220; loss: 1.46; acc: 0.59
Batch: 240; loss: 1.34; acc: 0.75
Batch: 260; loss: 1.39; acc: 0.61
Batch: 280; loss: 1.48; acc: 0.58
Batch: 300; loss: 1.34; acc: 0.7
Batch: 320; loss: 1.4; acc: 0.64
Batch: 340; loss: 1.38; acc: 0.56
Batch: 360; loss: 1.32; acc: 0.72
Batch: 380; loss: 1.46; acc: 0.59
Batch: 400; loss: 1.33; acc: 0.67
Batch: 420; loss: 1.43; acc: 0.66
Batch: 440; loss: 1.45; acc: 0.58
Batch: 460; loss: 1.3; acc: 0.7
Batch: 480; loss: 1.35; acc: 0.7
Batch: 500; loss: 1.24; acc: 0.69
Batch: 520; loss: 1.22; acc: 0.78
Batch: 540; loss: 1.24; acc: 0.62
Batch: 560; loss: 1.29; acc: 0.66
Batch: 580; loss: 1.43; acc: 0.62
Batch: 600; loss: 1.22; acc: 0.78
Batch: 620; loss: 1.33; acc: 0.67
Batch: 640; loss: 1.26; acc: 0.72
Batch: 660; loss: 1.39; acc: 0.64
Batch: 680; loss: 1.31; acc: 0.69
Batch: 700; loss: 1.35; acc: 0.62
Batch: 720; loss: 1.37; acc: 0.56
Batch: 740; loss: 1.3; acc: 0.7
Batch: 760; loss: 1.37; acc: 0.62
Batch: 780; loss: 1.45; acc: 0.64
Train Epoch over. train_loss: 1.36; train_accuracy: 0.66 

0.00012060425069648772
0.00011681584146572277
Batch: 0; loss: 1.42; acc: 0.72
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.07; acc: 0.81
Batch: 60; loss: 1.2; acc: 0.77
Batch: 80; loss: 1.19; acc: 0.77
Batch: 100; loss: 1.32; acc: 0.73
Batch: 120; loss: 1.37; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.89
Val Epoch over. val_loss: 1.2839481720499173; val_accuracy: 0.7043192675159236 

The current subspace-distance is: 0.00011681584146572277 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.45
Batch: 20; loss: 1.17; acc: 0.8
Batch: 40; loss: 1.36; acc: 0.67
Batch: 60; loss: 1.3; acc: 0.66
Batch: 80; loss: 1.35; acc: 0.64
Batch: 100; loss: 1.42; acc: 0.66
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.72
Batch: 160; loss: 1.35; acc: 0.67
Batch: 180; loss: 1.3; acc: 0.62
Batch: 200; loss: 1.56; acc: 0.53
Batch: 220; loss: 1.26; acc: 0.66
Batch: 240; loss: 1.41; acc: 0.61
Batch: 260; loss: 1.38; acc: 0.67
Batch: 280; loss: 1.31; acc: 0.72
Batch: 300; loss: 1.43; acc: 0.58
Batch: 320; loss: 1.28; acc: 0.64
Batch: 340; loss: 1.23; acc: 0.77
Batch: 360; loss: 1.33; acc: 0.67
Batch: 380; loss: 1.22; acc: 0.7
Batch: 400; loss: 1.32; acc: 0.69
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 1.35; acc: 0.62
Batch: 460; loss: 1.27; acc: 0.7
Batch: 480; loss: 1.24; acc: 0.75
Batch: 500; loss: 1.28; acc: 0.69
Batch: 520; loss: 1.38; acc: 0.66
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.27; acc: 0.67
Batch: 580; loss: 1.34; acc: 0.64
Batch: 600; loss: 1.4; acc: 0.64
Batch: 620; loss: 1.32; acc: 0.64
Batch: 640; loss: 1.14; acc: 0.73
Batch: 660; loss: 1.21; acc: 0.67
Batch: 680; loss: 1.19; acc: 0.73
Batch: 700; loss: 1.31; acc: 0.61
Batch: 720; loss: 1.3; acc: 0.67
Batch: 740; loss: 1.36; acc: 0.66
Batch: 760; loss: 1.29; acc: 0.66
Batch: 780; loss: 1.3; acc: 0.69
Train Epoch over. train_loss: 1.3; train_accuracy: 0.67 

0.00013548439892474562
0.00012884823081549257
Batch: 0; loss: 1.34; acc: 0.67
Batch: 20; loss: 1.41; acc: 0.62
Batch: 40; loss: 0.96; acc: 0.88
Batch: 60; loss: 1.1; acc: 0.8
Batch: 80; loss: 1.09; acc: 0.8
Batch: 100; loss: 1.27; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.7
Batch: 140; loss: 0.94; acc: 0.89
Val Epoch over. val_loss: 1.218370553414533; val_accuracy: 0.7102906050955414 

The current subspace-distance is: 0.00012884823081549257 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.33; acc: 0.66
Batch: 40; loss: 1.34; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.75
Batch: 80; loss: 1.19; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.66
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 1.23; acc: 0.77
Batch: 160; loss: 1.29; acc: 0.62
Batch: 180; loss: 1.32; acc: 0.62
Batch: 200; loss: 1.36; acc: 0.58
Batch: 220; loss: 1.11; acc: 0.75
Batch: 240; loss: 1.22; acc: 0.7
Batch: 260; loss: 1.27; acc: 0.67
Batch: 280; loss: 1.22; acc: 0.69
Batch: 300; loss: 1.26; acc: 0.67
Batch: 320; loss: 1.22; acc: 0.7
Batch: 340; loss: 1.29; acc: 0.66
Batch: 360; loss: 1.3; acc: 0.73
Batch: 380; loss: 1.34; acc: 0.58
Batch: 400; loss: 1.35; acc: 0.66
Batch: 420; loss: 1.11; acc: 0.81
Batch: 440; loss: 1.25; acc: 0.66
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.23; acc: 0.66
Batch: 500; loss: 1.29; acc: 0.59
Batch: 520; loss: 1.27; acc: 0.7
Batch: 540; loss: 1.17; acc: 0.78
Batch: 560; loss: 1.13; acc: 0.7
Batch: 580; loss: 1.14; acc: 0.72
Batch: 600; loss: 1.28; acc: 0.66
Batch: 620; loss: 1.32; acc: 0.64
Batch: 640; loss: 1.37; acc: 0.59
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.34; acc: 0.64
Batch: 700; loss: 1.35; acc: 0.56
Batch: 720; loss: 1.35; acc: 0.62
Batch: 740; loss: 1.26; acc: 0.67
Batch: 760; loss: 1.3; acc: 0.59
Batch: 780; loss: 1.15; acc: 0.69
Train Epoch over. train_loss: 1.25; train_accuracy: 0.68 

0.00014636518608313054
0.00013940977805759758
Batch: 0; loss: 1.27; acc: 0.67
Batch: 20; loss: 1.35; acc: 0.61
Batch: 40; loss: 0.89; acc: 0.89
Batch: 60; loss: 1.05; acc: 0.77
Batch: 80; loss: 1.01; acc: 0.81
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.24; acc: 0.72
Batch: 140; loss: 0.87; acc: 0.92
Val Epoch over. val_loss: 1.1648756914837346; val_accuracy: 0.7060111464968153 

The current subspace-distance is: 0.00013940977805759758 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.69
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 1.1; acc: 0.72
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 1.32; acc: 0.58
Batch: 100; loss: 1.31; acc: 0.67
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 1.33; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.64
Batch: 180; loss: 1.37; acc: 0.52
Batch: 200; loss: 1.11; acc: 0.72
Batch: 220; loss: 1.18; acc: 0.72
Batch: 240; loss: 1.33; acc: 0.61
Batch: 260; loss: 1.29; acc: 0.67
Batch: 280; loss: 1.16; acc: 0.69
Batch: 300; loss: 1.2; acc: 0.67
Batch: 320; loss: 1.1; acc: 0.69
Batch: 340; loss: 1.15; acc: 0.69
Batch: 360; loss: 1.28; acc: 0.69
Batch: 380; loss: 1.23; acc: 0.64
Batch: 400; loss: 0.97; acc: 0.8
Batch: 420; loss: 1.22; acc: 0.59
Batch: 440; loss: 1.2; acc: 0.66
Batch: 460; loss: 1.22; acc: 0.64
Batch: 480; loss: 1.09; acc: 0.73
Batch: 500; loss: 1.23; acc: 0.67
Batch: 520; loss: 1.19; acc: 0.66
Batch: 540; loss: 1.33; acc: 0.61
Batch: 560; loss: 1.08; acc: 0.72
Batch: 580; loss: 1.2; acc: 0.69
Batch: 600; loss: 1.28; acc: 0.56
Batch: 620; loss: 0.96; acc: 0.8
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.62
Batch: 680; loss: 1.28; acc: 0.61
Batch: 700; loss: 1.11; acc: 0.72
Batch: 720; loss: 1.26; acc: 0.66
Batch: 740; loss: 1.2; acc: 0.64
Batch: 760; loss: 1.19; acc: 0.72
Batch: 780; loss: 1.49; acc: 0.58
Train Epoch over. train_loss: 1.21; train_accuracy: 0.68 

0.0001538204523967579
0.00014945831208024174
Batch: 0; loss: 1.23; acc: 0.67
Batch: 20; loss: 1.31; acc: 0.61
Batch: 40; loss: 0.84; acc: 0.88
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 0.96; acc: 0.83
Batch: 100; loss: 1.23; acc: 0.72
Batch: 120; loss: 1.21; acc: 0.73
Batch: 140; loss: 0.82; acc: 0.88
Val Epoch over. val_loss: 1.1264705931305126; val_accuracy: 0.7153662420382165 

The current subspace-distance is: 0.00014945831208024174 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 1.3; acc: 0.62
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.14; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.11; acc: 0.69
Batch: 160; loss: 1.24; acc: 0.69
Batch: 180; loss: 1.19; acc: 0.72
Batch: 200; loss: 1.02; acc: 0.73
Batch: 220; loss: 1.19; acc: 0.78
Batch: 240; loss: 1.3; acc: 0.61
Batch: 260; loss: 1.39; acc: 0.55
Batch: 280; loss: 1.28; acc: 0.56
Batch: 300; loss: 1.12; acc: 0.77
Batch: 320; loss: 1.21; acc: 0.67
Batch: 340; loss: 0.99; acc: 0.77
Batch: 360; loss: 1.13; acc: 0.73
Batch: 380; loss: 1.14; acc: 0.72
Batch: 400; loss: 1.3; acc: 0.7
Batch: 420; loss: 1.24; acc: 0.66
Batch: 440; loss: 1.26; acc: 0.66
Batch: 460; loss: 1.04; acc: 0.78
Batch: 480; loss: 1.16; acc: 0.59
Batch: 500; loss: 1.32; acc: 0.61
Batch: 520; loss: 1.23; acc: 0.58
Batch: 540; loss: 1.08; acc: 0.72
Batch: 560; loss: 1.24; acc: 0.62
Batch: 580; loss: 1.26; acc: 0.72
Batch: 600; loss: 1.32; acc: 0.59
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 1.25; acc: 0.69
Batch: 660; loss: 1.26; acc: 0.61
Batch: 680; loss: 1.2; acc: 0.61
Batch: 700; loss: 1.32; acc: 0.62
Batch: 720; loss: 1.14; acc: 0.69
Batch: 740; loss: 1.17; acc: 0.64
Batch: 760; loss: 1.17; acc: 0.7
Batch: 780; loss: 1.14; acc: 0.66
Train Epoch over. train_loss: 1.18; train_accuracy: 0.68 

0.00016594649059697986
0.00015777528460603207
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.82; acc: 0.88
Batch: 60; loss: 1.03; acc: 0.77
Batch: 80; loss: 0.95; acc: 0.8
Batch: 100; loss: 1.21; acc: 0.75
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.78; acc: 0.91
Val Epoch over. val_loss: 1.1086355246556032; val_accuracy: 0.7079020700636943 

The current subspace-distance is: 0.00015777528460603207 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.72
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 1.1; acc: 0.75
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.11; acc: 0.69
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.61
Batch: 160; loss: 1.24; acc: 0.64
Batch: 180; loss: 1.26; acc: 0.59
Batch: 200; loss: 1.21; acc: 0.7
Batch: 220; loss: 1.23; acc: 0.67
Batch: 240; loss: 1.09; acc: 0.69
Batch: 260; loss: 1.01; acc: 0.77
Batch: 280; loss: 1.28; acc: 0.55
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.11; acc: 0.69
Batch: 340; loss: 1.14; acc: 0.66
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.11; acc: 0.64
Batch: 400; loss: 1.23; acc: 0.64
Batch: 420; loss: 1.15; acc: 0.64
Batch: 440; loss: 1.02; acc: 0.83
Batch: 460; loss: 1.23; acc: 0.62
Batch: 480; loss: 1.12; acc: 0.73
Batch: 500; loss: 1.4; acc: 0.56
Batch: 520; loss: 1.2; acc: 0.67
Batch: 540; loss: 1.11; acc: 0.67
Batch: 560; loss: 1.13; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.72
Batch: 600; loss: 1.26; acc: 0.58
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.17; acc: 0.69
Batch: 660; loss: 1.23; acc: 0.66
Batch: 680; loss: 1.25; acc: 0.64
Batch: 700; loss: 1.06; acc: 0.78
Batch: 720; loss: 1.17; acc: 0.66
Batch: 740; loss: 0.95; acc: 0.75
Batch: 760; loss: 1.19; acc: 0.64
Batch: 780; loss: 1.21; acc: 0.64
Train Epoch over. train_loss: 1.16; train_accuracy: 0.68 

0.00016893188876565546
0.00016454253636766225
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.88
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 1.22; acc: 0.72
Batch: 120; loss: 1.21; acc: 0.7
Batch: 140; loss: 0.76; acc: 0.92
Val Epoch over. val_loss: 1.1013048974571713; val_accuracy: 0.7072054140127388 

The current subspace-distance is: 0.00016454253636766225 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.08; acc: 0.69
Batch: 40; loss: 1.18; acc: 0.62
Batch: 60; loss: 1.04; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 1.12; acc: 0.66
Batch: 140; loss: 1.1; acc: 0.73
Batch: 160; loss: 1.26; acc: 0.62
Batch: 180; loss: 1.18; acc: 0.67
Batch: 200; loss: 1.17; acc: 0.64
Batch: 220; loss: 1.16; acc: 0.67
Batch: 240; loss: 1.09; acc: 0.75
Batch: 260; loss: 1.1; acc: 0.67
Batch: 280; loss: 1.11; acc: 0.72
Batch: 300; loss: 1.2; acc: 0.7
Batch: 320; loss: 0.95; acc: 0.72
Batch: 340; loss: 1.25; acc: 0.72
Batch: 360; loss: 1.14; acc: 0.64
Batch: 380; loss: 1.07; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.73
Batch: 420; loss: 1.01; acc: 0.66
Batch: 440; loss: 1.06; acc: 0.72
Batch: 460; loss: 1.06; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.8
Batch: 500; loss: 1.14; acc: 0.67
Batch: 520; loss: 1.25; acc: 0.67
Batch: 540; loss: 1.23; acc: 0.61
Batch: 560; loss: 1.11; acc: 0.56
Batch: 580; loss: 1.28; acc: 0.66
Batch: 600; loss: 1.12; acc: 0.62
Batch: 620; loss: 1.04; acc: 0.7
Batch: 640; loss: 1.26; acc: 0.58
Batch: 660; loss: 1.22; acc: 0.61
Batch: 680; loss: 1.18; acc: 0.66
Batch: 700; loss: 1.21; acc: 0.64
Batch: 720; loss: 1.11; acc: 0.72
Batch: 740; loss: 1.08; acc: 0.77
Batch: 760; loss: 1.22; acc: 0.66
Batch: 780; loss: 1.14; acc: 0.69
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

0.00017359602497890592
0.00016567819693591446
Batch: 0; loss: 1.19; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.82; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.77; acc: 0.89
Val Epoch over. val_loss: 1.0915288966932115; val_accuracy: 0.7105891719745223 

The current subspace-distance is: 0.00016567819693591446 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.61
Batch: 20; loss: 1.25; acc: 0.66
Batch: 40; loss: 1.05; acc: 0.73
Batch: 60; loss: 1.07; acc: 0.69
Batch: 80; loss: 1.17; acc: 0.64
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 1.07; acc: 0.64
Batch: 160; loss: 1.17; acc: 0.66
Batch: 180; loss: 1.09; acc: 0.77
Batch: 200; loss: 1.32; acc: 0.61
Batch: 220; loss: 0.99; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.66
Batch: 260; loss: 1.07; acc: 0.7
Batch: 280; loss: 1.24; acc: 0.7
Batch: 300; loss: 1.36; acc: 0.56
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.19; acc: 0.69
Batch: 360; loss: 1.06; acc: 0.7
Batch: 380; loss: 1.04; acc: 0.73
Batch: 400; loss: 1.17; acc: 0.67
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 1.17; acc: 0.69
Batch: 460; loss: 1.2; acc: 0.62
Batch: 480; loss: 1.12; acc: 0.7
Batch: 500; loss: 1.22; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.67
Batch: 540; loss: 0.97; acc: 0.8
Batch: 560; loss: 1.05; acc: 0.78
Batch: 580; loss: 1.18; acc: 0.73
Batch: 600; loss: 1.21; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.8
Batch: 640; loss: 1.17; acc: 0.61
Batch: 660; loss: 1.17; acc: 0.64
Batch: 680; loss: 1.15; acc: 0.69
Batch: 700; loss: 1.31; acc: 0.61
Batch: 720; loss: 1.11; acc: 0.72
Batch: 740; loss: 1.24; acc: 0.67
Batch: 760; loss: 1.19; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

0.00017377578478772193
0.0001686132891336456
Batch: 0; loss: 1.19; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 1.19; acc: 0.73
Batch: 120; loss: 1.17; acc: 0.72
Batch: 140; loss: 0.77; acc: 0.91
Val Epoch over. val_loss: 1.0844201532898434; val_accuracy: 0.7086982484076433 

The current subspace-distance is: 0.0001686132891336456 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 1.15; acc: 0.73
Batch: 60; loss: 1.02; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 1.1; acc: 0.69
Batch: 160; loss: 1.15; acc: 0.66
Batch: 180; loss: 1.11; acc: 0.67
Batch: 200; loss: 1.18; acc: 0.62
Batch: 220; loss: 1.24; acc: 0.61
Batch: 240; loss: 1.15; acc: 0.7
Batch: 260; loss: 1.03; acc: 0.69
Batch: 280; loss: 1.11; acc: 0.67
Batch: 300; loss: 1.14; acc: 0.72
Batch: 320; loss: 1.15; acc: 0.72
Batch: 340; loss: 1.14; acc: 0.7
Batch: 360; loss: 1.1; acc: 0.7
Batch: 380; loss: 1.14; acc: 0.64
Batch: 400; loss: 1.05; acc: 0.78
Batch: 420; loss: 1.1; acc: 0.66
Batch: 440; loss: 1.23; acc: 0.64
Batch: 460; loss: 1.14; acc: 0.69
Batch: 480; loss: 1.07; acc: 0.77
Batch: 500; loss: 1.04; acc: 0.69
Batch: 520; loss: 1.02; acc: 0.78
Batch: 540; loss: 1.31; acc: 0.58
Batch: 560; loss: 1.18; acc: 0.67
Batch: 580; loss: 1.16; acc: 0.62
Batch: 600; loss: 1.37; acc: 0.55
Batch: 620; loss: 1.11; acc: 0.7
Batch: 640; loss: 1.1; acc: 0.7
Batch: 660; loss: 0.98; acc: 0.75
Batch: 680; loss: 1.18; acc: 0.64
Batch: 700; loss: 1.08; acc: 0.69
Batch: 720; loss: 0.93; acc: 0.83
Batch: 740; loss: 1.07; acc: 0.72
Batch: 760; loss: 1.17; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.66
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00017636152915656567
0.00016999122453853488
Batch: 0; loss: 1.19; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.19; acc: 0.73
Batch: 120; loss: 1.18; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.91
Val Epoch over. val_loss: 1.0815176375352653; val_accuracy: 0.7092953821656051 

The current subspace-distance is: 0.00016999122453853488 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.7
Batch: 20; loss: 1.25; acc: 0.58
Batch: 40; loss: 1.1; acc: 0.64
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 1.22; acc: 0.61
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.67
Batch: 140; loss: 1.14; acc: 0.69
Batch: 160; loss: 1.26; acc: 0.55
Batch: 180; loss: 1.07; acc: 0.67
Batch: 200; loss: 1.19; acc: 0.61
Batch: 220; loss: 1.08; acc: 0.73
Batch: 240; loss: 1.05; acc: 0.75
Batch: 260; loss: 1.12; acc: 0.7
Batch: 280; loss: 1.24; acc: 0.62
Batch: 300; loss: 1.19; acc: 0.7
Batch: 320; loss: 1.18; acc: 0.66
Batch: 340; loss: 1.1; acc: 0.64
Batch: 360; loss: 1.07; acc: 0.69
Batch: 380; loss: 1.26; acc: 0.61
Batch: 400; loss: 1.0; acc: 0.67
Batch: 420; loss: 1.15; acc: 0.66
Batch: 440; loss: 1.13; acc: 0.67
Batch: 460; loss: 1.12; acc: 0.77
Batch: 480; loss: 1.08; acc: 0.7
Batch: 500; loss: 1.04; acc: 0.7
Batch: 520; loss: 1.12; acc: 0.75
Batch: 540; loss: 0.98; acc: 0.8
Batch: 560; loss: 1.0; acc: 0.77
Batch: 580; loss: 1.15; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.62
Batch: 620; loss: 1.21; acc: 0.64
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.35; acc: 0.55
Batch: 680; loss: 1.28; acc: 0.58
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 0.99; acc: 0.75
Batch: 740; loss: 1.17; acc: 0.59
Batch: 760; loss: 0.98; acc: 0.77
Batch: 780; loss: 1.03; acc: 0.64
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

0.00018007295147981495
0.0001736427511787042
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.88
Batch: 60; loss: 1.0; acc: 0.7
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 1.17; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.77
Batch: 140; loss: 0.76; acc: 0.92
Val Epoch over. val_loss: 1.070021076566854; val_accuracy: 0.7102906050955414 

The current subspace-distance is: 0.0001736427511787042 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.66
Batch: 20; loss: 1.08; acc: 0.66
Batch: 40; loss: 1.33; acc: 0.56
Batch: 60; loss: 0.95; acc: 0.78
Batch: 80; loss: 1.11; acc: 0.55
Batch: 100; loss: 1.01; acc: 0.81
Batch: 120; loss: 1.26; acc: 0.59
Batch: 140; loss: 1.02; acc: 0.69
Batch: 160; loss: 1.2; acc: 0.62
Batch: 180; loss: 1.08; acc: 0.72
Batch: 200; loss: 1.11; acc: 0.67
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 1.21; acc: 0.62
Batch: 260; loss: 1.16; acc: 0.72
Batch: 280; loss: 1.12; acc: 0.69
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 1.29; acc: 0.61
Batch: 340; loss: 1.17; acc: 0.64
Batch: 360; loss: 1.03; acc: 0.75
Batch: 380; loss: 1.11; acc: 0.67
Batch: 400; loss: 1.16; acc: 0.7
Batch: 420; loss: 1.19; acc: 0.59
Batch: 440; loss: 1.23; acc: 0.59
Batch: 460; loss: 1.08; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.72
Batch: 500; loss: 1.06; acc: 0.66
Batch: 520; loss: 1.11; acc: 0.73
Batch: 540; loss: 1.12; acc: 0.69
Batch: 560; loss: 1.0; acc: 0.7
Batch: 580; loss: 1.11; acc: 0.72
Batch: 600; loss: 1.03; acc: 0.75
Batch: 620; loss: 1.15; acc: 0.7
Batch: 640; loss: 1.24; acc: 0.61
Batch: 660; loss: 1.13; acc: 0.66
Batch: 680; loss: 1.1; acc: 0.61
Batch: 700; loss: 1.06; acc: 0.67
Batch: 720; loss: 1.12; acc: 0.67
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 1.2; acc: 0.69
Batch: 780; loss: 1.19; acc: 0.59
Train Epoch over. train_loss: 1.12; train_accuracy: 0.68 

0.00018562788318376988
0.00017666049825493246
Batch: 0; loss: 1.18; acc: 0.64
Batch: 20; loss: 1.25; acc: 0.64
Batch: 40; loss: 0.81; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.18; acc: 0.73
Batch: 120; loss: 1.16; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.92
Val Epoch over. val_loss: 1.074961690006742; val_accuracy: 0.7112858280254777 

The current subspace-distance is: 0.00017666049825493246 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.2; acc: 0.62
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 1.12; acc: 0.69
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 1.19; acc: 0.66
Batch: 120; loss: 1.1; acc: 0.69
Batch: 140; loss: 1.31; acc: 0.64
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 1.1; acc: 0.72
Batch: 200; loss: 1.09; acc: 0.66
Batch: 220; loss: 0.91; acc: 0.78
Batch: 240; loss: 1.03; acc: 0.81
Batch: 260; loss: 1.42; acc: 0.53
Batch: 280; loss: 1.13; acc: 0.67
Batch: 300; loss: 1.21; acc: 0.56
Batch: 320; loss: 1.02; acc: 0.69
Batch: 340; loss: 1.26; acc: 0.61
Batch: 360; loss: 0.96; acc: 0.83
Batch: 380; loss: 1.05; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.61
Batch: 420; loss: 1.01; acc: 0.77
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 1.06; acc: 0.69
Batch: 480; loss: 1.12; acc: 0.67
Batch: 500; loss: 1.18; acc: 0.64
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.02; acc: 0.7
Batch: 560; loss: 1.18; acc: 0.59
Batch: 580; loss: 1.12; acc: 0.73
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 1.24; acc: 0.61
Batch: 640; loss: 1.01; acc: 0.77
Batch: 660; loss: 1.11; acc: 0.67
Batch: 680; loss: 1.04; acc: 0.75
Batch: 700; loss: 0.99; acc: 0.75
Batch: 720; loss: 1.12; acc: 0.67
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 1.03; acc: 0.73
Batch: 780; loss: 1.12; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.68 

0.00018621743947733194
0.00018031538638751954
Batch: 0; loss: 1.15; acc: 0.62
Batch: 20; loss: 1.24; acc: 0.67
Batch: 40; loss: 0.79; acc: 0.88
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 0.9; acc: 0.78
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.89
Val Epoch over. val_loss: 1.0576155907029559; val_accuracy: 0.7126791401273885 

The current subspace-distance is: 0.00018031538638751954 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.04; acc: 0.64
Batch: 40; loss: 1.0; acc: 0.72
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 1.17; acc: 0.64
Batch: 160; loss: 1.3; acc: 0.56
Batch: 180; loss: 1.23; acc: 0.58
Batch: 200; loss: 1.11; acc: 0.64
Batch: 220; loss: 1.07; acc: 0.72
Batch: 240; loss: 1.08; acc: 0.66
Batch: 260; loss: 1.04; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.66
Batch: 300; loss: 1.06; acc: 0.7
Batch: 320; loss: 0.95; acc: 0.78
Batch: 340; loss: 1.09; acc: 0.77
Batch: 360; loss: 1.37; acc: 0.59
Batch: 380; loss: 1.24; acc: 0.59
Batch: 400; loss: 1.21; acc: 0.66
Batch: 420; loss: 1.28; acc: 0.56
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.23; acc: 0.64
Batch: 480; loss: 1.13; acc: 0.69
Batch: 500; loss: 1.05; acc: 0.67
Batch: 520; loss: 1.28; acc: 0.72
Batch: 540; loss: 1.2; acc: 0.66
Batch: 560; loss: 1.22; acc: 0.67
Batch: 580; loss: 1.05; acc: 0.7
Batch: 600; loss: 1.09; acc: 0.67
Batch: 620; loss: 1.09; acc: 0.69
Batch: 640; loss: 1.17; acc: 0.69
Batch: 660; loss: 1.09; acc: 0.69
Batch: 680; loss: 1.26; acc: 0.62
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 0.94; acc: 0.81
Batch: 740; loss: 1.24; acc: 0.58
Batch: 760; loss: 1.11; acc: 0.64
Batch: 780; loss: 1.24; acc: 0.59
Train Epoch over. train_loss: 1.11; train_accuracy: 0.68 

0.00018969786469824612
0.00018338917288929224
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.23; acc: 0.64
Batch: 40; loss: 0.79; acc: 0.88
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 0.88; acc: 0.75
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.7
Batch: 140; loss: 0.76; acc: 0.91
Val Epoch over. val_loss: 1.0494122683622276; val_accuracy: 0.7131767515923567 

The current subspace-distance is: 0.00018338917288929224 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.73
Batch: 40; loss: 1.09; acc: 0.72
Batch: 60; loss: 1.09; acc: 0.73
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.94; acc: 0.78
Batch: 120; loss: 1.31; acc: 0.61
Batch: 140; loss: 1.05; acc: 0.72
Batch: 160; loss: 0.98; acc: 0.72
Batch: 180; loss: 1.1; acc: 0.67
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 1.11; acc: 0.72
Batch: 240; loss: 1.21; acc: 0.67
Batch: 260; loss: 1.07; acc: 0.64
Batch: 280; loss: 1.04; acc: 0.72
Batch: 300; loss: 1.18; acc: 0.67
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 1.04; acc: 0.7
Batch: 360; loss: 1.12; acc: 0.66
Batch: 380; loss: 1.04; acc: 0.78
Batch: 400; loss: 1.08; acc: 0.7
Batch: 420; loss: 1.11; acc: 0.67
Batch: 440; loss: 1.11; acc: 0.61
Batch: 460; loss: 1.23; acc: 0.59
Batch: 480; loss: 1.17; acc: 0.61
Batch: 500; loss: 1.17; acc: 0.67
Batch: 520; loss: 1.21; acc: 0.62
Batch: 540; loss: 1.05; acc: 0.66
Batch: 560; loss: 1.0; acc: 0.7
Batch: 580; loss: 1.23; acc: 0.62
Batch: 600; loss: 1.14; acc: 0.7
Batch: 620; loss: 1.04; acc: 0.78
Batch: 640; loss: 1.06; acc: 0.67
Batch: 660; loss: 1.09; acc: 0.7
Batch: 680; loss: 1.05; acc: 0.69
Batch: 700; loss: 1.02; acc: 0.75
Batch: 720; loss: 0.96; acc: 0.75
Batch: 740; loss: 1.16; acc: 0.69
Batch: 760; loss: 1.07; acc: 0.7
Batch: 780; loss: 1.09; acc: 0.62
Train Epoch over. train_loss: 1.1; train_accuracy: 0.69 

0.00019061304919887334
0.0001844844955485314
Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.1; acc: 0.73
Batch: 140; loss: 0.74; acc: 0.91
Val Epoch over. val_loss: 1.037949194194405; val_accuracy: 0.7177547770700637 

The current subspace-distance is: 0.0001844844955485314 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 1.05; acc: 0.73
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.69
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 1.2; acc: 0.58
Batch: 160; loss: 1.17; acc: 0.67
Batch: 180; loss: 1.12; acc: 0.67
Batch: 200; loss: 1.17; acc: 0.66
Batch: 220; loss: 1.13; acc: 0.67
Batch: 240; loss: 1.1; acc: 0.59
Batch: 260; loss: 0.96; acc: 0.73
Batch: 280; loss: 1.21; acc: 0.69
Batch: 300; loss: 1.04; acc: 0.7
Batch: 320; loss: 0.94; acc: 0.73
Batch: 340; loss: 1.07; acc: 0.64
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.35; acc: 0.53
Batch: 400; loss: 1.06; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.67
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.2; acc: 0.64
Batch: 480; loss: 1.06; acc: 0.67
Batch: 500; loss: 1.04; acc: 0.67
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.22; acc: 0.69
Batch: 560; loss: 1.06; acc: 0.64
Batch: 580; loss: 1.08; acc: 0.72
Batch: 600; loss: 1.11; acc: 0.61
Batch: 620; loss: 1.18; acc: 0.61
Batch: 640; loss: 1.08; acc: 0.73
Batch: 660; loss: 1.08; acc: 0.69
Batch: 680; loss: 1.13; acc: 0.64
Batch: 700; loss: 1.25; acc: 0.58
Batch: 720; loss: 1.02; acc: 0.73
Batch: 740; loss: 0.97; acc: 0.7
Batch: 760; loss: 1.02; acc: 0.67
Batch: 780; loss: 1.22; acc: 0.61
Train Epoch over. train_loss: 1.1; train_accuracy: 0.69 

0.00019372021779417992
0.0001865060767158866
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.64
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.88; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.09; acc: 0.75
Batch: 140; loss: 0.75; acc: 0.91
Val Epoch over. val_loss: 1.033752261073726; val_accuracy: 0.71875 

The current subspace-distance is: 0.0001865060767158866 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.15; acc: 0.7
Batch: 20; loss: 1.04; acc: 0.81
Batch: 40; loss: 0.98; acc: 0.78
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 1.17; acc: 0.66
Batch: 100; loss: 1.19; acc: 0.69
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 1.22; acc: 0.64
Batch: 160; loss: 1.22; acc: 0.64
Batch: 180; loss: 1.14; acc: 0.64
Batch: 200; loss: 0.96; acc: 0.8
Batch: 220; loss: 1.32; acc: 0.64
Batch: 240; loss: 0.98; acc: 0.72
Batch: 260; loss: 1.14; acc: 0.67
Batch: 280; loss: 1.24; acc: 0.66
Batch: 300; loss: 1.0; acc: 0.73
Batch: 320; loss: 1.12; acc: 0.7
Batch: 340; loss: 1.15; acc: 0.67
Batch: 360; loss: 1.1; acc: 0.73
Batch: 380; loss: 1.09; acc: 0.69
Batch: 400; loss: 1.13; acc: 0.61
Batch: 420; loss: 1.04; acc: 0.67
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 1.05; acc: 0.69
Batch: 480; loss: 1.06; acc: 0.67
Batch: 500; loss: 1.24; acc: 0.62
Batch: 520; loss: 0.9; acc: 0.8
Batch: 540; loss: 1.1; acc: 0.67
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 1.23; acc: 0.58
Batch: 600; loss: 1.04; acc: 0.73
Batch: 620; loss: 1.04; acc: 0.72
Batch: 640; loss: 1.06; acc: 0.66
Batch: 660; loss: 1.2; acc: 0.62
Batch: 680; loss: 1.06; acc: 0.69
Batch: 700; loss: 1.0; acc: 0.7
Batch: 720; loss: 1.11; acc: 0.66
Batch: 740; loss: 1.26; acc: 0.55
Batch: 760; loss: 0.96; acc: 0.77
Batch: 780; loss: 1.26; acc: 0.61
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00019317533588036895
0.00018556216673459858
Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.21; acc: 0.62
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.77
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 0.74; acc: 0.92
Val Epoch over. val_loss: 1.0375503483851245; val_accuracy: 0.7157643312101911 

The current subspace-distance is: 0.00018556216673459858 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 1.14; acc: 0.69
Batch: 40; loss: 0.99; acc: 0.67
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.66
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 1.1; acc: 0.72
Batch: 140; loss: 1.05; acc: 0.7
Batch: 160; loss: 1.04; acc: 0.7
Batch: 180; loss: 1.06; acc: 0.69
Batch: 200; loss: 0.87; acc: 0.78
Batch: 220; loss: 1.23; acc: 0.56
Batch: 240; loss: 0.94; acc: 0.78
Batch: 260; loss: 1.02; acc: 0.73
Batch: 280; loss: 0.89; acc: 0.8
Batch: 300; loss: 0.94; acc: 0.75
Batch: 320; loss: 1.13; acc: 0.69
Batch: 340; loss: 1.02; acc: 0.72
Batch: 360; loss: 1.2; acc: 0.66
Batch: 380; loss: 1.15; acc: 0.64
Batch: 400; loss: 1.16; acc: 0.67
Batch: 420; loss: 1.1; acc: 0.77
Batch: 440; loss: 1.04; acc: 0.78
Batch: 460; loss: 1.19; acc: 0.69
Batch: 480; loss: 1.17; acc: 0.64
Batch: 500; loss: 1.16; acc: 0.69
Batch: 520; loss: 1.07; acc: 0.7
Batch: 540; loss: 1.07; acc: 0.66
Batch: 560; loss: 1.09; acc: 0.7
Batch: 580; loss: 0.93; acc: 0.7
Batch: 600; loss: 1.08; acc: 0.69
Batch: 620; loss: 0.98; acc: 0.78
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.09; acc: 0.67
Batch: 680; loss: 1.11; acc: 0.67
Batch: 700; loss: 1.2; acc: 0.61
Batch: 720; loss: 0.96; acc: 0.78
Batch: 740; loss: 1.1; acc: 0.69
Batch: 760; loss: 1.05; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.8
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00019501324277371168
0.000187214624020271
Batch: 0; loss: 1.13; acc: 0.66
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.89; acc: 0.77
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.88
Val Epoch over. val_loss: 1.0358989151420108; val_accuracy: 0.7123805732484076 

The current subspace-distance is: 0.000187214624020271 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.15; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 1.1; acc: 0.61
Batch: 60; loss: 1.21; acc: 0.64
Batch: 80; loss: 0.96; acc: 0.75
Batch: 100; loss: 0.94; acc: 0.72
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.67
Batch: 160; loss: 1.13; acc: 0.69
Batch: 180; loss: 1.17; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.7
Batch: 220; loss: 1.0; acc: 0.77
Batch: 240; loss: 1.01; acc: 0.75
Batch: 260; loss: 0.97; acc: 0.78
Batch: 280; loss: 1.08; acc: 0.75
Batch: 300; loss: 1.22; acc: 0.66
Batch: 320; loss: 0.96; acc: 0.77
Batch: 340; loss: 1.14; acc: 0.72
Batch: 360; loss: 1.07; acc: 0.77
Batch: 380; loss: 1.06; acc: 0.62
Batch: 400; loss: 1.23; acc: 0.69
Batch: 420; loss: 1.29; acc: 0.64
Batch: 440; loss: 1.0; acc: 0.69
Batch: 460; loss: 1.08; acc: 0.73
Batch: 480; loss: 0.97; acc: 0.8
Batch: 500; loss: 1.12; acc: 0.7
Batch: 520; loss: 1.19; acc: 0.64
Batch: 540; loss: 1.13; acc: 0.64
Batch: 560; loss: 1.15; acc: 0.61
Batch: 580; loss: 1.05; acc: 0.67
Batch: 600; loss: 1.04; acc: 0.66
Batch: 620; loss: 1.17; acc: 0.67
Batch: 640; loss: 1.01; acc: 0.73
Batch: 660; loss: 1.07; acc: 0.7
Batch: 680; loss: 1.06; acc: 0.7
Batch: 700; loss: 1.1; acc: 0.7
Batch: 720; loss: 1.17; acc: 0.62
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 1.25; acc: 0.67
Batch: 780; loss: 1.17; acc: 0.66
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.0001961597881745547
0.00019007384253200144
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.88; acc: 0.78
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 0.75; acc: 0.91
Val Epoch over. val_loss: 1.0322064165097133; val_accuracy: 0.7147691082802548 

The current subspace-distance is: 0.00019007384253200144 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 1.24; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.27; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 1.09; acc: 0.66
Batch: 180; loss: 1.09; acc: 0.7
Batch: 200; loss: 1.15; acc: 0.61
Batch: 220; loss: 1.11; acc: 0.77
Batch: 240; loss: 1.12; acc: 0.64
Batch: 260; loss: 1.18; acc: 0.7
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 1.02; acc: 0.72
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.21; acc: 0.61
Batch: 360; loss: 1.16; acc: 0.64
Batch: 380; loss: 1.29; acc: 0.56
Batch: 400; loss: 1.05; acc: 0.78
Batch: 420; loss: 1.18; acc: 0.64
Batch: 440; loss: 1.14; acc: 0.64
Batch: 460; loss: 1.11; acc: 0.72
Batch: 480; loss: 1.07; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 1.09; acc: 0.72
Batch: 540; loss: 1.2; acc: 0.66
Batch: 560; loss: 1.19; acc: 0.59
Batch: 580; loss: 1.11; acc: 0.67
Batch: 600; loss: 1.29; acc: 0.56
Batch: 620; loss: 0.93; acc: 0.77
Batch: 640; loss: 1.02; acc: 0.72
Batch: 660; loss: 1.13; acc: 0.7
Batch: 680; loss: 1.04; acc: 0.73
Batch: 700; loss: 1.03; acc: 0.69
Batch: 720; loss: 1.0; acc: 0.78
Batch: 740; loss: 1.14; acc: 0.66
Batch: 760; loss: 1.16; acc: 0.67
Batch: 780; loss: 0.98; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00019864313071593642
0.00019207409059163183
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 0.78; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.77
Batch: 80; loss: 0.87; acc: 0.78
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.08; acc: 0.77
Batch: 140; loss: 0.74; acc: 0.91
Val Epoch over. val_loss: 1.0249677084054156; val_accuracy: 0.7248208598726115 

The current subspace-distance is: 0.00019207409059163183 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.7
Batch: 40; loss: 1.25; acc: 0.59
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.02; acc: 0.75
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.28; acc: 0.58
Batch: 140; loss: 1.09; acc: 0.67
Batch: 160; loss: 1.04; acc: 0.7
Batch: 180; loss: 1.11; acc: 0.7
Batch: 200; loss: 1.07; acc: 0.64
Batch: 220; loss: 0.94; acc: 0.78
Batch: 240; loss: 1.03; acc: 0.66
Batch: 260; loss: 0.95; acc: 0.77
Batch: 280; loss: 1.13; acc: 0.69
Batch: 300; loss: 1.09; acc: 0.73
Batch: 320; loss: 1.21; acc: 0.58
Batch: 340; loss: 1.11; acc: 0.7
Batch: 360; loss: 1.02; acc: 0.73
Batch: 380; loss: 1.12; acc: 0.73
Batch: 400; loss: 1.13; acc: 0.64
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.11; acc: 0.7
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 1.16; acc: 0.66
Batch: 500; loss: 0.93; acc: 0.81
Batch: 520; loss: 1.09; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.77
Batch: 560; loss: 1.23; acc: 0.59
Batch: 580; loss: 1.13; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.73
Batch: 620; loss: 1.03; acc: 0.64
Batch: 640; loss: 0.95; acc: 0.78
Batch: 660; loss: 0.97; acc: 0.75
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.21; acc: 0.61
Batch: 720; loss: 1.28; acc: 0.58
Batch: 740; loss: 1.35; acc: 0.56
Batch: 760; loss: 0.97; acc: 0.73
Batch: 780; loss: 0.98; acc: 0.7
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00020015501650050282
0.0001918324560392648
Batch: 0; loss: 1.13; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.64
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 0.73; acc: 0.91
Val Epoch over. val_loss: 1.0275444821187645; val_accuracy: 0.7169585987261147 

The current subspace-distance is: 0.0001918324560392648 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 1.14; acc: 0.67
Batch: 60; loss: 1.04; acc: 0.7
Batch: 80; loss: 1.08; acc: 0.69
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 1.02; acc: 0.73
Batch: 180; loss: 1.06; acc: 0.72
Batch: 200; loss: 1.2; acc: 0.66
Batch: 220; loss: 1.15; acc: 0.61
Batch: 240; loss: 1.06; acc: 0.77
Batch: 260; loss: 1.11; acc: 0.66
Batch: 280; loss: 1.06; acc: 0.73
Batch: 300; loss: 1.2; acc: 0.62
Batch: 320; loss: 0.88; acc: 0.8
Batch: 340; loss: 1.07; acc: 0.72
Batch: 360; loss: 1.17; acc: 0.67
Batch: 380; loss: 1.15; acc: 0.64
Batch: 400; loss: 1.22; acc: 0.67
Batch: 420; loss: 1.05; acc: 0.69
Batch: 440; loss: 0.98; acc: 0.73
Batch: 460; loss: 1.04; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.7
Batch: 500; loss: 1.07; acc: 0.62
Batch: 520; loss: 1.14; acc: 0.77
Batch: 540; loss: 1.15; acc: 0.64
Batch: 560; loss: 1.0; acc: 0.7
Batch: 580; loss: 1.17; acc: 0.61
Batch: 600; loss: 1.05; acc: 0.69
Batch: 620; loss: 0.96; acc: 0.73
Batch: 640; loss: 0.99; acc: 0.75
Batch: 660; loss: 1.06; acc: 0.73
Batch: 680; loss: 1.05; acc: 0.62
Batch: 700; loss: 0.94; acc: 0.73
Batch: 720; loss: 1.05; acc: 0.7
Batch: 740; loss: 1.16; acc: 0.64
Batch: 760; loss: 0.96; acc: 0.78
Batch: 780; loss: 1.11; acc: 0.67
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00019723722652997822
0.00019083384540863335
Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.07; acc: 0.73
Batch: 140; loss: 0.74; acc: 0.91
Val Epoch over. val_loss: 1.0248612104707462; val_accuracy: 0.7170581210191083 

The current subspace-distance is: 0.00019083384540863335 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 1.12; acc: 0.64
Batch: 60; loss: 1.13; acc: 0.67
Batch: 80; loss: 1.15; acc: 0.69
Batch: 100; loss: 1.03; acc: 0.72
Batch: 120; loss: 1.08; acc: 0.64
Batch: 140; loss: 1.15; acc: 0.58
Batch: 160; loss: 1.21; acc: 0.62
Batch: 180; loss: 1.25; acc: 0.56
Batch: 200; loss: 1.22; acc: 0.67
Batch: 220; loss: 1.02; acc: 0.72
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.15; acc: 0.67
Batch: 280; loss: 1.07; acc: 0.64
Batch: 300; loss: 1.09; acc: 0.72
Batch: 320; loss: 1.36; acc: 0.59
Batch: 340; loss: 1.24; acc: 0.58
Batch: 360; loss: 1.15; acc: 0.64
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.09; acc: 0.69
Batch: 420; loss: 1.2; acc: 0.62
Batch: 440; loss: 1.15; acc: 0.66
Batch: 460; loss: 1.0; acc: 0.78
Batch: 480; loss: 1.11; acc: 0.72
Batch: 500; loss: 1.25; acc: 0.59
Batch: 520; loss: 1.09; acc: 0.67
Batch: 540; loss: 0.86; acc: 0.78
Batch: 560; loss: 0.94; acc: 0.75
Batch: 580; loss: 0.96; acc: 0.73
Batch: 600; loss: 1.04; acc: 0.77
Batch: 620; loss: 1.2; acc: 0.66
Batch: 640; loss: 0.96; acc: 0.78
Batch: 660; loss: 1.11; acc: 0.67
Batch: 680; loss: 1.06; acc: 0.67
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 1.19; acc: 0.66
Batch: 740; loss: 1.09; acc: 0.73
Batch: 760; loss: 1.19; acc: 0.66
Batch: 780; loss: 1.07; acc: 0.66
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00020192809461150318
0.00019631783652585
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.07; acc: 0.72
Batch: 140; loss: 0.74; acc: 0.91
Val Epoch over. val_loss: 1.0262862041497687; val_accuracy: 0.7165605095541401 

The current subspace-distance is: 0.00019631783652585 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.07; acc: 0.69
Batch: 20; loss: 0.9; acc: 0.84
Batch: 40; loss: 1.18; acc: 0.64
Batch: 60; loss: 0.95; acc: 0.75
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.23; acc: 0.66
Batch: 140; loss: 1.02; acc: 0.73
Batch: 160; loss: 1.22; acc: 0.61
Batch: 180; loss: 0.89; acc: 0.73
Batch: 200; loss: 1.04; acc: 0.73
Batch: 220; loss: 1.13; acc: 0.64
Batch: 240; loss: 0.93; acc: 0.77
Batch: 260; loss: 1.18; acc: 0.66
Batch: 280; loss: 0.99; acc: 0.69
Batch: 300; loss: 0.94; acc: 0.78
Batch: 320; loss: 1.19; acc: 0.69
Batch: 340; loss: 1.16; acc: 0.66
Batch: 360; loss: 1.21; acc: 0.66
Batch: 380; loss: 1.22; acc: 0.64
Batch: 400; loss: 1.18; acc: 0.62
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 1.24; acc: 0.62
Batch: 460; loss: 1.04; acc: 0.75
Batch: 480; loss: 1.25; acc: 0.61
Batch: 500; loss: 1.14; acc: 0.64
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 1.11; acc: 0.67
Batch: 560; loss: 1.09; acc: 0.7
Batch: 580; loss: 1.19; acc: 0.69
Batch: 600; loss: 0.94; acc: 0.77
Batch: 620; loss: 1.01; acc: 0.72
Batch: 640; loss: 1.21; acc: 0.62
Batch: 660; loss: 1.03; acc: 0.72
Batch: 680; loss: 0.96; acc: 0.75
Batch: 700; loss: 1.13; acc: 0.69
Batch: 720; loss: 1.0; acc: 0.8
Batch: 740; loss: 1.19; acc: 0.61
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 1.08; acc: 0.67
Train Epoch over. train_loss: 1.08; train_accuracy: 0.69 

0.0001991919707506895
0.00019246741430833936
Batch: 0; loss: 1.13; acc: 0.64
Batch: 20; loss: 1.21; acc: 0.64
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 0.75; acc: 0.91
Val Epoch over. val_loss: 1.0346855407307862; val_accuracy: 0.7076035031847133 

The current subspace-distance is: 0.00019246741430833936 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.7
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 1.0; acc: 0.69
Batch: 60; loss: 0.99; acc: 0.8
Batch: 80; loss: 1.28; acc: 0.58
Batch: 100; loss: 0.99; acc: 0.72
Batch: 120; loss: 1.06; acc: 0.72
Batch: 140; loss: 1.36; acc: 0.56
Batch: 160; loss: 1.13; acc: 0.62
Batch: 180; loss: 0.97; acc: 0.73
Batch: 200; loss: 1.17; acc: 0.59
Batch: 220; loss: 1.08; acc: 0.7
Batch: 240; loss: 1.1; acc: 0.73
Batch: 260; loss: 1.11; acc: 0.66
Batch: 280; loss: 1.03; acc: 0.7
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 1.01; acc: 0.67
Batch: 360; loss: 1.15; acc: 0.64
Batch: 380; loss: 1.13; acc: 0.69
Batch: 400; loss: 1.27; acc: 0.67
Batch: 420; loss: 1.07; acc: 0.64
Batch: 440; loss: 0.96; acc: 0.81
Batch: 460; loss: 1.09; acc: 0.69
Batch: 480; loss: 0.9; acc: 0.81
Batch: 500; loss: 1.2; acc: 0.61
Batch: 520; loss: 1.18; acc: 0.62
Batch: 540; loss: 1.04; acc: 0.67
Batch: 560; loss: 1.03; acc: 0.67
Batch: 580; loss: 1.05; acc: 0.73
Batch: 600; loss: 1.02; acc: 0.69
Batch: 620; loss: 1.09; acc: 0.7
Batch: 640; loss: 1.09; acc: 0.66
Batch: 660; loss: 1.17; acc: 0.7
Batch: 680; loss: 1.36; acc: 0.56
Batch: 700; loss: 1.23; acc: 0.61
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 0.98; acc: 0.73
Batch: 760; loss: 1.17; acc: 0.64
Batch: 780; loss: 1.07; acc: 0.7
Train Epoch over. train_loss: 1.09; train_accuracy: 0.69 

0.00020306128135416657
0.00019472565327305347
Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.62
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.89; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.72
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 0.75; acc: 0.91
Val Epoch over. val_loss: 1.0203122029638594; val_accuracy: 0.7190485668789809 

The current subspace-distance is: 0.00019472565327305347 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.64
Batch: 20; loss: 0.88; acc: 0.8
Batch: 40; loss: 0.94; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 1.01; acc: 0.72
Batch: 160; loss: 1.06; acc: 0.69
Batch: 180; loss: 1.04; acc: 0.7
Batch: 200; loss: 1.06; acc: 0.72
Batch: 220; loss: 1.09; acc: 0.73
Batch: 240; loss: 1.1; acc: 0.73
Batch: 260; loss: 1.04; acc: 0.7
Batch: 280; loss: 1.16; acc: 0.67
Batch: 300; loss: 1.12; acc: 0.72
Batch: 320; loss: 1.1; acc: 0.67
Batch: 340; loss: 1.1; acc: 0.67
Batch: 360; loss: 1.01; acc: 0.69
Batch: 380; loss: 1.11; acc: 0.64
Batch: 400; loss: 1.14; acc: 0.62
Batch: 420; loss: 0.9; acc: 0.77
Batch: 440; loss: 0.92; acc: 0.81
Batch: 460; loss: 1.1; acc: 0.66
Batch: 480; loss: 0.98; acc: 0.69
Batch: 500; loss: 1.04; acc: 0.72
Batch: 520; loss: 1.02; acc: 0.67
Batch: 540; loss: 1.11; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.66
Batch: 580; loss: 1.1; acc: 0.7
Batch: 600; loss: 1.01; acc: 0.77
Batch: 620; loss: 1.01; acc: 0.7
Batch: 640; loss: 1.1; acc: 0.66
Batch: 660; loss: 1.15; acc: 0.64
Batch: 680; loss: 1.11; acc: 0.66
Batch: 700; loss: 0.99; acc: 0.78
Batch: 720; loss: 1.1; acc: 0.7
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 1.1; acc: 0.67
Batch: 780; loss: 1.15; acc: 0.64
Train Epoch over. train_loss: 1.08; train_accuracy: 0.69 

0.00020045333076268435
0.00019405336934141815
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 0.95; acc: 0.73
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 0.74; acc: 0.88
Val Epoch over. val_loss: 1.0166632047124728; val_accuracy: 0.7197452229299363 

The current subspace-distance is: 0.00019405336934141815 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_4_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.170581494228353

The number of parameters is: 276580

The number of individual parameters is:

34
544
34
34
51
48552
51
51
101
144228
101
101
64
77568
64
64
4096
64
640
10
64
64

nonzero elements in E: 55315996
elements in E: 55316000
fraction nonzero: 0.9999999276881915
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.17; acc: 0.23
Batch: 40; loss: 2.02; acc: 0.31
Batch: 60; loss: 2.02; acc: 0.33
Batch: 80; loss: 1.88; acc: 0.52
Batch: 100; loss: 1.77; acc: 0.52
Batch: 120; loss: 1.87; acc: 0.42
Batch: 140; loss: 1.83; acc: 0.41
Batch: 160; loss: 1.72; acc: 0.56
Batch: 180; loss: 1.65; acc: 0.61
Batch: 200; loss: 1.65; acc: 0.58
Batch: 220; loss: 1.7; acc: 0.56
Batch: 240; loss: 1.66; acc: 0.55
Batch: 260; loss: 1.49; acc: 0.72
Batch: 280; loss: 1.61; acc: 0.59
Batch: 300; loss: 1.6; acc: 0.62
Batch: 320; loss: 1.6; acc: 0.58
Batch: 340; loss: 1.48; acc: 0.66
Batch: 360; loss: 1.5; acc: 0.69
Batch: 380; loss: 1.63; acc: 0.55
Batch: 400; loss: 1.64; acc: 0.52
Batch: 420; loss: 1.55; acc: 0.59
Batch: 440; loss: 1.51; acc: 0.62
Batch: 460; loss: 1.37; acc: 0.78
Batch: 480; loss: 1.44; acc: 0.69
Batch: 500; loss: 1.36; acc: 0.69
Batch: 520; loss: 1.53; acc: 0.56
Batch: 540; loss: 1.44; acc: 0.64
Batch: 560; loss: 1.4; acc: 0.72
Batch: 580; loss: 1.63; acc: 0.5
Batch: 600; loss: 1.4; acc: 0.64
Batch: 620; loss: 1.55; acc: 0.62
Batch: 640; loss: 1.54; acc: 0.61
Batch: 660; loss: 1.42; acc: 0.69
Batch: 680; loss: 1.36; acc: 0.72
Batch: 700; loss: 1.43; acc: 0.64
Batch: 720; loss: 1.36; acc: 0.67
Batch: 740; loss: 1.36; acc: 0.72
Batch: 760; loss: 1.49; acc: 0.7
Batch: 780; loss: 1.35; acc: 0.73
Train Epoch over. train_loss: 1.59; train_accuracy: 0.59 

6.213432789081708e-05
5.658492227667011e-05
Batch: 0; loss: 1.34; acc: 0.72
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.07; acc: 0.91
Batch: 60; loss: 1.27; acc: 0.73
Batch: 80; loss: 1.17; acc: 0.8
Batch: 100; loss: 1.3; acc: 0.78
Batch: 120; loss: 1.41; acc: 0.7
Batch: 140; loss: 1.18; acc: 0.78
Val Epoch over. val_loss: 1.3075254206444806; val_accuracy: 0.7210390127388535 

The current subspace-distance is: 5.658492227667011e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.69
Batch: 20; loss: 1.31; acc: 0.73
Batch: 40; loss: 1.39; acc: 0.56
Batch: 60; loss: 1.25; acc: 0.72
Batch: 80; loss: 1.39; acc: 0.66
Batch: 100; loss: 1.35; acc: 0.78
Batch: 120; loss: 1.34; acc: 0.7
Batch: 140; loss: 1.36; acc: 0.7
Batch: 160; loss: 1.3; acc: 0.75
Batch: 180; loss: 1.42; acc: 0.67
Batch: 200; loss: 1.19; acc: 0.81
Batch: 220; loss: 1.33; acc: 0.7
Batch: 240; loss: 1.47; acc: 0.62
Batch: 260; loss: 1.29; acc: 0.72
Batch: 280; loss: 1.36; acc: 0.72
Batch: 300; loss: 1.28; acc: 0.66
Batch: 320; loss: 1.32; acc: 0.59
Batch: 340; loss: 1.27; acc: 0.78
Batch: 360; loss: 1.18; acc: 0.77
Batch: 380; loss: 1.4; acc: 0.56
Batch: 400; loss: 1.28; acc: 0.8
Batch: 420; loss: 1.27; acc: 0.72
Batch: 440; loss: 1.15; acc: 0.78
Batch: 460; loss: 1.32; acc: 0.69
Batch: 480; loss: 1.19; acc: 0.78
Batch: 500; loss: 1.36; acc: 0.67
Batch: 520; loss: 1.38; acc: 0.61
Batch: 540; loss: 1.25; acc: 0.72
Batch: 560; loss: 1.25; acc: 0.72
Batch: 580; loss: 1.32; acc: 0.66
Batch: 600; loss: 1.16; acc: 0.69
Batch: 620; loss: 1.36; acc: 0.64
Batch: 640; loss: 1.17; acc: 0.73
Batch: 660; loss: 1.14; acc: 0.81
Batch: 680; loss: 1.17; acc: 0.77
Batch: 700; loss: 1.25; acc: 0.69
Batch: 720; loss: 1.05; acc: 0.81
Batch: 740; loss: 1.14; acc: 0.78
Batch: 760; loss: 1.21; acc: 0.77
Batch: 780; loss: 1.17; acc: 0.78
Train Epoch over. train_loss: 1.26; train_accuracy: 0.72 

8.14886370790191e-05
7.727562478976324e-05
Batch: 0; loss: 1.16; acc: 0.77
Batch: 20; loss: 1.38; acc: 0.67
Batch: 40; loss: 0.85; acc: 0.89
Batch: 60; loss: 1.11; acc: 0.8
Batch: 80; loss: 1.0; acc: 0.92
Batch: 100; loss: 1.18; acc: 0.78
Batch: 120; loss: 1.26; acc: 0.69
Batch: 140; loss: 0.94; acc: 0.84
Val Epoch over. val_loss: 1.142762450275907; val_accuracy: 0.7573646496815286 

The current subspace-distance is: 7.727562478976324e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.16; acc: 0.77
Batch: 20; loss: 1.35; acc: 0.64
Batch: 40; loss: 1.15; acc: 0.73
Batch: 60; loss: 1.2; acc: 0.7
Batch: 80; loss: 1.25; acc: 0.72
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.06; acc: 0.8
Batch: 140; loss: 1.2; acc: 0.72
Batch: 160; loss: 1.29; acc: 0.69
Batch: 180; loss: 1.36; acc: 0.64
Batch: 200; loss: 1.23; acc: 0.72
Batch: 220; loss: 1.25; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.8
Batch: 260; loss: 1.13; acc: 0.78
Batch: 280; loss: 1.15; acc: 0.72
Batch: 300; loss: 1.15; acc: 0.73
Batch: 320; loss: 1.31; acc: 0.64
Batch: 340; loss: 1.36; acc: 0.62
Batch: 360; loss: 1.12; acc: 0.73
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.07; acc: 0.73
Batch: 420; loss: 1.09; acc: 0.75
Batch: 440; loss: 1.05; acc: 0.81
Batch: 460; loss: 1.05; acc: 0.8
Batch: 480; loss: 1.33; acc: 0.67
Batch: 500; loss: 1.2; acc: 0.66
Batch: 520; loss: 1.05; acc: 0.86
Batch: 540; loss: 1.14; acc: 0.75
Batch: 560; loss: 1.03; acc: 0.81
Batch: 580; loss: 1.03; acc: 0.78
Batch: 600; loss: 1.09; acc: 0.81
Batch: 620; loss: 1.07; acc: 0.77
Batch: 640; loss: 1.13; acc: 0.75
Batch: 660; loss: 1.11; acc: 0.72
Batch: 680; loss: 1.09; acc: 0.81
Batch: 700; loss: 1.07; acc: 0.75
Batch: 720; loss: 1.12; acc: 0.75
Batch: 740; loss: 1.25; acc: 0.67
Batch: 760; loss: 1.04; acc: 0.78
Batch: 780; loss: 1.2; acc: 0.73
Train Epoch over. train_loss: 1.15; train_accuracy: 0.74 

9.699756628833711e-05
9.295631753047928e-05
Batch: 0; loss: 1.12; acc: 0.75
Batch: 20; loss: 1.33; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.94
Batch: 60; loss: 1.04; acc: 0.8
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.81
Batch: 120; loss: 1.22; acc: 0.7
Batch: 140; loss: 0.89; acc: 0.83
Val Epoch over. val_loss: 1.0818980647500154; val_accuracy: 0.7695063694267515 

The current subspace-distance is: 9.295631753047928e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.88
Batch: 20; loss: 1.0; acc: 0.8
Batch: 40; loss: 1.28; acc: 0.72
Batch: 60; loss: 0.93; acc: 0.8
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.01; acc: 0.83
Batch: 120; loss: 1.02; acc: 0.8
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 1.03; acc: 0.78
Batch: 180; loss: 1.11; acc: 0.73
Batch: 200; loss: 1.23; acc: 0.69
Batch: 220; loss: 1.03; acc: 0.78
Batch: 240; loss: 1.12; acc: 0.77
Batch: 260; loss: 1.1; acc: 0.7
Batch: 280; loss: 1.09; acc: 0.75
Batch: 300; loss: 1.24; acc: 0.72
Batch: 320; loss: 0.93; acc: 0.8
Batch: 340; loss: 1.04; acc: 0.78
Batch: 360; loss: 1.18; acc: 0.7
Batch: 380; loss: 0.95; acc: 0.81
Batch: 400; loss: 1.1; acc: 0.75
Batch: 420; loss: 1.12; acc: 0.8
Batch: 440; loss: 1.02; acc: 0.77
Batch: 460; loss: 0.85; acc: 0.84
Batch: 480; loss: 0.97; acc: 0.84
Batch: 500; loss: 1.04; acc: 0.77
Batch: 520; loss: 1.01; acc: 0.77
Batch: 540; loss: 1.06; acc: 0.72
Batch: 560; loss: 1.03; acc: 0.75
Batch: 580; loss: 1.08; acc: 0.8
Batch: 600; loss: 1.07; acc: 0.81
Batch: 620; loss: 1.13; acc: 0.66
Batch: 640; loss: 1.09; acc: 0.75
Batch: 660; loss: 1.02; acc: 0.77
Batch: 680; loss: 1.07; acc: 0.72
Batch: 700; loss: 1.0; acc: 0.83
Batch: 720; loss: 1.06; acc: 0.77
Batch: 740; loss: 1.0; acc: 0.8
Batch: 760; loss: 1.08; acc: 0.72
Batch: 780; loss: 0.97; acc: 0.8
Train Epoch over. train_loss: 1.08; train_accuracy: 0.75 

0.0001083283350453712
0.000103124970337376
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.25; acc: 0.67
Batch: 40; loss: 0.71; acc: 0.88
Batch: 60; loss: 0.96; acc: 0.8
Batch: 80; loss: 0.93; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 0.85; acc: 0.84
Val Epoch over. val_loss: 1.0144327423375123; val_accuracy: 0.7784633757961783 

The current subspace-distance is: 0.000103124970337376 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.77
Batch: 20; loss: 0.94; acc: 0.8
Batch: 40; loss: 1.03; acc: 0.78
Batch: 60; loss: 0.78; acc: 0.92
Batch: 80; loss: 1.15; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.06; acc: 0.77
Batch: 140; loss: 0.87; acc: 0.91
Batch: 160; loss: 1.1; acc: 0.64
Batch: 180; loss: 1.01; acc: 0.73
Batch: 200; loss: 1.14; acc: 0.66
Batch: 220; loss: 0.95; acc: 0.8
Batch: 240; loss: 1.12; acc: 0.7
Batch: 260; loss: 1.04; acc: 0.75
Batch: 280; loss: 0.99; acc: 0.8
Batch: 300; loss: 1.13; acc: 0.75
Batch: 320; loss: 0.94; acc: 0.8
Batch: 340; loss: 1.06; acc: 0.7
Batch: 360; loss: 1.0; acc: 0.81
Batch: 380; loss: 0.99; acc: 0.8
Batch: 400; loss: 1.02; acc: 0.73
Batch: 420; loss: 0.95; acc: 0.81
Batch: 440; loss: 1.15; acc: 0.69
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.05; acc: 0.75
Batch: 500; loss: 1.0; acc: 0.8
Batch: 520; loss: 0.86; acc: 0.88
Batch: 540; loss: 1.07; acc: 0.73
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 0.84; acc: 0.83
Batch: 600; loss: 0.95; acc: 0.83
Batch: 620; loss: 0.9; acc: 0.81
Batch: 640; loss: 1.0; acc: 0.77
Batch: 660; loss: 1.1; acc: 0.73
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.0; acc: 0.78
Batch: 720; loss: 0.95; acc: 0.77
Batch: 740; loss: 1.04; acc: 0.77
Batch: 760; loss: 1.16; acc: 0.7
Batch: 780; loss: 0.9; acc: 0.8
Train Epoch over. train_loss: 1.03; train_accuracy: 0.76 

0.00011780163913499564
0.00011310808622511104
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.66; acc: 0.92
Batch: 60; loss: 0.9; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.81
Batch: 100; loss: 0.93; acc: 0.83
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.78; acc: 0.88
Val Epoch over. val_loss: 0.9632623142497555; val_accuracy: 0.7878184713375797 

The current subspace-distance is: 0.00011310808622511104 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.78
Batch: 20; loss: 0.97; acc: 0.8
Batch: 40; loss: 1.0; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.75
Batch: 80; loss: 0.96; acc: 0.8
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.02; acc: 0.77
Batch: 140; loss: 0.99; acc: 0.8
Batch: 160; loss: 0.96; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.89
Batch: 200; loss: 1.01; acc: 0.75
Batch: 220; loss: 1.04; acc: 0.75
Batch: 240; loss: 0.91; acc: 0.84
Batch: 260; loss: 0.98; acc: 0.78
Batch: 280; loss: 0.94; acc: 0.75
Batch: 300; loss: 0.86; acc: 0.78
Batch: 320; loss: 1.17; acc: 0.72
Batch: 340; loss: 1.06; acc: 0.78
Batch: 360; loss: 0.94; acc: 0.75
Batch: 380; loss: 0.99; acc: 0.77
Batch: 400; loss: 0.98; acc: 0.83
Batch: 420; loss: 0.98; acc: 0.83
Batch: 440; loss: 0.94; acc: 0.81
Batch: 460; loss: 1.0; acc: 0.75
Batch: 480; loss: 1.03; acc: 0.73
Batch: 500; loss: 0.98; acc: 0.78
Batch: 520; loss: 0.86; acc: 0.89
Batch: 540; loss: 1.04; acc: 0.73
Batch: 560; loss: 0.94; acc: 0.77
Batch: 580; loss: 0.98; acc: 0.77
Batch: 600; loss: 0.83; acc: 0.88
Batch: 620; loss: 0.89; acc: 0.83
Batch: 640; loss: 1.0; acc: 0.73
Batch: 660; loss: 0.87; acc: 0.8
Batch: 680; loss: 1.18; acc: 0.59
Batch: 700; loss: 0.8; acc: 0.89
Batch: 720; loss: 1.02; acc: 0.73
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 1.02; acc: 0.72
Batch: 780; loss: 0.89; acc: 0.8
Train Epoch over. train_loss: 0.98; train_accuracy: 0.77 

0.00013110451982356608
0.00012545380741357803
Batch: 0; loss: 0.94; acc: 0.77
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.86
Batch: 120; loss: 0.99; acc: 0.72
Batch: 140; loss: 0.69; acc: 0.91
Val Epoch over. val_loss: 0.8981018571337317; val_accuracy: 0.7989649681528662 

The current subspace-distance is: 0.00012545380741357803 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.86
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 1.0; acc: 0.77
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.88; acc: 0.8
Batch: 100; loss: 1.09; acc: 0.67
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.89; acc: 0.75
Batch: 160; loss: 0.97; acc: 0.77
Batch: 180; loss: 0.98; acc: 0.77
Batch: 200; loss: 0.85; acc: 0.86
Batch: 220; loss: 0.79; acc: 0.8
Batch: 240; loss: 0.95; acc: 0.78
Batch: 260; loss: 0.98; acc: 0.77
Batch: 280; loss: 1.03; acc: 0.72
Batch: 300; loss: 0.99; acc: 0.72
Batch: 320; loss: 0.89; acc: 0.83
Batch: 340; loss: 0.95; acc: 0.75
Batch: 360; loss: 1.0; acc: 0.78
Batch: 380; loss: 0.96; acc: 0.8
Batch: 400; loss: 0.89; acc: 0.81
Batch: 420; loss: 0.96; acc: 0.77
Batch: 440; loss: 0.86; acc: 0.77
Batch: 460; loss: 0.79; acc: 0.83
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 0.99; acc: 0.73
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.76; acc: 0.84
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.8; acc: 0.84
Batch: 600; loss: 0.94; acc: 0.78
Batch: 620; loss: 1.04; acc: 0.7
Batch: 640; loss: 0.83; acc: 0.81
Batch: 660; loss: 0.87; acc: 0.81
Batch: 680; loss: 0.89; acc: 0.81
Batch: 700; loss: 0.89; acc: 0.81
Batch: 720; loss: 0.93; acc: 0.72
Batch: 740; loss: 0.78; acc: 0.83
Batch: 760; loss: 1.06; acc: 0.72
Batch: 780; loss: 0.88; acc: 0.78
Train Epoch over. train_loss: 0.92; train_accuracy: 0.78 

0.0001439446787117049
0.00013909915287513286
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 0.6; acc: 0.91
Batch: 60; loss: 0.8; acc: 0.86
Batch: 80; loss: 0.81; acc: 0.81
Batch: 100; loss: 0.8; acc: 0.86
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.86
Val Epoch over. val_loss: 0.85451107685733; val_accuracy: 0.7999601910828026 

The current subspace-distance is: 0.00013909915287513286 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 0.83; acc: 0.84
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 0.85; acc: 0.81
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 0.88; acc: 0.73
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.96; acc: 0.78
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.87; acc: 0.8
Batch: 220; loss: 0.81; acc: 0.88
Batch: 240; loss: 0.94; acc: 0.83
Batch: 260; loss: 0.9; acc: 0.75
Batch: 280; loss: 0.93; acc: 0.75
Batch: 300; loss: 0.98; acc: 0.75
Batch: 320; loss: 0.91; acc: 0.8
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.93; acc: 0.77
Batch: 380; loss: 1.04; acc: 0.72
Batch: 400; loss: 0.93; acc: 0.78
Batch: 420; loss: 0.99; acc: 0.7
Batch: 440; loss: 0.88; acc: 0.77
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.88; acc: 0.77
Batch: 500; loss: 0.99; acc: 0.69
Batch: 520; loss: 0.85; acc: 0.81
Batch: 540; loss: 0.87; acc: 0.8
Batch: 560; loss: 0.81; acc: 0.83
Batch: 580; loss: 0.87; acc: 0.78
Batch: 600; loss: 0.82; acc: 0.8
Batch: 620; loss: 0.73; acc: 0.86
Batch: 640; loss: 0.87; acc: 0.78
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 0.86; acc: 0.83
Batch: 700; loss: 0.83; acc: 0.83
Batch: 720; loss: 0.71; acc: 0.89
Batch: 740; loss: 0.74; acc: 0.86
Batch: 760; loss: 0.81; acc: 0.8
Batch: 780; loss: 0.72; acc: 0.86
Train Epoch over. train_loss: 0.87; train_accuracy: 0.79 

0.00015470065409317613
0.00014915064093656838
Batch: 0; loss: 0.8; acc: 0.83
Batch: 20; loss: 1.0; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.92
Val Epoch over. val_loss: 0.8005745422308612; val_accuracy: 0.8191679936305732 

The current subspace-distance is: 0.00014915064093656838 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.88
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.81; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.89
Batch: 80; loss: 0.98; acc: 0.77
Batch: 100; loss: 0.83; acc: 0.81
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.89; acc: 0.77
Batch: 160; loss: 1.03; acc: 0.75
Batch: 180; loss: 0.81; acc: 0.8
Batch: 200; loss: 0.9; acc: 0.78
Batch: 220; loss: 0.96; acc: 0.8
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.9; acc: 0.77
Batch: 280; loss: 0.74; acc: 0.84
Batch: 300; loss: 0.86; acc: 0.78
Batch: 320; loss: 0.96; acc: 0.77
Batch: 340; loss: 0.79; acc: 0.78
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.84; acc: 0.78
Batch: 400; loss: 0.9; acc: 0.78
Batch: 420; loss: 0.84; acc: 0.83
Batch: 440; loss: 1.01; acc: 0.7
Batch: 460; loss: 0.79; acc: 0.81
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 1.11; acc: 0.7
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 0.69; acc: 0.88
Batch: 560; loss: 0.76; acc: 0.83
Batch: 580; loss: 0.93; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.78
Batch: 620; loss: 0.84; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.86
Batch: 660; loss: 0.78; acc: 0.84
Batch: 680; loss: 0.76; acc: 0.83
Batch: 700; loss: 0.75; acc: 0.84
Batch: 720; loss: 0.8; acc: 0.84
Batch: 740; loss: 0.86; acc: 0.69
Batch: 760; loss: 0.88; acc: 0.8
Batch: 780; loss: 0.76; acc: 0.77
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

0.0001650884805712849
0.0001590366882737726
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.54; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.92
Batch: 120; loss: 0.87; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.91
Val Epoch over. val_loss: 0.7727505834239303; val_accuracy: 0.8241441082802548 

The current subspace-distance is: 0.0001590366882737726 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.75
Batch: 20; loss: 0.85; acc: 0.83
Batch: 40; loss: 0.84; acc: 0.75
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 0.76; acc: 0.86
Batch: 140; loss: 0.9; acc: 0.75
Batch: 160; loss: 0.93; acc: 0.77
Batch: 180; loss: 0.8; acc: 0.86
Batch: 200; loss: 0.79; acc: 0.83
Batch: 220; loss: 0.73; acc: 0.86
Batch: 240; loss: 0.82; acc: 0.83
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.86
Batch: 300; loss: 0.9; acc: 0.73
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.82; acc: 0.77
Batch: 360; loss: 0.9; acc: 0.73
Batch: 380; loss: 0.69; acc: 0.86
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 1.03; acc: 0.67
Batch: 440; loss: 0.92; acc: 0.77
Batch: 460; loss: 0.77; acc: 0.83
Batch: 480; loss: 0.84; acc: 0.8
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.7
Batch: 540; loss: 0.62; acc: 0.88
Batch: 560; loss: 0.78; acc: 0.81
Batch: 580; loss: 0.97; acc: 0.7
Batch: 600; loss: 0.83; acc: 0.75
Batch: 620; loss: 0.74; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.89
Batch: 680; loss: 0.78; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.83
Batch: 720; loss: 0.87; acc: 0.78
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.94
Batch: 780; loss: 0.78; acc: 0.78
Train Epoch over. train_loss: 0.8; train_accuracy: 0.81 

0.00017658245633356273
0.00016937864711508155
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.91
Val Epoch over. val_loss: 0.7378871028970002; val_accuracy: 0.8286226114649682 

The current subspace-distance is: 0.00016937864711508155 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.67; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.88
Batch: 160; loss: 0.77; acc: 0.83
Batch: 180; loss: 0.75; acc: 0.81
Batch: 200; loss: 0.85; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.91
Batch: 240; loss: 0.7; acc: 0.88
Batch: 260; loss: 0.77; acc: 0.81
Batch: 280; loss: 0.86; acc: 0.77
Batch: 300; loss: 0.7; acc: 0.86
Batch: 320; loss: 0.83; acc: 0.73
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.81; acc: 0.77
Batch: 380; loss: 0.89; acc: 0.75
Batch: 400; loss: 0.76; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.84
Batch: 440; loss: 0.79; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.91; acc: 0.73
Batch: 500; loss: 0.86; acc: 0.78
Batch: 520; loss: 0.8; acc: 0.81
Batch: 540; loss: 0.82; acc: 0.81
Batch: 560; loss: 0.79; acc: 0.77
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.89; acc: 0.77
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.71; acc: 0.86
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.73; acc: 0.78
Batch: 700; loss: 0.89; acc: 0.77
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.69; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.85; acc: 0.78
Train Epoch over. train_loss: 0.78; train_accuracy: 0.82 

0.00018058142450172454
0.00017260397726204246
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.92
Val Epoch over. val_loss: 0.7240225405070433; val_accuracy: 0.8384753184713376 

The current subspace-distance is: 0.00017260397726204246 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.84
Batch: 20; loss: 0.95; acc: 0.78
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.8
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.74; acc: 0.78
Batch: 160; loss: 0.71; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.85; acc: 0.77
Batch: 220; loss: 0.78; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.89
Batch: 260; loss: 0.78; acc: 0.81
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.7; acc: 0.86
Batch: 340; loss: 0.84; acc: 0.81
Batch: 360; loss: 0.85; acc: 0.75
Batch: 380; loss: 0.66; acc: 0.89
Batch: 400; loss: 0.79; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.84
Batch: 440; loss: 0.78; acc: 0.83
Batch: 460; loss: 0.82; acc: 0.78
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.77; acc: 0.83
Batch: 540; loss: 0.79; acc: 0.83
Batch: 560; loss: 0.99; acc: 0.72
Batch: 580; loss: 0.77; acc: 0.83
Batch: 600; loss: 0.86; acc: 0.77
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.92
Batch: 660; loss: 0.77; acc: 0.81
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.88
Batch: 720; loss: 0.77; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.86
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00018035020912066102
0.0001739770668791607
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.92
Val Epoch over. val_loss: 0.7155551618071878; val_accuracy: 0.8373805732484076 

The current subspace-distance is: 0.0001739770668791607 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.74; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.78; acc: 0.86
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.87; acc: 0.77
Batch: 260; loss: 0.81; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.88
Batch: 300; loss: 0.84; acc: 0.67
Batch: 320; loss: 0.8; acc: 0.8
Batch: 340; loss: 0.67; acc: 0.88
Batch: 360; loss: 0.65; acc: 0.89
Batch: 380; loss: 0.77; acc: 0.78
Batch: 400; loss: 0.61; acc: 0.92
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.8
Batch: 480; loss: 0.8; acc: 0.83
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 0.68; acc: 0.88
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.69; acc: 0.89
Batch: 620; loss: 0.74; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.88
Batch: 660; loss: 0.78; acc: 0.81
Batch: 680; loss: 0.79; acc: 0.8
Batch: 700; loss: 0.92; acc: 0.78
Batch: 720; loss: 0.77; acc: 0.81
Batch: 740; loss: 0.73; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.86
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.82 

0.0001852336572483182
0.00017848469724413007
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.7030613629301642; val_accuracy: 0.8401671974522293 

The current subspace-distance is: 0.00017848469724413007 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.75; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.88
Batch: 200; loss: 0.88; acc: 0.81
Batch: 220; loss: 0.84; acc: 0.83
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.85; acc: 0.8
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.62; acc: 0.89
Batch: 320; loss: 0.82; acc: 0.8
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.78; acc: 0.73
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.75; acc: 0.78
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.71; acc: 0.88
Batch: 480; loss: 0.8; acc: 0.8
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.84
Batch: 580; loss: 0.74; acc: 0.83
Batch: 600; loss: 0.87; acc: 0.8
Batch: 620; loss: 0.83; acc: 0.8
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.84
Batch: 700; loss: 0.6; acc: 0.92
Batch: 720; loss: 0.68; acc: 0.91
Batch: 740; loss: 0.82; acc: 0.83
Batch: 760; loss: 0.91; acc: 0.73
Batch: 780; loss: 0.8; acc: 0.77
Train Epoch over. train_loss: 0.76; train_accuracy: 0.82 

0.00018542452016845345
0.00018041253497358412
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6982086103433257; val_accuracy: 0.8428542993630573 

The current subspace-distance is: 0.00018041253497358412 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.61; acc: 0.95
Batch: 60; loss: 0.95; acc: 0.73
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.66
Batch: 140; loss: 0.58; acc: 0.92
Batch: 160; loss: 0.71; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.91
Batch: 200; loss: 0.78; acc: 0.8
Batch: 220; loss: 0.73; acc: 0.78
Batch: 240; loss: 0.78; acc: 0.81
Batch: 260; loss: 0.8; acc: 0.81
Batch: 280; loss: 0.73; acc: 0.88
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.92; acc: 0.7
Batch: 360; loss: 0.81; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.8
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.85; acc: 0.77
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.77; acc: 0.75
Batch: 500; loss: 0.67; acc: 0.88
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.84
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.88
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.6; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.91; acc: 0.73
Batch: 740; loss: 0.86; acc: 0.75
Batch: 760; loss: 0.77; acc: 0.88
Batch: 780; loss: 0.9; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00019235711079090834
0.0001861008204286918
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6844136128380041; val_accuracy: 0.8455414012738853 

The current subspace-distance is: 0.0001861008204286918 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.84
Batch: 40; loss: 0.82; acc: 0.75
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.65; acc: 0.89
Batch: 100; loss: 0.83; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.8; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.79; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.92
Batch: 220; loss: 0.75; acc: 0.73
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.83; acc: 0.77
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.93; acc: 0.77
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.87; acc: 0.7
Batch: 420; loss: 0.8; acc: 0.75
Batch: 440; loss: 0.66; acc: 0.86
Batch: 460; loss: 0.79; acc: 0.77
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.93; acc: 0.75
Batch: 540; loss: 0.71; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.78; acc: 0.83
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.98; acc: 0.73
Batch: 660; loss: 0.79; acc: 0.81
Batch: 680; loss: 0.83; acc: 0.73
Batch: 700; loss: 0.74; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.76; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.000190828533959575
0.00018409470794722438
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6856706776436726; val_accuracy: 0.8430533439490446 

The current subspace-distance is: 0.00018409470794722438 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.95
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.73; acc: 0.84
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.89
Batch: 240; loss: 0.71; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.73
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.84; acc: 0.81
Batch: 360; loss: 0.82; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.88
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.88; acc: 0.77
Batch: 480; loss: 0.96; acc: 0.7
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.79; acc: 0.84
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.82; acc: 0.75
Batch: 580; loss: 0.75; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.8; acc: 0.8
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.73; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.8; acc: 0.78
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.00019494486332405359
0.00018811346672009677
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6836485371088526; val_accuracy: 0.8454418789808917 

The current subspace-distance is: 0.00018811346672009677 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.94
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 1.0; acc: 0.67
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.78
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.78; acc: 0.8
Batch: 200; loss: 0.74; acc: 0.83
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.83; acc: 0.8
Batch: 260; loss: 0.68; acc: 0.81
Batch: 280; loss: 0.77; acc: 0.84
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.83; acc: 0.81
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.63; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.93; acc: 0.72
Batch: 420; loss: 0.62; acc: 0.92
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.72; acc: 0.84
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.82; acc: 0.8
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.52; acc: 0.95
Batch: 600; loss: 0.88; acc: 0.73
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.92
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.85; acc: 0.8
Batch: 760; loss: 0.75; acc: 0.81
Batch: 780; loss: 0.9; acc: 0.69
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.0001979528897209093
0.00019037061429116875
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.6752187057285551; val_accuracy: 0.8442476114649682 

The current subspace-distance is: 0.00019037061429116875 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.84
Batch: 100; loss: 0.82; acc: 0.78
Batch: 120; loss: 0.81; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.78; acc: 0.78
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.89
Batch: 280; loss: 0.76; acc: 0.83
Batch: 300; loss: 0.7; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.94; acc: 0.72
Batch: 400; loss: 0.77; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.87; acc: 0.78
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.83
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.84
Batch: 600; loss: 0.79; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.91
Batch: 640; loss: 0.72; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.89
Batch: 720; loss: 0.68; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.74; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.0002009443414863199
0.00019443774363026023
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.92; acc: 0.69
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6680257840521017; val_accuracy: 0.8452428343949044 

The current subspace-distance is: 0.00019443774363026023 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.75
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.8; acc: 0.78
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.72; acc: 0.77
Batch: 280; loss: 0.85; acc: 0.75
Batch: 300; loss: 0.65; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.73; acc: 0.8
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.83
Batch: 460; loss: 0.83; acc: 0.78
Batch: 480; loss: 0.75; acc: 0.75
Batch: 500; loss: 0.89; acc: 0.8
Batch: 520; loss: 0.77; acc: 0.84
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.88; acc: 0.73
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.93; acc: 0.7
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 0.77; acc: 0.78
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00020270230015739799
0.00019686613813973963
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6598538363435466; val_accuracy: 0.8482285031847133 

The current subspace-distance is: 0.00019686613813973963 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.81
Batch: 40; loss: 0.61; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.86
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.67; acc: 0.86
Batch: 180; loss: 0.81; acc: 0.73
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.9; acc: 0.77
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.76; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.89
Batch: 360; loss: 0.65; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.83
Batch: 400; loss: 0.87; acc: 0.78
Batch: 420; loss: 0.51; acc: 0.94
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.71; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.92
Batch: 500; loss: 0.87; acc: 0.73
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.83; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.88
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.84
Batch: 720; loss: 0.83; acc: 0.75
Batch: 740; loss: 0.83; acc: 0.77
Batch: 760; loss: 0.78; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.77
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00020250891975592822
0.0001960431836778298
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.94; acc: 0.69
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6648074521380625; val_accuracy: 0.8486265923566879 

The current subspace-distance is: 0.0001960431836778298 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.64; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.91
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.71; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.73; acc: 0.84
Batch: 300; loss: 0.63; acc: 0.84
Batch: 320; loss: 0.71; acc: 0.91
Batch: 340; loss: 0.73; acc: 0.75
Batch: 360; loss: 0.6; acc: 0.86
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.73; acc: 0.84
Batch: 420; loss: 0.81; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.78
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.88
Batch: 500; loss: 0.69; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.88
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.88
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.91; acc: 0.77
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.89; acc: 0.78
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.76; acc: 0.73
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.88
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.85; acc: 0.77
Batch: 780; loss: 0.71; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.000202926792553626
0.00019766575132962316
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.92; acc: 0.67
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.5; acc: 0.91
Val Epoch over. val_loss: 0.6587284180768735; val_accuracy: 0.8446457006369427 

The current subspace-distance is: 0.00019766575132962316 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.84
Batch: 160; loss: 0.72; acc: 0.84
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.82; acc: 0.77
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.91
Batch: 340; loss: 0.79; acc: 0.8
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.89
Batch: 400; loss: 0.73; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.89; acc: 0.78
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.75
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.81; acc: 0.77
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.62; acc: 0.92
Batch: 740; loss: 0.76; acc: 0.84
Batch: 760; loss: 0.91; acc: 0.77
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00020451738964766264
0.0001969247532542795
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6504830228295296; val_accuracy: 0.8471337579617835 

The current subspace-distance is: 0.0001969247532542795 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.67; acc: 0.88
Batch: 160; loss: 0.79; acc: 0.73
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.97; acc: 0.67
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.79; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.84; acc: 0.81
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.79; acc: 0.81
Batch: 380; loss: 0.67; acc: 0.86
Batch: 400; loss: 0.78; acc: 0.77
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.68; acc: 0.88
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.8; acc: 0.81
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.98; acc: 0.69
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 0.53; acc: 0.95
Batch: 620; loss: 0.9; acc: 0.77
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.7; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.0002037256199400872
0.00019618726219050586
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.94; acc: 0.67
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.92
Val Epoch over. val_loss: 0.6572620684553863; val_accuracy: 0.8477308917197452 

The current subspace-distance is: 0.00019618726219050586 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.75
Batch: 20; loss: 0.75; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.91
Batch: 160; loss: 0.84; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.83
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.76; acc: 0.8
Batch: 240; loss: 0.55; acc: 0.92
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.8; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.78
Batch: 400; loss: 0.85; acc: 0.8
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.7; acc: 0.8
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.84; acc: 0.77
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.86
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002080599806504324
0.00020052320905961096
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.49; acc: 0.92
Val Epoch over. val_loss: 0.6528652866554868; val_accuracy: 0.849422770700637 

The current subspace-distance is: 0.00020052320905961096 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.76; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.76; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.77
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.75; acc: 0.77
Batch: 220; loss: 0.77; acc: 0.83
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.7; acc: 0.84
Batch: 280; loss: 0.82; acc: 0.8
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 0.53; acc: 0.94
Batch: 360; loss: 0.83; acc: 0.8
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.91
Batch: 460; loss: 0.83; acc: 0.8
Batch: 480; loss: 0.62; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.77
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.75; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.8
Batch: 660; loss: 0.69; acc: 0.8
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.81
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.91
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002084653970086947
0.00020099234825465828
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.67
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.91
Val Epoch over. val_loss: 0.653720615016427; val_accuracy: 0.8484275477707006 

The current subspace-distance is: 0.00020099234825465828 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.77
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.77
Batch: 280; loss: 0.78; acc: 0.75
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.91
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.93; acc: 0.75
Batch: 440; loss: 0.85; acc: 0.72
Batch: 460; loss: 0.81; acc: 0.8
Batch: 480; loss: 0.55; acc: 0.92
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.83
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.68; acc: 0.91
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.8
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.73; acc: 0.8
Batch: 700; loss: 0.67; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.83
Batch: 740; loss: 0.81; acc: 0.77
Batch: 760; loss: 0.87; acc: 0.83
Batch: 780; loss: 0.81; acc: 0.83
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002067610912490636
0.00019866961520165205
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.89; acc: 0.69
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.95
Val Epoch over. val_loss: 0.6400601577227283; val_accuracy: 0.8507165605095541 

The current subspace-distance is: 0.00019866961520165205 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.91
Batch: 160; loss: 0.67; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.75; acc: 0.81
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.89
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.84; acc: 0.81
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.83
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.91
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.93; acc: 0.7
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.8
Batch: 640; loss: 0.74; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.75
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.89
Batch: 740; loss: 0.76; acc: 0.77
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.77; acc: 0.88
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002092218928737566
0.00020223083265591413
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.92; acc: 0.69
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.91
Val Epoch over. val_loss: 0.6573749021360069; val_accuracy: 0.8450437898089171 

The current subspace-distance is: 0.00020223083265591413 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.7; acc: 0.86
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.78
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.76; acc: 0.78
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.8; acc: 0.8
Batch: 340; loss: 0.8; acc: 0.83
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.66; acc: 0.86
Batch: 400; loss: 0.8; acc: 0.78
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.92
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.76; acc: 0.77
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.65; acc: 0.81
Batch: 660; loss: 0.63; acc: 0.89
Batch: 680; loss: 0.83; acc: 0.75
Batch: 700; loss: 0.54; acc: 0.94
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.8
Batch: 760; loss: 0.58; acc: 0.89
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002072492934530601
0.00019954344315920025
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.91; acc: 0.69
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6405599010977775; val_accuracy: 0.8498208598726115 

The current subspace-distance is: 0.00019954344315920025 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.87; acc: 0.73
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.91
Batch: 320; loss: 0.59; acc: 0.92
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.71; acc: 0.83
Batch: 380; loss: 0.76; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.92
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.91; acc: 0.67
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.73; acc: 0.86
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.66; acc: 0.86
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.9; acc: 0.7
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.89; acc: 0.75
Batch: 760; loss: 0.73; acc: 0.84
Batch: 780; loss: 0.88; acc: 0.73
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00020859479263890535
0.00020192834199406207
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.92; acc: 0.67
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6465381985637033; val_accuracy: 0.8482285031847133 

The current subspace-distance is: 0.00020192834199406207 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_4_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.170581494228353

The number of parameters is: 276580

The number of individual parameters is:

34
544
34
34
51
48552
51
51
101
144228
101
101
64
77568
64
64
4096
64
640
10
64
64

nonzero elements in E: 82973994
elements in E: 82974000
fraction nonzero: 0.9999999276881915
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.08
Batch: 20; loss: 2.2; acc: 0.2
Batch: 40; loss: 2.0; acc: 0.34
Batch: 60; loss: 1.88; acc: 0.45
Batch: 80; loss: 1.85; acc: 0.42
Batch: 100; loss: 1.74; acc: 0.61
Batch: 120; loss: 1.72; acc: 0.52
Batch: 140; loss: 1.62; acc: 0.61
Batch: 160; loss: 1.66; acc: 0.59
Batch: 180; loss: 1.63; acc: 0.56
Batch: 200; loss: 1.48; acc: 0.62
Batch: 220; loss: 1.64; acc: 0.64
Batch: 240; loss: 1.54; acc: 0.58
Batch: 260; loss: 1.5; acc: 0.73
Batch: 280; loss: 1.56; acc: 0.56
Batch: 300; loss: 1.5; acc: 0.67
Batch: 320; loss: 1.41; acc: 0.7
Batch: 340; loss: 1.37; acc: 0.69
Batch: 360; loss: 1.46; acc: 0.69
Batch: 380; loss: 1.37; acc: 0.73
Batch: 400; loss: 1.38; acc: 0.72
Batch: 420; loss: 1.35; acc: 0.7
Batch: 440; loss: 1.26; acc: 0.77
Batch: 460; loss: 1.29; acc: 0.75
Batch: 480; loss: 1.32; acc: 0.69
Batch: 500; loss: 1.29; acc: 0.72
Batch: 520; loss: 1.33; acc: 0.64
Batch: 540; loss: 1.26; acc: 0.73
Batch: 560; loss: 1.3; acc: 0.66
Batch: 580; loss: 1.25; acc: 0.72
Batch: 600; loss: 1.28; acc: 0.66
Batch: 620; loss: 1.21; acc: 0.73
Batch: 640; loss: 1.22; acc: 0.77
Batch: 660; loss: 1.13; acc: 0.8
Batch: 680; loss: 1.27; acc: 0.7
Batch: 700; loss: 1.2; acc: 0.77
Batch: 720; loss: 1.23; acc: 0.75
Batch: 740; loss: 1.17; acc: 0.75
Batch: 760; loss: 1.27; acc: 0.67
Batch: 780; loss: 1.22; acc: 0.7
Train Epoch over. train_loss: 1.45; train_accuracy: 0.64 

6.703573308186606e-05
6.234682950889692e-05
Batch: 0; loss: 1.21; acc: 0.73
Batch: 20; loss: 1.37; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.86
Batch: 60; loss: 1.01; acc: 0.83
Batch: 80; loss: 1.03; acc: 0.81
Batch: 100; loss: 1.18; acc: 0.77
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 1.12; acc: 0.73
Val Epoch over. val_loss: 1.14394339444531; val_accuracy: 0.7514928343949044 

The current subspace-distance is: 6.234682950889692e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.81
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 1.01; acc: 0.83
Batch: 60; loss: 1.27; acc: 0.73
Batch: 80; loss: 1.08; acc: 0.81
Batch: 100; loss: 1.03; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.81
Batch: 140; loss: 1.18; acc: 0.7
Batch: 160; loss: 1.09; acc: 0.8
Batch: 180; loss: 1.17; acc: 0.72
Batch: 200; loss: 0.93; acc: 0.84
Batch: 220; loss: 1.11; acc: 0.7
Batch: 240; loss: 1.31; acc: 0.59
Batch: 260; loss: 1.05; acc: 0.83
Batch: 280; loss: 1.16; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.72
Batch: 320; loss: 1.11; acc: 0.73
Batch: 340; loss: 1.11; acc: 0.72
Batch: 360; loss: 1.25; acc: 0.67
Batch: 380; loss: 1.15; acc: 0.73
Batch: 400; loss: 1.29; acc: 0.62
Batch: 420; loss: 1.09; acc: 0.78
Batch: 440; loss: 1.04; acc: 0.81
Batch: 460; loss: 1.2; acc: 0.67
Batch: 480; loss: 1.02; acc: 0.75
Batch: 500; loss: 1.03; acc: 0.8
Batch: 520; loss: 1.16; acc: 0.69
Batch: 540; loss: 1.15; acc: 0.69
Batch: 560; loss: 1.1; acc: 0.75
Batch: 580; loss: 1.08; acc: 0.72
Batch: 600; loss: 1.06; acc: 0.73
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.05; acc: 0.77
Batch: 660; loss: 0.94; acc: 0.8
Batch: 680; loss: 0.97; acc: 0.78
Batch: 700; loss: 1.0; acc: 0.8
Batch: 720; loss: 1.15; acc: 0.72
Batch: 740; loss: 1.04; acc: 0.78
Batch: 760; loss: 0.99; acc: 0.81
Batch: 780; loss: 0.75; acc: 0.91
Train Epoch over. train_loss: 1.06; train_accuracy: 0.77 

8.887202420737594e-05
8.411942690145224e-05
Batch: 0; loss: 0.98; acc: 0.83
Batch: 20; loss: 1.2; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.86
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 0.86; acc: 0.83
Val Epoch over. val_loss: 0.920868385749258; val_accuracy: 0.8082205414012739 

The current subspace-distance is: 8.411942690145224e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.8
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 0.93; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.78
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.81; acc: 0.84
Batch: 160; loss: 0.9; acc: 0.83
Batch: 180; loss: 0.97; acc: 0.75
Batch: 200; loss: 0.98; acc: 0.86
Batch: 220; loss: 0.86; acc: 0.81
Batch: 240; loss: 0.89; acc: 0.84
Batch: 260; loss: 0.79; acc: 0.84
Batch: 280; loss: 0.88; acc: 0.78
Batch: 300; loss: 0.9; acc: 0.81
Batch: 320; loss: 0.83; acc: 0.78
Batch: 340; loss: 0.95; acc: 0.78
Batch: 360; loss: 0.97; acc: 0.73
Batch: 380; loss: 0.83; acc: 0.86
Batch: 400; loss: 0.83; acc: 0.84
Batch: 420; loss: 0.9; acc: 0.84
Batch: 440; loss: 0.85; acc: 0.83
Batch: 460; loss: 0.9; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.86
Batch: 500; loss: 0.93; acc: 0.77
Batch: 520; loss: 0.85; acc: 0.83
Batch: 540; loss: 0.89; acc: 0.81
Batch: 560; loss: 0.92; acc: 0.86
Batch: 580; loss: 0.84; acc: 0.86
Batch: 600; loss: 1.08; acc: 0.69
Batch: 620; loss: 0.81; acc: 0.88
Batch: 640; loss: 0.85; acc: 0.83
Batch: 660; loss: 0.85; acc: 0.84
Batch: 680; loss: 0.85; acc: 0.83
Batch: 700; loss: 0.88; acc: 0.81
Batch: 720; loss: 0.82; acc: 0.83
Batch: 740; loss: 0.73; acc: 0.88
Batch: 760; loss: 0.93; acc: 0.8
Batch: 780; loss: 0.96; acc: 0.8
Train Epoch over. train_loss: 0.9; train_accuracy: 0.81 

0.00010870895494008437
0.00010417204612167552
Batch: 0; loss: 0.84; acc: 0.89
Batch: 20; loss: 1.06; acc: 0.69
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.91
Batch: 100; loss: 0.82; acc: 0.84
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.7906187002066594; val_accuracy: 0.8485270700636943 

The current subspace-distance is: 0.00010417204612167552 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.86
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.91
Batch: 160; loss: 0.86; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.83
Batch: 200; loss: 0.85; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.89
Batch: 240; loss: 0.74; acc: 0.91
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.73; acc: 0.91
Batch: 300; loss: 0.82; acc: 0.86
Batch: 320; loss: 0.82; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.68; acc: 0.92
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 0.82; acc: 0.77
Batch: 420; loss: 0.76; acc: 0.86
Batch: 440; loss: 0.81; acc: 0.81
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.92
Batch: 500; loss: 0.66; acc: 0.89
Batch: 520; loss: 0.9; acc: 0.8
Batch: 540; loss: 0.66; acc: 0.91
Batch: 560; loss: 0.72; acc: 0.86
Batch: 580; loss: 0.83; acc: 0.81
Batch: 600; loss: 0.65; acc: 0.91
Batch: 620; loss: 0.84; acc: 0.81
Batch: 640; loss: 0.75; acc: 0.83
Batch: 660; loss: 0.7; acc: 0.88
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.71; acc: 0.88
Batch: 720; loss: 0.76; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.95
Train Epoch over. train_loss: 0.78; train_accuracy: 0.84 

0.000128782179672271
0.0001232046924997121
Batch: 0; loss: 0.69; acc: 0.94
Batch: 20; loss: 0.94; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.94
Val Epoch over. val_loss: 0.6850588006578433; val_accuracy: 0.8659434713375797 

The current subspace-distance is: 0.0001232046924997121 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.86
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.84; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.65; acc: 0.92
Batch: 160; loss: 0.65; acc: 0.89
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.84; acc: 0.77
Batch: 220; loss: 0.78; acc: 0.84
Batch: 240; loss: 0.79; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.84; acc: 0.78
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.9; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.89
Batch: 400; loss: 0.73; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.88
Batch: 500; loss: 0.74; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.94
Batch: 540; loss: 0.71; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.91
Batch: 580; loss: 0.7; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.84; acc: 0.84
Batch: 640; loss: 0.63; acc: 0.92
Batch: 660; loss: 0.67; acc: 0.91
Batch: 680; loss: 0.72; acc: 0.84
Batch: 700; loss: 0.7; acc: 0.91
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.63; acc: 0.84
Batch: 780; loss: 0.69; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.85 

0.00014229836233425885
0.00013529231364373118
Batch: 0; loss: 0.63; acc: 0.97
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.92
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.94
Val Epoch over. val_loss: 0.6404357210845705; val_accuracy: 0.8706210191082803 

The current subspace-distance is: 0.00013529231364373118 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.92
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.95
Batch: 160; loss: 0.62; acc: 0.94
Batch: 180; loss: 0.68; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.92
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.76; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.58; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.94
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.91
Batch: 600; loss: 0.66; acc: 0.88
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.89
Batch: 720; loss: 0.7; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.91
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.67; train_accuracy: 0.86 

0.00015436136163771152
0.00014781607023905963
Batch: 0; loss: 0.59; acc: 0.95
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.6018035971814659; val_accuracy: 0.8751990445859873 

The current subspace-distance is: 0.00014781607023905963 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.92
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.94
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.91
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.66; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.91
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.6; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.94
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.61; acc: 0.91
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.64; train_accuracy: 0.86 

0.00016520034114364535
0.00016083900118246675
Batch: 0; loss: 0.57; acc: 0.94
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.36; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.39; acc: 0.97
Val Epoch over. val_loss: 0.577992707490921; val_accuracy: 0.8753980891719745 

The current subspace-distance is: 0.00016083900118246675 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.97
Batch: 240; loss: 0.65; acc: 0.89
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.91
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.72; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.86
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.8
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.74; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.92
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.88
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.61; train_accuracy: 0.86 

0.00017498157103545964
0.00016902077186387032
Batch: 0; loss: 0.53; acc: 0.97
Batch: 20; loss: 0.77; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.98
Val Epoch over. val_loss: 0.5371839495221521; val_accuracy: 0.881468949044586 

The current subspace-distance is: 0.00016902077186387032 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.97
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.84
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.8
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.81
Batch: 780; loss: 0.71; acc: 0.81
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.0001867168175522238
0.0001798665034584701
Batch: 0; loss: 0.52; acc: 0.94
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.3; acc: 1.0
Val Epoch over. val_loss: 0.5204345671234617; val_accuracy: 0.8838574840764332 

The current subspace-distance is: 0.0001798665034584701 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.64; acc: 0.89
Batch: 420; loss: 0.73; acc: 0.8
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.78
Batch: 620; loss: 0.65; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00019543484086170793
0.00019061406783293933
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.28; acc: 1.0
Val Epoch over. val_loss: 0.4989794289610188; val_accuracy: 0.8858479299363057 

The current subspace-distance is: 0.00019061406783293933 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.97
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.92
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.69; acc: 0.78
Batch: 380; loss: 0.66; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.74; acc: 0.77
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00019802985480055213
0.00019253655045758933
Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.48577717876738047; val_accuracy: 0.8885350318471338 

The current subspace-distance is: 0.00019253655045758933 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.8; acc: 0.78
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.48; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.0002008231240324676
0.000193708052393049
Batch: 0; loss: 0.48; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.48417051108020126; val_accuracy: 0.8892316878980892 

The current subspace-distance is: 0.000193708052393049 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.89
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.56; acc: 0.81
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00020507961744442582
0.00019959236669819802
Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.474997776139314; val_accuracy: 0.8889331210191083 

The current subspace-distance is: 0.00019959236669819802 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.48; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.95
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.98
Batch: 140; loss: 0.41; acc: 0.95
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.73; acc: 0.77
Batch: 420; loss: 0.61; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.45; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.8
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.53; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.95
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020732484699692577
0.00020028486324008554
Batch: 0; loss: 0.47; acc: 0.95
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.4743659211571809; val_accuracy: 0.8897292993630573 

The current subspace-distance is: 0.00020028486324008554 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.71; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.94
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.8
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.65; acc: 0.91
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.43; acc: 0.95
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00021052664669696242
0.0002030936739174649
Batch: 0; loss: 0.46; acc: 0.95
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.467964801059407; val_accuracy: 0.8911226114649682 

The current subspace-distance is: 0.0002030936739174649 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.95
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.98
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.0002125697792507708
0.00020564586156979203
Batch: 0; loss: 0.43; acc: 0.98
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.4557567810177044; val_accuracy: 0.8966958598726115 

The current subspace-distance is: 0.00020564586156979203 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.65; acc: 0.78
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.8
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00021549302618950605
0.00020981792476959527
Batch: 0; loss: 0.43; acc: 0.98
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45810965633696055; val_accuracy: 0.8949044585987261 

The current subspace-distance is: 0.00020981792476959527 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.95
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.63; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.65; acc: 0.8
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00021790793107356876
0.00020983850117772818
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.44403848887249164; val_accuracy: 0.8946058917197452 

The current subspace-distance is: 0.00020983850117772818 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.71; acc: 0.78
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.69; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.92
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.77
Batch: 540; loss: 0.32; acc: 0.95
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00021904353343416005
0.00021222977375146002
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.58; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.4409635773130283; val_accuracy: 0.8976910828025477 

The current subspace-distance is: 0.00021222977375146002 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.85; acc: 0.78
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.95
Batch: 160; loss: 0.57; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0002227475051768124
0.00021704706887248904
Batch: 0; loss: 0.4; acc: 0.97
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.43151267224056705; val_accuracy: 0.8968949044585988 

The current subspace-distance is: 0.00021704706887248904 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.45; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00022006250219419599
0.00021346875291783363
Batch: 0; loss: 0.4; acc: 0.98
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.43470590794162384; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 0.00021346875291783363 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.65; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.95
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.97
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022379084839485586
0.00021575189020950347
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.4280953961572829; val_accuracy: 0.8973925159235668 

The current subspace-distance is: 0.00021575189020950347 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.58; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022442886256612837
0.00021921041479799896
Batch: 0; loss: 0.38; acc: 0.98
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.4268659829713736; val_accuracy: 0.9002786624203821 

The current subspace-distance is: 0.00021921041479799896 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022552322479896247
0.0002177932910853997
Batch: 0; loss: 0.37; acc: 0.98
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4231078113150445; val_accuracy: 0.9006767515923567 

The current subspace-distance is: 0.0002177932910853997 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.95
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.97
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022701705165673047
0.00021857595129404217
Batch: 0; loss: 0.39; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.4269248132303262; val_accuracy: 0.9006767515923567 

The current subspace-distance is: 0.00021857595129404217 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.72; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.77
Batch: 320; loss: 0.72; acc: 0.73
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022486838861368597
0.00021755002671852708
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.4259563311459912; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.00021755002671852708 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.77
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.0002279706095578149
0.00022137182531878352
Batch: 0; loss: 0.38; acc: 0.98
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.42201617122835416; val_accuracy: 0.9002786624203821 

The current subspace-distance is: 0.00022137182531878352 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00022634911874774843
0.0002197150606662035
Batch: 0; loss: 0.38; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.42076019363798156; val_accuracy: 0.9015724522292994 

The current subspace-distance is: 0.0002197150606662035 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.94
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.000227729047765024
0.00022181382519192994
Batch: 0; loss: 0.38; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.4202634417896817; val_accuracy: 0.9004777070063694 

The current subspace-distance is: 0.00022181382519192994 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.48; acc: 0.81
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.95
Batch: 660; loss: 0.65; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.8; acc: 0.77
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00022894711582921445
0.0002236245636595413
Batch: 0; loss: 0.37; acc: 0.98
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.4158464636013007; val_accuracy: 0.9023686305732485 

The current subspace-distance is: 0.0002236245636595413 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_4_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.170581494228353

The number of parameters is: 276580

The number of individual parameters is:

34
544
34
34
51
48552
51
51
101
144228
101
101
64
77568
64
64
4096
64
640
10
64
64

nonzero elements in E: 110631990
elements in E: 110632000
fraction nonzero: 0.9999999096102393
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.11
Batch: 20; loss: 2.03; acc: 0.34
Batch: 40; loss: 1.9; acc: 0.55
Batch: 60; loss: 1.83; acc: 0.45
Batch: 80; loss: 1.81; acc: 0.41
Batch: 100; loss: 1.54; acc: 0.69
Batch: 120; loss: 1.65; acc: 0.61
Batch: 140; loss: 1.44; acc: 0.73
Batch: 160; loss: 1.39; acc: 0.75
Batch: 180; loss: 1.42; acc: 0.61
Batch: 200; loss: 1.32; acc: 0.75
Batch: 220; loss: 1.41; acc: 0.72
Batch: 240; loss: 1.38; acc: 0.67
Batch: 260; loss: 1.38; acc: 0.75
Batch: 280; loss: 1.33; acc: 0.73
Batch: 300; loss: 1.35; acc: 0.7
Batch: 320; loss: 1.37; acc: 0.69
Batch: 340; loss: 1.33; acc: 0.75
Batch: 360; loss: 1.17; acc: 0.84
Batch: 380; loss: 1.19; acc: 0.81
Batch: 400; loss: 1.2; acc: 0.8
Batch: 420; loss: 1.13; acc: 0.88
Batch: 440; loss: 1.21; acc: 0.81
Batch: 460; loss: 1.17; acc: 0.83
Batch: 480; loss: 1.1; acc: 0.83
Batch: 500; loss: 1.2; acc: 0.73
Batch: 520; loss: 1.17; acc: 0.83
Batch: 540; loss: 1.14; acc: 0.78
Batch: 560; loss: 1.13; acc: 0.78
Batch: 580; loss: 1.08; acc: 0.81
Batch: 600; loss: 1.0; acc: 0.84
Batch: 620; loss: 1.1; acc: 0.83
Batch: 640; loss: 1.07; acc: 0.78
Batch: 660; loss: 1.14; acc: 0.81
Batch: 680; loss: 1.09; acc: 0.81
Batch: 700; loss: 1.04; acc: 0.81
Batch: 720; loss: 1.13; acc: 0.81
Batch: 740; loss: 0.98; acc: 0.81
Batch: 760; loss: 1.07; acc: 0.8
Batch: 780; loss: 1.04; acc: 0.86
Train Epoch over. train_loss: 1.3; train_accuracy: 0.73 

2.661710459506139e-05
8.804632670944557e-06
Batch: 0; loss: 1.09; acc: 0.78
Batch: 20; loss: 1.1; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.94
Batch: 60; loss: 0.96; acc: 0.8
Batch: 80; loss: 0.9; acc: 0.91
Batch: 100; loss: 0.94; acc: 0.91
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 0.85; acc: 0.86
Val Epoch over. val_loss: 0.970611483428129; val_accuracy: 0.8376791401273885 

The current subspace-distance is: 8.804632670944557e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 1.09; acc: 0.75
Batch: 40; loss: 1.11; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.84
Batch: 80; loss: 0.93; acc: 0.88
Batch: 100; loss: 0.99; acc: 0.84
Batch: 120; loss: 0.99; acc: 0.86
Batch: 140; loss: 0.96; acc: 0.81
Batch: 160; loss: 1.03; acc: 0.83
Batch: 180; loss: 0.95; acc: 0.81
Batch: 200; loss: 0.97; acc: 0.84
Batch: 220; loss: 0.98; acc: 0.81
Batch: 240; loss: 0.9; acc: 0.84
Batch: 260; loss: 1.06; acc: 0.78
Batch: 280; loss: 0.9; acc: 0.88
Batch: 300; loss: 1.02; acc: 0.78
Batch: 320; loss: 1.0; acc: 0.78
Batch: 340; loss: 0.94; acc: 0.83
Batch: 360; loss: 0.89; acc: 0.84
Batch: 380; loss: 0.95; acc: 0.86
Batch: 400; loss: 0.82; acc: 0.92
Batch: 420; loss: 1.02; acc: 0.8
Batch: 440; loss: 0.89; acc: 0.83
Batch: 460; loss: 0.91; acc: 0.84
Batch: 480; loss: 0.9; acc: 0.84
Batch: 500; loss: 0.97; acc: 0.81
Batch: 520; loss: 0.82; acc: 0.92
Batch: 540; loss: 1.0; acc: 0.84
Batch: 560; loss: 1.02; acc: 0.81
Batch: 580; loss: 0.91; acc: 0.8
Batch: 600; loss: 0.93; acc: 0.83
Batch: 620; loss: 0.92; acc: 0.8
Batch: 640; loss: 0.93; acc: 0.77
Batch: 660; loss: 0.89; acc: 0.84
Batch: 680; loss: 0.8; acc: 0.84
Batch: 700; loss: 1.03; acc: 0.75
Batch: 720; loss: 0.93; acc: 0.83
Batch: 740; loss: 0.84; acc: 0.86
Batch: 760; loss: 1.0; acc: 0.8
Batch: 780; loss: 0.93; acc: 0.81
Train Epoch over. train_loss: 0.94; train_accuracy: 0.83 

3.098568777204491e-05
1.1434939551691059e-05
Batch: 0; loss: 0.91; acc: 0.84
Batch: 20; loss: 0.96; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.86
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.91
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 0.67; acc: 0.91
Val Epoch over. val_loss: 0.8100817989391885; val_accuracy: 0.8660429936305732 

The current subspace-distance is: 1.1434939551691059e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.83
Batch: 40; loss: 0.74; acc: 0.94
Batch: 60; loss: 1.02; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.83
Batch: 140; loss: 0.76; acc: 0.91
Batch: 160; loss: 0.71; acc: 0.86
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.86; acc: 0.78
Batch: 220; loss: 0.99; acc: 0.77
Batch: 240; loss: 0.96; acc: 0.77
Batch: 260; loss: 0.87; acc: 0.81
Batch: 280; loss: 0.81; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.92
Batch: 320; loss: 0.84; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.89
Batch: 360; loss: 0.81; acc: 0.84
Batch: 380; loss: 0.86; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.86
Batch: 420; loss: 0.75; acc: 0.91
Batch: 440; loss: 0.83; acc: 0.83
Batch: 460; loss: 0.73; acc: 0.92
Batch: 480; loss: 0.75; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.84
Batch: 520; loss: 0.83; acc: 0.86
Batch: 540; loss: 0.81; acc: 0.83
Batch: 560; loss: 0.88; acc: 0.84
Batch: 580; loss: 0.83; acc: 0.84
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.84; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.92
Batch: 660; loss: 0.85; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.86
Batch: 700; loss: 0.78; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.92
Batch: 740; loss: 0.8; acc: 0.86
Batch: 760; loss: 0.73; acc: 0.86
Batch: 780; loss: 0.78; acc: 0.89
Train Epoch over. train_loss: 0.81; train_accuracy: 0.85 

3.5294211556902155e-05
1.4533314242726192e-05
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.81
Batch: 40; loss: 0.49; acc: 0.98
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.75; acc: 0.91
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.92
Val Epoch over. val_loss: 0.7023753214413953; val_accuracy: 0.8781847133757962 

The current subspace-distance is: 1.4533314242726192e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.84
Batch: 100; loss: 0.81; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.85; acc: 0.81
Batch: 160; loss: 0.76; acc: 0.86
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.77; acc: 0.92
Batch: 220; loss: 0.8; acc: 0.84
Batch: 240; loss: 0.65; acc: 0.91
Batch: 260; loss: 0.9; acc: 0.8
Batch: 280; loss: 0.56; acc: 0.94
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.74; acc: 0.86
Batch: 340; loss: 0.68; acc: 0.94
Batch: 360; loss: 0.77; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.83
Batch: 400; loss: 0.7; acc: 0.88
Batch: 420; loss: 0.8; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.92
Batch: 460; loss: 0.81; acc: 0.81
Batch: 480; loss: 0.64; acc: 0.91
Batch: 500; loss: 0.71; acc: 0.88
Batch: 520; loss: 0.62; acc: 0.91
Batch: 540; loss: 0.78; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.69; acc: 0.88
Batch: 620; loss: 0.67; acc: 0.86
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.89
Batch: 680; loss: 0.62; acc: 0.92
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.88
Batch: 760; loss: 0.64; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.91
Train Epoch over. train_loss: 0.71; train_accuracy: 0.86 

3.923125404980965e-05
1.6104057067423128e-05
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.91
Batch: 120; loss: 0.87; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.61564574120151; val_accuracy: 0.8863455414012739 

The current subspace-distance is: 1.6104057067423128e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.95
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.94
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.94
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.56; acc: 0.92
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.76; acc: 0.75
Batch: 280; loss: 0.62; acc: 0.89
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.83
Batch: 340; loss: 0.69; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.58; acc: 0.94
Batch: 460; loss: 0.77; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.66; acc: 0.91
Batch: 660; loss: 0.62; acc: 0.89
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.62; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.71; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.87 

4.19023672293406e-05
1.816917210817337e-05
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.5588433484362948; val_accuracy: 0.8922173566878981 

The current subspace-distance is: 1.816917210817337e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.91
Batch: 160; loss: 0.62; acc: 0.86
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.92
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.92
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.49; acc: 0.94
Batch: 460; loss: 0.66; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.59; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.62; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.6; train_accuracy: 0.87 

4.517744673648849e-05
2.0218996723997407e-05
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.5237480584223559; val_accuracy: 0.8941082802547771 

The current subspace-distance is: 2.0218996723997407e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.94
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.95
Batch: 760; loss: 0.52; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.56; train_accuracy: 0.88 

4.803915726370178e-05
2.0795443560928106e-05
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.49413989408380665; val_accuracy: 0.8954020700636943 

The current subspace-distance is: 2.0795443560928106e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.6; acc: 0.88
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

5.070539191365242e-05
2.3239605070557445e-05
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4613104231987789; val_accuracy: 0.9006767515923567 

The current subspace-distance is: 2.3239605070557445e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.94
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.78
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

5.298583346302621e-05
2.3684353436692618e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4438468678172227; val_accuracy: 0.8998805732484076 

The current subspace-distance is: 2.3684353436692618e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

5.4968983022263274e-05
2.4010911147342995e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4183940844740837; val_accuracy: 0.903562898089172 

The current subspace-distance is: 2.4010911147342995e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

5.639877053909004e-05
2.4147901058313437e-05
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.41337627125014165; val_accuracy: 0.9080414012738853 

The current subspace-distance is: 2.4147901058313437e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

5.706668889615685e-05
2.7192290872335434e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.40833020020442407; val_accuracy: 0.9084394904458599 

The current subspace-distance is: 2.7192290872335434e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.97
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.89 

5.6686603784328327e-05
2.4634895453345962e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.40534019043111497; val_accuracy: 0.908140923566879 

The current subspace-distance is: 2.4634895453345962e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.64; acc: 0.77
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.7; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.89 

5.786084147985093e-05
2.5908519091899507e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.396967763069329; val_accuracy: 0.9078423566878981 

The current subspace-distance is: 2.5908519091899507e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.97
Train Epoch over. train_loss: 0.44; train_accuracy: 0.89 

5.830720692756586e-05
2.700397999433335e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.3970423439505753; val_accuracy: 0.9098328025477707 

The current subspace-distance is: 2.700397999433335e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.97
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.49; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.94
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.44; train_accuracy: 0.89 

5.896156653761864e-05
2.7250827770330943e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.39399675075795243; val_accuracy: 0.9091361464968153 

The current subspace-distance is: 2.7250827770330943e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.29; acc: 0.97
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.81
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.48; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.95
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.89 

5.9025740483775735e-05
2.523371222196147e-05
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3877888961582427; val_accuracy: 0.9093351910828026 

The current subspace-distance is: 2.523371222196147e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.8
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.052402386558242e-05
2.7254805900156498e-05
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3839709374365533; val_accuracy: 0.9115246815286624 

The current subspace-distance is: 2.7254805900156498e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.97
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.97
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.97
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.027211929904297e-05
2.6198003979516216e-05
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3823683763005931; val_accuracy: 0.9134156050955414 

The current subspace-distance is: 2.6198003979516216e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.78
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.0096961533417925e-05
2.6374247681815177e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.367490635271285; val_accuracy: 0.9147093949044586 

The current subspace-distance is: 2.6374247681815177e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.8
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.17752957623452e-05
2.821600719471462e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3723206378661903; val_accuracy: 0.9134156050955414 

The current subspace-distance is: 2.821600719471462e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.97
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.13278680248186e-05
2.7918435080209747e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.37029859205340127; val_accuracy: 0.9147093949044586 

The current subspace-distance is: 2.7918435080209747e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.78
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.208974082255736e-05
2.711924935283605e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.3730348579253361; val_accuracy: 0.912718949044586 

The current subspace-distance is: 2.711924935283605e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.163276702864096e-05
2.817398853949271e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.36530340970701475; val_accuracy: 0.9134156050955414 

The current subspace-distance is: 2.817398853949271e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.232521263882518e-05
2.8803378882003017e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3704175344508165; val_accuracy: 0.9118232484076433 

The current subspace-distance is: 2.8803378882003017e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.127018423285335e-05
2.7126556233270094e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.36510286921528495; val_accuracy: 0.9141122611464968 

The current subspace-distance is: 2.7126556233270094e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.44; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.221398507477716e-05
2.8288546673138626e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3674856774556409; val_accuracy: 0.9132165605095541 

The current subspace-distance is: 2.8288546673138626e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.199066410772502e-05
2.7849862817674875e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3656333493199318; val_accuracy: 0.9131170382165605 

The current subspace-distance is: 2.7849862817674875e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.81
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.7; acc: 0.77
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.58; acc: 0.78
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.333937199087813e-05
2.8719990950776264e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.3620634319106485; val_accuracy: 0.9164012738853503 

The current subspace-distance is: 2.8719990950776264e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.97
Batch: 500; loss: 0.43; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.19124184595421e-05
2.864396446966566e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.35955170888429994; val_accuracy: 0.9169984076433121 

The current subspace-distance is: 2.864396446966566e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_4_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 4.170581494228353

The number of parameters is: 276580

The number of individual parameters is:

34
544
34
34
51
48552
51
51
101
144228
101
101
64
77568
64
64
4096
64
640
10
64
64

nonzero elements in E: 138289989
elements in E: 138290000
fraction nonzero: 0.9999999204570106
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.14
Batch: 20; loss: 1.94; acc: 0.38
Batch: 40; loss: 1.71; acc: 0.58
Batch: 60; loss: 1.75; acc: 0.47
Batch: 80; loss: 1.67; acc: 0.5
Batch: 100; loss: 1.59; acc: 0.64
Batch: 120; loss: 1.49; acc: 0.67
Batch: 140; loss: 1.4; acc: 0.64
Batch: 160; loss: 1.55; acc: 0.59
Batch: 180; loss: 1.38; acc: 0.77
Batch: 200; loss: 1.23; acc: 0.81
Batch: 220; loss: 1.33; acc: 0.69
Batch: 240; loss: 1.27; acc: 0.69
Batch: 260; loss: 1.14; acc: 0.81
Batch: 280; loss: 1.23; acc: 0.8
Batch: 300; loss: 1.12; acc: 0.84
Batch: 320; loss: 1.13; acc: 0.81
Batch: 340; loss: 1.13; acc: 0.77
Batch: 360; loss: 1.06; acc: 0.83
Batch: 380; loss: 1.21; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.92
Batch: 420; loss: 0.98; acc: 0.84
Batch: 440; loss: 1.14; acc: 0.75
Batch: 460; loss: 1.07; acc: 0.75
Batch: 480; loss: 1.06; acc: 0.8
Batch: 500; loss: 0.88; acc: 0.91
Batch: 520; loss: 1.08; acc: 0.77
Batch: 540; loss: 1.11; acc: 0.77
Batch: 560; loss: 0.98; acc: 0.84
Batch: 580; loss: 0.96; acc: 0.81
Batch: 600; loss: 1.02; acc: 0.75
Batch: 620; loss: 1.06; acc: 0.81
Batch: 640; loss: 1.05; acc: 0.72
Batch: 660; loss: 0.96; acc: 0.86
Batch: 680; loss: 0.9; acc: 0.86
Batch: 700; loss: 0.73; acc: 0.95
Batch: 720; loss: 1.03; acc: 0.8
Batch: 740; loss: 1.04; acc: 0.75
Batch: 760; loss: 0.98; acc: 0.78
Batch: 780; loss: 0.98; acc: 0.84
Train Epoch over. train_loss: 1.2; train_accuracy: 0.74 

2.696191950235516e-05
9.21290757105453e-06
Batch: 0; loss: 0.94; acc: 0.89
Batch: 20; loss: 1.01; acc: 0.78
Batch: 40; loss: 0.61; acc: 0.94
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.89
Batch: 100; loss: 0.89; acc: 0.88
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 0.68; acc: 0.89
Val Epoch over. val_loss: 0.8599001127443496; val_accuracy: 0.8450437898089171 

The current subspace-distance is: 9.21290757105453e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.92
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 0.89; acc: 0.86
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.83
Batch: 100; loss: 0.9; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.88
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.8; acc: 0.89
Batch: 180; loss: 0.87; acc: 0.83
Batch: 200; loss: 0.89; acc: 0.78
Batch: 220; loss: 0.78; acc: 0.86
Batch: 240; loss: 0.9; acc: 0.78
Batch: 260; loss: 0.91; acc: 0.78
Batch: 280; loss: 0.8; acc: 0.89
Batch: 300; loss: 0.88; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.88
Batch: 340; loss: 0.73; acc: 0.92
Batch: 360; loss: 0.92; acc: 0.83
Batch: 380; loss: 0.8; acc: 0.86
Batch: 400; loss: 0.8; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.84
Batch: 440; loss: 0.77; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.97
Batch: 480; loss: 0.78; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.78; acc: 0.8
Batch: 560; loss: 0.77; acc: 0.89
Batch: 580; loss: 0.68; acc: 0.89
Batch: 600; loss: 0.75; acc: 0.84
Batch: 620; loss: 0.9; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.95
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.86
Batch: 720; loss: 0.84; acc: 0.78
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.69; acc: 0.86
Train Epoch over. train_loss: 0.8; train_accuracy: 0.85 

3.26600311382208e-05
1.3064170161669608e-05
Batch: 0; loss: 0.77; acc: 0.86
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.94
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.92; acc: 0.7
Batch: 140; loss: 0.5; acc: 0.89
Val Epoch over. val_loss: 0.661278861343481; val_accuracy: 0.878781847133758 

The current subspace-distance is: 1.3064170161669608e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.94
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.91
Batch: 140; loss: 0.69; acc: 0.88
Batch: 160; loss: 0.84; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.89
Batch: 220; loss: 0.76; acc: 0.86
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.65; acc: 0.91
Batch: 280; loss: 0.58; acc: 0.92
Batch: 300; loss: 0.79; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.88
Batch: 360; loss: 0.65; acc: 0.91
Batch: 380; loss: 0.65; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.92
Batch: 420; loss: 0.58; acc: 0.91
Batch: 440; loss: 0.68; acc: 0.89
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.95
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.91
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.88
Batch: 680; loss: 0.62; acc: 0.92
Batch: 700; loss: 0.64; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.65; train_accuracy: 0.87 

3.763464337680489e-05
1.562689612910617e-05
Batch: 0; loss: 0.61; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.92
Val Epoch over. val_loss: 0.5425122351783096; val_accuracy: 0.8954020700636943 

The current subspace-distance is: 1.562689612910617e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.64; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.77; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.92
Batch: 280; loss: 0.55; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.4; acc: 0.97
Batch: 500; loss: 0.48; acc: 0.94
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.97
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.94
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.94
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.56; train_accuracy: 0.88 

4.156657450948842e-05
1.9588740542531013e-05
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.98
Val Epoch over. val_loss: 0.4749439279934403; val_accuracy: 0.9084394904458599 

The current subspace-distance is: 1.9588740542531013e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.6; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.97
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.6; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.92
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.98
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

4.448381150723435e-05
2.0157769540674053e-05
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.43209982867453506; val_accuracy: 0.9121218152866242 

The current subspace-distance is: 2.0157769540674053e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.94
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.53; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.61; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

4.669375266530551e-05
2.0974188373656943e-05
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4071623589013033; val_accuracy: 0.9145103503184714 

The current subspace-distance is: 2.0974188373656943e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.017421062802896e-05
2.4389646569034085e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.22; acc: 1.0
Val Epoch over. val_loss: 0.37542801744239346; val_accuracy: 0.9179936305732485 

The current subspace-distance is: 2.4389646569034085e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.97
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.052248889114708e-05
2.25929579755757e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.21; acc: 1.0
Val Epoch over. val_loss: 0.3594276864247717; val_accuracy: 0.9215764331210191 

The current subspace-distance is: 2.25929579755757e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.97
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.4362506489269435e-05
2.5988134439103305e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.34011408961882256; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.5988134439103305e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.98
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.97
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.519762999028899e-05
2.4925588149926625e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.32108248627869185; val_accuracy: 0.9260549363057324 

The current subspace-distance is: 2.4925588149926625e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.97
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.692426202585921e-05
2.631233110150788e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.15; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3170390260067715; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.631233110150788e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.83
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.799996870337054e-05
2.7481028155307285e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3146767930430212; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.7481028155307285e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.7951128837885335e-05
2.538038643251639e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3123796174575569; val_accuracy: 0.9278463375796179 

The current subspace-distance is: 2.538038643251639e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.8
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.98
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.8164696383755654e-05
2.5984778403653763e-05
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3089672793533392; val_accuracy: 0.9287420382165605 

The current subspace-distance is: 2.5984778403653763e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

5.886601502425037e-05
2.6543983040028252e-05
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.14; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.30246705742208824; val_accuracy: 0.9297372611464968 

The current subspace-distance is: 2.6543983040028252e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.9218091337243095e-05
2.6303463528165594e-05
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3027362133951704; val_accuracy: 0.9290406050955414 

The current subspace-distance is: 2.6303463528165594e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.98
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

6.017497071297839e-05
2.8491675038821995e-05
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.296257406995175; val_accuracy: 0.9298367834394905 

The current subspace-distance is: 2.8491675038821995e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.97
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.087239307817072e-05
2.8292399292695336e-05
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.29472539441031254; val_accuracy: 0.9304339171974523 

The current subspace-distance is: 2.8292399292695336e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.159822805784643e-05
2.8203117835801095e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.29241393202809013; val_accuracy: 0.9313296178343949 

The current subspace-distance is: 2.8203117835801095e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.97
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.18; acc: 1.0
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.130861584097147e-05
2.762748408713378e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2894043230991455; val_accuracy: 0.9306329617834395 

The current subspace-distance is: 2.762748408713378e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.97
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.164476508274674e-05
2.8509675757959485e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.28830185081738574; val_accuracy: 0.9307324840764332 

The current subspace-distance is: 2.8509675757959485e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.62; acc: 0.77
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.98
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.183421646710485e-05
2.7038344342145137e-05
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.28380451878164986; val_accuracy: 0.9319267515923567 

The current subspace-distance is: 2.7038344342145137e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.8
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.24; acc: 0.98
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.98
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.20243008597754e-05
2.8233231205376796e-05
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.28430320920458263; val_accuracy: 0.931827229299363 

The current subspace-distance is: 2.8233231205376796e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.98
Batch: 240; loss: 0.24; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.169229163788259e-05
2.8131700673839077e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.13; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.2863341660541334; val_accuracy: 0.9311305732484076 

The current subspace-distance is: 2.8131700673839077e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.093911360949278e-05
2.6941983378492296e-05
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.28241606797002683; val_accuracy: 0.9323248407643312 

The current subspace-distance is: 2.6941983378492296e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.197502079885453e-05
2.838721229636576e-05
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2833473374889155; val_accuracy: 0.9321257961783439 

The current subspace-distance is: 2.838721229636576e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.167872197693214e-05
2.808226599881891e-05
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2787637647929465; val_accuracy: 0.9333200636942676 

The current subspace-distance is: 2.808226599881891e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.298506195889786e-05
3.136517261737026e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.27917261986405983; val_accuracy: 0.933718152866242 

The current subspace-distance is: 3.136517261737026e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.22952647972852e-05
2.7346131901140325e-05
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.28136612304077024; val_accuracy: 0.9313296178343949 

The current subspace-distance is: 2.7346131901140325e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.2; acc: 1.0
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.258481153054163e-05
2.8078175091650337e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.274646269098209; val_accuracy: 0.9327229299363057 

The current subspace-distance is: 2.8078175091650337e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_4_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_4_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
