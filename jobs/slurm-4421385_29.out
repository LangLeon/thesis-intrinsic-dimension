model : table13slim
N : 13
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 16:00:01

Channel scaling factor: 1.11

The number of parameters is: 272274

The number of individual parameters is:

9
162
9
9
14
33012
14
14
27
99036
27
27
64
134784
64
64
4096
64
640
10
64
64

nonzero elements in E: 13613698
elements in E: 13613700
fraction nonzero: 0.9999998530891675
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.41; acc: 0.11
Batch: 20; loss: 2.43; acc: 0.03
Batch: 40; loss: 2.27; acc: 0.12
Batch: 60; loss: 2.42; acc: 0.09
Batch: 80; loss: 2.26; acc: 0.14
Batch: 100; loss: 2.28; acc: 0.06
Batch: 120; loss: 2.11; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.3
Batch: 160; loss: 2.2; acc: 0.2
Batch: 180; loss: 2.23; acc: 0.17
Batch: 200; loss: 2.27; acc: 0.11
Batch: 220; loss: 2.18; acc: 0.22
Batch: 240; loss: 2.14; acc: 0.25
Batch: 260; loss: 2.14; acc: 0.25
Batch: 280; loss: 2.12; acc: 0.28
Batch: 300; loss: 2.0; acc: 0.41
Batch: 320; loss: 1.91; acc: 0.44
Batch: 340; loss: 2.0; acc: 0.33
Batch: 360; loss: 2.0; acc: 0.33
Batch: 380; loss: 2.13; acc: 0.28
Batch: 400; loss: 2.06; acc: 0.27
Batch: 420; loss: 2.0; acc: 0.3
Batch: 440; loss: 1.94; acc: 0.31
Batch: 460; loss: 1.9; acc: 0.41
Batch: 480; loss: 1.95; acc: 0.38
Batch: 500; loss: 2.05; acc: 0.23
Batch: 520; loss: 1.99; acc: 0.33
Batch: 540; loss: 1.95; acc: 0.33
Batch: 560; loss: 1.97; acc: 0.38
Batch: 580; loss: 1.99; acc: 0.38
Batch: 600; loss: 1.89; acc: 0.45
Batch: 620; loss: 1.97; acc: 0.3
Batch: 640; loss: 1.9; acc: 0.45
Batch: 660; loss: 1.95; acc: 0.38
Batch: 680; loss: 1.92; acc: 0.39
Batch: 700; loss: 1.89; acc: 0.42
Batch: 720; loss: 1.89; acc: 0.38
Batch: 740; loss: 1.87; acc: 0.36
Batch: 760; loss: 2.0; acc: 0.33
Batch: 780; loss: 1.84; acc: 0.47
Train Epoch over. train_loss: 2.06; train_accuracy: 0.3 

2.3960288672242314e-05
4.789081685885321e-06
Batch: 0; loss: 1.9; acc: 0.39
Batch: 20; loss: 2.03; acc: 0.36
Batch: 40; loss: 1.75; acc: 0.53
Batch: 60; loss: 1.87; acc: 0.44
Batch: 80; loss: 1.82; acc: 0.41
Batch: 100; loss: 1.88; acc: 0.39
Batch: 120; loss: 1.91; acc: 0.42
Batch: 140; loss: 1.8; acc: 0.53
Val Epoch over. val_loss: 1.8732586066434338; val_accuracy: 0.42147691082802546 

The current subspace-distance is: 4.789081685885321e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.87; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.33
Batch: 40; loss: 1.9; acc: 0.39
Batch: 60; loss: 1.89; acc: 0.41
Batch: 80; loss: 1.81; acc: 0.45
Batch: 100; loss: 1.8; acc: 0.44
Batch: 120; loss: 1.88; acc: 0.52
Batch: 140; loss: 1.87; acc: 0.45
Batch: 160; loss: 1.97; acc: 0.33
Batch: 180; loss: 1.88; acc: 0.39
Batch: 200; loss: 1.8; acc: 0.45
Batch: 220; loss: 1.84; acc: 0.47
Batch: 240; loss: 2.01; acc: 0.33
Batch: 260; loss: 1.77; acc: 0.45
Batch: 280; loss: 1.79; acc: 0.53
Batch: 300; loss: 1.84; acc: 0.36
Batch: 320; loss: 1.94; acc: 0.41
Batch: 340; loss: 1.96; acc: 0.36
Batch: 360; loss: 1.85; acc: 0.44
Batch: 380; loss: 1.88; acc: 0.39
Batch: 400; loss: 1.87; acc: 0.38
Batch: 420; loss: 1.72; acc: 0.5
Batch: 440; loss: 2.06; acc: 0.27
Batch: 460; loss: 1.92; acc: 0.33
Batch: 480; loss: 1.83; acc: 0.45
Batch: 500; loss: 1.79; acc: 0.45
Batch: 520; loss: 1.83; acc: 0.42
Batch: 540; loss: 1.86; acc: 0.39
Batch: 560; loss: 1.82; acc: 0.47
Batch: 580; loss: 1.95; acc: 0.28
Batch: 600; loss: 1.78; acc: 0.47
Batch: 620; loss: 1.84; acc: 0.5
Batch: 640; loss: 1.85; acc: 0.48
Batch: 660; loss: 1.73; acc: 0.5
Batch: 680; loss: 1.74; acc: 0.5
Batch: 700; loss: 1.7; acc: 0.55
Batch: 720; loss: 1.78; acc: 0.45
Batch: 740; loss: 1.85; acc: 0.39
Batch: 760; loss: 1.82; acc: 0.38
Batch: 780; loss: 1.77; acc: 0.53
Train Epoch over. train_loss: 1.87; train_accuracy: 0.42 

2.619978113216348e-05
5.631642125081271e-06
Batch: 0; loss: 1.82; acc: 0.41
Batch: 20; loss: 1.95; acc: 0.31
Batch: 40; loss: 1.71; acc: 0.53
Batch: 60; loss: 1.78; acc: 0.48
Batch: 80; loss: 1.81; acc: 0.45
Batch: 100; loss: 1.8; acc: 0.47
Batch: 120; loss: 1.87; acc: 0.5
Batch: 140; loss: 1.75; acc: 0.55
Val Epoch over. val_loss: 1.8125773227898179; val_accuracy: 0.45710589171974525 

The current subspace-distance is: 5.631642125081271e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.88; acc: 0.38
Batch: 20; loss: 1.88; acc: 0.47
Batch: 40; loss: 1.85; acc: 0.44
Batch: 60; loss: 1.82; acc: 0.48
Batch: 80; loss: 1.8; acc: 0.45
Batch: 100; loss: 1.85; acc: 0.34
Batch: 120; loss: 1.8; acc: 0.42
Batch: 140; loss: 1.81; acc: 0.44
Batch: 160; loss: 1.91; acc: 0.36
Batch: 180; loss: 1.8; acc: 0.44
Batch: 200; loss: 1.86; acc: 0.33
Batch: 220; loss: 1.82; acc: 0.52
Batch: 240; loss: 1.84; acc: 0.39
Batch: 260; loss: 1.93; acc: 0.33
Batch: 280; loss: 1.78; acc: 0.47
Batch: 300; loss: 1.92; acc: 0.41
Batch: 320; loss: 1.71; acc: 0.56
Batch: 340; loss: 1.89; acc: 0.41
Batch: 360; loss: 1.78; acc: 0.52
Batch: 380; loss: 1.87; acc: 0.39
Batch: 400; loss: 1.87; acc: 0.38
Batch: 420; loss: 1.78; acc: 0.48
Batch: 440; loss: 1.88; acc: 0.41
Batch: 460; loss: 1.8; acc: 0.45
Batch: 480; loss: 1.77; acc: 0.56
Batch: 500; loss: 1.77; acc: 0.44
Batch: 520; loss: 1.9; acc: 0.38
Batch: 540; loss: 1.79; acc: 0.45
Batch: 560; loss: 1.78; acc: 0.45
Batch: 580; loss: 1.83; acc: 0.47
Batch: 600; loss: 1.8; acc: 0.45
Batch: 620; loss: 1.69; acc: 0.5
Batch: 640; loss: 1.82; acc: 0.52
Batch: 660; loss: 1.76; acc: 0.44
Batch: 680; loss: 1.74; acc: 0.47
Batch: 700; loss: 1.75; acc: 0.41
Batch: 720; loss: 1.73; acc: 0.53
Batch: 740; loss: 1.88; acc: 0.33
Batch: 760; loss: 1.83; acc: 0.41
Batch: 780; loss: 1.84; acc: 0.36
Train Epoch over. train_loss: 1.82; train_accuracy: 0.45 

2.9096350772306323e-05
7.303511210920988e-06
Batch: 0; loss: 1.75; acc: 0.5
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.67; acc: 0.56
Batch: 60; loss: 1.71; acc: 0.58
Batch: 80; loss: 1.76; acc: 0.56
Batch: 100; loss: 1.71; acc: 0.53
Batch: 120; loss: 1.8; acc: 0.48
Batch: 140; loss: 1.68; acc: 0.59
Val Epoch over. val_loss: 1.760456480038394; val_accuracy: 0.48546974522292996 

The current subspace-distance is: 7.303511210920988e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.83; acc: 0.5
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.79; acc: 0.36
Batch: 60; loss: 1.74; acc: 0.52
Batch: 80; loss: 1.7; acc: 0.53
Batch: 100; loss: 1.77; acc: 0.44
Batch: 120; loss: 1.87; acc: 0.47
Batch: 140; loss: 1.8; acc: 0.48
Batch: 160; loss: 1.77; acc: 0.5
Batch: 180; loss: 1.62; acc: 0.62
Batch: 200; loss: 1.95; acc: 0.34
Batch: 220; loss: 1.86; acc: 0.45
Batch: 240; loss: 1.69; acc: 0.56
Batch: 260; loss: 1.74; acc: 0.52
Batch: 280; loss: 1.67; acc: 0.45
Batch: 300; loss: 1.7; acc: 0.47
Batch: 320; loss: 1.76; acc: 0.53
Batch: 340; loss: 1.81; acc: 0.44
Batch: 360; loss: 1.8; acc: 0.44
Batch: 380; loss: 1.8; acc: 0.39
Batch: 400; loss: 1.74; acc: 0.45
Batch: 420; loss: 1.74; acc: 0.47
Batch: 440; loss: 1.61; acc: 0.62
Batch: 460; loss: 1.74; acc: 0.47
Batch: 480; loss: 1.73; acc: 0.53
Batch: 500; loss: 1.82; acc: 0.38
Batch: 520; loss: 1.76; acc: 0.38
Batch: 540; loss: 1.85; acc: 0.48
Batch: 560; loss: 1.74; acc: 0.5
Batch: 580; loss: 1.84; acc: 0.45
Batch: 600; loss: 1.83; acc: 0.41
Batch: 620; loss: 1.83; acc: 0.44
Batch: 640; loss: 1.69; acc: 0.5
Batch: 660; loss: 1.81; acc: 0.5
Batch: 680; loss: 1.86; acc: 0.38
Batch: 700; loss: 1.78; acc: 0.45
Batch: 720; loss: 1.67; acc: 0.58
Batch: 740; loss: 1.69; acc: 0.48
Batch: 760; loss: 1.73; acc: 0.45
Batch: 780; loss: 1.66; acc: 0.62
Train Epoch over. train_loss: 1.75; train_accuracy: 0.47 

3.0337998396134935e-05
7.459665084752487e-06
Batch: 0; loss: 1.68; acc: 0.5
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.59; acc: 0.64
Batch: 60; loss: 1.62; acc: 0.58
Batch: 80; loss: 1.65; acc: 0.59
Batch: 100; loss: 1.63; acc: 0.5
Batch: 120; loss: 1.72; acc: 0.52
Batch: 140; loss: 1.61; acc: 0.58
Val Epoch over. val_loss: 1.6800250407237156; val_accuracy: 0.5251791401273885 

The current subspace-distance is: 7.459665084752487e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.56
Batch: 20; loss: 1.75; acc: 0.47
Batch: 40; loss: 1.83; acc: 0.38
Batch: 60; loss: 1.77; acc: 0.41
Batch: 80; loss: 1.79; acc: 0.41
Batch: 100; loss: 1.77; acc: 0.42
Batch: 120; loss: 1.75; acc: 0.47
Batch: 140; loss: 1.77; acc: 0.41
Batch: 160; loss: 1.67; acc: 0.5
Batch: 180; loss: 1.77; acc: 0.48
Batch: 200; loss: 1.89; acc: 0.42
Batch: 220; loss: 1.71; acc: 0.52
Batch: 240; loss: 1.82; acc: 0.44
Batch: 260; loss: 1.68; acc: 0.56
Batch: 280; loss: 1.6; acc: 0.59
Batch: 300; loss: 1.75; acc: 0.42
Batch: 320; loss: 1.75; acc: 0.48
Batch: 340; loss: 1.73; acc: 0.48
Batch: 360; loss: 1.68; acc: 0.52
Batch: 380; loss: 1.74; acc: 0.5
Batch: 400; loss: 1.69; acc: 0.53
Batch: 420; loss: 1.63; acc: 0.56
Batch: 440; loss: 1.66; acc: 0.52
Batch: 460; loss: 1.75; acc: 0.42
Batch: 480; loss: 1.62; acc: 0.53
Batch: 500; loss: 1.62; acc: 0.58
Batch: 520; loss: 1.68; acc: 0.47
Batch: 540; loss: 1.71; acc: 0.48
Batch: 560; loss: 1.8; acc: 0.44
Batch: 580; loss: 1.59; acc: 0.58
Batch: 600; loss: 1.6; acc: 0.52
Batch: 620; loss: 1.59; acc: 0.59
Batch: 640; loss: 1.7; acc: 0.52
Batch: 660; loss: 1.72; acc: 0.45
Batch: 680; loss: 1.63; acc: 0.53
Batch: 700; loss: 1.77; acc: 0.47
Batch: 720; loss: 1.7; acc: 0.55
Batch: 740; loss: 1.65; acc: 0.5
Batch: 760; loss: 1.69; acc: 0.48
Batch: 780; loss: 1.7; acc: 0.5
Train Epoch over. train_loss: 1.69; train_accuracy: 0.51 

3.364480289747007e-05
1.205225180456182e-05
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.76; acc: 0.41
Batch: 40; loss: 1.53; acc: 0.62
Batch: 60; loss: 1.56; acc: 0.64
Batch: 80; loss: 1.59; acc: 0.58
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.67; acc: 0.55
Batch: 140; loss: 1.57; acc: 0.58
Val Epoch over. val_loss: 1.634244821633503; val_accuracy: 0.5507563694267515 

The current subspace-distance is: 1.205225180456182e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.71; acc: 0.53
Batch: 20; loss: 1.76; acc: 0.42
Batch: 40; loss: 1.73; acc: 0.44
Batch: 60; loss: 1.78; acc: 0.41
Batch: 80; loss: 1.68; acc: 0.55
Batch: 100; loss: 1.81; acc: 0.39
Batch: 120; loss: 1.67; acc: 0.56
Batch: 140; loss: 1.67; acc: 0.5
Batch: 160; loss: 1.66; acc: 0.53
Batch: 180; loss: 1.74; acc: 0.55
Batch: 200; loss: 1.65; acc: 0.53
Batch: 220; loss: 1.69; acc: 0.45
Batch: 240; loss: 1.6; acc: 0.5
Batch: 260; loss: 1.71; acc: 0.47
Batch: 280; loss: 1.56; acc: 0.61
Batch: 300; loss: 1.61; acc: 0.52
Batch: 320; loss: 1.62; acc: 0.48
Batch: 340; loss: 1.72; acc: 0.48
Batch: 360; loss: 1.72; acc: 0.48
Batch: 380; loss: 1.62; acc: 0.61
Batch: 400; loss: 1.71; acc: 0.56
Batch: 420; loss: 1.69; acc: 0.44
Batch: 440; loss: 1.67; acc: 0.53
Batch: 460; loss: 1.55; acc: 0.58
Batch: 480; loss: 1.61; acc: 0.58
Batch: 500; loss: 1.71; acc: 0.52
Batch: 520; loss: 1.65; acc: 0.56
Batch: 540; loss: 1.68; acc: 0.5
Batch: 560; loss: 1.66; acc: 0.5
Batch: 580; loss: 1.65; acc: 0.55
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.6; acc: 0.56
Batch: 640; loss: 1.76; acc: 0.53
Batch: 660; loss: 1.55; acc: 0.64
Batch: 680; loss: 1.59; acc: 0.59
Batch: 700; loss: 1.6; acc: 0.53
Batch: 720; loss: 1.54; acc: 0.59
Batch: 740; loss: 1.7; acc: 0.5
Batch: 760; loss: 1.55; acc: 0.56
Batch: 780; loss: 1.73; acc: 0.5
Train Epoch over. train_loss: 1.66; train_accuracy: 0.52 

3.5499822843121365e-05
9.722267350298353e-06
Batch: 0; loss: 1.65; acc: 0.55
Batch: 20; loss: 1.73; acc: 0.44
Batch: 40; loss: 1.51; acc: 0.66
Batch: 60; loss: 1.53; acc: 0.64
Batch: 80; loss: 1.56; acc: 0.61
Batch: 100; loss: 1.6; acc: 0.55
Batch: 120; loss: 1.65; acc: 0.55
Batch: 140; loss: 1.55; acc: 0.53
Val Epoch over. val_loss: 1.6110033859872515; val_accuracy: 0.5656847133757962 

The current subspace-distance is: 9.722267350298353e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.51; acc: 0.64
Batch: 20; loss: 1.61; acc: 0.55
Batch: 40; loss: 1.65; acc: 0.53
Batch: 60; loss: 1.82; acc: 0.44
Batch: 80; loss: 1.74; acc: 0.44
Batch: 100; loss: 1.63; acc: 0.56
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.61
Batch: 180; loss: 1.74; acc: 0.52
Batch: 200; loss: 1.53; acc: 0.58
Batch: 220; loss: 1.64; acc: 0.52
Batch: 240; loss: 1.59; acc: 0.62
Batch: 260; loss: 1.68; acc: 0.47
Batch: 280; loss: 1.77; acc: 0.45
Batch: 300; loss: 1.58; acc: 0.62
Batch: 320; loss: 1.66; acc: 0.52
Batch: 340; loss: 1.54; acc: 0.55
Batch: 360; loss: 1.8; acc: 0.41
Batch: 380; loss: 1.63; acc: 0.48
Batch: 400; loss: 1.64; acc: 0.55
Batch: 420; loss: 1.71; acc: 0.45
Batch: 440; loss: 1.73; acc: 0.48
Batch: 460; loss: 1.72; acc: 0.47
Batch: 480; loss: 1.72; acc: 0.48
Batch: 500; loss: 1.6; acc: 0.58
Batch: 520; loss: 1.58; acc: 0.56
Batch: 540; loss: 1.66; acc: 0.59
Batch: 560; loss: 1.6; acc: 0.66
Batch: 580; loss: 1.81; acc: 0.42
Batch: 600; loss: 1.68; acc: 0.5
Batch: 620; loss: 1.7; acc: 0.47
Batch: 640; loss: 1.74; acc: 0.45
Batch: 660; loss: 1.69; acc: 0.56
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.53; acc: 0.61
Batch: 720; loss: 1.58; acc: 0.56
Batch: 740; loss: 1.76; acc: 0.47
Batch: 760; loss: 1.75; acc: 0.42
Batch: 780; loss: 1.59; acc: 0.59
Train Epoch over. train_loss: 1.65; train_accuracy: 0.53 

3.7355745007516816e-05
1.210544905916322e-05
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.72; acc: 0.45
Batch: 40; loss: 1.52; acc: 0.66
Batch: 60; loss: 1.53; acc: 0.66
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.63; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.54; acc: 0.55
Val Epoch over. val_loss: 1.6084415616503187; val_accuracy: 0.5583200636942676 

The current subspace-distance is: 1.210544905916322e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.62; acc: 0.52
Batch: 40; loss: 1.59; acc: 0.56
Batch: 60; loss: 1.75; acc: 0.42
Batch: 80; loss: 1.68; acc: 0.53
Batch: 100; loss: 1.6; acc: 0.5
Batch: 120; loss: 1.57; acc: 0.53
Batch: 140; loss: 1.54; acc: 0.61
Batch: 160; loss: 1.72; acc: 0.45
Batch: 180; loss: 1.7; acc: 0.42
Batch: 200; loss: 1.69; acc: 0.45
Batch: 220; loss: 1.61; acc: 0.5
Batch: 240; loss: 1.61; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.59
Batch: 280; loss: 1.56; acc: 0.52
Batch: 300; loss: 1.63; acc: 0.53
Batch: 320; loss: 1.74; acc: 0.42
Batch: 340; loss: 1.69; acc: 0.47
Batch: 360; loss: 1.58; acc: 0.48
Batch: 380; loss: 1.73; acc: 0.47
Batch: 400; loss: 1.64; acc: 0.53
Batch: 420; loss: 1.95; acc: 0.39
Batch: 440; loss: 1.69; acc: 0.47
Batch: 460; loss: 1.52; acc: 0.64
Batch: 480; loss: 1.69; acc: 0.52
Batch: 500; loss: 1.74; acc: 0.44
Batch: 520; loss: 1.67; acc: 0.39
Batch: 540; loss: 1.56; acc: 0.59
Batch: 560; loss: 1.6; acc: 0.55
Batch: 580; loss: 1.55; acc: 0.58
Batch: 600; loss: 1.67; acc: 0.53
Batch: 620; loss: 1.72; acc: 0.38
Batch: 640; loss: 1.81; acc: 0.45
Batch: 660; loss: 1.56; acc: 0.62
Batch: 680; loss: 1.84; acc: 0.42
Batch: 700; loss: 1.59; acc: 0.5
Batch: 720; loss: 1.67; acc: 0.47
Batch: 740; loss: 1.51; acc: 0.66
Batch: 760; loss: 1.65; acc: 0.48
Batch: 780; loss: 1.66; acc: 0.47
Train Epoch over. train_loss: 1.63; train_accuracy: 0.53 

3.852286681649275e-05
1.1323588296363596e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.69; acc: 0.44
Batch: 40; loss: 1.49; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.64
Batch: 80; loss: 1.55; acc: 0.53
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.5; acc: 0.59
Val Epoch over. val_loss: 1.592795954388418; val_accuracy: 0.5580214968152867 

The current subspace-distance is: 1.1323588296363596e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.58
Batch: 20; loss: 1.56; acc: 0.55
Batch: 40; loss: 1.61; acc: 0.59
Batch: 60; loss: 1.42; acc: 0.69
Batch: 80; loss: 1.55; acc: 0.61
Batch: 100; loss: 1.64; acc: 0.45
Batch: 120; loss: 1.66; acc: 0.47
Batch: 140; loss: 1.47; acc: 0.64
Batch: 160; loss: 1.49; acc: 0.66
Batch: 180; loss: 1.68; acc: 0.55
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.46; acc: 0.64
Batch: 240; loss: 1.66; acc: 0.5
Batch: 260; loss: 1.69; acc: 0.45
Batch: 280; loss: 1.53; acc: 0.56
Batch: 300; loss: 1.61; acc: 0.47
Batch: 320; loss: 1.46; acc: 0.61
Batch: 340; loss: 1.48; acc: 0.62
Batch: 360; loss: 1.65; acc: 0.58
Batch: 380; loss: 1.57; acc: 0.58
Batch: 400; loss: 1.66; acc: 0.44
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.7; acc: 0.45
Batch: 460; loss: 1.52; acc: 0.58
Batch: 480; loss: 1.48; acc: 0.61
Batch: 500; loss: 1.59; acc: 0.5
Batch: 520; loss: 1.61; acc: 0.53
Batch: 540; loss: 1.51; acc: 0.58
Batch: 560; loss: 1.6; acc: 0.56
Batch: 580; loss: 1.54; acc: 0.55
Batch: 600; loss: 1.62; acc: 0.48
Batch: 620; loss: 1.44; acc: 0.62
Batch: 640; loss: 1.71; acc: 0.44
Batch: 660; loss: 1.48; acc: 0.58
Batch: 680; loss: 1.52; acc: 0.64
Batch: 700; loss: 1.51; acc: 0.55
Batch: 720; loss: 1.61; acc: 0.52
Batch: 740; loss: 1.73; acc: 0.44
Batch: 760; loss: 1.66; acc: 0.48
Batch: 780; loss: 1.62; acc: 0.47
Train Epoch over. train_loss: 1.62; train_accuracy: 0.53 

4.0089726098813117e-05
1.2790278560714796e-05
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.67; acc: 0.47
Batch: 40; loss: 1.49; acc: 0.61
Batch: 60; loss: 1.51; acc: 0.61
Batch: 80; loss: 1.54; acc: 0.55
Batch: 100; loss: 1.63; acc: 0.47
Batch: 120; loss: 1.65; acc: 0.55
Batch: 140; loss: 1.47; acc: 0.61
Val Epoch over. val_loss: 1.5866819065847215; val_accuracy: 0.5498606687898089 

The current subspace-distance is: 1.2790278560714796e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.44
Batch: 20; loss: 1.58; acc: 0.52
Batch: 40; loss: 1.53; acc: 0.55
Batch: 60; loss: 1.54; acc: 0.58
Batch: 80; loss: 1.67; acc: 0.5
Batch: 100; loss: 1.57; acc: 0.55
Batch: 120; loss: 1.58; acc: 0.61
Batch: 140; loss: 1.53; acc: 0.61
Batch: 160; loss: 1.54; acc: 0.58
Batch: 180; loss: 1.58; acc: 0.58
Batch: 200; loss: 1.62; acc: 0.59
Batch: 220; loss: 1.63; acc: 0.47
Batch: 240; loss: 1.72; acc: 0.48
Batch: 260; loss: 1.67; acc: 0.5
Batch: 280; loss: 1.69; acc: 0.47
Batch: 300; loss: 1.61; acc: 0.5
Batch: 320; loss: 1.65; acc: 0.47
Batch: 340; loss: 1.51; acc: 0.55
Batch: 360; loss: 1.59; acc: 0.5
Batch: 380; loss: 1.63; acc: 0.52
Batch: 400; loss: 1.68; acc: 0.47
Batch: 420; loss: 1.78; acc: 0.45
Batch: 440; loss: 1.63; acc: 0.52
Batch: 460; loss: 1.63; acc: 0.62
Batch: 480; loss: 1.72; acc: 0.5
Batch: 500; loss: 1.72; acc: 0.39
Batch: 520; loss: 1.71; acc: 0.42
Batch: 540; loss: 1.65; acc: 0.42
Batch: 560; loss: 1.74; acc: 0.48
Batch: 580; loss: 1.63; acc: 0.52
Batch: 600; loss: 1.6; acc: 0.56
Batch: 620; loss: 1.73; acc: 0.41
Batch: 640; loss: 1.72; acc: 0.56
Batch: 660; loss: 1.67; acc: 0.45
Batch: 680; loss: 1.74; acc: 0.41
Batch: 700; loss: 1.52; acc: 0.59
Batch: 720; loss: 1.75; acc: 0.42
Batch: 740; loss: 1.6; acc: 0.47
Batch: 760; loss: 1.66; acc: 0.48
Batch: 780; loss: 1.42; acc: 0.64
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.204572178423405e-05
1.3473253602569457e-05
Batch: 0; loss: 1.71; acc: 0.47
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.44; acc: 0.64
Batch: 60; loss: 1.49; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.59
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.56
Batch: 140; loss: 1.42; acc: 0.64
Val Epoch over. val_loss: 1.5553837634955243; val_accuracy: 0.557921974522293 

The current subspace-distance is: 1.3473253602569457e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.55
Batch: 20; loss: 1.54; acc: 0.45
Batch: 40; loss: 1.53; acc: 0.58
Batch: 60; loss: 1.64; acc: 0.45
Batch: 80; loss: 1.63; acc: 0.52
Batch: 100; loss: 1.73; acc: 0.45
Batch: 120; loss: 1.56; acc: 0.58
Batch: 140; loss: 1.61; acc: 0.47
Batch: 160; loss: 1.75; acc: 0.34
Batch: 180; loss: 1.65; acc: 0.5
Batch: 200; loss: 1.56; acc: 0.52
Batch: 220; loss: 1.62; acc: 0.53
Batch: 240; loss: 1.65; acc: 0.56
Batch: 260; loss: 1.59; acc: 0.5
Batch: 280; loss: 1.7; acc: 0.45
Batch: 300; loss: 1.65; acc: 0.5
Batch: 320; loss: 1.5; acc: 0.56
Batch: 340; loss: 1.44; acc: 0.61
Batch: 360; loss: 1.5; acc: 0.59
Batch: 380; loss: 1.52; acc: 0.56
Batch: 400; loss: 1.66; acc: 0.48
Batch: 420; loss: 1.6; acc: 0.53
Batch: 440; loss: 1.61; acc: 0.52
Batch: 460; loss: 1.55; acc: 0.53
Batch: 480; loss: 1.62; acc: 0.48
Batch: 500; loss: 1.59; acc: 0.55
Batch: 520; loss: 1.59; acc: 0.53
Batch: 540; loss: 1.49; acc: 0.55
Batch: 560; loss: 1.56; acc: 0.62
Batch: 580; loss: 1.7; acc: 0.42
Batch: 600; loss: 1.47; acc: 0.59
Batch: 620; loss: 1.58; acc: 0.56
Batch: 640; loss: 1.65; acc: 0.41
Batch: 660; loss: 1.52; acc: 0.53
Batch: 680; loss: 1.55; acc: 0.5
Batch: 700; loss: 1.62; acc: 0.55
Batch: 720; loss: 1.54; acc: 0.55
Batch: 740; loss: 1.59; acc: 0.48
Batch: 760; loss: 1.83; acc: 0.38
Batch: 780; loss: 1.54; acc: 0.55
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

4.163317498750985e-05
9.46198542806087e-06
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.64; acc: 0.48
Batch: 40; loss: 1.44; acc: 0.62
Batch: 60; loss: 1.48; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.64
Batch: 100; loss: 1.61; acc: 0.5
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.41; acc: 0.66
Val Epoch over. val_loss: 1.5556227416749213; val_accuracy: 0.551453025477707 

The current subspace-distance is: 9.46198542806087e-06 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.72; acc: 0.5
Batch: 20; loss: 1.52; acc: 0.56
Batch: 40; loss: 1.75; acc: 0.44
Batch: 60; loss: 1.49; acc: 0.64
Batch: 80; loss: 1.72; acc: 0.38
Batch: 100; loss: 1.55; acc: 0.55
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.5; acc: 0.56
Batch: 160; loss: 1.49; acc: 0.66
Batch: 180; loss: 1.72; acc: 0.41
Batch: 200; loss: 1.56; acc: 0.5
Batch: 220; loss: 1.58; acc: 0.56
Batch: 240; loss: 1.7; acc: 0.53
Batch: 260; loss: 1.65; acc: 0.42
Batch: 280; loss: 1.62; acc: 0.47
Batch: 300; loss: 1.64; acc: 0.5
Batch: 320; loss: 1.55; acc: 0.55
Batch: 340; loss: 1.54; acc: 0.59
Batch: 360; loss: 1.8; acc: 0.44
Batch: 380; loss: 1.67; acc: 0.47
Batch: 400; loss: 1.69; acc: 0.48
Batch: 420; loss: 1.57; acc: 0.52
Batch: 440; loss: 1.56; acc: 0.56
Batch: 460; loss: 1.68; acc: 0.47
Batch: 480; loss: 1.66; acc: 0.47
Batch: 500; loss: 1.62; acc: 0.56
Batch: 520; loss: 1.63; acc: 0.45
Batch: 540; loss: 1.6; acc: 0.52
Batch: 560; loss: 1.61; acc: 0.47
Batch: 580; loss: 1.52; acc: 0.56
Batch: 600; loss: 1.59; acc: 0.56
Batch: 620; loss: 1.58; acc: 0.66
Batch: 640; loss: 1.7; acc: 0.41
Batch: 660; loss: 1.64; acc: 0.53
Batch: 680; loss: 1.51; acc: 0.52
Batch: 700; loss: 1.69; acc: 0.44
Batch: 720; loss: 1.57; acc: 0.55
Batch: 740; loss: 1.54; acc: 0.59
Batch: 760; loss: 1.61; acc: 0.48
Batch: 780; loss: 1.55; acc: 0.58
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

4.207204983686097e-05
1.0204777936451137e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.44; acc: 0.66
Batch: 60; loss: 1.48; acc: 0.61
Batch: 80; loss: 1.5; acc: 0.62
Batch: 100; loss: 1.61; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.53
Batch: 140; loss: 1.42; acc: 0.64
Val Epoch over. val_loss: 1.5552968940917093; val_accuracy: 0.5522492038216561 

The current subspace-distance is: 1.0204777936451137e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.53; acc: 0.59
Batch: 40; loss: 1.53; acc: 0.62
Batch: 60; loss: 1.64; acc: 0.44
Batch: 80; loss: 1.66; acc: 0.48
Batch: 100; loss: 1.69; acc: 0.45
Batch: 120; loss: 1.56; acc: 0.56
Batch: 140; loss: 1.62; acc: 0.52
Batch: 160; loss: 1.59; acc: 0.58
Batch: 180; loss: 1.51; acc: 0.58
Batch: 200; loss: 1.61; acc: 0.53
Batch: 220; loss: 1.78; acc: 0.5
Batch: 240; loss: 1.49; acc: 0.61
Batch: 260; loss: 1.5; acc: 0.61
Batch: 280; loss: 1.64; acc: 0.52
Batch: 300; loss: 1.64; acc: 0.5
Batch: 320; loss: 1.56; acc: 0.47
Batch: 340; loss: 1.51; acc: 0.64
Batch: 360; loss: 1.55; acc: 0.53
Batch: 380; loss: 1.53; acc: 0.58
Batch: 400; loss: 1.66; acc: 0.5
Batch: 420; loss: 1.67; acc: 0.48
Batch: 440; loss: 1.58; acc: 0.61
Batch: 460; loss: 1.81; acc: 0.38
Batch: 480; loss: 1.56; acc: 0.58
Batch: 500; loss: 1.76; acc: 0.47
Batch: 520; loss: 1.43; acc: 0.66
Batch: 540; loss: 1.55; acc: 0.5
Batch: 560; loss: 1.5; acc: 0.59
Batch: 580; loss: 1.66; acc: 0.45
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.79; acc: 0.44
Batch: 640; loss: 1.7; acc: 0.5
Batch: 660; loss: 1.58; acc: 0.53
Batch: 680; loss: 1.47; acc: 0.59
Batch: 700; loss: 1.57; acc: 0.58
Batch: 720; loss: 1.53; acc: 0.62
Batch: 740; loss: 1.47; acc: 0.59
Batch: 760; loss: 1.74; acc: 0.41
Batch: 780; loss: 1.48; acc: 0.58
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

4.3118208850501105e-05
1.255638289876515e-05
Batch: 0; loss: 1.7; acc: 0.48
Batch: 20; loss: 1.65; acc: 0.47
Batch: 40; loss: 1.44; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.49; acc: 0.62
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.65; acc: 0.53
Batch: 140; loss: 1.41; acc: 0.64
Val Epoch over. val_loss: 1.5463267633110096; val_accuracy: 0.5502587579617835 

The current subspace-distance is: 1.255638289876515e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.62; acc: 0.44
Batch: 40; loss: 1.47; acc: 0.56
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.6; acc: 0.48
Batch: 100; loss: 1.45; acc: 0.61
Batch: 120; loss: 1.47; acc: 0.62
Batch: 140; loss: 1.67; acc: 0.5
Batch: 160; loss: 1.46; acc: 0.62
Batch: 180; loss: 1.49; acc: 0.56
Batch: 200; loss: 1.61; acc: 0.45
Batch: 220; loss: 1.62; acc: 0.44
Batch: 240; loss: 1.58; acc: 0.55
Batch: 260; loss: 1.38; acc: 0.72
Batch: 280; loss: 1.64; acc: 0.48
Batch: 300; loss: 1.67; acc: 0.45
Batch: 320; loss: 1.51; acc: 0.56
Batch: 340; loss: 1.58; acc: 0.53
Batch: 360; loss: 1.68; acc: 0.48
Batch: 380; loss: 1.67; acc: 0.47
Batch: 400; loss: 1.64; acc: 0.45
Batch: 420; loss: 1.6; acc: 0.55
Batch: 440; loss: 1.57; acc: 0.53
Batch: 460; loss: 1.51; acc: 0.61
Batch: 480; loss: 1.63; acc: 0.45
Batch: 500; loss: 1.67; acc: 0.5
Batch: 520; loss: 1.59; acc: 0.5
Batch: 540; loss: 1.55; acc: 0.55
Batch: 560; loss: 1.58; acc: 0.47
Batch: 580; loss: 1.64; acc: 0.52
Batch: 600; loss: 1.62; acc: 0.5
Batch: 620; loss: 1.68; acc: 0.45
Batch: 640; loss: 1.65; acc: 0.45
Batch: 660; loss: 1.46; acc: 0.56
Batch: 680; loss: 1.7; acc: 0.53
Batch: 700; loss: 1.51; acc: 0.58
Batch: 720; loss: 1.61; acc: 0.52
Batch: 740; loss: 1.48; acc: 0.62
Batch: 760; loss: 1.68; acc: 0.42
Batch: 780; loss: 1.65; acc: 0.45
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

4.306035771151073e-05
1.4287968951975927e-05
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.64; acc: 0.48
Batch: 40; loss: 1.43; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.49; acc: 0.61
Batch: 100; loss: 1.6; acc: 0.47
Batch: 120; loss: 1.64; acc: 0.53
Batch: 140; loss: 1.38; acc: 0.66
Val Epoch over. val_loss: 1.5472533862302258; val_accuracy: 0.5428941082802548 

The current subspace-distance is: 1.4287968951975927e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.73; acc: 0.39
Batch: 20; loss: 1.6; acc: 0.45
Batch: 40; loss: 1.6; acc: 0.58
Batch: 60; loss: 1.54; acc: 0.5
Batch: 80; loss: 1.53; acc: 0.53
Batch: 100; loss: 1.5; acc: 0.55
Batch: 120; loss: 1.43; acc: 0.58
Batch: 140; loss: 1.66; acc: 0.5
Batch: 160; loss: 1.74; acc: 0.39
Batch: 180; loss: 1.52; acc: 0.55
Batch: 200; loss: 1.67; acc: 0.52
Batch: 220; loss: 1.63; acc: 0.5
Batch: 240; loss: 1.61; acc: 0.56
Batch: 260; loss: 1.59; acc: 0.52
Batch: 280; loss: 1.58; acc: 0.47
Batch: 300; loss: 1.64; acc: 0.44
Batch: 320; loss: 1.57; acc: 0.53
Batch: 340; loss: 1.54; acc: 0.52
Batch: 360; loss: 1.67; acc: 0.45
Batch: 380; loss: 1.63; acc: 0.47
Batch: 400; loss: 1.63; acc: 0.42
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.59; acc: 0.56
Batch: 460; loss: 1.54; acc: 0.52
Batch: 480; loss: 1.49; acc: 0.5
Batch: 500; loss: 1.7; acc: 0.36
Batch: 520; loss: 1.49; acc: 0.5
Batch: 540; loss: 1.6; acc: 0.5
Batch: 560; loss: 1.6; acc: 0.55
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.59; acc: 0.52
Batch: 620; loss: 1.58; acc: 0.5
Batch: 640; loss: 1.5; acc: 0.53
Batch: 660; loss: 1.54; acc: 0.47
Batch: 680; loss: 1.61; acc: 0.47
Batch: 700; loss: 1.48; acc: 0.55
Batch: 720; loss: 1.6; acc: 0.5
Batch: 740; loss: 1.53; acc: 0.62
Batch: 760; loss: 1.62; acc: 0.56
Batch: 780; loss: 1.67; acc: 0.39
Train Epoch over. train_loss: 1.59; train_accuracy: 0.51 

4.371803879621439e-05
1.4304213436844293e-05
Batch: 0; loss: 1.69; acc: 0.48
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.43; acc: 0.62
Batch: 60; loss: 1.48; acc: 0.59
Batch: 80; loss: 1.49; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.39; acc: 0.62
Val Epoch over. val_loss: 1.5487829629023364; val_accuracy: 0.5489649681528662 

The current subspace-distance is: 1.4304213436844293e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.53; acc: 0.52
Batch: 20; loss: 1.52; acc: 0.55
Batch: 40; loss: 1.57; acc: 0.55
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.64; acc: 0.48
Batch: 100; loss: 1.74; acc: 0.42
Batch: 120; loss: 1.6; acc: 0.52
Batch: 140; loss: 1.38; acc: 0.66
Batch: 160; loss: 1.54; acc: 0.5
Batch: 180; loss: 1.45; acc: 0.58
Batch: 200; loss: 1.47; acc: 0.52
Batch: 220; loss: 1.62; acc: 0.42
Batch: 240; loss: 1.48; acc: 0.61
Batch: 260; loss: 1.68; acc: 0.55
Batch: 280; loss: 1.62; acc: 0.42
Batch: 300; loss: 1.57; acc: 0.5
Batch: 320; loss: 1.61; acc: 0.5
Batch: 340; loss: 1.5; acc: 0.56
Batch: 360; loss: 1.68; acc: 0.44
Batch: 380; loss: 1.57; acc: 0.55
Batch: 400; loss: 1.56; acc: 0.55
Batch: 420; loss: 1.52; acc: 0.55
Batch: 440; loss: 1.54; acc: 0.53
Batch: 460; loss: 1.64; acc: 0.45
Batch: 480; loss: 1.57; acc: 0.5
Batch: 500; loss: 1.52; acc: 0.61
Batch: 520; loss: 1.61; acc: 0.52
Batch: 540; loss: 1.78; acc: 0.47
Batch: 560; loss: 1.72; acc: 0.38
Batch: 580; loss: 1.55; acc: 0.55
Batch: 600; loss: 1.66; acc: 0.45
Batch: 620; loss: 1.6; acc: 0.47
Batch: 640; loss: 1.45; acc: 0.53
Batch: 660; loss: 1.49; acc: 0.59
Batch: 680; loss: 1.58; acc: 0.5
Batch: 700; loss: 1.72; acc: 0.42
Batch: 720; loss: 1.58; acc: 0.48
Batch: 740; loss: 1.58; acc: 0.55
Batch: 760; loss: 1.66; acc: 0.42
Batch: 780; loss: 1.67; acc: 0.44
Train Epoch over. train_loss: 1.59; train_accuracy: 0.51 

4.494584572967142e-05
1.592714397702366e-05
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.44
Batch: 40; loss: 1.41; acc: 0.62
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.48; acc: 0.59
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.64; acc: 0.52
Batch: 140; loss: 1.36; acc: 0.62
Val Epoch over. val_loss: 1.532983464040574; val_accuracy: 0.5461783439490446 

The current subspace-distance is: 1.592714397702366e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.49; acc: 0.56
Batch: 20; loss: 1.71; acc: 0.38
Batch: 40; loss: 1.61; acc: 0.47
Batch: 60; loss: 1.65; acc: 0.45
Batch: 80; loss: 1.57; acc: 0.48
Batch: 100; loss: 1.66; acc: 0.44
Batch: 120; loss: 1.57; acc: 0.53
Batch: 140; loss: 1.57; acc: 0.5
Batch: 160; loss: 1.48; acc: 0.53
Batch: 180; loss: 1.69; acc: 0.42
Batch: 200; loss: 1.58; acc: 0.5
Batch: 220; loss: 1.51; acc: 0.58
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.66; acc: 0.55
Batch: 280; loss: 1.61; acc: 0.48
Batch: 300; loss: 1.6; acc: 0.52
Batch: 320; loss: 1.43; acc: 0.56
Batch: 340; loss: 1.52; acc: 0.53
Batch: 360; loss: 1.71; acc: 0.45
Batch: 380; loss: 1.51; acc: 0.53
Batch: 400; loss: 1.79; acc: 0.44
Batch: 420; loss: 1.56; acc: 0.52
Batch: 440; loss: 1.73; acc: 0.47
Batch: 460; loss: 1.49; acc: 0.58
Batch: 480; loss: 1.69; acc: 0.41
Batch: 500; loss: 1.67; acc: 0.5
Batch: 520; loss: 1.65; acc: 0.47
Batch: 540; loss: 1.72; acc: 0.44
Batch: 560; loss: 1.58; acc: 0.56
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.7; acc: 0.42
Batch: 620; loss: 1.56; acc: 0.53
Batch: 640; loss: 1.48; acc: 0.56
Batch: 660; loss: 1.73; acc: 0.45
Batch: 680; loss: 1.57; acc: 0.47
Batch: 700; loss: 1.59; acc: 0.56
Batch: 720; loss: 1.66; acc: 0.45
Batch: 740; loss: 1.66; acc: 0.42
Batch: 760; loss: 1.54; acc: 0.55
Batch: 780; loss: 1.61; acc: 0.45
Train Epoch over. train_loss: 1.58; train_accuracy: 0.51 

4.403622733661905e-05
1.1558271580724977e-05
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.46; acc: 0.59
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.35; acc: 0.61
Val Epoch over. val_loss: 1.5401227626071614; val_accuracy: 0.5346337579617835 

The current subspace-distance is: 1.1558271580724977e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.49; acc: 0.5
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.61; acc: 0.55
Batch: 60; loss: 1.62; acc: 0.56
Batch: 80; loss: 1.62; acc: 0.55
Batch: 100; loss: 1.55; acc: 0.52
Batch: 120; loss: 1.39; acc: 0.69
Batch: 140; loss: 1.51; acc: 0.59
Batch: 160; loss: 1.54; acc: 0.5
Batch: 180; loss: 1.55; acc: 0.52
Batch: 200; loss: 1.61; acc: 0.5
Batch: 220; loss: 1.51; acc: 0.58
Batch: 240; loss: 1.6; acc: 0.41
Batch: 260; loss: 1.59; acc: 0.53
Batch: 280; loss: 1.54; acc: 0.55
Batch: 300; loss: 1.47; acc: 0.59
Batch: 320; loss: 1.65; acc: 0.45
Batch: 340; loss: 1.6; acc: 0.47
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 1.49; acc: 0.62
Batch: 400; loss: 1.61; acc: 0.58
Batch: 420; loss: 1.49; acc: 0.58
Batch: 440; loss: 1.5; acc: 0.56
Batch: 460; loss: 1.57; acc: 0.53
Batch: 480; loss: 1.57; acc: 0.44
Batch: 500; loss: 1.66; acc: 0.52
Batch: 520; loss: 1.55; acc: 0.58
Batch: 540; loss: 1.5; acc: 0.52
Batch: 560; loss: 1.58; acc: 0.45
Batch: 580; loss: 1.55; acc: 0.48
Batch: 600; loss: 1.45; acc: 0.59
Batch: 620; loss: 1.68; acc: 0.44
Batch: 640; loss: 1.69; acc: 0.45
Batch: 660; loss: 1.74; acc: 0.45
Batch: 680; loss: 1.53; acc: 0.5
Batch: 700; loss: 1.55; acc: 0.55
Batch: 720; loss: 1.82; acc: 0.34
Batch: 740; loss: 1.72; acc: 0.45
Batch: 760; loss: 1.53; acc: 0.52
Batch: 780; loss: 1.62; acc: 0.55
Train Epoch over. train_loss: 1.58; train_accuracy: 0.51 

4.54496948805172e-05
1.4068427844904363e-05
Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.65; acc: 0.45
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.46; acc: 0.59
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.65; acc: 0.5
Batch: 140; loss: 1.34; acc: 0.62
Val Epoch over. val_loss: 1.528443633371098; val_accuracy: 0.5411027070063694 

The current subspace-distance is: 1.4068427844904363e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.57; acc: 0.53
Batch: 40; loss: 1.55; acc: 0.55
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.61; acc: 0.48
Batch: 100; loss: 1.5; acc: 0.59
Batch: 120; loss: 1.57; acc: 0.53
Batch: 140; loss: 1.48; acc: 0.62
Batch: 160; loss: 1.59; acc: 0.5
Batch: 180; loss: 1.42; acc: 0.55
Batch: 200; loss: 1.75; acc: 0.41
Batch: 220; loss: 1.73; acc: 0.42
Batch: 240; loss: 1.61; acc: 0.5
Batch: 260; loss: 1.58; acc: 0.48
Batch: 280; loss: 1.45; acc: 0.59
Batch: 300; loss: 1.57; acc: 0.41
Batch: 320; loss: 1.56; acc: 0.52
Batch: 340; loss: 1.49; acc: 0.64
Batch: 360; loss: 1.48; acc: 0.52
Batch: 380; loss: 1.47; acc: 0.53
Batch: 400; loss: 1.66; acc: 0.39
Batch: 420; loss: 1.53; acc: 0.52
Batch: 440; loss: 1.69; acc: 0.38
Batch: 460; loss: 1.56; acc: 0.53
Batch: 480; loss: 1.52; acc: 0.53
Batch: 500; loss: 1.6; acc: 0.53
Batch: 520; loss: 1.47; acc: 0.61
Batch: 540; loss: 1.64; acc: 0.47
Batch: 560; loss: 1.42; acc: 0.59
Batch: 580; loss: 1.56; acc: 0.48
Batch: 600; loss: 1.56; acc: 0.53
Batch: 620; loss: 1.53; acc: 0.52
Batch: 640; loss: 1.63; acc: 0.52
Batch: 660; loss: 1.58; acc: 0.55
Batch: 680; loss: 1.63; acc: 0.52
Batch: 700; loss: 1.7; acc: 0.48
Batch: 720; loss: 1.79; acc: 0.44
Batch: 740; loss: 1.61; acc: 0.53
Batch: 760; loss: 1.75; acc: 0.38
Batch: 780; loss: 1.45; acc: 0.56
Train Epoch over. train_loss: 1.57; train_accuracy: 0.51 

4.524526229943149e-05
1.3864904758520424e-05
Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.66; acc: 0.44
Batch: 40; loss: 1.4; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.61
Batch: 80; loss: 1.44; acc: 0.58
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.65; acc: 0.48
Batch: 140; loss: 1.32; acc: 0.62
Val Epoch over. val_loss: 1.5237283835745161; val_accuracy: 0.5376194267515924 

The current subspace-distance is: 1.3864904758520424e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.49; acc: 0.56
Batch: 40; loss: 1.36; acc: 0.67
Batch: 60; loss: 1.56; acc: 0.52
Batch: 80; loss: 1.63; acc: 0.53
Batch: 100; loss: 1.57; acc: 0.45
Batch: 120; loss: 1.66; acc: 0.36
Batch: 140; loss: 1.47; acc: 0.59
Batch: 160; loss: 1.55; acc: 0.55
Batch: 180; loss: 1.54; acc: 0.52
Batch: 200; loss: 1.62; acc: 0.48
Batch: 220; loss: 1.66; acc: 0.42
Batch: 240; loss: 1.58; acc: 0.5
Batch: 260; loss: 1.62; acc: 0.56
Batch: 280; loss: 1.61; acc: 0.55
Batch: 300; loss: 1.51; acc: 0.56
Batch: 320; loss: 1.7; acc: 0.53
Batch: 340; loss: 1.57; acc: 0.56
Batch: 360; loss: 1.61; acc: 0.52
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.67; acc: 0.48
Batch: 420; loss: 1.59; acc: 0.48
Batch: 440; loss: 1.63; acc: 0.45
Batch: 460; loss: 1.43; acc: 0.61
Batch: 480; loss: 1.55; acc: 0.44
Batch: 500; loss: 1.59; acc: 0.48
Batch: 520; loss: 1.55; acc: 0.56
Batch: 540; loss: 1.69; acc: 0.41
Batch: 560; loss: 1.65; acc: 0.42
Batch: 580; loss: 1.54; acc: 0.5
Batch: 600; loss: 1.61; acc: 0.42
Batch: 620; loss: 1.57; acc: 0.55
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.43; acc: 0.58
Batch: 680; loss: 1.61; acc: 0.56
Batch: 700; loss: 1.52; acc: 0.44
Batch: 720; loss: 1.59; acc: 0.52
Batch: 740; loss: 1.59; acc: 0.52
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.38; acc: 0.73
Train Epoch over. train_loss: 1.57; train_accuracy: 0.51 

4.620191975845955e-05
1.3658515854331199e-05
Batch: 0; loss: 1.65; acc: 0.47
Batch: 20; loss: 1.66; acc: 0.45
Batch: 40; loss: 1.39; acc: 0.62
Batch: 60; loss: 1.45; acc: 0.58
Batch: 80; loss: 1.45; acc: 0.56
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.31; acc: 0.64
Val Epoch over. val_loss: 1.5218053471510578; val_accuracy: 0.538515127388535 

The current subspace-distance is: 1.3658515854331199e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.53
Batch: 40; loss: 1.45; acc: 0.64
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.57; acc: 0.52
Batch: 100; loss: 1.62; acc: 0.41
Batch: 120; loss: 1.6; acc: 0.45
Batch: 140; loss: 1.47; acc: 0.61
Batch: 160; loss: 1.76; acc: 0.41
Batch: 180; loss: 1.53; acc: 0.52
Batch: 200; loss: 1.55; acc: 0.48
Batch: 220; loss: 1.5; acc: 0.48
Batch: 240; loss: 1.46; acc: 0.59
Batch: 260; loss: 1.7; acc: 0.45
Batch: 280; loss: 1.59; acc: 0.47
Batch: 300; loss: 1.55; acc: 0.5
Batch: 320; loss: 1.52; acc: 0.5
Batch: 340; loss: 1.67; acc: 0.5
Batch: 360; loss: 1.52; acc: 0.53
Batch: 380; loss: 1.61; acc: 0.5
Batch: 400; loss: 1.62; acc: 0.53
Batch: 420; loss: 1.69; acc: 0.47
Batch: 440; loss: 1.44; acc: 0.62
Batch: 460; loss: 1.62; acc: 0.41
Batch: 480; loss: 1.62; acc: 0.48
Batch: 500; loss: 1.54; acc: 0.41
Batch: 520; loss: 1.54; acc: 0.52
Batch: 540; loss: 1.58; acc: 0.45
Batch: 560; loss: 1.5; acc: 0.47
Batch: 580; loss: 1.47; acc: 0.5
Batch: 600; loss: 1.75; acc: 0.41
Batch: 620; loss: 1.59; acc: 0.52
Batch: 640; loss: 1.51; acc: 0.61
Batch: 660; loss: 1.53; acc: 0.52
Batch: 680; loss: 1.6; acc: 0.44
Batch: 700; loss: 1.6; acc: 0.59
Batch: 720; loss: 1.79; acc: 0.36
Batch: 740; loss: 1.61; acc: 0.42
Batch: 760; loss: 1.56; acc: 0.39
Batch: 780; loss: 1.6; acc: 0.52
Train Epoch over. train_loss: 1.57; train_accuracy: 0.51 

4.544166586128995e-05
1.2512899957073387e-05
Batch: 0; loss: 1.62; acc: 0.47
Batch: 20; loss: 1.64; acc: 0.44
Batch: 40; loss: 1.37; acc: 0.66
Batch: 60; loss: 1.43; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.56
Batch: 100; loss: 1.58; acc: 0.52
Batch: 120; loss: 1.63; acc: 0.48
Batch: 140; loss: 1.3; acc: 0.62
Val Epoch over. val_loss: 1.5095909439074766; val_accuracy: 0.540406050955414 

The current subspace-distance is: 1.2512899957073387e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.53
Batch: 20; loss: 1.56; acc: 0.48
Batch: 40; loss: 1.65; acc: 0.47
Batch: 60; loss: 1.64; acc: 0.48
Batch: 80; loss: 1.44; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.47
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.49; acc: 0.52
Batch: 160; loss: 1.73; acc: 0.42
Batch: 180; loss: 1.58; acc: 0.55
Batch: 200; loss: 1.56; acc: 0.52
Batch: 220; loss: 1.52; acc: 0.56
Batch: 240; loss: 1.61; acc: 0.52
Batch: 260; loss: 1.62; acc: 0.44
Batch: 280; loss: 1.69; acc: 0.47
Batch: 300; loss: 1.63; acc: 0.45
Batch: 320; loss: 1.54; acc: 0.48
Batch: 340; loss: 1.47; acc: 0.58
Batch: 360; loss: 1.58; acc: 0.52
Batch: 380; loss: 1.54; acc: 0.5
Batch: 400; loss: 1.44; acc: 0.56
Batch: 420; loss: 1.5; acc: 0.53
Batch: 440; loss: 1.39; acc: 0.59
Batch: 460; loss: 1.49; acc: 0.55
Batch: 480; loss: 1.61; acc: 0.45
Batch: 500; loss: 1.68; acc: 0.48
Batch: 520; loss: 1.65; acc: 0.41
Batch: 540; loss: 1.7; acc: 0.44
Batch: 560; loss: 1.61; acc: 0.56
Batch: 580; loss: 1.76; acc: 0.41
Batch: 600; loss: 1.53; acc: 0.5
Batch: 620; loss: 1.53; acc: 0.55
Batch: 640; loss: 1.51; acc: 0.56
Batch: 660; loss: 1.68; acc: 0.53
Batch: 680; loss: 1.45; acc: 0.58
Batch: 700; loss: 1.56; acc: 0.58
Batch: 720; loss: 1.59; acc: 0.48
Batch: 740; loss: 1.47; acc: 0.55
Batch: 760; loss: 1.54; acc: 0.52
Batch: 780; loss: 1.61; acc: 0.47
Train Epoch over. train_loss: 1.56; train_accuracy: 0.51 

4.663427171180956e-05
1.6372963727917522e-05
Batch: 0; loss: 1.64; acc: 0.45
Batch: 20; loss: 1.67; acc: 0.42
Batch: 40; loss: 1.38; acc: 0.66
Batch: 60; loss: 1.43; acc: 0.61
Batch: 80; loss: 1.43; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.44
Batch: 140; loss: 1.3; acc: 0.61
Val Epoch over. val_loss: 1.516274342871016; val_accuracy: 0.5344347133757962 

The current subspace-distance is: 1.6372963727917522e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.58; acc: 0.44
Batch: 20; loss: 1.58; acc: 0.5
Batch: 40; loss: 1.42; acc: 0.62
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.43; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.52
Batch: 120; loss: 1.7; acc: 0.39
Batch: 140; loss: 1.57; acc: 0.52
Batch: 160; loss: 1.71; acc: 0.47
Batch: 180; loss: 1.53; acc: 0.5
Batch: 200; loss: 1.45; acc: 0.59
Batch: 220; loss: 1.69; acc: 0.39
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 1.61; acc: 0.47
Batch: 280; loss: 1.79; acc: 0.39
Batch: 300; loss: 1.59; acc: 0.55
Batch: 320; loss: 1.38; acc: 0.66
Batch: 340; loss: 1.48; acc: 0.53
Batch: 360; loss: 1.5; acc: 0.58
Batch: 380; loss: 1.65; acc: 0.41
Batch: 400; loss: 1.64; acc: 0.44
Batch: 420; loss: 1.68; acc: 0.5
Batch: 440; loss: 1.75; acc: 0.45
Batch: 460; loss: 1.5; acc: 0.45
Batch: 480; loss: 1.64; acc: 0.47
Batch: 500; loss: 1.59; acc: 0.48
Batch: 520; loss: 1.58; acc: 0.48
Batch: 540; loss: 1.7; acc: 0.5
Batch: 560; loss: 1.43; acc: 0.56
Batch: 580; loss: 1.58; acc: 0.53
Batch: 600; loss: 1.54; acc: 0.44
Batch: 620; loss: 1.64; acc: 0.45
Batch: 640; loss: 1.38; acc: 0.61
Batch: 660; loss: 1.37; acc: 0.56
Batch: 680; loss: 1.62; acc: 0.41
Batch: 700; loss: 1.51; acc: 0.53
Batch: 720; loss: 1.56; acc: 0.53
Batch: 740; loss: 1.53; acc: 0.53
Batch: 760; loss: 1.51; acc: 0.53
Batch: 780; loss: 1.52; acc: 0.52
Train Epoch over. train_loss: 1.56; train_accuracy: 0.51 

4.576388164423406e-05
1.3424930330074858e-05
Batch: 0; loss: 1.61; acc: 0.47
Batch: 20; loss: 1.65; acc: 0.44
Batch: 40; loss: 1.37; acc: 0.66
Batch: 60; loss: 1.43; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.59; acc: 0.5
Batch: 120; loss: 1.65; acc: 0.48
Batch: 140; loss: 1.29; acc: 0.62
Val Epoch over. val_loss: 1.5072695366136588; val_accuracy: 0.5361265923566879 

The current subspace-distance is: 1.3424930330074858e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.4; acc: 0.62
Batch: 40; loss: 1.68; acc: 0.38
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.48; acc: 0.48
Batch: 100; loss: 1.39; acc: 0.59
Batch: 120; loss: 1.51; acc: 0.59
Batch: 140; loss: 1.6; acc: 0.47
Batch: 160; loss: 1.61; acc: 0.45
Batch: 180; loss: 1.69; acc: 0.42
Batch: 200; loss: 1.46; acc: 0.55
Batch: 220; loss: 1.43; acc: 0.58
Batch: 240; loss: 1.52; acc: 0.44
Batch: 260; loss: 1.63; acc: 0.55
Batch: 280; loss: 1.48; acc: 0.48
Batch: 300; loss: 1.58; acc: 0.52
Batch: 320; loss: 1.58; acc: 0.53
Batch: 340; loss: 1.55; acc: 0.53
Batch: 360; loss: 1.64; acc: 0.53
Batch: 380; loss: 1.57; acc: 0.56
Batch: 400; loss: 1.6; acc: 0.5
Batch: 420; loss: 1.53; acc: 0.47
Batch: 440; loss: 1.62; acc: 0.47
Batch: 460; loss: 1.55; acc: 0.47
Batch: 480; loss: 1.34; acc: 0.66
Batch: 500; loss: 1.59; acc: 0.55
Batch: 520; loss: 1.54; acc: 0.45
Batch: 540; loss: 1.6; acc: 0.59
Batch: 560; loss: 1.61; acc: 0.47
Batch: 580; loss: 1.63; acc: 0.44
Batch: 600; loss: 1.61; acc: 0.45
Batch: 620; loss: 1.51; acc: 0.52
Batch: 640; loss: 1.57; acc: 0.5
Batch: 660; loss: 1.45; acc: 0.5
Batch: 680; loss: 1.7; acc: 0.39
Batch: 700; loss: 1.67; acc: 0.47
Batch: 720; loss: 1.56; acc: 0.56
Batch: 740; loss: 1.46; acc: 0.52
Batch: 760; loss: 1.47; acc: 0.58
Batch: 780; loss: 1.76; acc: 0.42
Train Epoch over. train_loss: 1.56; train_accuracy: 0.51 

4.597531005856581e-05
1.3273736840346828e-05
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.38; acc: 0.62
Batch: 60; loss: 1.44; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.47
Batch: 140; loss: 1.31; acc: 0.61
Val Epoch over. val_loss: 1.5153357511872698; val_accuracy: 0.5321457006369427 

The current subspace-distance is: 1.3273736840346828e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.55
Batch: 20; loss: 1.49; acc: 0.47
Batch: 40; loss: 1.58; acc: 0.47
Batch: 60; loss: 1.48; acc: 0.5
Batch: 80; loss: 1.45; acc: 0.56
Batch: 100; loss: 1.51; acc: 0.48
Batch: 120; loss: 1.72; acc: 0.39
Batch: 140; loss: 1.58; acc: 0.53
Batch: 160; loss: 1.5; acc: 0.55
Batch: 180; loss: 1.64; acc: 0.41
Batch: 200; loss: 1.75; acc: 0.33
Batch: 220; loss: 1.7; acc: 0.41
Batch: 240; loss: 1.4; acc: 0.58
Batch: 260; loss: 1.4; acc: 0.58
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.48; acc: 0.53
Batch: 320; loss: 1.5; acc: 0.59
Batch: 340; loss: 1.53; acc: 0.5
Batch: 360; loss: 1.56; acc: 0.53
Batch: 380; loss: 1.64; acc: 0.55
Batch: 400; loss: 1.55; acc: 0.58
Batch: 420; loss: 1.62; acc: 0.47
Batch: 440; loss: 1.51; acc: 0.5
Batch: 460; loss: 1.59; acc: 0.53
Batch: 480; loss: 1.6; acc: 0.44
Batch: 500; loss: 1.45; acc: 0.56
Batch: 520; loss: 1.54; acc: 0.53
Batch: 540; loss: 1.61; acc: 0.42
Batch: 560; loss: 1.64; acc: 0.45
Batch: 580; loss: 1.71; acc: 0.33
Batch: 600; loss: 1.56; acc: 0.55
Batch: 620; loss: 1.42; acc: 0.62
Batch: 640; loss: 1.47; acc: 0.55
Batch: 660; loss: 1.52; acc: 0.56
Batch: 680; loss: 1.54; acc: 0.55
Batch: 700; loss: 1.53; acc: 0.55
Batch: 720; loss: 1.43; acc: 0.61
Batch: 740; loss: 1.51; acc: 0.47
Batch: 760; loss: 1.55; acc: 0.53
Batch: 780; loss: 1.5; acc: 0.5
Train Epoch over. train_loss: 1.56; train_accuracy: 0.51 

4.6634391765110195e-05
1.5941259334795177e-05
Batch: 0; loss: 1.63; acc: 0.5
Batch: 20; loss: 1.69; acc: 0.41
Batch: 40; loss: 1.37; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.59
Batch: 80; loss: 1.43; acc: 0.58
Batch: 100; loss: 1.61; acc: 0.47
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.3; acc: 0.59
Val Epoch over. val_loss: 1.5164181882408774; val_accuracy: 0.5311504777070064 

The current subspace-distance is: 1.5941259334795177e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.56
Batch: 20; loss: 1.59; acc: 0.59
Batch: 40; loss: 1.46; acc: 0.55
Batch: 60; loss: 1.43; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.58
Batch: 100; loss: 1.62; acc: 0.48
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.55
Batch: 160; loss: 1.42; acc: 0.56
Batch: 180; loss: 1.46; acc: 0.56
Batch: 200; loss: 1.74; acc: 0.41
Batch: 220; loss: 1.45; acc: 0.56
Batch: 240; loss: 1.52; acc: 0.5
Batch: 260; loss: 1.59; acc: 0.52
Batch: 280; loss: 1.56; acc: 0.48
Batch: 300; loss: 1.49; acc: 0.52
Batch: 320; loss: 1.68; acc: 0.39
Batch: 340; loss: 1.59; acc: 0.47
Batch: 360; loss: 1.65; acc: 0.47
Batch: 380; loss: 1.63; acc: 0.52
Batch: 400; loss: 1.51; acc: 0.58
Batch: 420; loss: 1.63; acc: 0.55
Batch: 440; loss: 1.59; acc: 0.53
Batch: 460; loss: 1.51; acc: 0.55
Batch: 480; loss: 1.52; acc: 0.55
Batch: 500; loss: 1.63; acc: 0.42
Batch: 520; loss: 1.49; acc: 0.48
Batch: 540; loss: 1.6; acc: 0.52
Batch: 560; loss: 1.52; acc: 0.5
Batch: 580; loss: 1.48; acc: 0.52
Batch: 600; loss: 1.57; acc: 0.48
Batch: 620; loss: 1.54; acc: 0.53
Batch: 640; loss: 1.52; acc: 0.55
Batch: 660; loss: 1.56; acc: 0.47
Batch: 680; loss: 1.53; acc: 0.39
Batch: 700; loss: 1.54; acc: 0.56
Batch: 720; loss: 1.5; acc: 0.45
Batch: 740; loss: 1.64; acc: 0.44
Batch: 760; loss: 1.56; acc: 0.5
Batch: 780; loss: 1.65; acc: 0.42
Train Epoch over. train_loss: 1.56; train_accuracy: 0.5 

4.652715870179236e-05
1.521989452157868e-05
Batch: 0; loss: 1.61; acc: 0.47
Batch: 20; loss: 1.67; acc: 0.41
Batch: 40; loss: 1.36; acc: 0.66
Batch: 60; loss: 1.42; acc: 0.62
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.5039633952887954; val_accuracy: 0.5387141719745223 

The current subspace-distance is: 1.521989452157868e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.71; acc: 0.45
Batch: 40; loss: 1.45; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.47
Batch: 80; loss: 1.56; acc: 0.58
Batch: 100; loss: 1.66; acc: 0.55
Batch: 120; loss: 1.54; acc: 0.5
Batch: 140; loss: 1.5; acc: 0.58
Batch: 160; loss: 1.6; acc: 0.41
Batch: 180; loss: 1.63; acc: 0.42
Batch: 200; loss: 1.4; acc: 0.61
Batch: 220; loss: 1.71; acc: 0.44
Batch: 240; loss: 1.64; acc: 0.47
Batch: 260; loss: 1.43; acc: 0.59
Batch: 280; loss: 1.53; acc: 0.52
Batch: 300; loss: 1.66; acc: 0.38
Batch: 320; loss: 1.73; acc: 0.45
Batch: 340; loss: 1.42; acc: 0.59
Batch: 360; loss: 1.49; acc: 0.47
Batch: 380; loss: 1.45; acc: 0.52
Batch: 400; loss: 1.56; acc: 0.52
Batch: 420; loss: 1.69; acc: 0.42
Batch: 440; loss: 1.54; acc: 0.44
Batch: 460; loss: 1.68; acc: 0.44
Batch: 480; loss: 1.66; acc: 0.42
Batch: 500; loss: 1.5; acc: 0.55
Batch: 520; loss: 1.47; acc: 0.59
Batch: 540; loss: 1.54; acc: 0.5
Batch: 560; loss: 1.5; acc: 0.5
Batch: 580; loss: 1.68; acc: 0.48
Batch: 600; loss: 1.41; acc: 0.58
Batch: 620; loss: 1.58; acc: 0.5
Batch: 640; loss: 1.46; acc: 0.5
Batch: 660; loss: 1.51; acc: 0.56
Batch: 680; loss: 1.64; acc: 0.47
Batch: 700; loss: 1.54; acc: 0.48
Batch: 720; loss: 1.61; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.56
Batch: 760; loss: 1.56; acc: 0.56
Batch: 780; loss: 1.45; acc: 0.61
Train Epoch over. train_loss: 1.56; train_accuracy: 0.51 

4.752773747895844e-05
1.6046966266003437e-05
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.68; acc: 0.42
Batch: 40; loss: 1.37; acc: 0.62
Batch: 60; loss: 1.43; acc: 0.58
Batch: 80; loss: 1.43; acc: 0.56
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.5080602313302884; val_accuracy: 0.5379179936305732 

The current subspace-distance is: 1.6046966266003437e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.44; acc: 0.55
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.58; acc: 0.5
Batch: 80; loss: 1.47; acc: 0.58
Batch: 100; loss: 1.6; acc: 0.45
Batch: 120; loss: 1.74; acc: 0.45
Batch: 140; loss: 1.36; acc: 0.62
Batch: 160; loss: 1.54; acc: 0.48
Batch: 180; loss: 1.52; acc: 0.53
Batch: 200; loss: 1.38; acc: 0.53
Batch: 220; loss: 1.6; acc: 0.53
Batch: 240; loss: 1.71; acc: 0.42
Batch: 260; loss: 1.66; acc: 0.52
Batch: 280; loss: 1.54; acc: 0.48
Batch: 300; loss: 1.49; acc: 0.55
Batch: 320; loss: 1.57; acc: 0.52
Batch: 340; loss: 1.5; acc: 0.47
Batch: 360; loss: 1.51; acc: 0.59
Batch: 380; loss: 1.43; acc: 0.58
Batch: 400; loss: 1.39; acc: 0.59
Batch: 420; loss: 1.53; acc: 0.48
Batch: 440; loss: 1.48; acc: 0.56
Batch: 460; loss: 1.63; acc: 0.42
Batch: 480; loss: 1.58; acc: 0.5
Batch: 500; loss: 1.45; acc: 0.59
Batch: 520; loss: 1.56; acc: 0.55
Batch: 540; loss: 1.61; acc: 0.48
Batch: 560; loss: 1.62; acc: 0.47
Batch: 580; loss: 1.56; acc: 0.5
Batch: 600; loss: 1.6; acc: 0.48
Batch: 620; loss: 1.59; acc: 0.41
Batch: 640; loss: 1.46; acc: 0.53
Batch: 660; loss: 1.55; acc: 0.5
Batch: 680; loss: 1.41; acc: 0.58
Batch: 700; loss: 1.46; acc: 0.58
Batch: 720; loss: 1.58; acc: 0.56
Batch: 740; loss: 1.53; acc: 0.52
Batch: 760; loss: 1.56; acc: 0.45
Batch: 780; loss: 1.65; acc: 0.52
Train Epoch over. train_loss: 1.55; train_accuracy: 0.51 

4.590379830915481e-05
1.2195402632642072e-05
Batch: 0; loss: 1.61; acc: 0.47
Batch: 20; loss: 1.66; acc: 0.39
Batch: 40; loss: 1.36; acc: 0.69
Batch: 60; loss: 1.42; acc: 0.62
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.52
Batch: 120; loss: 1.65; acc: 0.5
Batch: 140; loss: 1.28; acc: 0.62
Val Epoch over. val_loss: 1.5018171633884405; val_accuracy: 0.5343351910828026 

The current subspace-distance is: 1.2195402632642072e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.44; acc: 0.58
Batch: 40; loss: 1.53; acc: 0.52
Batch: 60; loss: 1.52; acc: 0.5
Batch: 80; loss: 1.72; acc: 0.41
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.6; acc: 0.48
Batch: 160; loss: 1.52; acc: 0.52
Batch: 180; loss: 1.52; acc: 0.52
Batch: 200; loss: 1.62; acc: 0.48
Batch: 220; loss: 1.64; acc: 0.5
Batch: 240; loss: 1.79; acc: 0.36
Batch: 260; loss: 1.63; acc: 0.42
Batch: 280; loss: 1.6; acc: 0.42
Batch: 300; loss: 1.4; acc: 0.59
Batch: 320; loss: 1.52; acc: 0.56
Batch: 340; loss: 1.51; acc: 0.53
Batch: 360; loss: 1.67; acc: 0.47
Batch: 380; loss: 1.37; acc: 0.59
Batch: 400; loss: 1.7; acc: 0.5
Batch: 420; loss: 1.67; acc: 0.39
Batch: 440; loss: 1.79; acc: 0.36
Batch: 460; loss: 1.49; acc: 0.58
Batch: 480; loss: 1.57; acc: 0.48
Batch: 500; loss: 1.57; acc: 0.5
Batch: 520; loss: 1.52; acc: 0.5
Batch: 540; loss: 1.52; acc: 0.52
Batch: 560; loss: 1.53; acc: 0.48
Batch: 580; loss: 1.65; acc: 0.44
Batch: 600; loss: 1.58; acc: 0.5
Batch: 620; loss: 1.55; acc: 0.5
Batch: 640; loss: 1.55; acc: 0.47
Batch: 660; loss: 1.52; acc: 0.56
Batch: 680; loss: 1.68; acc: 0.42
Batch: 700; loss: 1.51; acc: 0.55
Batch: 720; loss: 1.5; acc: 0.53
Batch: 740; loss: 1.54; acc: 0.52
Batch: 760; loss: 1.57; acc: 0.5
Batch: 780; loss: 1.79; acc: 0.39
Train Epoch over. train_loss: 1.55; train_accuracy: 0.51 

4.764166078530252e-05
1.5816889572306536e-05
Batch: 0; loss: 1.6; acc: 0.5
Batch: 20; loss: 1.67; acc: 0.42
Batch: 40; loss: 1.36; acc: 0.67
Batch: 60; loss: 1.42; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.65; acc: 0.48
Batch: 140; loss: 1.27; acc: 0.64
Val Epoch over. val_loss: 1.5008704351012114; val_accuracy: 0.5370222929936306 

The current subspace-distance is: 1.5816889572306536e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.63; acc: 0.45
Batch: 20; loss: 1.63; acc: 0.44
Batch: 40; loss: 1.63; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.62; acc: 0.48
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.58; acc: 0.41
Batch: 140; loss: 1.52; acc: 0.52
Batch: 160; loss: 1.59; acc: 0.53
Batch: 180; loss: 1.32; acc: 0.64
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.57; acc: 0.47
Batch: 240; loss: 1.44; acc: 0.58
Batch: 260; loss: 1.36; acc: 0.61
Batch: 280; loss: 1.56; acc: 0.53
Batch: 300; loss: 1.56; acc: 0.41
Batch: 320; loss: 1.63; acc: 0.34
Batch: 340; loss: 1.58; acc: 0.53
Batch: 360; loss: 1.68; acc: 0.44
Batch: 380; loss: 1.42; acc: 0.56
Batch: 400; loss: 1.51; acc: 0.53
Batch: 420; loss: 1.68; acc: 0.45
Batch: 440; loss: 1.57; acc: 0.5
Batch: 460; loss: 1.45; acc: 0.56
Batch: 480; loss: 1.65; acc: 0.36
Batch: 500; loss: 1.48; acc: 0.61
Batch: 520; loss: 1.47; acc: 0.5
Batch: 540; loss: 1.5; acc: 0.55
Batch: 560; loss: 1.43; acc: 0.64
Batch: 580; loss: 1.58; acc: 0.5
Batch: 600; loss: 1.54; acc: 0.58
Batch: 620; loss: 1.58; acc: 0.48
Batch: 640; loss: 1.55; acc: 0.47
Batch: 660; loss: 1.56; acc: 0.52
Batch: 680; loss: 1.59; acc: 0.48
Batch: 700; loss: 1.54; acc: 0.52
Batch: 720; loss: 1.41; acc: 0.56
Batch: 740; loss: 1.76; acc: 0.41
Batch: 760; loss: 1.53; acc: 0.59
Batch: 780; loss: 1.65; acc: 0.45
Train Epoch over. train_loss: 1.55; train_accuracy: 0.5 

4.711841756943613e-05
1.3067591680737678e-05
Batch: 0; loss: 1.59; acc: 0.5
Batch: 20; loss: 1.67; acc: 0.39
Batch: 40; loss: 1.36; acc: 0.69
Batch: 60; loss: 1.41; acc: 0.62
Batch: 80; loss: 1.41; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.64; acc: 0.48
Batch: 140; loss: 1.27; acc: 0.59
Val Epoch over. val_loss: 1.4954214483309702; val_accuracy: 0.535828025477707 

The current subspace-distance is: 1.3067591680737678e-05 

plots/subspace_training/table13slim/2020-01-29 16:00:01/N_13_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.11

The number of parameters is: 272274

The number of individual parameters is:

9
162
9
9
14
33012
14
14
27
99036
27
27
64
134784
64
64
4096
64
640
10
64
64

nonzero elements in E: 27227397
elements in E: 27227400
fraction nonzero: 0.9999998898168756
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.44; acc: 0.03
Batch: 20; loss: 2.3; acc: 0.19
Batch: 40; loss: 2.29; acc: 0.11
Batch: 60; loss: 2.23; acc: 0.16
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.07; acc: 0.19
Batch: 120; loss: 2.07; acc: 0.25
Batch: 140; loss: 2.14; acc: 0.22
Batch: 160; loss: 2.05; acc: 0.36
Batch: 180; loss: 2.08; acc: 0.3
Batch: 200; loss: 2.04; acc: 0.2
Batch: 220; loss: 1.98; acc: 0.41
Batch: 240; loss: 2.04; acc: 0.31
Batch: 260; loss: 1.92; acc: 0.38
Batch: 280; loss: 2.07; acc: 0.34
Batch: 300; loss: 2.06; acc: 0.27
Batch: 320; loss: 1.89; acc: 0.45
Batch: 340; loss: 1.94; acc: 0.42
Batch: 360; loss: 1.91; acc: 0.5
Batch: 380; loss: 1.89; acc: 0.41
Batch: 400; loss: 1.99; acc: 0.31
Batch: 420; loss: 1.97; acc: 0.38
Batch: 440; loss: 1.93; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.38
Batch: 480; loss: 1.93; acc: 0.41
Batch: 500; loss: 1.84; acc: 0.5
Batch: 520; loss: 1.92; acc: 0.38
Batch: 540; loss: 1.87; acc: 0.41
Batch: 560; loss: 1.73; acc: 0.53
Batch: 580; loss: 2.0; acc: 0.33
Batch: 600; loss: 1.8; acc: 0.47
Batch: 620; loss: 1.86; acc: 0.47
Batch: 640; loss: 1.87; acc: 0.39
Batch: 660; loss: 1.79; acc: 0.48
Batch: 680; loss: 1.89; acc: 0.41
Batch: 700; loss: 1.84; acc: 0.5
Batch: 720; loss: 1.83; acc: 0.5
Batch: 740; loss: 1.81; acc: 0.5
Batch: 760; loss: 1.76; acc: 0.56
Batch: 780; loss: 1.91; acc: 0.39
Train Epoch over. train_loss: 1.98; train_accuracy: 0.35 

5.076029265183024e-05
4.451423228601925e-05
Batch: 0; loss: 1.85; acc: 0.47
Batch: 20; loss: 1.87; acc: 0.42
Batch: 40; loss: 1.56; acc: 0.73
Batch: 60; loss: 1.71; acc: 0.59
Batch: 80; loss: 1.74; acc: 0.47
Batch: 100; loss: 1.78; acc: 0.48
Batch: 120; loss: 1.87; acc: 0.45
Batch: 140; loss: 1.71; acc: 0.44
Val Epoch over. val_loss: 1.7797344110573932; val_accuracy: 0.47999601910828027 

The current subspace-distance is: 4.451423228601925e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.89; acc: 0.41
Batch: 20; loss: 1.8; acc: 0.48
Batch: 40; loss: 1.76; acc: 0.53
Batch: 60; loss: 1.83; acc: 0.45
Batch: 80; loss: 1.71; acc: 0.48
Batch: 100; loss: 1.77; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.62
Batch: 140; loss: 1.8; acc: 0.47
Batch: 160; loss: 1.76; acc: 0.45
Batch: 180; loss: 1.78; acc: 0.44
Batch: 200; loss: 1.81; acc: 0.52
Batch: 220; loss: 1.76; acc: 0.53
Batch: 240; loss: 1.92; acc: 0.39
Batch: 260; loss: 1.71; acc: 0.58
Batch: 280; loss: 1.58; acc: 0.64
Batch: 300; loss: 1.58; acc: 0.66
Batch: 320; loss: 1.68; acc: 0.59
Batch: 340; loss: 1.78; acc: 0.5
Batch: 360; loss: 1.76; acc: 0.52
Batch: 380; loss: 1.61; acc: 0.61
Batch: 400; loss: 1.83; acc: 0.42
Batch: 420; loss: 1.75; acc: 0.47
Batch: 440; loss: 1.74; acc: 0.48
Batch: 460; loss: 1.78; acc: 0.39
Batch: 480; loss: 1.73; acc: 0.55
Batch: 500; loss: 1.73; acc: 0.47
Batch: 520; loss: 1.81; acc: 0.42
Batch: 540; loss: 1.72; acc: 0.45
Batch: 560; loss: 1.76; acc: 0.48
Batch: 580; loss: 1.77; acc: 0.47
Batch: 600; loss: 1.62; acc: 0.62
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.64; acc: 0.58
Batch: 660; loss: 1.77; acc: 0.52
Batch: 680; loss: 1.76; acc: 0.47
Batch: 700; loss: 1.59; acc: 0.61
Batch: 720; loss: 1.63; acc: 0.58
Batch: 740; loss: 1.72; acc: 0.45
Batch: 760; loss: 1.81; acc: 0.42
Batch: 780; loss: 1.69; acc: 0.52
Train Epoch over. train_loss: 1.73; train_accuracy: 0.51 

7.005145016591996e-05
6.40306097920984e-05
Batch: 0; loss: 1.75; acc: 0.52
Batch: 20; loss: 1.78; acc: 0.48
Batch: 40; loss: 1.38; acc: 0.75
Batch: 60; loss: 1.58; acc: 0.67
Batch: 80; loss: 1.58; acc: 0.66
Batch: 100; loss: 1.7; acc: 0.47
Batch: 120; loss: 1.76; acc: 0.5
Batch: 140; loss: 1.52; acc: 0.64
Val Epoch over. val_loss: 1.648185550786887; val_accuracy: 0.5534434713375797 

The current subspace-distance is: 6.40306097920984e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.52
Batch: 20; loss: 1.71; acc: 0.41
Batch: 40; loss: 1.66; acc: 0.55
Batch: 60; loss: 1.6; acc: 0.66
Batch: 80; loss: 1.68; acc: 0.58
Batch: 100; loss: 1.64; acc: 0.55
Batch: 120; loss: 1.59; acc: 0.61
Batch: 140; loss: 1.72; acc: 0.5
Batch: 160; loss: 1.57; acc: 0.61
Batch: 180; loss: 1.68; acc: 0.61
Batch: 200; loss: 1.61; acc: 0.58
Batch: 220; loss: 1.7; acc: 0.48
Batch: 240; loss: 1.67; acc: 0.48
Batch: 260; loss: 1.72; acc: 0.53
Batch: 280; loss: 1.74; acc: 0.44
Batch: 300; loss: 1.73; acc: 0.44
Batch: 320; loss: 1.66; acc: 0.5
Batch: 340; loss: 1.7; acc: 0.48
Batch: 360; loss: 1.66; acc: 0.45
Batch: 380; loss: 1.55; acc: 0.67
Batch: 400; loss: 1.53; acc: 0.56
Batch: 420; loss: 1.71; acc: 0.55
Batch: 440; loss: 1.62; acc: 0.47
Batch: 460; loss: 1.55; acc: 0.62
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.8; acc: 0.34
Batch: 520; loss: 1.5; acc: 0.58
Batch: 540; loss: 1.72; acc: 0.5
Batch: 560; loss: 1.57; acc: 0.61
Batch: 580; loss: 1.64; acc: 0.55
Batch: 600; loss: 1.56; acc: 0.58
Batch: 620; loss: 1.5; acc: 0.64
Batch: 640; loss: 1.6; acc: 0.53
Batch: 660; loss: 1.66; acc: 0.55
Batch: 680; loss: 1.53; acc: 0.48
Batch: 700; loss: 1.55; acc: 0.58
Batch: 720; loss: 1.55; acc: 0.59
Batch: 740; loss: 1.43; acc: 0.64
Batch: 760; loss: 1.47; acc: 0.59
Batch: 780; loss: 1.6; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.55 

8.627046190667897e-05
8.206933125620708e-05
Batch: 0; loss: 1.6; acc: 0.62
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.2; acc: 0.81
Batch: 60; loss: 1.47; acc: 0.64
Batch: 80; loss: 1.44; acc: 0.62
Batch: 100; loss: 1.55; acc: 0.62
Batch: 120; loss: 1.6; acc: 0.55
Batch: 140; loss: 1.36; acc: 0.69
Val Epoch over. val_loss: 1.5151752051274487; val_accuracy: 0.5990246815286624 

The current subspace-distance is: 8.206933125620708e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.59; acc: 0.52
Batch: 60; loss: 1.46; acc: 0.64
Batch: 80; loss: 1.56; acc: 0.61
Batch: 100; loss: 1.52; acc: 0.66
Batch: 120; loss: 1.52; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.72
Batch: 160; loss: 1.56; acc: 0.62
Batch: 180; loss: 1.5; acc: 0.59
Batch: 200; loss: 1.47; acc: 0.64
Batch: 220; loss: 1.52; acc: 0.62
Batch: 240; loss: 1.49; acc: 0.58
Batch: 260; loss: 1.47; acc: 0.67
Batch: 280; loss: 1.59; acc: 0.61
Batch: 300; loss: 1.57; acc: 0.55
Batch: 320; loss: 1.52; acc: 0.64
Batch: 340; loss: 1.56; acc: 0.48
Batch: 360; loss: 1.61; acc: 0.53
Batch: 380; loss: 1.46; acc: 0.59
Batch: 400; loss: 1.64; acc: 0.47
Batch: 420; loss: 1.36; acc: 0.69
Batch: 440; loss: 1.46; acc: 0.61
Batch: 460; loss: 1.42; acc: 0.69
Batch: 480; loss: 1.43; acc: 0.64
Batch: 500; loss: 1.42; acc: 0.62
Batch: 520; loss: 1.53; acc: 0.56
Batch: 540; loss: 1.49; acc: 0.56
Batch: 560; loss: 1.48; acc: 0.56
Batch: 580; loss: 1.54; acc: 0.56
Batch: 600; loss: 1.43; acc: 0.58
Batch: 620; loss: 1.45; acc: 0.64
Batch: 640; loss: 1.36; acc: 0.62
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.35; acc: 0.66
Batch: 700; loss: 1.56; acc: 0.61
Batch: 720; loss: 1.39; acc: 0.64
Batch: 740; loss: 1.4; acc: 0.64
Batch: 760; loss: 1.57; acc: 0.56
Batch: 780; loss: 1.47; acc: 0.53
Train Epoch over. train_loss: 1.51; train_accuracy: 0.59 

0.00010437757737236097
9.879542631097138e-05
Batch: 0; loss: 1.52; acc: 0.64
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.1; acc: 0.83
Batch: 60; loss: 1.4; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.66
Batch: 120; loss: 1.51; acc: 0.66
Batch: 140; loss: 1.24; acc: 0.83
Val Epoch over. val_loss: 1.4307857227932876; val_accuracy: 0.640625 

The current subspace-distance is: 9.879542631097138e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.62
Batch: 20; loss: 1.41; acc: 0.61
Batch: 40; loss: 1.48; acc: 0.58
Batch: 60; loss: 1.27; acc: 0.77
Batch: 80; loss: 1.33; acc: 0.7
Batch: 100; loss: 1.57; acc: 0.52
Batch: 120; loss: 1.41; acc: 0.56
Batch: 140; loss: 1.52; acc: 0.58
Batch: 160; loss: 1.51; acc: 0.61
Batch: 180; loss: 1.48; acc: 0.58
Batch: 200; loss: 1.5; acc: 0.58
Batch: 220; loss: 1.65; acc: 0.47
Batch: 240; loss: 1.54; acc: 0.56
Batch: 260; loss: 1.53; acc: 0.62
Batch: 280; loss: 1.39; acc: 0.67
Batch: 300; loss: 1.53; acc: 0.61
Batch: 320; loss: 1.31; acc: 0.66
Batch: 340; loss: 1.41; acc: 0.61
Batch: 360; loss: 1.28; acc: 0.75
Batch: 380; loss: 1.46; acc: 0.59
Batch: 400; loss: 1.65; acc: 0.45
Batch: 420; loss: 1.36; acc: 0.69
Batch: 440; loss: 1.31; acc: 0.75
Batch: 460; loss: 1.43; acc: 0.62
Batch: 480; loss: 1.38; acc: 0.59
Batch: 500; loss: 1.39; acc: 0.64
Batch: 520; loss: 1.32; acc: 0.62
Batch: 540; loss: 1.53; acc: 0.52
Batch: 560; loss: 1.46; acc: 0.61
Batch: 580; loss: 1.45; acc: 0.61
Batch: 600; loss: 1.52; acc: 0.58
Batch: 620; loss: 1.42; acc: 0.61
Batch: 640; loss: 1.26; acc: 0.72
Batch: 660; loss: 1.33; acc: 0.66
Batch: 680; loss: 1.43; acc: 0.53
Batch: 700; loss: 1.57; acc: 0.5
Batch: 720; loss: 1.51; acc: 0.59
Batch: 740; loss: 1.28; acc: 0.7
Batch: 760; loss: 1.56; acc: 0.59
Batch: 780; loss: 1.45; acc: 0.62
Train Epoch over. train_loss: 1.44; train_accuracy: 0.62 

0.00011551254283403978
0.00011056442599510774
Batch: 0; loss: 1.45; acc: 0.67
Batch: 20; loss: 1.57; acc: 0.58
Batch: 40; loss: 1.02; acc: 0.84
Batch: 60; loss: 1.32; acc: 0.66
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.41; acc: 0.69
Batch: 120; loss: 1.47; acc: 0.64
Batch: 140; loss: 1.16; acc: 0.8
Val Epoch over. val_loss: 1.3654272533526086; val_accuracy: 0.6596337579617835 

The current subspace-distance is: 0.00011056442599510774 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.69
Batch: 20; loss: 1.41; acc: 0.59
Batch: 40; loss: 1.52; acc: 0.61
Batch: 60; loss: 1.27; acc: 0.7
Batch: 80; loss: 1.29; acc: 0.69
Batch: 100; loss: 1.25; acc: 0.73
Batch: 120; loss: 1.3; acc: 0.75
Batch: 140; loss: 1.48; acc: 0.58
Batch: 160; loss: 1.44; acc: 0.69
Batch: 180; loss: 1.28; acc: 0.72
Batch: 200; loss: 1.32; acc: 0.66
Batch: 220; loss: 1.28; acc: 0.67
Batch: 240; loss: 1.47; acc: 0.66
Batch: 260; loss: 1.5; acc: 0.56
Batch: 280; loss: 1.44; acc: 0.66
Batch: 300; loss: 1.53; acc: 0.5
Batch: 320; loss: 1.27; acc: 0.67
Batch: 340; loss: 1.34; acc: 0.7
Batch: 360; loss: 1.39; acc: 0.66
Batch: 380; loss: 1.54; acc: 0.55
Batch: 400; loss: 1.45; acc: 0.59
Batch: 420; loss: 1.44; acc: 0.55
Batch: 440; loss: 1.27; acc: 0.7
Batch: 460; loss: 1.42; acc: 0.58
Batch: 480; loss: 1.42; acc: 0.58
Batch: 500; loss: 1.43; acc: 0.61
Batch: 520; loss: 1.42; acc: 0.59
Batch: 540; loss: 1.35; acc: 0.61
Batch: 560; loss: 1.46; acc: 0.64
Batch: 580; loss: 1.41; acc: 0.59
Batch: 600; loss: 1.39; acc: 0.61
Batch: 620; loss: 1.35; acc: 0.62
Batch: 640; loss: 1.36; acc: 0.66
Batch: 660; loss: 1.39; acc: 0.66
Batch: 680; loss: 1.26; acc: 0.67
Batch: 700; loss: 1.38; acc: 0.58
Batch: 720; loss: 1.29; acc: 0.73
Batch: 740; loss: 1.49; acc: 0.53
Batch: 760; loss: 1.29; acc: 0.67
Batch: 780; loss: 1.34; acc: 0.66
Train Epoch over. train_loss: 1.38; train_accuracy: 0.63 

0.00012800368131138384
0.00012369827891234308
Batch: 0; loss: 1.4; acc: 0.64
Batch: 20; loss: 1.52; acc: 0.53
Batch: 40; loss: 0.95; acc: 0.86
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.67
Batch: 120; loss: 1.44; acc: 0.61
Batch: 140; loss: 1.09; acc: 0.81
Val Epoch over. val_loss: 1.3120480403778658; val_accuracy: 0.6655055732484076 

The current subspace-distance is: 0.00012369827891234308 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.35; acc: 0.67
Batch: 40; loss: 1.24; acc: 0.72
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.37; acc: 0.62
Batch: 100; loss: 1.49; acc: 0.62
Batch: 120; loss: 1.18; acc: 0.78
Batch: 140; loss: 1.35; acc: 0.66
Batch: 160; loss: 1.36; acc: 0.62
Batch: 180; loss: 1.36; acc: 0.61
Batch: 200; loss: 1.44; acc: 0.58
Batch: 220; loss: 1.37; acc: 0.53
Batch: 240; loss: 1.33; acc: 0.62
Batch: 260; loss: 1.41; acc: 0.66
Batch: 280; loss: 1.37; acc: 0.64
Batch: 300; loss: 1.45; acc: 0.59
Batch: 320; loss: 1.36; acc: 0.64
Batch: 340; loss: 1.23; acc: 0.73
Batch: 360; loss: 1.26; acc: 0.7
Batch: 380; loss: 1.33; acc: 0.64
Batch: 400; loss: 1.4; acc: 0.66
Batch: 420; loss: 1.28; acc: 0.69
Batch: 440; loss: 1.3; acc: 0.64
Batch: 460; loss: 1.21; acc: 0.77
Batch: 480; loss: 1.33; acc: 0.62
Batch: 500; loss: 1.28; acc: 0.62
Batch: 520; loss: 1.35; acc: 0.64
Batch: 540; loss: 1.29; acc: 0.69
Batch: 560; loss: 1.39; acc: 0.58
Batch: 580; loss: 1.28; acc: 0.67
Batch: 600; loss: 1.42; acc: 0.58
Batch: 620; loss: 1.38; acc: 0.64
Batch: 640; loss: 1.35; acc: 0.62
Batch: 660; loss: 1.38; acc: 0.62
Batch: 680; loss: 1.18; acc: 0.73
Batch: 700; loss: 1.32; acc: 0.66
Batch: 720; loss: 1.26; acc: 0.67
Batch: 740; loss: 1.26; acc: 0.73
Batch: 760; loss: 1.26; acc: 0.69
Batch: 780; loss: 1.35; acc: 0.64
Train Epoch over. train_loss: 1.34; train_accuracy: 0.64 

0.00014194879622664303
0.00013765098992735147
Batch: 0; loss: 1.36; acc: 0.62
Batch: 20; loss: 1.52; acc: 0.53
Batch: 40; loss: 0.89; acc: 0.86
Batch: 60; loss: 1.26; acc: 0.64
Batch: 80; loss: 1.23; acc: 0.66
Batch: 100; loss: 1.34; acc: 0.69
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 1.04; acc: 0.78
Val Epoch over. val_loss: 1.2632939443466769; val_accuracy: 0.6768511146496815 

The current subspace-distance is: 0.00013765098992735147 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.75
Batch: 40; loss: 1.29; acc: 0.64
Batch: 60; loss: 1.25; acc: 0.64
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.3; acc: 0.67
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 1.34; acc: 0.62
Batch: 160; loss: 1.55; acc: 0.58
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.54; acc: 0.56
Batch: 220; loss: 1.38; acc: 0.56
Batch: 240; loss: 1.14; acc: 0.78
Batch: 260; loss: 1.46; acc: 0.53
Batch: 280; loss: 1.51; acc: 0.55
Batch: 300; loss: 1.33; acc: 0.62
Batch: 320; loss: 1.32; acc: 0.67
Batch: 340; loss: 1.3; acc: 0.62
Batch: 360; loss: 1.26; acc: 0.64
Batch: 380; loss: 1.41; acc: 0.53
Batch: 400; loss: 1.22; acc: 0.73
Batch: 420; loss: 1.38; acc: 0.55
Batch: 440; loss: 1.21; acc: 0.73
Batch: 460; loss: 1.28; acc: 0.66
Batch: 480; loss: 1.41; acc: 0.61
Batch: 500; loss: 1.22; acc: 0.66
Batch: 520; loss: 1.21; acc: 0.73
Batch: 540; loss: 1.2; acc: 0.72
Batch: 560; loss: 1.43; acc: 0.53
Batch: 580; loss: 1.38; acc: 0.64
Batch: 600; loss: 1.36; acc: 0.59
Batch: 620; loss: 1.45; acc: 0.5
Batch: 640; loss: 1.27; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.81
Batch: 680; loss: 1.28; acc: 0.66
Batch: 700; loss: 1.44; acc: 0.66
Batch: 720; loss: 1.41; acc: 0.58
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 1.2; acc: 0.67
Batch: 780; loss: 1.22; acc: 0.67
Train Epoch over. train_loss: 1.29; train_accuracy: 0.65 

0.00015530727978330106
0.00014874653425067663
Batch: 0; loss: 1.34; acc: 0.64
Batch: 20; loss: 1.47; acc: 0.56
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 1.26; acc: 0.62
Batch: 80; loss: 1.21; acc: 0.64
Batch: 100; loss: 1.31; acc: 0.7
Batch: 120; loss: 1.39; acc: 0.56
Batch: 140; loss: 1.0; acc: 0.77
Val Epoch over. val_loss: 1.2174989778524752; val_accuracy: 0.6873009554140127 

The current subspace-distance is: 0.00014874653425067663 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.59
Batch: 20; loss: 1.03; acc: 0.78
Batch: 40; loss: 1.15; acc: 0.73
Batch: 60; loss: 1.24; acc: 0.7
Batch: 80; loss: 1.4; acc: 0.62
Batch: 100; loss: 1.25; acc: 0.66
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 1.33; acc: 0.64
Batch: 160; loss: 1.25; acc: 0.7
Batch: 180; loss: 1.29; acc: 0.69
Batch: 200; loss: 1.28; acc: 0.64
Batch: 220; loss: 1.22; acc: 0.66
Batch: 240; loss: 1.17; acc: 0.67
Batch: 260; loss: 1.35; acc: 0.64
Batch: 280; loss: 1.26; acc: 0.61
Batch: 300; loss: 1.22; acc: 0.66
Batch: 320; loss: 1.39; acc: 0.55
Batch: 340; loss: 1.32; acc: 0.66
Batch: 360; loss: 1.26; acc: 0.67
Batch: 380; loss: 1.25; acc: 0.62
Batch: 400; loss: 1.14; acc: 0.72
Batch: 420; loss: 1.14; acc: 0.69
Batch: 440; loss: 1.2; acc: 0.73
Batch: 460; loss: 1.29; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.66
Batch: 500; loss: 1.2; acc: 0.75
Batch: 520; loss: 1.12; acc: 0.77
Batch: 540; loss: 1.05; acc: 0.78
Batch: 560; loss: 1.39; acc: 0.59
Batch: 580; loss: 1.12; acc: 0.73
Batch: 600; loss: 1.25; acc: 0.62
Batch: 620; loss: 1.24; acc: 0.66
Batch: 640; loss: 1.2; acc: 0.73
Batch: 660; loss: 1.34; acc: 0.61
Batch: 680; loss: 1.22; acc: 0.58
Batch: 700; loss: 1.14; acc: 0.72
Batch: 720; loss: 1.16; acc: 0.67
Batch: 740; loss: 1.16; acc: 0.72
Batch: 760; loss: 1.07; acc: 0.8
Batch: 780; loss: 1.27; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.66 

0.0001662785653024912
0.00015926490596029907
Batch: 0; loss: 1.32; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.58
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 1.26; acc: 0.62
Batch: 80; loss: 1.19; acc: 0.66
Batch: 100; loss: 1.29; acc: 0.73
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.1885285472414295; val_accuracy: 0.6876990445859873 

The current subspace-distance is: 0.00015926490596029907 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.62
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.25; acc: 0.64
Batch: 80; loss: 1.33; acc: 0.56
Batch: 100; loss: 1.18; acc: 0.7
Batch: 120; loss: 1.21; acc: 0.72
Batch: 140; loss: 1.16; acc: 0.66
Batch: 160; loss: 1.29; acc: 0.61
Batch: 180; loss: 1.05; acc: 0.75
Batch: 200; loss: 1.17; acc: 0.61
Batch: 220; loss: 1.13; acc: 0.7
Batch: 240; loss: 1.24; acc: 0.66
Batch: 260; loss: 1.27; acc: 0.62
Batch: 280; loss: 1.37; acc: 0.52
Batch: 300; loss: 1.24; acc: 0.66
Batch: 320; loss: 1.06; acc: 0.8
Batch: 340; loss: 1.21; acc: 0.72
Batch: 360; loss: 1.32; acc: 0.61
Batch: 380; loss: 1.27; acc: 0.61
Batch: 400; loss: 1.35; acc: 0.58
Batch: 420; loss: 1.3; acc: 0.56
Batch: 440; loss: 1.12; acc: 0.7
Batch: 460; loss: 1.28; acc: 0.64
Batch: 480; loss: 1.29; acc: 0.58
Batch: 500; loss: 1.09; acc: 0.77
Batch: 520; loss: 1.08; acc: 0.75
Batch: 540; loss: 1.11; acc: 0.67
Batch: 560; loss: 1.39; acc: 0.64
Batch: 580; loss: 1.17; acc: 0.73
Batch: 600; loss: 1.24; acc: 0.67
Batch: 620; loss: 1.31; acc: 0.61
Batch: 640; loss: 1.27; acc: 0.58
Batch: 660; loss: 1.33; acc: 0.66
Batch: 680; loss: 1.25; acc: 0.62
Batch: 700; loss: 1.25; acc: 0.67
Batch: 720; loss: 1.1; acc: 0.72
Batch: 740; loss: 1.31; acc: 0.53
Batch: 760; loss: 1.12; acc: 0.77
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.21; train_accuracy: 0.67 

0.00017502812261227518
0.00017099904653150588
Batch: 0; loss: 1.29; acc: 0.64
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 1.22; acc: 0.61
Batch: 80; loss: 1.17; acc: 0.66
Batch: 100; loss: 1.23; acc: 0.7
Batch: 120; loss: 1.31; acc: 0.62
Batch: 140; loss: 0.94; acc: 0.77
Val Epoch over. val_loss: 1.1421615375075371; val_accuracy: 0.6973527070063694 

The current subspace-distance is: 0.00017099904653150588 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.7
Batch: 20; loss: 1.28; acc: 0.64
Batch: 40; loss: 0.99; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.23; acc: 0.62
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 1.14; acc: 0.67
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.22; acc: 0.62
Batch: 200; loss: 1.31; acc: 0.55
Batch: 220; loss: 1.1; acc: 0.73
Batch: 240; loss: 1.2; acc: 0.69
Batch: 260; loss: 1.29; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.62
Batch: 300; loss: 1.23; acc: 0.7
Batch: 320; loss: 1.09; acc: 0.75
Batch: 340; loss: 0.98; acc: 0.75
Batch: 360; loss: 1.25; acc: 0.62
Batch: 380; loss: 1.28; acc: 0.61
Batch: 400; loss: 1.41; acc: 0.61
Batch: 420; loss: 1.13; acc: 0.77
Batch: 440; loss: 1.23; acc: 0.69
Batch: 460; loss: 0.99; acc: 0.78
Batch: 480; loss: 1.1; acc: 0.67
Batch: 500; loss: 1.15; acc: 0.66
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.0; acc: 0.84
Batch: 560; loss: 1.12; acc: 0.73
Batch: 580; loss: 1.11; acc: 0.7
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 1.28; acc: 0.66
Batch: 640; loss: 1.16; acc: 0.69
Batch: 660; loss: 1.08; acc: 0.8
Batch: 680; loss: 1.29; acc: 0.61
Batch: 700; loss: 1.25; acc: 0.67
Batch: 720; loss: 1.41; acc: 0.53
Batch: 740; loss: 1.18; acc: 0.64
Batch: 760; loss: 1.1; acc: 0.73
Batch: 780; loss: 1.45; acc: 0.56
Train Epoch over. train_loss: 1.19; train_accuracy: 0.68 

0.00017899622616823763
0.00017174685490317643
Batch: 0; loss: 1.31; acc: 0.61
Batch: 20; loss: 1.45; acc: 0.53
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 1.24; acc: 0.56
Batch: 80; loss: 1.18; acc: 0.66
Batch: 100; loss: 1.24; acc: 0.72
Batch: 120; loss: 1.31; acc: 0.61
Batch: 140; loss: 0.93; acc: 0.77
Val Epoch over. val_loss: 1.1456883857204656; val_accuracy: 0.6886942675159236 

The current subspace-distance is: 0.00017174685490317643 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 1.12; acc: 0.69
Batch: 60; loss: 1.15; acc: 0.77
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.66
Batch: 120; loss: 1.16; acc: 0.77
Batch: 140; loss: 1.12; acc: 0.67
Batch: 160; loss: 1.31; acc: 0.66
Batch: 180; loss: 1.26; acc: 0.67
Batch: 200; loss: 1.3; acc: 0.62
Batch: 220; loss: 1.07; acc: 0.77
Batch: 240; loss: 1.28; acc: 0.59
Batch: 260; loss: 1.19; acc: 0.67
Batch: 280; loss: 1.22; acc: 0.59
Batch: 300; loss: 1.09; acc: 0.72
Batch: 320; loss: 1.25; acc: 0.64
Batch: 340; loss: 1.11; acc: 0.67
Batch: 360; loss: 1.17; acc: 0.66
Batch: 380; loss: 1.26; acc: 0.62
Batch: 400; loss: 1.19; acc: 0.7
Batch: 420; loss: 1.16; acc: 0.73
Batch: 440; loss: 1.14; acc: 0.72
Batch: 460; loss: 1.03; acc: 0.77
Batch: 480; loss: 0.97; acc: 0.84
Batch: 500; loss: 1.14; acc: 0.64
Batch: 520; loss: 1.09; acc: 0.69
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.03; acc: 0.7
Batch: 600; loss: 1.45; acc: 0.53
Batch: 620; loss: 1.15; acc: 0.7
Batch: 640; loss: 1.16; acc: 0.7
Batch: 660; loss: 1.16; acc: 0.75
Batch: 680; loss: 1.26; acc: 0.61
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 1.22; acc: 0.69
Batch: 740; loss: 1.18; acc: 0.69
Batch: 760; loss: 1.27; acc: 0.67
Batch: 780; loss: 1.14; acc: 0.75
Train Epoch over. train_loss: 1.18; train_accuracy: 0.68 

0.00018103561887983233
0.00017402649973519146
Batch: 0; loss: 1.29; acc: 0.64
Batch: 20; loss: 1.41; acc: 0.55
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 1.21; acc: 0.58
Batch: 80; loss: 1.15; acc: 0.66
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.29; acc: 0.61
Batch: 140; loss: 0.92; acc: 0.78
Val Epoch over. val_loss: 1.1253427267074585; val_accuracy: 0.698546974522293 

The current subspace-distance is: 0.00017402649973519146 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 1.28; acc: 0.56
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.81
Batch: 160; loss: 1.16; acc: 0.69
Batch: 180; loss: 1.22; acc: 0.7
Batch: 200; loss: 0.96; acc: 0.8
Batch: 220; loss: 1.16; acc: 0.69
Batch: 240; loss: 1.23; acc: 0.64
Batch: 260; loss: 1.16; acc: 0.66
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.2; acc: 0.7
Batch: 320; loss: 1.27; acc: 0.66
Batch: 340; loss: 1.32; acc: 0.64
Batch: 360; loss: 1.14; acc: 0.66
Batch: 380; loss: 1.23; acc: 0.69
Batch: 400; loss: 1.27; acc: 0.66
Batch: 420; loss: 0.98; acc: 0.81
Batch: 440; loss: 1.28; acc: 0.61
Batch: 460; loss: 1.31; acc: 0.64
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.07; acc: 0.75
Batch: 540; loss: 1.33; acc: 0.64
Batch: 560; loss: 1.25; acc: 0.58
Batch: 580; loss: 1.3; acc: 0.62
Batch: 600; loss: 1.2; acc: 0.66
Batch: 620; loss: 1.07; acc: 0.72
Batch: 640; loss: 1.21; acc: 0.67
Batch: 660; loss: 1.08; acc: 0.72
Batch: 680; loss: 1.22; acc: 0.64
Batch: 700; loss: 1.03; acc: 0.73
Batch: 720; loss: 1.33; acc: 0.56
Batch: 740; loss: 1.09; acc: 0.77
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 1.4; acc: 0.53
Train Epoch over. train_loss: 1.17; train_accuracy: 0.68 

0.00018528517102822661
0.00017581271822564304
Batch: 0; loss: 1.3; acc: 0.64
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 1.21; acc: 0.56
Batch: 80; loss: 1.15; acc: 0.66
Batch: 100; loss: 1.2; acc: 0.69
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 0.91; acc: 0.78
Val Epoch over. val_loss: 1.1289782087514355; val_accuracy: 0.6982484076433121 

The current subspace-distance is: 0.00017581271822564304 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 1.1; acc: 0.72
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 1.08; acc: 0.77
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.07; acc: 0.75
Batch: 140; loss: 1.43; acc: 0.59
Batch: 160; loss: 0.96; acc: 0.72
Batch: 180; loss: 0.99; acc: 0.8
Batch: 200; loss: 1.29; acc: 0.62
Batch: 220; loss: 1.1; acc: 0.69
Batch: 240; loss: 1.17; acc: 0.69
Batch: 260; loss: 1.08; acc: 0.77
Batch: 280; loss: 1.14; acc: 0.69
Batch: 300; loss: 1.1; acc: 0.73
Batch: 320; loss: 1.33; acc: 0.5
Batch: 340; loss: 1.14; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.14; acc: 0.7
Batch: 400; loss: 1.33; acc: 0.59
Batch: 420; loss: 1.01; acc: 0.75
Batch: 440; loss: 1.23; acc: 0.64
Batch: 460; loss: 1.14; acc: 0.67
Batch: 480; loss: 1.35; acc: 0.66
Batch: 500; loss: 0.95; acc: 0.77
Batch: 520; loss: 0.9; acc: 0.81
Batch: 540; loss: 1.19; acc: 0.64
Batch: 560; loss: 1.17; acc: 0.7
Batch: 580; loss: 1.13; acc: 0.66
Batch: 600; loss: 1.19; acc: 0.69
Batch: 620; loss: 1.15; acc: 0.7
Batch: 640; loss: 1.19; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.7
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.05; acc: 0.75
Batch: 720; loss: 1.11; acc: 0.72
Batch: 740; loss: 1.24; acc: 0.58
Batch: 760; loss: 1.25; acc: 0.67
Batch: 780; loss: 1.18; acc: 0.72
Train Epoch over. train_loss: 1.16; train_accuracy: 0.68 

0.00018495241238269955
0.00017971215129364282
Batch: 0; loss: 1.27; acc: 0.64
Batch: 20; loss: 1.41; acc: 0.58
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 1.17; acc: 0.59
Batch: 80; loss: 1.12; acc: 0.67
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.88; acc: 0.77
Val Epoch over. val_loss: 1.093854281932685; val_accuracy: 0.7064092356687898 

The current subspace-distance is: 0.00017971215129364282 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.3; acc: 0.64
Batch: 40; loss: 1.17; acc: 0.7
Batch: 60; loss: 1.22; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 0.96; acc: 0.77
Batch: 120; loss: 1.17; acc: 0.64
Batch: 140; loss: 1.09; acc: 0.69
Batch: 160; loss: 1.08; acc: 0.69
Batch: 180; loss: 1.22; acc: 0.62
Batch: 200; loss: 1.0; acc: 0.8
Batch: 220; loss: 1.33; acc: 0.62
Batch: 240; loss: 1.14; acc: 0.64
Batch: 260; loss: 1.09; acc: 0.67
Batch: 280; loss: 1.23; acc: 0.67
Batch: 300; loss: 1.26; acc: 0.64
Batch: 320; loss: 1.22; acc: 0.67
Batch: 340; loss: 1.13; acc: 0.72
Batch: 360; loss: 0.99; acc: 0.72
Batch: 380; loss: 1.18; acc: 0.66
Batch: 400; loss: 1.24; acc: 0.61
Batch: 420; loss: 1.07; acc: 0.72
Batch: 440; loss: 0.97; acc: 0.72
Batch: 460; loss: 1.04; acc: 0.73
Batch: 480; loss: 0.98; acc: 0.73
Batch: 500; loss: 1.04; acc: 0.75
Batch: 520; loss: 1.27; acc: 0.64
Batch: 540; loss: 1.07; acc: 0.69
Batch: 560; loss: 1.1; acc: 0.72
Batch: 580; loss: 1.19; acc: 0.69
Batch: 600; loss: 1.04; acc: 0.75
Batch: 620; loss: 1.3; acc: 0.56
Batch: 640; loss: 1.17; acc: 0.72
Batch: 660; loss: 1.07; acc: 0.72
Batch: 680; loss: 1.32; acc: 0.59
Batch: 700; loss: 1.02; acc: 0.75
Batch: 720; loss: 1.21; acc: 0.59
Batch: 740; loss: 1.02; acc: 0.72
Batch: 760; loss: 1.11; acc: 0.72
Batch: 780; loss: 0.96; acc: 0.77
Train Epoch over. train_loss: 1.15; train_accuracy: 0.69 

0.00019288707699161023
0.00018390617333352566
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.42; acc: 0.56
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 1.19; acc: 0.56
Batch: 80; loss: 1.14; acc: 0.67
Batch: 100; loss: 1.18; acc: 0.72
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 0.89; acc: 0.78
Val Epoch over. val_loss: 1.1146226942918862; val_accuracy: 0.6995421974522293 

The current subspace-distance is: 0.00018390617333352566 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.66
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 1.11; acc: 0.7
Batch: 60; loss: 1.04; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.72
Batch: 100; loss: 1.19; acc: 0.72
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 1.11; acc: 0.72
Batch: 160; loss: 1.16; acc: 0.72
Batch: 180; loss: 1.11; acc: 0.72
Batch: 200; loss: 1.16; acc: 0.75
Batch: 220; loss: 0.98; acc: 0.8
Batch: 240; loss: 1.06; acc: 0.67
Batch: 260; loss: 1.1; acc: 0.7
Batch: 280; loss: 1.13; acc: 0.69
Batch: 300; loss: 1.12; acc: 0.67
Batch: 320; loss: 1.14; acc: 0.73
Batch: 340; loss: 1.16; acc: 0.67
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.15; acc: 0.75
Batch: 400; loss: 1.12; acc: 0.64
Batch: 420; loss: 1.18; acc: 0.61
Batch: 440; loss: 1.15; acc: 0.67
Batch: 460; loss: 1.06; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.19; acc: 0.61
Batch: 520; loss: 1.14; acc: 0.67
Batch: 540; loss: 1.25; acc: 0.58
Batch: 560; loss: 0.97; acc: 0.8
Batch: 580; loss: 1.13; acc: 0.55
Batch: 600; loss: 1.26; acc: 0.62
Batch: 620; loss: 1.01; acc: 0.77
Batch: 640; loss: 1.22; acc: 0.62
Batch: 660; loss: 1.03; acc: 0.78
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 1.08; acc: 0.75
Batch: 740; loss: 1.01; acc: 0.73
Batch: 760; loss: 1.04; acc: 0.78
Batch: 780; loss: 1.24; acc: 0.64
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

0.0001927844132296741
0.00018284944235347211
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.39; acc: 0.59
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 1.13; acc: 0.67
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.89; acc: 0.77
Val Epoch over. val_loss: 1.1050200963475902; val_accuracy: 0.7018312101910829 

The current subspace-distance is: 0.00018284944235347211 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.1; acc: 0.7
Batch: 40; loss: 1.16; acc: 0.69
Batch: 60; loss: 1.25; acc: 0.66
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.34; acc: 0.64
Batch: 120; loss: 1.09; acc: 0.75
Batch: 140; loss: 1.2; acc: 0.7
Batch: 160; loss: 1.16; acc: 0.7
Batch: 180; loss: 1.21; acc: 0.64
Batch: 200; loss: 1.06; acc: 0.72
Batch: 220; loss: 1.07; acc: 0.64
Batch: 240; loss: 1.25; acc: 0.64
Batch: 260; loss: 1.14; acc: 0.75
Batch: 280; loss: 1.15; acc: 0.62
Batch: 300; loss: 1.15; acc: 0.66
Batch: 320; loss: 1.23; acc: 0.66
Batch: 340; loss: 1.05; acc: 0.7
Batch: 360; loss: 1.18; acc: 0.73
Batch: 380; loss: 0.99; acc: 0.73
Batch: 400; loss: 1.13; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.72
Batch: 440; loss: 1.12; acc: 0.72
Batch: 460; loss: 0.89; acc: 0.84
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 1.08; acc: 0.77
Batch: 520; loss: 1.12; acc: 0.7
Batch: 540; loss: 1.18; acc: 0.67
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.21; acc: 0.69
Batch: 600; loss: 1.12; acc: 0.66
Batch: 620; loss: 0.86; acc: 0.83
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.69
Batch: 680; loss: 1.14; acc: 0.73
Batch: 700; loss: 1.19; acc: 0.62
Batch: 720; loss: 1.0; acc: 0.75
Batch: 740; loss: 1.04; acc: 0.72
Batch: 760; loss: 1.19; acc: 0.62
Batch: 780; loss: 1.26; acc: 0.62
Train Epoch over. train_loss: 1.14; train_accuracy: 0.69 

0.00019450770923867822
0.000187504876521416
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.39; acc: 0.58
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 1.17; acc: 0.56
Batch: 80; loss: 1.11; acc: 0.67
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.24; acc: 0.69
Batch: 140; loss: 0.86; acc: 0.78
Val Epoch over. val_loss: 1.0943198731750439; val_accuracy: 0.7026273885350318 

The current subspace-distance is: 0.000187504876521416 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.08; acc: 0.78
Batch: 40; loss: 1.08; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.55
Batch: 80; loss: 1.01; acc: 0.73
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.69
Batch: 140; loss: 1.16; acc: 0.62
Batch: 160; loss: 1.15; acc: 0.69
Batch: 180; loss: 1.12; acc: 0.66
Batch: 200; loss: 1.18; acc: 0.69
Batch: 220; loss: 1.13; acc: 0.7
Batch: 240; loss: 1.16; acc: 0.64
Batch: 260; loss: 1.2; acc: 0.7
Batch: 280; loss: 1.14; acc: 0.72
Batch: 300; loss: 1.14; acc: 0.69
Batch: 320; loss: 1.09; acc: 0.78
Batch: 340; loss: 1.27; acc: 0.61
Batch: 360; loss: 1.09; acc: 0.7
Batch: 380; loss: 1.42; acc: 0.52
Batch: 400; loss: 1.17; acc: 0.64
Batch: 420; loss: 1.13; acc: 0.69
Batch: 440; loss: 1.27; acc: 0.64
Batch: 460; loss: 1.07; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.72
Batch: 500; loss: 1.05; acc: 0.72
Batch: 520; loss: 0.86; acc: 0.83
Batch: 540; loss: 1.18; acc: 0.64
Batch: 560; loss: 1.21; acc: 0.69
Batch: 580; loss: 0.97; acc: 0.77
Batch: 600; loss: 1.04; acc: 0.78
Batch: 620; loss: 1.16; acc: 0.67
Batch: 640; loss: 1.11; acc: 0.72
Batch: 660; loss: 0.99; acc: 0.75
Batch: 680; loss: 1.1; acc: 0.64
Batch: 700; loss: 1.25; acc: 0.66
Batch: 720; loss: 1.15; acc: 0.64
Batch: 740; loss: 1.14; acc: 0.67
Batch: 760; loss: 1.1; acc: 0.72
Batch: 780; loss: 1.17; acc: 0.66
Train Epoch over. train_loss: 1.13; train_accuracy: 0.69 

0.00020023486285936087
0.00019085902022197843
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.39; acc: 0.58
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 1.16; acc: 0.55
Batch: 80; loss: 1.11; acc: 0.69
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.24; acc: 0.69
Batch: 140; loss: 0.84; acc: 0.78
Val Epoch over. val_loss: 1.0831911476554386; val_accuracy: 0.7046178343949044 

The current subspace-distance is: 0.00019085902022197843 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 1.17; acc: 0.66
Batch: 60; loss: 1.13; acc: 0.66
Batch: 80; loss: 1.15; acc: 0.66
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.01; acc: 0.8
Batch: 160; loss: 1.25; acc: 0.69
Batch: 180; loss: 1.05; acc: 0.72
Batch: 200; loss: 1.14; acc: 0.69
Batch: 220; loss: 1.2; acc: 0.59
Batch: 240; loss: 1.24; acc: 0.67
Batch: 260; loss: 1.21; acc: 0.58
Batch: 280; loss: 1.06; acc: 0.69
Batch: 300; loss: 0.99; acc: 0.75
Batch: 320; loss: 1.05; acc: 0.77
Batch: 340; loss: 1.28; acc: 0.59
Batch: 360; loss: 1.06; acc: 0.69
Batch: 380; loss: 1.25; acc: 0.62
Batch: 400; loss: 0.96; acc: 0.78
Batch: 420; loss: 1.2; acc: 0.66
Batch: 440; loss: 0.98; acc: 0.72
Batch: 460; loss: 1.21; acc: 0.72
Batch: 480; loss: 1.1; acc: 0.7
Batch: 500; loss: 0.97; acc: 0.86
Batch: 520; loss: 1.1; acc: 0.67
Batch: 540; loss: 1.12; acc: 0.72
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.14; acc: 0.75
Batch: 620; loss: 1.17; acc: 0.72
Batch: 640; loss: 1.07; acc: 0.75
Batch: 660; loss: 1.29; acc: 0.62
Batch: 680; loss: 1.02; acc: 0.72
Batch: 700; loss: 0.92; acc: 0.73
Batch: 720; loss: 1.09; acc: 0.73
Batch: 740; loss: 1.38; acc: 0.56
Batch: 760; loss: 1.09; acc: 0.73
Batch: 780; loss: 1.24; acc: 0.64
Train Epoch over. train_loss: 1.13; train_accuracy: 0.69 

0.00020234196563251317
0.00019251579942647368
Batch: 0; loss: 1.25; acc: 0.64
Batch: 20; loss: 1.38; acc: 0.58
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 1.14; acc: 0.61
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.83; acc: 0.78
Val Epoch over. val_loss: 1.065950731942608; val_accuracy: 0.7129777070063694 

The current subspace-distance is: 0.00019251579942647368 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.28; acc: 0.56
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 1.09; acc: 0.61
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 0.95; acc: 0.84
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.7
Batch: 160; loss: 0.99; acc: 0.77
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 1.09; acc: 0.7
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.1; acc: 0.69
Batch: 260; loss: 1.07; acc: 0.72
Batch: 280; loss: 1.0; acc: 0.78
Batch: 300; loss: 1.03; acc: 0.72
Batch: 320; loss: 1.21; acc: 0.64
Batch: 340; loss: 1.18; acc: 0.64
Batch: 360; loss: 1.22; acc: 0.66
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.17; acc: 0.61
Batch: 420; loss: 1.02; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.66
Batch: 460; loss: 1.07; acc: 0.73
Batch: 480; loss: 1.07; acc: 0.75
Batch: 500; loss: 1.25; acc: 0.62
Batch: 520; loss: 1.03; acc: 0.83
Batch: 540; loss: 1.15; acc: 0.66
Batch: 560; loss: 1.17; acc: 0.67
Batch: 580; loss: 1.16; acc: 0.7
Batch: 600; loss: 1.1; acc: 0.7
Batch: 620; loss: 1.29; acc: 0.64
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.21; acc: 0.66
Batch: 680; loss: 1.27; acc: 0.64
Batch: 700; loss: 0.99; acc: 0.8
Batch: 720; loss: 1.05; acc: 0.69
Batch: 740; loss: 1.42; acc: 0.56
Batch: 760; loss: 1.21; acc: 0.62
Batch: 780; loss: 1.35; acc: 0.59
Train Epoch over. train_loss: 1.12; train_accuracy: 0.69 

0.00020200795552227646
0.00019482428615447134
Batch: 0; loss: 1.26; acc: 0.62
Batch: 20; loss: 1.37; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 1.14; acc: 0.59
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 0.83; acc: 0.78
Val Epoch over. val_loss: 1.0751757094055225; val_accuracy: 0.7051154458598726 

The current subspace-distance is: 0.00019482428615447134 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 1.0; acc: 0.77
Batch: 40; loss: 1.16; acc: 0.72
Batch: 60; loss: 1.03; acc: 0.69
Batch: 80; loss: 1.24; acc: 0.67
Batch: 100; loss: 0.97; acc: 0.77
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 1.12; acc: 0.7
Batch: 160; loss: 1.33; acc: 0.61
Batch: 180; loss: 1.11; acc: 0.66
Batch: 200; loss: 1.24; acc: 0.61
Batch: 220; loss: 0.99; acc: 0.72
Batch: 240; loss: 0.9; acc: 0.8
Batch: 260; loss: 1.2; acc: 0.64
Batch: 280; loss: 1.33; acc: 0.56
Batch: 300; loss: 0.97; acc: 0.73
Batch: 320; loss: 1.14; acc: 0.67
Batch: 340; loss: 0.85; acc: 0.81
Batch: 360; loss: 1.22; acc: 0.66
Batch: 380; loss: 1.09; acc: 0.72
Batch: 400; loss: 1.1; acc: 0.7
Batch: 420; loss: 1.11; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.69
Batch: 480; loss: 1.24; acc: 0.67
Batch: 500; loss: 1.28; acc: 0.58
Batch: 520; loss: 1.19; acc: 0.66
Batch: 540; loss: 0.97; acc: 0.75
Batch: 560; loss: 1.23; acc: 0.64
Batch: 580; loss: 1.14; acc: 0.64
Batch: 600; loss: 1.07; acc: 0.77
Batch: 620; loss: 1.24; acc: 0.59
Batch: 640; loss: 0.95; acc: 0.81
Batch: 660; loss: 1.01; acc: 0.7
Batch: 680; loss: 1.23; acc: 0.59
Batch: 700; loss: 1.07; acc: 0.67
Batch: 720; loss: 1.22; acc: 0.58
Batch: 740; loss: 1.04; acc: 0.7
Batch: 760; loss: 1.02; acc: 0.69
Batch: 780; loss: 0.91; acc: 0.78
Train Epoch over. train_loss: 1.12; train_accuracy: 0.69 

0.00020026369020342827
0.0001919769711093977
Batch: 0; loss: 1.25; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.55
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 1.14; acc: 0.59
Batch: 80; loss: 1.08; acc: 0.69
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.82; acc: 0.78
Val Epoch over. val_loss: 1.0669282511541038; val_accuracy: 0.7091958598726115 

The current subspace-distance is: 0.0001919769711093977 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 1.13; acc: 0.69
Batch: 60; loss: 1.2; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.18; acc: 0.72
Batch: 120; loss: 1.01; acc: 0.78
Batch: 140; loss: 1.05; acc: 0.72
Batch: 160; loss: 1.06; acc: 0.77
Batch: 180; loss: 1.17; acc: 0.7
Batch: 200; loss: 1.1; acc: 0.7
Batch: 220; loss: 0.97; acc: 0.81
Batch: 240; loss: 1.14; acc: 0.67
Batch: 260; loss: 1.24; acc: 0.64
Batch: 280; loss: 1.18; acc: 0.67
Batch: 300; loss: 1.08; acc: 0.77
Batch: 320; loss: 1.09; acc: 0.72
Batch: 340; loss: 0.84; acc: 0.83
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.13; acc: 0.7
Batch: 400; loss: 1.14; acc: 0.69
Batch: 420; loss: 1.03; acc: 0.75
Batch: 440; loss: 1.14; acc: 0.72
Batch: 460; loss: 1.15; acc: 0.69
Batch: 480; loss: 1.06; acc: 0.7
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 1.03; acc: 0.73
Batch: 540; loss: 1.13; acc: 0.67
Batch: 560; loss: 1.23; acc: 0.62
Batch: 580; loss: 1.1; acc: 0.72
Batch: 600; loss: 1.23; acc: 0.58
Batch: 620; loss: 0.97; acc: 0.78
Batch: 640; loss: 1.27; acc: 0.61
Batch: 660; loss: 1.17; acc: 0.69
Batch: 680; loss: 1.14; acc: 0.66
Batch: 700; loss: 1.03; acc: 0.72
Batch: 720; loss: 1.16; acc: 0.69
Batch: 740; loss: 1.08; acc: 0.75
Batch: 760; loss: 1.24; acc: 0.58
Batch: 780; loss: 1.06; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00020046929421368986
0.00019412992696743459
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.36; acc: 0.56
Batch: 40; loss: 0.73; acc: 0.88
Batch: 60; loss: 1.12; acc: 0.62
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.11; acc: 0.77
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.82; acc: 0.78
Val Epoch over. val_loss: 1.0604738782925212; val_accuracy: 0.7131767515923567 

The current subspace-distance is: 0.00019412992696743459 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.99; acc: 0.72
Batch: 20; loss: 1.11; acc: 0.69
Batch: 40; loss: 1.08; acc: 0.7
Batch: 60; loss: 1.13; acc: 0.69
Batch: 80; loss: 1.13; acc: 0.67
Batch: 100; loss: 1.02; acc: 0.78
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.08; acc: 0.72
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.03; acc: 0.69
Batch: 220; loss: 0.98; acc: 0.78
Batch: 240; loss: 1.05; acc: 0.69
Batch: 260; loss: 1.09; acc: 0.69
Batch: 280; loss: 1.11; acc: 0.73
Batch: 300; loss: 1.06; acc: 0.69
Batch: 320; loss: 1.12; acc: 0.62
Batch: 340; loss: 1.1; acc: 0.67
Batch: 360; loss: 0.98; acc: 0.7
Batch: 380; loss: 1.06; acc: 0.73
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 1.12; acc: 0.7
Batch: 440; loss: 1.1; acc: 0.67
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.15; acc: 0.69
Batch: 500; loss: 0.94; acc: 0.78
Batch: 520; loss: 1.15; acc: 0.67
Batch: 540; loss: 1.12; acc: 0.64
Batch: 560; loss: 1.23; acc: 0.64
Batch: 580; loss: 1.18; acc: 0.62
Batch: 600; loss: 1.13; acc: 0.62
Batch: 620; loss: 1.09; acc: 0.64
Batch: 640; loss: 1.1; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.7
Batch: 680; loss: 1.09; acc: 0.7
Batch: 700; loss: 1.15; acc: 0.62
Batch: 720; loss: 0.97; acc: 0.72
Batch: 740; loss: 1.08; acc: 0.66
Batch: 760; loss: 1.18; acc: 0.69
Batch: 780; loss: 1.06; acc: 0.75
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.0002031061303569004
0.0001938080386025831
Batch: 0; loss: 1.25; acc: 0.64
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 1.12; acc: 0.61
Batch: 80; loss: 1.07; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 0.81; acc: 0.78
Val Epoch over. val_loss: 1.054774085047898; val_accuracy: 0.7121815286624203 

The current subspace-distance is: 0.0001938080386025831 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.21; acc: 0.7
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 0.93; acc: 0.8
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.1; acc: 0.75
Batch: 140; loss: 1.17; acc: 0.66
Batch: 160; loss: 1.08; acc: 0.73
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 1.29; acc: 0.64
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.3; acc: 0.56
Batch: 260; loss: 1.07; acc: 0.7
Batch: 280; loss: 0.95; acc: 0.73
Batch: 300; loss: 0.98; acc: 0.72
Batch: 320; loss: 1.1; acc: 0.69
Batch: 340; loss: 1.13; acc: 0.66
Batch: 360; loss: 0.87; acc: 0.83
Batch: 380; loss: 1.15; acc: 0.72
Batch: 400; loss: 1.15; acc: 0.69
Batch: 420; loss: 1.22; acc: 0.61
Batch: 440; loss: 1.06; acc: 0.69
Batch: 460; loss: 1.15; acc: 0.69
Batch: 480; loss: 1.23; acc: 0.66
Batch: 500; loss: 1.19; acc: 0.66
Batch: 520; loss: 1.18; acc: 0.64
Batch: 540; loss: 1.1; acc: 0.69
Batch: 560; loss: 0.95; acc: 0.8
Batch: 580; loss: 1.15; acc: 0.64
Batch: 600; loss: 0.87; acc: 0.83
Batch: 620; loss: 1.17; acc: 0.66
Batch: 640; loss: 1.16; acc: 0.7
Batch: 660; loss: 1.1; acc: 0.7
Batch: 680; loss: 1.08; acc: 0.73
Batch: 700; loss: 1.15; acc: 0.69
Batch: 720; loss: 1.12; acc: 0.66
Batch: 740; loss: 1.02; acc: 0.72
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.27; acc: 0.62
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00020393809245433658
0.0001956282212631777
Batch: 0; loss: 1.25; acc: 0.61
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 1.13; acc: 0.59
Batch: 80; loss: 1.08; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.81; acc: 0.78
Val Epoch over. val_loss: 1.066692592611738; val_accuracy: 0.7094944267515924 

The current subspace-distance is: 0.0001956282212631777 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.01; acc: 0.78
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 1.01; acc: 0.72
Batch: 60; loss: 1.06; acc: 0.67
Batch: 80; loss: 0.94; acc: 0.75
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.22; acc: 0.58
Batch: 140; loss: 1.03; acc: 0.75
Batch: 160; loss: 1.15; acc: 0.67
Batch: 180; loss: 1.13; acc: 0.64
Batch: 200; loss: 1.08; acc: 0.67
Batch: 220; loss: 1.01; acc: 0.73
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.13; acc: 0.69
Batch: 280; loss: 1.06; acc: 0.67
Batch: 300; loss: 1.04; acc: 0.78
Batch: 320; loss: 1.03; acc: 0.75
Batch: 340; loss: 1.04; acc: 0.73
Batch: 360; loss: 1.32; acc: 0.61
Batch: 380; loss: 1.34; acc: 0.64
Batch: 400; loss: 1.29; acc: 0.59
Batch: 420; loss: 1.03; acc: 0.7
Batch: 440; loss: 1.39; acc: 0.66
Batch: 460; loss: 1.21; acc: 0.59
Batch: 480; loss: 1.19; acc: 0.64
Batch: 500; loss: 1.06; acc: 0.72
Batch: 520; loss: 0.97; acc: 0.8
Batch: 540; loss: 1.13; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.66
Batch: 580; loss: 1.12; acc: 0.66
Batch: 600; loss: 1.11; acc: 0.69
Batch: 620; loss: 0.97; acc: 0.67
Batch: 640; loss: 0.97; acc: 0.75
Batch: 660; loss: 0.94; acc: 0.75
Batch: 680; loss: 1.08; acc: 0.64
Batch: 700; loss: 1.26; acc: 0.64
Batch: 720; loss: 0.92; acc: 0.75
Batch: 740; loss: 1.01; acc: 0.75
Batch: 760; loss: 1.17; acc: 0.69
Batch: 780; loss: 1.12; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00021126253705006093
0.00020087711163796484
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.88
Batch: 60; loss: 1.12; acc: 0.59
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.82; acc: 0.78
Val Epoch over. val_loss: 1.0614302560781976; val_accuracy: 0.7091958598726115 

The current subspace-distance is: 0.00020087711163796484 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 1.1; acc: 0.7
Batch: 40; loss: 1.02; acc: 0.73
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 1.29; acc: 0.59
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 0.98; acc: 0.78
Batch: 160; loss: 1.42; acc: 0.58
Batch: 180; loss: 1.08; acc: 0.69
Batch: 200; loss: 1.08; acc: 0.67
Batch: 220; loss: 0.93; acc: 0.75
Batch: 240; loss: 1.21; acc: 0.62
Batch: 260; loss: 1.16; acc: 0.7
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.05; acc: 0.72
Batch: 320; loss: 1.17; acc: 0.7
Batch: 340; loss: 1.01; acc: 0.72
Batch: 360; loss: 1.01; acc: 0.7
Batch: 380; loss: 1.03; acc: 0.75
Batch: 400; loss: 1.26; acc: 0.64
Batch: 420; loss: 1.08; acc: 0.67
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 1.2; acc: 0.61
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 1.01; acc: 0.75
Batch: 520; loss: 1.24; acc: 0.66
Batch: 540; loss: 1.2; acc: 0.66
Batch: 560; loss: 1.25; acc: 0.64
Batch: 580; loss: 1.08; acc: 0.72
Batch: 600; loss: 1.25; acc: 0.69
Batch: 620; loss: 1.11; acc: 0.62
Batch: 640; loss: 0.91; acc: 0.8
Batch: 660; loss: 1.04; acc: 0.75
Batch: 680; loss: 1.18; acc: 0.64
Batch: 700; loss: 1.09; acc: 0.78
Batch: 720; loss: 1.1; acc: 0.7
Batch: 740; loss: 0.95; acc: 0.8
Batch: 760; loss: 1.32; acc: 0.58
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00020817197219002992
0.0001993691548705101
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.34; acc: 0.58
Batch: 40; loss: 0.72; acc: 0.89
Batch: 60; loss: 1.11; acc: 0.61
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 0.81; acc: 0.78
Val Epoch over. val_loss: 1.049933671951294; val_accuracy: 0.7164609872611465 

The current subspace-distance is: 0.0001993691548705101 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 1.13; acc: 0.69
Batch: 60; loss: 1.27; acc: 0.66
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 1.03; acc: 0.73
Batch: 120; loss: 1.07; acc: 0.66
Batch: 140; loss: 1.32; acc: 0.61
Batch: 160; loss: 1.33; acc: 0.58
Batch: 180; loss: 1.04; acc: 0.75
Batch: 200; loss: 1.07; acc: 0.66
Batch: 220; loss: 1.09; acc: 0.66
Batch: 240; loss: 1.18; acc: 0.62
Batch: 260; loss: 1.33; acc: 0.55
Batch: 280; loss: 1.18; acc: 0.67
Batch: 300; loss: 0.96; acc: 0.77
Batch: 320; loss: 1.07; acc: 0.73
Batch: 340; loss: 1.1; acc: 0.7
Batch: 360; loss: 1.18; acc: 0.67
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 1.01; acc: 0.72
Batch: 420; loss: 0.93; acc: 0.78
Batch: 440; loss: 1.05; acc: 0.69
Batch: 460; loss: 1.05; acc: 0.72
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 0.95; acc: 0.81
Batch: 520; loss: 1.05; acc: 0.77
Batch: 540; loss: 0.94; acc: 0.78
Batch: 560; loss: 1.16; acc: 0.67
Batch: 580; loss: 1.06; acc: 0.72
Batch: 600; loss: 1.01; acc: 0.69
Batch: 620; loss: 1.1; acc: 0.69
Batch: 640; loss: 1.07; acc: 0.73
Batch: 660; loss: 1.11; acc: 0.67
Batch: 680; loss: 1.25; acc: 0.7
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 1.18; acc: 0.7
Batch: 740; loss: 1.22; acc: 0.62
Batch: 760; loss: 1.18; acc: 0.62
Batch: 780; loss: 0.93; acc: 0.73
Train Epoch over. train_loss: 1.11; train_accuracy: 0.69 

0.00020625701290555298
0.00019898224854841828
Batch: 0; loss: 1.25; acc: 0.64
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 0.72; acc: 0.89
Batch: 60; loss: 1.12; acc: 0.59
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 0.8; acc: 0.78
Val Epoch over. val_loss: 1.051579589676705; val_accuracy: 0.7110867834394905 

The current subspace-distance is: 0.00019898224854841828 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.04; acc: 0.69
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 1.18; acc: 0.69
Batch: 100; loss: 0.97; acc: 0.77
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 1.07; acc: 0.73
Batch: 160; loss: 0.94; acc: 0.72
Batch: 180; loss: 1.14; acc: 0.64
Batch: 200; loss: 1.04; acc: 0.77
Batch: 220; loss: 0.92; acc: 0.8
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.27; acc: 0.61
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 1.19; acc: 0.62
Batch: 320; loss: 1.03; acc: 0.75
Batch: 340; loss: 1.08; acc: 0.72
Batch: 360; loss: 1.06; acc: 0.67
Batch: 380; loss: 1.0; acc: 0.75
Batch: 400; loss: 1.21; acc: 0.69
Batch: 420; loss: 0.98; acc: 0.77
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 1.03; acc: 0.73
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 0.91; acc: 0.78
Batch: 540; loss: 0.94; acc: 0.81
Batch: 560; loss: 1.06; acc: 0.78
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.0; acc: 0.72
Batch: 640; loss: 1.11; acc: 0.67
Batch: 660; loss: 1.25; acc: 0.61
Batch: 680; loss: 0.91; acc: 0.81
Batch: 700; loss: 1.19; acc: 0.62
Batch: 720; loss: 0.8; acc: 0.84
Batch: 740; loss: 1.02; acc: 0.75
Batch: 760; loss: 1.18; acc: 0.69
Batch: 780; loss: 1.16; acc: 0.69
Train Epoch over. train_loss: 1.1; train_accuracy: 0.69 

0.00020678769215010107
0.00020135933300480247
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 1.34; acc: 0.58
Batch: 40; loss: 0.71; acc: 0.89
Batch: 60; loss: 1.11; acc: 0.62
Batch: 80; loss: 1.06; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 1.19; acc: 0.69
Batch: 140; loss: 0.8; acc: 0.78
Val Epoch over. val_loss: 1.0487985303447505; val_accuracy: 0.7132762738853503 

The current subspace-distance is: 0.00020135933300480247 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 1.06; acc: 0.69
Batch: 40; loss: 0.99; acc: 0.78
Batch: 60; loss: 1.45; acc: 0.58
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 1.15; acc: 0.62
Batch: 160; loss: 1.44; acc: 0.56
Batch: 180; loss: 1.06; acc: 0.69
Batch: 200; loss: 1.17; acc: 0.67
Batch: 220; loss: 1.1; acc: 0.64
Batch: 240; loss: 1.16; acc: 0.67
Batch: 260; loss: 1.1; acc: 0.73
Batch: 280; loss: 0.86; acc: 0.8
Batch: 300; loss: 1.29; acc: 0.59
Batch: 320; loss: 1.18; acc: 0.67
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.04; acc: 0.73
Batch: 380; loss: 1.3; acc: 0.61
Batch: 400; loss: 1.13; acc: 0.61
Batch: 420; loss: 1.04; acc: 0.75
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 1.03; acc: 0.73
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 1.05; acc: 0.72
Batch: 520; loss: 1.0; acc: 0.78
Batch: 540; loss: 1.36; acc: 0.66
Batch: 560; loss: 1.29; acc: 0.64
Batch: 580; loss: 1.1; acc: 0.7
Batch: 600; loss: 1.0; acc: 0.73
Batch: 620; loss: 0.99; acc: 0.77
Batch: 640; loss: 0.83; acc: 0.86
Batch: 660; loss: 1.19; acc: 0.69
Batch: 680; loss: 1.08; acc: 0.7
Batch: 700; loss: 1.34; acc: 0.56
Batch: 720; loss: 1.08; acc: 0.72
Batch: 740; loss: 0.95; acc: 0.78
Batch: 760; loss: 0.9; acc: 0.84
Batch: 780; loss: 1.17; acc: 0.62
Train Epoch over. train_loss: 1.1; train_accuracy: 0.69 

0.00021197652677074075
0.00020366892567835748
Batch: 0; loss: 1.24; acc: 0.67
Batch: 20; loss: 1.35; acc: 0.56
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 1.11; acc: 0.58
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.77
Batch: 120; loss: 1.21; acc: 0.7
Batch: 140; loss: 0.8; acc: 0.78
Val Epoch over. val_loss: 1.0512276534821576; val_accuracy: 0.7125796178343949 

The current subspace-distance is: 0.00020366892567835748 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.23; acc: 0.64
Batch: 40; loss: 1.25; acc: 0.66
Batch: 60; loss: 1.19; acc: 0.61
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.35; acc: 0.56
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 1.11; acc: 0.67
Batch: 160; loss: 0.98; acc: 0.84
Batch: 180; loss: 0.96; acc: 0.7
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 1.05; acc: 0.7
Batch: 240; loss: 1.1; acc: 0.64
Batch: 260; loss: 1.01; acc: 0.72
Batch: 280; loss: 1.0; acc: 0.75
Batch: 300; loss: 0.98; acc: 0.75
Batch: 320; loss: 1.15; acc: 0.64
Batch: 340; loss: 1.1; acc: 0.66
Batch: 360; loss: 1.18; acc: 0.69
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.18; acc: 0.7
Batch: 420; loss: 1.11; acc: 0.7
Batch: 440; loss: 1.27; acc: 0.58
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 1.15; acc: 0.72
Batch: 500; loss: 1.15; acc: 0.62
Batch: 520; loss: 1.19; acc: 0.72
Batch: 540; loss: 1.06; acc: 0.69
Batch: 560; loss: 1.14; acc: 0.66
Batch: 580; loss: 1.21; acc: 0.62
Batch: 600; loss: 1.1; acc: 0.64
Batch: 620; loss: 1.19; acc: 0.7
Batch: 640; loss: 1.06; acc: 0.73
Batch: 660; loss: 1.04; acc: 0.75
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 1.08; acc: 0.73
Batch: 740; loss: 1.25; acc: 0.61
Batch: 760; loss: 1.01; acc: 0.77
Batch: 780; loss: 0.93; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.69 

0.00020623575255740434
0.00020100976689718664
Batch: 0; loss: 1.24; acc: 0.66
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 1.11; acc: 0.62
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 0.8; acc: 0.77
Val Epoch over. val_loss: 1.0495014050204283; val_accuracy: 0.7110867834394905 

The current subspace-distance is: 0.00020100976689718664 

plots/subspace_training/table13slim/2020-01-29 16:00:01/N_13_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.11

The number of parameters is: 272274

The number of individual parameters is:

9
162
9
9
14
33012
14
14
27
99036
27
27
64
134784
64
64
4096
64
640
10
64
64

nonzero elements in E: 54454795
elements in E: 54454800
fraction nonzero: 0.9999999081807297
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.5; acc: 0.03
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.15; acc: 0.33
Batch: 60; loss: 2.0; acc: 0.41
Batch: 80; loss: 2.01; acc: 0.38
Batch: 100; loss: 1.96; acc: 0.42
Batch: 120; loss: 1.76; acc: 0.56
Batch: 140; loss: 1.81; acc: 0.5
Batch: 160; loss: 1.85; acc: 0.44
Batch: 180; loss: 1.81; acc: 0.56
Batch: 200; loss: 1.73; acc: 0.58
Batch: 220; loss: 1.66; acc: 0.52
Batch: 240; loss: 1.74; acc: 0.5
Batch: 260; loss: 1.68; acc: 0.56
Batch: 280; loss: 1.59; acc: 0.62
Batch: 300; loss: 1.62; acc: 0.62
Batch: 320; loss: 1.59; acc: 0.67
Batch: 340; loss: 1.63; acc: 0.59
Batch: 360; loss: 1.64; acc: 0.61
Batch: 380; loss: 1.54; acc: 0.69
Batch: 400; loss: 1.56; acc: 0.64
Batch: 420; loss: 1.51; acc: 0.67
Batch: 440; loss: 1.46; acc: 0.66
Batch: 460; loss: 1.46; acc: 0.7
Batch: 480; loss: 1.53; acc: 0.56
Batch: 500; loss: 1.5; acc: 0.62
Batch: 520; loss: 1.45; acc: 0.64
Batch: 540; loss: 1.48; acc: 0.66
Batch: 560; loss: 1.49; acc: 0.66
Batch: 580; loss: 1.44; acc: 0.67
Batch: 600; loss: 1.49; acc: 0.59
Batch: 620; loss: 1.52; acc: 0.59
Batch: 640; loss: 1.43; acc: 0.75
Batch: 660; loss: 1.49; acc: 0.66
Batch: 680; loss: 1.42; acc: 0.66
Batch: 700; loss: 1.49; acc: 0.61
Batch: 720; loss: 1.44; acc: 0.66
Batch: 740; loss: 1.36; acc: 0.83
Batch: 760; loss: 1.39; acc: 0.75
Batch: 780; loss: 1.47; acc: 0.64
Train Epoch over. train_loss: 1.64; train_accuracy: 0.58 

6.028389907442033e-05
5.524478547158651e-05
Batch: 0; loss: 1.43; acc: 0.69
Batch: 20; loss: 1.55; acc: 0.62
Batch: 40; loss: 1.11; acc: 0.84
Batch: 60; loss: 1.4; acc: 0.66
Batch: 80; loss: 1.19; acc: 0.81
Batch: 100; loss: 1.33; acc: 0.77
Batch: 120; loss: 1.5; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.8
Val Epoch over. val_loss: 1.3712878508172976; val_accuracy: 0.7151671974522293 

The current subspace-distance is: 5.524478547158651e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.51; acc: 0.64
Batch: 20; loss: 1.46; acc: 0.69
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.37; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.72
Batch: 100; loss: 1.38; acc: 0.69
Batch: 120; loss: 1.38; acc: 0.64
Batch: 140; loss: 1.41; acc: 0.69
Batch: 160; loss: 1.32; acc: 0.73
Batch: 180; loss: 1.32; acc: 0.75
Batch: 200; loss: 1.35; acc: 0.73
Batch: 220; loss: 1.26; acc: 0.7
Batch: 240; loss: 1.39; acc: 0.77
Batch: 260; loss: 1.3; acc: 0.78
Batch: 280; loss: 1.24; acc: 0.77
Batch: 300; loss: 1.36; acc: 0.77
Batch: 320; loss: 1.28; acc: 0.72
Batch: 340; loss: 1.4; acc: 0.73
Batch: 360; loss: 1.29; acc: 0.7
Batch: 380; loss: 1.36; acc: 0.69
Batch: 400; loss: 1.29; acc: 0.72
Batch: 420; loss: 1.33; acc: 0.73
Batch: 440; loss: 1.38; acc: 0.64
Batch: 460; loss: 1.2; acc: 0.84
Batch: 480; loss: 1.32; acc: 0.67
Batch: 500; loss: 1.28; acc: 0.7
Batch: 520; loss: 1.33; acc: 0.7
Batch: 540; loss: 1.34; acc: 0.72
Batch: 560; loss: 1.29; acc: 0.7
Batch: 580; loss: 1.29; acc: 0.7
Batch: 600; loss: 1.26; acc: 0.83
Batch: 620; loss: 1.27; acc: 0.75
Batch: 640; loss: 1.23; acc: 0.78
Batch: 660; loss: 1.32; acc: 0.73
Batch: 680; loss: 1.31; acc: 0.66
Batch: 700; loss: 1.3; acc: 0.69
Batch: 720; loss: 1.35; acc: 0.69
Batch: 740; loss: 1.35; acc: 0.66
Batch: 760; loss: 1.28; acc: 0.72
Batch: 780; loss: 1.33; acc: 0.69
Train Epoch over. train_loss: 1.32; train_accuracy: 0.72 

8.202621393138543e-05
7.810200622770935e-05
Batch: 0; loss: 1.27; acc: 0.77
Batch: 20; loss: 1.38; acc: 0.69
Batch: 40; loss: 0.91; acc: 0.89
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.84
Batch: 100; loss: 1.17; acc: 0.78
Batch: 120; loss: 1.32; acc: 0.69
Batch: 140; loss: 1.08; acc: 0.86
Val Epoch over. val_loss: 1.2090950198234267; val_accuracy: 0.7627388535031847 

The current subspace-distance is: 7.810200622770935e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.81
Batch: 20; loss: 1.35; acc: 0.75
Batch: 40; loss: 1.19; acc: 0.75
Batch: 60; loss: 1.23; acc: 0.7
Batch: 80; loss: 1.19; acc: 0.77
Batch: 100; loss: 1.21; acc: 0.81
Batch: 120; loss: 1.27; acc: 0.69
Batch: 140; loss: 1.22; acc: 0.73
Batch: 160; loss: 1.26; acc: 0.75
Batch: 180; loss: 1.1; acc: 0.8
Batch: 200; loss: 1.23; acc: 0.69
Batch: 220; loss: 1.3; acc: 0.73
Batch: 240; loss: 1.28; acc: 0.75
Batch: 260; loss: 1.09; acc: 0.83
Batch: 280; loss: 1.16; acc: 0.86
Batch: 300; loss: 1.32; acc: 0.7
Batch: 320; loss: 1.13; acc: 0.78
Batch: 340; loss: 1.2; acc: 0.78
Batch: 360; loss: 1.2; acc: 0.77
Batch: 380; loss: 1.31; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.78
Batch: 420; loss: 1.1; acc: 0.86
Batch: 440; loss: 1.12; acc: 0.83
Batch: 460; loss: 1.31; acc: 0.73
Batch: 480; loss: 1.29; acc: 0.72
Batch: 500; loss: 1.15; acc: 0.77
Batch: 520; loss: 1.28; acc: 0.67
Batch: 540; loss: 1.12; acc: 0.75
Batch: 560; loss: 1.17; acc: 0.77
Batch: 580; loss: 1.22; acc: 0.72
Batch: 600; loss: 1.23; acc: 0.66
Batch: 620; loss: 1.19; acc: 0.8
Batch: 640; loss: 1.21; acc: 0.75
Batch: 660; loss: 1.2; acc: 0.73
Batch: 680; loss: 1.12; acc: 0.78
Batch: 700; loss: 1.19; acc: 0.72
Batch: 720; loss: 0.97; acc: 0.88
Batch: 740; loss: 1.24; acc: 0.69
Batch: 760; loss: 1.12; acc: 0.78
Batch: 780; loss: 1.21; acc: 0.72
Train Epoch over. train_loss: 1.19; train_accuracy: 0.76 

0.00010069982090499252
9.527765359962359e-05
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 1.21; acc: 0.72
Batch: 40; loss: 0.77; acc: 0.94
Batch: 60; loss: 1.09; acc: 0.8
Batch: 80; loss: 0.92; acc: 0.86
Batch: 100; loss: 1.04; acc: 0.84
Batch: 120; loss: 1.19; acc: 0.78
Batch: 140; loss: 0.89; acc: 0.94
Val Epoch over. val_loss: 1.0921395326116283; val_accuracy: 0.7944864649681529 

The current subspace-distance is: 9.527765359962359e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.78
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.0; acc: 0.89
Batch: 80; loss: 1.15; acc: 0.77
Batch: 100; loss: 1.18; acc: 0.81
Batch: 120; loss: 1.12; acc: 0.72
Batch: 140; loss: 1.04; acc: 0.8
Batch: 160; loss: 1.1; acc: 0.81
Batch: 180; loss: 1.1; acc: 0.75
Batch: 200; loss: 1.09; acc: 0.77
Batch: 220; loss: 1.13; acc: 0.75
Batch: 240; loss: 1.15; acc: 0.7
Batch: 260; loss: 1.03; acc: 0.8
Batch: 280; loss: 1.13; acc: 0.77
Batch: 300; loss: 1.21; acc: 0.72
Batch: 320; loss: 1.05; acc: 0.77
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 1.07; acc: 0.78
Batch: 380; loss: 1.1; acc: 0.72
Batch: 400; loss: 1.05; acc: 0.78
Batch: 420; loss: 1.19; acc: 0.73
Batch: 440; loss: 1.05; acc: 0.81
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.07; acc: 0.8
Batch: 500; loss: 1.11; acc: 0.77
Batch: 520; loss: 1.06; acc: 0.78
Batch: 540; loss: 1.03; acc: 0.8
Batch: 560; loss: 1.12; acc: 0.78
Batch: 580; loss: 1.0; acc: 0.83
Batch: 600; loss: 1.04; acc: 0.81
Batch: 620; loss: 0.99; acc: 0.77
Batch: 640; loss: 1.03; acc: 0.78
Batch: 660; loss: 1.01; acc: 0.81
Batch: 680; loss: 1.03; acc: 0.81
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.08; acc: 0.72
Batch: 740; loss: 1.14; acc: 0.75
Batch: 760; loss: 1.16; acc: 0.75
Batch: 780; loss: 1.09; acc: 0.78
Train Epoch over. train_loss: 1.09; train_accuracy: 0.77 

0.00011725160584319383
0.00011237632133997977
Batch: 0; loss: 1.03; acc: 0.86
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 0.69; acc: 0.94
Batch: 60; loss: 0.97; acc: 0.81
Batch: 80; loss: 0.82; acc: 0.89
Batch: 100; loss: 0.97; acc: 0.81
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.77; acc: 0.91
Val Epoch over. val_loss: 0.9980430185415183; val_accuracy: 0.8067277070063694 

The current subspace-distance is: 0.00011237632133997977 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.78
Batch: 20; loss: 1.03; acc: 0.8
Batch: 40; loss: 1.09; acc: 0.72
Batch: 60; loss: 1.0; acc: 0.75
Batch: 80; loss: 0.99; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.78
Batch: 120; loss: 0.98; acc: 0.81
Batch: 140; loss: 0.98; acc: 0.83
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.8
Batch: 200; loss: 1.09; acc: 0.75
Batch: 220; loss: 1.03; acc: 0.75
Batch: 240; loss: 1.05; acc: 0.73
Batch: 260; loss: 1.0; acc: 0.78
Batch: 280; loss: 0.98; acc: 0.81
Batch: 300; loss: 1.1; acc: 0.75
Batch: 320; loss: 1.07; acc: 0.75
Batch: 340; loss: 1.06; acc: 0.8
Batch: 360; loss: 0.92; acc: 0.77
Batch: 380; loss: 1.01; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.78
Batch: 420; loss: 0.99; acc: 0.75
Batch: 440; loss: 0.98; acc: 0.83
Batch: 460; loss: 0.96; acc: 0.81
Batch: 480; loss: 0.97; acc: 0.8
Batch: 500; loss: 1.05; acc: 0.75
Batch: 520; loss: 0.98; acc: 0.77
Batch: 540; loss: 1.01; acc: 0.73
Batch: 560; loss: 0.9; acc: 0.81
Batch: 580; loss: 0.93; acc: 0.83
Batch: 600; loss: 1.04; acc: 0.77
Batch: 620; loss: 0.97; acc: 0.78
Batch: 640; loss: 1.04; acc: 0.77
Batch: 660; loss: 1.03; acc: 0.77
Batch: 680; loss: 0.83; acc: 0.89
Batch: 700; loss: 0.88; acc: 0.84
Batch: 720; loss: 0.93; acc: 0.8
Batch: 740; loss: 0.92; acc: 0.83
Batch: 760; loss: 0.9; acc: 0.81
Batch: 780; loss: 1.17; acc: 0.7
Train Epoch over. train_loss: 1.01; train_accuracy: 0.78 

0.00013742499868385494
0.00013103822129778564
Batch: 0; loss: 0.93; acc: 0.83
Batch: 20; loss: 1.04; acc: 0.73
Batch: 40; loss: 0.62; acc: 0.94
Batch: 60; loss: 0.87; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.88
Batch: 100; loss: 0.91; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.81
Batch: 140; loss: 0.67; acc: 0.91
Val Epoch over. val_loss: 0.9102632965251898; val_accuracy: 0.8185708598726115 

The current subspace-distance is: 0.00013103822129778564 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 0.94; acc: 0.8
Batch: 40; loss: 0.94; acc: 0.78
Batch: 60; loss: 0.9; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 0.9; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 1.15; acc: 0.7
Batch: 160; loss: 0.89; acc: 0.86
Batch: 180; loss: 1.01; acc: 0.84
Batch: 200; loss: 0.89; acc: 0.86
Batch: 220; loss: 0.87; acc: 0.84
Batch: 240; loss: 0.93; acc: 0.8
Batch: 260; loss: 0.89; acc: 0.83
Batch: 280; loss: 0.91; acc: 0.8
Batch: 300; loss: 0.97; acc: 0.73
Batch: 320; loss: 0.95; acc: 0.77
Batch: 340; loss: 0.92; acc: 0.8
Batch: 360; loss: 1.03; acc: 0.7
Batch: 380; loss: 0.97; acc: 0.73
Batch: 400; loss: 1.0; acc: 0.78
Batch: 420; loss: 0.83; acc: 0.81
Batch: 440; loss: 1.08; acc: 0.7
Batch: 460; loss: 0.9; acc: 0.84
Batch: 480; loss: 0.94; acc: 0.72
Batch: 500; loss: 0.98; acc: 0.75
Batch: 520; loss: 1.08; acc: 0.73
Batch: 540; loss: 0.97; acc: 0.75
Batch: 560; loss: 0.81; acc: 0.89
Batch: 580; loss: 0.88; acc: 0.81
Batch: 600; loss: 0.88; acc: 0.88
Batch: 620; loss: 0.87; acc: 0.84
Batch: 640; loss: 1.08; acc: 0.77
Batch: 660; loss: 0.89; acc: 0.86
Batch: 680; loss: 0.89; acc: 0.8
Batch: 700; loss: 0.9; acc: 0.84
Batch: 720; loss: 0.87; acc: 0.86
Batch: 740; loss: 0.96; acc: 0.8
Batch: 760; loss: 0.94; acc: 0.75
Batch: 780; loss: 0.81; acc: 0.84
Train Epoch over. train_loss: 0.95; train_accuracy: 0.79 

0.00014910621393937618
0.0001447242684662342
Batch: 0; loss: 0.88; acc: 0.81
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.92
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.87; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.91
Val Epoch over. val_loss: 0.857221571123524; val_accuracy: 0.8241441082802548 

The current subspace-distance is: 0.0001447242684662342 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.81
Batch: 20; loss: 0.94; acc: 0.78
Batch: 40; loss: 1.04; acc: 0.75
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 1.01; acc: 0.81
Batch: 100; loss: 0.89; acc: 0.81
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.93; acc: 0.75
Batch: 160; loss: 0.86; acc: 0.84
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 0.99; acc: 0.8
Batch: 220; loss: 0.81; acc: 0.84
Batch: 240; loss: 0.91; acc: 0.81
Batch: 260; loss: 0.91; acc: 0.8
Batch: 280; loss: 0.8; acc: 0.81
Batch: 300; loss: 0.83; acc: 0.84
Batch: 320; loss: 0.9; acc: 0.86
Batch: 340; loss: 0.89; acc: 0.81
Batch: 360; loss: 0.83; acc: 0.83
Batch: 380; loss: 0.71; acc: 0.89
Batch: 400; loss: 0.8; acc: 0.84
Batch: 420; loss: 0.97; acc: 0.78
Batch: 440; loss: 1.01; acc: 0.73
Batch: 460; loss: 0.82; acc: 0.8
Batch: 480; loss: 0.9; acc: 0.8
Batch: 500; loss: 0.89; acc: 0.75
Batch: 520; loss: 0.88; acc: 0.84
Batch: 540; loss: 0.93; acc: 0.78
Batch: 560; loss: 0.95; acc: 0.83
Batch: 580; loss: 0.8; acc: 0.88
Batch: 600; loss: 0.81; acc: 0.83
Batch: 620; loss: 0.91; acc: 0.78
Batch: 640; loss: 0.92; acc: 0.75
Batch: 660; loss: 0.91; acc: 0.8
Batch: 680; loss: 0.91; acc: 0.75
Batch: 700; loss: 0.89; acc: 0.8
Batch: 720; loss: 0.89; acc: 0.77
Batch: 740; loss: 0.61; acc: 0.92
Batch: 760; loss: 0.82; acc: 0.88
Batch: 780; loss: 0.88; acc: 0.83
Train Epoch over. train_loss: 0.89; train_accuracy: 0.8 

0.00016186201537493616
0.00015642584185115993
Batch: 0; loss: 0.84; acc: 0.83
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.57; acc: 0.91
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.81; acc: 0.84
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.92
Val Epoch over. val_loss: 0.8143313568868454; val_accuracy: 0.8289211783439491 

The current subspace-distance is: 0.00015642584185115993 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.74; acc: 0.94
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.83
Batch: 140; loss: 0.85; acc: 0.84
Batch: 160; loss: 0.83; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.71; acc: 0.86
Batch: 220; loss: 0.83; acc: 0.77
Batch: 240; loss: 0.91; acc: 0.81
Batch: 260; loss: 0.77; acc: 0.83
Batch: 280; loss: 0.8; acc: 0.83
Batch: 300; loss: 0.86; acc: 0.83
Batch: 320; loss: 0.59; acc: 0.95
Batch: 340; loss: 0.92; acc: 0.73
Batch: 360; loss: 0.89; acc: 0.77
Batch: 380; loss: 0.89; acc: 0.8
Batch: 400; loss: 0.86; acc: 0.77
Batch: 420; loss: 0.82; acc: 0.84
Batch: 440; loss: 0.89; acc: 0.81
Batch: 460; loss: 0.96; acc: 0.77
Batch: 480; loss: 0.86; acc: 0.81
Batch: 500; loss: 0.76; acc: 0.84
Batch: 520; loss: 0.93; acc: 0.77
Batch: 540; loss: 0.92; acc: 0.8
Batch: 560; loss: 0.9; acc: 0.73
Batch: 580; loss: 0.72; acc: 0.91
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.88
Batch: 640; loss: 1.14; acc: 0.61
Batch: 660; loss: 1.03; acc: 0.7
Batch: 680; loss: 0.85; acc: 0.83
Batch: 700; loss: 0.98; acc: 0.73
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.9; acc: 0.77
Batch: 760; loss: 0.89; acc: 0.81
Batch: 780; loss: 0.81; acc: 0.86
Train Epoch over. train_loss: 0.85; train_accuracy: 0.81 

0.00017447074060328305
0.00016726860485505313
Batch: 0; loss: 0.82; acc: 0.84
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.7599284861497818; val_accuracy: 0.8429538216560509 

The current subspace-distance is: 0.00016726860485505313 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 0.73; acc: 0.88
Batch: 40; loss: 0.97; acc: 0.75
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 1.02; acc: 0.72
Batch: 100; loss: 0.81; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.88; acc: 0.81
Batch: 160; loss: 0.95; acc: 0.73
Batch: 180; loss: 0.86; acc: 0.78
Batch: 200; loss: 0.72; acc: 0.88
Batch: 220; loss: 0.74; acc: 0.81
Batch: 240; loss: 0.72; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.92
Batch: 280; loss: 0.68; acc: 0.91
Batch: 300; loss: 0.74; acc: 0.86
Batch: 320; loss: 0.86; acc: 0.8
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.94; acc: 0.75
Batch: 380; loss: 0.88; acc: 0.81
Batch: 400; loss: 0.87; acc: 0.84
Batch: 420; loss: 0.74; acc: 0.86
Batch: 440; loss: 0.76; acc: 0.83
Batch: 460; loss: 0.73; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.85; acc: 0.81
Batch: 520; loss: 0.88; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.89
Batch: 580; loss: 0.86; acc: 0.73
Batch: 600; loss: 0.83; acc: 0.81
Batch: 620; loss: 0.86; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.88
Batch: 660; loss: 0.79; acc: 0.83
Batch: 680; loss: 0.79; acc: 0.84
Batch: 700; loss: 0.75; acc: 0.84
Batch: 720; loss: 0.79; acc: 0.78
Batch: 740; loss: 0.7; acc: 0.88
Batch: 760; loss: 0.89; acc: 0.78
Batch: 780; loss: 0.79; acc: 0.83
Train Epoch over. train_loss: 0.81; train_accuracy: 0.82 

0.0001852677232818678
0.00018014144734479487
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.7263493437296266; val_accuracy: 0.8489251592356688 

The current subspace-distance is: 0.00018014144734479487 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.88; acc: 0.75
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.79; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.73; acc: 0.88
Batch: 160; loss: 0.83; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.91; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.83
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.73; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.83
Batch: 320; loss: 0.8; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.81; acc: 0.88
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.8; acc: 0.81
Batch: 440; loss: 0.79; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.89
Batch: 500; loss: 0.71; acc: 0.91
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 0.67; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.73; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.89
Batch: 620; loss: 1.01; acc: 0.75
Batch: 640; loss: 0.76; acc: 0.86
Batch: 660; loss: 0.92; acc: 0.78
Batch: 680; loss: 0.64; acc: 0.88
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.77; acc: 0.8
Batch: 740; loss: 1.07; acc: 0.7
Batch: 760; loss: 0.86; acc: 0.8
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.78; train_accuracy: 0.82 

0.00019422467448748648
0.00018763310799840838
Batch: 0; loss: 0.73; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.94
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.6968275663579345; val_accuracy: 0.8497213375796179 

The current subspace-distance is: 0.00018763310799840838 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.94
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.77; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.76; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.81
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.84; acc: 0.83
Batch: 280; loss: 0.76; acc: 0.83
Batch: 300; loss: 0.78; acc: 0.73
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.78; acc: 0.83
Batch: 360; loss: 0.84; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.88
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.9; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.8
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.84
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.75
Batch: 580; loss: 0.81; acc: 0.81
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.68; acc: 0.91
Batch: 640; loss: 0.67; acc: 0.86
Batch: 660; loss: 0.81; acc: 0.81
Batch: 680; loss: 0.82; acc: 0.81
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.8; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.89
Batch: 760; loss: 0.87; acc: 0.81
Batch: 780; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.0001978749205591157
0.0001896239264169708
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.6823893064146589; val_accuracy: 0.8543988853503185 

The current subspace-distance is: 0.0001896239264169708 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.67; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.79; acc: 0.78
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.79; acc: 0.81
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.88
Batch: 320; loss: 0.77; acc: 0.81
Batch: 340; loss: 0.85; acc: 0.75
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.86
Batch: 400; loss: 0.67; acc: 0.89
Batch: 420; loss: 0.79; acc: 0.83
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.89
Batch: 500; loss: 1.02; acc: 0.67
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.79; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.84
Batch: 640; loss: 0.76; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.84; acc: 0.75
Batch: 740; loss: 0.8; acc: 0.84
Batch: 760; loss: 0.91; acc: 0.75
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00020131110795773566
0.00019321098807267845
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.6842481516728736; val_accuracy: 0.8518113057324841 

The current subspace-distance is: 0.00019321098807267845 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 0.73; acc: 0.88
Batch: 140; loss: 0.88; acc: 0.75
Batch: 160; loss: 0.63; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.83; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.71; acc: 0.78
Batch: 300; loss: 0.81; acc: 0.83
Batch: 320; loss: 0.6; acc: 0.92
Batch: 340; loss: 0.68; acc: 0.86
Batch: 360; loss: 0.82; acc: 0.77
Batch: 380; loss: 0.86; acc: 0.78
Batch: 400; loss: 0.93; acc: 0.69
Batch: 420; loss: 0.76; acc: 0.86
Batch: 440; loss: 0.73; acc: 0.83
Batch: 460; loss: 0.8; acc: 0.75
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.76; acc: 0.81
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.81; acc: 0.75
Batch: 600; loss: 0.8; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.79; acc: 0.78
Batch: 660; loss: 0.72; acc: 0.89
Batch: 680; loss: 0.76; acc: 0.81
Batch: 700; loss: 0.95; acc: 0.73
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.86
Batch: 760; loss: 0.78; acc: 0.81
Batch: 780; loss: 0.67; acc: 0.88
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.0002031319891102612
0.00019820842135231942
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.664014160063616; val_accuracy: 0.8568869426751592 

The current subspace-distance is: 0.00019820842135231942 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.81
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.87; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.83; acc: 0.84
Batch: 180; loss: 0.91; acc: 0.75
Batch: 200; loss: 0.73; acc: 0.86
Batch: 220; loss: 0.94; acc: 0.77
Batch: 240; loss: 0.54; acc: 0.92
Batch: 260; loss: 0.86; acc: 0.77
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.82; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.89
Batch: 440; loss: 0.78; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.71; acc: 0.84
Batch: 500; loss: 0.9; acc: 0.72
Batch: 520; loss: 0.69; acc: 0.83
Batch: 540; loss: 0.86; acc: 0.78
Batch: 560; loss: 0.61; acc: 0.91
Batch: 580; loss: 0.92; acc: 0.78
Batch: 600; loss: 0.68; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.63; acc: 0.88
Batch: 660; loss: 0.87; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.81
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.85; acc: 0.78
Batch: 740; loss: 0.82; acc: 0.84
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.73; train_accuracy: 0.83 

0.00020639767171815038
0.0001978594227693975
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.6607344524495921; val_accuracy: 0.8557921974522293 

The current subspace-distance is: 0.0001978594227693975 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 0.6; acc: 0.91
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.88
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.76; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.81
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.74; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.83
Batch: 340; loss: 0.95; acc: 0.7
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.75; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.88
Batch: 440; loss: 0.91; acc: 0.73
Batch: 460; loss: 0.96; acc: 0.81
Batch: 480; loss: 0.77; acc: 0.81
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.64; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.8
Batch: 580; loss: 0.69; acc: 0.89
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 0.85; acc: 0.8
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.69; acc: 0.77
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.7; acc: 0.84
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.68; acc: 0.78
Batch: 780; loss: 0.8; acc: 0.83
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.0002073486102744937
0.00019836725550703704
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.6566142367709215; val_accuracy: 0.8581807324840764 

The current subspace-distance is: 0.00019836725550703704 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.66; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.74; acc: 0.78
Batch: 180; loss: 0.73; acc: 0.83
Batch: 200; loss: 0.8; acc: 0.84
Batch: 220; loss: 0.81; acc: 0.77
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.78; acc: 0.8
Batch: 280; loss: 0.76; acc: 0.84
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.86
Batch: 360; loss: 0.93; acc: 0.75
Batch: 380; loss: 0.76; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.89
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.92
Batch: 460; loss: 0.82; acc: 0.83
Batch: 480; loss: 0.74; acc: 0.84
Batch: 500; loss: 0.85; acc: 0.78
Batch: 520; loss: 0.81; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.78
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.67; acc: 0.89
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.89
Batch: 680; loss: 0.76; acc: 0.81
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.57; acc: 0.91
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.73; acc: 0.81
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.0002107407635776326
0.00020205033069942147
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.64998747303987; val_accuracy: 0.8553941082802548 

The current subspace-distance is: 0.00020205033069942147 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 0.71; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.78
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.77; acc: 0.81
Batch: 220; loss: 0.84; acc: 0.77
Batch: 240; loss: 0.89; acc: 0.69
Batch: 260; loss: 0.84; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.92
Batch: 340; loss: 0.89; acc: 0.75
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.74; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.8
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.68; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.59; acc: 0.92
Batch: 560; loss: 0.89; acc: 0.81
Batch: 580; loss: 0.92; acc: 0.73
Batch: 600; loss: 0.51; acc: 0.94
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.64; acc: 0.88
Batch: 680; loss: 0.66; acc: 0.88
Batch: 700; loss: 0.74; acc: 0.75
Batch: 720; loss: 0.83; acc: 0.78
Batch: 740; loss: 0.62; acc: 0.86
Batch: 760; loss: 0.65; acc: 0.89
Batch: 780; loss: 0.71; acc: 0.84
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.0002141536388080567
0.00020598860282916576
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.94
Val Epoch over. val_loss: 0.6411255254487324; val_accuracy: 0.85828025477707 

The current subspace-distance is: 0.00020598860282916576 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.91
Batch: 140; loss: 0.61; acc: 0.89
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.8; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.86
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.77; acc: 0.78
Batch: 400; loss: 0.81; acc: 0.77
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.69; acc: 0.8
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.93; acc: 0.72
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.81; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.86
Batch: 620; loss: 0.83; acc: 0.75
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.73; acc: 0.81
Batch: 680; loss: 0.82; acc: 0.77
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.0002157095877919346
0.00020940171089023352
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.38; acc: 0.92
Val Epoch over. val_loss: 0.6387110944766148; val_accuracy: 0.8581807324840764 

The current subspace-distance is: 0.00020940171089023352 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.73; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.84; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.91
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.94
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.73; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.43; acc: 0.97
Batch: 400; loss: 0.71; acc: 0.81
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.6; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.94
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.92
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.86
Batch: 600; loss: 0.65; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.45; acc: 0.94
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.0002193781256210059
0.0002100385318044573
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.6224890002019846; val_accuracy: 0.8614649681528662 

The current subspace-distance is: 0.0002100385318044573 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.73; acc: 0.86
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 0.55; acc: 0.94
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.68; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.73; acc: 0.84
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.84; acc: 0.77
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.82; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.8
Batch: 400; loss: 0.76; acc: 0.77
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.7; acc: 0.88
Batch: 480; loss: 0.66; acc: 0.89
Batch: 500; loss: 0.83; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.91
Batch: 580; loss: 1.01; acc: 0.72
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.89
Batch: 640; loss: 0.92; acc: 0.75
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00021910179930273443
0.00021286739502102137
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.6308303944244507; val_accuracy: 0.8608678343949044 

The current subspace-distance is: 0.00021286739502102137 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.79; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.84
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.65; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.8; acc: 0.75
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.84; acc: 0.72
Batch: 440; loss: 0.62; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.89
Batch: 480; loss: 0.72; acc: 0.78
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.68; acc: 0.86
Batch: 560; loss: 0.63; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.77
Batch: 640; loss: 0.73; acc: 0.81
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.81
Batch: 740; loss: 0.73; acc: 0.81
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00022185692796483636
0.00021460934658534825
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.6192164320474977; val_accuracy: 0.8627587579617835 

The current subspace-distance is: 0.00021460934658534825 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.86
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.94
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.76; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.93; acc: 0.75
Batch: 240; loss: 0.72; acc: 0.84
Batch: 260; loss: 0.79; acc: 0.8
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.65; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.92
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.81
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.7; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.88
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.86
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 0.82; acc: 0.75
Batch: 640; loss: 0.76; acc: 0.81
Batch: 660; loss: 0.86; acc: 0.72
Batch: 680; loss: 0.74; acc: 0.8
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00022036468726582825
0.000214390383916907
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.6172781136764842; val_accuracy: 0.8633558917197452 

The current subspace-distance is: 0.000214390383916907 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.92
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.92
Batch: 200; loss: 0.82; acc: 0.75
Batch: 220; loss: 0.61; acc: 0.88
Batch: 240; loss: 0.75; acc: 0.8
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.72; acc: 0.81
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.63; acc: 0.78
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.94
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.78
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.78
Batch: 580; loss: 0.72; acc: 0.84
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.55; acc: 0.91
Batch: 680; loss: 0.65; acc: 0.89
Batch: 700; loss: 0.73; acc: 0.83
Batch: 720; loss: 0.8; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.0002187585341744125
0.00021261433721520007
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.6185467474779506; val_accuracy: 0.8616640127388535 

The current subspace-distance is: 0.00021261433721520007 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.78; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.89
Batch: 380; loss: 0.83; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.78; acc: 0.78
Batch: 460; loss: 0.63; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.91
Batch: 500; loss: 0.74; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.74; acc: 0.78
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.77; acc: 0.75
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.75; acc: 0.83
Batch: 680; loss: 0.76; acc: 0.81
Batch: 700; loss: 0.81; acc: 0.81
Batch: 720; loss: 0.72; acc: 0.89
Batch: 740; loss: 0.56; acc: 0.94
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.86; acc: 0.73
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00022369559155777097
0.0002139009884558618
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.614150194985092; val_accuracy: 0.8629578025477707 

The current subspace-distance is: 0.0002139009884558618 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.78
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.75; acc: 0.77
Batch: 200; loss: 0.75; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.8
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.82; acc: 0.78
Batch: 380; loss: 0.7; acc: 0.84
Batch: 400; loss: 0.67; acc: 0.86
Batch: 420; loss: 0.89; acc: 0.75
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.77; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.91
Batch: 580; loss: 0.7; acc: 0.84
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.57; acc: 0.91
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.91
Batch: 700; loss: 0.6; acc: 0.89
Batch: 720; loss: 0.81; acc: 0.72
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.73; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00022370301303453743
0.0002161762968171388
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.6116034756800172; val_accuracy: 0.8613654458598726 

The current subspace-distance is: 0.0002161762968171388 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.83
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.62; acc: 0.88
Batch: 220; loss: 0.69; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.97
Batch: 260; loss: 0.71; acc: 0.77
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.71; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.72; acc: 0.8
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 0.85; acc: 0.78
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.82; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.71; acc: 0.86
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 0.48; acc: 0.94
Batch: 660; loss: 0.6; acc: 0.88
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.76; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.0002223258779849857
0.00021669163834303617
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.6102989555164507; val_accuracy: 0.8651472929936306 

The current subspace-distance is: 0.00021669163834303617 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.7; acc: 0.77
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.86
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.62; acc: 0.88
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.72; acc: 0.86
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.81; acc: 0.75
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.66; acc: 0.88
Batch: 560; loss: 0.78; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.86; acc: 0.75
Batch: 680; loss: 0.82; acc: 0.77
Batch: 700; loss: 0.51; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00022289440676104277
0.0002170900406781584
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.615086622109079; val_accuracy: 0.8638535031847133 

The current subspace-distance is: 0.0002170900406781584 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.88; acc: 0.73
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.64; acc: 0.81
Batch: 240; loss: 0.76; acc: 0.77
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.7; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.92
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.78
Batch: 420; loss: 0.84; acc: 0.72
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.82; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.91
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.64; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00022737270046491176
0.00021968786313664168
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.6109908659746692; val_accuracy: 0.8643511146496815 

The current subspace-distance is: 0.00021968786313664168 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.81; acc: 0.77
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.74; acc: 0.77
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.75
Batch: 260; loss: 0.43; acc: 0.95
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.89
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.83; acc: 0.77
Batch: 380; loss: 0.69; acc: 0.83
Batch: 400; loss: 0.71; acc: 0.81
Batch: 420; loss: 0.73; acc: 0.78
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.88
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 0.57; acc: 0.92
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.64; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.71; acc: 0.78
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.77; acc: 0.78
Batch: 760; loss: 0.79; acc: 0.81
Batch: 780; loss: 0.77; acc: 0.77
Train Epoch over. train_loss: 0.67; train_accuracy: 0.83 

0.00022697998792864382
0.0002172592212446034
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.6074572118224612; val_accuracy: 0.8654458598726115 

The current subspace-distance is: 0.0002172592212446034 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.6; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.68; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.74; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.74; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.94
Batch: 360; loss: 0.65; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.6; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.91
Batch: 460; loss: 0.88; acc: 0.73
Batch: 480; loss: 0.66; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.88
Batch: 520; loss: 0.64; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.66; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.94
Batch: 620; loss: 0.79; acc: 0.8
Batch: 640; loss: 0.5; acc: 0.92
Batch: 660; loss: 0.72; acc: 0.84
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.67; acc: 0.83
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00022938105394132435
0.0002212061226600781
Batch: 0; loss: 0.64; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.95
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.6057234501383107; val_accuracy: 0.8634554140127388 

The current subspace-distance is: 0.0002212061226600781 

plots/subspace_training/table13slim/2020-01-29 16:00:01/N_13_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.11

The number of parameters is: 272274

The number of individual parameters is:

9
162
9
9
14
33012
14
14
27
99036
27
27
64
134784
64
64
4096
64
640
10
64
64

nonzero elements in E: 81682191
elements in E: 81682200
fraction nonzero: 0.9999998898168756
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.45; acc: 0.08
Batch: 20; loss: 2.24; acc: 0.11
Batch: 40; loss: 2.14; acc: 0.3
Batch: 60; loss: 1.94; acc: 0.41
Batch: 80; loss: 1.86; acc: 0.47
Batch: 100; loss: 1.73; acc: 0.52
Batch: 120; loss: 1.72; acc: 0.52
Batch: 140; loss: 1.61; acc: 0.61
Batch: 160; loss: 1.55; acc: 0.61
Batch: 180; loss: 1.62; acc: 0.5
Batch: 200; loss: 1.5; acc: 0.61
Batch: 220; loss: 1.55; acc: 0.61
Batch: 240; loss: 1.52; acc: 0.62
Batch: 260; loss: 1.56; acc: 0.64
Batch: 280; loss: 1.5; acc: 0.67
Batch: 300; loss: 1.36; acc: 0.72
Batch: 320; loss: 1.45; acc: 0.75
Batch: 340; loss: 1.34; acc: 0.73
Batch: 360; loss: 1.47; acc: 0.64
Batch: 380; loss: 1.52; acc: 0.62
Batch: 400; loss: 1.51; acc: 0.62
Batch: 420; loss: 1.34; acc: 0.73
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.43; acc: 0.7
Batch: 480; loss: 1.39; acc: 0.69
Batch: 500; loss: 1.38; acc: 0.64
Batch: 520; loss: 1.27; acc: 0.78
Batch: 540; loss: 1.26; acc: 0.77
Batch: 560; loss: 1.32; acc: 0.69
Batch: 580; loss: 1.31; acc: 0.8
Batch: 600; loss: 1.21; acc: 0.8
Batch: 620; loss: 1.3; acc: 0.69
Batch: 640; loss: 1.31; acc: 0.7
Batch: 660; loss: 1.36; acc: 0.64
Batch: 680; loss: 1.4; acc: 0.62
Batch: 700; loss: 1.27; acc: 0.72
Batch: 720; loss: 1.2; acc: 0.78
Batch: 740; loss: 1.32; acc: 0.66
Batch: 760; loss: 1.25; acc: 0.7
Batch: 780; loss: 1.21; acc: 0.77
Train Epoch over. train_loss: 1.49; train_accuracy: 0.64 

6.62603706587106e-05
6.114838470239192e-05
Batch: 0; loss: 1.2; acc: 0.73
Batch: 20; loss: 1.28; acc: 0.67
Batch: 40; loss: 0.91; acc: 0.91
Batch: 60; loss: 1.16; acc: 0.77
Batch: 80; loss: 1.03; acc: 0.89
Batch: 100; loss: 1.12; acc: 0.78
Batch: 120; loss: 1.33; acc: 0.66
Batch: 140; loss: 1.03; acc: 0.91
Val Epoch over. val_loss: 1.1672550314550947; val_accuracy: 0.7937898089171974 

The current subspace-distance is: 6.114838470239192e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.77
Batch: 20; loss: 1.28; acc: 0.77
Batch: 40; loss: 1.2; acc: 0.78
Batch: 60; loss: 1.16; acc: 0.84
Batch: 80; loss: 1.15; acc: 0.86
Batch: 100; loss: 1.07; acc: 0.84
Batch: 120; loss: 1.14; acc: 0.78
Batch: 140; loss: 1.18; acc: 0.78
Batch: 160; loss: 1.05; acc: 0.88
Batch: 180; loss: 1.22; acc: 0.7
Batch: 200; loss: 1.15; acc: 0.78
Batch: 220; loss: 1.18; acc: 0.78
Batch: 240; loss: 1.23; acc: 0.64
Batch: 260; loss: 1.14; acc: 0.8
Batch: 280; loss: 1.12; acc: 0.78
Batch: 300; loss: 1.11; acc: 0.81
Batch: 320; loss: 1.29; acc: 0.72
Batch: 340; loss: 1.1; acc: 0.8
Batch: 360; loss: 1.07; acc: 0.75
Batch: 380; loss: 1.03; acc: 0.78
Batch: 400; loss: 1.13; acc: 0.77
Batch: 420; loss: 1.03; acc: 0.84
Batch: 440; loss: 1.03; acc: 0.88
Batch: 460; loss: 1.06; acc: 0.81
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 1.22; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 1.15; acc: 0.73
Batch: 560; loss: 1.05; acc: 0.77
Batch: 580; loss: 0.99; acc: 0.84
Batch: 600; loss: 1.02; acc: 0.83
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.06; acc: 0.78
Batch: 660; loss: 0.97; acc: 0.78
Batch: 680; loss: 1.07; acc: 0.8
Batch: 700; loss: 1.11; acc: 0.73
Batch: 720; loss: 0.99; acc: 0.8
Batch: 740; loss: 0.94; acc: 0.83
Batch: 760; loss: 0.98; acc: 0.83
Batch: 780; loss: 0.86; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.79 

9.16052158572711e-05
8.74520483193919e-05
Batch: 0; loss: 0.99; acc: 0.83
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 0.68; acc: 0.92
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 0.82; acc: 0.91
Batch: 100; loss: 0.89; acc: 0.84
Batch: 120; loss: 1.1; acc: 0.73
Batch: 140; loss: 0.8; acc: 0.88
Val Epoch over. val_loss: 0.9450559642664187; val_accuracy: 0.8363853503184714 

The current subspace-distance is: 8.74520483193919e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.83
Batch: 40; loss: 1.03; acc: 0.84
Batch: 60; loss: 0.96; acc: 0.8
Batch: 80; loss: 0.94; acc: 0.83
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 1.04; acc: 0.78
Batch: 140; loss: 0.91; acc: 0.86
Batch: 160; loss: 0.96; acc: 0.78
Batch: 180; loss: 1.01; acc: 0.81
Batch: 200; loss: 0.8; acc: 0.92
Batch: 220; loss: 0.95; acc: 0.78
Batch: 240; loss: 0.92; acc: 0.86
Batch: 260; loss: 0.98; acc: 0.8
Batch: 280; loss: 0.92; acc: 0.8
Batch: 300; loss: 1.03; acc: 0.75
Batch: 320; loss: 0.89; acc: 0.84
Batch: 340; loss: 1.01; acc: 0.8
Batch: 360; loss: 0.92; acc: 0.81
Batch: 380; loss: 1.02; acc: 0.78
Batch: 400; loss: 0.81; acc: 0.83
Batch: 420; loss: 0.93; acc: 0.78
Batch: 440; loss: 0.8; acc: 0.92
Batch: 460; loss: 0.85; acc: 0.83
Batch: 480; loss: 0.9; acc: 0.88
Batch: 500; loss: 0.94; acc: 0.81
Batch: 520; loss: 0.85; acc: 0.81
Batch: 540; loss: 0.87; acc: 0.83
Batch: 560; loss: 0.79; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.91
Batch: 600; loss: 0.79; acc: 0.86
Batch: 620; loss: 0.89; acc: 0.84
Batch: 640; loss: 0.87; acc: 0.83
Batch: 660; loss: 0.89; acc: 0.81
Batch: 680; loss: 0.86; acc: 0.88
Batch: 700; loss: 0.78; acc: 0.88
Batch: 720; loss: 0.9; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.88
Batch: 760; loss: 0.93; acc: 0.77
Batch: 780; loss: 0.93; acc: 0.81
Train Epoch over. train_loss: 0.92; train_accuracy: 0.82 

0.000113606387458276
0.0001099771325243637
Batch: 0; loss: 0.87; acc: 0.84
Batch: 20; loss: 0.99; acc: 0.69
Batch: 40; loss: 0.56; acc: 0.92
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.77; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.62; acc: 0.92
Val Epoch over. val_loss: 0.7943611919500266; val_accuracy: 0.8541998407643312 

The current subspace-distance is: 0.0001099771325243637 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.94
Batch: 40; loss: 0.84; acc: 0.81
Batch: 60; loss: 0.86; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.88
Batch: 100; loss: 0.84; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 0.94; acc: 0.83
Batch: 160; loss: 0.86; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.89
Batch: 200; loss: 0.86; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.89
Batch: 240; loss: 0.72; acc: 0.91
Batch: 260; loss: 0.9; acc: 0.8
Batch: 280; loss: 0.85; acc: 0.8
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.92; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.86
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.86; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.91
Batch: 460; loss: 0.66; acc: 0.92
Batch: 480; loss: 0.62; acc: 0.97
Batch: 500; loss: 0.82; acc: 0.84
Batch: 520; loss: 0.83; acc: 0.84
Batch: 540; loss: 0.83; acc: 0.86
Batch: 560; loss: 0.86; acc: 0.78
Batch: 580; loss: 0.89; acc: 0.84
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.84; acc: 0.83
Batch: 640; loss: 0.71; acc: 0.91
Batch: 660; loss: 0.92; acc: 0.86
Batch: 680; loss: 0.81; acc: 0.86
Batch: 700; loss: 0.64; acc: 0.92
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.84; acc: 0.81
Batch: 780; loss: 0.75; acc: 0.88
Train Epoch over. train_loss: 0.8; train_accuracy: 0.84 

0.0001319925213465467
0.00012544709898065776
Batch: 0; loss: 0.79; acc: 0.88
Batch: 20; loss: 0.91; acc: 0.67
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.702752174845167; val_accuracy: 0.859375 

The current subspace-distance is: 0.00012544709898065776 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.86
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 0.71; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.89
Batch: 140; loss: 0.77; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.88
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.92
Batch: 240; loss: 0.89; acc: 0.81
Batch: 260; loss: 0.74; acc: 0.81
Batch: 280; loss: 0.7; acc: 0.88
Batch: 300; loss: 0.7; acc: 0.88
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.95
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.73; acc: 0.81
Batch: 400; loss: 0.81; acc: 0.78
Batch: 420; loss: 0.72; acc: 0.86
Batch: 440; loss: 0.87; acc: 0.8
Batch: 460; loss: 0.76; acc: 0.83
Batch: 480; loss: 0.79; acc: 0.84
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.63; acc: 0.89
Batch: 540; loss: 0.67; acc: 0.86
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.73; acc: 0.84
Batch: 720; loss: 0.75; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.86
Batch: 760; loss: 0.67; acc: 0.94
Batch: 780; loss: 0.74; acc: 0.84
Train Epoch over. train_loss: 0.72; train_accuracy: 0.85 

0.00014617065608035773
0.0001405672956025228
Batch: 0; loss: 0.71; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.6329680924202986; val_accuracy: 0.8690286624203821 

The current subspace-distance is: 0.0001405672956025228 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.91
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.59; acc: 0.91
Batch: 280; loss: 0.78; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.58; acc: 0.91
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.73; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.91
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.7; acc: 0.78
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.62; acc: 0.88
Batch: 640; loss: 0.67; acc: 0.92
Batch: 660; loss: 0.67; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.86 

0.00015571899712085724
0.00015010211791377515
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.4; acc: 0.95
Val Epoch over. val_loss: 0.5901733357815226; val_accuracy: 0.873109076433121 

The current subspace-distance is: 0.00015010211791377515 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.74; acc: 0.83
Batch: 160; loss: 0.72; acc: 0.78
Batch: 180; loss: 0.5; acc: 0.97
Batch: 200; loss: 0.56; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.91
Batch: 240; loss: 0.76; acc: 0.8
Batch: 260; loss: 0.78; acc: 0.8
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.7; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.94
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.72; acc: 0.86
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.65; acc: 0.89
Batch: 440; loss: 0.69; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.91
Batch: 480; loss: 0.8; acc: 0.81
Batch: 500; loss: 0.74; acc: 0.83
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.61; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.71; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.63; train_accuracy: 0.86 

0.0001671987265581265
0.00015995143621694297
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.5549218593888982; val_accuracy: 0.8801751592356688 

The current subspace-distance is: 0.00015995143621694297 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.92
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.94
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.94
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.92
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.77; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.69; acc: 0.89
Batch: 440; loss: 0.66; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.92
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.73; acc: 0.81
Batch: 580; loss: 0.71; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.92
Batch: 620; loss: 0.58; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.79; acc: 0.78
Batch: 700; loss: 0.46; acc: 0.94
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.94
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.0001751800300553441
0.00016822462202981114
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5212505562290265; val_accuracy: 0.886046974522293 

The current subspace-distance is: 0.00016822462202981114 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.94
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.92
Batch: 440; loss: 0.67; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.66; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.62; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.97
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.75; acc: 0.73
Batch: 760; loss: 0.63; acc: 0.84
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00018370001635048538
0.00017607957124710083
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.5010289633350008; val_accuracy: 0.8876393312101911 

The current subspace-distance is: 0.00017607957124710083 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.94
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.94
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.95
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.92
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.63; acc: 0.89
Batch: 620; loss: 0.63; acc: 0.8
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0001907262922031805
0.00018352306506130844
Batch: 0; loss: 0.48; acc: 0.97
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.47691807891153226; val_accuracy: 0.8912221337579618 

The current subspace-distance is: 0.00018352306506130844 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.97
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.95
Batch: 600; loss: 0.48; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.0001938865170814097
0.0001854128495324403
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.73
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.47561561662680024; val_accuracy: 0.8919187898089171 

The current subspace-distance is: 0.0001854128495324403 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.94
Batch: 280; loss: 0.64; acc: 0.78
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.79; acc: 0.77
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.8
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00019491036073304713
0.00018783345876727253
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.46537166302371175; val_accuracy: 0.892515923566879 

The current subspace-distance is: 0.00018783345876727253 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.41; acc: 0.95
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.94
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.74; acc: 0.77
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019899330800399184
0.00019102469377685338
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4582334428456179; val_accuracy: 0.8941082802547771 

The current subspace-distance is: 0.00019102469377685338 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.63; acc: 0.78
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.81
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.77; acc: 0.78
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020008522551506758
0.00019452074775472283
Batch: 0; loss: 0.45; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.45192744995758033; val_accuracy: 0.8958001592356688 

The current subspace-distance is: 0.00019452074775472283 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.44; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.75
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020144258451182395
0.00019262103887740523
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.44931321719269846; val_accuracy: 0.8960987261146497 

The current subspace-distance is: 0.00019262103887740523 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.61; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.0002028605085797608
0.0001955582993105054
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.44975062920029757; val_accuracy: 0.8943073248407644 

The current subspace-distance is: 0.0001955582993105054 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.95
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.95
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00020688114454969764
0.00019930685812141746
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.434836108213777; val_accuracy: 0.8986863057324841 

The current subspace-distance is: 0.00019930685812141746 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.97
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.97
Batch: 240; loss: 0.6; acc: 0.78
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.77
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00020767920068465173
0.00019869653624482453
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.4404398683150103; val_accuracy: 0.8969944267515924 

The current subspace-distance is: 0.00019869653624482453 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.98
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.81
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00021073051902931184
0.00020335511362645775
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.43124959537178087; val_accuracy: 0.8982882165605095 

The current subspace-distance is: 0.00020335511362645775 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.78
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0002122123696608469
0.00020378796034492552
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.42456566936271206; val_accuracy: 0.9007762738853503 

The current subspace-distance is: 0.00020378796034492552 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.94
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.0002109242050210014
0.00020443393441382796
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.42773975166165906; val_accuracy: 0.8979896496815286 

The current subspace-distance is: 0.00020443393441382796 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.52; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021318842482287437
0.00020555000810418278
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.42323326922146376; val_accuracy: 0.9004777070063694 

The current subspace-distance is: 0.00020555000810418278 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.76; acc: 0.75
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021234547602944076
0.00020527243032120168
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.78
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.419124375483033; val_accuracy: 0.9013734076433121 

The current subspace-distance is: 0.00020527243032120168 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.95
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021459124400280416
0.00020628419588319957
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.42133465523173097; val_accuracy: 0.9015724522292994 

The current subspace-distance is: 0.00020628419588319957 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.95
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.6; acc: 0.8
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.95
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021643663058057427
0.00020908788428641856
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.42144942919539796; val_accuracy: 0.9008757961783439 

The current subspace-distance is: 0.00020908788428641856 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.72; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.78
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.0002149722567992285
0.00020703462359961122
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.4236177038994564; val_accuracy: 0.9000796178343949 

The current subspace-distance is: 0.00020703462359961122 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00021635982557199895
0.0002063171996269375
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.42063555700384125; val_accuracy: 0.9003781847133758 

The current subspace-distance is: 0.0002063171996269375 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.95
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00021622019994538277
0.00020725588547065854
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.41382725356490746; val_accuracy: 0.9014729299363057 

The current subspace-distance is: 0.00020725588547065854 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.94
Batch: 220; loss: 0.59; acc: 0.78
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.000217118053114973
0.0002095601084874943
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.4171630036868867; val_accuracy: 0.9003781847133758 

The current subspace-distance is: 0.0002095601084874943 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.95
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.8
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00021683018712792546
0.0002070278860628605
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.41121788237504897; val_accuracy: 0.9030652866242038 

The current subspace-distance is: 0.0002070278860628605 

plots/subspace_training/table13slim/2020-01-29 16:00:01/N_13_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.11

The number of parameters is: 272274

The number of individual parameters is:

9
162
9
9
14
33012
14
14
27
99036
27
27
64
134784
64
64
4096
64
640
10
64
64

nonzero elements in E: 108909589
elements in E: 108909600
fraction nonzero: 0.9999998989988027
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.46; acc: 0.06
Batch: 20; loss: 2.27; acc: 0.14
Batch: 40; loss: 1.93; acc: 0.41
Batch: 60; loss: 1.74; acc: 0.56
Batch: 80; loss: 1.84; acc: 0.52
Batch: 100; loss: 1.65; acc: 0.61
Batch: 120; loss: 1.56; acc: 0.62
Batch: 140; loss: 1.52; acc: 0.7
Batch: 160; loss: 1.43; acc: 0.72
Batch: 180; loss: 1.48; acc: 0.64
Batch: 200; loss: 1.47; acc: 0.62
Batch: 220; loss: 1.44; acc: 0.73
Batch: 240; loss: 1.36; acc: 0.7
Batch: 260; loss: 1.41; acc: 0.72
Batch: 280; loss: 1.34; acc: 0.78
Batch: 300; loss: 1.43; acc: 0.66
Batch: 320; loss: 1.41; acc: 0.66
Batch: 340; loss: 1.28; acc: 0.77
Batch: 360; loss: 1.28; acc: 0.72
Batch: 380; loss: 1.21; acc: 0.81
Batch: 400; loss: 1.26; acc: 0.75
Batch: 420; loss: 1.19; acc: 0.81
Batch: 440; loss: 1.25; acc: 0.78
Batch: 460; loss: 1.33; acc: 0.72
Batch: 480; loss: 1.18; acc: 0.75
Batch: 500; loss: 1.21; acc: 0.73
Batch: 520; loss: 1.05; acc: 0.81
Batch: 540; loss: 1.12; acc: 0.8
Batch: 560; loss: 1.05; acc: 0.8
Batch: 580; loss: 1.02; acc: 0.84
Batch: 600; loss: 1.07; acc: 0.81
Batch: 620; loss: 1.18; acc: 0.75
Batch: 640; loss: 1.02; acc: 0.81
Batch: 660; loss: 1.13; acc: 0.84
Batch: 680; loss: 1.22; acc: 0.75
Batch: 700; loss: 1.19; acc: 0.73
Batch: 720; loss: 1.07; acc: 0.81
Batch: 740; loss: 1.05; acc: 0.81
Batch: 760; loss: 1.18; acc: 0.78
Batch: 780; loss: 1.07; acc: 0.84
Train Epoch over. train_loss: 1.34; train_accuracy: 0.71 

2.613955075503327e-05
8.838098437990993e-06
Batch: 0; loss: 1.07; acc: 0.84
Batch: 20; loss: 1.06; acc: 0.8
Batch: 40; loss: 0.69; acc: 0.91
Batch: 60; loss: 1.0; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.94
Batch: 100; loss: 0.94; acc: 0.86
Batch: 120; loss: 1.11; acc: 0.78
Batch: 140; loss: 0.82; acc: 0.88
Val Epoch over. val_loss: 0.9863243004319014; val_accuracy: 0.8367834394904459 

The current subspace-distance is: 8.838098437990993e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.06; acc: 0.77
Batch: 40; loss: 0.99; acc: 0.8
Batch: 60; loss: 1.01; acc: 0.86
Batch: 80; loss: 1.04; acc: 0.8
Batch: 100; loss: 0.99; acc: 0.77
Batch: 120; loss: 0.98; acc: 0.88
Batch: 140; loss: 0.88; acc: 0.88
Batch: 160; loss: 0.97; acc: 0.8
Batch: 180; loss: 0.98; acc: 0.78
Batch: 200; loss: 0.9; acc: 0.88
Batch: 220; loss: 1.02; acc: 0.78
Batch: 240; loss: 0.96; acc: 0.86
Batch: 260; loss: 1.1; acc: 0.75
Batch: 280; loss: 1.03; acc: 0.84
Batch: 300; loss: 1.0; acc: 0.86
Batch: 320; loss: 1.0; acc: 0.8
Batch: 340; loss: 0.91; acc: 0.81
Batch: 360; loss: 0.89; acc: 0.88
Batch: 380; loss: 0.9; acc: 0.88
Batch: 400; loss: 0.99; acc: 0.78
Batch: 420; loss: 0.98; acc: 0.8
Batch: 440; loss: 0.99; acc: 0.8
Batch: 460; loss: 1.02; acc: 0.84
Batch: 480; loss: 0.97; acc: 0.81
Batch: 500; loss: 0.76; acc: 0.91
Batch: 520; loss: 0.91; acc: 0.83
Batch: 540; loss: 0.86; acc: 0.88
Batch: 560; loss: 0.74; acc: 0.97
Batch: 580; loss: 0.82; acc: 0.89
Batch: 600; loss: 0.82; acc: 0.84
Batch: 620; loss: 1.03; acc: 0.75
Batch: 640; loss: 0.88; acc: 0.81
Batch: 660; loss: 1.0; acc: 0.78
Batch: 680; loss: 0.87; acc: 0.86
Batch: 700; loss: 0.83; acc: 0.86
Batch: 720; loss: 0.79; acc: 0.89
Batch: 740; loss: 0.97; acc: 0.78
Batch: 760; loss: 0.72; acc: 0.94
Batch: 780; loss: 0.86; acc: 0.84
Train Epoch over. train_loss: 0.95; train_accuracy: 0.83 

3.1505594961345196e-05
1.2570225408126134e-05
Batch: 0; loss: 0.9; acc: 0.88
Batch: 20; loss: 0.91; acc: 0.78
Batch: 40; loss: 0.54; acc: 0.95
Batch: 60; loss: 0.82; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.97
Batch: 100; loss: 0.78; acc: 0.84
Batch: 120; loss: 0.96; acc: 0.81
Batch: 140; loss: 0.61; acc: 0.94
Val Epoch over. val_loss: 0.8093129396438599; val_accuracy: 0.8613654458598726 

The current subspace-distance is: 1.2570225408126134e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.88
Batch: 20; loss: 0.89; acc: 0.8
Batch: 40; loss: 0.92; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.91
Batch: 80; loss: 0.83; acc: 0.84
Batch: 100; loss: 0.85; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.83; acc: 0.84
Batch: 160; loss: 1.02; acc: 0.73
Batch: 180; loss: 0.89; acc: 0.83
Batch: 200; loss: 0.94; acc: 0.77
Batch: 220; loss: 0.94; acc: 0.75
Batch: 240; loss: 0.86; acc: 0.83
Batch: 260; loss: 0.91; acc: 0.8
Batch: 280; loss: 0.85; acc: 0.89
Batch: 300; loss: 0.8; acc: 0.84
Batch: 320; loss: 0.88; acc: 0.83
Batch: 340; loss: 0.93; acc: 0.84
Batch: 360; loss: 0.73; acc: 0.91
Batch: 380; loss: 0.94; acc: 0.75
Batch: 400; loss: 0.78; acc: 0.91
Batch: 420; loss: 0.84; acc: 0.8
Batch: 440; loss: 0.69; acc: 0.89
Batch: 460; loss: 0.87; acc: 0.81
Batch: 480; loss: 0.99; acc: 0.73
Batch: 500; loss: 0.95; acc: 0.77
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.78
Batch: 560; loss: 0.86; acc: 0.84
Batch: 580; loss: 0.9; acc: 0.81
Batch: 600; loss: 0.85; acc: 0.89
Batch: 620; loss: 0.92; acc: 0.77
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.81; acc: 0.78
Batch: 680; loss: 0.87; acc: 0.86
Batch: 700; loss: 0.82; acc: 0.84
Batch: 720; loss: 0.88; acc: 0.8
Batch: 740; loss: 0.84; acc: 0.84
Batch: 760; loss: 0.8; acc: 0.86
Batch: 780; loss: 0.83; acc: 0.86
Train Epoch over. train_loss: 0.82; train_accuracy: 0.84 

3.6270364944357425e-05
1.5267842172761448e-05
Batch: 0; loss: 0.77; acc: 0.89
Batch: 20; loss: 0.81; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.95
Batch: 100; loss: 0.68; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.704056768470509; val_accuracy: 0.8741042993630573 

The current subspace-distance is: 1.5267842172761448e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.81; acc: 0.81
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 0.83; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.89
Batch: 140; loss: 0.77; acc: 0.84
Batch: 160; loss: 0.81; acc: 0.8
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.89
Batch: 220; loss: 0.92; acc: 0.73
Batch: 240; loss: 0.7; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.92
Batch: 280; loss: 0.75; acc: 0.86
Batch: 300; loss: 0.7; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.89
Batch: 340; loss: 0.79; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.86
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.83; acc: 0.81
Batch: 460; loss: 0.9; acc: 0.78
Batch: 480; loss: 0.68; acc: 0.88
Batch: 500; loss: 0.83; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.91
Batch: 540; loss: 0.66; acc: 0.89
Batch: 560; loss: 0.66; acc: 0.91
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.8; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.65; acc: 0.91
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.61; acc: 0.94
Batch: 700; loss: 0.73; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.74; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.83; acc: 0.75
Train Epoch over. train_loss: 0.74; train_accuracy: 0.86 

3.9807047869544476e-05
1.682486072240863e-05
Batch: 0; loss: 0.68; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.59; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6286831342490615; val_accuracy: 0.8862460191082803 

The current subspace-distance is: 1.682486072240863e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.84
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.94
Batch: 140; loss: 0.67; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.66; acc: 0.92
Batch: 220; loss: 0.55; acc: 0.92
Batch: 240; loss: 0.62; acc: 0.89
Batch: 260; loss: 0.67; acc: 0.89
Batch: 280; loss: 0.7; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.63; acc: 0.89
Batch: 340; loss: 0.7; acc: 0.88
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.68; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.91
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.68; acc: 0.91
Batch: 540; loss: 0.68; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.89
Batch: 640; loss: 0.74; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.63; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.91
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.92
Train Epoch over. train_loss: 0.67; train_accuracy: 0.87 

4.370074384496547e-05
1.9395865820115432e-05
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.7; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.95
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5711209263391556; val_accuracy: 0.8958001592356688 

The current subspace-distance is: 1.9395865820115432e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.77
Batch: 200; loss: 0.51; acc: 0.92
Batch: 220; loss: 0.69; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.7; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.92
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.63; acc: 0.89
Batch: 360; loss: 0.61; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.94
Batch: 460; loss: 0.85; acc: 0.72
Batch: 480; loss: 0.61; acc: 0.92
Batch: 500; loss: 0.55; acc: 0.91
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.78; acc: 0.8
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.62; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.92
Batch: 660; loss: 0.69; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.91
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.62; train_accuracy: 0.87 

4.670513953897171e-05
2.098887307511177e-05
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.95
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.5205673484285925; val_accuracy: 0.9019705414012739 

The current subspace-distance is: 2.098887307511177e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.95
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.88
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.51; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.84
Batch: 380; loss: 0.57; acc: 0.97
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.54; acc: 0.92
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.92
Batch: 700; loss: 0.64; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.88 

4.9382957513444126e-05
2.2029349565855227e-05
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.48590035622666594; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 2.2029349565855227e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.94
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.94
Batch: 160; loss: 0.55; acc: 0.83
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.94
Batch: 240; loss: 0.55; acc: 0.91
Batch: 260; loss: 0.61; acc: 0.91
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.95
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.56; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.95
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.58; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

5.200907980906777e-05
2.3939550374052487e-05
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.97
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4571481252172191; val_accuracy: 0.9093351910828026 

The current subspace-distance is: 2.3939550374052487e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.55; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.97
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.32; acc: 0.97
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

5.561635043704882e-05
2.540481546020601e-05
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.97
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.98
Val Epoch over. val_loss: 0.43015829829653357; val_accuracy: 0.9116242038216561 

The current subspace-distance is: 2.540481546020601e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.95
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

5.6853263231460005e-05
2.542007132433355e-05
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.98
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4044833475617087; val_accuracy: 0.9144108280254777 

The current subspace-distance is: 2.542007132433355e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.97
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.8
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

5.8094530686503276e-05
2.531384780013468e-05
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.40258942905125344; val_accuracy: 0.9150079617834395 

The current subspace-distance is: 2.531384780013468e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.98
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.83
Batch: 660; loss: 0.4; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.97
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

6.0019869124516845e-05
2.8633005058509298e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.3976763836137808; val_accuracy: 0.9166003184713376 

The current subspace-distance is: 2.8633005058509298e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.8
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

6.082230174797587e-05
2.753922854026314e-05
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.3921919207853876; val_accuracy: 0.9174960191082803 

The current subspace-distance is: 2.753922854026314e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.97
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.55; acc: 0.81
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.939622860751115e-05
2.4806849978631362e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.38697839884241675; val_accuracy: 0.9169984076433121 

The current subspace-distance is: 2.4806849978631362e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

6.139616016298532e-05
2.8365400794427842e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.37974307263732715; val_accuracy: 0.9167993630573248 

The current subspace-distance is: 2.8365400794427842e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.97
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

6.165193190099671e-05
2.8167321943328716e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.38340123035725515; val_accuracy: 0.9152070063694268 

The current subspace-distance is: 2.8167321943328716e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.5; acc: 0.92
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.95
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.367320020217448e-05
3.0827319278614596e-05
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.3720079412695709; val_accuracy: 0.9184912420382165 

The current subspace-distance is: 3.0827319278614596e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.98
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.31291331956163e-05
2.9541510230046697e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3716387913864889; val_accuracy: 0.9166998407643312 

The current subspace-distance is: 2.9541510230046697e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.98
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.97
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.95
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.27608023933135e-05
2.887319897126872e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.36709683440673124; val_accuracy: 0.9188893312101911 

The current subspace-distance is: 2.887319897126872e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.94
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.347380258375779e-05
2.8395956178428605e-05
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.36571759175343116; val_accuracy: 0.9198845541401274 

The current subspace-distance is: 2.8395956178428605e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.24; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.97
Batch: 600; loss: 0.49; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.98
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.382976425811648e-05
2.8831966119469143e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.36325330168578274; val_accuracy: 0.9182921974522293 

The current subspace-distance is: 2.8831966119469143e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.95
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.29; acc: 0.97
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.477040005847812e-05
2.8989526981604286e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3618486174352609; val_accuracy: 0.9208797770700637 

The current subspace-distance is: 2.8989526981604286e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.459875294240192e-05
3.074200503760949e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.36026031519197355; val_accuracy: 0.919984076433121 

The current subspace-distance is: 3.074200503760949e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.21; acc: 0.98
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.98
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.45631953375414e-05
2.8830416340497322e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3570124348447581; val_accuracy: 0.9195859872611465 

The current subspace-distance is: 2.8830416340497322e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.97
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.97
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.473028770415112e-05
2.9583517971332185e-05
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3569795003362522; val_accuracy: 0.9197850318471338 

The current subspace-distance is: 2.9583517971332185e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.97
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.97
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.97
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.531165854539722e-05
3.0108780265436508e-05
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3625938047648995; val_accuracy: 0.919187898089172 

The current subspace-distance is: 3.0108780265436508e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.98
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.56227275612764e-05
3.1554536690237e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.358177610263703; val_accuracy: 0.9203821656050956 

The current subspace-distance is: 3.1554536690237e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.97
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.8
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.541128095705062e-05
2.8687543817795813e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.35378602773520595; val_accuracy: 0.9202826433121019 

The current subspace-distance is: 2.8687543817795813e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.98
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.27; acc: 1.0
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.98
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.508266233140603e-05
2.8000398742733523e-05
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3573749358676801; val_accuracy: 0.9209792993630573 

The current subspace-distance is: 2.8000398742733523e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.611091521335766e-05
3.1754338124301285e-05
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.35032372655952054; val_accuracy: 0.9198845541401274 

The current subspace-distance is: 3.1754338124301285e-05 

plots/subspace_training/table13slim/2020-01-29 16:00:01/N_13_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.11

The number of parameters is: 272274

The number of individual parameters is:

9
162
9
9
14
33012
14
14
27
99036
27
27
64
134784
64
64
4096
64
640
10
64
64

nonzero elements in E: 136136989
elements in E: 136137000
fraction nonzero: 0.9999999191990422
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.41; acc: 0.09
Batch: 20; loss: 2.1; acc: 0.25
Batch: 40; loss: 1.85; acc: 0.45
Batch: 60; loss: 1.72; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.62
Batch: 100; loss: 1.61; acc: 0.66
Batch: 120; loss: 1.56; acc: 0.61
Batch: 140; loss: 1.41; acc: 0.72
Batch: 160; loss: 1.63; acc: 0.61
Batch: 180; loss: 1.41; acc: 0.7
Batch: 200; loss: 1.35; acc: 0.69
Batch: 220; loss: 1.37; acc: 0.62
Batch: 240; loss: 1.29; acc: 0.81
Batch: 260; loss: 1.27; acc: 0.77
Batch: 280; loss: 1.28; acc: 0.7
Batch: 300; loss: 1.19; acc: 0.78
Batch: 320; loss: 1.18; acc: 0.72
Batch: 340; loss: 1.2; acc: 0.72
Batch: 360; loss: 1.25; acc: 0.7
Batch: 380; loss: 1.28; acc: 0.75
Batch: 400; loss: 1.14; acc: 0.78
Batch: 420; loss: 1.2; acc: 0.84
Batch: 440; loss: 1.14; acc: 0.8
Batch: 460; loss: 1.11; acc: 0.84
Batch: 480; loss: 1.06; acc: 0.8
Batch: 500; loss: 1.14; acc: 0.8
Batch: 520; loss: 1.08; acc: 0.75
Batch: 540; loss: 1.02; acc: 0.78
Batch: 560; loss: 1.01; acc: 0.84
Batch: 580; loss: 0.97; acc: 0.88
Batch: 600; loss: 1.13; acc: 0.77
Batch: 620; loss: 0.98; acc: 0.78
Batch: 640; loss: 1.04; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.83
Batch: 680; loss: 1.07; acc: 0.75
Batch: 700; loss: 1.03; acc: 0.75
Batch: 720; loss: 0.99; acc: 0.84
Batch: 740; loss: 1.09; acc: 0.77
Batch: 760; loss: 0.88; acc: 0.89
Batch: 780; loss: 0.79; acc: 0.92
Train Epoch over. train_loss: 1.25; train_accuracy: 0.73 

2.5995270334533416e-05
9.105382559937425e-06
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.7
Batch: 40; loss: 0.57; acc: 0.95
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.95
Batch: 100; loss: 0.83; acc: 0.91
Batch: 120; loss: 0.98; acc: 0.83
Batch: 140; loss: 0.73; acc: 0.91
Val Epoch over. val_loss: 0.8694850692323818; val_accuracy: 0.8503184713375797 

The current subspace-distance is: 9.105382559937425e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.78
Batch: 20; loss: 0.86; acc: 0.89
Batch: 40; loss: 0.83; acc: 0.88
Batch: 60; loss: 0.89; acc: 0.88
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.83; acc: 0.81
Batch: 120; loss: 0.73; acc: 0.92
Batch: 140; loss: 0.88; acc: 0.83
Batch: 160; loss: 0.85; acc: 0.89
Batch: 180; loss: 0.91; acc: 0.83
Batch: 200; loss: 0.98; acc: 0.83
Batch: 220; loss: 1.09; acc: 0.67
Batch: 240; loss: 0.76; acc: 0.92
Batch: 260; loss: 0.84; acc: 0.83
Batch: 280; loss: 1.03; acc: 0.73
Batch: 300; loss: 0.78; acc: 0.92
Batch: 320; loss: 0.75; acc: 0.92
Batch: 340; loss: 0.85; acc: 0.81
Batch: 360; loss: 0.91; acc: 0.81
Batch: 380; loss: 0.86; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.89
Batch: 420; loss: 0.81; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.88
Batch: 460; loss: 0.83; acc: 0.8
Batch: 480; loss: 0.89; acc: 0.84
Batch: 500; loss: 0.76; acc: 0.88
Batch: 520; loss: 0.83; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.84
Batch: 560; loss: 0.77; acc: 0.84
Batch: 580; loss: 0.73; acc: 0.94
Batch: 600; loss: 0.79; acc: 0.81
Batch: 620; loss: 0.74; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.89
Batch: 660; loss: 0.7; acc: 0.86
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.99; acc: 0.73
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.78; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.89
Batch: 780; loss: 0.65; acc: 0.89
Train Epoch over. train_loss: 0.82; train_accuracy: 0.85 

3.221176302758977e-05
1.2752145266858861e-05
Batch: 0; loss: 0.72; acc: 0.92
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.98
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.52; acc: 0.94
Val Epoch over. val_loss: 0.6626811811498775; val_accuracy: 0.8851512738853503 

The current subspace-distance is: 1.2752145266858861e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.68; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.92
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.79; acc: 0.83
Batch: 180; loss: 0.51; acc: 0.94
Batch: 200; loss: 0.72; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.91
Batch: 240; loss: 0.66; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.92
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.97
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.66; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.83
Batch: 440; loss: 0.69; acc: 0.91
Batch: 460; loss: 0.7; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.84
Batch: 560; loss: 0.82; acc: 0.73
Batch: 580; loss: 0.68; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.71; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.87 

3.690628000185825e-05
1.4974917576182634e-05
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.542836167440293; val_accuracy: 0.9028662420382165 

The current subspace-distance is: 1.4974917576182634e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.94
Batch: 140; loss: 0.64; acc: 0.89
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.52; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.68; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.94
Batch: 600; loss: 0.61; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.92
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.55; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.69; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.89 

4.089176218258217e-05
1.7254078557016328e-05
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.469546670367004; val_accuracy: 0.9098328025477707 

The current subspace-distance is: 1.7254078557016328e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.75
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.63; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.64; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.89 

4.380846439744346e-05
1.7980995835387148e-05
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.97
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.43016191424837535; val_accuracy: 0.9173964968152867 

The current subspace-distance is: 1.7980995835387148e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.98
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

4.655477459891699e-05
1.9385211999178864e-05
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.39655570515022154; val_accuracy: 0.9190883757961783 

The current subspace-distance is: 1.9385211999178864e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.78
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.8
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.92
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.97
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

4.992754475097172e-05
2.069725996989291e-05
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3666628610556293; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.069725996989291e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.156177212484181e-05
2.1796964574605227e-05
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.34928477493820675; val_accuracy: 0.9268511146496815 

The current subspace-distance is: 2.1796964574605227e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.98
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.439238157123327e-05
2.3796101231710054e-05
Batch: 0; loss: 0.35; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.33426584664044107; val_accuracy: 0.9286425159235668 

The current subspace-distance is: 2.3796101231710054e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.98
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.97
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.608029096038081e-05
2.499196125427261e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.31754044860981073; val_accuracy: 0.9298367834394905 

The current subspace-distance is: 2.499196125427261e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.7006473070941865e-05
2.4901271899580024e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3119803714524409; val_accuracy: 0.9313296178343949 

The current subspace-distance is: 2.4901271899580024e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.98
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.804000102216378e-05
2.5457249648752622e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3115780660585993; val_accuracy: 0.9315286624203821 

The current subspace-distance is: 2.5457249648752622e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.819839134346694e-05
2.4534054318792187e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3059342252505813; val_accuracy: 0.9324243630573248 

The current subspace-distance is: 2.4534054318792187e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.98
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.5; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.8543180784909055e-05
2.5640205421950668e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.30363974862607424; val_accuracy: 0.9323248407643312 

The current subspace-distance is: 2.5640205421950668e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.831729504279792e-05
2.592542841739487e-05
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.30108837859266124; val_accuracy: 0.9323248407643312 

The current subspace-distance is: 2.592542841739487e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.98
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.98
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.925398727413267e-05
2.578827661636751e-05
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.98
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.30018409222933895; val_accuracy: 0.9330214968152867 

The current subspace-distance is: 2.578827661636751e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.81
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.98
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.97
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.97022553847637e-05
2.4935305191320367e-05
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.29648175868828586; val_accuracy: 0.9335191082802548 

The current subspace-distance is: 2.4935305191320367e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.98
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.97
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.0490034229587764e-05
2.617462996568065e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.29470201591207723; val_accuracy: 0.9324243630573248 

The current subspace-distance is: 2.617462996568065e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.98
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.98
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

6.077977013774216e-05
2.5922518034349196e-05
Batch: 0; loss: 0.28; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.29218923419144505; val_accuracy: 0.9336186305732485 

The current subspace-distance is: 2.5922518034349196e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.0831363953184336e-05
2.6180665372521617e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2851766776886715; val_accuracy: 0.9331210191082803 

The current subspace-distance is: 2.6180665372521617e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.16; acc: 1.0
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.98
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.98
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.204604142112657e-05
2.7794914785772562e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2882711744517278; val_accuracy: 0.9332205414012739 

The current subspace-distance is: 2.7794914785772562e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.98
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.090222450438887e-05
2.5743698643054813e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2847901364895189; val_accuracy: 0.9339171974522293 

The current subspace-distance is: 2.5743698643054813e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.119805038906634e-05
2.512386708986014e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2860509948745655; val_accuracy: 0.9342157643312102 

The current subspace-distance is: 2.512386708986014e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.4; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.97
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.253035098779947e-05
2.8165737603558227e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2833696014846966; val_accuracy: 0.9345143312101911 

The current subspace-distance is: 2.8165737603558227e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.205704994499683e-05
2.6811760108103044e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.28348172209255257; val_accuracy: 0.9347133757961783 

The current subspace-distance is: 2.6811760108103044e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.98
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.224337994353846e-05
2.7367579605197534e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.28288271154757516; val_accuracy: 0.9345143312101911 

The current subspace-distance is: 2.7367579605197534e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.83
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.98
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.97
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

6.175513408379629e-05
2.6975420041708276e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2873589074725558; val_accuracy: 0.9334195859872612 

The current subspace-distance is: 2.6975420041708276e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.98
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.283001130213961e-05
2.789241807477083e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2801179577875289; val_accuracy: 0.9347133757961783 

The current subspace-distance is: 2.789241807477083e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.215456960489973e-05
2.719471012824215e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.28402678772902035; val_accuracy: 0.9351114649681529 

The current subspace-distance is: 2.719471012824215e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.98
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.269133882597089e-05
2.7458430849947035e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2823128563108718; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.7458430849947035e-05 

plots/subspace_training/table13slim/2020-01-29 16:00:01/N_13_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 16:00:01/N_13_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
