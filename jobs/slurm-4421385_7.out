model : table13slim
N : 7
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:46

Channel scaling factor: 2.6540064054180426

The number of parameters is: 270776

The number of individual parameters is:

22
396
22
22
32
45760
32
32
64
133120
64
64
64
86016
64
64
4096
64
640
10
64
64

nonzero elements in E: 13538799
elements in E: 13538800
fraction nonzero: 0.9999999261382102
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.43; acc: 0.09
Batch: 20; loss: 2.36; acc: 0.11
Batch: 40; loss: 2.46; acc: 0.06
Batch: 60; loss: 2.32; acc: 0.17
Batch: 80; loss: 2.38; acc: 0.11
Batch: 100; loss: 2.27; acc: 0.22
Batch: 120; loss: 2.24; acc: 0.27
Batch: 140; loss: 2.33; acc: 0.14
Batch: 160; loss: 2.26; acc: 0.17
Batch: 180; loss: 2.2; acc: 0.2
Batch: 200; loss: 2.16; acc: 0.23
Batch: 220; loss: 2.18; acc: 0.27
Batch: 240; loss: 2.25; acc: 0.2
Batch: 260; loss: 2.22; acc: 0.19
Batch: 280; loss: 2.19; acc: 0.17
Batch: 300; loss: 2.15; acc: 0.33
Batch: 320; loss: 2.14; acc: 0.22
Batch: 340; loss: 2.2; acc: 0.2
Batch: 360; loss: 2.16; acc: 0.19
Batch: 380; loss: 2.23; acc: 0.23
Batch: 400; loss: 2.19; acc: 0.16
Batch: 420; loss: 2.14; acc: 0.3
Batch: 440; loss: 2.19; acc: 0.2
Batch: 460; loss: 2.06; acc: 0.28
Batch: 480; loss: 2.15; acc: 0.28
Batch: 500; loss: 2.07; acc: 0.27
Batch: 520; loss: 1.94; acc: 0.39
Batch: 540; loss: 1.97; acc: 0.38
Batch: 560; loss: 2.05; acc: 0.33
Batch: 580; loss: 2.1; acc: 0.3
Batch: 600; loss: 2.03; acc: 0.33
Batch: 620; loss: 2.05; acc: 0.34
Batch: 640; loss: 2.09; acc: 0.27
Batch: 660; loss: 2.12; acc: 0.22
Batch: 680; loss: 2.06; acc: 0.28
Batch: 700; loss: 1.87; acc: 0.44
Batch: 720; loss: 2.04; acc: 0.3
Batch: 740; loss: 1.98; acc: 0.39
Batch: 760; loss: 1.99; acc: 0.28
Batch: 780; loss: 2.0; acc: 0.34
Train Epoch over. train_loss: 2.16; train_accuracy: 0.24 

2.2564581740880385e-05
4.618187176674837e-06
Batch: 0; loss: 2.1; acc: 0.34
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 1.8; acc: 0.45
Batch: 60; loss: 1.91; acc: 0.38
Batch: 80; loss: 1.83; acc: 0.38
Batch: 100; loss: 1.97; acc: 0.38
Batch: 120; loss: 2.13; acc: 0.27
Batch: 140; loss: 1.96; acc: 0.33
Val Epoch over. val_loss: 1.9800636168498142; val_accuracy: 0.34942277070063693 

The current subspace-distance is: 4.618187176674837e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.03; acc: 0.34
Batch: 20; loss: 1.89; acc: 0.44
Batch: 40; loss: 2.0; acc: 0.33
Batch: 60; loss: 1.98; acc: 0.44
Batch: 80; loss: 1.97; acc: 0.39
Batch: 100; loss: 1.94; acc: 0.44
Batch: 120; loss: 2.04; acc: 0.41
Batch: 140; loss: 2.08; acc: 0.28
Batch: 160; loss: 2.03; acc: 0.33
Batch: 180; loss: 1.96; acc: 0.42
Batch: 200; loss: 2.03; acc: 0.38
Batch: 220; loss: 2.06; acc: 0.28
Batch: 240; loss: 1.92; acc: 0.36
Batch: 260; loss: 1.97; acc: 0.42
Batch: 280; loss: 1.79; acc: 0.48
Batch: 300; loss: 1.84; acc: 0.45
Batch: 320; loss: 2.02; acc: 0.31
Batch: 340; loss: 1.84; acc: 0.44
Batch: 360; loss: 2.0; acc: 0.38
Batch: 380; loss: 1.95; acc: 0.36
Batch: 400; loss: 1.85; acc: 0.47
Batch: 420; loss: 1.87; acc: 0.44
Batch: 440; loss: 1.98; acc: 0.33
Batch: 460; loss: 1.88; acc: 0.45
Batch: 480; loss: 1.93; acc: 0.36
Batch: 500; loss: 1.97; acc: 0.39
Batch: 520; loss: 1.87; acc: 0.42
Batch: 540; loss: 1.82; acc: 0.47
Batch: 560; loss: 1.97; acc: 0.39
Batch: 580; loss: 1.83; acc: 0.41
Batch: 600; loss: 1.84; acc: 0.44
Batch: 620; loss: 1.89; acc: 0.45
Batch: 640; loss: 1.78; acc: 0.5
Batch: 660; loss: 1.92; acc: 0.41
Batch: 680; loss: 1.9; acc: 0.34
Batch: 700; loss: 1.86; acc: 0.41
Batch: 720; loss: 1.84; acc: 0.47
Batch: 740; loss: 1.79; acc: 0.42
Batch: 760; loss: 1.84; acc: 0.44
Batch: 780; loss: 1.83; acc: 0.38
Train Epoch over. train_loss: 1.91; train_accuracy: 0.4 

2.5253102649003267e-05
6.954005129955476e-06
Batch: 0; loss: 1.89; acc: 0.39
Batch: 20; loss: 1.88; acc: 0.44
Batch: 40; loss: 1.6; acc: 0.64
Batch: 60; loss: 1.76; acc: 0.62
Batch: 80; loss: 1.69; acc: 0.52
Batch: 100; loss: 1.83; acc: 0.38
Batch: 120; loss: 1.91; acc: 0.39
Batch: 140; loss: 1.64; acc: 0.56
Val Epoch over. val_loss: 1.795583977820767; val_accuracy: 0.473328025477707 

The current subspace-distance is: 6.954005129955476e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.82; acc: 0.44
Batch: 20; loss: 1.87; acc: 0.47
Batch: 40; loss: 1.8; acc: 0.42
Batch: 60; loss: 1.85; acc: 0.42
Batch: 80; loss: 1.83; acc: 0.47
Batch: 100; loss: 1.83; acc: 0.47
Batch: 120; loss: 1.93; acc: 0.36
Batch: 140; loss: 1.86; acc: 0.42
Batch: 160; loss: 1.95; acc: 0.38
Batch: 180; loss: 1.83; acc: 0.45
Batch: 200; loss: 1.72; acc: 0.53
Batch: 220; loss: 1.88; acc: 0.36
Batch: 240; loss: 1.71; acc: 0.56
Batch: 260; loss: 1.85; acc: 0.44
Batch: 280; loss: 1.69; acc: 0.52
Batch: 300; loss: 1.74; acc: 0.42
Batch: 320; loss: 1.77; acc: 0.53
Batch: 340; loss: 1.85; acc: 0.42
Batch: 360; loss: 1.7; acc: 0.52
Batch: 380; loss: 1.74; acc: 0.56
Batch: 400; loss: 1.74; acc: 0.47
Batch: 420; loss: 1.74; acc: 0.52
Batch: 440; loss: 1.69; acc: 0.5
Batch: 460; loss: 1.81; acc: 0.45
Batch: 480; loss: 1.78; acc: 0.45
Batch: 500; loss: 1.72; acc: 0.55
Batch: 520; loss: 1.68; acc: 0.62
Batch: 540; loss: 1.69; acc: 0.56
Batch: 560; loss: 1.74; acc: 0.48
Batch: 580; loss: 1.73; acc: 0.47
Batch: 600; loss: 1.74; acc: 0.5
Batch: 620; loss: 1.78; acc: 0.45
Batch: 640; loss: 1.58; acc: 0.64
Batch: 660; loss: 1.66; acc: 0.61
Batch: 680; loss: 1.74; acc: 0.45
Batch: 700; loss: 1.74; acc: 0.39
Batch: 720; loss: 1.66; acc: 0.59
Batch: 740; loss: 1.7; acc: 0.41
Batch: 760; loss: 1.74; acc: 0.5
Batch: 780; loss: 1.75; acc: 0.48
Train Epoch over. train_loss: 1.75; train_accuracy: 0.49 

2.936777309514582e-05
9.559739737596828e-06
Batch: 0; loss: 1.69; acc: 0.52
Batch: 20; loss: 1.74; acc: 0.45
Batch: 40; loss: 1.46; acc: 0.61
Batch: 60; loss: 1.66; acc: 0.64
Batch: 80; loss: 1.56; acc: 0.58
Batch: 100; loss: 1.69; acc: 0.58
Batch: 120; loss: 1.71; acc: 0.53
Batch: 140; loss: 1.51; acc: 0.64
Val Epoch over. val_loss: 1.6514860049934144; val_accuracy: 0.5555334394904459 

The current subspace-distance is: 9.559739737596828e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.44
Batch: 20; loss: 1.73; acc: 0.52
Batch: 40; loss: 1.82; acc: 0.34
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.6; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.5
Batch: 120; loss: 1.72; acc: 0.53
Batch: 140; loss: 1.65; acc: 0.48
Batch: 160; loss: 1.72; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.45
Batch: 200; loss: 1.72; acc: 0.53
Batch: 220; loss: 1.6; acc: 0.62
Batch: 240; loss: 1.73; acc: 0.5
Batch: 260; loss: 1.71; acc: 0.5
Batch: 280; loss: 1.63; acc: 0.59
Batch: 300; loss: 1.66; acc: 0.53
Batch: 320; loss: 1.71; acc: 0.48
Batch: 340; loss: 1.53; acc: 0.55
Batch: 360; loss: 1.63; acc: 0.58
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.62; acc: 0.59
Batch: 420; loss: 1.66; acc: 0.59
Batch: 440; loss: 1.52; acc: 0.66
Batch: 460; loss: 1.52; acc: 0.62
Batch: 480; loss: 1.47; acc: 0.66
Batch: 500; loss: 1.68; acc: 0.55
Batch: 520; loss: 1.68; acc: 0.52
Batch: 540; loss: 1.61; acc: 0.53
Batch: 560; loss: 1.54; acc: 0.64
Batch: 580; loss: 1.71; acc: 0.47
Batch: 600; loss: 1.63; acc: 0.53
Batch: 620; loss: 1.76; acc: 0.53
Batch: 640; loss: 1.67; acc: 0.56
Batch: 660; loss: 1.73; acc: 0.48
Batch: 680; loss: 1.72; acc: 0.42
Batch: 700; loss: 1.63; acc: 0.58
Batch: 720; loss: 1.66; acc: 0.59
Batch: 740; loss: 1.62; acc: 0.52
Batch: 760; loss: 1.59; acc: 0.61
Batch: 780; loss: 1.61; acc: 0.61
Train Epoch over. train_loss: 1.66; train_accuracy: 0.53 

3.2353520509786904e-05
1.3420646610029507e-05
Batch: 0; loss: 1.62; acc: 0.53
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.41; acc: 0.61
Batch: 60; loss: 1.6; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.62
Batch: 100; loss: 1.63; acc: 0.58
Batch: 120; loss: 1.61; acc: 0.58
Batch: 140; loss: 1.46; acc: 0.72
Val Epoch over. val_loss: 1.5967766137639428; val_accuracy: 0.5687699044585988 

The current subspace-distance is: 1.3420646610029507e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.5; acc: 0.72
Batch: 40; loss: 1.74; acc: 0.48
Batch: 60; loss: 1.55; acc: 0.58
Batch: 80; loss: 1.59; acc: 0.61
Batch: 100; loss: 1.49; acc: 0.61
Batch: 120; loss: 1.65; acc: 0.52
Batch: 140; loss: 1.67; acc: 0.53
Batch: 160; loss: 1.59; acc: 0.56
Batch: 180; loss: 1.66; acc: 0.53
Batch: 200; loss: 1.56; acc: 0.64
Batch: 220; loss: 1.64; acc: 0.53
Batch: 240; loss: 1.64; acc: 0.61
Batch: 260; loss: 1.79; acc: 0.45
Batch: 280; loss: 1.52; acc: 0.61
Batch: 300; loss: 1.66; acc: 0.47
Batch: 320; loss: 1.63; acc: 0.45
Batch: 340; loss: 1.71; acc: 0.55
Batch: 360; loss: 1.6; acc: 0.55
Batch: 380; loss: 1.58; acc: 0.53
Batch: 400; loss: 1.6; acc: 0.58
Batch: 420; loss: 1.56; acc: 0.56
Batch: 440; loss: 1.58; acc: 0.55
Batch: 460; loss: 1.55; acc: 0.58
Batch: 480; loss: 1.53; acc: 0.53
Batch: 500; loss: 1.52; acc: 0.55
Batch: 520; loss: 1.53; acc: 0.61
Batch: 540; loss: 1.8; acc: 0.41
Batch: 560; loss: 1.55; acc: 0.55
Batch: 580; loss: 1.6; acc: 0.53
Batch: 600; loss: 1.61; acc: 0.52
Batch: 620; loss: 1.62; acc: 0.55
Batch: 640; loss: 1.51; acc: 0.59
Batch: 660; loss: 1.73; acc: 0.44
Batch: 680; loss: 1.67; acc: 0.52
Batch: 700; loss: 1.61; acc: 0.53
Batch: 720; loss: 1.59; acc: 0.55
Batch: 740; loss: 1.71; acc: 0.45
Batch: 760; loss: 1.68; acc: 0.48
Batch: 780; loss: 1.68; acc: 0.55
Train Epoch over. train_loss: 1.62; train_accuracy: 0.54 

3.4767119359457865e-05
1.2723039617412724e-05
Batch: 0; loss: 1.59; acc: 0.55
Batch: 20; loss: 1.65; acc: 0.53
Batch: 40; loss: 1.38; acc: 0.62
Batch: 60; loss: 1.53; acc: 0.55
Batch: 80; loss: 1.47; acc: 0.62
Batch: 100; loss: 1.61; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.66
Batch: 140; loss: 1.43; acc: 0.67
Val Epoch over. val_loss: 1.5561509747414073; val_accuracy: 0.5705613057324841 

The current subspace-distance is: 1.2723039617412724e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.64; acc: 0.44
Batch: 20; loss: 1.62; acc: 0.53
Batch: 40; loss: 1.62; acc: 0.52
Batch: 60; loss: 1.66; acc: 0.58
Batch: 80; loss: 1.68; acc: 0.5
Batch: 100; loss: 1.54; acc: 0.62
Batch: 120; loss: 1.53; acc: 0.64
Batch: 140; loss: 1.53; acc: 0.64
Batch: 160; loss: 1.59; acc: 0.58
Batch: 180; loss: 1.52; acc: 0.59
Batch: 200; loss: 1.42; acc: 0.64
Batch: 220; loss: 1.5; acc: 0.66
Batch: 240; loss: 1.56; acc: 0.55
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.49; acc: 0.61
Batch: 300; loss: 1.51; acc: 0.52
Batch: 320; loss: 1.54; acc: 0.56
Batch: 340; loss: 1.62; acc: 0.59
Batch: 360; loss: 1.72; acc: 0.45
Batch: 380; loss: 1.61; acc: 0.55
Batch: 400; loss: 1.75; acc: 0.42
Batch: 420; loss: 1.61; acc: 0.53
Batch: 440; loss: 1.56; acc: 0.62
Batch: 460; loss: 1.46; acc: 0.62
Batch: 480; loss: 1.59; acc: 0.55
Batch: 500; loss: 1.66; acc: 0.42
Batch: 520; loss: 1.78; acc: 0.45
Batch: 540; loss: 1.62; acc: 0.48
Batch: 560; loss: 1.51; acc: 0.64
Batch: 580; loss: 1.58; acc: 0.58
Batch: 600; loss: 1.55; acc: 0.58
Batch: 620; loss: 1.61; acc: 0.5
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.57; acc: 0.56
Batch: 680; loss: 1.65; acc: 0.41
Batch: 700; loss: 1.71; acc: 0.5
Batch: 720; loss: 1.62; acc: 0.53
Batch: 740; loss: 1.63; acc: 0.53
Batch: 760; loss: 1.48; acc: 0.56
Batch: 780; loss: 1.58; acc: 0.53
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

3.697810461744666e-05
1.3797122846881393e-05
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.55
Batch: 40; loss: 1.36; acc: 0.61
Batch: 60; loss: 1.51; acc: 0.55
Batch: 80; loss: 1.48; acc: 0.61
Batch: 100; loss: 1.58; acc: 0.66
Batch: 120; loss: 1.53; acc: 0.62
Batch: 140; loss: 1.41; acc: 0.62
Val Epoch over. val_loss: 1.543093682094744; val_accuracy: 0.5672770700636943 

The current subspace-distance is: 1.3797122846881393e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.47
Batch: 20; loss: 1.52; acc: 0.52
Batch: 40; loss: 1.74; acc: 0.5
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.58
Batch: 100; loss: 1.51; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.42
Batch: 140; loss: 1.55; acc: 0.56
Batch: 160; loss: 1.63; acc: 0.47
Batch: 180; loss: 1.51; acc: 0.62
Batch: 200; loss: 1.64; acc: 0.47
Batch: 220; loss: 1.56; acc: 0.56
Batch: 240; loss: 1.49; acc: 0.58
Batch: 260; loss: 1.58; acc: 0.59
Batch: 280; loss: 1.64; acc: 0.56
Batch: 300; loss: 1.48; acc: 0.55
Batch: 320; loss: 1.62; acc: 0.52
Batch: 340; loss: 1.56; acc: 0.56
Batch: 360; loss: 1.47; acc: 0.61
Batch: 380; loss: 1.56; acc: 0.5
Batch: 400; loss: 1.58; acc: 0.52
Batch: 420; loss: 1.39; acc: 0.66
Batch: 440; loss: 1.68; acc: 0.48
Batch: 460; loss: 1.55; acc: 0.45
Batch: 480; loss: 1.6; acc: 0.56
Batch: 500; loss: 1.53; acc: 0.55
Batch: 520; loss: 1.61; acc: 0.56
Batch: 540; loss: 1.57; acc: 0.56
Batch: 560; loss: 1.43; acc: 0.62
Batch: 580; loss: 1.45; acc: 0.61
Batch: 600; loss: 1.41; acc: 0.58
Batch: 620; loss: 1.65; acc: 0.48
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.51; acc: 0.64
Batch: 680; loss: 1.54; acc: 0.59
Batch: 700; loss: 1.74; acc: 0.42
Batch: 720; loss: 1.54; acc: 0.56
Batch: 740; loss: 1.41; acc: 0.59
Batch: 760; loss: 1.43; acc: 0.66
Batch: 780; loss: 1.56; acc: 0.56
Train Epoch over. train_loss: 1.56; train_accuracy: 0.55 

3.775291406782344e-05
1.1940237527596764e-05
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.59
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.48; acc: 0.61
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.53; acc: 0.67
Batch: 120; loss: 1.52; acc: 0.61
Batch: 140; loss: 1.36; acc: 0.66
Val Epoch over. val_loss: 1.517037347623497; val_accuracy: 0.57703025477707 

The current subspace-distance is: 1.1940237527596764e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.62
Batch: 20; loss: 1.45; acc: 0.61
Batch: 40; loss: 1.66; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.61
Batch: 80; loss: 1.56; acc: 0.56
Batch: 100; loss: 1.72; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.59
Batch: 160; loss: 1.57; acc: 0.58
Batch: 180; loss: 1.52; acc: 0.58
Batch: 200; loss: 1.54; acc: 0.56
Batch: 220; loss: 1.57; acc: 0.52
Batch: 240; loss: 1.58; acc: 0.56
Batch: 260; loss: 1.56; acc: 0.45
Batch: 280; loss: 1.51; acc: 0.64
Batch: 300; loss: 1.69; acc: 0.47
Batch: 320; loss: 1.51; acc: 0.53
Batch: 340; loss: 1.55; acc: 0.56
Batch: 360; loss: 1.53; acc: 0.58
Batch: 380; loss: 1.72; acc: 0.38
Batch: 400; loss: 1.66; acc: 0.45
Batch: 420; loss: 1.54; acc: 0.55
Batch: 440; loss: 1.61; acc: 0.56
Batch: 460; loss: 1.49; acc: 0.59
Batch: 480; loss: 1.47; acc: 0.62
Batch: 500; loss: 1.62; acc: 0.48
Batch: 520; loss: 1.49; acc: 0.56
Batch: 540; loss: 1.44; acc: 0.61
Batch: 560; loss: 1.52; acc: 0.67
Batch: 580; loss: 1.45; acc: 0.59
Batch: 600; loss: 1.56; acc: 0.44
Batch: 620; loss: 1.5; acc: 0.55
Batch: 640; loss: 1.47; acc: 0.62
Batch: 660; loss: 1.53; acc: 0.55
Batch: 680; loss: 1.59; acc: 0.53
Batch: 700; loss: 1.49; acc: 0.58
Batch: 720; loss: 1.64; acc: 0.52
Batch: 740; loss: 1.51; acc: 0.59
Batch: 760; loss: 1.63; acc: 0.47
Batch: 780; loss: 1.56; acc: 0.44
Train Epoch over. train_loss: 1.55; train_accuracy: 0.55 

4.1444753151154146e-05
1.7063004634110257e-05
Batch: 0; loss: 1.54; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.32; acc: 0.66
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.44; acc: 0.59
Batch: 100; loss: 1.49; acc: 0.62
Batch: 120; loss: 1.5; acc: 0.62
Batch: 140; loss: 1.35; acc: 0.64
Val Epoch over. val_loss: 1.5001628247036296; val_accuracy: 0.5761345541401274 

The current subspace-distance is: 1.7063004634110257e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.55
Batch: 20; loss: 1.46; acc: 0.64
Batch: 40; loss: 1.66; acc: 0.5
Batch: 60; loss: 1.52; acc: 0.55
Batch: 80; loss: 1.62; acc: 0.47
Batch: 100; loss: 1.4; acc: 0.62
Batch: 120; loss: 1.64; acc: 0.45
Batch: 140; loss: 1.53; acc: 0.56
Batch: 160; loss: 1.5; acc: 0.59
Batch: 180; loss: 1.51; acc: 0.59
Batch: 200; loss: 1.63; acc: 0.45
Batch: 220; loss: 1.48; acc: 0.62
Batch: 240; loss: 1.55; acc: 0.55
Batch: 260; loss: 1.67; acc: 0.53
Batch: 280; loss: 1.41; acc: 0.59
Batch: 300; loss: 1.61; acc: 0.53
Batch: 320; loss: 1.64; acc: 0.53
Batch: 340; loss: 1.56; acc: 0.56
Batch: 360; loss: 1.51; acc: 0.64
Batch: 380; loss: 1.56; acc: 0.44
Batch: 400; loss: 1.4; acc: 0.66
Batch: 420; loss: 1.58; acc: 0.47
Batch: 440; loss: 1.44; acc: 0.64
Batch: 460; loss: 1.55; acc: 0.55
Batch: 480; loss: 1.44; acc: 0.61
Batch: 500; loss: 1.61; acc: 0.45
Batch: 520; loss: 1.39; acc: 0.61
Batch: 540; loss: 1.35; acc: 0.62
Batch: 560; loss: 1.59; acc: 0.58
Batch: 580; loss: 1.59; acc: 0.53
Batch: 600; loss: 1.41; acc: 0.62
Batch: 620; loss: 1.62; acc: 0.55
Batch: 640; loss: 1.61; acc: 0.48
Batch: 660; loss: 1.5; acc: 0.58
Batch: 680; loss: 1.57; acc: 0.42
Batch: 700; loss: 1.62; acc: 0.44
Batch: 720; loss: 1.53; acc: 0.61
Batch: 740; loss: 1.49; acc: 0.64
Batch: 760; loss: 1.45; acc: 0.61
Batch: 780; loss: 1.46; acc: 0.64
Train Epoch over. train_loss: 1.54; train_accuracy: 0.55 

4.087034540134482e-05
1.3704591765417717e-05
Batch: 0; loss: 1.53; acc: 0.58
Batch: 20; loss: 1.57; acc: 0.58
Batch: 40; loss: 1.29; acc: 0.69
Batch: 60; loss: 1.44; acc: 0.59
Batch: 80; loss: 1.4; acc: 0.62
Batch: 100; loss: 1.48; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.64
Batch: 140; loss: 1.35; acc: 0.62
Val Epoch over. val_loss: 1.4842119110617669; val_accuracy: 0.5873805732484076 

The current subspace-distance is: 1.3704591765417717e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.55
Batch: 20; loss: 1.42; acc: 0.64
Batch: 40; loss: 1.56; acc: 0.56
Batch: 60; loss: 1.65; acc: 0.52
Batch: 80; loss: 1.52; acc: 0.48
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.58; acc: 0.5
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.52
Batch: 200; loss: 1.67; acc: 0.45
Batch: 220; loss: 1.41; acc: 0.66
Batch: 240; loss: 1.54; acc: 0.5
Batch: 260; loss: 1.56; acc: 0.5
Batch: 280; loss: 1.61; acc: 0.53
Batch: 300; loss: 1.44; acc: 0.58
Batch: 320; loss: 1.55; acc: 0.61
Batch: 340; loss: 1.66; acc: 0.47
Batch: 360; loss: 1.48; acc: 0.56
Batch: 380; loss: 1.38; acc: 0.64
Batch: 400; loss: 1.6; acc: 0.53
Batch: 420; loss: 1.66; acc: 0.47
Batch: 440; loss: 1.58; acc: 0.55
Batch: 460; loss: 1.53; acc: 0.55
Batch: 480; loss: 1.59; acc: 0.52
Batch: 500; loss: 1.38; acc: 0.62
Batch: 520; loss: 1.39; acc: 0.62
Batch: 540; loss: 1.4; acc: 0.64
Batch: 560; loss: 1.48; acc: 0.55
Batch: 580; loss: 1.7; acc: 0.44
Batch: 600; loss: 1.59; acc: 0.55
Batch: 620; loss: 1.6; acc: 0.52
Batch: 640; loss: 1.55; acc: 0.47
Batch: 660; loss: 1.55; acc: 0.55
Batch: 680; loss: 1.48; acc: 0.64
Batch: 700; loss: 1.62; acc: 0.53
Batch: 720; loss: 1.39; acc: 0.67
Batch: 740; loss: 1.57; acc: 0.52
Batch: 760; loss: 1.42; acc: 0.67
Batch: 780; loss: 1.54; acc: 0.52
Train Epoch over. train_loss: 1.53; train_accuracy: 0.56 

4.31623739132192e-05
1.7481588656664826e-05
Batch: 0; loss: 1.53; acc: 0.61
Batch: 20; loss: 1.57; acc: 0.53
Batch: 40; loss: 1.29; acc: 0.7
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.43; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.64
Batch: 120; loss: 1.52; acc: 0.56
Batch: 140; loss: 1.36; acc: 0.64
Val Epoch over. val_loss: 1.4897443018141825; val_accuracy: 0.5842953821656051 

The current subspace-distance is: 1.7481588656664826e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.43; acc: 0.69
Batch: 20; loss: 1.55; acc: 0.58
Batch: 40; loss: 1.72; acc: 0.45
Batch: 60; loss: 1.5; acc: 0.53
Batch: 80; loss: 1.61; acc: 0.44
Batch: 100; loss: 1.43; acc: 0.67
Batch: 120; loss: 1.68; acc: 0.52
Batch: 140; loss: 1.56; acc: 0.55
Batch: 160; loss: 1.53; acc: 0.56
Batch: 180; loss: 1.51; acc: 0.52
Batch: 200; loss: 1.45; acc: 0.64
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.37; acc: 0.64
Batch: 260; loss: 1.51; acc: 0.53
Batch: 280; loss: 1.48; acc: 0.62
Batch: 300; loss: 1.6; acc: 0.53
Batch: 320; loss: 1.43; acc: 0.61
Batch: 340; loss: 1.55; acc: 0.59
Batch: 360; loss: 1.53; acc: 0.59
Batch: 380; loss: 1.42; acc: 0.58
Batch: 400; loss: 1.51; acc: 0.52
Batch: 420; loss: 1.48; acc: 0.62
Batch: 440; loss: 1.47; acc: 0.5
Batch: 460; loss: 1.59; acc: 0.56
Batch: 480; loss: 1.49; acc: 0.61
Batch: 500; loss: 1.54; acc: 0.56
Batch: 520; loss: 1.48; acc: 0.61
Batch: 540; loss: 1.43; acc: 0.64
Batch: 560; loss: 1.65; acc: 0.45
Batch: 580; loss: 1.63; acc: 0.45
Batch: 600; loss: 1.53; acc: 0.56
Batch: 620; loss: 1.57; acc: 0.53
Batch: 640; loss: 1.59; acc: 0.48
Batch: 660; loss: 1.52; acc: 0.56
Batch: 680; loss: 1.6; acc: 0.53
Batch: 700; loss: 1.65; acc: 0.5
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.5; acc: 0.59
Batch: 760; loss: 1.49; acc: 0.55
Batch: 780; loss: 1.58; acc: 0.55
Train Epoch over. train_loss: 1.52; train_accuracy: 0.56 

4.321287269704044e-05
1.640846130612772e-05
Batch: 0; loss: 1.5; acc: 0.59
Batch: 20; loss: 1.55; acc: 0.59
Batch: 40; loss: 1.26; acc: 0.72
Batch: 60; loss: 1.43; acc: 0.62
Batch: 80; loss: 1.38; acc: 0.64
Batch: 100; loss: 1.43; acc: 0.66
Batch: 120; loss: 1.5; acc: 0.58
Batch: 140; loss: 1.31; acc: 0.64
Val Epoch over. val_loss: 1.456951618953875; val_accuracy: 0.5981289808917197 

The current subspace-distance is: 1.640846130612772e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.56; acc: 0.53
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.53; acc: 0.53
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.49; acc: 0.58
Batch: 160; loss: 1.63; acc: 0.45
Batch: 180; loss: 1.52; acc: 0.55
Batch: 200; loss: 1.34; acc: 0.64
Batch: 220; loss: 1.54; acc: 0.56
Batch: 240; loss: 1.46; acc: 0.61
Batch: 260; loss: 1.44; acc: 0.61
Batch: 280; loss: 1.45; acc: 0.56
Batch: 300; loss: 1.64; acc: 0.48
Batch: 320; loss: 1.51; acc: 0.55
Batch: 340; loss: 1.48; acc: 0.61
Batch: 360; loss: 1.54; acc: 0.58
Batch: 380; loss: 1.58; acc: 0.5
Batch: 400; loss: 1.47; acc: 0.58
Batch: 420; loss: 1.7; acc: 0.42
Batch: 440; loss: 1.51; acc: 0.62
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.49; acc: 0.55
Batch: 500; loss: 1.62; acc: 0.52
Batch: 520; loss: 1.54; acc: 0.56
Batch: 540; loss: 1.47; acc: 0.52
Batch: 560; loss: 1.41; acc: 0.64
Batch: 580; loss: 1.47; acc: 0.55
Batch: 600; loss: 1.54; acc: 0.53
Batch: 620; loss: 1.58; acc: 0.47
Batch: 640; loss: 1.43; acc: 0.64
Batch: 660; loss: 1.43; acc: 0.62
Batch: 680; loss: 1.46; acc: 0.55
Batch: 700; loss: 1.36; acc: 0.69
Batch: 720; loss: 1.4; acc: 0.64
Batch: 740; loss: 1.62; acc: 0.52
Batch: 760; loss: 1.39; acc: 0.64
Batch: 780; loss: 1.43; acc: 0.61
Train Epoch over. train_loss: 1.52; train_accuracy: 0.56 

4.4243057345738634e-05
1.8271952285431325e-05
Batch: 0; loss: 1.53; acc: 0.59
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.27; acc: 0.7
Batch: 60; loss: 1.45; acc: 0.61
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.66
Batch: 120; loss: 1.52; acc: 0.55
Batch: 140; loss: 1.36; acc: 0.64
Val Epoch over. val_loss: 1.483408447283848; val_accuracy: 0.5875796178343949 

The current subspace-distance is: 1.8271952285431325e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.47; acc: 0.5
Batch: 60; loss: 1.64; acc: 0.44
Batch: 80; loss: 1.63; acc: 0.48
Batch: 100; loss: 1.56; acc: 0.53
Batch: 120; loss: 1.64; acc: 0.44
Batch: 140; loss: 1.43; acc: 0.64
Batch: 160; loss: 1.58; acc: 0.55
Batch: 180; loss: 1.44; acc: 0.58
Batch: 200; loss: 1.49; acc: 0.55
Batch: 220; loss: 1.58; acc: 0.52
Batch: 240; loss: 1.55; acc: 0.61
Batch: 260; loss: 1.39; acc: 0.66
Batch: 280; loss: 1.44; acc: 0.61
Batch: 300; loss: 1.51; acc: 0.56
Batch: 320; loss: 1.52; acc: 0.59
Batch: 340; loss: 1.35; acc: 0.64
Batch: 360; loss: 1.48; acc: 0.56
Batch: 380; loss: 1.58; acc: 0.58
Batch: 400; loss: 1.41; acc: 0.62
Batch: 420; loss: 1.6; acc: 0.53
Batch: 440; loss: 1.58; acc: 0.53
Batch: 460; loss: 1.52; acc: 0.61
Batch: 480; loss: 1.53; acc: 0.53
Batch: 500; loss: 1.49; acc: 0.58
Batch: 520; loss: 1.43; acc: 0.59
Batch: 540; loss: 1.56; acc: 0.58
Batch: 560; loss: 1.53; acc: 0.52
Batch: 580; loss: 1.59; acc: 0.5
Batch: 600; loss: 1.56; acc: 0.52
Batch: 620; loss: 1.45; acc: 0.56
Batch: 640; loss: 1.68; acc: 0.48
Batch: 660; loss: 1.56; acc: 0.55
Batch: 680; loss: 1.33; acc: 0.62
Batch: 700; loss: 1.49; acc: 0.56
Batch: 720; loss: 1.37; acc: 0.64
Batch: 740; loss: 1.52; acc: 0.55
Batch: 760; loss: 1.55; acc: 0.53
Batch: 780; loss: 1.47; acc: 0.58
Train Epoch over. train_loss: 1.51; train_accuracy: 0.56 

4.4711156078847125e-05
1.670708297751844e-05
Batch: 0; loss: 1.51; acc: 0.61
Batch: 20; loss: 1.57; acc: 0.58
Batch: 40; loss: 1.25; acc: 0.7
Batch: 60; loss: 1.44; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.59
Batch: 100; loss: 1.45; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.35; acc: 0.64
Val Epoch over. val_loss: 1.473216242091671; val_accuracy: 0.5895700636942676 

The current subspace-distance is: 1.670708297751844e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.39; acc: 0.64
Batch: 20; loss: 1.58; acc: 0.56
Batch: 40; loss: 1.43; acc: 0.61
Batch: 60; loss: 1.5; acc: 0.5
Batch: 80; loss: 1.3; acc: 0.75
Batch: 100; loss: 1.64; acc: 0.52
Batch: 120; loss: 1.51; acc: 0.52
Batch: 140; loss: 1.59; acc: 0.52
Batch: 160; loss: 1.41; acc: 0.66
Batch: 180; loss: 1.52; acc: 0.52
Batch: 200; loss: 1.3; acc: 0.62
Batch: 220; loss: 1.61; acc: 0.55
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.34; acc: 0.69
Batch: 280; loss: 1.49; acc: 0.58
Batch: 300; loss: 1.54; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.56
Batch: 340; loss: 1.39; acc: 0.62
Batch: 360; loss: 1.7; acc: 0.5
Batch: 380; loss: 1.45; acc: 0.59
Batch: 400; loss: 1.44; acc: 0.66
Batch: 420; loss: 1.49; acc: 0.58
Batch: 440; loss: 1.62; acc: 0.48
Batch: 460; loss: 1.41; acc: 0.66
Batch: 480; loss: 1.49; acc: 0.62
Batch: 500; loss: 1.47; acc: 0.56
Batch: 520; loss: 1.59; acc: 0.52
Batch: 540; loss: 1.41; acc: 0.58
Batch: 560; loss: 1.45; acc: 0.58
Batch: 580; loss: 1.49; acc: 0.53
Batch: 600; loss: 1.52; acc: 0.53
Batch: 620; loss: 1.46; acc: 0.66
Batch: 640; loss: 1.48; acc: 0.61
Batch: 660; loss: 1.39; acc: 0.64
Batch: 680; loss: 1.38; acc: 0.67
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.33; acc: 0.66
Batch: 740; loss: 1.53; acc: 0.55
Batch: 760; loss: 1.46; acc: 0.64
Batch: 780; loss: 1.4; acc: 0.61
Train Epoch over. train_loss: 1.51; train_accuracy: 0.56 

4.3774893129011616e-05
1.3366076927923132e-05
Batch: 0; loss: 1.51; acc: 0.62
Batch: 20; loss: 1.56; acc: 0.59
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.45; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.43; acc: 0.66
Batch: 120; loss: 1.51; acc: 0.56
Batch: 140; loss: 1.33; acc: 0.69
Val Epoch over. val_loss: 1.4737265026493438; val_accuracy: 0.5898686305732485 

The current subspace-distance is: 1.3366076927923132e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.38; acc: 0.64
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.37; acc: 0.69
Batch: 60; loss: 1.56; acc: 0.48
Batch: 80; loss: 1.6; acc: 0.53
Batch: 100; loss: 1.53; acc: 0.5
Batch: 120; loss: 1.47; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.53
Batch: 160; loss: 1.6; acc: 0.53
Batch: 180; loss: 1.35; acc: 0.66
Batch: 200; loss: 1.51; acc: 0.59
Batch: 220; loss: 1.66; acc: 0.5
Batch: 240; loss: 1.62; acc: 0.42
Batch: 260; loss: 1.52; acc: 0.56
Batch: 280; loss: 1.54; acc: 0.62
Batch: 300; loss: 1.49; acc: 0.59
Batch: 320; loss: 1.57; acc: 0.47
Batch: 340; loss: 1.51; acc: 0.53
Batch: 360; loss: 1.3; acc: 0.72
Batch: 380; loss: 1.3; acc: 0.69
Batch: 400; loss: 1.67; acc: 0.5
Batch: 420; loss: 1.44; acc: 0.62
Batch: 440; loss: 1.59; acc: 0.52
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.81; acc: 0.36
Batch: 500; loss: 1.5; acc: 0.52
Batch: 520; loss: 1.54; acc: 0.5
Batch: 540; loss: 1.55; acc: 0.53
Batch: 560; loss: 1.44; acc: 0.61
Batch: 580; loss: 1.48; acc: 0.56
Batch: 600; loss: 1.56; acc: 0.47
Batch: 620; loss: 1.64; acc: 0.48
Batch: 640; loss: 1.45; acc: 0.56
Batch: 660; loss: 1.42; acc: 0.64
Batch: 680; loss: 1.51; acc: 0.5
Batch: 700; loss: 1.37; acc: 0.67
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.31; acc: 0.69
Batch: 760; loss: 1.48; acc: 0.61
Batch: 780; loss: 1.49; acc: 0.61
Train Epoch over. train_loss: 1.5; train_accuracy: 0.56 

4.56130837847013e-05
1.7889762602862902e-05
Batch: 0; loss: 1.49; acc: 0.62
Batch: 20; loss: 1.56; acc: 0.59
Batch: 40; loss: 1.24; acc: 0.69
Batch: 60; loss: 1.43; acc: 0.59
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.41; acc: 0.66
Batch: 120; loss: 1.51; acc: 0.56
Batch: 140; loss: 1.33; acc: 0.67
Val Epoch over. val_loss: 1.4595210939455943; val_accuracy: 0.5897691082802548 

The current subspace-distance is: 1.7889762602862902e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.5
Batch: 20; loss: 1.52; acc: 0.48
Batch: 40; loss: 1.54; acc: 0.52
Batch: 60; loss: 1.45; acc: 0.58
Batch: 80; loss: 1.51; acc: 0.62
Batch: 100; loss: 1.4; acc: 0.66
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 1.61; acc: 0.52
Batch: 160; loss: 1.36; acc: 0.62
Batch: 180; loss: 1.6; acc: 0.53
Batch: 200; loss: 1.66; acc: 0.5
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.56; acc: 0.47
Batch: 260; loss: 1.49; acc: 0.58
Batch: 280; loss: 1.51; acc: 0.55
Batch: 300; loss: 1.49; acc: 0.55
Batch: 320; loss: 1.51; acc: 0.62
Batch: 340; loss: 1.35; acc: 0.64
Batch: 360; loss: 1.39; acc: 0.64
Batch: 380; loss: 1.66; acc: 0.48
Batch: 400; loss: 1.59; acc: 0.55
Batch: 420; loss: 1.57; acc: 0.58
Batch: 440; loss: 1.48; acc: 0.56
Batch: 460; loss: 1.61; acc: 0.44
Batch: 480; loss: 1.48; acc: 0.56
Batch: 500; loss: 1.43; acc: 0.58
Batch: 520; loss: 1.52; acc: 0.53
Batch: 540; loss: 1.52; acc: 0.61
Batch: 560; loss: 1.66; acc: 0.44
Batch: 580; loss: 1.53; acc: 0.58
Batch: 600; loss: 1.48; acc: 0.5
Batch: 620; loss: 1.42; acc: 0.56
Batch: 640; loss: 1.44; acc: 0.62
Batch: 660; loss: 1.66; acc: 0.45
Batch: 680; loss: 1.41; acc: 0.58
Batch: 700; loss: 1.55; acc: 0.52
Batch: 720; loss: 1.49; acc: 0.55
Batch: 740; loss: 1.4; acc: 0.64
Batch: 760; loss: 1.52; acc: 0.55
Batch: 780; loss: 1.52; acc: 0.53
Train Epoch over. train_loss: 1.5; train_accuracy: 0.56 

4.468217957764864e-05
1.3534386198443826e-05
Batch: 0; loss: 1.49; acc: 0.62
Batch: 20; loss: 1.56; acc: 0.58
Batch: 40; loss: 1.23; acc: 0.7
Batch: 60; loss: 1.44; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.61
Batch: 100; loss: 1.39; acc: 0.67
Batch: 120; loss: 1.51; acc: 0.56
Batch: 140; loss: 1.32; acc: 0.62
Val Epoch over. val_loss: 1.4592422748067577; val_accuracy: 0.5902667197452229 

The current subspace-distance is: 1.3534386198443826e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.52; acc: 0.48
Batch: 20; loss: 1.54; acc: 0.62
Batch: 40; loss: 1.42; acc: 0.61
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.72; acc: 0.45
Batch: 120; loss: 1.38; acc: 0.64
Batch: 140; loss: 1.45; acc: 0.64
Batch: 160; loss: 1.62; acc: 0.47
Batch: 180; loss: 1.59; acc: 0.55
Batch: 200; loss: 1.42; acc: 0.64
Batch: 220; loss: 1.55; acc: 0.53
Batch: 240; loss: 1.4; acc: 0.59
Batch: 260; loss: 1.54; acc: 0.52
Batch: 280; loss: 1.4; acc: 0.69
Batch: 300; loss: 1.48; acc: 0.55
Batch: 320; loss: 1.65; acc: 0.48
Batch: 340; loss: 1.43; acc: 0.59
Batch: 360; loss: 1.43; acc: 0.61
Batch: 380; loss: 1.53; acc: 0.48
Batch: 400; loss: 1.67; acc: 0.47
Batch: 420; loss: 1.29; acc: 0.7
Batch: 440; loss: 1.61; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.37; acc: 0.66
Batch: 500; loss: 1.42; acc: 0.62
Batch: 520; loss: 1.53; acc: 0.66
Batch: 540; loss: 1.58; acc: 0.48
Batch: 560; loss: 1.46; acc: 0.61
Batch: 580; loss: 1.49; acc: 0.55
Batch: 600; loss: 1.42; acc: 0.62
Batch: 620; loss: 1.51; acc: 0.58
Batch: 640; loss: 1.65; acc: 0.53
Batch: 660; loss: 1.69; acc: 0.44
Batch: 680; loss: 1.48; acc: 0.64
Batch: 700; loss: 1.59; acc: 0.45
Batch: 720; loss: 1.57; acc: 0.56
Batch: 740; loss: 1.43; acc: 0.58
Batch: 760; loss: 1.4; acc: 0.64
Batch: 780; loss: 1.41; acc: 0.64
Train Epoch over. train_loss: 1.5; train_accuracy: 0.56 

4.559826265904121e-05
1.721476473903749e-05
Batch: 0; loss: 1.49; acc: 0.62
Batch: 20; loss: 1.57; acc: 0.58
Batch: 40; loss: 1.22; acc: 0.73
Batch: 60; loss: 1.45; acc: 0.58
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.4; acc: 0.69
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 1.33; acc: 0.66
Val Epoch over. val_loss: 1.4626395216413364; val_accuracy: 0.5944466560509554 

The current subspace-distance is: 1.721476473903749e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.53
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 1.58; acc: 0.53
Batch: 60; loss: 1.44; acc: 0.61
Batch: 80; loss: 1.41; acc: 0.59
Batch: 100; loss: 1.4; acc: 0.55
Batch: 120; loss: 1.35; acc: 0.58
Batch: 140; loss: 1.69; acc: 0.55
Batch: 160; loss: 1.38; acc: 0.61
Batch: 180; loss: 1.61; acc: 0.48
Batch: 200; loss: 1.55; acc: 0.59
Batch: 220; loss: 1.31; acc: 0.69
Batch: 240; loss: 1.35; acc: 0.66
Batch: 260; loss: 1.43; acc: 0.61
Batch: 280; loss: 1.35; acc: 0.61
Batch: 300; loss: 1.68; acc: 0.47
Batch: 320; loss: 1.58; acc: 0.58
Batch: 340; loss: 1.44; acc: 0.62
Batch: 360; loss: 1.44; acc: 0.55
Batch: 380; loss: 1.5; acc: 0.59
Batch: 400; loss: 1.58; acc: 0.48
Batch: 420; loss: 1.44; acc: 0.61
Batch: 440; loss: 1.51; acc: 0.58
Batch: 460; loss: 1.76; acc: 0.44
Batch: 480; loss: 1.55; acc: 0.47
Batch: 500; loss: 1.42; acc: 0.66
Batch: 520; loss: 1.47; acc: 0.59
Batch: 540; loss: 1.47; acc: 0.61
Batch: 560; loss: 1.43; acc: 0.59
Batch: 580; loss: 1.52; acc: 0.53
Batch: 600; loss: 1.59; acc: 0.53
Batch: 620; loss: 1.33; acc: 0.61
Batch: 640; loss: 1.51; acc: 0.62
Batch: 660; loss: 1.45; acc: 0.53
Batch: 680; loss: 1.33; acc: 0.64
Batch: 700; loss: 1.42; acc: 0.62
Batch: 720; loss: 1.41; acc: 0.56
Batch: 740; loss: 1.59; acc: 0.55
Batch: 760; loss: 1.53; acc: 0.58
Batch: 780; loss: 1.31; acc: 0.67
Train Epoch over. train_loss: 1.49; train_accuracy: 0.56 

4.606211223290302e-05
1.7444910554331727e-05
Batch: 0; loss: 1.49; acc: 0.66
Batch: 20; loss: 1.56; acc: 0.55
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.69
Batch: 100; loss: 1.38; acc: 0.7
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 1.32; acc: 0.66
Val Epoch over. val_loss: 1.4529045129277904; val_accuracy: 0.5992237261146497 

The current subspace-distance is: 1.7444910554331727e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.56
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.37; acc: 0.66
Batch: 60; loss: 1.64; acc: 0.47
Batch: 80; loss: 1.56; acc: 0.44
Batch: 100; loss: 1.64; acc: 0.55
Batch: 120; loss: 1.44; acc: 0.59
Batch: 140; loss: 1.46; acc: 0.55
Batch: 160; loss: 1.4; acc: 0.62
Batch: 180; loss: 1.51; acc: 0.52
Batch: 200; loss: 1.61; acc: 0.45
Batch: 220; loss: 1.27; acc: 0.64
Batch: 240; loss: 1.59; acc: 0.55
Batch: 260; loss: 1.5; acc: 0.58
Batch: 280; loss: 1.43; acc: 0.53
Batch: 300; loss: 1.46; acc: 0.58
Batch: 320; loss: 1.37; acc: 0.64
Batch: 340; loss: 1.47; acc: 0.59
Batch: 360; loss: 1.72; acc: 0.36
Batch: 380; loss: 1.39; acc: 0.61
Batch: 400; loss: 1.42; acc: 0.59
Batch: 420; loss: 1.53; acc: 0.56
Batch: 440; loss: 1.35; acc: 0.62
Batch: 460; loss: 1.38; acc: 0.62
Batch: 480; loss: 1.68; acc: 0.52
Batch: 500; loss: 1.33; acc: 0.62
Batch: 520; loss: 1.58; acc: 0.5
Batch: 540; loss: 1.45; acc: 0.56
Batch: 560; loss: 1.48; acc: 0.61
Batch: 580; loss: 1.42; acc: 0.61
Batch: 600; loss: 1.4; acc: 0.55
Batch: 620; loss: 1.46; acc: 0.53
Batch: 640; loss: 1.44; acc: 0.53
Batch: 660; loss: 1.38; acc: 0.61
Batch: 680; loss: 1.36; acc: 0.69
Batch: 700; loss: 1.32; acc: 0.7
Batch: 720; loss: 1.6; acc: 0.55
Batch: 740; loss: 1.43; acc: 0.61
Batch: 760; loss: 1.5; acc: 0.55
Batch: 780; loss: 1.5; acc: 0.52
Train Epoch over. train_loss: 1.49; train_accuracy: 0.56 

4.5910655899206176e-05
1.5272273230948485e-05
Batch: 0; loss: 1.49; acc: 0.62
Batch: 20; loss: 1.57; acc: 0.53
Batch: 40; loss: 1.2; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.69
Batch: 120; loss: 1.51; acc: 0.53
Batch: 140; loss: 1.33; acc: 0.64
Val Epoch over. val_loss: 1.4511503474727558; val_accuracy: 0.5923566878980892 

The current subspace-distance is: 1.5272273230948485e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.58
Batch: 20; loss: 1.67; acc: 0.47
Batch: 40; loss: 1.38; acc: 0.64
Batch: 60; loss: 1.54; acc: 0.42
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.51; acc: 0.52
Batch: 120; loss: 1.41; acc: 0.56
Batch: 140; loss: 1.5; acc: 0.56
Batch: 160; loss: 1.52; acc: 0.55
Batch: 180; loss: 1.6; acc: 0.42
Batch: 200; loss: 1.44; acc: 0.55
Batch: 220; loss: 1.38; acc: 0.66
Batch: 240; loss: 1.64; acc: 0.53
Batch: 260; loss: 1.54; acc: 0.61
Batch: 280; loss: 1.52; acc: 0.58
Batch: 300; loss: 1.5; acc: 0.55
Batch: 320; loss: 1.27; acc: 0.64
Batch: 340; loss: 1.41; acc: 0.61
Batch: 360; loss: 1.23; acc: 0.73
Batch: 380; loss: 1.43; acc: 0.59
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.37; acc: 0.62
Batch: 440; loss: 1.36; acc: 0.62
Batch: 460; loss: 1.59; acc: 0.48
Batch: 480; loss: 1.37; acc: 0.64
Batch: 500; loss: 1.54; acc: 0.52
Batch: 520; loss: 1.42; acc: 0.64
Batch: 540; loss: 1.31; acc: 0.64
Batch: 560; loss: 1.51; acc: 0.62
Batch: 580; loss: 1.56; acc: 0.53
Batch: 600; loss: 1.59; acc: 0.45
Batch: 620; loss: 1.52; acc: 0.56
Batch: 640; loss: 1.47; acc: 0.55
Batch: 660; loss: 1.45; acc: 0.61
Batch: 680; loss: 1.39; acc: 0.62
Batch: 700; loss: 1.48; acc: 0.58
Batch: 720; loss: 1.64; acc: 0.47
Batch: 740; loss: 1.34; acc: 0.7
Batch: 760; loss: 1.3; acc: 0.7
Batch: 780; loss: 1.46; acc: 0.55
Train Epoch over. train_loss: 1.49; train_accuracy: 0.56 

4.666284439736046e-05
1.7484835552750155e-05
Batch: 0; loss: 1.48; acc: 0.61
Batch: 20; loss: 1.58; acc: 0.52
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.43; acc: 0.58
Batch: 80; loss: 1.37; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.7
Batch: 120; loss: 1.51; acc: 0.52
Batch: 140; loss: 1.32; acc: 0.67
Val Epoch over. val_loss: 1.4473920445533315; val_accuracy: 0.5990246815286624 

The current subspace-distance is: 1.7484835552750155e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.55; acc: 0.56
Batch: 20; loss: 1.44; acc: 0.58
Batch: 40; loss: 1.59; acc: 0.52
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.69; acc: 0.45
Batch: 100; loss: 1.63; acc: 0.5
Batch: 120; loss: 1.57; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.56
Batch: 160; loss: 1.37; acc: 0.64
Batch: 180; loss: 1.53; acc: 0.56
Batch: 200; loss: 1.57; acc: 0.55
Batch: 220; loss: 1.35; acc: 0.64
Batch: 240; loss: 1.43; acc: 0.56
Batch: 260; loss: 1.44; acc: 0.64
Batch: 280; loss: 1.42; acc: 0.62
Batch: 300; loss: 1.46; acc: 0.55
Batch: 320; loss: 1.37; acc: 0.66
Batch: 340; loss: 1.48; acc: 0.59
Batch: 360; loss: 1.61; acc: 0.53
Batch: 380; loss: 1.49; acc: 0.58
Batch: 400; loss: 1.41; acc: 0.56
Batch: 420; loss: 1.48; acc: 0.59
Batch: 440; loss: 1.46; acc: 0.58
Batch: 460; loss: 1.5; acc: 0.56
Batch: 480; loss: 1.43; acc: 0.58
Batch: 500; loss: 1.64; acc: 0.45
Batch: 520; loss: 1.57; acc: 0.53
Batch: 540; loss: 1.46; acc: 0.59
Batch: 560; loss: 1.6; acc: 0.48
Batch: 580; loss: 1.46; acc: 0.62
Batch: 600; loss: 1.36; acc: 0.59
Batch: 620; loss: 1.48; acc: 0.59
Batch: 640; loss: 1.58; acc: 0.5
Batch: 660; loss: 1.53; acc: 0.53
Batch: 680; loss: 1.43; acc: 0.61
Batch: 700; loss: 1.38; acc: 0.58
Batch: 720; loss: 1.46; acc: 0.59
Batch: 740; loss: 1.42; acc: 0.58
Batch: 760; loss: 1.46; acc: 0.56
Batch: 780; loss: 1.41; acc: 0.59
Train Epoch over. train_loss: 1.49; train_accuracy: 0.56 

4.901300053461455e-05
2.2494377844850533e-05
Batch: 0; loss: 1.48; acc: 0.61
Batch: 20; loss: 1.57; acc: 0.5
Batch: 40; loss: 1.19; acc: 0.73
Batch: 60; loss: 1.43; acc: 0.56
Batch: 80; loss: 1.36; acc: 0.67
Batch: 100; loss: 1.36; acc: 0.72
Batch: 120; loss: 1.5; acc: 0.53
Batch: 140; loss: 1.31; acc: 0.67
Val Epoch over. val_loss: 1.4401130129577249; val_accuracy: 0.6016122611464968 

The current subspace-distance is: 2.2494377844850533e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.52
Batch: 20; loss: 1.56; acc: 0.56
Batch: 40; loss: 1.56; acc: 0.58
Batch: 60; loss: 1.41; acc: 0.66
Batch: 80; loss: 1.61; acc: 0.55
Batch: 100; loss: 1.56; acc: 0.53
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 1.5; acc: 0.56
Batch: 160; loss: 1.53; acc: 0.5
Batch: 180; loss: 1.58; acc: 0.55
Batch: 200; loss: 1.73; acc: 0.39
Batch: 220; loss: 1.39; acc: 0.62
Batch: 240; loss: 1.41; acc: 0.58
Batch: 260; loss: 1.41; acc: 0.59
Batch: 280; loss: 1.47; acc: 0.56
Batch: 300; loss: 1.43; acc: 0.62
Batch: 320; loss: 1.47; acc: 0.59
Batch: 340; loss: 1.43; acc: 0.67
Batch: 360; loss: 1.67; acc: 0.48
Batch: 380; loss: 1.55; acc: 0.5
Batch: 400; loss: 1.65; acc: 0.48
Batch: 420; loss: 1.62; acc: 0.47
Batch: 440; loss: 1.53; acc: 0.62
Batch: 460; loss: 1.48; acc: 0.56
Batch: 480; loss: 1.43; acc: 0.59
Batch: 500; loss: 1.41; acc: 0.69
Batch: 520; loss: 1.34; acc: 0.61
Batch: 540; loss: 1.37; acc: 0.61
Batch: 560; loss: 1.45; acc: 0.56
Batch: 580; loss: 1.42; acc: 0.61
Batch: 600; loss: 1.41; acc: 0.59
Batch: 620; loss: 1.41; acc: 0.52
Batch: 640; loss: 1.56; acc: 0.48
Batch: 660; loss: 1.57; acc: 0.52
Batch: 680; loss: 1.56; acc: 0.5
Batch: 700; loss: 1.55; acc: 0.55
Batch: 720; loss: 1.44; acc: 0.62
Batch: 740; loss: 1.26; acc: 0.72
Batch: 760; loss: 1.49; acc: 0.56
Batch: 780; loss: 1.45; acc: 0.55
Train Epoch over. train_loss: 1.49; train_accuracy: 0.56 

4.7925896069500595e-05
1.831553890951909e-05
Batch: 0; loss: 1.47; acc: 0.62
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.19; acc: 0.73
Batch: 60; loss: 1.43; acc: 0.55
Batch: 80; loss: 1.37; acc: 0.67
Batch: 100; loss: 1.36; acc: 0.72
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.31; acc: 0.67
Val Epoch over. val_loss: 1.4426635443025333; val_accuracy: 0.5958399681528662 

The current subspace-distance is: 1.831553890951909e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.52
Batch: 20; loss: 1.65; acc: 0.45
Batch: 40; loss: 1.47; acc: 0.58
Batch: 60; loss: 1.48; acc: 0.56
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.64
Batch: 120; loss: 1.43; acc: 0.66
Batch: 140; loss: 1.51; acc: 0.44
Batch: 160; loss: 1.38; acc: 0.61
Batch: 180; loss: 1.52; acc: 0.55
Batch: 200; loss: 1.35; acc: 0.69
Batch: 220; loss: 1.39; acc: 0.66
Batch: 240; loss: 1.56; acc: 0.5
Batch: 260; loss: 1.35; acc: 0.62
Batch: 280; loss: 1.42; acc: 0.55
Batch: 300; loss: 1.58; acc: 0.5
Batch: 320; loss: 1.51; acc: 0.48
Batch: 340; loss: 1.52; acc: 0.55
Batch: 360; loss: 1.38; acc: 0.64
Batch: 380; loss: 1.5; acc: 0.55
Batch: 400; loss: 1.68; acc: 0.52
Batch: 420; loss: 1.55; acc: 0.56
Batch: 440; loss: 1.42; acc: 0.67
Batch: 460; loss: 1.55; acc: 0.59
Batch: 480; loss: 1.5; acc: 0.52
Batch: 500; loss: 1.64; acc: 0.45
Batch: 520; loss: 1.42; acc: 0.55
Batch: 540; loss: 1.38; acc: 0.59
Batch: 560; loss: 1.23; acc: 0.67
Batch: 580; loss: 1.65; acc: 0.53
Batch: 600; loss: 1.57; acc: 0.52
Batch: 620; loss: 1.42; acc: 0.59
Batch: 640; loss: 1.43; acc: 0.58
Batch: 660; loss: 1.48; acc: 0.56
Batch: 680; loss: 1.47; acc: 0.48
Batch: 700; loss: 1.5; acc: 0.66
Batch: 720; loss: 1.51; acc: 0.58
Batch: 740; loss: 1.38; acc: 0.64
Batch: 760; loss: 1.3; acc: 0.62
Batch: 780; loss: 1.56; acc: 0.52
Train Epoch over. train_loss: 1.48; train_accuracy: 0.57 

4.787201396538876e-05
1.8078175344271585e-05
Batch: 0; loss: 1.47; acc: 0.64
Batch: 20; loss: 1.57; acc: 0.48
Batch: 40; loss: 1.19; acc: 0.73
Batch: 60; loss: 1.44; acc: 0.56
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.37; acc: 0.72
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.32; acc: 0.67
Val Epoch over. val_loss: 1.4448763268768408; val_accuracy: 0.6000199044585988 

The current subspace-distance is: 1.8078175344271585e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.53
Batch: 20; loss: 1.48; acc: 0.59
Batch: 40; loss: 1.43; acc: 0.61
Batch: 60; loss: 1.51; acc: 0.55
Batch: 80; loss: 1.37; acc: 0.64
Batch: 100; loss: 1.52; acc: 0.52
Batch: 120; loss: 1.5; acc: 0.69
Batch: 140; loss: 1.5; acc: 0.55
Batch: 160; loss: 1.38; acc: 0.59
Batch: 180; loss: 1.39; acc: 0.56
Batch: 200; loss: 1.72; acc: 0.38
Batch: 220; loss: 1.4; acc: 0.61
Batch: 240; loss: 1.48; acc: 0.53
Batch: 260; loss: 1.48; acc: 0.59
Batch: 280; loss: 1.6; acc: 0.52
Batch: 300; loss: 1.38; acc: 0.59
Batch: 320; loss: 1.5; acc: 0.53
Batch: 340; loss: 1.52; acc: 0.56
Batch: 360; loss: 1.54; acc: 0.48
Batch: 380; loss: 1.5; acc: 0.56
Batch: 400; loss: 1.36; acc: 0.62
Batch: 420; loss: 1.37; acc: 0.62
Batch: 440; loss: 1.5; acc: 0.56
Batch: 460; loss: 1.4; acc: 0.61
Batch: 480; loss: 1.57; acc: 0.48
Batch: 500; loss: 1.6; acc: 0.48
Batch: 520; loss: 1.57; acc: 0.48
Batch: 540; loss: 1.31; acc: 0.64
Batch: 560; loss: 1.32; acc: 0.72
Batch: 580; loss: 1.46; acc: 0.56
Batch: 600; loss: 1.45; acc: 0.59
Batch: 620; loss: 1.54; acc: 0.48
Batch: 640; loss: 1.36; acc: 0.61
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.46; acc: 0.56
Batch: 700; loss: 1.41; acc: 0.58
Batch: 720; loss: 1.47; acc: 0.55
Batch: 740; loss: 1.53; acc: 0.52
Batch: 760; loss: 1.62; acc: 0.45
Batch: 780; loss: 1.53; acc: 0.58
Train Epoch over. train_loss: 1.48; train_accuracy: 0.56 

4.664787047659047e-05
1.4399338397197425e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.56; acc: 0.48
Batch: 40; loss: 1.19; acc: 0.73
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.69
Batch: 100; loss: 1.36; acc: 0.72
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.31; acc: 0.67
Val Epoch over. val_loss: 1.4474011834260005; val_accuracy: 0.5965366242038217 

The current subspace-distance is: 1.4399338397197425e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.56; acc: 0.58
Batch: 40; loss: 1.58; acc: 0.47
Batch: 60; loss: 1.33; acc: 0.64
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.42; acc: 0.62
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.41; acc: 0.61
Batch: 160; loss: 1.52; acc: 0.59
Batch: 180; loss: 1.58; acc: 0.47
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 1.41; acc: 0.58
Batch: 240; loss: 1.53; acc: 0.45
Batch: 260; loss: 1.49; acc: 0.56
Batch: 280; loss: 1.49; acc: 0.52
Batch: 300; loss: 1.51; acc: 0.52
Batch: 320; loss: 1.47; acc: 0.61
Batch: 340; loss: 1.43; acc: 0.66
Batch: 360; loss: 1.64; acc: 0.45
Batch: 380; loss: 1.41; acc: 0.55
Batch: 400; loss: 1.46; acc: 0.58
Batch: 420; loss: 1.35; acc: 0.64
Batch: 440; loss: 1.62; acc: 0.56
Batch: 460; loss: 1.3; acc: 0.66
Batch: 480; loss: 1.64; acc: 0.48
Batch: 500; loss: 1.42; acc: 0.61
Batch: 520; loss: 1.4; acc: 0.62
Batch: 540; loss: 1.5; acc: 0.61
Batch: 560; loss: 1.52; acc: 0.58
Batch: 580; loss: 1.6; acc: 0.52
Batch: 600; loss: 1.69; acc: 0.42
Batch: 620; loss: 1.59; acc: 0.53
Batch: 640; loss: 1.38; acc: 0.62
Batch: 660; loss: 1.53; acc: 0.56
Batch: 680; loss: 1.4; acc: 0.56
Batch: 700; loss: 1.51; acc: 0.53
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.36; acc: 0.64
Batch: 760; loss: 1.55; acc: 0.52
Batch: 780; loss: 1.43; acc: 0.53
Train Epoch over. train_loss: 1.48; train_accuracy: 0.57 

4.640410770662129e-05
1.3989483704790473e-05
Batch: 0; loss: 1.47; acc: 0.58
Batch: 20; loss: 1.56; acc: 0.47
Batch: 40; loss: 1.19; acc: 0.72
Batch: 60; loss: 1.43; acc: 0.55
Batch: 80; loss: 1.38; acc: 0.66
Batch: 100; loss: 1.34; acc: 0.72
Batch: 120; loss: 1.48; acc: 0.55
Batch: 140; loss: 1.31; acc: 0.67
Val Epoch over. val_loss: 1.442004605463356; val_accuracy: 0.5923566878980892 

The current subspace-distance is: 1.3989483704790473e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.45; acc: 0.69
Batch: 40; loss: 1.31; acc: 0.73
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.49; acc: 0.52
Batch: 100; loss: 1.51; acc: 0.53
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.32; acc: 0.66
Batch: 160; loss: 1.49; acc: 0.55
Batch: 180; loss: 1.59; acc: 0.48
Batch: 200; loss: 1.35; acc: 0.62
Batch: 220; loss: 1.36; acc: 0.62
Batch: 240; loss: 1.39; acc: 0.56
Batch: 260; loss: 1.56; acc: 0.59
Batch: 280; loss: 1.59; acc: 0.55
Batch: 300; loss: 1.56; acc: 0.5
Batch: 320; loss: 1.5; acc: 0.58
Batch: 340; loss: 1.44; acc: 0.58
Batch: 360; loss: 1.52; acc: 0.53
Batch: 380; loss: 1.41; acc: 0.64
Batch: 400; loss: 1.49; acc: 0.58
Batch: 420; loss: 1.56; acc: 0.5
Batch: 440; loss: 1.54; acc: 0.56
Batch: 460; loss: 1.38; acc: 0.64
Batch: 480; loss: 1.5; acc: 0.58
Batch: 500; loss: 1.47; acc: 0.58
Batch: 520; loss: 1.36; acc: 0.62
Batch: 540; loss: 1.32; acc: 0.66
Batch: 560; loss: 1.51; acc: 0.56
Batch: 580; loss: 1.48; acc: 0.56
Batch: 600; loss: 1.42; acc: 0.55
Batch: 620; loss: 1.54; acc: 0.52
Batch: 640; loss: 1.28; acc: 0.73
Batch: 660; loss: 1.54; acc: 0.5
Batch: 680; loss: 1.51; acc: 0.52
Batch: 700; loss: 1.41; acc: 0.58
Batch: 720; loss: 1.44; acc: 0.53
Batch: 740; loss: 1.41; acc: 0.61
Batch: 760; loss: 1.58; acc: 0.41
Batch: 780; loss: 1.45; acc: 0.59
Train Epoch over. train_loss: 1.48; train_accuracy: 0.57 

4.857393287238665e-05
1.8568382074590772e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.57; acc: 0.47
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.67
Batch: 100; loss: 1.35; acc: 0.75
Batch: 120; loss: 1.49; acc: 0.55
Batch: 140; loss: 1.31; acc: 0.67
Val Epoch over. val_loss: 1.4446634259193567; val_accuracy: 0.595640923566879 

The current subspace-distance is: 1.8568382074590772e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.4; acc: 0.61
Batch: 20; loss: 1.36; acc: 0.64
Batch: 40; loss: 1.7; acc: 0.39
Batch: 60; loss: 1.3; acc: 0.67
Batch: 80; loss: 1.46; acc: 0.56
Batch: 100; loss: 1.47; acc: 0.52
Batch: 120; loss: 1.52; acc: 0.47
Batch: 140; loss: 1.5; acc: 0.59
Batch: 160; loss: 1.57; acc: 0.5
Batch: 180; loss: 1.5; acc: 0.48
Batch: 200; loss: 1.47; acc: 0.52
Batch: 220; loss: 1.49; acc: 0.56
Batch: 240; loss: 1.45; acc: 0.59
Batch: 260; loss: 1.45; acc: 0.58
Batch: 280; loss: 1.51; acc: 0.48
Batch: 300; loss: 1.46; acc: 0.55
Batch: 320; loss: 1.36; acc: 0.66
Batch: 340; loss: 1.61; acc: 0.45
Batch: 360; loss: 1.5; acc: 0.5
Batch: 380; loss: 1.66; acc: 0.45
Batch: 400; loss: 1.49; acc: 0.62
Batch: 420; loss: 1.45; acc: 0.58
Batch: 440; loss: 1.53; acc: 0.58
Batch: 460; loss: 1.62; acc: 0.5
Batch: 480; loss: 1.42; acc: 0.61
Batch: 500; loss: 1.35; acc: 0.8
Batch: 520; loss: 1.4; acc: 0.64
Batch: 540; loss: 1.59; acc: 0.5
Batch: 560; loss: 1.52; acc: 0.61
Batch: 580; loss: 1.58; acc: 0.59
Batch: 600; loss: 1.36; acc: 0.59
Batch: 620; loss: 1.6; acc: 0.47
Batch: 640; loss: 1.47; acc: 0.56
Batch: 660; loss: 1.7; acc: 0.45
Batch: 680; loss: 1.51; acc: 0.58
Batch: 700; loss: 1.38; acc: 0.62
Batch: 720; loss: 1.32; acc: 0.73
Batch: 740; loss: 1.57; acc: 0.48
Batch: 760; loss: 1.35; acc: 0.69
Batch: 780; loss: 1.56; acc: 0.56
Train Epoch over. train_loss: 1.48; train_accuracy: 0.57 

4.7655663365731016e-05
1.750432602420915e-05
Batch: 0; loss: 1.48; acc: 0.56
Batch: 20; loss: 1.57; acc: 0.48
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.39; acc: 0.64
Batch: 100; loss: 1.36; acc: 0.7
Batch: 120; loss: 1.51; acc: 0.52
Batch: 140; loss: 1.35; acc: 0.64
Val Epoch over. val_loss: 1.4514535020111472; val_accuracy: 0.5866839171974523 

The current subspace-distance is: 1.750432602420915e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.55
Batch: 20; loss: 1.45; acc: 0.58
Batch: 40; loss: 1.49; acc: 0.59
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.27; acc: 0.69
Batch: 100; loss: 1.42; acc: 0.61
Batch: 120; loss: 1.41; acc: 0.58
Batch: 140; loss: 1.61; acc: 0.5
Batch: 160; loss: 1.4; acc: 0.53
Batch: 180; loss: 1.31; acc: 0.67
Batch: 200; loss: 1.59; acc: 0.52
Batch: 220; loss: 1.57; acc: 0.47
Batch: 240; loss: 1.62; acc: 0.48
Batch: 260; loss: 1.61; acc: 0.53
Batch: 280; loss: 1.45; acc: 0.56
Batch: 300; loss: 1.44; acc: 0.55
Batch: 320; loss: 1.38; acc: 0.69
Batch: 340; loss: 1.34; acc: 0.67
Batch: 360; loss: 1.57; acc: 0.5
Batch: 380; loss: 1.44; acc: 0.62
Batch: 400; loss: 1.43; acc: 0.52
Batch: 420; loss: 1.43; acc: 0.59
Batch: 440; loss: 1.43; acc: 0.58
Batch: 460; loss: 1.51; acc: 0.52
Batch: 480; loss: 1.38; acc: 0.67
Batch: 500; loss: 1.5; acc: 0.58
Batch: 520; loss: 1.61; acc: 0.47
Batch: 540; loss: 1.47; acc: 0.55
Batch: 560; loss: 1.5; acc: 0.59
Batch: 580; loss: 1.55; acc: 0.56
Batch: 600; loss: 1.68; acc: 0.47
Batch: 620; loss: 1.42; acc: 0.64
Batch: 640; loss: 1.55; acc: 0.55
Batch: 660; loss: 1.46; acc: 0.61
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.58; acc: 0.56
Batch: 720; loss: 1.51; acc: 0.52
Batch: 740; loss: 1.51; acc: 0.52
Batch: 760; loss: 1.45; acc: 0.56
Batch: 780; loss: 1.55; acc: 0.53
Train Epoch over. train_loss: 1.48; train_accuracy: 0.56 

4.6916833525756374e-05
1.57216600200627e-05
Batch: 0; loss: 1.47; acc: 0.61
Batch: 20; loss: 1.6; acc: 0.47
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.43; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.66
Batch: 100; loss: 1.36; acc: 0.73
Batch: 120; loss: 1.5; acc: 0.53
Batch: 140; loss: 1.33; acc: 0.67
Val Epoch over. val_loss: 1.4431689772636267; val_accuracy: 0.5970342356687898 

The current subspace-distance is: 1.57216600200627e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.27; acc: 0.66
Batch: 20; loss: 1.38; acc: 0.64
Batch: 40; loss: 1.48; acc: 0.56
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.36; acc: 0.62
Batch: 100; loss: 1.4; acc: 0.67
Batch: 120; loss: 1.53; acc: 0.58
Batch: 140; loss: 1.46; acc: 0.52
Batch: 160; loss: 1.53; acc: 0.53
Batch: 180; loss: 1.34; acc: 0.59
Batch: 200; loss: 1.54; acc: 0.55
Batch: 220; loss: 1.56; acc: 0.44
Batch: 240; loss: 1.46; acc: 0.53
Batch: 260; loss: 1.62; acc: 0.5
Batch: 280; loss: 1.59; acc: 0.5
Batch: 300; loss: 1.37; acc: 0.62
Batch: 320; loss: 1.43; acc: 0.58
Batch: 340; loss: 1.4; acc: 0.66
Batch: 360; loss: 1.54; acc: 0.55
Batch: 380; loss: 1.45; acc: 0.62
Batch: 400; loss: 1.68; acc: 0.5
Batch: 420; loss: 1.38; acc: 0.56
Batch: 440; loss: 1.45; acc: 0.58
Batch: 460; loss: 1.35; acc: 0.67
Batch: 480; loss: 1.43; acc: 0.59
Batch: 500; loss: 1.55; acc: 0.61
Batch: 520; loss: 1.54; acc: 0.52
Batch: 540; loss: 1.61; acc: 0.5
Batch: 560; loss: 1.58; acc: 0.53
Batch: 580; loss: 1.63; acc: 0.47
Batch: 600; loss: 1.54; acc: 0.52
Batch: 620; loss: 1.52; acc: 0.47
Batch: 640; loss: 1.51; acc: 0.5
Batch: 660; loss: 1.55; acc: 0.5
Batch: 680; loss: 1.49; acc: 0.62
Batch: 700; loss: 1.46; acc: 0.53
Batch: 720; loss: 1.38; acc: 0.66
Batch: 740; loss: 1.42; acc: 0.59
Batch: 760; loss: 1.4; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.64
Train Epoch over. train_loss: 1.48; train_accuracy: 0.56 

4.9016281991498545e-05
2.249984027002938e-05
Batch: 0; loss: 1.48; acc: 0.61
Batch: 20; loss: 1.58; acc: 0.47
Batch: 40; loss: 1.19; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.56
Batch: 80; loss: 1.37; acc: 0.69
Batch: 100; loss: 1.34; acc: 0.73
Batch: 120; loss: 1.51; acc: 0.53
Batch: 140; loss: 1.3; acc: 0.67
Val Epoch over. val_loss: 1.4424013871296195; val_accuracy: 0.6001194267515924 

The current subspace-distance is: 2.249984027002938e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.36; acc: 0.61
Batch: 20; loss: 1.35; acc: 0.59
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.62; acc: 0.45
Batch: 100; loss: 1.51; acc: 0.55
Batch: 120; loss: 1.55; acc: 0.5
Batch: 140; loss: 1.51; acc: 0.56
Batch: 160; loss: 1.29; acc: 0.67
Batch: 180; loss: 1.45; acc: 0.62
Batch: 200; loss: 1.46; acc: 0.64
Batch: 220; loss: 1.41; acc: 0.64
Batch: 240; loss: 1.55; acc: 0.48
Batch: 260; loss: 1.48; acc: 0.56
Batch: 280; loss: 1.43; acc: 0.67
Batch: 300; loss: 1.4; acc: 0.67
Batch: 320; loss: 1.61; acc: 0.41
Batch: 340; loss: 1.51; acc: 0.56
Batch: 360; loss: 1.53; acc: 0.56
Batch: 380; loss: 1.38; acc: 0.66
Batch: 400; loss: 1.7; acc: 0.53
Batch: 420; loss: 1.6; acc: 0.48
Batch: 440; loss: 1.6; acc: 0.52
Batch: 460; loss: 1.5; acc: 0.53
Batch: 480; loss: 1.52; acc: 0.58
Batch: 500; loss: 1.49; acc: 0.5
Batch: 520; loss: 1.44; acc: 0.55
Batch: 540; loss: 1.47; acc: 0.61
Batch: 560; loss: 1.35; acc: 0.66
Batch: 580; loss: 1.4; acc: 0.61
Batch: 600; loss: 1.46; acc: 0.59
Batch: 620; loss: 1.41; acc: 0.58
Batch: 640; loss: 1.51; acc: 0.52
Batch: 660; loss: 1.26; acc: 0.67
Batch: 680; loss: 1.55; acc: 0.47
Batch: 700; loss: 1.54; acc: 0.52
Batch: 720; loss: 1.45; acc: 0.61
Batch: 740; loss: 1.41; acc: 0.62
Batch: 760; loss: 1.56; acc: 0.53
Batch: 780; loss: 1.53; acc: 0.55
Train Epoch over. train_loss: 1.48; train_accuracy: 0.57 

4.8382436943938956e-05
1.9462462660158053e-05
Batch: 0; loss: 1.45; acc: 0.59
Batch: 20; loss: 1.57; acc: 0.45
Batch: 40; loss: 1.17; acc: 0.73
Batch: 60; loss: 1.43; acc: 0.58
Batch: 80; loss: 1.36; acc: 0.66
Batch: 100; loss: 1.33; acc: 0.75
Batch: 120; loss: 1.49; acc: 0.56
Batch: 140; loss: 1.3; acc: 0.69
Val Epoch over. val_loss: 1.4315608815782388; val_accuracy: 0.599422770700637 

The current subspace-distance is: 1.9462462660158053e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_7_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.6540064054180426

The number of parameters is: 270776

The number of individual parameters is:

22
396
22
22
32
45760
32
32
64
133120
64
64
64
86016
64
64
4096
64
640
10
64
64

nonzero elements in E: 27077597
elements in E: 27077600
fraction nonzero: 0.9999998892073153
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.49; acc: 0.11
Batch: 20; loss: 2.5; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.17
Batch: 60; loss: 2.28; acc: 0.19
Batch: 80; loss: 2.13; acc: 0.23
Batch: 100; loss: 2.14; acc: 0.19
Batch: 120; loss: 2.08; acc: 0.28
Batch: 140; loss: 1.99; acc: 0.3
Batch: 160; loss: 2.01; acc: 0.3
Batch: 180; loss: 1.99; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.36
Batch: 220; loss: 2.01; acc: 0.34
Batch: 240; loss: 2.07; acc: 0.28
Batch: 260; loss: 1.85; acc: 0.45
Batch: 280; loss: 1.9; acc: 0.41
Batch: 300; loss: 1.82; acc: 0.58
Batch: 320; loss: 1.92; acc: 0.41
Batch: 340; loss: 1.81; acc: 0.52
Batch: 360; loss: 1.84; acc: 0.45
Batch: 380; loss: 1.89; acc: 0.47
Batch: 400; loss: 1.76; acc: 0.62
Batch: 420; loss: 1.91; acc: 0.39
Batch: 440; loss: 1.78; acc: 0.52
Batch: 460; loss: 1.83; acc: 0.45
Batch: 480; loss: 1.81; acc: 0.52
Batch: 500; loss: 1.8; acc: 0.53
Batch: 520; loss: 1.71; acc: 0.59
Batch: 540; loss: 1.73; acc: 0.56
Batch: 560; loss: 1.83; acc: 0.45
Batch: 580; loss: 1.86; acc: 0.45
Batch: 600; loss: 1.81; acc: 0.48
Batch: 620; loss: 1.71; acc: 0.52
Batch: 640; loss: 1.81; acc: 0.47
Batch: 660; loss: 1.86; acc: 0.44
Batch: 680; loss: 1.78; acc: 0.5
Batch: 700; loss: 1.75; acc: 0.58
Batch: 720; loss: 1.72; acc: 0.53
Batch: 740; loss: 1.83; acc: 0.42
Batch: 760; loss: 1.85; acc: 0.45
Batch: 780; loss: 1.7; acc: 0.58
Train Epoch over. train_loss: 1.91; train_accuracy: 0.42 

5.5675351177342236e-05
5.1115879614371806e-05
Batch: 0; loss: 1.93; acc: 0.38
Batch: 20; loss: 1.91; acc: 0.45
Batch: 40; loss: 1.51; acc: 0.73
Batch: 60; loss: 1.68; acc: 0.56
Batch: 80; loss: 1.55; acc: 0.73
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.79; acc: 0.48
Batch: 140; loss: 1.67; acc: 0.66
Val Epoch over. val_loss: 1.7250010754652083; val_accuracy: 0.5528463375796179 

The current subspace-distance is: 5.1115879614371806e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.81; acc: 0.47
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.74; acc: 0.52
Batch: 60; loss: 1.72; acc: 0.52
Batch: 80; loss: 1.71; acc: 0.52
Batch: 100; loss: 1.78; acc: 0.52
Batch: 120; loss: 1.75; acc: 0.52
Batch: 140; loss: 1.82; acc: 0.47
Batch: 160; loss: 1.7; acc: 0.52
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.75; acc: 0.53
Batch: 220; loss: 1.74; acc: 0.44
Batch: 240; loss: 1.72; acc: 0.53
Batch: 260; loss: 1.72; acc: 0.53
Batch: 280; loss: 1.74; acc: 0.42
Batch: 300; loss: 1.67; acc: 0.56
Batch: 320; loss: 1.67; acc: 0.58
Batch: 340; loss: 1.68; acc: 0.58
Batch: 360; loss: 1.65; acc: 0.58
Batch: 380; loss: 1.75; acc: 0.55
Batch: 400; loss: 1.68; acc: 0.61
Batch: 420; loss: 1.74; acc: 0.5
Batch: 440; loss: 1.68; acc: 0.58
Batch: 460; loss: 1.7; acc: 0.5
Batch: 480; loss: 1.58; acc: 0.64
Batch: 500; loss: 1.67; acc: 0.48
Batch: 520; loss: 1.68; acc: 0.56
Batch: 540; loss: 1.76; acc: 0.44
Batch: 560; loss: 1.59; acc: 0.64
Batch: 580; loss: 1.64; acc: 0.53
Batch: 600; loss: 1.68; acc: 0.52
Batch: 620; loss: 1.69; acc: 0.5
Batch: 640; loss: 1.65; acc: 0.53
Batch: 660; loss: 1.7; acc: 0.42
Batch: 680; loss: 1.65; acc: 0.58
Batch: 700; loss: 1.6; acc: 0.64
Batch: 720; loss: 1.63; acc: 0.64
Batch: 740; loss: 1.61; acc: 0.62
Batch: 760; loss: 1.73; acc: 0.56
Batch: 780; loss: 1.74; acc: 0.47
Train Epoch over. train_loss: 1.7; train_accuracy: 0.55 

7.32665866962634e-05
6.853100057924166e-05
Batch: 0; loss: 1.82; acc: 0.39
Batch: 20; loss: 1.82; acc: 0.48
Batch: 40; loss: 1.39; acc: 0.73
Batch: 60; loss: 1.58; acc: 0.62
Batch: 80; loss: 1.48; acc: 0.69
Batch: 100; loss: 1.66; acc: 0.62
Batch: 120; loss: 1.66; acc: 0.53
Batch: 140; loss: 1.6; acc: 0.64
Val Epoch over. val_loss: 1.6406851133723168; val_accuracy: 0.5776273885350318 

The current subspace-distance is: 6.853100057924166e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.42
Batch: 20; loss: 1.72; acc: 0.48
Batch: 40; loss: 1.6; acc: 0.62
Batch: 60; loss: 1.54; acc: 0.62
Batch: 80; loss: 1.65; acc: 0.56
Batch: 100; loss: 1.61; acc: 0.61
Batch: 120; loss: 1.79; acc: 0.47
Batch: 140; loss: 1.77; acc: 0.5
Batch: 160; loss: 1.69; acc: 0.58
Batch: 180; loss: 1.56; acc: 0.59
Batch: 200; loss: 1.61; acc: 0.52
Batch: 220; loss: 1.76; acc: 0.53
Batch: 240; loss: 1.72; acc: 0.39
Batch: 260; loss: 1.7; acc: 0.53
Batch: 280; loss: 1.71; acc: 0.53
Batch: 300; loss: 1.66; acc: 0.59
Batch: 320; loss: 1.63; acc: 0.59
Batch: 340; loss: 1.62; acc: 0.61
Batch: 360; loss: 1.61; acc: 0.59
Batch: 380; loss: 1.68; acc: 0.55
Batch: 400; loss: 1.6; acc: 0.61
Batch: 420; loss: 1.68; acc: 0.45
Batch: 440; loss: 1.63; acc: 0.58
Batch: 460; loss: 1.61; acc: 0.61
Batch: 480; loss: 1.65; acc: 0.56
Batch: 500; loss: 1.53; acc: 0.7
Batch: 520; loss: 1.67; acc: 0.53
Batch: 540; loss: 1.63; acc: 0.48
Batch: 560; loss: 1.6; acc: 0.5
Batch: 580; loss: 1.6; acc: 0.59
Batch: 600; loss: 1.6; acc: 0.58
Batch: 620; loss: 1.62; acc: 0.62
Batch: 640; loss: 1.74; acc: 0.55
Batch: 660; loss: 1.62; acc: 0.53
Batch: 680; loss: 1.63; acc: 0.56
Batch: 700; loss: 1.68; acc: 0.5
Batch: 720; loss: 1.54; acc: 0.66
Batch: 740; loss: 1.72; acc: 0.52
Batch: 760; loss: 1.61; acc: 0.55
Batch: 780; loss: 1.66; acc: 0.5
Train Epoch over. train_loss: 1.64; train_accuracy: 0.55 

8.544169395463541e-05
8.042086119530722e-05
Batch: 0; loss: 1.78; acc: 0.34
Batch: 20; loss: 1.76; acc: 0.48
Batch: 40; loss: 1.37; acc: 0.73
Batch: 60; loss: 1.54; acc: 0.59
Batch: 80; loss: 1.46; acc: 0.73
Batch: 100; loss: 1.63; acc: 0.62
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.56; acc: 0.64
Val Epoch over. val_loss: 1.6047526582790788; val_accuracy: 0.5625 

The current subspace-distance is: 8.042086119530722e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.58
Batch: 20; loss: 1.75; acc: 0.45
Batch: 40; loss: 1.69; acc: 0.5
Batch: 60; loss: 1.53; acc: 0.56
Batch: 80; loss: 1.62; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.45
Batch: 120; loss: 1.67; acc: 0.55
Batch: 140; loss: 1.62; acc: 0.42
Batch: 160; loss: 1.67; acc: 0.48
Batch: 180; loss: 1.56; acc: 0.59
Batch: 200; loss: 1.66; acc: 0.47
Batch: 220; loss: 1.59; acc: 0.61
Batch: 240; loss: 1.65; acc: 0.55
Batch: 260; loss: 1.61; acc: 0.56
Batch: 280; loss: 1.52; acc: 0.53
Batch: 300; loss: 1.59; acc: 0.55
Batch: 320; loss: 1.61; acc: 0.52
Batch: 340; loss: 1.52; acc: 0.61
Batch: 360; loss: 1.61; acc: 0.56
Batch: 380; loss: 1.51; acc: 0.64
Batch: 400; loss: 1.63; acc: 0.55
Batch: 420; loss: 1.6; acc: 0.52
Batch: 440; loss: 1.58; acc: 0.56
Batch: 460; loss: 1.52; acc: 0.59
Batch: 480; loss: 1.55; acc: 0.66
Batch: 500; loss: 1.59; acc: 0.62
Batch: 520; loss: 1.62; acc: 0.53
Batch: 540; loss: 1.63; acc: 0.52
Batch: 560; loss: 1.57; acc: 0.53
Batch: 580; loss: 1.58; acc: 0.58
Batch: 600; loss: 1.62; acc: 0.55
Batch: 620; loss: 1.56; acc: 0.62
Batch: 640; loss: 1.51; acc: 0.61
Batch: 660; loss: 1.5; acc: 0.64
Batch: 680; loss: 1.67; acc: 0.48
Batch: 700; loss: 1.45; acc: 0.72
Batch: 720; loss: 1.52; acc: 0.62
Batch: 740; loss: 1.62; acc: 0.59
Batch: 760; loss: 1.66; acc: 0.52
Batch: 780; loss: 1.64; acc: 0.48
Train Epoch over. train_loss: 1.59; train_accuracy: 0.56 

9.881897130981088e-05
9.577131277183071e-05
Batch: 0; loss: 1.63; acc: 0.44
Batch: 20; loss: 1.68; acc: 0.48
Batch: 40; loss: 1.28; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.59
Batch: 80; loss: 1.38; acc: 0.78
Batch: 100; loss: 1.49; acc: 0.67
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.52; acc: 0.64
Val Epoch over. val_loss: 1.5264372370045656; val_accuracy: 0.5962380573248408 

The current subspace-distance is: 9.577131277183071e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.55
Batch: 20; loss: 1.52; acc: 0.58
Batch: 40; loss: 1.56; acc: 0.59
Batch: 60; loss: 1.73; acc: 0.55
Batch: 80; loss: 1.53; acc: 0.62
Batch: 100; loss: 1.48; acc: 0.59
Batch: 120; loss: 1.56; acc: 0.66
Batch: 140; loss: 1.59; acc: 0.53
Batch: 160; loss: 1.47; acc: 0.59
Batch: 180; loss: 1.6; acc: 0.52
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.51; acc: 0.55
Batch: 240; loss: 1.56; acc: 0.59
Batch: 260; loss: 1.52; acc: 0.64
Batch: 280; loss: 1.46; acc: 0.66
Batch: 300; loss: 1.5; acc: 0.72
Batch: 320; loss: 1.56; acc: 0.61
Batch: 340; loss: 1.43; acc: 0.64
Batch: 360; loss: 1.5; acc: 0.62
Batch: 380; loss: 1.56; acc: 0.56
Batch: 400; loss: 1.39; acc: 0.67
Batch: 420; loss: 1.63; acc: 0.55
Batch: 440; loss: 1.45; acc: 0.67
Batch: 460; loss: 1.48; acc: 0.66
Batch: 480; loss: 1.49; acc: 0.69
Batch: 500; loss: 1.48; acc: 0.59
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.46; acc: 0.61
Batch: 560; loss: 1.52; acc: 0.61
Batch: 580; loss: 1.39; acc: 0.7
Batch: 600; loss: 1.43; acc: 0.69
Batch: 620; loss: 1.52; acc: 0.58
Batch: 640; loss: 1.51; acc: 0.5
Batch: 660; loss: 1.49; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.66
Batch: 700; loss: 1.55; acc: 0.59
Batch: 720; loss: 1.54; acc: 0.53
Batch: 740; loss: 1.56; acc: 0.58
Batch: 760; loss: 1.54; acc: 0.62
Batch: 780; loss: 1.48; acc: 0.61
Train Epoch over. train_loss: 1.51; train_accuracy: 0.6 

0.00011523366993060336
0.000110561893961858
Batch: 0; loss: 1.52; acc: 0.59
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.18; acc: 0.83
Batch: 60; loss: 1.41; acc: 0.64
Batch: 80; loss: 1.36; acc: 0.72
Batch: 100; loss: 1.41; acc: 0.64
Batch: 120; loss: 1.56; acc: 0.64
Batch: 140; loss: 1.42; acc: 0.7
Val Epoch over. val_loss: 1.4451222632341325; val_accuracy: 0.6445063694267515 

The current subspace-distance is: 0.000110561893961858 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.52
Batch: 20; loss: 1.51; acc: 0.56
Batch: 40; loss: 1.48; acc: 0.61
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.41; acc: 0.69
Batch: 120; loss: 1.55; acc: 0.59
Batch: 140; loss: 1.44; acc: 0.59
Batch: 160; loss: 1.55; acc: 0.5
Batch: 180; loss: 1.42; acc: 0.61
Batch: 200; loss: 1.53; acc: 0.58
Batch: 220; loss: 1.48; acc: 0.59
Batch: 240; loss: 1.45; acc: 0.58
Batch: 260; loss: 1.42; acc: 0.67
Batch: 280; loss: 1.51; acc: 0.58
Batch: 300; loss: 1.65; acc: 0.53
Batch: 320; loss: 1.47; acc: 0.61
Batch: 340; loss: 1.38; acc: 0.69
Batch: 360; loss: 1.43; acc: 0.67
Batch: 380; loss: 1.49; acc: 0.53
Batch: 400; loss: 1.58; acc: 0.59
Batch: 420; loss: 1.46; acc: 0.64
Batch: 440; loss: 1.51; acc: 0.58
Batch: 460; loss: 1.44; acc: 0.59
Batch: 480; loss: 1.54; acc: 0.55
Batch: 500; loss: 1.49; acc: 0.61
Batch: 520; loss: 1.48; acc: 0.56
Batch: 540; loss: 1.51; acc: 0.64
Batch: 560; loss: 1.32; acc: 0.67
Batch: 580; loss: 1.43; acc: 0.61
Batch: 600; loss: 1.32; acc: 0.66
Batch: 620; loss: 1.36; acc: 0.61
Batch: 640; loss: 1.43; acc: 0.59
Batch: 660; loss: 1.46; acc: 0.67
Batch: 680; loss: 1.37; acc: 0.69
Batch: 700; loss: 1.53; acc: 0.62
Batch: 720; loss: 1.53; acc: 0.56
Batch: 740; loss: 1.34; acc: 0.72
Batch: 760; loss: 1.45; acc: 0.64
Batch: 780; loss: 1.39; acc: 0.67
Train Epoch over. train_loss: 1.45; train_accuracy: 0.62 

0.00013032491551712155
0.0001256567338714376
Batch: 0; loss: 1.42; acc: 0.66
Batch: 20; loss: 1.57; acc: 0.55
Batch: 40; loss: 1.13; acc: 0.83
Batch: 60; loss: 1.36; acc: 0.69
Batch: 80; loss: 1.34; acc: 0.7
Batch: 100; loss: 1.37; acc: 0.62
Batch: 120; loss: 1.54; acc: 0.59
Batch: 140; loss: 1.39; acc: 0.67
Val Epoch over. val_loss: 1.3922874494722695; val_accuracy: 0.6508757961783439 

The current subspace-distance is: 0.0001256567338714376 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.55; acc: 0.58
Batch: 20; loss: 1.36; acc: 0.67
Batch: 40; loss: 1.7; acc: 0.52
Batch: 60; loss: 1.51; acc: 0.53
Batch: 80; loss: 1.4; acc: 0.62
Batch: 100; loss: 1.36; acc: 0.7
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.27; acc: 0.77
Batch: 160; loss: 1.41; acc: 0.62
Batch: 180; loss: 1.32; acc: 0.7
Batch: 200; loss: 1.45; acc: 0.62
Batch: 220; loss: 1.33; acc: 0.72
Batch: 240; loss: 1.5; acc: 0.62
Batch: 260; loss: 1.43; acc: 0.64
Batch: 280; loss: 1.37; acc: 0.61
Batch: 300; loss: 1.35; acc: 0.64
Batch: 320; loss: 1.4; acc: 0.64
Batch: 340; loss: 1.35; acc: 0.69
Batch: 360; loss: 1.3; acc: 0.72
Batch: 380; loss: 1.44; acc: 0.64
Batch: 400; loss: 1.48; acc: 0.61
Batch: 420; loss: 1.43; acc: 0.58
Batch: 440; loss: 1.38; acc: 0.61
Batch: 460; loss: 1.41; acc: 0.59
Batch: 480; loss: 1.42; acc: 0.61
Batch: 500; loss: 1.48; acc: 0.5
Batch: 520; loss: 1.41; acc: 0.58
Batch: 540; loss: 1.28; acc: 0.78
Batch: 560; loss: 1.58; acc: 0.59
Batch: 580; loss: 1.34; acc: 0.69
Batch: 600; loss: 1.57; acc: 0.48
Batch: 620; loss: 1.48; acc: 0.53
Batch: 640; loss: 1.53; acc: 0.5
Batch: 660; loss: 1.32; acc: 0.62
Batch: 680; loss: 1.31; acc: 0.69
Batch: 700; loss: 1.35; acc: 0.64
Batch: 720; loss: 1.41; acc: 0.59
Batch: 740; loss: 1.36; acc: 0.64
Batch: 760; loss: 1.42; acc: 0.56
Batch: 780; loss: 1.43; acc: 0.59
Train Epoch over. train_loss: 1.4; train_accuracy: 0.63 

0.00014281367475632578
0.00013894634321331978
Batch: 0; loss: 1.31; acc: 0.62
Batch: 20; loss: 1.53; acc: 0.53
Batch: 40; loss: 1.1; acc: 0.81
Batch: 60; loss: 1.28; acc: 0.7
Batch: 80; loss: 1.3; acc: 0.67
Batch: 100; loss: 1.31; acc: 0.62
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.37; acc: 0.67
Val Epoch over. val_loss: 1.3398801308528634; val_accuracy: 0.651671974522293 

The current subspace-distance is: 0.00013894634321331978 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.7
Batch: 20; loss: 1.3; acc: 0.75
Batch: 40; loss: 1.4; acc: 0.66
Batch: 60; loss: 1.36; acc: 0.64
Batch: 80; loss: 1.22; acc: 0.7
Batch: 100; loss: 1.41; acc: 0.64
Batch: 120; loss: 1.42; acc: 0.64
Batch: 140; loss: 1.32; acc: 0.61
Batch: 160; loss: 1.28; acc: 0.67
Batch: 180; loss: 1.33; acc: 0.67
Batch: 200; loss: 1.31; acc: 0.69
Batch: 220; loss: 1.3; acc: 0.67
Batch: 240; loss: 1.19; acc: 0.78
Batch: 260; loss: 1.31; acc: 0.58
Batch: 280; loss: 1.37; acc: 0.66
Batch: 300; loss: 1.42; acc: 0.56
Batch: 320; loss: 1.31; acc: 0.66
Batch: 340; loss: 1.42; acc: 0.58
Batch: 360; loss: 1.34; acc: 0.59
Batch: 380; loss: 1.32; acc: 0.64
Batch: 400; loss: 1.19; acc: 0.72
Batch: 420; loss: 1.19; acc: 0.7
Batch: 440; loss: 1.44; acc: 0.59
Batch: 460; loss: 1.45; acc: 0.56
Batch: 480; loss: 1.52; acc: 0.52
Batch: 500; loss: 1.4; acc: 0.62
Batch: 520; loss: 1.33; acc: 0.67
Batch: 540; loss: 1.33; acc: 0.61
Batch: 560; loss: 1.21; acc: 0.69
Batch: 580; loss: 1.39; acc: 0.58
Batch: 600; loss: 1.27; acc: 0.69
Batch: 620; loss: 1.4; acc: 0.62
Batch: 640; loss: 1.41; acc: 0.56
Batch: 660; loss: 1.44; acc: 0.55
Batch: 680; loss: 1.37; acc: 0.66
Batch: 700; loss: 1.22; acc: 0.72
Batch: 720; loss: 1.32; acc: 0.64
Batch: 740; loss: 1.34; acc: 0.69
Batch: 760; loss: 1.2; acc: 0.67
Batch: 780; loss: 1.37; acc: 0.61
Train Epoch over. train_loss: 1.36; train_accuracy: 0.63 

0.00015560824249405414
0.00015108716615941375
Batch: 0; loss: 1.26; acc: 0.64
Batch: 20; loss: 1.52; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.26; acc: 0.7
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.27; acc: 0.64
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.67
Val Epoch over. val_loss: 1.3012064475162772; val_accuracy: 0.6484872611464968 

The current subspace-distance is: 0.00015108716615941375 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.59
Batch: 20; loss: 1.32; acc: 0.66
Batch: 40; loss: 1.41; acc: 0.56
Batch: 60; loss: 1.25; acc: 0.66
Batch: 80; loss: 1.28; acc: 0.72
Batch: 100; loss: 1.36; acc: 0.69
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 1.36; acc: 0.55
Batch: 160; loss: 1.24; acc: 0.69
Batch: 180; loss: 1.36; acc: 0.56
Batch: 200; loss: 1.34; acc: 0.64
Batch: 220; loss: 1.3; acc: 0.67
Batch: 240; loss: 1.25; acc: 0.61
Batch: 260; loss: 1.34; acc: 0.59
Batch: 280; loss: 1.45; acc: 0.59
Batch: 300; loss: 1.34; acc: 0.61
Batch: 320; loss: 1.46; acc: 0.58
Batch: 340; loss: 1.35; acc: 0.59
Batch: 360; loss: 1.39; acc: 0.62
Batch: 380; loss: 1.42; acc: 0.53
Batch: 400; loss: 1.38; acc: 0.62
Batch: 420; loss: 1.45; acc: 0.58
Batch: 440; loss: 1.28; acc: 0.66
Batch: 460; loss: 1.42; acc: 0.59
Batch: 480; loss: 1.44; acc: 0.56
Batch: 500; loss: 1.27; acc: 0.64
Batch: 520; loss: 1.31; acc: 0.62
Batch: 540; loss: 1.23; acc: 0.72
Batch: 560; loss: 1.26; acc: 0.62
Batch: 580; loss: 1.4; acc: 0.61
Batch: 600; loss: 1.39; acc: 0.56
Batch: 620; loss: 1.27; acc: 0.58
Batch: 640; loss: 1.27; acc: 0.64
Batch: 660; loss: 1.39; acc: 0.61
Batch: 680; loss: 1.38; acc: 0.56
Batch: 700; loss: 1.25; acc: 0.69
Batch: 720; loss: 1.37; acc: 0.56
Batch: 740; loss: 1.34; acc: 0.61
Batch: 760; loss: 1.42; acc: 0.56
Batch: 780; loss: 1.33; acc: 0.55
Train Epoch over. train_loss: 1.33; train_accuracy: 0.63 

0.00016537623014301062
0.00015935635019559413
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.54; acc: 0.44
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 1.24; acc: 0.73
Batch: 80; loss: 1.23; acc: 0.61
Batch: 100; loss: 1.25; acc: 0.67
Batch: 120; loss: 1.48; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.66
Val Epoch over. val_loss: 1.276226511426792; val_accuracy: 0.645203025477707 

The current subspace-distance is: 0.00015935635019559413 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.33; acc: 0.62
Batch: 80; loss: 1.27; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.61
Batch: 120; loss: 1.32; acc: 0.64
Batch: 140; loss: 1.36; acc: 0.62
Batch: 160; loss: 1.22; acc: 0.67
Batch: 180; loss: 1.41; acc: 0.58
Batch: 200; loss: 1.3; acc: 0.62
Batch: 220; loss: 1.4; acc: 0.56
Batch: 240; loss: 1.37; acc: 0.59
Batch: 260; loss: 1.35; acc: 0.58
Batch: 280; loss: 1.28; acc: 0.61
Batch: 300; loss: 1.39; acc: 0.55
Batch: 320; loss: 1.17; acc: 0.7
Batch: 340; loss: 1.19; acc: 0.73
Batch: 360; loss: 1.22; acc: 0.67
Batch: 380; loss: 1.45; acc: 0.5
Batch: 400; loss: 1.34; acc: 0.59
Batch: 420; loss: 1.21; acc: 0.67
Batch: 440; loss: 1.17; acc: 0.7
Batch: 460; loss: 1.37; acc: 0.55
Batch: 480; loss: 1.4; acc: 0.53
Batch: 500; loss: 1.24; acc: 0.67
Batch: 520; loss: 1.28; acc: 0.61
Batch: 540; loss: 1.35; acc: 0.56
Batch: 560; loss: 1.33; acc: 0.64
Batch: 580; loss: 1.36; acc: 0.61
Batch: 600; loss: 1.37; acc: 0.59
Batch: 620; loss: 1.15; acc: 0.72
Batch: 640; loss: 1.43; acc: 0.62
Batch: 660; loss: 1.27; acc: 0.69
Batch: 680; loss: 1.33; acc: 0.53
Batch: 700; loss: 1.23; acc: 0.62
Batch: 720; loss: 1.34; acc: 0.66
Batch: 740; loss: 1.29; acc: 0.56
Batch: 760; loss: 1.35; acc: 0.5
Batch: 780; loss: 1.16; acc: 0.72
Train Epoch over. train_loss: 1.31; train_accuracy: 0.63 

0.00017644980107434094
0.0001717500708764419
Batch: 0; loss: 1.24; acc: 0.59
Batch: 20; loss: 1.53; acc: 0.42
Batch: 40; loss: 1.05; acc: 0.77
Batch: 60; loss: 1.23; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.64
Batch: 100; loss: 1.24; acc: 0.66
Batch: 120; loss: 1.47; acc: 0.53
Batch: 140; loss: 1.21; acc: 0.67
Val Epoch over. val_loss: 1.2623580436038364; val_accuracy: 0.6435111464968153 

The current subspace-distance is: 0.0001717500708764419 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 1.2; acc: 0.62
Batch: 40; loss: 1.4; acc: 0.61
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.38; acc: 0.53
Batch: 120; loss: 1.35; acc: 0.61
Batch: 140; loss: 1.35; acc: 0.56
Batch: 160; loss: 1.32; acc: 0.66
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 1.42; acc: 0.61
Batch: 220; loss: 1.37; acc: 0.58
Batch: 240; loss: 1.29; acc: 0.59
Batch: 260; loss: 1.29; acc: 0.69
Batch: 280; loss: 1.2; acc: 0.69
Batch: 300; loss: 1.27; acc: 0.64
Batch: 320; loss: 1.27; acc: 0.58
Batch: 340; loss: 1.19; acc: 0.73
Batch: 360; loss: 1.46; acc: 0.48
Batch: 380; loss: 1.21; acc: 0.56
Batch: 400; loss: 1.31; acc: 0.59
Batch: 420; loss: 1.26; acc: 0.64
Batch: 440; loss: 1.31; acc: 0.66
Batch: 460; loss: 1.15; acc: 0.69
Batch: 480; loss: 1.29; acc: 0.62
Batch: 500; loss: 1.17; acc: 0.72
Batch: 520; loss: 1.46; acc: 0.55
Batch: 540; loss: 1.36; acc: 0.61
Batch: 560; loss: 1.29; acc: 0.62
Batch: 580; loss: 1.23; acc: 0.66
Batch: 600; loss: 1.24; acc: 0.64
Batch: 620; loss: 1.4; acc: 0.52
Batch: 640; loss: 1.22; acc: 0.69
Batch: 660; loss: 1.49; acc: 0.5
Batch: 680; loss: 1.14; acc: 0.73
Batch: 700; loss: 1.2; acc: 0.69
Batch: 720; loss: 1.45; acc: 0.58
Batch: 740; loss: 1.26; acc: 0.56
Batch: 760; loss: 1.45; acc: 0.58
Batch: 780; loss: 1.38; acc: 0.56
Train Epoch over. train_loss: 1.29; train_accuracy: 0.63 

0.00017912591283675283
0.00017301610205322504
Batch: 0; loss: 1.22; acc: 0.56
Batch: 20; loss: 1.52; acc: 0.42
Batch: 40; loss: 1.04; acc: 0.72
Batch: 60; loss: 1.25; acc: 0.7
Batch: 80; loss: 1.21; acc: 0.62
Batch: 100; loss: 1.24; acc: 0.62
Batch: 120; loss: 1.48; acc: 0.55
Batch: 140; loss: 1.2; acc: 0.64
Val Epoch over. val_loss: 1.2536415013538045; val_accuracy: 0.6378383757961783 

The current subspace-distance is: 0.00017301610205322504 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.61
Batch: 20; loss: 1.3; acc: 0.62
Batch: 40; loss: 1.36; acc: 0.62
Batch: 60; loss: 1.3; acc: 0.62
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 1.27; acc: 0.66
Batch: 160; loss: 1.19; acc: 0.64
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 1.13; acc: 0.72
Batch: 220; loss: 1.31; acc: 0.56
Batch: 240; loss: 1.22; acc: 0.67
Batch: 260; loss: 1.51; acc: 0.48
Batch: 280; loss: 1.1; acc: 0.75
Batch: 300; loss: 1.11; acc: 0.78
Batch: 320; loss: 1.27; acc: 0.56
Batch: 340; loss: 1.22; acc: 0.7
Batch: 360; loss: 1.13; acc: 0.62
Batch: 380; loss: 1.28; acc: 0.67
Batch: 400; loss: 1.37; acc: 0.58
Batch: 420; loss: 1.21; acc: 0.67
Batch: 440; loss: 1.25; acc: 0.7
Batch: 460; loss: 1.35; acc: 0.58
Batch: 480; loss: 1.1; acc: 0.67
Batch: 500; loss: 1.48; acc: 0.5
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 1.3; acc: 0.64
Batch: 560; loss: 1.38; acc: 0.53
Batch: 580; loss: 1.08; acc: 0.8
Batch: 600; loss: 1.22; acc: 0.67
Batch: 620; loss: 1.26; acc: 0.61
Batch: 640; loss: 1.29; acc: 0.59
Batch: 660; loss: 1.18; acc: 0.69
Batch: 680; loss: 1.24; acc: 0.69
Batch: 700; loss: 1.37; acc: 0.59
Batch: 720; loss: 1.16; acc: 0.69
Batch: 740; loss: 1.13; acc: 0.7
Batch: 760; loss: 1.25; acc: 0.67
Batch: 780; loss: 1.25; acc: 0.61
Train Epoch over. train_loss: 1.29; train_accuracy: 0.63 

0.00018036046822089702
0.0001737292914185673
Batch: 0; loss: 1.22; acc: 0.58
Batch: 20; loss: 1.52; acc: 0.45
Batch: 40; loss: 1.04; acc: 0.73
Batch: 60; loss: 1.25; acc: 0.7
Batch: 80; loss: 1.2; acc: 0.64
Batch: 100; loss: 1.24; acc: 0.64
Batch: 120; loss: 1.47; acc: 0.58
Batch: 140; loss: 1.19; acc: 0.69
Val Epoch over. val_loss: 1.2509057779980313; val_accuracy: 0.6422173566878981 

The current subspace-distance is: 0.0001737292914185673 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.62
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 1.24; acc: 0.66
Batch: 60; loss: 1.26; acc: 0.66
Batch: 80; loss: 1.38; acc: 0.61
Batch: 100; loss: 1.25; acc: 0.64
Batch: 120; loss: 1.14; acc: 0.73
Batch: 140; loss: 1.47; acc: 0.59
Batch: 160; loss: 1.29; acc: 0.61
Batch: 180; loss: 1.27; acc: 0.64
Batch: 200; loss: 1.33; acc: 0.66
Batch: 220; loss: 1.27; acc: 0.61
Batch: 240; loss: 1.18; acc: 0.69
Batch: 260; loss: 1.18; acc: 0.69
Batch: 280; loss: 1.3; acc: 0.66
Batch: 300; loss: 1.33; acc: 0.61
Batch: 320; loss: 1.14; acc: 0.77
Batch: 340; loss: 1.16; acc: 0.75
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.27; acc: 0.67
Batch: 400; loss: 1.28; acc: 0.66
Batch: 420; loss: 1.3; acc: 0.62
Batch: 440; loss: 1.26; acc: 0.64
Batch: 460; loss: 1.39; acc: 0.62
Batch: 480; loss: 1.35; acc: 0.64
Batch: 500; loss: 1.27; acc: 0.7
Batch: 520; loss: 1.2; acc: 0.67
Batch: 540; loss: 1.19; acc: 0.61
Batch: 560; loss: 1.18; acc: 0.69
Batch: 580; loss: 1.3; acc: 0.64
Batch: 600; loss: 1.26; acc: 0.66
Batch: 620; loss: 1.15; acc: 0.77
Batch: 640; loss: 1.26; acc: 0.64
Batch: 660; loss: 1.13; acc: 0.72
Batch: 680; loss: 1.49; acc: 0.53
Batch: 700; loss: 1.3; acc: 0.58
Batch: 720; loss: 1.38; acc: 0.56
Batch: 740; loss: 1.08; acc: 0.66
Batch: 760; loss: 1.22; acc: 0.66
Batch: 780; loss: 1.3; acc: 0.64
Train Epoch over. train_loss: 1.28; train_accuracy: 0.63 

0.00018234332674182951
0.00017585756722837687
Batch: 0; loss: 1.22; acc: 0.61
Batch: 20; loss: 1.51; acc: 0.42
Batch: 40; loss: 1.04; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.2; acc: 0.62
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.46; acc: 0.58
Batch: 140; loss: 1.16; acc: 0.66
Val Epoch over. val_loss: 1.2405702608406164; val_accuracy: 0.6421178343949044 

The current subspace-distance is: 0.00017585756722837687 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.32; acc: 0.58
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 1.4; acc: 0.5
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.2; acc: 0.72
Batch: 100; loss: 1.33; acc: 0.62
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 1.43; acc: 0.55
Batch: 160; loss: 1.13; acc: 0.73
Batch: 180; loss: 1.2; acc: 0.7
Batch: 200; loss: 1.35; acc: 0.56
Batch: 220; loss: 1.21; acc: 0.62
Batch: 240; loss: 1.42; acc: 0.56
Batch: 260; loss: 1.21; acc: 0.64
Batch: 280; loss: 1.21; acc: 0.59
Batch: 300; loss: 1.36; acc: 0.7
Batch: 320; loss: 1.18; acc: 0.62
Batch: 340; loss: 1.29; acc: 0.59
Batch: 360; loss: 1.42; acc: 0.61
Batch: 380; loss: 1.27; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.35; acc: 0.58
Batch: 440; loss: 1.27; acc: 0.66
Batch: 460; loss: 1.19; acc: 0.66
Batch: 480; loss: 1.34; acc: 0.58
Batch: 500; loss: 1.26; acc: 0.58
Batch: 520; loss: 1.3; acc: 0.62
Batch: 540; loss: 1.22; acc: 0.66
Batch: 560; loss: 1.32; acc: 0.58
Batch: 580; loss: 1.34; acc: 0.58
Batch: 600; loss: 1.24; acc: 0.64
Batch: 620; loss: 1.28; acc: 0.64
Batch: 640; loss: 1.12; acc: 0.72
Batch: 660; loss: 1.19; acc: 0.73
Batch: 680; loss: 1.26; acc: 0.59
Batch: 700; loss: 1.23; acc: 0.64
Batch: 720; loss: 1.25; acc: 0.59
Batch: 740; loss: 1.34; acc: 0.59
Batch: 760; loss: 1.25; acc: 0.7
Batch: 780; loss: 1.26; acc: 0.61
Train Epoch over. train_loss: 1.28; train_accuracy: 0.63 

0.00018508937500882894
0.0001791280519682914
Batch: 0; loss: 1.22; acc: 0.58
Batch: 20; loss: 1.51; acc: 0.44
Batch: 40; loss: 1.05; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.46; acc: 0.58
Batch: 140; loss: 1.15; acc: 0.67
Val Epoch over. val_loss: 1.235953446786115; val_accuracy: 0.6434116242038217 

The current subspace-distance is: 0.0001791280519682914 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.67
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 1.26; acc: 0.66
Batch: 60; loss: 1.38; acc: 0.66
Batch: 80; loss: 1.32; acc: 0.67
Batch: 100; loss: 1.21; acc: 0.62
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 1.52; acc: 0.47
Batch: 160; loss: 1.34; acc: 0.59
Batch: 180; loss: 1.35; acc: 0.58
Batch: 200; loss: 1.2; acc: 0.7
Batch: 220; loss: 1.27; acc: 0.58
Batch: 240; loss: 1.05; acc: 0.78
Batch: 260; loss: 1.2; acc: 0.62
Batch: 280; loss: 1.26; acc: 0.66
Batch: 300; loss: 1.37; acc: 0.58
Batch: 320; loss: 1.19; acc: 0.62
Batch: 340; loss: 1.26; acc: 0.59
Batch: 360; loss: 1.43; acc: 0.48
Batch: 380; loss: 1.29; acc: 0.64
Batch: 400; loss: 1.33; acc: 0.55
Batch: 420; loss: 1.31; acc: 0.59
Batch: 440; loss: 1.46; acc: 0.52
Batch: 460; loss: 1.44; acc: 0.53
Batch: 480; loss: 1.33; acc: 0.59
Batch: 500; loss: 1.26; acc: 0.66
Batch: 520; loss: 1.21; acc: 0.64
Batch: 540; loss: 1.32; acc: 0.61
Batch: 560; loss: 1.28; acc: 0.66
Batch: 580; loss: 1.26; acc: 0.61
Batch: 600; loss: 1.35; acc: 0.55
Batch: 620; loss: 1.32; acc: 0.52
Batch: 640; loss: 1.25; acc: 0.62
Batch: 660; loss: 1.17; acc: 0.66
Batch: 680; loss: 1.31; acc: 0.62
Batch: 700; loss: 1.33; acc: 0.62
Batch: 720; loss: 1.31; acc: 0.52
Batch: 740; loss: 1.35; acc: 0.58
Batch: 760; loss: 1.15; acc: 0.7
Batch: 780; loss: 1.19; acc: 0.69
Train Epoch over. train_loss: 1.27; train_accuracy: 0.63 

0.0001857852330431342
0.00018224808445665985
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.5; acc: 0.42
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.69
Batch: 80; loss: 1.17; acc: 0.62
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.12; acc: 0.7
Val Epoch over. val_loss: 1.2185712490871454; val_accuracy: 0.6429140127388535 

The current subspace-distance is: 0.00018224808445665985 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.41; acc: 0.56
Batch: 20; loss: 1.4; acc: 0.64
Batch: 40; loss: 1.37; acc: 0.61
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 1.31; acc: 0.64
Batch: 100; loss: 1.21; acc: 0.67
Batch: 120; loss: 1.27; acc: 0.61
Batch: 140; loss: 1.26; acc: 0.67
Batch: 160; loss: 1.27; acc: 0.69
Batch: 180; loss: 1.24; acc: 0.66
Batch: 200; loss: 1.08; acc: 0.77
Batch: 220; loss: 1.37; acc: 0.61
Batch: 240; loss: 1.2; acc: 0.66
Batch: 260; loss: 1.17; acc: 0.7
Batch: 280; loss: 1.45; acc: 0.5
Batch: 300; loss: 1.21; acc: 0.64
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.3; acc: 0.62
Batch: 360; loss: 1.33; acc: 0.66
Batch: 380; loss: 1.33; acc: 0.53
Batch: 400; loss: 1.23; acc: 0.66
Batch: 420; loss: 1.39; acc: 0.56
Batch: 440; loss: 1.33; acc: 0.62
Batch: 460; loss: 1.41; acc: 0.55
Batch: 480; loss: 1.23; acc: 0.67
Batch: 500; loss: 1.23; acc: 0.62
Batch: 520; loss: 1.3; acc: 0.58
Batch: 540; loss: 1.37; acc: 0.55
Batch: 560; loss: 1.17; acc: 0.7
Batch: 580; loss: 1.32; acc: 0.61
Batch: 600; loss: 1.3; acc: 0.64
Batch: 620; loss: 1.18; acc: 0.66
Batch: 640; loss: 1.14; acc: 0.73
Batch: 660; loss: 1.18; acc: 0.66
Batch: 680; loss: 1.23; acc: 0.66
Batch: 700; loss: 1.33; acc: 0.61
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.26; acc: 0.64
Batch: 760; loss: 1.22; acc: 0.58
Batch: 780; loss: 1.29; acc: 0.58
Train Epoch over. train_loss: 1.27; train_accuracy: 0.63 

0.00019037816673517227
0.00018460895807947963
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 1.53; acc: 0.42
Batch: 40; loss: 1.03; acc: 0.73
Batch: 60; loss: 1.25; acc: 0.69
Batch: 80; loss: 1.19; acc: 0.67
Batch: 100; loss: 1.24; acc: 0.62
Batch: 120; loss: 1.48; acc: 0.53
Batch: 140; loss: 1.14; acc: 0.7
Val Epoch over. val_loss: 1.2355070850651735; val_accuracy: 0.6399283439490446 

The current subspace-distance is: 0.00018460895807947963 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.26; acc: 0.72
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 1.42; acc: 0.5
Batch: 60; loss: 1.1; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 1.21; acc: 0.69
Batch: 160; loss: 1.19; acc: 0.69
Batch: 180; loss: 1.39; acc: 0.56
Batch: 200; loss: 1.06; acc: 0.73
Batch: 220; loss: 1.12; acc: 0.67
Batch: 240; loss: 1.24; acc: 0.66
Batch: 260; loss: 1.33; acc: 0.62
Batch: 280; loss: 1.45; acc: 0.56
Batch: 300; loss: 1.35; acc: 0.59
Batch: 320; loss: 1.32; acc: 0.66
Batch: 340; loss: 1.28; acc: 0.64
Batch: 360; loss: 1.33; acc: 0.58
Batch: 380; loss: 1.27; acc: 0.59
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 1.29; acc: 0.59
Batch: 440; loss: 1.35; acc: 0.62
Batch: 460; loss: 1.29; acc: 0.61
Batch: 480; loss: 1.46; acc: 0.5
Batch: 500; loss: 1.5; acc: 0.53
Batch: 520; loss: 1.19; acc: 0.67
Batch: 540; loss: 1.25; acc: 0.67
Batch: 560; loss: 1.19; acc: 0.64
Batch: 580; loss: 1.23; acc: 0.61
Batch: 600; loss: 1.08; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.66
Batch: 640; loss: 1.27; acc: 0.64
Batch: 660; loss: 1.31; acc: 0.56
Batch: 680; loss: 1.22; acc: 0.64
Batch: 700; loss: 1.15; acc: 0.72
Batch: 720; loss: 1.28; acc: 0.58
Batch: 740; loss: 1.19; acc: 0.66
Batch: 760; loss: 1.4; acc: 0.61
Batch: 780; loss: 1.14; acc: 0.67
Train Epoch over. train_loss: 1.26; train_accuracy: 0.63 

0.00019353815878275782
0.00018539726443123072
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.5; acc: 0.45
Batch: 40; loss: 1.03; acc: 0.69
Batch: 60; loss: 1.24; acc: 0.69
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.44; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.7
Val Epoch over. val_loss: 1.2201420250971606; val_accuracy: 0.646297770700637 

The current subspace-distance is: 0.00018539726443123072 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.7
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 1.29; acc: 0.61
Batch: 60; loss: 1.18; acc: 0.69
Batch: 80; loss: 1.3; acc: 0.59
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 1.3; acc: 0.53
Batch: 140; loss: 1.31; acc: 0.66
Batch: 160; loss: 1.22; acc: 0.67
Batch: 180; loss: 1.36; acc: 0.56
Batch: 200; loss: 1.37; acc: 0.59
Batch: 220; loss: 1.39; acc: 0.58
Batch: 240; loss: 1.25; acc: 0.58
Batch: 260; loss: 1.27; acc: 0.61
Batch: 280; loss: 1.35; acc: 0.59
Batch: 300; loss: 1.25; acc: 0.64
Batch: 320; loss: 1.37; acc: 0.53
Batch: 340; loss: 1.19; acc: 0.62
Batch: 360; loss: 1.26; acc: 0.62
Batch: 380; loss: 1.3; acc: 0.66
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.22; acc: 0.69
Batch: 440; loss: 1.35; acc: 0.56
Batch: 460; loss: 1.26; acc: 0.62
Batch: 480; loss: 1.34; acc: 0.59
Batch: 500; loss: 1.54; acc: 0.53
Batch: 520; loss: 1.1; acc: 0.67
Batch: 540; loss: 1.11; acc: 0.7
Batch: 560; loss: 1.23; acc: 0.62
Batch: 580; loss: 1.04; acc: 0.81
Batch: 600; loss: 1.28; acc: 0.58
Batch: 620; loss: 1.28; acc: 0.62
Batch: 640; loss: 1.18; acc: 0.69
Batch: 660; loss: 1.34; acc: 0.58
Batch: 680; loss: 1.28; acc: 0.55
Batch: 700; loss: 1.34; acc: 0.58
Batch: 720; loss: 1.16; acc: 0.69
Batch: 740; loss: 1.36; acc: 0.55
Batch: 760; loss: 1.26; acc: 0.61
Batch: 780; loss: 1.36; acc: 0.53
Train Epoch over. train_loss: 1.25; train_accuracy: 0.63 

0.0001930590224219486
0.00018623308278620243
Batch: 0; loss: 1.21; acc: 0.61
Batch: 20; loss: 1.5; acc: 0.45
Batch: 40; loss: 1.04; acc: 0.72
Batch: 60; loss: 1.25; acc: 0.64
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.67
Batch: 120; loss: 1.47; acc: 0.56
Batch: 140; loss: 1.1; acc: 0.72
Val Epoch over. val_loss: 1.2206574545544424; val_accuracy: 0.6456011146496815 

The current subspace-distance is: 0.00018623308278620243 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 1.08; acc: 0.7
Batch: 60; loss: 1.25; acc: 0.66
Batch: 80; loss: 1.22; acc: 0.67
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.23; acc: 0.67
Batch: 140; loss: 1.21; acc: 0.64
Batch: 160; loss: 1.29; acc: 0.64
Batch: 180; loss: 1.05; acc: 0.7
Batch: 200; loss: 1.12; acc: 0.7
Batch: 220; loss: 1.28; acc: 0.59
Batch: 240; loss: 1.17; acc: 0.62
Batch: 260; loss: 1.26; acc: 0.61
Batch: 280; loss: 1.3; acc: 0.56
Batch: 300; loss: 1.39; acc: 0.59
Batch: 320; loss: 1.29; acc: 0.56
Batch: 340; loss: 1.28; acc: 0.62
Batch: 360; loss: 1.22; acc: 0.58
Batch: 380; loss: 1.25; acc: 0.69
Batch: 400; loss: 1.09; acc: 0.73
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 1.19; acc: 0.67
Batch: 460; loss: 1.23; acc: 0.62
Batch: 480; loss: 1.35; acc: 0.56
Batch: 500; loss: 1.22; acc: 0.56
Batch: 520; loss: 1.27; acc: 0.66
Batch: 540; loss: 1.36; acc: 0.5
Batch: 560; loss: 1.14; acc: 0.64
Batch: 580; loss: 1.26; acc: 0.61
Batch: 600; loss: 1.46; acc: 0.48
Batch: 620; loss: 1.06; acc: 0.73
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 1.35; acc: 0.56
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 1.1; acc: 0.75
Batch: 720; loss: 1.28; acc: 0.62
Batch: 740; loss: 1.33; acc: 0.56
Batch: 760; loss: 1.34; acc: 0.62
Batch: 780; loss: 1.34; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.63 

0.00019814242841675878
0.00019159838848281652
Batch: 0; loss: 1.21; acc: 0.61
Batch: 20; loss: 1.51; acc: 0.45
Batch: 40; loss: 1.01; acc: 0.73
Batch: 60; loss: 1.24; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.66
Batch: 100; loss: 1.22; acc: 0.66
Batch: 120; loss: 1.47; acc: 0.53
Batch: 140; loss: 1.09; acc: 0.72
Val Epoch over. val_loss: 1.2143767255886344; val_accuracy: 0.6426154458598726 

The current subspace-distance is: 0.00019159838848281652 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 1.25; acc: 0.64
Batch: 40; loss: 1.28; acc: 0.58
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.15; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.62
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.64
Batch: 160; loss: 1.28; acc: 0.61
Batch: 180; loss: 1.17; acc: 0.7
Batch: 200; loss: 1.27; acc: 0.66
Batch: 220; loss: 1.31; acc: 0.61
Batch: 240; loss: 1.08; acc: 0.69
Batch: 260; loss: 1.13; acc: 0.67
Batch: 280; loss: 1.06; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.67
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 1.18; acc: 0.69
Batch: 360; loss: 1.2; acc: 0.66
Batch: 380; loss: 1.32; acc: 0.55
Batch: 400; loss: 1.16; acc: 0.67
Batch: 420; loss: 1.31; acc: 0.61
Batch: 440; loss: 1.32; acc: 0.56
Batch: 460; loss: 1.51; acc: 0.47
Batch: 480; loss: 1.34; acc: 0.56
Batch: 500; loss: 1.22; acc: 0.62
Batch: 520; loss: 1.21; acc: 0.62
Batch: 540; loss: 1.3; acc: 0.62
Batch: 560; loss: 1.22; acc: 0.59
Batch: 580; loss: 1.25; acc: 0.62
Batch: 600; loss: 1.38; acc: 0.56
Batch: 620; loss: 1.29; acc: 0.61
Batch: 640; loss: 1.18; acc: 0.69
Batch: 660; loss: 1.17; acc: 0.69
Batch: 680; loss: 1.21; acc: 0.66
Batch: 700; loss: 1.33; acc: 0.62
Batch: 720; loss: 1.34; acc: 0.58
Batch: 740; loss: 1.28; acc: 0.62
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.2; acc: 0.61
Train Epoch over. train_loss: 1.24; train_accuracy: 0.63 

0.0001977584615815431
0.0001918190100695938
Batch: 0; loss: 1.21; acc: 0.56
Batch: 20; loss: 1.49; acc: 0.47
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.72
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.09; acc: 0.73
Val Epoch over. val_loss: 1.2101792738695814; val_accuracy: 0.6506767515923567 

The current subspace-distance is: 0.0001918190100695938 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.25; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.64
Batch: 40; loss: 1.1; acc: 0.72
Batch: 60; loss: 1.49; acc: 0.5
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 1.09; acc: 0.67
Batch: 120; loss: 1.29; acc: 0.52
Batch: 140; loss: 1.24; acc: 0.62
Batch: 160; loss: 1.09; acc: 0.69
Batch: 180; loss: 1.36; acc: 0.42
Batch: 200; loss: 1.17; acc: 0.67
Batch: 220; loss: 1.31; acc: 0.62
Batch: 240; loss: 1.34; acc: 0.59
Batch: 260; loss: 1.32; acc: 0.56
Batch: 280; loss: 1.22; acc: 0.61
Batch: 300; loss: 1.25; acc: 0.59
Batch: 320; loss: 1.15; acc: 0.66
Batch: 340; loss: 1.2; acc: 0.66
Batch: 360; loss: 1.24; acc: 0.61
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 1.3; acc: 0.61
Batch: 420; loss: 1.36; acc: 0.59
Batch: 440; loss: 1.2; acc: 0.67
Batch: 460; loss: 1.12; acc: 0.67
Batch: 480; loss: 1.36; acc: 0.59
Batch: 500; loss: 1.55; acc: 0.44
Batch: 520; loss: 1.24; acc: 0.64
Batch: 540; loss: 1.1; acc: 0.66
Batch: 560; loss: 1.32; acc: 0.61
Batch: 580; loss: 1.14; acc: 0.73
Batch: 600; loss: 1.19; acc: 0.7
Batch: 620; loss: 1.07; acc: 0.78
Batch: 640; loss: 1.34; acc: 0.62
Batch: 660; loss: 1.34; acc: 0.52
Batch: 680; loss: 1.14; acc: 0.67
Batch: 700; loss: 1.31; acc: 0.64
Batch: 720; loss: 1.2; acc: 0.58
Batch: 740; loss: 1.37; acc: 0.56
Batch: 760; loss: 1.45; acc: 0.52
Batch: 780; loss: 1.22; acc: 0.66
Train Epoch over. train_loss: 1.24; train_accuracy: 0.63 

0.00019519471970852464
0.00018980997265316546
Batch: 0; loss: 1.21; acc: 0.59
Batch: 20; loss: 1.48; acc: 0.45
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.18; acc: 0.72
Batch: 100; loss: 1.23; acc: 0.69
Batch: 120; loss: 1.46; acc: 0.55
Batch: 140; loss: 1.09; acc: 0.7
Val Epoch over. val_loss: 1.2133860303338166; val_accuracy: 0.647890127388535 

The current subspace-distance is: 0.00018980997265316546 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.23; acc: 0.67
Batch: 40; loss: 1.4; acc: 0.5
Batch: 60; loss: 1.32; acc: 0.62
Batch: 80; loss: 1.21; acc: 0.58
Batch: 100; loss: 1.25; acc: 0.64
Batch: 120; loss: 1.37; acc: 0.55
Batch: 140; loss: 1.25; acc: 0.62
Batch: 160; loss: 1.34; acc: 0.61
Batch: 180; loss: 1.14; acc: 0.66
Batch: 200; loss: 1.25; acc: 0.62
Batch: 220; loss: 1.07; acc: 0.69
Batch: 240; loss: 1.18; acc: 0.67
Batch: 260; loss: 1.29; acc: 0.62
Batch: 280; loss: 1.21; acc: 0.59
Batch: 300; loss: 1.31; acc: 0.56
Batch: 320; loss: 1.34; acc: 0.52
Batch: 340; loss: 1.19; acc: 0.62
Batch: 360; loss: 1.29; acc: 0.61
Batch: 380; loss: 1.21; acc: 0.62
Batch: 400; loss: 1.35; acc: 0.48
Batch: 420; loss: 1.18; acc: 0.69
Batch: 440; loss: 1.19; acc: 0.67
Batch: 460; loss: 1.2; acc: 0.66
Batch: 480; loss: 1.35; acc: 0.66
Batch: 500; loss: 1.1; acc: 0.67
Batch: 520; loss: 1.19; acc: 0.7
Batch: 540; loss: 1.43; acc: 0.53
Batch: 560; loss: 1.13; acc: 0.72
Batch: 580; loss: 1.29; acc: 0.62
Batch: 600; loss: 1.24; acc: 0.64
Batch: 620; loss: 1.28; acc: 0.59
Batch: 640; loss: 1.38; acc: 0.55
Batch: 660; loss: 1.18; acc: 0.59
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.18; acc: 0.64
Batch: 720; loss: 1.19; acc: 0.72
Batch: 740; loss: 1.16; acc: 0.69
Batch: 760; loss: 1.17; acc: 0.61
Batch: 780; loss: 1.26; acc: 0.61
Train Epoch over. train_loss: 1.24; train_accuracy: 0.63 

0.00019953794253524393
0.00019117769261356443
Batch: 0; loss: 1.2; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.47
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 1.45; acc: 0.59
Batch: 140; loss: 1.07; acc: 0.72
Val Epoch over. val_loss: 1.2075578471657578; val_accuracy: 0.648984872611465 

The current subspace-distance is: 0.00019117769261356443 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.67
Batch: 20; loss: 1.4; acc: 0.59
Batch: 40; loss: 1.22; acc: 0.69
Batch: 60; loss: 1.2; acc: 0.58
Batch: 80; loss: 1.15; acc: 0.69
Batch: 100; loss: 1.25; acc: 0.64
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 1.23; acc: 0.61
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.42; acc: 0.48
Batch: 200; loss: 1.2; acc: 0.61
Batch: 220; loss: 1.27; acc: 0.59
Batch: 240; loss: 1.41; acc: 0.59
Batch: 260; loss: 1.39; acc: 0.5
Batch: 280; loss: 1.2; acc: 0.66
Batch: 300; loss: 1.36; acc: 0.64
Batch: 320; loss: 1.22; acc: 0.59
Batch: 340; loss: 1.26; acc: 0.59
Batch: 360; loss: 1.44; acc: 0.5
Batch: 380; loss: 1.26; acc: 0.61
Batch: 400; loss: 1.16; acc: 0.7
Batch: 420; loss: 1.1; acc: 0.75
Batch: 440; loss: 1.16; acc: 0.62
Batch: 460; loss: 1.01; acc: 0.8
Batch: 480; loss: 1.34; acc: 0.52
Batch: 500; loss: 1.17; acc: 0.62
Batch: 520; loss: 1.31; acc: 0.64
Batch: 540; loss: 1.22; acc: 0.61
Batch: 560; loss: 1.3; acc: 0.61
Batch: 580; loss: 1.25; acc: 0.62
Batch: 600; loss: 1.29; acc: 0.59
Batch: 620; loss: 1.15; acc: 0.66
Batch: 640; loss: 1.27; acc: 0.59
Batch: 660; loss: 1.17; acc: 0.62
Batch: 680; loss: 1.22; acc: 0.72
Batch: 700; loss: 1.37; acc: 0.56
Batch: 720; loss: 1.24; acc: 0.62
Batch: 740; loss: 1.2; acc: 0.64
Batch: 760; loss: 1.42; acc: 0.56
Batch: 780; loss: 1.2; acc: 0.66
Train Epoch over. train_loss: 1.24; train_accuracy: 0.63 

0.0001995459315367043
0.00019324396271258593
Batch: 0; loss: 1.2; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.45
Batch: 40; loss: 1.01; acc: 0.72
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.67
Batch: 100; loss: 1.21; acc: 0.67
Batch: 120; loss: 1.45; acc: 0.58
Batch: 140; loss: 1.07; acc: 0.69
Val Epoch over. val_loss: 1.2074676619213858; val_accuracy: 0.6482882165605095 

The current subspace-distance is: 0.00019324396271258593 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.32; acc: 0.53
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 1.14; acc: 0.62
Batch: 60; loss: 1.26; acc: 0.59
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 1.22; acc: 0.62
Batch: 120; loss: 1.2; acc: 0.58
Batch: 140; loss: 1.28; acc: 0.55
Batch: 160; loss: 1.23; acc: 0.66
Batch: 180; loss: 1.19; acc: 0.69
Batch: 200; loss: 1.18; acc: 0.69
Batch: 220; loss: 1.26; acc: 0.56
Batch: 240; loss: 1.5; acc: 0.52
Batch: 260; loss: 1.23; acc: 0.62
Batch: 280; loss: 1.36; acc: 0.59
Batch: 300; loss: 1.43; acc: 0.59
Batch: 320; loss: 1.35; acc: 0.55
Batch: 340; loss: 1.17; acc: 0.64
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.21; acc: 0.64
Batch: 400; loss: 1.04; acc: 0.73
Batch: 420; loss: 1.18; acc: 0.62
Batch: 440; loss: 1.16; acc: 0.67
Batch: 460; loss: 1.21; acc: 0.66
Batch: 480; loss: 1.27; acc: 0.59
Batch: 500; loss: 1.24; acc: 0.56
Batch: 520; loss: 1.43; acc: 0.58
Batch: 540; loss: 1.24; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.59
Batch: 580; loss: 1.21; acc: 0.66
Batch: 600; loss: 1.4; acc: 0.5
Batch: 620; loss: 1.27; acc: 0.61
Batch: 640; loss: 1.14; acc: 0.67
Batch: 660; loss: 1.21; acc: 0.62
Batch: 680; loss: 1.33; acc: 0.55
Batch: 700; loss: 1.2; acc: 0.64
Batch: 720; loss: 1.16; acc: 0.64
Batch: 740; loss: 1.18; acc: 0.72
Batch: 760; loss: 1.18; acc: 0.73
Batch: 780; loss: 1.21; acc: 0.66
Train Epoch over. train_loss: 1.24; train_accuracy: 0.63 

0.00019772700034081936
0.00019226134463679045
Batch: 0; loss: 1.21; acc: 0.59
Batch: 20; loss: 1.48; acc: 0.47
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.67
Batch: 80; loss: 1.18; acc: 0.64
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.45; acc: 0.58
Batch: 140; loss: 1.07; acc: 0.7
Val Epoch over. val_loss: 1.206833372829826; val_accuracy: 0.6467953821656051 

The current subspace-distance is: 0.00019226134463679045 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.34; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 1.31; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 1.29; acc: 0.58
Batch: 100; loss: 1.22; acc: 0.61
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.62
Batch: 160; loss: 1.31; acc: 0.59
Batch: 180; loss: 1.23; acc: 0.64
Batch: 200; loss: 1.17; acc: 0.69
Batch: 220; loss: 1.31; acc: 0.59
Batch: 240; loss: 1.33; acc: 0.56
Batch: 260; loss: 1.25; acc: 0.69
Batch: 280; loss: 1.17; acc: 0.7
Batch: 300; loss: 1.18; acc: 0.69
Batch: 320; loss: 1.3; acc: 0.62
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 1.33; acc: 0.52
Batch: 380; loss: 1.07; acc: 0.67
Batch: 400; loss: 1.26; acc: 0.61
Batch: 420; loss: 1.17; acc: 0.64
Batch: 440; loss: 1.43; acc: 0.5
Batch: 460; loss: 1.17; acc: 0.7
Batch: 480; loss: 1.23; acc: 0.69
Batch: 500; loss: 1.2; acc: 0.66
Batch: 520; loss: 1.19; acc: 0.66
Batch: 540; loss: 1.02; acc: 0.81
Batch: 560; loss: 1.36; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.69
Batch: 600; loss: 1.14; acc: 0.72
Batch: 620; loss: 1.13; acc: 0.64
Batch: 640; loss: 1.38; acc: 0.55
Batch: 660; loss: 1.33; acc: 0.61
Batch: 680; loss: 1.29; acc: 0.56
Batch: 700; loss: 1.2; acc: 0.61
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.27; acc: 0.66
Batch: 760; loss: 1.28; acc: 0.64
Batch: 780; loss: 1.08; acc: 0.67
Train Epoch over. train_loss: 1.24; train_accuracy: 0.63 

0.00019889742543455213
0.0001931438164319843
Batch: 0; loss: 1.21; acc: 0.61
Batch: 20; loss: 1.48; acc: 0.47
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.67
Batch: 80; loss: 1.18; acc: 0.66
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.47; acc: 0.55
Batch: 140; loss: 1.07; acc: 0.73
Val Epoch over. val_loss: 1.210293166576677; val_accuracy: 0.6479896496815286 

The current subspace-distance is: 0.0001931438164319843 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.29; acc: 0.61
Batch: 20; loss: 1.15; acc: 0.64
Batch: 40; loss: 1.29; acc: 0.56
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.27; acc: 0.64
Batch: 100; loss: 1.42; acc: 0.52
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 1.21; acc: 0.67
Batch: 160; loss: 1.22; acc: 0.62
Batch: 180; loss: 1.29; acc: 0.61
Batch: 200; loss: 1.46; acc: 0.52
Batch: 220; loss: 1.17; acc: 0.73
Batch: 240; loss: 1.25; acc: 0.62
Batch: 260; loss: 1.35; acc: 0.59
Batch: 280; loss: 1.35; acc: 0.59
Batch: 300; loss: 1.14; acc: 0.72
Batch: 320; loss: 1.17; acc: 0.7
Batch: 340; loss: 1.13; acc: 0.7
Batch: 360; loss: 1.21; acc: 0.62
Batch: 380; loss: 1.11; acc: 0.69
Batch: 400; loss: 1.23; acc: 0.61
Batch: 420; loss: 1.27; acc: 0.59
Batch: 440; loss: 1.2; acc: 0.69
Batch: 460; loss: 1.51; acc: 0.52
Batch: 480; loss: 1.38; acc: 0.48
Batch: 500; loss: 1.36; acc: 0.59
Batch: 520; loss: 1.41; acc: 0.58
Batch: 540; loss: 1.24; acc: 0.69
Batch: 560; loss: 1.22; acc: 0.64
Batch: 580; loss: 1.53; acc: 0.47
Batch: 600; loss: 1.26; acc: 0.61
Batch: 620; loss: 1.26; acc: 0.64
Batch: 640; loss: 1.16; acc: 0.72
Batch: 660; loss: 1.29; acc: 0.59
Batch: 680; loss: 1.08; acc: 0.69
Batch: 700; loss: 1.32; acc: 0.61
Batch: 720; loss: 1.18; acc: 0.69
Batch: 740; loss: 1.2; acc: 0.62
Batch: 760; loss: 1.33; acc: 0.62
Batch: 780; loss: 1.17; acc: 0.64
Train Epoch over. train_loss: 1.23; train_accuracy: 0.63 

0.0001996715145651251
0.00019529082055669278
Batch: 0; loss: 1.2; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.47
Batch: 40; loss: 1.02; acc: 0.72
Batch: 60; loss: 1.24; acc: 0.67
Batch: 80; loss: 1.18; acc: 0.64
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.47; acc: 0.55
Batch: 140; loss: 1.07; acc: 0.69
Val Epoch over. val_loss: 1.205383647778991; val_accuracy: 0.6465963375796179 

The current subspace-distance is: 0.00019529082055669278 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.32; acc: 0.64
Batch: 40; loss: 1.26; acc: 0.56
Batch: 60; loss: 1.3; acc: 0.55
Batch: 80; loss: 1.06; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.15; acc: 0.67
Batch: 140; loss: 1.19; acc: 0.67
Batch: 160; loss: 1.39; acc: 0.62
Batch: 180; loss: 1.33; acc: 0.61
Batch: 200; loss: 1.4; acc: 0.53
Batch: 220; loss: 1.11; acc: 0.67
Batch: 240; loss: 1.03; acc: 0.75
Batch: 260; loss: 1.19; acc: 0.66
Batch: 280; loss: 1.19; acc: 0.69
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 1.28; acc: 0.67
Batch: 340; loss: 1.27; acc: 0.64
Batch: 360; loss: 1.24; acc: 0.72
Batch: 380; loss: 1.24; acc: 0.62
Batch: 400; loss: 1.13; acc: 0.69
Batch: 420; loss: 1.2; acc: 0.66
Batch: 440; loss: 1.09; acc: 0.73
Batch: 460; loss: 1.38; acc: 0.5
Batch: 480; loss: 1.07; acc: 0.66
Batch: 500; loss: 1.22; acc: 0.66
Batch: 520; loss: 1.32; acc: 0.58
Batch: 540; loss: 1.26; acc: 0.64
Batch: 560; loss: 1.12; acc: 0.72
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.24; acc: 0.61
Batch: 620; loss: 1.17; acc: 0.62
Batch: 640; loss: 1.15; acc: 0.72
Batch: 660; loss: 1.18; acc: 0.62
Batch: 680; loss: 1.28; acc: 0.61
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.17; acc: 0.67
Batch: 740; loss: 1.25; acc: 0.66
Batch: 760; loss: 1.18; acc: 0.64
Batch: 780; loss: 1.18; acc: 0.66
Train Epoch over. train_loss: 1.24; train_accuracy: 0.63 

0.0002007958828471601
0.00019862149201799184
Batch: 0; loss: 1.21; acc: 0.59
Batch: 20; loss: 1.47; acc: 0.47
Batch: 40; loss: 1.01; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.17; acc: 0.69
Batch: 100; loss: 1.22; acc: 0.67
Batch: 120; loss: 1.44; acc: 0.58
Batch: 140; loss: 1.06; acc: 0.72
Val Epoch over. val_loss: 1.2008945455976352; val_accuracy: 0.6510748407643312 

The current subspace-distance is: 0.00019862149201799184 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 1.13; acc: 0.66
Batch: 60; loss: 1.26; acc: 0.7
Batch: 80; loss: 1.31; acc: 0.56
Batch: 100; loss: 1.14; acc: 0.7
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 1.2; acc: 0.69
Batch: 160; loss: 1.1; acc: 0.72
Batch: 180; loss: 1.11; acc: 0.7
Batch: 200; loss: 1.26; acc: 0.62
Batch: 220; loss: 1.17; acc: 0.61
Batch: 240; loss: 1.22; acc: 0.62
Batch: 260; loss: 1.31; acc: 0.56
Batch: 280; loss: 1.37; acc: 0.56
Batch: 300; loss: 1.1; acc: 0.67
Batch: 320; loss: 1.35; acc: 0.53
Batch: 340; loss: 1.18; acc: 0.7
Batch: 360; loss: 1.22; acc: 0.62
Batch: 380; loss: 1.08; acc: 0.75
Batch: 400; loss: 1.28; acc: 0.67
Batch: 420; loss: 1.31; acc: 0.62
Batch: 440; loss: 1.32; acc: 0.61
Batch: 460; loss: 1.24; acc: 0.59
Batch: 480; loss: 1.14; acc: 0.7
Batch: 500; loss: 1.4; acc: 0.53
Batch: 520; loss: 1.1; acc: 0.7
Batch: 540; loss: 1.17; acc: 0.67
Batch: 560; loss: 1.29; acc: 0.67
Batch: 580; loss: 1.29; acc: 0.58
Batch: 600; loss: 1.16; acc: 0.62
Batch: 620; loss: 1.13; acc: 0.7
Batch: 640; loss: 1.37; acc: 0.53
Batch: 660; loss: 1.13; acc: 0.69
Batch: 680; loss: 1.2; acc: 0.69
Batch: 700; loss: 1.38; acc: 0.56
Batch: 720; loss: 1.2; acc: 0.61
Batch: 740; loss: 1.21; acc: 0.62
Batch: 760; loss: 1.16; acc: 0.64
Batch: 780; loss: 1.2; acc: 0.64
Train Epoch over. train_loss: 1.23; train_accuracy: 0.63 

0.00020079118257854134
0.0001939550566021353
Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.46; acc: 0.45
Batch: 40; loss: 1.01; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.64
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 1.07; acc: 0.7
Val Epoch over. val_loss: 1.1992821962970077; val_accuracy: 0.6500796178343949 

The current subspace-distance is: 0.0001939550566021353 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 1.35; acc: 0.58
Batch: 60; loss: 1.18; acc: 0.62
Batch: 80; loss: 1.16; acc: 0.64
Batch: 100; loss: 1.25; acc: 0.58
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 1.27; acc: 0.62
Batch: 160; loss: 1.28; acc: 0.58
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 1.36; acc: 0.55
Batch: 220; loss: 1.2; acc: 0.66
Batch: 240; loss: 1.23; acc: 0.69
Batch: 260; loss: 1.45; acc: 0.55
Batch: 280; loss: 1.16; acc: 0.66
Batch: 300; loss: 1.42; acc: 0.58
Batch: 320; loss: 1.22; acc: 0.56
Batch: 340; loss: 1.15; acc: 0.7
Batch: 360; loss: 1.27; acc: 0.61
Batch: 380; loss: 1.21; acc: 0.64
Batch: 400; loss: 1.41; acc: 0.62
Batch: 420; loss: 1.14; acc: 0.69
Batch: 440; loss: 1.37; acc: 0.53
Batch: 460; loss: 1.25; acc: 0.62
Batch: 480; loss: 1.26; acc: 0.62
Batch: 500; loss: 1.25; acc: 0.67
Batch: 520; loss: 1.23; acc: 0.62
Batch: 540; loss: 1.17; acc: 0.69
Batch: 560; loss: 1.28; acc: 0.66
Batch: 580; loss: 1.05; acc: 0.78
Batch: 600; loss: 1.37; acc: 0.59
Batch: 620; loss: 1.19; acc: 0.62
Batch: 640; loss: 1.36; acc: 0.52
Batch: 660; loss: 1.07; acc: 0.7
Batch: 680; loss: 1.24; acc: 0.62
Batch: 700; loss: 1.38; acc: 0.59
Batch: 720; loss: 1.45; acc: 0.5
Batch: 740; loss: 1.25; acc: 0.56
Batch: 760; loss: 1.3; acc: 0.55
Batch: 780; loss: 1.13; acc: 0.66
Train Epoch over. train_loss: 1.23; train_accuracy: 0.63 

0.00020201070583425462
0.00019685481674969196
Batch: 0; loss: 1.2; acc: 0.58
Batch: 20; loss: 1.47; acc: 0.47
Batch: 40; loss: 1.01; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.16; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.66
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 1.05; acc: 0.7
Val Epoch over. val_loss: 1.1981709720982108; val_accuracy: 0.648984872611465 

The current subspace-distance is: 0.00019685481674969196 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.27; acc: 0.69
Batch: 40; loss: 1.43; acc: 0.55
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 1.31; acc: 0.64
Batch: 100; loss: 1.38; acc: 0.58
Batch: 120; loss: 1.35; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.56
Batch: 160; loss: 1.19; acc: 0.64
Batch: 180; loss: 1.42; acc: 0.59
Batch: 200; loss: 1.2; acc: 0.62
Batch: 220; loss: 1.18; acc: 0.59
Batch: 240; loss: 1.15; acc: 0.67
Batch: 260; loss: 1.19; acc: 0.7
Batch: 280; loss: 1.21; acc: 0.64
Batch: 300; loss: 1.44; acc: 0.44
Batch: 320; loss: 1.35; acc: 0.59
Batch: 340; loss: 1.2; acc: 0.7
Batch: 360; loss: 1.19; acc: 0.67
Batch: 380; loss: 1.24; acc: 0.61
Batch: 400; loss: 1.24; acc: 0.66
Batch: 420; loss: 1.31; acc: 0.62
Batch: 440; loss: 1.2; acc: 0.62
Batch: 460; loss: 1.19; acc: 0.66
Batch: 480; loss: 1.05; acc: 0.77
Batch: 500; loss: 1.36; acc: 0.53
Batch: 520; loss: 1.17; acc: 0.62
Batch: 540; loss: 1.14; acc: 0.7
Batch: 560; loss: 1.09; acc: 0.67
Batch: 580; loss: 1.09; acc: 0.66
Batch: 600; loss: 1.17; acc: 0.59
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 1.13; acc: 0.7
Batch: 660; loss: 1.1; acc: 0.72
Batch: 680; loss: 1.16; acc: 0.66
Batch: 700; loss: 1.11; acc: 0.72
Batch: 720; loss: 1.3; acc: 0.59
Batch: 740; loss: 1.04; acc: 0.72
Batch: 760; loss: 1.25; acc: 0.61
Batch: 780; loss: 1.27; acc: 0.64
Train Epoch over. train_loss: 1.23; train_accuracy: 0.63 

0.00020441268861759454
0.00019702636927831918
Batch: 0; loss: 1.21; acc: 0.59
Batch: 20; loss: 1.46; acc: 0.47
Batch: 40; loss: 1.0; acc: 0.72
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.17; acc: 0.64
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 1.05; acc: 0.69
Val Epoch over. val_loss: 1.1988817464773822; val_accuracy: 0.6501791401273885 

The current subspace-distance is: 0.00019702636927831918 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_7_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.6540064054180426

The number of parameters is: 270776

The number of individual parameters is:

22
396
22
22
32
45760
32
32
64
133120
64
64
64
86016
64
64
4096
64
640
10
64
64

nonzero elements in E: 54155196
elements in E: 54155200
fraction nonzero: 0.9999999261382102
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.48; acc: 0.06
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.16; acc: 0.12
Batch: 60; loss: 2.09; acc: 0.22
Batch: 80; loss: 1.94; acc: 0.34
Batch: 100; loss: 1.92; acc: 0.48
Batch: 120; loss: 1.84; acc: 0.44
Batch: 140; loss: 1.93; acc: 0.47
Batch: 160; loss: 1.78; acc: 0.52
Batch: 180; loss: 1.66; acc: 0.61
Batch: 200; loss: 1.7; acc: 0.59
Batch: 220; loss: 1.82; acc: 0.47
Batch: 240; loss: 1.64; acc: 0.56
Batch: 260; loss: 1.71; acc: 0.48
Batch: 280; loss: 1.63; acc: 0.59
Batch: 300; loss: 1.6; acc: 0.62
Batch: 320; loss: 1.57; acc: 0.56
Batch: 340; loss: 1.51; acc: 0.73
Batch: 360; loss: 1.54; acc: 0.72
Batch: 380; loss: 1.53; acc: 0.72
Batch: 400; loss: 1.61; acc: 0.64
Batch: 420; loss: 1.53; acc: 0.62
Batch: 440; loss: 1.52; acc: 0.67
Batch: 460; loss: 1.6; acc: 0.62
Batch: 480; loss: 1.47; acc: 0.7
Batch: 500; loss: 1.43; acc: 0.7
Batch: 520; loss: 1.66; acc: 0.55
Batch: 540; loss: 1.54; acc: 0.62
Batch: 560; loss: 1.47; acc: 0.67
Batch: 580; loss: 1.57; acc: 0.64
Batch: 600; loss: 1.5; acc: 0.62
Batch: 620; loss: 1.45; acc: 0.72
Batch: 640; loss: 1.47; acc: 0.67
Batch: 660; loss: 1.46; acc: 0.7
Batch: 680; loss: 1.56; acc: 0.64
Batch: 700; loss: 1.52; acc: 0.64
Batch: 720; loss: 1.46; acc: 0.69
Batch: 740; loss: 1.32; acc: 0.73
Batch: 760; loss: 1.42; acc: 0.69
Batch: 780; loss: 1.47; acc: 0.67
Train Epoch over. train_loss: 1.66; train_accuracy: 0.57 

5.919668546994217e-05
5.40841901965905e-05
Batch: 0; loss: 1.56; acc: 0.52
Batch: 20; loss: 1.54; acc: 0.61
Batch: 40; loss: 1.16; acc: 0.83
Batch: 60; loss: 1.34; acc: 0.73
Batch: 80; loss: 1.26; acc: 0.72
Batch: 100; loss: 1.47; acc: 0.73
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.72
Val Epoch over. val_loss: 1.413309542236814; val_accuracy: 0.6879976114649682 

The current subspace-distance is: 5.40841901965905e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.6; acc: 0.58
Batch: 20; loss: 1.33; acc: 0.8
Batch: 40; loss: 1.32; acc: 0.8
Batch: 60; loss: 1.49; acc: 0.59
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.41; acc: 0.67
Batch: 120; loss: 1.35; acc: 0.75
Batch: 140; loss: 1.46; acc: 0.55
Batch: 160; loss: 1.49; acc: 0.62
Batch: 180; loss: 1.38; acc: 0.73
Batch: 200; loss: 1.32; acc: 0.66
Batch: 220; loss: 1.32; acc: 0.72
Batch: 240; loss: 1.26; acc: 0.78
Batch: 260; loss: 1.4; acc: 0.64
Batch: 280; loss: 1.57; acc: 0.59
Batch: 300; loss: 1.3; acc: 0.69
Batch: 320; loss: 1.37; acc: 0.66
Batch: 340; loss: 1.3; acc: 0.77
Batch: 360; loss: 1.32; acc: 0.75
Batch: 380; loss: 1.3; acc: 0.77
Batch: 400; loss: 1.32; acc: 0.73
Batch: 420; loss: 1.35; acc: 0.64
Batch: 440; loss: 1.23; acc: 0.77
Batch: 460; loss: 1.35; acc: 0.67
Batch: 480; loss: 1.42; acc: 0.67
Batch: 500; loss: 1.38; acc: 0.61
Batch: 520; loss: 1.32; acc: 0.72
Batch: 540; loss: 1.36; acc: 0.62
Batch: 560; loss: 1.15; acc: 0.88
Batch: 580; loss: 1.26; acc: 0.78
Batch: 600; loss: 1.32; acc: 0.77
Batch: 620; loss: 1.28; acc: 0.78
Batch: 640; loss: 1.26; acc: 0.67
Batch: 660; loss: 1.27; acc: 0.73
Batch: 680; loss: 1.37; acc: 0.73
Batch: 700; loss: 1.28; acc: 0.73
Batch: 720; loss: 1.22; acc: 0.77
Batch: 740; loss: 1.12; acc: 0.81
Batch: 760; loss: 1.2; acc: 0.69
Batch: 780; loss: 1.24; acc: 0.7
Train Epoch over. train_loss: 1.35; train_accuracy: 0.7 

7.812307740096003e-05
7.386882498394698e-05
Batch: 0; loss: 1.37; acc: 0.59
Batch: 20; loss: 1.31; acc: 0.69
Batch: 40; loss: 0.97; acc: 0.88
Batch: 60; loss: 1.11; acc: 0.81
Batch: 80; loss: 1.04; acc: 0.84
Batch: 100; loss: 1.25; acc: 0.75
Batch: 120; loss: 1.39; acc: 0.61
Batch: 140; loss: 1.05; acc: 0.78
Val Epoch over. val_loss: 1.2160978993033147; val_accuracy: 0.7440286624203821 

The current subspace-distance is: 7.386882498394698e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.44; acc: 0.67
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.29; acc: 0.7
Batch: 80; loss: 1.26; acc: 0.7
Batch: 100; loss: 1.3; acc: 0.69
Batch: 120; loss: 1.12; acc: 0.78
Batch: 140; loss: 1.2; acc: 0.67
Batch: 160; loss: 1.19; acc: 0.73
Batch: 180; loss: 1.43; acc: 0.66
Batch: 200; loss: 1.2; acc: 0.73
Batch: 220; loss: 1.12; acc: 0.81
Batch: 240; loss: 1.09; acc: 0.81
Batch: 260; loss: 1.29; acc: 0.73
Batch: 280; loss: 1.16; acc: 0.77
Batch: 300; loss: 1.23; acc: 0.72
Batch: 320; loss: 1.1; acc: 0.84
Batch: 340; loss: 1.08; acc: 0.83
Batch: 360; loss: 1.25; acc: 0.69
Batch: 380; loss: 1.15; acc: 0.8
Batch: 400; loss: 1.28; acc: 0.72
Batch: 420; loss: 1.14; acc: 0.75
Batch: 440; loss: 1.19; acc: 0.75
Batch: 460; loss: 1.23; acc: 0.73
Batch: 480; loss: 1.24; acc: 0.73
Batch: 500; loss: 1.17; acc: 0.77
Batch: 520; loss: 1.27; acc: 0.66
Batch: 540; loss: 1.17; acc: 0.72
Batch: 560; loss: 1.21; acc: 0.69
Batch: 580; loss: 1.2; acc: 0.77
Batch: 600; loss: 1.05; acc: 0.8
Batch: 620; loss: 1.1; acc: 0.86
Batch: 640; loss: 1.27; acc: 0.69
Batch: 660; loss: 1.16; acc: 0.75
Batch: 680; loss: 1.12; acc: 0.75
Batch: 700; loss: 1.24; acc: 0.7
Batch: 720; loss: 1.16; acc: 0.77
Batch: 740; loss: 1.13; acc: 0.8
Batch: 760; loss: 0.99; acc: 0.89
Batch: 780; loss: 1.2; acc: 0.73
Train Epoch over. train_loss: 1.2; train_accuracy: 0.74 

9.657539339968935e-05
9.257716010324657e-05
Batch: 0; loss: 1.25; acc: 0.7
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 0.87; acc: 0.83
Batch: 60; loss: 1.03; acc: 0.81
Batch: 80; loss: 0.96; acc: 0.88
Batch: 100; loss: 1.11; acc: 0.84
Batch: 120; loss: 1.23; acc: 0.72
Batch: 140; loss: 0.92; acc: 0.84
Val Epoch over. val_loss: 1.099456942005522; val_accuracy: 0.7794585987261147 

The current subspace-distance is: 9.257716010324657e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.81
Batch: 20; loss: 1.02; acc: 0.83
Batch: 40; loss: 1.02; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.77
Batch: 80; loss: 1.33; acc: 0.64
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.3; acc: 0.7
Batch: 140; loss: 1.07; acc: 0.78
Batch: 160; loss: 1.19; acc: 0.75
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 1.06; acc: 0.81
Batch: 220; loss: 1.41; acc: 0.53
Batch: 240; loss: 1.13; acc: 0.72
Batch: 260; loss: 1.03; acc: 0.86
Batch: 280; loss: 1.03; acc: 0.86
Batch: 300; loss: 1.07; acc: 0.81
Batch: 320; loss: 1.04; acc: 0.8
Batch: 340; loss: 1.19; acc: 0.73
Batch: 360; loss: 1.25; acc: 0.69
Batch: 380; loss: 0.98; acc: 0.78
Batch: 400; loss: 1.07; acc: 0.77
Batch: 420; loss: 1.08; acc: 0.8
Batch: 440; loss: 1.04; acc: 0.77
Batch: 460; loss: 1.09; acc: 0.69
Batch: 480; loss: 1.16; acc: 0.72
Batch: 500; loss: 1.07; acc: 0.77
Batch: 520; loss: 1.15; acc: 0.8
Batch: 540; loss: 1.1; acc: 0.78
Batch: 560; loss: 1.04; acc: 0.78
Batch: 580; loss: 1.05; acc: 0.83
Batch: 600; loss: 1.0; acc: 0.8
Batch: 620; loss: 1.18; acc: 0.78
Batch: 640; loss: 1.07; acc: 0.75
Batch: 660; loss: 1.14; acc: 0.75
Batch: 680; loss: 0.98; acc: 0.83
Batch: 700; loss: 1.14; acc: 0.7
Batch: 720; loss: 1.18; acc: 0.73
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.78
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.1; train_accuracy: 0.76 

0.00010899683547904715
0.0001035738387145102
Batch: 0; loss: 1.16; acc: 0.73
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 0.81; acc: 0.89
Batch: 60; loss: 1.01; acc: 0.8
Batch: 80; loss: 0.92; acc: 0.86
Batch: 100; loss: 1.02; acc: 0.84
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.86
Val Epoch over. val_loss: 1.0354262689116653; val_accuracy: 0.7955812101910829 

The current subspace-distance is: 0.0001035738387145102 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.03; acc: 0.81
Batch: 40; loss: 0.99; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.81
Batch: 80; loss: 1.0; acc: 0.86
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 0.95; acc: 0.83
Batch: 140; loss: 1.01; acc: 0.78
Batch: 160; loss: 1.02; acc: 0.83
Batch: 180; loss: 0.92; acc: 0.88
Batch: 200; loss: 0.92; acc: 0.84
Batch: 220; loss: 1.28; acc: 0.61
Batch: 240; loss: 1.1; acc: 0.81
Batch: 260; loss: 1.05; acc: 0.75
Batch: 280; loss: 1.16; acc: 0.78
Batch: 300; loss: 0.9; acc: 0.83
Batch: 320; loss: 1.15; acc: 0.75
Batch: 340; loss: 1.26; acc: 0.67
Batch: 360; loss: 1.05; acc: 0.73
Batch: 380; loss: 1.19; acc: 0.75
Batch: 400; loss: 0.94; acc: 0.88
Batch: 420; loss: 0.96; acc: 0.83
Batch: 440; loss: 1.09; acc: 0.73
Batch: 460; loss: 1.18; acc: 0.67
Batch: 480; loss: 0.92; acc: 0.84
Batch: 500; loss: 1.0; acc: 0.8
Batch: 520; loss: 1.05; acc: 0.8
Batch: 540; loss: 1.19; acc: 0.62
Batch: 560; loss: 1.09; acc: 0.77
Batch: 580; loss: 1.01; acc: 0.77
Batch: 600; loss: 0.98; acc: 0.77
Batch: 620; loss: 0.95; acc: 0.84
Batch: 640; loss: 0.92; acc: 0.89
Batch: 660; loss: 0.94; acc: 0.8
Batch: 680; loss: 1.11; acc: 0.73
Batch: 700; loss: 1.02; acc: 0.83
Batch: 720; loss: 0.96; acc: 0.8
Batch: 740; loss: 1.06; acc: 0.78
Batch: 760; loss: 0.97; acc: 0.81
Batch: 780; loss: 1.04; acc: 0.77
Train Epoch over. train_loss: 1.04; train_accuracy: 0.78 

0.00012400367995724082
0.00011714209540514275
Batch: 0; loss: 1.07; acc: 0.73
Batch: 20; loss: 1.09; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.89
Batch: 60; loss: 0.95; acc: 0.8
Batch: 80; loss: 0.86; acc: 0.81
Batch: 100; loss: 0.91; acc: 0.86
Batch: 120; loss: 1.11; acc: 0.72
Batch: 140; loss: 0.74; acc: 0.91
Val Epoch over. val_loss: 0.9607888065325986; val_accuracy: 0.8110071656050956 

The current subspace-distance is: 0.00011714209540514275 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.83
Batch: 20; loss: 0.88; acc: 0.84
Batch: 40; loss: 1.09; acc: 0.72
Batch: 60; loss: 0.9; acc: 0.83
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 0.92; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.91
Batch: 140; loss: 0.92; acc: 0.83
Batch: 160; loss: 0.97; acc: 0.78
Batch: 180; loss: 0.91; acc: 0.83
Batch: 200; loss: 1.02; acc: 0.73
Batch: 220; loss: 0.91; acc: 0.86
Batch: 240; loss: 0.96; acc: 0.8
Batch: 260; loss: 0.93; acc: 0.83
Batch: 280; loss: 1.06; acc: 0.78
Batch: 300; loss: 0.94; acc: 0.81
Batch: 320; loss: 0.98; acc: 0.8
Batch: 340; loss: 1.03; acc: 0.81
Batch: 360; loss: 0.97; acc: 0.8
Batch: 380; loss: 1.03; acc: 0.78
Batch: 400; loss: 0.96; acc: 0.8
Batch: 420; loss: 1.04; acc: 0.78
Batch: 440; loss: 0.98; acc: 0.84
Batch: 460; loss: 0.84; acc: 0.86
Batch: 480; loss: 1.03; acc: 0.78
Batch: 500; loss: 1.04; acc: 0.77
Batch: 520; loss: 0.86; acc: 0.89
Batch: 540; loss: 0.98; acc: 0.78
Batch: 560; loss: 1.01; acc: 0.75
Batch: 580; loss: 1.04; acc: 0.8
Batch: 600; loss: 1.01; acc: 0.78
Batch: 620; loss: 0.96; acc: 0.81
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.92; acc: 0.86
Batch: 680; loss: 0.81; acc: 0.84
Batch: 700; loss: 1.01; acc: 0.78
Batch: 720; loss: 0.94; acc: 0.81
Batch: 740; loss: 1.01; acc: 0.73
Batch: 760; loss: 1.02; acc: 0.78
Batch: 780; loss: 0.96; acc: 0.75
Train Epoch over. train_loss: 0.98; train_accuracy: 0.79 

0.00013611734902951866
0.00013056154421065003
Batch: 0; loss: 0.97; acc: 0.81
Batch: 20; loss: 1.04; acc: 0.72
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.88; acc: 0.81
Batch: 80; loss: 0.81; acc: 0.84
Batch: 100; loss: 0.81; acc: 0.88
Batch: 120; loss: 1.04; acc: 0.73
Batch: 140; loss: 0.67; acc: 0.88
Val Epoch over. val_loss: 0.8900107813488906; val_accuracy: 0.8265326433121019 

The current subspace-distance is: 0.00013056154421065003 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.78
Batch: 20; loss: 0.86; acc: 0.89
Batch: 40; loss: 1.08; acc: 0.72
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 0.89; acc: 0.83
Batch: 100; loss: 0.93; acc: 0.77
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 0.84; acc: 0.81
Batch: 160; loss: 0.92; acc: 0.83
Batch: 180; loss: 0.97; acc: 0.83
Batch: 200; loss: 0.95; acc: 0.8
Batch: 220; loss: 0.98; acc: 0.8
Batch: 240; loss: 0.93; acc: 0.77
Batch: 260; loss: 1.02; acc: 0.73
Batch: 280; loss: 0.93; acc: 0.8
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 0.85; acc: 0.84
Batch: 340; loss: 0.93; acc: 0.77
Batch: 360; loss: 0.93; acc: 0.8
Batch: 380; loss: 0.87; acc: 0.83
Batch: 400; loss: 0.83; acc: 0.86
Batch: 420; loss: 0.97; acc: 0.77
Batch: 440; loss: 0.81; acc: 0.84
Batch: 460; loss: 0.93; acc: 0.8
Batch: 480; loss: 0.85; acc: 0.88
Batch: 500; loss: 0.97; acc: 0.8
Batch: 520; loss: 0.97; acc: 0.78
Batch: 540; loss: 1.06; acc: 0.75
Batch: 560; loss: 0.84; acc: 0.81
Batch: 580; loss: 0.98; acc: 0.75
Batch: 600; loss: 0.78; acc: 0.91
Batch: 620; loss: 0.97; acc: 0.7
Batch: 640; loss: 0.84; acc: 0.89
Batch: 660; loss: 0.77; acc: 0.84
Batch: 680; loss: 0.87; acc: 0.8
Batch: 700; loss: 0.9; acc: 0.84
Batch: 720; loss: 0.96; acc: 0.73
Batch: 740; loss: 0.94; acc: 0.77
Batch: 760; loss: 0.8; acc: 0.84
Batch: 780; loss: 0.98; acc: 0.78
Train Epoch over. train_loss: 0.93; train_accuracy: 0.8 

0.0001462004438508302
0.0001410104741808027
Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.73
Batch: 40; loss: 0.62; acc: 0.92
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.77; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.92
Batch: 120; loss: 1.03; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.92
Val Epoch over. val_loss: 0.8552024933942564; val_accuracy: 0.8296178343949044 

The current subspace-distance is: 0.0001410104741808027 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 0.9; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.83; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.88
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 1.14; acc: 0.69
Batch: 160; loss: 0.94; acc: 0.75
Batch: 180; loss: 0.77; acc: 0.86
Batch: 200; loss: 0.87; acc: 0.78
Batch: 220; loss: 0.77; acc: 0.89
Batch: 240; loss: 0.9; acc: 0.78
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.74; acc: 0.92
Batch: 300; loss: 1.06; acc: 0.72
Batch: 320; loss: 1.02; acc: 0.73
Batch: 340; loss: 0.88; acc: 0.78
Batch: 360; loss: 0.9; acc: 0.8
Batch: 380; loss: 0.8; acc: 0.86
Batch: 400; loss: 0.92; acc: 0.84
Batch: 420; loss: 0.97; acc: 0.78
Batch: 440; loss: 0.9; acc: 0.8
Batch: 460; loss: 0.76; acc: 0.89
Batch: 480; loss: 0.9; acc: 0.78
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 0.89; acc: 0.83
Batch: 540; loss: 0.97; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.85; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.81
Batch: 640; loss: 0.82; acc: 0.84
Batch: 660; loss: 0.88; acc: 0.83
Batch: 680; loss: 0.92; acc: 0.77
Batch: 700; loss: 0.87; acc: 0.84
Batch: 720; loss: 0.74; acc: 0.86
Batch: 740; loss: 1.03; acc: 0.72
Batch: 760; loss: 0.84; acc: 0.84
Batch: 780; loss: 0.97; acc: 0.83
Train Epoch over. train_loss: 0.89; train_accuracy: 0.81 

0.00015699205687269568
0.00015380856348201632
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 1.0; acc: 0.7
Batch: 40; loss: 0.58; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.81
Batch: 80; loss: 0.74; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.91
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 0.65; acc: 0.89
Val Epoch over. val_loss: 0.8271810294716222; val_accuracy: 0.8321058917197452 

The current subspace-distance is: 0.00015380856348201632 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.85; acc: 0.81
Batch: 60; loss: 0.89; acc: 0.8
Batch: 80; loss: 0.88; acc: 0.8
Batch: 100; loss: 0.83; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 1.03; acc: 0.7
Batch: 180; loss: 0.81; acc: 0.88
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.88; acc: 0.77
Batch: 240; loss: 0.8; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.84
Batch: 280; loss: 0.9; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.91
Batch: 320; loss: 0.94; acc: 0.8
Batch: 340; loss: 0.88; acc: 0.83
Batch: 360; loss: 0.86; acc: 0.8
Batch: 380; loss: 0.93; acc: 0.75
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.95; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.88
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.8; acc: 0.83
Batch: 500; loss: 1.03; acc: 0.8
Batch: 520; loss: 0.81; acc: 0.88
Batch: 540; loss: 0.93; acc: 0.8
Batch: 560; loss: 0.94; acc: 0.78
Batch: 580; loss: 0.79; acc: 0.88
Batch: 600; loss: 0.84; acc: 0.84
Batch: 620; loss: 0.78; acc: 0.84
Batch: 640; loss: 0.75; acc: 0.83
Batch: 660; loss: 0.98; acc: 0.69
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.88; acc: 0.77
Batch: 720; loss: 0.69; acc: 0.91
Batch: 740; loss: 1.04; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.75
Batch: 780; loss: 0.69; acc: 0.88
Train Epoch over. train_loss: 0.86; train_accuracy: 0.81 

0.00016584507829975337
0.00015901034930720925
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 1.01; acc: 0.72
Batch: 40; loss: 0.54; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.94
Batch: 120; loss: 1.0; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.91
Val Epoch over. val_loss: 0.793712273524825; val_accuracy: 0.8354896496815286 

The current subspace-distance is: 0.00015901034930720925 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 0.91; acc: 0.8
Batch: 80; loss: 0.78; acc: 0.84
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.83
Batch: 140; loss: 0.87; acc: 0.77
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.83
Batch: 200; loss: 0.85; acc: 0.83
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.89; acc: 0.77
Batch: 260; loss: 0.82; acc: 0.84
Batch: 280; loss: 0.95; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.78
Batch: 320; loss: 0.83; acc: 0.84
Batch: 340; loss: 0.85; acc: 0.78
Batch: 360; loss: 0.79; acc: 0.86
Batch: 380; loss: 0.81; acc: 0.81
Batch: 400; loss: 0.99; acc: 0.8
Batch: 420; loss: 0.87; acc: 0.83
Batch: 440; loss: 0.82; acc: 0.8
Batch: 460; loss: 0.76; acc: 0.8
Batch: 480; loss: 0.84; acc: 0.83
Batch: 500; loss: 0.74; acc: 0.86
Batch: 520; loss: 0.75; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.78
Batch: 560; loss: 0.93; acc: 0.73
Batch: 580; loss: 0.78; acc: 0.86
Batch: 600; loss: 0.83; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.84
Batch: 640; loss: 0.77; acc: 0.86
Batch: 660; loss: 0.75; acc: 0.89
Batch: 680; loss: 0.79; acc: 0.81
Batch: 700; loss: 0.89; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.84
Batch: 760; loss: 0.98; acc: 0.72
Batch: 780; loss: 0.76; acc: 0.84
Train Epoch over. train_loss: 0.84; train_accuracy: 0.81 

0.00017400701472070068
0.0001691258657956496
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.96; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.92
Batch: 120; loss: 0.99; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.88
Val Epoch over. val_loss: 0.7596863531003333; val_accuracy: 0.8378781847133758 

The current subspace-distance is: 0.0001691258657956496 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.77; acc: 0.88
Batch: 60; loss: 0.81; acc: 0.86
Batch: 80; loss: 1.0; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.73
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.88; acc: 0.8
Batch: 160; loss: 0.83; acc: 0.81
Batch: 180; loss: 0.83; acc: 0.81
Batch: 200; loss: 0.69; acc: 0.89
Batch: 220; loss: 0.81; acc: 0.84
Batch: 240; loss: 0.76; acc: 0.86
Batch: 260; loss: 0.86; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.84
Batch: 300; loss: 0.87; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.92
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.79; acc: 0.84
Batch: 380; loss: 0.81; acc: 0.8
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.84; acc: 0.75
Batch: 440; loss: 0.66; acc: 0.92
Batch: 460; loss: 0.83; acc: 0.81
Batch: 480; loss: 0.85; acc: 0.81
Batch: 500; loss: 0.73; acc: 0.86
Batch: 520; loss: 0.7; acc: 0.84
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.82; acc: 0.8
Batch: 580; loss: 0.67; acc: 0.89
Batch: 600; loss: 0.94; acc: 0.7
Batch: 620; loss: 0.85; acc: 0.8
Batch: 640; loss: 0.84; acc: 0.83
Batch: 660; loss: 0.84; acc: 0.81
Batch: 680; loss: 0.92; acc: 0.7
Batch: 700; loss: 0.84; acc: 0.83
Batch: 720; loss: 0.89; acc: 0.78
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.93; acc: 0.8
Batch: 780; loss: 0.81; acc: 0.81
Train Epoch over. train_loss: 0.82; train_accuracy: 0.81 

0.00017770749400369823
0.00017040273814927787
Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.5; acc: 0.95
Batch: 60; loss: 0.74; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.91
Batch: 120; loss: 0.99; acc: 0.77
Batch: 140; loss: 0.62; acc: 0.88
Val Epoch over. val_loss: 0.7584253247756108; val_accuracy: 0.837281050955414 

The current subspace-distance is: 0.00017040273814927787 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.77; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.89
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.81; acc: 0.8
Batch: 160; loss: 0.89; acc: 0.78
Batch: 180; loss: 0.79; acc: 0.86
Batch: 200; loss: 0.94; acc: 0.73
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.8
Batch: 260; loss: 0.84; acc: 0.86
Batch: 280; loss: 0.95; acc: 0.75
Batch: 300; loss: 0.86; acc: 0.83
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.79; acc: 0.8
Batch: 360; loss: 0.9; acc: 0.77
Batch: 380; loss: 0.77; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.92
Batch: 420; loss: 0.73; acc: 0.88
Batch: 440; loss: 0.66; acc: 0.89
Batch: 460; loss: 0.8; acc: 0.78
Batch: 480; loss: 0.87; acc: 0.78
Batch: 500; loss: 0.84; acc: 0.83
Batch: 520; loss: 0.72; acc: 0.91
Batch: 540; loss: 0.87; acc: 0.77
Batch: 560; loss: 0.91; acc: 0.73
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.73; acc: 0.86
Batch: 620; loss: 0.87; acc: 0.81
Batch: 640; loss: 0.97; acc: 0.72
Batch: 660; loss: 0.85; acc: 0.81
Batch: 680; loss: 0.83; acc: 0.78
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.89; acc: 0.84
Batch: 740; loss: 0.74; acc: 0.84
Batch: 760; loss: 0.86; acc: 0.83
Batch: 780; loss: 0.86; acc: 0.78
Train Epoch over. train_loss: 0.81; train_accuracy: 0.81 

0.00017995151574723423
0.00017278987797908485
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.5; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.92
Batch: 120; loss: 0.98; acc: 0.78
Batch: 140; loss: 0.61; acc: 0.89
Val Epoch over. val_loss: 0.7554938221813008; val_accuracy: 0.8368829617834395 

The current subspace-distance is: 0.00017278987797908485 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.8; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.88
Batch: 80; loss: 0.73; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 0.97; acc: 0.75
Batch: 160; loss: 0.91; acc: 0.81
Batch: 180; loss: 0.77; acc: 0.84
Batch: 200; loss: 0.8; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.78; acc: 0.84
Batch: 280; loss: 0.86; acc: 0.78
Batch: 300; loss: 0.77; acc: 0.88
Batch: 320; loss: 0.79; acc: 0.83
Batch: 340; loss: 0.7; acc: 0.81
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.82; acc: 0.83
Batch: 420; loss: 0.56; acc: 0.94
Batch: 440; loss: 0.79; acc: 0.91
Batch: 460; loss: 0.87; acc: 0.75
Batch: 480; loss: 0.67; acc: 0.88
Batch: 500; loss: 0.82; acc: 0.84
Batch: 520; loss: 0.83; acc: 0.77
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.99; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.88
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 0.85; acc: 0.78
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.79; acc: 0.86
Batch: 680; loss: 0.8; acc: 0.84
Batch: 700; loss: 0.88; acc: 0.77
Batch: 720; loss: 0.92; acc: 0.77
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.82; acc: 0.81
Batch: 780; loss: 0.92; acc: 0.78
Train Epoch over. train_loss: 0.81; train_accuracy: 0.82 

0.00018154949066229165
0.00017483820556662977
Batch: 0; loss: 0.77; acc: 0.86
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.88
Val Epoch over. val_loss: 0.7535250684258284; val_accuracy: 0.8365843949044586 

The current subspace-distance is: 0.00017483820556662977 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.94; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.84; acc: 0.77
Batch: 100; loss: 0.72; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.87; acc: 0.78
Batch: 160; loss: 0.83; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.82; acc: 0.81
Batch: 220; loss: 0.89; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.89
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.86; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.73
Batch: 340; loss: 0.79; acc: 0.75
Batch: 360; loss: 0.67; acc: 0.91
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.8; acc: 0.75
Batch: 420; loss: 0.77; acc: 0.86
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.88; acc: 0.78
Batch: 480; loss: 0.8; acc: 0.81
Batch: 500; loss: 0.68; acc: 0.89
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.89
Batch: 560; loss: 0.84; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.75
Batch: 600; loss: 0.94; acc: 0.81
Batch: 620; loss: 0.86; acc: 0.8
Batch: 640; loss: 0.74; acc: 0.83
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.87; acc: 0.8
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.8; train_accuracy: 0.82 

0.0001842253259383142
0.00017660841695033014
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.7354897034775679; val_accuracy: 0.8407643312101911 

The current subspace-distance is: 0.00017660841695033014 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.86
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 0.66; acc: 0.91
Batch: 80; loss: 0.86; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.88
Batch: 140; loss: 0.67; acc: 0.86
Batch: 160; loss: 0.82; acc: 0.81
Batch: 180; loss: 0.74; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.89
Batch: 220; loss: 0.78; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.88
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.91
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.73; acc: 0.86
Batch: 340; loss: 0.75; acc: 0.83
Batch: 360; loss: 0.68; acc: 0.91
Batch: 380; loss: 0.85; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.91
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.92
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 0.7; acc: 0.88
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.83; acc: 0.78
Batch: 580; loss: 0.6; acc: 0.95
Batch: 600; loss: 0.84; acc: 0.77
Batch: 620; loss: 0.81; acc: 0.81
Batch: 640; loss: 0.77; acc: 0.86
Batch: 660; loss: 0.74; acc: 0.78
Batch: 680; loss: 0.85; acc: 0.83
Batch: 700; loss: 0.65; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.83; acc: 0.81
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.79; train_accuracy: 0.82 

0.00018672899750526994
0.00017958151875063777
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.92
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 0.6; acc: 0.88
Val Epoch over. val_loss: 0.7352721493714934; val_accuracy: 0.8407643312101911 

The current subspace-distance is: 0.00017958151875063777 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.86; acc: 0.8
Batch: 20; loss: 0.76; acc: 0.83
Batch: 40; loss: 0.63; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.88; acc: 0.75
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.89
Batch: 160; loss: 1.04; acc: 0.67
Batch: 180; loss: 0.83; acc: 0.83
Batch: 200; loss: 0.88; acc: 0.77
Batch: 220; loss: 0.79; acc: 0.86
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.8
Batch: 280; loss: 0.97; acc: 0.73
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.73; acc: 0.89
Batch: 340; loss: 0.85; acc: 0.78
Batch: 360; loss: 0.89; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.86
Batch: 400; loss: 0.76; acc: 0.83
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.97; acc: 0.75
Batch: 460; loss: 0.66; acc: 0.92
Batch: 480; loss: 0.8; acc: 0.84
Batch: 500; loss: 0.82; acc: 0.86
Batch: 520; loss: 0.69; acc: 0.83
Batch: 540; loss: 0.81; acc: 0.83
Batch: 560; loss: 0.88; acc: 0.75
Batch: 580; loss: 0.75; acc: 0.86
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.8; acc: 0.8
Batch: 640; loss: 0.61; acc: 0.89
Batch: 660; loss: 0.64; acc: 0.89
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.8; acc: 0.75
Batch: 720; loss: 0.69; acc: 0.89
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.69; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.79; train_accuracy: 0.82 

0.00018894700042437762
0.00018173536227550358
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.92
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.7300675241810501; val_accuracy: 0.8376791401273885 

The current subspace-distance is: 0.00018173536227550358 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.85; acc: 0.81
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.86; acc: 0.73
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.85; acc: 0.8
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.84; acc: 0.8
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.85; acc: 0.81
Batch: 260; loss: 0.68; acc: 0.88
Batch: 280; loss: 0.78; acc: 0.81
Batch: 300; loss: 0.78; acc: 0.83
Batch: 320; loss: 0.8; acc: 0.88
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.81
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.65; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.92
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 0.72; acc: 0.86
Batch: 500; loss: 0.82; acc: 0.8
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.83
Batch: 580; loss: 0.79; acc: 0.77
Batch: 600; loss: 0.68; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.84
Batch: 640; loss: 0.83; acc: 0.81
Batch: 660; loss: 0.63; acc: 0.91
Batch: 680; loss: 0.81; acc: 0.8
Batch: 700; loss: 0.96; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.91
Batch: 740; loss: 0.73; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.86
Batch: 780; loss: 0.63; acc: 0.91
Train Epoch over. train_loss: 0.78; train_accuracy: 0.82 

0.00019251265621278435
0.00018499561701901257
Batch: 0; loss: 0.73; acc: 0.86
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.89
Val Epoch over. val_loss: 0.7208428965632323; val_accuracy: 0.8416600318471338 

The current subspace-distance is: 0.00018499561701901257 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.92; acc: 0.73
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.76; acc: 0.89
Batch: 160; loss: 0.78; acc: 0.77
Batch: 180; loss: 0.75; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.84
Batch: 240; loss: 0.91; acc: 0.8
Batch: 260; loss: 0.86; acc: 0.8
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.79; acc: 0.83
Batch: 320; loss: 0.84; acc: 0.78
Batch: 340; loss: 0.75; acc: 0.8
Batch: 360; loss: 0.86; acc: 0.81
Batch: 380; loss: 0.84; acc: 0.8
Batch: 400; loss: 0.82; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.84
Batch: 500; loss: 0.8; acc: 0.81
Batch: 520; loss: 0.83; acc: 0.8
Batch: 540; loss: 0.71; acc: 0.89
Batch: 560; loss: 0.76; acc: 0.84
Batch: 580; loss: 0.89; acc: 0.8
Batch: 600; loss: 0.88; acc: 0.78
Batch: 620; loss: 0.84; acc: 0.72
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 0.62; acc: 0.92
Batch: 700; loss: 0.8; acc: 0.81
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.79; acc: 0.78
Batch: 780; loss: 0.89; acc: 0.77
Train Epoch over. train_loss: 0.77; train_accuracy: 0.82 

0.0001938783680088818
0.0001859904295997694
Batch: 0; loss: 0.72; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.7
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.92
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.91
Val Epoch over. val_loss: 0.7138398071383215; val_accuracy: 0.8440485668789809 

The current subspace-distance is: 0.0001859904295997694 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.87; acc: 0.81
Batch: 220; loss: 0.86; acc: 0.8
Batch: 240; loss: 0.9; acc: 0.73
Batch: 260; loss: 0.85; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.88
Batch: 300; loss: 0.75; acc: 0.81
Batch: 320; loss: 0.78; acc: 0.83
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.75; acc: 0.75
Batch: 400; loss: 0.82; acc: 0.77
Batch: 420; loss: 0.79; acc: 0.83
Batch: 440; loss: 0.97; acc: 0.77
Batch: 460; loss: 0.72; acc: 0.88
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.75; acc: 0.88
Batch: 520; loss: 0.69; acc: 0.89
Batch: 540; loss: 0.81; acc: 0.83
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.62; acc: 0.92
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.76; acc: 0.83
Batch: 640; loss: 0.68; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.91
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.73; acc: 0.86
Batch: 720; loss: 0.57; acc: 0.92
Batch: 740; loss: 0.77; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.89
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.82 

0.0001940093789016828
0.00018828539759851992
Batch: 0; loss: 0.73; acc: 0.89
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.45; acc: 0.95
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.56; acc: 0.91
Val Epoch over. val_loss: 0.7174167166090315; val_accuracy: 0.8454418789808917 

The current subspace-distance is: 0.00018828539759851992 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.91
Batch: 80; loss: 0.83; acc: 0.8
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.77; acc: 0.83
Batch: 220; loss: 0.67; acc: 0.91
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.89
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.72; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.84; acc: 0.81
Batch: 400; loss: 0.8; acc: 0.8
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.7; acc: 0.86
Batch: 460; loss: 0.8; acc: 0.81
Batch: 480; loss: 0.87; acc: 0.75
Batch: 500; loss: 0.87; acc: 0.73
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.62; acc: 0.92
Batch: 560; loss: 0.88; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.83; acc: 0.78
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.68; acc: 0.83
Batch: 660; loss: 0.85; acc: 0.78
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.87; acc: 0.77
Batch: 720; loss: 0.81; acc: 0.8
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.62; acc: 0.91
Batch: 780; loss: 0.8; acc: 0.78
Train Epoch over. train_loss: 0.76; train_accuracy: 0.82 

0.0001982156973099336
0.00019087741384282708
Batch: 0; loss: 0.72; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.7
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.91
Val Epoch over. val_loss: 0.7072996948934664; val_accuracy: 0.8429538216560509 

The current subspace-distance is: 0.00019087741384282708 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 0.82; acc: 0.84
Batch: 100; loss: 0.96; acc: 0.72
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.91
Batch: 160; loss: 0.81; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.88
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.82; acc: 0.77
Batch: 240; loss: 0.84; acc: 0.72
Batch: 260; loss: 0.72; acc: 0.88
Batch: 280; loss: 0.77; acc: 0.83
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.78; acc: 0.78
Batch: 380; loss: 0.66; acc: 0.88
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 0.73; acc: 0.84
Batch: 440; loss: 0.73; acc: 0.83
Batch: 460; loss: 0.71; acc: 0.89
Batch: 480; loss: 0.84; acc: 0.8
Batch: 500; loss: 0.68; acc: 0.86
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.68; acc: 0.88
Batch: 560; loss: 0.78; acc: 0.81
Batch: 580; loss: 0.79; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.81
Batch: 620; loss: 0.92; acc: 0.73
Batch: 640; loss: 0.72; acc: 0.86
Batch: 660; loss: 0.78; acc: 0.83
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.76; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.88
Batch: 780; loss: 0.85; acc: 0.75
Train Epoch over. train_loss: 0.76; train_accuracy: 0.82 

0.0001993418700294569
0.00019054072618018836
Batch: 0; loss: 0.72; acc: 0.88
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.55; acc: 0.91
Val Epoch over. val_loss: 0.7048195936497609; val_accuracy: 0.8439490445859873 

The current subspace-distance is: 0.00019054072618018836 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.8
Batch: 140; loss: 0.9; acc: 0.8
Batch: 160; loss: 0.72; acc: 0.78
Batch: 180; loss: 0.79; acc: 0.78
Batch: 200; loss: 0.82; acc: 0.84
Batch: 220; loss: 0.94; acc: 0.73
Batch: 240; loss: 0.74; acc: 0.83
Batch: 260; loss: 0.7; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.78; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.81
Batch: 340; loss: 0.77; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.89
Batch: 380; loss: 0.73; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.87; acc: 0.78
Batch: 460; loss: 0.79; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.88; acc: 0.81
Batch: 520; loss: 0.84; acc: 0.78
Batch: 540; loss: 0.86; acc: 0.73
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.77; acc: 0.84
Batch: 600; loss: 0.75; acc: 0.84
Batch: 620; loss: 0.74; acc: 0.83
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.77; acc: 0.83
Batch: 680; loss: 0.79; acc: 0.83
Batch: 700; loss: 0.91; acc: 0.7
Batch: 720; loss: 0.97; acc: 0.7
Batch: 740; loss: 0.83; acc: 0.8
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.91
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00019983276433777064
0.00019204930867999792
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.91
Val Epoch over. val_loss: 0.6968685350600322; val_accuracy: 0.8462380573248408 

The current subspace-distance is: 0.00019204930867999792 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.82; acc: 0.77
Batch: 240; loss: 0.9; acc: 0.8
Batch: 260; loss: 0.67; acc: 0.89
Batch: 280; loss: 0.92; acc: 0.7
Batch: 300; loss: 0.79; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.86
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.67; acc: 0.86
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.88
Batch: 420; loss: 0.91; acc: 0.75
Batch: 440; loss: 0.73; acc: 0.77
Batch: 460; loss: 0.64; acc: 0.94
Batch: 480; loss: 0.78; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.84
Batch: 520; loss: 0.81; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.88; acc: 0.8
Batch: 580; loss: 0.87; acc: 0.8
Batch: 600; loss: 0.77; acc: 0.81
Batch: 620; loss: 0.67; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.8; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.74; acc: 0.83
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00019710954802576452
0.00018955762789119035
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.69322346862714; val_accuracy: 0.8462380573248408 

The current subspace-distance is: 0.00018955762789119035 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.81; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.74; acc: 0.86
Batch: 160; loss: 0.77; acc: 0.83
Batch: 180; loss: 0.8; acc: 0.81
Batch: 200; loss: 0.7; acc: 0.86
Batch: 220; loss: 0.85; acc: 0.8
Batch: 240; loss: 0.69; acc: 0.88
Batch: 260; loss: 0.73; acc: 0.83
Batch: 280; loss: 0.92; acc: 0.78
Batch: 300; loss: 0.82; acc: 0.8
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.78; acc: 0.88
Batch: 360; loss: 0.9; acc: 0.73
Batch: 380; loss: 0.91; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.86; acc: 0.72
Batch: 460; loss: 0.85; acc: 0.75
Batch: 480; loss: 0.9; acc: 0.75
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.79; acc: 0.84
Batch: 540; loss: 0.78; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.77; acc: 0.8
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.86
Batch: 720; loss: 0.8; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.79; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.0002025587746175006
0.00019504321971908212
Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.692527551939533; val_accuracy: 0.8451433121019108 

The current subspace-distance is: 0.00019504321971908212 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.83
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.72; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.79; acc: 0.8
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.74; acc: 0.8
Batch: 320; loss: 0.8; acc: 0.81
Batch: 340; loss: 0.76; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.77; acc: 0.83
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.75
Batch: 460; loss: 0.76; acc: 0.84
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.72; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.81; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.82; acc: 0.81
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.68; acc: 0.86
Batch: 660; loss: 0.82; acc: 0.8
Batch: 680; loss: 0.78; acc: 0.84
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

0.00020326492085587233
0.00019470103143248707
Batch: 0; loss: 0.69; acc: 0.91
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6883679901718334; val_accuracy: 0.8484275477707006 

The current subspace-distance is: 0.00019470103143248707 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.68; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.81; acc: 0.78
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.79; acc: 0.77
Batch: 220; loss: 0.71; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.68; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.8; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.73; acc: 0.84
Batch: 400; loss: 0.78; acc: 0.81
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.83; acc: 0.73
Batch: 480; loss: 0.77; acc: 0.8
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.69; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.86
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.8; acc: 0.77
Batch: 640; loss: 0.66; acc: 0.88
Batch: 660; loss: 0.69; acc: 0.78
Batch: 680; loss: 0.66; acc: 0.78
Batch: 700; loss: 0.68; acc: 0.86
Batch: 720; loss: 0.9; acc: 0.77
Batch: 740; loss: 0.78; acc: 0.83
Batch: 760; loss: 0.88; acc: 0.73
Batch: 780; loss: 0.79; acc: 0.8
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.00020466599380597472
0.00019639408856164664
Batch: 0; loss: 0.7; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6922841514371763; val_accuracy: 0.8481289808917197 

The current subspace-distance is: 0.00019639408856164664 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.75; acc: 0.83
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.81; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.89
Batch: 160; loss: 0.83; acc: 0.72
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.76; acc: 0.88
Batch: 220; loss: 0.81; acc: 0.81
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.81
Batch: 280; loss: 0.76; acc: 0.84
Batch: 300; loss: 0.61; acc: 0.91
Batch: 320; loss: 0.87; acc: 0.81
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.82; acc: 0.78
Batch: 400; loss: 0.87; acc: 0.75
Batch: 420; loss: 0.73; acc: 0.83
Batch: 440; loss: 0.85; acc: 0.8
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.8; acc: 0.75
Batch: 500; loss: 0.75; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.94; acc: 0.8
Batch: 600; loss: 0.8; acc: 0.8
Batch: 620; loss: 0.76; acc: 0.81
Batch: 640; loss: 0.95; acc: 0.64
Batch: 660; loss: 0.69; acc: 0.88
Batch: 680; loss: 0.79; acc: 0.84
Batch: 700; loss: 0.74; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.83; acc: 0.78
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.0002033236378338188
0.00019772480300161988
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6850131290733434; val_accuracy: 0.848328025477707 

The current subspace-distance is: 0.00019772480300161988 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.79; acc: 0.86
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.97; acc: 0.78
Batch: 220; loss: 0.69; acc: 0.91
Batch: 240; loss: 0.83; acc: 0.78
Batch: 260; loss: 0.77; acc: 0.88
Batch: 280; loss: 0.74; acc: 0.86
Batch: 300; loss: 0.75; acc: 0.83
Batch: 320; loss: 0.8; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.92
Batch: 360; loss: 0.86; acc: 0.78
Batch: 380; loss: 0.68; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.89
Batch: 440; loss: 0.83; acc: 0.8
Batch: 460; loss: 0.84; acc: 0.81
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 0.79; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 0.65; acc: 0.89
Batch: 600; loss: 0.8; acc: 0.8
Batch: 620; loss: 0.7; acc: 0.86
Batch: 640; loss: 0.84; acc: 0.78
Batch: 660; loss: 0.91; acc: 0.7
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.84
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.75; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.89
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.00020368455443531275
0.0001942458184203133
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.91
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.684227666467618; val_accuracy: 0.8482285031847133 

The current subspace-distance is: 0.0001942458184203133 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.81
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 0.75; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.87; acc: 0.75
Batch: 200; loss: 0.73; acc: 0.86
Batch: 220; loss: 0.82; acc: 0.73
Batch: 240; loss: 0.78; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.94
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.68; acc: 0.91
Batch: 320; loss: 0.67; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.74; acc: 0.8
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.83
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 0.82; acc: 0.72
Batch: 500; loss: 0.79; acc: 0.86
Batch: 520; loss: 0.73; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.76; acc: 0.8
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.84; acc: 0.8
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.86; acc: 0.75
Batch: 680; loss: 0.81; acc: 0.72
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.73; acc: 0.78
Batch: 740; loss: 0.91; acc: 0.75
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.82 

0.00020654556283261627
0.00019695077207870781
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.95; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.6834360634445384; val_accuracy: 0.8471337579617835 

The current subspace-distance is: 0.00019695077207870781 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.81
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.75; acc: 0.81
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.92
Batch: 300; loss: 0.74; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.86
Batch: 340; loss: 0.74; acc: 0.8
Batch: 360; loss: 0.82; acc: 0.83
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.84
Batch: 420; loss: 0.9; acc: 0.75
Batch: 440; loss: 0.9; acc: 0.73
Batch: 460; loss: 0.69; acc: 0.89
Batch: 480; loss: 0.74; acc: 0.83
Batch: 500; loss: 0.85; acc: 0.78
Batch: 520; loss: 0.89; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.86; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.67; acc: 0.83
Batch: 640; loss: 0.71; acc: 0.86
Batch: 660; loss: 0.73; acc: 0.83
Batch: 680; loss: 0.88; acc: 0.77
Batch: 700; loss: 0.76; acc: 0.81
Batch: 720; loss: 0.73; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.81
Batch: 760; loss: 0.85; acc: 0.8
Batch: 780; loss: 0.84; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.00020338280592113733
0.00019467664242256433
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.92
Val Epoch over. val_loss: 0.680224131626688; val_accuracy: 0.8469347133757962 

The current subspace-distance is: 0.00019467664242256433 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_7_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.6540064054180426

The number of parameters is: 270776

The number of individual parameters is:

22
396
22
22
32
45760
32
32
64
133120
64
64
64
86016
64
64
4096
64
640
10
64
64

nonzero elements in E: 81232794
elements in E: 81232800
fraction nonzero: 0.9999999261382102
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.4; acc: 0.06
Batch: 20; loss: 2.13; acc: 0.23
Batch: 40; loss: 2.06; acc: 0.33
Batch: 60; loss: 1.91; acc: 0.41
Batch: 80; loss: 1.88; acc: 0.47
Batch: 100; loss: 1.81; acc: 0.47
Batch: 120; loss: 1.66; acc: 0.62
Batch: 140; loss: 1.8; acc: 0.53
Batch: 160; loss: 1.64; acc: 0.52
Batch: 180; loss: 1.65; acc: 0.58
Batch: 200; loss: 1.69; acc: 0.56
Batch: 220; loss: 1.6; acc: 0.61
Batch: 240; loss: 1.64; acc: 0.59
Batch: 260; loss: 1.48; acc: 0.67
Batch: 280; loss: 1.52; acc: 0.67
Batch: 300; loss: 1.52; acc: 0.64
Batch: 320; loss: 1.38; acc: 0.8
Batch: 340; loss: 1.51; acc: 0.58
Batch: 360; loss: 1.45; acc: 0.69
Batch: 380; loss: 1.4; acc: 0.69
Batch: 400; loss: 1.37; acc: 0.72
Batch: 420; loss: 1.5; acc: 0.66
Batch: 440; loss: 1.36; acc: 0.78
Batch: 460; loss: 1.33; acc: 0.81
Batch: 480; loss: 1.36; acc: 0.72
Batch: 500; loss: 1.32; acc: 0.78
Batch: 520; loss: 1.4; acc: 0.64
Batch: 540; loss: 1.25; acc: 0.86
Batch: 560; loss: 1.36; acc: 0.72
Batch: 580; loss: 1.34; acc: 0.75
Batch: 600; loss: 1.35; acc: 0.77
Batch: 620; loss: 1.2; acc: 0.83
Batch: 640; loss: 1.31; acc: 0.72
Batch: 660; loss: 1.32; acc: 0.73
Batch: 680; loss: 1.3; acc: 0.73
Batch: 700; loss: 1.15; acc: 0.83
Batch: 720; loss: 1.25; acc: 0.75
Batch: 740; loss: 1.15; acc: 0.8
Batch: 760; loss: 1.24; acc: 0.7
Batch: 780; loss: 1.2; acc: 0.77
Train Epoch over. train_loss: 1.49; train_accuracy: 0.66 

6.621140346396714e-05
6.223374657565728e-05
Batch: 0; loss: 1.26; acc: 0.75
Batch: 20; loss: 1.24; acc: 0.77
Batch: 40; loss: 0.89; acc: 0.94
Batch: 60; loss: 1.13; acc: 0.78
Batch: 80; loss: 0.92; acc: 0.91
Batch: 100; loss: 1.14; acc: 0.86
Batch: 120; loss: 1.27; acc: 0.75
Batch: 140; loss: 0.97; acc: 0.88
Val Epoch over. val_loss: 1.1210332038296256; val_accuracy: 0.8176751592356688 

The current subspace-distance is: 6.223374657565728e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.78
Batch: 20; loss: 1.19; acc: 0.7
Batch: 40; loss: 1.17; acc: 0.78
Batch: 60; loss: 1.19; acc: 0.84
Batch: 80; loss: 1.29; acc: 0.73
Batch: 100; loss: 1.11; acc: 0.83
Batch: 120; loss: 1.09; acc: 0.77
Batch: 140; loss: 1.09; acc: 0.75
Batch: 160; loss: 1.15; acc: 0.78
Batch: 180; loss: 1.13; acc: 0.8
Batch: 200; loss: 1.16; acc: 0.78
Batch: 220; loss: 1.19; acc: 0.73
Batch: 240; loss: 1.09; acc: 0.78
Batch: 260; loss: 1.15; acc: 0.72
Batch: 280; loss: 1.06; acc: 0.8
Batch: 300; loss: 1.13; acc: 0.78
Batch: 320; loss: 0.96; acc: 0.84
Batch: 340; loss: 0.97; acc: 0.89
Batch: 360; loss: 1.02; acc: 0.84
Batch: 380; loss: 1.08; acc: 0.73
Batch: 400; loss: 1.06; acc: 0.81
Batch: 420; loss: 1.13; acc: 0.78
Batch: 440; loss: 0.83; acc: 0.84
Batch: 460; loss: 0.99; acc: 0.8
Batch: 480; loss: 1.06; acc: 0.72
Batch: 500; loss: 1.11; acc: 0.75
Batch: 520; loss: 1.1; acc: 0.77
Batch: 540; loss: 1.02; acc: 0.8
Batch: 560; loss: 1.05; acc: 0.83
Batch: 580; loss: 1.06; acc: 0.78
Batch: 600; loss: 0.97; acc: 0.83
Batch: 620; loss: 0.97; acc: 0.83
Batch: 640; loss: 0.94; acc: 0.83
Batch: 660; loss: 0.93; acc: 0.78
Batch: 680; loss: 0.81; acc: 0.86
Batch: 700; loss: 0.94; acc: 0.8
Batch: 720; loss: 1.03; acc: 0.81
Batch: 740; loss: 1.05; acc: 0.7
Batch: 760; loss: 0.85; acc: 0.88
Batch: 780; loss: 1.04; acc: 0.81
Train Epoch over. train_loss: 1.03; train_accuracy: 0.81 

9.361617412650958e-05
8.846405398799106e-05
Batch: 0; loss: 1.0; acc: 0.81
Batch: 20; loss: 0.99; acc: 0.75
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.92; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.94
Batch: 100; loss: 0.81; acc: 0.91
Batch: 120; loss: 1.01; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.94
Val Epoch over. val_loss: 0.8629249296370586; val_accuracy: 0.8479299363057324 

The current subspace-distance is: 8.846405398799106e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 1.0; acc: 0.77
Batch: 80; loss: 0.89; acc: 0.84
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 1.03; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.81
Batch: 160; loss: 1.04; acc: 0.77
Batch: 180; loss: 0.82; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.81
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.93; acc: 0.81
Batch: 260; loss: 1.01; acc: 0.75
Batch: 280; loss: 0.99; acc: 0.81
Batch: 300; loss: 0.87; acc: 0.83
Batch: 320; loss: 0.96; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.86
Batch: 360; loss: 0.92; acc: 0.8
Batch: 380; loss: 0.77; acc: 0.88
Batch: 400; loss: 0.75; acc: 0.89
Batch: 420; loss: 0.84; acc: 0.84
Batch: 440; loss: 0.83; acc: 0.84
Batch: 460; loss: 0.83; acc: 0.83
Batch: 480; loss: 0.77; acc: 0.83
Batch: 500; loss: 0.92; acc: 0.84
Batch: 520; loss: 0.84; acc: 0.84
Batch: 540; loss: 0.9; acc: 0.8
Batch: 560; loss: 0.76; acc: 0.86
Batch: 580; loss: 0.82; acc: 0.81
Batch: 600; loss: 0.84; acc: 0.84
Batch: 620; loss: 0.8; acc: 0.86
Batch: 640; loss: 0.95; acc: 0.8
Batch: 660; loss: 0.85; acc: 0.78
Batch: 680; loss: 0.84; acc: 0.8
Batch: 700; loss: 0.67; acc: 0.91
Batch: 720; loss: 0.73; acc: 0.91
Batch: 740; loss: 0.74; acc: 0.88
Batch: 760; loss: 0.88; acc: 0.81
Batch: 780; loss: 0.87; acc: 0.75
Train Epoch over. train_loss: 0.86; train_accuracy: 0.83 

0.0001136139253503643
0.00010809244122356176
Batch: 0; loss: 0.88; acc: 0.8
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.94
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.95
Val Epoch over. val_loss: 0.7466566636683835; val_accuracy: 0.8587778662420382 

The current subspace-distance is: 0.00010809244122356176 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.88
Batch: 40; loss: 0.77; acc: 0.89
Batch: 60; loss: 0.91; acc: 0.78
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.91
Batch: 160; loss: 0.81; acc: 0.83
Batch: 180; loss: 0.7; acc: 0.92
Batch: 200; loss: 0.92; acc: 0.81
Batch: 220; loss: 0.8; acc: 0.84
Batch: 240; loss: 0.94; acc: 0.81
Batch: 260; loss: 0.78; acc: 0.81
Batch: 280; loss: 0.85; acc: 0.83
Batch: 300; loss: 0.74; acc: 0.89
Batch: 320; loss: 0.82; acc: 0.81
Batch: 340; loss: 0.88; acc: 0.83
Batch: 360; loss: 0.73; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.91
Batch: 400; loss: 0.68; acc: 0.91
Batch: 420; loss: 0.67; acc: 0.92
Batch: 440; loss: 0.72; acc: 0.89
Batch: 460; loss: 0.62; acc: 0.92
Batch: 480; loss: 0.85; acc: 0.83
Batch: 500; loss: 0.82; acc: 0.8
Batch: 520; loss: 0.86; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.84
Batch: 560; loss: 0.72; acc: 0.86
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.77; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.88
Batch: 680; loss: 0.86; acc: 0.81
Batch: 700; loss: 0.75; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.92
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.84
Batch: 780; loss: 0.89; acc: 0.78
Train Epoch over. train_loss: 0.78; train_accuracy: 0.84 

0.00013242477143649012
0.0001264566380996257
Batch: 0; loss: 0.78; acc: 0.91
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.97
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.55; acc: 0.95
Val Epoch over. val_loss: 0.6859158767256767; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 0.0001264566380996257 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.91
Batch: 40; loss: 0.77; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.88
Batch: 80; loss: 0.83; acc: 0.81
Batch: 100; loss: 0.8; acc: 0.83
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.78; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.71; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.81
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.75; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.81; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.83; acc: 0.81
Batch: 380; loss: 0.79; acc: 0.81
Batch: 400; loss: 0.86; acc: 0.77
Batch: 420; loss: 0.78; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.89
Batch: 460; loss: 0.66; acc: 0.89
Batch: 480; loss: 0.76; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.74; acc: 0.83
Batch: 540; loss: 0.85; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.89
Batch: 580; loss: 0.83; acc: 0.78
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.72; acc: 0.8
Batch: 640; loss: 0.73; acc: 0.84
Batch: 660; loss: 0.65; acc: 0.91
Batch: 680; loss: 0.77; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.88
Batch: 720; loss: 0.6; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.94
Batch: 760; loss: 0.68; acc: 0.86
Batch: 780; loss: 0.86; acc: 0.78
Train Epoch over. train_loss: 0.72; train_accuracy: 0.85 

0.00014838043716736138
0.0001431549753760919
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.95
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.94
Val Epoch over. val_loss: 0.6302689150640159; val_accuracy: 0.8711186305732485 

The current subspace-distance is: 0.0001431549753760919 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.5; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.94
Batch: 200; loss: 0.71; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.91
Batch: 300; loss: 0.71; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.6; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.92
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.67; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.68; acc: 0.88
Batch: 580; loss: 0.75; acc: 0.8
Batch: 600; loss: 0.55; acc: 0.91
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.77; acc: 0.8
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.67; train_accuracy: 0.86 

0.0001615852233953774
0.0001579598756507039
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.5881332259648925; val_accuracy: 0.8779856687898089 

The current subspace-distance is: 0.0001579598756507039 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.92
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.92
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.72; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.89
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.91
Batch: 660; loss: 0.67; acc: 0.8
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.63; train_accuracy: 0.86 

0.00017503144044894725
0.00016984198009595275
Batch: 0; loss: 0.55; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.45; acc: 0.94
Val Epoch over. val_loss: 0.553777629023145; val_accuracy: 0.8818670382165605 

The current subspace-distance is: 0.00016984198009595275 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.92
Batch: 40; loss: 0.71; acc: 0.78
Batch: 60; loss: 0.58; acc: 0.92
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.92
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.56; acc: 0.78
Batch: 420; loss: 0.7; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.00018459963030181825
0.00017923158884514123
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.5162796212989054; val_accuracy: 0.8897292993630573 

The current subspace-distance is: 0.00017923158884514123 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.84; acc: 0.77
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.92
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.65; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.67; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.91
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.78
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.83; acc: 0.73
Batch: 460; loss: 0.65; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.66; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.58; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.97
Batch: 760; loss: 0.62; acc: 0.8
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00019604912085924298
0.00018940643349196762
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.91
Val Epoch over. val_loss: 0.4892644316527494; val_accuracy: 0.8914211783439491 

The current subspace-distance is: 0.00018940643349196762 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.91
Batch: 80; loss: 0.62; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.92
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.7; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00020562372810672969
0.0001977693900698796
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.94
Val Epoch over. val_loss: 0.4732823694587513; val_accuracy: 0.8973925159235668 

The current subspace-distance is: 0.0001977693900698796 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.97
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.63; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.97
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.94
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.94
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.46; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.0002067953028017655
0.00019938155310228467
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.4654542704106896; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 0.00019938155310228467 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.95
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.83
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.43; acc: 0.95
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00021161598851904273
0.00020447892893571407
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.4626393375123382; val_accuracy: 0.8963972929936306 

The current subspace-distance is: 0.00020447892893571407 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.43; acc: 0.94
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.95
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.94
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.72; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.78
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00021255190949887037
0.0002053016796708107
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.94
Val Epoch over. val_loss: 0.45430430712973235; val_accuracy: 0.8974920382165605 

The current subspace-distance is: 0.0002053016796708107 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.81
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.78
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.95
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00021801215189043432
0.00021043032757006586
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.94
Val Epoch over. val_loss: 0.4492281321317527; val_accuracy: 0.8992834394904459 

The current subspace-distance is: 0.00021043032757006586 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.97
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.94
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00021839694818481803
0.00021000731794629246
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.44652080004382283; val_accuracy: 0.8994824840764332 

The current subspace-distance is: 0.00021000731794629246 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.95
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.95
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00021895847748965025
0.0002134378592018038
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.4381226914323819; val_accuracy: 0.899781050955414 

The current subspace-distance is: 0.0002134378592018038 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.97
Batch: 500; loss: 0.69; acc: 0.86
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00022264287690632045
0.00021757587091997266
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.4295229637508939; val_accuracy: 0.9017714968152867 

The current subspace-distance is: 0.00021757587091997266 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.81
Batch: 60; loss: 0.32; acc: 0.97
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.73; acc: 0.78
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.97
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.73; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.94
Batch: 600; loss: 0.6; acc: 0.78
Batch: 620; loss: 0.67; acc: 0.81
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00022597212227992713
0.00022036791779100895
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.42861697780098884; val_accuracy: 0.901671974522293 

The current subspace-distance is: 0.00022036791779100895 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.98
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.98
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00022665844880975783
0.0002202080941060558
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.42874240068493374; val_accuracy: 0.9023686305732485 

The current subspace-distance is: 0.0002202080941060558 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.97
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.94
Batch: 580; loss: 0.58; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.95
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.42; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00022831883688922971
0.00022423197515308857
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4256843123466346; val_accuracy: 0.9038614649681529 

The current subspace-distance is: 0.00022423197515308857 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.95
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.95
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.54; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00023093372874427587
0.00022207155416253954
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.41728543874564444; val_accuracy: 0.9046576433121019 

The current subspace-distance is: 0.00022207155416253954 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.95
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.95
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.00023071139003150165
0.00022098873159848154
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4155464448556779; val_accuracy: 0.9049562101910829 

The current subspace-distance is: 0.00022098873159848154 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.92
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.94
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.95
Batch: 460; loss: 0.75; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.94
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

0.0002301070053363219
0.0002221040631411597
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.41774201383636256; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 0.0002221040631411597 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.29; acc: 1.0
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.95
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.95
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00023485865676775575
0.0002257482410641387
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.41551471610737456; val_accuracy: 0.9050557324840764 

The current subspace-distance is: 0.0002257482410641387 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.94
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.61; acc: 0.78
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.84
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00023235376283992082
0.0002251119294669479
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.41248732757796147; val_accuracy: 0.9053542993630573 

The current subspace-distance is: 0.0002251119294669479 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.97
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.66; acc: 0.77
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.53; acc: 0.8
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.8
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.74; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.95
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.0002347928675590083
0.00022760553110856563
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4115132250034126; val_accuracy: 0.9068471337579618 

The current subspace-distance is: 0.00022760553110856563 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.42; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.97
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.66; acc: 0.77
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00023507642617914826
0.00022699355031363666
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.40970438965566597; val_accuracy: 0.9068471337579618 

The current subspace-distance is: 0.00022699355031363666 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.95
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.8
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.000234570077736862
0.00022932840511202812
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4111945308317804; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 0.00022932840511202812 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.37; acc: 0.95
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.61; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.71; acc: 0.77
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.63; acc: 0.78
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00023792758292984217
0.00022943082149140537
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.4100716019132335; val_accuracy: 0.9065485668789809 

The current subspace-distance is: 0.00022943082149140537 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.67; acc: 0.75
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

0.00023597682593390346
0.00022806999913882464
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4086013421131547; val_accuracy: 0.9061504777070064 

The current subspace-distance is: 0.00022806999913882464 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_7_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.6540064054180426

The number of parameters is: 270776

The number of individual parameters is:

22
396
22
22
32
45760
32
32
64
133120
64
64
64
86016
64
64
4096
64
640
10
64
64

nonzero elements in E: 108310390
elements in E: 108310400
fraction nonzero: 0.9999999076727627
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.39; acc: 0.05
Batch: 20; loss: 2.07; acc: 0.3
Batch: 40; loss: 2.06; acc: 0.36
Batch: 60; loss: 1.91; acc: 0.39
Batch: 80; loss: 1.86; acc: 0.42
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.64; acc: 0.55
Batch: 140; loss: 1.65; acc: 0.59
Batch: 160; loss: 1.54; acc: 0.62
Batch: 180; loss: 1.46; acc: 0.72
Batch: 200; loss: 1.42; acc: 0.7
Batch: 220; loss: 1.36; acc: 0.72
Batch: 240; loss: 1.43; acc: 0.69
Batch: 260; loss: 1.26; acc: 0.8
Batch: 280; loss: 1.37; acc: 0.66
Batch: 300; loss: 1.34; acc: 0.77
Batch: 320; loss: 1.37; acc: 0.7
Batch: 340; loss: 1.19; acc: 0.8
Batch: 360; loss: 1.14; acc: 0.86
Batch: 380; loss: 1.22; acc: 0.84
Batch: 400; loss: 1.23; acc: 0.77
Batch: 420; loss: 1.24; acc: 0.7
Batch: 440; loss: 1.18; acc: 0.8
Batch: 460; loss: 1.12; acc: 0.84
Batch: 480; loss: 1.22; acc: 0.8
Batch: 500; loss: 1.18; acc: 0.72
Batch: 520; loss: 1.02; acc: 0.94
Batch: 540; loss: 1.14; acc: 0.78
Batch: 560; loss: 1.11; acc: 0.75
Batch: 580; loss: 1.13; acc: 0.83
Batch: 600; loss: 1.18; acc: 0.77
Batch: 620; loss: 1.03; acc: 0.86
Batch: 640; loss: 1.19; acc: 0.81
Batch: 660; loss: 1.16; acc: 0.77
Batch: 680; loss: 1.12; acc: 0.78
Batch: 700; loss: 1.07; acc: 0.8
Batch: 720; loss: 1.2; acc: 0.72
Batch: 740; loss: 0.97; acc: 0.86
Batch: 760; loss: 0.99; acc: 0.86
Batch: 780; loss: 1.18; acc: 0.7
Train Epoch over. train_loss: 1.34; train_accuracy: 0.7 

2.539881461416371e-05
9.498621693637688e-06
Batch: 0; loss: 1.08; acc: 0.83
Batch: 20; loss: 1.22; acc: 0.75
Batch: 40; loss: 0.74; acc: 0.94
Batch: 60; loss: 1.02; acc: 0.8
Batch: 80; loss: 0.82; acc: 0.92
Batch: 100; loss: 0.94; acc: 0.94
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 0.79; acc: 0.89
Val Epoch over. val_loss: 0.9622318418162643; val_accuracy: 0.841859076433121 

The current subspace-distance is: 9.498621693637688e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.84
Batch: 20; loss: 1.0; acc: 0.83
Batch: 40; loss: 0.85; acc: 0.94
Batch: 60; loss: 1.15; acc: 0.77
Batch: 80; loss: 1.0; acc: 0.88
Batch: 100; loss: 0.9; acc: 0.83
Batch: 120; loss: 1.03; acc: 0.73
Batch: 140; loss: 0.94; acc: 0.83
Batch: 160; loss: 0.95; acc: 0.89
Batch: 180; loss: 0.94; acc: 0.84
Batch: 200; loss: 0.94; acc: 0.88
Batch: 220; loss: 0.96; acc: 0.81
Batch: 240; loss: 0.93; acc: 0.81
Batch: 260; loss: 1.05; acc: 0.8
Batch: 280; loss: 0.97; acc: 0.83
Batch: 300; loss: 0.99; acc: 0.81
Batch: 320; loss: 0.92; acc: 0.86
Batch: 340; loss: 0.95; acc: 0.86
Batch: 360; loss: 0.92; acc: 0.81
Batch: 380; loss: 1.03; acc: 0.8
Batch: 400; loss: 0.88; acc: 0.86
Batch: 420; loss: 0.87; acc: 0.84
Batch: 440; loss: 0.91; acc: 0.83
Batch: 460; loss: 0.84; acc: 0.88
Batch: 480; loss: 0.95; acc: 0.86
Batch: 500; loss: 0.98; acc: 0.8
Batch: 520; loss: 0.82; acc: 0.86
Batch: 540; loss: 0.81; acc: 0.89
Batch: 560; loss: 0.87; acc: 0.83
Batch: 580; loss: 0.81; acc: 0.88
Batch: 600; loss: 0.86; acc: 0.86
Batch: 620; loss: 0.79; acc: 0.89
Batch: 640; loss: 0.88; acc: 0.86
Batch: 660; loss: 0.77; acc: 0.91
Batch: 680; loss: 0.79; acc: 0.91
Batch: 700; loss: 0.67; acc: 0.97
Batch: 720; loss: 0.79; acc: 0.89
Batch: 740; loss: 0.8; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.86
Batch: 780; loss: 0.76; acc: 0.91
Train Epoch over. train_loss: 0.91; train_accuracy: 0.84 

3.17412632284686e-05
1.371623784507392e-05
Batch: 0; loss: 0.84; acc: 0.91
Batch: 20; loss: 0.96; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.97
Batch: 60; loss: 0.88; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.95
Batch: 100; loss: 0.73; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.57; acc: 0.94
Val Epoch over. val_loss: 0.7542646060324019; val_accuracy: 0.8815684713375797 

The current subspace-distance is: 1.371623784507392e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.91
Batch: 20; loss: 0.9; acc: 0.84
Batch: 40; loss: 0.93; acc: 0.86
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.76; acc: 0.92
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.91
Batch: 140; loss: 0.75; acc: 0.89
Batch: 160; loss: 0.8; acc: 0.84
Batch: 180; loss: 0.8; acc: 0.83
Batch: 200; loss: 0.79; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.91
Batch: 240; loss: 0.75; acc: 0.91
Batch: 260; loss: 0.74; acc: 0.89
Batch: 280; loss: 0.82; acc: 0.81
Batch: 300; loss: 0.95; acc: 0.75
Batch: 320; loss: 0.77; acc: 0.86
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.75; acc: 0.89
Batch: 380; loss: 0.66; acc: 0.91
Batch: 400; loss: 0.7; acc: 0.89
Batch: 420; loss: 0.65; acc: 0.92
Batch: 440; loss: 0.81; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.89
Batch: 480; loss: 0.79; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.95
Batch: 520; loss: 0.69; acc: 0.94
Batch: 540; loss: 0.71; acc: 0.88
Batch: 560; loss: 0.68; acc: 0.89
Batch: 580; loss: 0.85; acc: 0.81
Batch: 600; loss: 0.7; acc: 0.89
Batch: 620; loss: 0.77; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.91
Batch: 660; loss: 0.82; acc: 0.84
Batch: 680; loss: 0.83; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.97
Batch: 720; loss: 0.69; acc: 0.94
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.79; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.89
Train Epoch over. train_loss: 0.76; train_accuracy: 0.87 

3.640427166828886e-05
1.668863660597708e-05
Batch: 0; loss: 0.7; acc: 0.92
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.95
Val Epoch over. val_loss: 0.6382537518337275; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 1.668863660597708e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.69; acc: 0.89
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.72; acc: 0.86
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.91
Batch: 160; loss: 0.68; acc: 0.88
Batch: 180; loss: 0.77; acc: 0.84
Batch: 200; loss: 0.57; acc: 0.92
Batch: 220; loss: 0.76; acc: 0.84
Batch: 240; loss: 0.72; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.95
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.91
Batch: 340; loss: 0.75; acc: 0.88
Batch: 360; loss: 0.64; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.78
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.88
Batch: 440; loss: 0.87; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.89
Batch: 480; loss: 0.64; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.81; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.94
Batch: 600; loss: 0.72; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.91
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.83
Batch: 680; loss: 0.7; acc: 0.83
Batch: 700; loss: 0.69; acc: 0.84
Batch: 720; loss: 0.8; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.92
Batch: 760; loss: 0.67; acc: 0.88
Batch: 780; loss: 0.65; acc: 0.92
Train Epoch over. train_loss: 0.67; train_accuracy: 0.88 

4.045341120217927e-05
1.8845938029699028e-05
Batch: 0; loss: 0.61; acc: 0.92
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.97
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.95
Val Epoch over. val_loss: 0.5683505094734727; val_accuracy: 0.9033638535031847 

The current subspace-distance is: 1.8845938029699028e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.92
Batch: 40; loss: 0.59; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.91
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.94
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.98
Batch: 260; loss: 0.55; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.94
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.58; acc: 0.91
Batch: 400; loss: 0.66; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 0.63; acc: 0.89
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.52; acc: 0.94
Batch: 520; loss: 0.64; acc: 0.91
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.98
Batch: 740; loss: 0.6; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.95
Train Epoch over. train_loss: 0.61; train_accuracy: 0.88 

4.3611365981632844e-05
2.0400526409503073e-05
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.98
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.5249490637308473; val_accuracy: 0.9066480891719745 

The current subspace-distance is: 2.0400526409503073e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.75; acc: 0.83
Batch: 240; loss: 0.49; acc: 0.98
Batch: 260; loss: 0.59; acc: 0.91
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.98
Batch: 360; loss: 0.62; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.94
Batch: 560; loss: 0.59; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.62; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.98
Batch: 660; loss: 0.58; acc: 0.91
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.89
Train Epoch over. train_loss: 0.57; train_accuracy: 0.89 

4.657087265513837e-05
2.2201405954547226e-05
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.27; acc: 1.0
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.38; acc: 0.94
Val Epoch over. val_loss: 0.49960746630361885; val_accuracy: 0.9066480891719745 

The current subspace-distance is: 2.2201405954547226e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.94
Batch: 160; loss: 0.52; acc: 0.94
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.59; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.94
Batch: 240; loss: 0.57; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.91
Batch: 380; loss: 0.62; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.61; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.97
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.58; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.94
Batch: 620; loss: 0.6; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.97
Batch: 680; loss: 0.44; acc: 0.94
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.89 

4.867085226578638e-05
2.2997139240032993e-05
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.25; acc: 1.0
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.92
Val Epoch over. val_loss: 0.471037476305749; val_accuracy: 0.9074442675159236 

The current subspace-distance is: 2.2997139240032993e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.91
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.92
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.95
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.94
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.95
Batch: 680; loss: 0.54; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

5.056114969193004e-05
2.349253190914169e-05
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4469671641375608; val_accuracy: 0.9091361464968153 

The current subspace-distance is: 2.349253190914169e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.94
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.94
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

5.260880061541684e-05
2.472593405400403e-05
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.42779942967329815; val_accuracy: 0.9128184713375797 

The current subspace-distance is: 2.472593405400403e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.97
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.92
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.97
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.97
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

5.477109516505152e-05
2.605291774671059e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.40351162974242194; val_accuracy: 0.9155055732484076 

The current subspace-distance is: 2.605291774671059e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.8
Batch: 440; loss: 0.33; acc: 0.98
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

5.720142144127749e-05
2.7028489057556726e-05
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.2; acc: 1.0
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.94
Val Epoch over. val_loss: 0.39512254762801396; val_accuracy: 0.9163017515923567 

The current subspace-distance is: 2.7028489057556726e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.97
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.73; acc: 0.78
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.95
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.708191019948572e-05
2.7678615879267454e-05
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.2; acc: 1.0
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.3981657737189797; val_accuracy: 0.9163017515923567 

The current subspace-distance is: 2.7678615879267454e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.97
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.626498023048043e-05
2.4892317014746368e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.3838170512466674; val_accuracy: 0.9160031847133758 

The current subspace-distance is: 2.4892317014746368e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.98
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.8
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.827764834975824e-05
2.7918040359509178e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.19; acc: 1.0
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.37989293162230475; val_accuracy: 0.9176950636942676 

The current subspace-distance is: 2.7918040359509178e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.95
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.97
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.63; acc: 0.78
Batch: 320; loss: 0.54; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.95
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.94
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.858918302692473e-05
2.8675442081294023e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.37216688312922314; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 2.8675442081294023e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.98
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.0075784858781844e-05
2.875127029255964e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.3687776528346311; val_accuracy: 0.9190883757961783 

The current subspace-distance is: 2.875127029255964e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.98
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.95
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.0378875787137076e-05
2.9867873308830895e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3595825693789561; val_accuracy: 0.9208797770700637 

The current subspace-distance is: 2.9867873308830895e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.95
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.49; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.97
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.0975118685746565e-05
2.9321345209609717e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.3569974307991137; val_accuracy: 0.9213773885350318 

The current subspace-distance is: 2.9321345209609717e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.58; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.97
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.8
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.98
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.81
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.128781387815252e-05
3.003132951562293e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.34557015425080706; val_accuracy: 0.9222730891719745 

The current subspace-distance is: 3.003132951562293e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.184380617924035e-05
3.09504903270863e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.3432099207951005; val_accuracy: 0.923765923566879 

The current subspace-distance is: 3.09504903270863e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.97
Batch: 480; loss: 0.36; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.266661512199789e-05
3.039378316316288e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.34447300424621363; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 3.039378316316288e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.208987906575203e-05
2.92252534563886e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.3386329270092545; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 2.92252534563886e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.97
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.219161878107116e-05
2.8645008569583297e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.3445768586009931; val_accuracy: 0.9222730891719745 

The current subspace-distance is: 2.8645008569583297e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.23; acc: 1.0
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.97
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

6.22894149273634e-05
3.0383864213945344e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.33854192087225093; val_accuracy: 0.9244625796178344 

The current subspace-distance is: 3.0383864213945344e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.97
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.281511741690338e-05
2.8752345315297134e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.3402947000447352; val_accuracy: 0.9247611464968153 

The current subspace-distance is: 2.8752345315297134e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.57; acc: 0.8
Batch: 420; loss: 0.4; acc: 0.94
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.307271542027593e-05
2.922008752648253e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.3377272237068529; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.922008752648253e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.98
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.256796041270718e-05
2.953581315523479e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.34345616504644894; val_accuracy: 0.9251592356687898 

The current subspace-distance is: 2.953581315523479e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.98
Batch: 240; loss: 0.44; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.55; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.98
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.57; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.42880768282339e-05
3.27109191857744e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3372126037527801; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 3.27109191857744e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.97
Batch: 600; loss: 0.28; acc: 0.98
Batch: 620; loss: 0.59; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.363388820318505e-05
3.1007988582132384e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.33558046181870116; val_accuracy: 0.9238654458598726 

The current subspace-distance is: 3.1007988582132384e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.98
Batch: 440; loss: 0.36; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.9 

6.314561323961243e-05
3.0188137316145003e-05
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.33224672392295423; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 3.0188137316145003e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_7_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.6540064054180426

The number of parameters is: 270776

The number of individual parameters is:

22
396
22
22
32
45760
32
32
64
133120
64
64
64
86016
64
64
4096
64
640
10
64
64

nonzero elements in E: 135387990
elements in E: 135388000
fraction nonzero: 0.9999999261382102
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.48; acc: 0.05
Batch: 20; loss: 2.16; acc: 0.17
Batch: 40; loss: 2.0; acc: 0.36
Batch: 60; loss: 1.75; acc: 0.59
Batch: 80; loss: 1.57; acc: 0.67
Batch: 100; loss: 1.52; acc: 0.7
Batch: 120; loss: 1.5; acc: 0.64
Batch: 140; loss: 1.45; acc: 0.72
Batch: 160; loss: 1.46; acc: 0.67
Batch: 180; loss: 1.35; acc: 0.75
Batch: 200; loss: 1.39; acc: 0.75
Batch: 220; loss: 1.15; acc: 0.86
Batch: 240; loss: 1.27; acc: 0.81
Batch: 260; loss: 1.2; acc: 0.8
Batch: 280; loss: 1.28; acc: 0.75
Batch: 300; loss: 1.13; acc: 0.86
Batch: 320; loss: 1.09; acc: 0.81
Batch: 340; loss: 1.06; acc: 0.86
Batch: 360; loss: 1.17; acc: 0.78
Batch: 380; loss: 1.01; acc: 0.89
Batch: 400; loss: 1.01; acc: 0.88
Batch: 420; loss: 1.06; acc: 0.88
Batch: 440; loss: 1.02; acc: 0.84
Batch: 460; loss: 0.97; acc: 0.92
Batch: 480; loss: 1.09; acc: 0.81
Batch: 500; loss: 1.03; acc: 0.86
Batch: 520; loss: 1.05; acc: 0.81
Batch: 540; loss: 0.95; acc: 0.92
Batch: 560; loss: 0.91; acc: 0.91
Batch: 580; loss: 0.93; acc: 0.88
Batch: 600; loss: 0.89; acc: 0.91
Batch: 620; loss: 0.99; acc: 0.81
Batch: 640; loss: 0.97; acc: 0.88
Batch: 660; loss: 0.98; acc: 0.8
Batch: 680; loss: 0.86; acc: 0.88
Batch: 700; loss: 0.91; acc: 0.88
Batch: 720; loss: 0.95; acc: 0.83
Batch: 740; loss: 0.81; acc: 0.92
Batch: 760; loss: 0.85; acc: 0.86
Batch: 780; loss: 1.02; acc: 0.86
Train Epoch over. train_loss: 1.19; train_accuracy: 0.77 

2.5459416065132245e-05
9.218500053975731e-06
Batch: 0; loss: 0.85; acc: 0.94
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 0.6; acc: 0.97
Batch: 60; loss: 0.89; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.92
Batch: 100; loss: 0.83; acc: 0.89
Batch: 120; loss: 0.99; acc: 0.78
Batch: 140; loss: 0.66; acc: 0.95
Val Epoch over. val_loss: 0.8213683716051138; val_accuracy: 0.8843550955414012 

The current subspace-distance is: 9.218500053975731e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.8
Batch: 20; loss: 0.84; acc: 0.86
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.92
Batch: 80; loss: 0.94; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.86
Batch: 140; loss: 0.9; acc: 0.83
Batch: 160; loss: 0.81; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.89
Batch: 200; loss: 0.89; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.92
Batch: 240; loss: 0.76; acc: 0.91
Batch: 260; loss: 0.78; acc: 0.92
Batch: 280; loss: 0.9; acc: 0.83
Batch: 300; loss: 0.76; acc: 0.91
Batch: 320; loss: 0.89; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.89
Batch: 360; loss: 0.72; acc: 0.91
Batch: 380; loss: 0.72; acc: 0.89
Batch: 400; loss: 0.72; acc: 0.89
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 0.82; acc: 0.81
Batch: 460; loss: 0.92; acc: 0.84
Batch: 480; loss: 0.69; acc: 0.92
Batch: 500; loss: 0.65; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.92
Batch: 540; loss: 0.72; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.92
Batch: 580; loss: 0.81; acc: 0.83
Batch: 600; loss: 0.64; acc: 0.91
Batch: 620; loss: 0.67; acc: 0.89
Batch: 640; loss: 0.65; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.76; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.94
Batch: 720; loss: 0.61; acc: 0.94
Batch: 740; loss: 0.65; acc: 0.88
Batch: 760; loss: 0.7; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.89
Train Epoch over. train_loss: 0.77; train_accuracy: 0.88 

3.11708718072623e-05
1.1924865248147398e-05
Batch: 0; loss: 0.63; acc: 0.95
Batch: 20; loss: 0.9; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.97
Batch: 60; loss: 0.69; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.97
Val Epoch over. val_loss: 0.6303186372966524; val_accuracy: 0.9066480891719745 

The current subspace-distance is: 1.1924865248147398e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.77; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.92
Batch: 140; loss: 0.68; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.97
Batch: 180; loss: 0.53; acc: 0.95
Batch: 200; loss: 0.67; acc: 0.88
Batch: 220; loss: 0.67; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.95
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.97
Batch: 320; loss: 0.65; acc: 0.89
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.95
Batch: 400; loss: 0.57; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.95
Batch: 440; loss: 0.71; acc: 0.81
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.95
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.94
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.91
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.84; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.92
Batch: 700; loss: 0.65; acc: 0.91
Batch: 720; loss: 0.61; acc: 0.91
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.63; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.89 

3.501604442135431e-05
1.4514841495838482e-05
Batch: 0; loss: 0.55; acc: 0.94
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.35; acc: 1.0
Batch: 60; loss: 0.6; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.95
Val Epoch over. val_loss: 0.5515189873185128; val_accuracy: 0.9095342356687898 

The current subspace-distance is: 1.4514841495838482e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.92
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.94
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.69; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.95
Batch: 300; loss: 0.49; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.98
Batch: 380; loss: 0.6; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.97
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.94
Batch: 520; loss: 0.67; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.95
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.92
Batch: 660; loss: 0.55; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.94
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.57; train_accuracy: 0.89 

3.8832935388199985e-05
1.6778236386016943e-05
Batch: 0; loss: 0.49; acc: 0.97
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.4901356850839724; val_accuracy: 0.915406050955414 

The current subspace-distance is: 1.6778236386016943e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.97
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.97
Batch: 260; loss: 0.52; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.97
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.58; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.92
Batch: 720; loss: 0.66; acc: 0.81
Batch: 740; loss: 0.48; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.9 

4.232439096085727e-05
1.843672180257272e-05
Batch: 0; loss: 0.43; acc: 0.97
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.43580721043477394; val_accuracy: 0.9176950636942676 

The current subspace-distance is: 1.843672180257272e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.95
Batch: 200; loss: 0.51; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.94
Batch: 240; loss: 0.4; acc: 1.0
Batch: 260; loss: 0.37; acc: 0.98
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.55; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.91 

4.512696614256129e-05
1.9816792701021768e-05
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3983359497254062; val_accuracy: 0.9203821656050956 

The current subspace-distance is: 1.9816792701021768e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.95
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.6; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.97
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

4.8198606236837804e-05
2.060767110378947e-05
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3783231355771897; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.060767110378947e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.3; acc: 1.0
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.92 

5.038595918449573e-05
2.0872170352959074e-05
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3436641228047146; val_accuracy: 0.9292396496815286 

The current subspace-distance is: 2.0872170352959074e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.95
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

5.349410275812261e-05
2.4887411200325005e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3345292072956729; val_accuracy: 0.9288415605095541 

The current subspace-distance is: 2.4887411200325005e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.98
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.98
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.98
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.97
Batch: 740; loss: 0.34; acc: 0.95
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.483132554218173e-05
2.5424049454159103e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.3149217165959109; val_accuracy: 0.9300358280254777 

The current subspace-distance is: 2.5424049454159103e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.98
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.5937154684215784e-05
2.5070745323318988e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.314402683810064; val_accuracy: 0.9314291401273885 

The current subspace-distance is: 2.5070745323318988e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.98
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.98
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.6885572121245787e-05
2.6473115212866105e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.30844043337615434; val_accuracy: 0.932921974522293 

The current subspace-distance is: 2.6473115212866105e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.22; acc: 1.0
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.97
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.7359553466085345e-05
2.574611789896153e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.3056916991711422; val_accuracy: 0.9331210191082803 

The current subspace-distance is: 2.574611789896153e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.95
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.21; acc: 1.0
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.792902811663225e-05
2.704622056626249e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.30082684337713156; val_accuracy: 0.9334195859872612 

The current subspace-distance is: 2.704622056626249e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.794155003968626e-05
2.6461884772288613e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.30053658669541594; val_accuracy: 0.9325238853503185 

The current subspace-distance is: 2.6461884772288613e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.865559433004819e-05
2.6595123927108943e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2996069579652161; val_accuracy: 0.9333200636942676 

The current subspace-distance is: 2.6595123927108943e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.97
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.958971451036632e-05
2.8532533178804442e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2915867442727848; val_accuracy: 0.9343152866242038 

The current subspace-distance is: 2.8532533178804442e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.18; acc: 1.0
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.28; acc: 1.0
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.97
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.98
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.95144992985297e-05
2.7229036277276464e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.29097824249487775; val_accuracy: 0.9344148089171974 

The current subspace-distance is: 2.7229036277276464e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.98
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.937841706327163e-05
2.6749852622742765e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.29130259298594896; val_accuracy: 0.932921974522293 

The current subspace-distance is: 2.6749852622742765e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.036468766978942e-05
2.789909194689244e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2885772697864824; val_accuracy: 0.9339171974522293 

The current subspace-distance is: 2.789909194689244e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.98
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.98
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.013241727487184e-05
2.709734508243855e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.28807553293029214; val_accuracy: 0.9325238853503185 

The current subspace-distance is: 2.709734508243855e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.98
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.22; acc: 1.0
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.97
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.135581497801468e-05
2.9126964363968e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2850516336453948; val_accuracy: 0.9344148089171974 

The current subspace-distance is: 2.9126964363968e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.98
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.130694964667782e-05
2.8942064091097564e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.28468324932133315; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.8942064091097564e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.98
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.98
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.052937897038646e-05
2.688480708457064e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.28235842681424633; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.688480708457064e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.211878644535318e-05
3.080622263951227e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2815330524922936; val_accuracy: 0.9345143312101911 

The current subspace-distance is: 3.080622263951227e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

6.043648681952618e-05
2.6538809834164567e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2862814149469327; val_accuracy: 0.9340167197452229 

The current subspace-distance is: 2.6538809834164567e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.97
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.16; acc: 1.0
Batch: 420; loss: 0.26; acc: 0.98
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.0718171880580485e-05
2.734193367359694e-05
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.28310434083650066; val_accuracy: 0.9335191082802548 

The current subspace-distance is: 2.734193367359694e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.83
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.148005923023447e-05
2.9656193873961456e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.27952700298113425; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.9656193873961456e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.072225733078085e-05
2.661725739017129e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2831684079044943; val_accuracy: 0.9341162420382165 

The current subspace-distance is: 2.661725739017129e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.98
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.98
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.190517888171598e-05
2.8781298169633374e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2843924759869363; val_accuracy: 0.9330214968152867 

The current subspace-distance is: 2.8781298169633374e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:46/N_7_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:46/N_7_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
