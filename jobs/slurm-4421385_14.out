model : table13slim
N : 14
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:47

Channel scaling factor: 1.6227353450270319

The number of parameters is: 249772

The number of individual parameters is:

13
234
13
13
20
34840
20
20
39
104520
39
39
64
104832
64
64
4096
64
640
10
64
64

nonzero elements in E: 12488599
elements in E: 12488600
fraction nonzero: 0.9999999199269733
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.33; acc: 0.11
Batch: 40; loss: 2.31; acc: 0.06
Batch: 60; loss: 2.29; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.16; acc: 0.14
Batch: 140; loss: 2.07; acc: 0.25
Batch: 160; loss: 2.11; acc: 0.22
Batch: 180; loss: 2.04; acc: 0.27
Batch: 200; loss: 2.06; acc: 0.36
Batch: 220; loss: 2.11; acc: 0.23
Batch: 240; loss: 2.05; acc: 0.27
Batch: 260; loss: 2.08; acc: 0.34
Batch: 280; loss: 2.06; acc: 0.23
Batch: 300; loss: 1.94; acc: 0.45
Batch: 320; loss: 2.03; acc: 0.28
Batch: 340; loss: 1.99; acc: 0.42
Batch: 360; loss: 2.01; acc: 0.38
Batch: 380; loss: 2.01; acc: 0.33
Batch: 400; loss: 1.97; acc: 0.33
Batch: 420; loss: 2.01; acc: 0.38
Batch: 440; loss: 2.0; acc: 0.31
Batch: 460; loss: 1.93; acc: 0.39
Batch: 480; loss: 1.88; acc: 0.41
Batch: 500; loss: 2.02; acc: 0.36
Batch: 520; loss: 1.99; acc: 0.39
Batch: 540; loss: 1.98; acc: 0.34
Batch: 560; loss: 1.95; acc: 0.34
Batch: 580; loss: 2.01; acc: 0.36
Batch: 600; loss: 1.94; acc: 0.45
Batch: 620; loss: 1.89; acc: 0.47
Batch: 640; loss: 2.04; acc: 0.3
Batch: 660; loss: 1.92; acc: 0.36
Batch: 680; loss: 1.83; acc: 0.44
Batch: 700; loss: 1.89; acc: 0.38
Batch: 720; loss: 1.96; acc: 0.44
Batch: 740; loss: 1.88; acc: 0.45
Batch: 760; loss: 1.87; acc: 0.47
Batch: 780; loss: 1.86; acc: 0.48
Train Epoch over. train_loss: 2.03; train_accuracy: 0.32 

2.2633095795754343e-05
4.472544333111728e-06
Batch: 0; loss: 1.95; acc: 0.44
Batch: 20; loss: 1.9; acc: 0.42
Batch: 40; loss: 1.8; acc: 0.47
Batch: 60; loss: 1.88; acc: 0.41
Batch: 80; loss: 1.78; acc: 0.5
Batch: 100; loss: 1.93; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 1.85; acc: 0.44
Val Epoch over. val_loss: 1.9070183160198722; val_accuracy: 0.4184912420382166 

The current subspace-distance is: 4.472544333111728e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.12; acc: 0.16
Batch: 20; loss: 1.89; acc: 0.44
Batch: 40; loss: 1.97; acc: 0.34
Batch: 60; loss: 1.93; acc: 0.41
Batch: 80; loss: 1.79; acc: 0.53
Batch: 100; loss: 1.94; acc: 0.42
Batch: 120; loss: 1.88; acc: 0.41
Batch: 140; loss: 1.95; acc: 0.45
Batch: 160; loss: 1.93; acc: 0.38
Batch: 180; loss: 1.75; acc: 0.52
Batch: 200; loss: 1.86; acc: 0.47
Batch: 220; loss: 1.96; acc: 0.36
Batch: 240; loss: 1.91; acc: 0.39
Batch: 260; loss: 1.84; acc: 0.5
Batch: 280; loss: 1.81; acc: 0.44
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.89; acc: 0.44
Batch: 340; loss: 1.89; acc: 0.47
Batch: 360; loss: 1.91; acc: 0.41
Batch: 380; loss: 1.78; acc: 0.5
Batch: 400; loss: 1.83; acc: 0.45
Batch: 420; loss: 1.75; acc: 0.53
Batch: 440; loss: 1.89; acc: 0.41
Batch: 460; loss: 1.94; acc: 0.34
Batch: 480; loss: 1.9; acc: 0.34
Batch: 500; loss: 1.82; acc: 0.48
Batch: 520; loss: 1.93; acc: 0.36
Batch: 540; loss: 1.84; acc: 0.48
Batch: 560; loss: 1.93; acc: 0.44
Batch: 580; loss: 1.92; acc: 0.41
Batch: 600; loss: 1.93; acc: 0.38
Batch: 620; loss: 2.04; acc: 0.34
Batch: 640; loss: 1.8; acc: 0.53
Batch: 660; loss: 1.83; acc: 0.44
Batch: 680; loss: 1.78; acc: 0.53
Batch: 700; loss: 1.8; acc: 0.34
Batch: 720; loss: 1.91; acc: 0.42
Batch: 740; loss: 1.74; acc: 0.56
Batch: 760; loss: 1.9; acc: 0.44
Batch: 780; loss: 1.79; acc: 0.53
Train Epoch over. train_loss: 1.87; train_accuracy: 0.43 

2.4339382434845902e-05
6.042246241122484e-06
Batch: 0; loss: 1.83; acc: 0.47
Batch: 20; loss: 1.85; acc: 0.45
Batch: 40; loss: 1.59; acc: 0.69
Batch: 60; loss: 1.73; acc: 0.59
Batch: 80; loss: 1.7; acc: 0.52
Batch: 100; loss: 1.8; acc: 0.5
Batch: 120; loss: 1.92; acc: 0.5
Batch: 140; loss: 1.74; acc: 0.52
Val Epoch over. val_loss: 1.7868333424732183; val_accuracy: 0.47571656050955413 

The current subspace-distance is: 6.042246241122484e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.56
Batch: 20; loss: 1.81; acc: 0.36
Batch: 40; loss: 1.77; acc: 0.42
Batch: 60; loss: 1.83; acc: 0.34
Batch: 80; loss: 1.82; acc: 0.44
Batch: 100; loss: 1.85; acc: 0.41
Batch: 120; loss: 1.74; acc: 0.5
Batch: 140; loss: 1.76; acc: 0.45
Batch: 160; loss: 1.74; acc: 0.56
Batch: 180; loss: 1.76; acc: 0.48
Batch: 200; loss: 1.85; acc: 0.41
Batch: 220; loss: 1.76; acc: 0.47
Batch: 240; loss: 1.87; acc: 0.33
Batch: 260; loss: 1.65; acc: 0.59
Batch: 280; loss: 1.8; acc: 0.5
Batch: 300; loss: 1.77; acc: 0.52
Batch: 320; loss: 1.75; acc: 0.61
Batch: 340; loss: 1.76; acc: 0.41
Batch: 360; loss: 1.61; acc: 0.53
Batch: 380; loss: 1.79; acc: 0.39
Batch: 400; loss: 1.85; acc: 0.38
Batch: 420; loss: 1.8; acc: 0.47
Batch: 440; loss: 1.8; acc: 0.41
Batch: 460; loss: 1.88; acc: 0.36
Batch: 480; loss: 1.71; acc: 0.48
Batch: 500; loss: 1.71; acc: 0.48
Batch: 520; loss: 1.7; acc: 0.64
Batch: 540; loss: 1.79; acc: 0.53
Batch: 560; loss: 1.72; acc: 0.53
Batch: 580; loss: 1.8; acc: 0.44
Batch: 600; loss: 1.61; acc: 0.59
Batch: 620; loss: 1.83; acc: 0.48
Batch: 640; loss: 1.77; acc: 0.39
Batch: 660; loss: 1.81; acc: 0.5
Batch: 680; loss: 1.75; acc: 0.47
Batch: 700; loss: 1.81; acc: 0.52
Batch: 720; loss: 1.67; acc: 0.58
Batch: 740; loss: 1.86; acc: 0.42
Batch: 760; loss: 1.83; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.48
Train Epoch over. train_loss: 1.77; train_accuracy: 0.48 

2.720813063206151e-05
7.97624215920223e-06
Batch: 0; loss: 1.74; acc: 0.52
Batch: 20; loss: 1.83; acc: 0.41
Batch: 40; loss: 1.44; acc: 0.73
Batch: 60; loss: 1.62; acc: 0.64
Batch: 80; loss: 1.58; acc: 0.58
Batch: 100; loss: 1.72; acc: 0.56
Batch: 120; loss: 1.85; acc: 0.55
Batch: 140; loss: 1.63; acc: 0.64
Val Epoch over. val_loss: 1.70477402741742; val_accuracy: 0.533937101910828 

The current subspace-distance is: 7.97624215920223e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.71; acc: 0.53
Batch: 40; loss: 1.67; acc: 0.45
Batch: 60; loss: 1.66; acc: 0.48
Batch: 80; loss: 1.78; acc: 0.47
Batch: 100; loss: 1.87; acc: 0.41
Batch: 120; loss: 1.84; acc: 0.47
Batch: 140; loss: 1.64; acc: 0.58
Batch: 160; loss: 1.64; acc: 0.62
Batch: 180; loss: 1.7; acc: 0.58
Batch: 200; loss: 1.72; acc: 0.5
Batch: 220; loss: 1.7; acc: 0.5
Batch: 240; loss: 1.66; acc: 0.58
Batch: 260; loss: 1.77; acc: 0.52
Batch: 280; loss: 1.62; acc: 0.59
Batch: 300; loss: 1.68; acc: 0.58
Batch: 320; loss: 1.69; acc: 0.53
Batch: 340; loss: 1.68; acc: 0.55
Batch: 360; loss: 1.7; acc: 0.58
Batch: 380; loss: 1.66; acc: 0.56
Batch: 400; loss: 1.74; acc: 0.5
Batch: 420; loss: 1.77; acc: 0.48
Batch: 440; loss: 1.65; acc: 0.56
Batch: 460; loss: 1.76; acc: 0.5
Batch: 480; loss: 1.73; acc: 0.55
Batch: 500; loss: 1.71; acc: 0.48
Batch: 520; loss: 1.73; acc: 0.48
Batch: 540; loss: 1.62; acc: 0.56
Batch: 560; loss: 1.69; acc: 0.53
Batch: 580; loss: 1.79; acc: 0.52
Batch: 600; loss: 1.72; acc: 0.53
Batch: 620; loss: 1.76; acc: 0.5
Batch: 640; loss: 1.64; acc: 0.59
Batch: 660; loss: 1.66; acc: 0.58
Batch: 680; loss: 1.72; acc: 0.47
Batch: 700; loss: 1.75; acc: 0.47
Batch: 720; loss: 1.61; acc: 0.59
Batch: 740; loss: 1.69; acc: 0.53
Batch: 760; loss: 1.74; acc: 0.47
Batch: 780; loss: 1.73; acc: 0.48
Train Epoch over. train_loss: 1.71; train_accuracy: 0.52 

2.9633591111632995e-05
7.851169357309118e-06
Batch: 0; loss: 1.7; acc: 0.53
Batch: 20; loss: 1.82; acc: 0.38
Batch: 40; loss: 1.37; acc: 0.81
Batch: 60; loss: 1.56; acc: 0.7
Batch: 80; loss: 1.54; acc: 0.64
Batch: 100; loss: 1.67; acc: 0.52
Batch: 120; loss: 1.74; acc: 0.61
Batch: 140; loss: 1.58; acc: 0.61
Val Epoch over. val_loss: 1.6557364175274114; val_accuracy: 0.5725517515923567 

The current subspace-distance is: 7.851169357309118e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.71; acc: 0.58
Batch: 20; loss: 1.75; acc: 0.52
Batch: 40; loss: 1.6; acc: 0.59
Batch: 60; loss: 1.74; acc: 0.61
Batch: 80; loss: 1.67; acc: 0.62
Batch: 100; loss: 1.59; acc: 0.58
Batch: 120; loss: 1.55; acc: 0.66
Batch: 140; loss: 1.58; acc: 0.64
Batch: 160; loss: 1.76; acc: 0.48
Batch: 180; loss: 1.63; acc: 0.62
Batch: 200; loss: 1.57; acc: 0.66
Batch: 220; loss: 1.68; acc: 0.5
Batch: 240; loss: 1.73; acc: 0.48
Batch: 260; loss: 1.68; acc: 0.56
Batch: 280; loss: 1.75; acc: 0.52
Batch: 300; loss: 1.64; acc: 0.56
Batch: 320; loss: 1.79; acc: 0.5
Batch: 340; loss: 1.72; acc: 0.39
Batch: 360; loss: 1.74; acc: 0.56
Batch: 380; loss: 1.68; acc: 0.53
Batch: 400; loss: 1.73; acc: 0.5
Batch: 420; loss: 1.71; acc: 0.58
Batch: 440; loss: 1.69; acc: 0.66
Batch: 460; loss: 1.65; acc: 0.59
Batch: 480; loss: 1.75; acc: 0.47
Batch: 500; loss: 1.63; acc: 0.55
Batch: 520; loss: 1.68; acc: 0.58
Batch: 540; loss: 1.67; acc: 0.56
Batch: 560; loss: 1.65; acc: 0.59
Batch: 580; loss: 1.64; acc: 0.56
Batch: 600; loss: 1.63; acc: 0.58
Batch: 620; loss: 1.59; acc: 0.59
Batch: 640; loss: 1.64; acc: 0.62
Batch: 660; loss: 1.6; acc: 0.56
Batch: 680; loss: 1.8; acc: 0.48
Batch: 700; loss: 1.62; acc: 0.55
Batch: 720; loss: 1.68; acc: 0.59
Batch: 740; loss: 1.64; acc: 0.55
Batch: 760; loss: 1.69; acc: 0.5
Batch: 780; loss: 1.79; acc: 0.45
Train Epoch over. train_loss: 1.68; train_accuracy: 0.54 

3.180075509590097e-05
8.634219739178661e-06
Batch: 0; loss: 1.67; acc: 0.55
Batch: 20; loss: 1.77; acc: 0.45
Batch: 40; loss: 1.32; acc: 0.86
Batch: 60; loss: 1.57; acc: 0.73
Batch: 80; loss: 1.54; acc: 0.56
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.68; acc: 0.61
Batch: 140; loss: 1.53; acc: 0.62
Val Epoch over. val_loss: 1.625966125992453; val_accuracy: 0.5749402866242038 

The current subspace-distance is: 8.634219739178661e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.52
Batch: 20; loss: 1.69; acc: 0.52
Batch: 40; loss: 1.63; acc: 0.59
Batch: 60; loss: 1.67; acc: 0.58
Batch: 80; loss: 1.74; acc: 0.45
Batch: 100; loss: 1.72; acc: 0.5
Batch: 120; loss: 1.56; acc: 0.7
Batch: 140; loss: 1.66; acc: 0.58
Batch: 160; loss: 1.6; acc: 0.67
Batch: 180; loss: 1.64; acc: 0.55
Batch: 200; loss: 1.69; acc: 0.55
Batch: 220; loss: 1.77; acc: 0.53
Batch: 240; loss: 1.78; acc: 0.44
Batch: 260; loss: 1.71; acc: 0.52
Batch: 280; loss: 1.59; acc: 0.61
Batch: 300; loss: 1.69; acc: 0.5
Batch: 320; loss: 1.63; acc: 0.52
Batch: 340; loss: 1.75; acc: 0.47
Batch: 360; loss: 1.64; acc: 0.53
Batch: 380; loss: 1.58; acc: 0.62
Batch: 400; loss: 1.7; acc: 0.56
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.67; acc: 0.56
Batch: 460; loss: 1.67; acc: 0.53
Batch: 480; loss: 1.56; acc: 0.69
Batch: 500; loss: 1.6; acc: 0.53
Batch: 520; loss: 1.71; acc: 0.56
Batch: 540; loss: 1.55; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.66
Batch: 580; loss: 1.65; acc: 0.55
Batch: 600; loss: 1.63; acc: 0.55
Batch: 620; loss: 1.77; acc: 0.45
Batch: 640; loss: 1.81; acc: 0.39
Batch: 660; loss: 1.73; acc: 0.48
Batch: 680; loss: 1.59; acc: 0.58
Batch: 700; loss: 1.76; acc: 0.5
Batch: 720; loss: 1.55; acc: 0.67
Batch: 740; loss: 1.8; acc: 0.47
Batch: 760; loss: 1.7; acc: 0.48
Batch: 780; loss: 1.65; acc: 0.52
Train Epoch over. train_loss: 1.66; train_accuracy: 0.54 

3.2658012059982866e-05
9.434932508156635e-06
Batch: 0; loss: 1.67; acc: 0.56
Batch: 20; loss: 1.73; acc: 0.47
Batch: 40; loss: 1.31; acc: 0.83
Batch: 60; loss: 1.59; acc: 0.72
Batch: 80; loss: 1.54; acc: 0.55
Batch: 100; loss: 1.59; acc: 0.52
Batch: 120; loss: 1.67; acc: 0.58
Batch: 140; loss: 1.5; acc: 0.67
Val Epoch over. val_loss: 1.6066307254657624; val_accuracy: 0.5821058917197452 

The current subspace-distance is: 9.434932508156635e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.62; acc: 0.5
Batch: 40; loss: 1.61; acc: 0.55
Batch: 60; loss: 1.59; acc: 0.61
Batch: 80; loss: 1.58; acc: 0.64
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.57; acc: 0.59
Batch: 140; loss: 1.52; acc: 0.69
Batch: 160; loss: 1.68; acc: 0.53
Batch: 180; loss: 1.6; acc: 0.52
Batch: 200; loss: 1.65; acc: 0.48
Batch: 220; loss: 1.62; acc: 0.56
Batch: 240; loss: 1.67; acc: 0.47
Batch: 260; loss: 1.83; acc: 0.45
Batch: 280; loss: 1.62; acc: 0.56
Batch: 300; loss: 1.68; acc: 0.53
Batch: 320; loss: 1.6; acc: 0.56
Batch: 340; loss: 1.75; acc: 0.42
Batch: 360; loss: 1.7; acc: 0.53
Batch: 380; loss: 1.53; acc: 0.59
Batch: 400; loss: 1.64; acc: 0.55
Batch: 420; loss: 1.59; acc: 0.56
Batch: 440; loss: 1.62; acc: 0.53
Batch: 460; loss: 1.62; acc: 0.58
Batch: 480; loss: 1.64; acc: 0.59
Batch: 500; loss: 1.51; acc: 0.66
Batch: 520; loss: 1.72; acc: 0.48
Batch: 540; loss: 1.65; acc: 0.52
Batch: 560; loss: 1.56; acc: 0.56
Batch: 580; loss: 1.61; acc: 0.59
Batch: 600; loss: 1.54; acc: 0.55
Batch: 620; loss: 1.71; acc: 0.56
Batch: 640; loss: 1.62; acc: 0.56
Batch: 660; loss: 1.62; acc: 0.52
Batch: 680; loss: 1.6; acc: 0.47
Batch: 700; loss: 1.69; acc: 0.53
Batch: 720; loss: 1.72; acc: 0.52
Batch: 740; loss: 1.63; acc: 0.58
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.71; acc: 0.47
Train Epoch over. train_loss: 1.65; train_accuracy: 0.54 

3.4032414987450466e-05
1.088619956135517e-05
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.71; acc: 0.47
Batch: 40; loss: 1.33; acc: 0.84
Batch: 60; loss: 1.62; acc: 0.64
Batch: 80; loss: 1.54; acc: 0.53
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.68; acc: 0.59
Batch: 140; loss: 1.5; acc: 0.69
Val Epoch over. val_loss: 1.6115327390136234; val_accuracy: 0.5687699044585988 

The current subspace-distance is: 1.088619956135517e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.53
Batch: 20; loss: 1.58; acc: 0.53
Batch: 40; loss: 1.59; acc: 0.58
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.64; acc: 0.56
Batch: 100; loss: 1.71; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.69
Batch: 160; loss: 1.65; acc: 0.56
Batch: 180; loss: 1.75; acc: 0.47
Batch: 200; loss: 1.75; acc: 0.41
Batch: 220; loss: 1.83; acc: 0.41
Batch: 240; loss: 1.67; acc: 0.52
Batch: 260; loss: 1.68; acc: 0.47
Batch: 280; loss: 1.6; acc: 0.58
Batch: 300; loss: 1.63; acc: 0.48
Batch: 320; loss: 1.54; acc: 0.62
Batch: 340; loss: 1.56; acc: 0.53
Batch: 360; loss: 1.73; acc: 0.45
Batch: 380; loss: 1.61; acc: 0.55
Batch: 400; loss: 1.6; acc: 0.61
Batch: 420; loss: 1.74; acc: 0.45
Batch: 440; loss: 1.55; acc: 0.55
Batch: 460; loss: 1.65; acc: 0.55
Batch: 480; loss: 1.63; acc: 0.67
Batch: 500; loss: 1.57; acc: 0.58
Batch: 520; loss: 1.61; acc: 0.53
Batch: 540; loss: 1.59; acc: 0.55
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.66; acc: 0.55
Batch: 600; loss: 1.7; acc: 0.48
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.44; acc: 0.59
Batch: 660; loss: 1.63; acc: 0.5
Batch: 680; loss: 1.81; acc: 0.48
Batch: 700; loss: 1.68; acc: 0.5
Batch: 720; loss: 1.74; acc: 0.5
Batch: 740; loss: 1.61; acc: 0.59
Batch: 760; loss: 1.54; acc: 0.56
Batch: 780; loss: 1.69; acc: 0.56
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.47746281477157e-05
1.1220490705454722e-05
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.68; acc: 0.5
Batch: 40; loss: 1.34; acc: 0.84
Batch: 60; loss: 1.61; acc: 0.61
Batch: 80; loss: 1.5; acc: 0.61
Batch: 100; loss: 1.54; acc: 0.56
Batch: 120; loss: 1.67; acc: 0.58
Batch: 140; loss: 1.49; acc: 0.73
Val Epoch over. val_loss: 1.5942993923357338; val_accuracy: 0.5725517515923567 

The current subspace-distance is: 1.1220490705454722e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.59
Batch: 20; loss: 1.6; acc: 0.62
Batch: 40; loss: 1.7; acc: 0.47
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.48; acc: 0.69
Batch: 100; loss: 1.61; acc: 0.59
Batch: 120; loss: 1.68; acc: 0.56
Batch: 140; loss: 1.69; acc: 0.48
Batch: 160; loss: 1.58; acc: 0.58
Batch: 180; loss: 1.71; acc: 0.55
Batch: 200; loss: 1.66; acc: 0.5
Batch: 220; loss: 1.53; acc: 0.59
Batch: 240; loss: 1.51; acc: 0.67
Batch: 260; loss: 1.6; acc: 0.52
Batch: 280; loss: 1.49; acc: 0.64
Batch: 300; loss: 1.66; acc: 0.48
Batch: 320; loss: 1.56; acc: 0.62
Batch: 340; loss: 1.67; acc: 0.53
Batch: 360; loss: 1.61; acc: 0.56
Batch: 380; loss: 1.64; acc: 0.5
Batch: 400; loss: 1.56; acc: 0.61
Batch: 420; loss: 1.67; acc: 0.55
Batch: 440; loss: 1.76; acc: 0.53
Batch: 460; loss: 1.71; acc: 0.45
Batch: 480; loss: 1.65; acc: 0.55
Batch: 500; loss: 1.74; acc: 0.55
Batch: 520; loss: 1.78; acc: 0.45
Batch: 540; loss: 1.6; acc: 0.56
Batch: 560; loss: 1.66; acc: 0.45
Batch: 580; loss: 1.67; acc: 0.52
Batch: 600; loss: 1.59; acc: 0.61
Batch: 620; loss: 1.67; acc: 0.5
Batch: 640; loss: 1.73; acc: 0.47
Batch: 660; loss: 1.63; acc: 0.56
Batch: 680; loss: 1.61; acc: 0.52
Batch: 700; loss: 1.67; acc: 0.55
Batch: 720; loss: 1.65; acc: 0.58
Batch: 740; loss: 1.6; acc: 0.55
Batch: 760; loss: 1.51; acc: 0.64
Batch: 780; loss: 1.52; acc: 0.55
Train Epoch over. train_loss: 1.62; train_accuracy: 0.55 

3.657431807368994e-05
1.3370158740144689e-05
Batch: 0; loss: 1.66; acc: 0.56
Batch: 20; loss: 1.66; acc: 0.47
Batch: 40; loss: 1.34; acc: 0.81
Batch: 60; loss: 1.6; acc: 0.62
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.52; acc: 0.64
Batch: 120; loss: 1.67; acc: 0.58
Batch: 140; loss: 1.46; acc: 0.7
Val Epoch over. val_loss: 1.585457196660862; val_accuracy: 0.5786226114649682 

The current subspace-distance is: 1.3370158740144689e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 1.6; acc: 0.58
Batch: 60; loss: 1.62; acc: 0.58
Batch: 80; loss: 1.69; acc: 0.47
Batch: 100; loss: 1.7; acc: 0.45
Batch: 120; loss: 1.55; acc: 0.62
Batch: 140; loss: 1.68; acc: 0.56
Batch: 160; loss: 1.64; acc: 0.58
Batch: 180; loss: 1.56; acc: 0.59
Batch: 200; loss: 1.61; acc: 0.52
Batch: 220; loss: 1.7; acc: 0.47
Batch: 240; loss: 1.47; acc: 0.64
Batch: 260; loss: 1.64; acc: 0.5
Batch: 280; loss: 1.67; acc: 0.53
Batch: 300; loss: 1.64; acc: 0.45
Batch: 320; loss: 1.52; acc: 0.58
Batch: 340; loss: 1.63; acc: 0.58
Batch: 360; loss: 1.65; acc: 0.53
Batch: 380; loss: 1.73; acc: 0.44
Batch: 400; loss: 1.57; acc: 0.58
Batch: 420; loss: 1.67; acc: 0.55
Batch: 440; loss: 1.6; acc: 0.5
Batch: 460; loss: 1.6; acc: 0.59
Batch: 480; loss: 1.56; acc: 0.62
Batch: 500; loss: 1.6; acc: 0.61
Batch: 520; loss: 1.56; acc: 0.5
Batch: 540; loss: 1.73; acc: 0.45
Batch: 560; loss: 1.61; acc: 0.59
Batch: 580; loss: 1.72; acc: 0.52
Batch: 600; loss: 1.69; acc: 0.47
Batch: 620; loss: 1.62; acc: 0.58
Batch: 640; loss: 1.63; acc: 0.53
Batch: 660; loss: 1.62; acc: 0.55
Batch: 680; loss: 1.66; acc: 0.52
Batch: 700; loss: 1.58; acc: 0.56
Batch: 720; loss: 1.52; acc: 0.59
Batch: 740; loss: 1.66; acc: 0.48
Batch: 760; loss: 1.58; acc: 0.52
Batch: 780; loss: 1.87; acc: 0.38
Train Epoch over. train_loss: 1.61; train_accuracy: 0.55 

3.6305002140579745e-05
1.3496342944563366e-05
Batch: 0; loss: 1.65; acc: 0.55
Batch: 20; loss: 1.64; acc: 0.5
Batch: 40; loss: 1.33; acc: 0.73
Batch: 60; loss: 1.57; acc: 0.64
Batch: 80; loss: 1.44; acc: 0.59
Batch: 100; loss: 1.46; acc: 0.64
Batch: 120; loss: 1.65; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.73
Val Epoch over. val_loss: 1.5632356435629973; val_accuracy: 0.5904657643312102 

The current subspace-distance is: 1.3496342944563366e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.52
Batch: 20; loss: 1.56; acc: 0.61
Batch: 40; loss: 1.59; acc: 0.56
Batch: 60; loss: 1.65; acc: 0.53
Batch: 80; loss: 1.5; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.47
Batch: 120; loss: 1.51; acc: 0.61
Batch: 140; loss: 1.48; acc: 0.58
Batch: 160; loss: 1.48; acc: 0.66
Batch: 180; loss: 1.52; acc: 0.58
Batch: 200; loss: 1.66; acc: 0.5
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.49; acc: 0.64
Batch: 260; loss: 1.46; acc: 0.67
Batch: 280; loss: 1.61; acc: 0.61
Batch: 300; loss: 1.58; acc: 0.53
Batch: 320; loss: 1.68; acc: 0.52
Batch: 340; loss: 1.64; acc: 0.52
Batch: 360; loss: 1.6; acc: 0.59
Batch: 380; loss: 1.54; acc: 0.56
Batch: 400; loss: 1.55; acc: 0.55
Batch: 420; loss: 1.65; acc: 0.52
Batch: 440; loss: 1.63; acc: 0.59
Batch: 460; loss: 1.5; acc: 0.62
Batch: 480; loss: 1.56; acc: 0.59
Batch: 500; loss: 1.53; acc: 0.58
Batch: 520; loss: 1.65; acc: 0.55
Batch: 540; loss: 1.71; acc: 0.48
Batch: 560; loss: 1.54; acc: 0.55
Batch: 580; loss: 1.53; acc: 0.66
Batch: 600; loss: 1.53; acc: 0.58
Batch: 620; loss: 1.65; acc: 0.52
Batch: 640; loss: 1.71; acc: 0.48
Batch: 660; loss: 1.71; acc: 0.53
Batch: 680; loss: 1.71; acc: 0.53
Batch: 700; loss: 1.58; acc: 0.52
Batch: 720; loss: 1.68; acc: 0.5
Batch: 740; loss: 1.54; acc: 0.56
Batch: 760; loss: 1.51; acc: 0.62
Batch: 780; loss: 1.53; acc: 0.61
Train Epoch over. train_loss: 1.6; train_accuracy: 0.56 

3.6868419556412846e-05
1.1501511835376732e-05
Batch: 0; loss: 1.67; acc: 0.58
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.33; acc: 0.77
Batch: 60; loss: 1.57; acc: 0.61
Batch: 80; loss: 1.43; acc: 0.61
Batch: 100; loss: 1.48; acc: 0.64
Batch: 120; loss: 1.66; acc: 0.56
Batch: 140; loss: 1.44; acc: 0.67
Val Epoch over. val_loss: 1.567962515885663; val_accuracy: 0.5878781847133758 

The current subspace-distance is: 1.1501511835376732e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.61; acc: 0.58
Batch: 40; loss: 1.58; acc: 0.58
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.58; acc: 0.59
Batch: 100; loss: 1.64; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.55
Batch: 140; loss: 1.67; acc: 0.52
Batch: 160; loss: 1.73; acc: 0.48
Batch: 180; loss: 1.73; acc: 0.56
Batch: 200; loss: 1.57; acc: 0.59
Batch: 220; loss: 1.64; acc: 0.47
Batch: 240; loss: 1.59; acc: 0.59
Batch: 260; loss: 1.53; acc: 0.59
Batch: 280; loss: 1.73; acc: 0.48
Batch: 300; loss: 1.54; acc: 0.58
Batch: 320; loss: 1.69; acc: 0.48
Batch: 340; loss: 1.53; acc: 0.61
Batch: 360; loss: 1.61; acc: 0.55
Batch: 380; loss: 1.7; acc: 0.48
Batch: 400; loss: 1.56; acc: 0.55
Batch: 420; loss: 1.51; acc: 0.66
Batch: 440; loss: 1.63; acc: 0.5
Batch: 460; loss: 1.7; acc: 0.47
Batch: 480; loss: 1.5; acc: 0.62
Batch: 500; loss: 1.67; acc: 0.47
Batch: 520; loss: 1.59; acc: 0.58
Batch: 540; loss: 1.58; acc: 0.52
Batch: 560; loss: 1.61; acc: 0.55
Batch: 580; loss: 1.64; acc: 0.55
Batch: 600; loss: 1.53; acc: 0.61
Batch: 620; loss: 1.6; acc: 0.59
Batch: 640; loss: 1.73; acc: 0.47
Batch: 660; loss: 1.52; acc: 0.61
Batch: 680; loss: 1.51; acc: 0.64
Batch: 700; loss: 1.62; acc: 0.52
Batch: 720; loss: 1.6; acc: 0.56
Batch: 740; loss: 1.56; acc: 0.59
Batch: 760; loss: 1.62; acc: 0.55
Batch: 780; loss: 1.66; acc: 0.5
Train Epoch over. train_loss: 1.6; train_accuracy: 0.56 

3.713558544404805e-05
1.0926822142209858e-05
Batch: 0; loss: 1.67; acc: 0.5
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.73
Batch: 60; loss: 1.55; acc: 0.62
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.66
Batch: 120; loss: 1.66; acc: 0.58
Batch: 140; loss: 1.43; acc: 0.69
Val Epoch over. val_loss: 1.5605428112540276; val_accuracy: 0.5876791401273885 

The current subspace-distance is: 1.0926822142209858e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.44
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.53; acc: 0.64
Batch: 60; loss: 1.63; acc: 0.55
Batch: 80; loss: 1.57; acc: 0.62
Batch: 100; loss: 1.51; acc: 0.56
Batch: 120; loss: 1.6; acc: 0.48
Batch: 140; loss: 1.52; acc: 0.64
Batch: 160; loss: 1.62; acc: 0.55
Batch: 180; loss: 1.53; acc: 0.59
Batch: 200; loss: 1.7; acc: 0.5
Batch: 220; loss: 1.64; acc: 0.56
Batch: 240; loss: 1.67; acc: 0.5
Batch: 260; loss: 1.63; acc: 0.55
Batch: 280; loss: 1.74; acc: 0.41
Batch: 300; loss: 1.5; acc: 0.58
Batch: 320; loss: 1.59; acc: 0.56
Batch: 340; loss: 1.69; acc: 0.5
Batch: 360; loss: 1.63; acc: 0.61
Batch: 380; loss: 1.71; acc: 0.55
Batch: 400; loss: 1.72; acc: 0.48
Batch: 420; loss: 1.66; acc: 0.59
Batch: 440; loss: 1.6; acc: 0.55
Batch: 460; loss: 1.63; acc: 0.55
Batch: 480; loss: 1.67; acc: 0.5
Batch: 500; loss: 1.72; acc: 0.47
Batch: 520; loss: 1.53; acc: 0.59
Batch: 540; loss: 1.62; acc: 0.53
Batch: 560; loss: 1.6; acc: 0.55
Batch: 580; loss: 1.69; acc: 0.55
Batch: 600; loss: 1.47; acc: 0.56
Batch: 620; loss: 1.63; acc: 0.55
Batch: 640; loss: 1.51; acc: 0.53
Batch: 660; loss: 1.64; acc: 0.52
Batch: 680; loss: 1.54; acc: 0.61
Batch: 700; loss: 1.58; acc: 0.58
Batch: 720; loss: 1.44; acc: 0.67
Batch: 740; loss: 1.58; acc: 0.58
Batch: 760; loss: 1.57; acc: 0.56
Batch: 780; loss: 1.66; acc: 0.52
Train Epoch over. train_loss: 1.59; train_accuracy: 0.56 

3.7283796700648963e-05
1.2381153283058666e-05
Batch: 0; loss: 1.66; acc: 0.55
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.31; acc: 0.75
Batch: 60; loss: 1.54; acc: 0.62
Batch: 80; loss: 1.41; acc: 0.62
Batch: 100; loss: 1.43; acc: 0.69
Batch: 120; loss: 1.63; acc: 0.59
Batch: 140; loss: 1.42; acc: 0.69
Val Epoch over. val_loss: 1.5448674411530707; val_accuracy: 0.5920581210191083 

The current subspace-distance is: 1.2381153283058666e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.56
Batch: 40; loss: 1.73; acc: 0.56
Batch: 60; loss: 1.51; acc: 0.62
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.64; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.5
Batch: 140; loss: 1.55; acc: 0.59
Batch: 160; loss: 1.77; acc: 0.52
Batch: 180; loss: 1.67; acc: 0.5
Batch: 200; loss: 1.59; acc: 0.59
Batch: 220; loss: 1.58; acc: 0.58
Batch: 240; loss: 1.57; acc: 0.58
Batch: 260; loss: 1.59; acc: 0.5
Batch: 280; loss: 1.55; acc: 0.58
Batch: 300; loss: 1.64; acc: 0.58
Batch: 320; loss: 1.62; acc: 0.58
Batch: 340; loss: 1.51; acc: 0.66
Batch: 360; loss: 1.6; acc: 0.59
Batch: 380; loss: 1.59; acc: 0.52
Batch: 400; loss: 1.61; acc: 0.53
Batch: 420; loss: 1.54; acc: 0.64
Batch: 440; loss: 1.53; acc: 0.64
Batch: 460; loss: 1.56; acc: 0.58
Batch: 480; loss: 1.61; acc: 0.64
Batch: 500; loss: 1.65; acc: 0.53
Batch: 520; loss: 1.56; acc: 0.64
Batch: 540; loss: 1.65; acc: 0.48
Batch: 560; loss: 1.48; acc: 0.66
Batch: 580; loss: 1.63; acc: 0.56
Batch: 600; loss: 1.6; acc: 0.59
Batch: 620; loss: 1.47; acc: 0.67
Batch: 640; loss: 1.61; acc: 0.5
Batch: 660; loss: 1.56; acc: 0.56
Batch: 680; loss: 1.61; acc: 0.53
Batch: 700; loss: 1.61; acc: 0.53
Batch: 720; loss: 1.54; acc: 0.58
Batch: 740; loss: 1.56; acc: 0.55
Batch: 760; loss: 1.48; acc: 0.59
Batch: 780; loss: 1.58; acc: 0.5
Train Epoch over. train_loss: 1.59; train_accuracy: 0.57 

3.747397204278968e-05
1.3941395991423633e-05
Batch: 0; loss: 1.67; acc: 0.52
Batch: 20; loss: 1.63; acc: 0.53
Batch: 40; loss: 1.31; acc: 0.78
Batch: 60; loss: 1.54; acc: 0.59
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.43; acc: 0.69
Batch: 120; loss: 1.64; acc: 0.58
Batch: 140; loss: 1.41; acc: 0.69
Val Epoch over. val_loss: 1.5472997647182198; val_accuracy: 0.5976313694267515 

The current subspace-distance is: 1.3941395991423633e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.61; acc: 0.52
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.73; acc: 0.44
Batch: 100; loss: 1.53; acc: 0.53
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.62; acc: 0.45
Batch: 160; loss: 1.54; acc: 0.59
Batch: 180; loss: 1.57; acc: 0.5
Batch: 200; loss: 1.69; acc: 0.47
Batch: 220; loss: 1.57; acc: 0.53
Batch: 240; loss: 1.51; acc: 0.67
Batch: 260; loss: 1.62; acc: 0.47
Batch: 280; loss: 1.53; acc: 0.58
Batch: 300; loss: 1.56; acc: 0.66
Batch: 320; loss: 1.56; acc: 0.55
Batch: 340; loss: 1.68; acc: 0.56
Batch: 360; loss: 1.59; acc: 0.55
Batch: 380; loss: 1.54; acc: 0.58
Batch: 400; loss: 1.5; acc: 0.61
Batch: 420; loss: 1.5; acc: 0.67
Batch: 440; loss: 1.64; acc: 0.53
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.55; acc: 0.55
Batch: 500; loss: 1.45; acc: 0.66
Batch: 520; loss: 1.53; acc: 0.66
Batch: 540; loss: 1.56; acc: 0.59
Batch: 560; loss: 1.54; acc: 0.61
Batch: 580; loss: 1.61; acc: 0.55
Batch: 600; loss: 1.65; acc: 0.5
Batch: 620; loss: 1.6; acc: 0.53
Batch: 640; loss: 1.56; acc: 0.5
Batch: 660; loss: 1.69; acc: 0.5
Batch: 680; loss: 1.48; acc: 0.59
Batch: 700; loss: 1.69; acc: 0.45
Batch: 720; loss: 1.54; acc: 0.58
Batch: 740; loss: 1.72; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.58
Batch: 780; loss: 1.47; acc: 0.66
Train Epoch over. train_loss: 1.58; train_accuracy: 0.57 

3.770496550714597e-05
1.23226391224307e-05
Batch: 0; loss: 1.69; acc: 0.48
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.31; acc: 0.77
Batch: 60; loss: 1.54; acc: 0.66
Batch: 80; loss: 1.41; acc: 0.67
Batch: 100; loss: 1.44; acc: 0.69
Batch: 120; loss: 1.65; acc: 0.55
Batch: 140; loss: 1.41; acc: 0.72
Val Epoch over. val_loss: 1.5488752643014216; val_accuracy: 0.5938495222929936 

The current subspace-distance is: 1.23226391224307e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.57; acc: 0.56
Batch: 20; loss: 1.54; acc: 0.48
Batch: 40; loss: 1.61; acc: 0.55
Batch: 60; loss: 1.74; acc: 0.47
Batch: 80; loss: 1.68; acc: 0.48
Batch: 100; loss: 1.58; acc: 0.53
Batch: 120; loss: 1.62; acc: 0.47
Batch: 140; loss: 1.48; acc: 0.61
Batch: 160; loss: 1.54; acc: 0.64
Batch: 180; loss: 1.52; acc: 0.59
Batch: 200; loss: 1.61; acc: 0.58
Batch: 220; loss: 1.59; acc: 0.61
Batch: 240; loss: 1.57; acc: 0.56
Batch: 260; loss: 1.63; acc: 0.52
Batch: 280; loss: 1.76; acc: 0.47
Batch: 300; loss: 1.49; acc: 0.59
Batch: 320; loss: 1.54; acc: 0.64
Batch: 340; loss: 1.45; acc: 0.67
Batch: 360; loss: 1.55; acc: 0.62
Batch: 380; loss: 1.67; acc: 0.44
Batch: 400; loss: 1.66; acc: 0.55
Batch: 420; loss: 1.59; acc: 0.59
Batch: 440; loss: 1.65; acc: 0.55
Batch: 460; loss: 1.6; acc: 0.53
Batch: 480; loss: 1.5; acc: 0.59
Batch: 500; loss: 1.49; acc: 0.61
Batch: 520; loss: 1.53; acc: 0.58
Batch: 540; loss: 1.58; acc: 0.56
Batch: 560; loss: 1.59; acc: 0.52
Batch: 580; loss: 1.65; acc: 0.53
Batch: 600; loss: 1.47; acc: 0.66
Batch: 620; loss: 1.64; acc: 0.48
Batch: 640; loss: 1.55; acc: 0.56
Batch: 660; loss: 1.64; acc: 0.48
Batch: 680; loss: 1.58; acc: 0.56
Batch: 700; loss: 1.66; acc: 0.58
Batch: 720; loss: 1.5; acc: 0.61
Batch: 740; loss: 1.44; acc: 0.67
Batch: 760; loss: 1.61; acc: 0.59
Batch: 780; loss: 1.6; acc: 0.61
Train Epoch over. train_loss: 1.58; train_accuracy: 0.57 

3.8613376091234386e-05
1.32999875859241e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.63; acc: 0.53
Batch: 40; loss: 1.31; acc: 0.83
Batch: 60; loss: 1.53; acc: 0.62
Batch: 80; loss: 1.39; acc: 0.67
Batch: 100; loss: 1.45; acc: 0.66
Batch: 120; loss: 1.66; acc: 0.61
Batch: 140; loss: 1.41; acc: 0.73
Val Epoch over. val_loss: 1.5497257474121775; val_accuracy: 0.5902667197452229 

The current subspace-distance is: 1.32999875859241e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.56; acc: 0.5
Batch: 40; loss: 1.46; acc: 0.67
Batch: 60; loss: 1.63; acc: 0.61
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.59; acc: 0.52
Batch: 120; loss: 1.59; acc: 0.45
Batch: 140; loss: 1.61; acc: 0.58
Batch: 160; loss: 1.61; acc: 0.61
Batch: 180; loss: 1.66; acc: 0.55
Batch: 200; loss: 1.59; acc: 0.56
Batch: 220; loss: 1.62; acc: 0.53
Batch: 240; loss: 1.59; acc: 0.59
Batch: 260; loss: 1.46; acc: 0.62
Batch: 280; loss: 1.59; acc: 0.5
Batch: 300; loss: 1.58; acc: 0.55
Batch: 320; loss: 1.58; acc: 0.58
Batch: 340; loss: 1.67; acc: 0.52
Batch: 360; loss: 1.67; acc: 0.5
Batch: 380; loss: 1.58; acc: 0.59
Batch: 400; loss: 1.6; acc: 0.55
Batch: 420; loss: 1.45; acc: 0.64
Batch: 440; loss: 1.44; acc: 0.59
Batch: 460; loss: 1.62; acc: 0.58
Batch: 480; loss: 1.54; acc: 0.64
Batch: 500; loss: 1.49; acc: 0.7
Batch: 520; loss: 1.65; acc: 0.53
Batch: 540; loss: 1.59; acc: 0.58
Batch: 560; loss: 1.47; acc: 0.64
Batch: 580; loss: 1.56; acc: 0.58
Batch: 600; loss: 1.65; acc: 0.48
Batch: 620; loss: 1.7; acc: 0.41
Batch: 640; loss: 1.59; acc: 0.53
Batch: 660; loss: 1.55; acc: 0.61
Batch: 680; loss: 1.49; acc: 0.62
Batch: 700; loss: 1.73; acc: 0.53
Batch: 720; loss: 1.65; acc: 0.48
Batch: 740; loss: 1.55; acc: 0.59
Batch: 760; loss: 1.72; acc: 0.5
Batch: 780; loss: 1.74; acc: 0.44
Train Epoch over. train_loss: 1.58; train_accuracy: 0.57 

3.855941031360999e-05
1.348467412753962e-05
Batch: 0; loss: 1.71; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.31; acc: 0.78
Batch: 60; loss: 1.52; acc: 0.62
Batch: 80; loss: 1.39; acc: 0.66
Batch: 100; loss: 1.44; acc: 0.69
Batch: 120; loss: 1.66; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.72
Val Epoch over. val_loss: 1.551320097249025; val_accuracy: 0.5857882165605095 

The current subspace-distance is: 1.348467412753962e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.61; acc: 0.58
Batch: 40; loss: 1.57; acc: 0.55
Batch: 60; loss: 1.45; acc: 0.61
Batch: 80; loss: 1.56; acc: 0.61
Batch: 100; loss: 1.45; acc: 0.66
Batch: 120; loss: 1.61; acc: 0.56
Batch: 140; loss: 1.53; acc: 0.55
Batch: 160; loss: 1.67; acc: 0.53
Batch: 180; loss: 1.47; acc: 0.66
Batch: 200; loss: 1.61; acc: 0.56
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.66; acc: 0.53
Batch: 260; loss: 1.56; acc: 0.56
Batch: 280; loss: 1.57; acc: 0.53
Batch: 300; loss: 1.55; acc: 0.59
Batch: 320; loss: 1.45; acc: 0.67
Batch: 340; loss: 1.4; acc: 0.75
Batch: 360; loss: 1.54; acc: 0.5
Batch: 380; loss: 1.66; acc: 0.48
Batch: 400; loss: 1.65; acc: 0.52
Batch: 420; loss: 1.52; acc: 0.62
Batch: 440; loss: 1.52; acc: 0.59
Batch: 460; loss: 1.6; acc: 0.58
Batch: 480; loss: 1.38; acc: 0.66
Batch: 500; loss: 1.53; acc: 0.62
Batch: 520; loss: 1.51; acc: 0.66
Batch: 540; loss: 1.6; acc: 0.56
Batch: 560; loss: 1.59; acc: 0.56
Batch: 580; loss: 1.43; acc: 0.69
Batch: 600; loss: 1.61; acc: 0.5
Batch: 620; loss: 1.58; acc: 0.55
Batch: 640; loss: 1.76; acc: 0.44
Batch: 660; loss: 1.56; acc: 0.53
Batch: 680; loss: 1.41; acc: 0.69
Batch: 700; loss: 1.64; acc: 0.52
Batch: 720; loss: 1.62; acc: 0.53
Batch: 740; loss: 1.53; acc: 0.58
Batch: 760; loss: 1.62; acc: 0.5
Batch: 780; loss: 1.48; acc: 0.66
Train Epoch over. train_loss: 1.57; train_accuracy: 0.57 

3.9648173697059974e-05
1.557570976729039e-05
Batch: 0; loss: 1.7; acc: 0.42
Batch: 20; loss: 1.63; acc: 0.56
Batch: 40; loss: 1.31; acc: 0.73
Batch: 60; loss: 1.51; acc: 0.64
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.42; acc: 0.72
Batch: 120; loss: 1.64; acc: 0.58
Batch: 140; loss: 1.39; acc: 0.73
Val Epoch over. val_loss: 1.5349821550830913; val_accuracy: 0.5895700636942676 

The current subspace-distance is: 1.557570976729039e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.56
Batch: 20; loss: 1.53; acc: 0.62
Batch: 40; loss: 1.58; acc: 0.58
Batch: 60; loss: 1.86; acc: 0.33
Batch: 80; loss: 1.38; acc: 0.66
Batch: 100; loss: 1.7; acc: 0.45
Batch: 120; loss: 1.55; acc: 0.59
Batch: 140; loss: 1.59; acc: 0.62
Batch: 160; loss: 1.57; acc: 0.58
Batch: 180; loss: 1.6; acc: 0.53
Batch: 200; loss: 1.57; acc: 0.56
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.62; acc: 0.53
Batch: 260; loss: 1.42; acc: 0.62
Batch: 280; loss: 1.59; acc: 0.48
Batch: 300; loss: 1.59; acc: 0.53
Batch: 320; loss: 1.63; acc: 0.52
Batch: 340; loss: 1.34; acc: 0.66
Batch: 360; loss: 1.49; acc: 0.61
Batch: 380; loss: 1.57; acc: 0.58
Batch: 400; loss: 1.52; acc: 0.62
Batch: 420; loss: 1.49; acc: 0.58
Batch: 440; loss: 1.55; acc: 0.58
Batch: 460; loss: 1.71; acc: 0.5
Batch: 480; loss: 1.83; acc: 0.41
Batch: 500; loss: 1.61; acc: 0.62
Batch: 520; loss: 1.56; acc: 0.61
Batch: 540; loss: 1.52; acc: 0.62
Batch: 560; loss: 1.58; acc: 0.61
Batch: 580; loss: 1.68; acc: 0.45
Batch: 600; loss: 1.48; acc: 0.64
Batch: 620; loss: 1.67; acc: 0.5
Batch: 640; loss: 1.61; acc: 0.56
Batch: 660; loss: 1.6; acc: 0.64
Batch: 680; loss: 1.54; acc: 0.69
Batch: 700; loss: 1.66; acc: 0.58
Batch: 720; loss: 1.66; acc: 0.5
Batch: 740; loss: 1.65; acc: 0.48
Batch: 760; loss: 1.57; acc: 0.55
Batch: 780; loss: 1.47; acc: 0.58
Train Epoch over. train_loss: 1.57; train_accuracy: 0.57 

4.00162061851006e-05
1.472244275646517e-05
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.53
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.49; acc: 0.62
Batch: 80; loss: 1.37; acc: 0.66
Batch: 100; loss: 1.4; acc: 0.72
Batch: 120; loss: 1.61; acc: 0.59
Batch: 140; loss: 1.38; acc: 0.73
Val Epoch over. val_loss: 1.519999323377184; val_accuracy: 0.5974323248407644 

The current subspace-distance is: 1.472244275646517e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.73; acc: 0.39
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.54; acc: 0.58
Batch: 60; loss: 1.5; acc: 0.62
Batch: 80; loss: 1.55; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.61
Batch: 140; loss: 1.47; acc: 0.64
Batch: 160; loss: 1.63; acc: 0.48
Batch: 180; loss: 1.47; acc: 0.62
Batch: 200; loss: 1.61; acc: 0.55
Batch: 220; loss: 1.49; acc: 0.58
Batch: 240; loss: 1.52; acc: 0.56
Batch: 260; loss: 1.71; acc: 0.44
Batch: 280; loss: 1.62; acc: 0.52
Batch: 300; loss: 1.64; acc: 0.53
Batch: 320; loss: 1.6; acc: 0.61
Batch: 340; loss: 1.57; acc: 0.55
Batch: 360; loss: 1.62; acc: 0.56
Batch: 380; loss: 1.5; acc: 0.56
Batch: 400; loss: 1.56; acc: 0.56
Batch: 420; loss: 1.61; acc: 0.56
Batch: 440; loss: 1.53; acc: 0.59
Batch: 460; loss: 1.53; acc: 0.59
Batch: 480; loss: 1.54; acc: 0.58
Batch: 500; loss: 1.53; acc: 0.61
Batch: 520; loss: 1.58; acc: 0.59
Batch: 540; loss: 1.46; acc: 0.66
Batch: 560; loss: 1.56; acc: 0.62
Batch: 580; loss: 1.61; acc: 0.48
Batch: 600; loss: 1.45; acc: 0.61
Batch: 620; loss: 1.57; acc: 0.52
Batch: 640; loss: 1.68; acc: 0.52
Batch: 660; loss: 1.7; acc: 0.52
Batch: 680; loss: 1.67; acc: 0.45
Batch: 700; loss: 1.52; acc: 0.53
Batch: 720; loss: 1.58; acc: 0.55
Batch: 740; loss: 1.48; acc: 0.61
Batch: 760; loss: 1.57; acc: 0.61
Batch: 780; loss: 1.49; acc: 0.62
Train Epoch over. train_loss: 1.57; train_accuracy: 0.57 

4.072078809258528e-05
1.5799862012499943e-05
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.62; acc: 0.53
Batch: 40; loss: 1.3; acc: 0.75
Batch: 60; loss: 1.49; acc: 0.62
Batch: 80; loss: 1.37; acc: 0.67
Batch: 100; loss: 1.41; acc: 0.72
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.38; acc: 0.75
Val Epoch over. val_loss: 1.5238122879319889; val_accuracy: 0.5931528662420382 

The current subspace-distance is: 1.5799862012499943e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.58
Batch: 20; loss: 1.49; acc: 0.64
Batch: 40; loss: 1.62; acc: 0.58
Batch: 60; loss: 1.59; acc: 0.58
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.57; acc: 0.53
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.49; acc: 0.66
Batch: 160; loss: 1.64; acc: 0.59
Batch: 180; loss: 1.51; acc: 0.61
Batch: 200; loss: 1.63; acc: 0.52
Batch: 220; loss: 1.47; acc: 0.61
Batch: 240; loss: 1.57; acc: 0.61
Batch: 260; loss: 1.58; acc: 0.53
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.54; acc: 0.55
Batch: 320; loss: 1.58; acc: 0.55
Batch: 340; loss: 1.56; acc: 0.59
Batch: 360; loss: 1.6; acc: 0.53
Batch: 380; loss: 1.61; acc: 0.56
Batch: 400; loss: 1.67; acc: 0.53
Batch: 420; loss: 1.59; acc: 0.58
Batch: 440; loss: 1.56; acc: 0.55
Batch: 460; loss: 1.61; acc: 0.53
Batch: 480; loss: 1.56; acc: 0.58
Batch: 500; loss: 1.55; acc: 0.59
Batch: 520; loss: 1.46; acc: 0.64
Batch: 540; loss: 1.76; acc: 0.42
Batch: 560; loss: 1.67; acc: 0.5
Batch: 580; loss: 1.5; acc: 0.56
Batch: 600; loss: 1.49; acc: 0.56
Batch: 620; loss: 1.45; acc: 0.66
Batch: 640; loss: 1.53; acc: 0.56
Batch: 660; loss: 1.52; acc: 0.61
Batch: 680; loss: 1.53; acc: 0.55
Batch: 700; loss: 1.6; acc: 0.5
Batch: 720; loss: 1.61; acc: 0.58
Batch: 740; loss: 1.57; acc: 0.45
Batch: 760; loss: 1.69; acc: 0.45
Batch: 780; loss: 1.64; acc: 0.52
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

4.053455268149264e-05
1.4887275938235689e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.49; acc: 0.62
Batch: 80; loss: 1.39; acc: 0.64
Batch: 100; loss: 1.41; acc: 0.7
Batch: 120; loss: 1.62; acc: 0.56
Batch: 140; loss: 1.39; acc: 0.75
Val Epoch over. val_loss: 1.5232022939973575; val_accuracy: 0.5934514331210191 

The current subspace-distance is: 1.4887275938235689e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.53
Batch: 20; loss: 1.57; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.61
Batch: 60; loss: 1.52; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.67
Batch: 100; loss: 1.54; acc: 0.55
Batch: 120; loss: 1.65; acc: 0.55
Batch: 140; loss: 1.52; acc: 0.64
Batch: 160; loss: 1.56; acc: 0.53
Batch: 180; loss: 1.53; acc: 0.53
Batch: 200; loss: 1.52; acc: 0.56
Batch: 220; loss: 1.46; acc: 0.62
Batch: 240; loss: 1.58; acc: 0.61
Batch: 260; loss: 1.5; acc: 0.72
Batch: 280; loss: 1.48; acc: 0.52
Batch: 300; loss: 1.49; acc: 0.64
Batch: 320; loss: 1.43; acc: 0.69
Batch: 340; loss: 1.61; acc: 0.52
Batch: 360; loss: 1.58; acc: 0.56
Batch: 380; loss: 1.59; acc: 0.56
Batch: 400; loss: 1.43; acc: 0.62
Batch: 420; loss: 1.6; acc: 0.52
Batch: 440; loss: 1.58; acc: 0.53
Batch: 460; loss: 1.63; acc: 0.53
Batch: 480; loss: 1.53; acc: 0.58
Batch: 500; loss: 1.57; acc: 0.58
Batch: 520; loss: 1.55; acc: 0.55
Batch: 540; loss: 1.6; acc: 0.52
Batch: 560; loss: 1.56; acc: 0.55
Batch: 580; loss: 1.52; acc: 0.55
Batch: 600; loss: 1.54; acc: 0.55
Batch: 620; loss: 1.63; acc: 0.56
Batch: 640; loss: 1.43; acc: 0.69
Batch: 660; loss: 1.5; acc: 0.56
Batch: 680; loss: 1.65; acc: 0.45
Batch: 700; loss: 1.57; acc: 0.61
Batch: 720; loss: 1.52; acc: 0.56
Batch: 740; loss: 1.45; acc: 0.67
Batch: 760; loss: 1.67; acc: 0.56
Batch: 780; loss: 1.53; acc: 0.56
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

4.096357224625535e-05
1.4842222299193963e-05
Batch: 0; loss: 1.7; acc: 0.48
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.31; acc: 0.75
Batch: 60; loss: 1.49; acc: 0.62
Batch: 80; loss: 1.38; acc: 0.66
Batch: 100; loss: 1.4; acc: 0.7
Batch: 120; loss: 1.61; acc: 0.55
Batch: 140; loss: 1.38; acc: 0.73
Val Epoch over. val_loss: 1.5235793795555261; val_accuracy: 0.5949442675159236 

The current subspace-distance is: 1.4842222299193963e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.77; acc: 0.47
Batch: 20; loss: 1.66; acc: 0.58
Batch: 40; loss: 1.53; acc: 0.58
Batch: 60; loss: 1.41; acc: 0.7
Batch: 80; loss: 1.65; acc: 0.5
Batch: 100; loss: 1.51; acc: 0.61
Batch: 120; loss: 1.44; acc: 0.69
Batch: 140; loss: 1.43; acc: 0.64
Batch: 160; loss: 1.57; acc: 0.59
Batch: 180; loss: 1.53; acc: 0.62
Batch: 200; loss: 1.54; acc: 0.64
Batch: 220; loss: 1.61; acc: 0.58
Batch: 240; loss: 1.54; acc: 0.53
Batch: 260; loss: 1.62; acc: 0.53
Batch: 280; loss: 1.59; acc: 0.55
Batch: 300; loss: 1.48; acc: 0.66
Batch: 320; loss: 1.8; acc: 0.41
Batch: 340; loss: 1.44; acc: 0.66
Batch: 360; loss: 1.75; acc: 0.44
Batch: 380; loss: 1.6; acc: 0.53
Batch: 400; loss: 1.59; acc: 0.61
Batch: 420; loss: 1.54; acc: 0.58
Batch: 440; loss: 1.57; acc: 0.55
Batch: 460; loss: 1.62; acc: 0.55
Batch: 480; loss: 1.58; acc: 0.56
Batch: 500; loss: 1.41; acc: 0.69
Batch: 520; loss: 1.52; acc: 0.55
Batch: 540; loss: 1.44; acc: 0.62
Batch: 560; loss: 1.45; acc: 0.64
Batch: 580; loss: 1.46; acc: 0.61
Batch: 600; loss: 1.51; acc: 0.58
Batch: 620; loss: 1.61; acc: 0.55
Batch: 640; loss: 1.68; acc: 0.45
Batch: 660; loss: 1.48; acc: 0.62
Batch: 680; loss: 1.49; acc: 0.61
Batch: 700; loss: 1.55; acc: 0.55
Batch: 720; loss: 1.66; acc: 0.58
Batch: 740; loss: 1.56; acc: 0.53
Batch: 760; loss: 1.65; acc: 0.48
Batch: 780; loss: 1.51; acc: 0.56
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

4.076473487657495e-05
1.4339679182739928e-05
Batch: 0; loss: 1.7; acc: 0.44
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.3; acc: 0.75
Batch: 60; loss: 1.48; acc: 0.64
Batch: 80; loss: 1.37; acc: 0.64
Batch: 100; loss: 1.41; acc: 0.67
Batch: 120; loss: 1.61; acc: 0.58
Batch: 140; loss: 1.38; acc: 0.75
Val Epoch over. val_loss: 1.5211502093418388; val_accuracy: 0.5974323248407644 

The current subspace-distance is: 1.4339679182739928e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.61
Batch: 20; loss: 1.55; acc: 0.45
Batch: 40; loss: 1.46; acc: 0.58
Batch: 60; loss: 1.53; acc: 0.62
Batch: 80; loss: 1.47; acc: 0.55
Batch: 100; loss: 1.51; acc: 0.61
Batch: 120; loss: 1.74; acc: 0.45
Batch: 140; loss: 1.51; acc: 0.61
Batch: 160; loss: 1.61; acc: 0.5
Batch: 180; loss: 1.5; acc: 0.58
Batch: 200; loss: 1.55; acc: 0.58
Batch: 220; loss: 1.48; acc: 0.66
Batch: 240; loss: 1.48; acc: 0.61
Batch: 260; loss: 1.5; acc: 0.64
Batch: 280; loss: 1.68; acc: 0.53
Batch: 300; loss: 1.61; acc: 0.56
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.61; acc: 0.56
Batch: 360; loss: 1.56; acc: 0.56
Batch: 380; loss: 1.57; acc: 0.53
Batch: 400; loss: 1.45; acc: 0.64
Batch: 420; loss: 1.4; acc: 0.64
Batch: 440; loss: 1.6; acc: 0.53
Batch: 460; loss: 1.49; acc: 0.64
Batch: 480; loss: 1.68; acc: 0.53
Batch: 500; loss: 1.46; acc: 0.67
Batch: 520; loss: 1.4; acc: 0.66
Batch: 540; loss: 1.58; acc: 0.55
Batch: 560; loss: 1.5; acc: 0.62
Batch: 580; loss: 1.48; acc: 0.66
Batch: 600; loss: 1.49; acc: 0.66
Batch: 620; loss: 1.52; acc: 0.66
Batch: 640; loss: 1.56; acc: 0.58
Batch: 660; loss: 1.5; acc: 0.55
Batch: 680; loss: 1.56; acc: 0.55
Batch: 700; loss: 1.66; acc: 0.48
Batch: 720; loss: 1.4; acc: 0.66
Batch: 740; loss: 1.39; acc: 0.66
Batch: 760; loss: 1.67; acc: 0.5
Batch: 780; loss: 1.53; acc: 0.64
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

4.0509468817617744e-05
1.4295758774096612e-05
Batch: 0; loss: 1.7; acc: 0.42
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.3; acc: 0.75
Batch: 60; loss: 1.48; acc: 0.66
Batch: 80; loss: 1.38; acc: 0.66
Batch: 100; loss: 1.4; acc: 0.67
Batch: 120; loss: 1.61; acc: 0.59
Batch: 140; loss: 1.38; acc: 0.73
Val Epoch over. val_loss: 1.5207626804424699; val_accuracy: 0.5938495222929936 

The current subspace-distance is: 1.4295758774096612e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.52
Batch: 20; loss: 1.55; acc: 0.58
Batch: 40; loss: 1.49; acc: 0.62
Batch: 60; loss: 1.58; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.62
Batch: 100; loss: 1.56; acc: 0.58
Batch: 120; loss: 1.41; acc: 0.69
Batch: 140; loss: 1.51; acc: 0.59
Batch: 160; loss: 1.39; acc: 0.7
Batch: 180; loss: 1.64; acc: 0.5
Batch: 200; loss: 1.49; acc: 0.62
Batch: 220; loss: 1.51; acc: 0.59
Batch: 240; loss: 1.65; acc: 0.48
Batch: 260; loss: 1.54; acc: 0.56
Batch: 280; loss: 1.5; acc: 0.61
Batch: 300; loss: 1.74; acc: 0.44
Batch: 320; loss: 1.55; acc: 0.58
Batch: 340; loss: 1.55; acc: 0.61
Batch: 360; loss: 1.52; acc: 0.64
Batch: 380; loss: 1.45; acc: 0.64
Batch: 400; loss: 1.42; acc: 0.59
Batch: 420; loss: 1.6; acc: 0.53
Batch: 440; loss: 1.72; acc: 0.45
Batch: 460; loss: 1.49; acc: 0.56
Batch: 480; loss: 1.41; acc: 0.67
Batch: 500; loss: 1.67; acc: 0.45
Batch: 520; loss: 1.55; acc: 0.55
Batch: 540; loss: 1.45; acc: 0.59
Batch: 560; loss: 1.47; acc: 0.58
Batch: 580; loss: 1.54; acc: 0.55
Batch: 600; loss: 1.56; acc: 0.58
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.5; acc: 0.62
Batch: 660; loss: 1.57; acc: 0.58
Batch: 680; loss: 1.59; acc: 0.56
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.51; acc: 0.62
Batch: 740; loss: 1.55; acc: 0.58
Batch: 760; loss: 1.66; acc: 0.47
Batch: 780; loss: 1.68; acc: 0.5
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

4.13672678405419e-05
1.6687361494405195e-05
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.28; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.62
Batch: 80; loss: 1.37; acc: 0.66
Batch: 100; loss: 1.39; acc: 0.7
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.37; acc: 0.73
Val Epoch over. val_loss: 1.5123776501151407; val_accuracy: 0.5957404458598726 

The current subspace-distance is: 1.6687361494405195e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.53
Batch: 20; loss: 1.6; acc: 0.47
Batch: 40; loss: 1.74; acc: 0.34
Batch: 60; loss: 1.59; acc: 0.62
Batch: 80; loss: 1.64; acc: 0.45
Batch: 100; loss: 1.64; acc: 0.47
Batch: 120; loss: 1.56; acc: 0.48
Batch: 140; loss: 1.52; acc: 0.56
Batch: 160; loss: 1.55; acc: 0.59
Batch: 180; loss: 1.44; acc: 0.66
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.58; acc: 0.56
Batch: 240; loss: 1.64; acc: 0.45
Batch: 260; loss: 1.57; acc: 0.48
Batch: 280; loss: 1.62; acc: 0.56
Batch: 300; loss: 1.56; acc: 0.52
Batch: 320; loss: 1.66; acc: 0.52
Batch: 340; loss: 1.59; acc: 0.62
Batch: 360; loss: 1.47; acc: 0.58
Batch: 380; loss: 1.5; acc: 0.52
Batch: 400; loss: 1.51; acc: 0.62
Batch: 420; loss: 1.5; acc: 0.59
Batch: 440; loss: 1.54; acc: 0.61
Batch: 460; loss: 1.53; acc: 0.61
Batch: 480; loss: 1.59; acc: 0.56
Batch: 500; loss: 1.47; acc: 0.62
Batch: 520; loss: 1.53; acc: 0.62
Batch: 540; loss: 1.59; acc: 0.53
Batch: 560; loss: 1.5; acc: 0.52
Batch: 580; loss: 1.55; acc: 0.64
Batch: 600; loss: 1.48; acc: 0.62
Batch: 620; loss: 1.47; acc: 0.61
Batch: 640; loss: 1.48; acc: 0.62
Batch: 660; loss: 1.61; acc: 0.55
Batch: 680; loss: 1.45; acc: 0.58
Batch: 700; loss: 1.52; acc: 0.56
Batch: 720; loss: 1.48; acc: 0.59
Batch: 740; loss: 1.69; acc: 0.44
Batch: 760; loss: 1.67; acc: 0.53
Batch: 780; loss: 1.52; acc: 0.56
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

4.120921221328899e-05
1.542285826872103e-05
Batch: 0; loss: 1.7; acc: 0.45
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.66
Batch: 80; loss: 1.37; acc: 0.64
Batch: 100; loss: 1.4; acc: 0.72
Batch: 120; loss: 1.6; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.73
Val Epoch over. val_loss: 1.5137067623199172; val_accuracy: 0.5955414012738853 

The current subspace-distance is: 1.542285826872103e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.56
Batch: 20; loss: 1.47; acc: 0.67
Batch: 40; loss: 1.59; acc: 0.52
Batch: 60; loss: 1.48; acc: 0.62
Batch: 80; loss: 1.66; acc: 0.41
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.48; acc: 0.64
Batch: 160; loss: 1.46; acc: 0.67
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.45; acc: 0.69
Batch: 220; loss: 1.63; acc: 0.44
Batch: 240; loss: 1.53; acc: 0.59
Batch: 260; loss: 1.49; acc: 0.58
Batch: 280; loss: 1.44; acc: 0.66
Batch: 300; loss: 1.47; acc: 0.55
Batch: 320; loss: 1.53; acc: 0.62
Batch: 340; loss: 1.57; acc: 0.53
Batch: 360; loss: 1.69; acc: 0.45
Batch: 380; loss: 1.55; acc: 0.64
Batch: 400; loss: 1.61; acc: 0.61
Batch: 420; loss: 1.53; acc: 0.55
Batch: 440; loss: 1.57; acc: 0.61
Batch: 460; loss: 1.56; acc: 0.58
Batch: 480; loss: 1.52; acc: 0.52
Batch: 500; loss: 1.59; acc: 0.55
Batch: 520; loss: 1.73; acc: 0.41
Batch: 540; loss: 1.55; acc: 0.61
Batch: 560; loss: 1.45; acc: 0.62
Batch: 580; loss: 1.49; acc: 0.58
Batch: 600; loss: 1.55; acc: 0.59
Batch: 620; loss: 1.52; acc: 0.59
Batch: 640; loss: 1.38; acc: 0.72
Batch: 660; loss: 1.4; acc: 0.69
Batch: 680; loss: 1.7; acc: 0.48
Batch: 700; loss: 1.57; acc: 0.59
Batch: 720; loss: 1.65; acc: 0.52
Batch: 740; loss: 1.63; acc: 0.58
Batch: 760; loss: 1.67; acc: 0.5
Batch: 780; loss: 1.52; acc: 0.52
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

4.13045818277169e-05
1.4144060514809098e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.62; acc: 0.5
Batch: 40; loss: 1.3; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.61
Batch: 80; loss: 1.37; acc: 0.69
Batch: 100; loss: 1.39; acc: 0.7
Batch: 120; loss: 1.6; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.75
Val Epoch over. val_loss: 1.5078324307302; val_accuracy: 0.597531847133758 

The current subspace-distance is: 1.4144060514809098e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.51; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.55
Batch: 40; loss: 1.54; acc: 0.59
Batch: 60; loss: 1.48; acc: 0.67
Batch: 80; loss: 1.55; acc: 0.56
Batch: 100; loss: 1.62; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.55
Batch: 140; loss: 1.6; acc: 0.55
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.53; acc: 0.62
Batch: 200; loss: 1.57; acc: 0.61
Batch: 220; loss: 1.53; acc: 0.55
Batch: 240; loss: 1.64; acc: 0.52
Batch: 260; loss: 1.67; acc: 0.48
Batch: 280; loss: 1.38; acc: 0.66
Batch: 300; loss: 1.67; acc: 0.48
Batch: 320; loss: 1.58; acc: 0.53
Batch: 340; loss: 1.72; acc: 0.48
Batch: 360; loss: 1.68; acc: 0.45
Batch: 380; loss: 1.52; acc: 0.59
Batch: 400; loss: 1.5; acc: 0.53
Batch: 420; loss: 1.59; acc: 0.53
Batch: 440; loss: 1.56; acc: 0.64
Batch: 460; loss: 1.76; acc: 0.41
Batch: 480; loss: 1.64; acc: 0.5
Batch: 500; loss: 1.55; acc: 0.56
Batch: 520; loss: 1.61; acc: 0.55
Batch: 540; loss: 1.54; acc: 0.64
Batch: 560; loss: 1.46; acc: 0.64
Batch: 580; loss: 1.55; acc: 0.52
Batch: 600; loss: 1.59; acc: 0.58
Batch: 620; loss: 1.66; acc: 0.47
Batch: 640; loss: 1.59; acc: 0.53
Batch: 660; loss: 1.52; acc: 0.53
Batch: 680; loss: 1.66; acc: 0.45
Batch: 700; loss: 1.49; acc: 0.64
Batch: 720; loss: 1.58; acc: 0.53
Batch: 740; loss: 1.45; acc: 0.67
Batch: 760; loss: 1.44; acc: 0.58
Batch: 780; loss: 1.66; acc: 0.5
Train Epoch over. train_loss: 1.56; train_accuracy: 0.57 

4.178647213848308e-05
1.5182323295448441e-05
Batch: 0; loss: 1.69; acc: 0.44
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.3; acc: 0.75
Batch: 60; loss: 1.47; acc: 0.64
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.37; acc: 0.75
Val Epoch over. val_loss: 1.5091649597617471; val_accuracy: 0.5961385350318471 

The current subspace-distance is: 1.5182323295448441e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.62
Batch: 20; loss: 1.62; acc: 0.52
Batch: 40; loss: 1.44; acc: 0.64
Batch: 60; loss: 1.52; acc: 0.55
Batch: 80; loss: 1.47; acc: 0.61
Batch: 100; loss: 1.55; acc: 0.59
Batch: 120; loss: 1.34; acc: 0.72
Batch: 140; loss: 1.54; acc: 0.62
Batch: 160; loss: 1.55; acc: 0.55
Batch: 180; loss: 1.58; acc: 0.59
Batch: 200; loss: 1.48; acc: 0.61
Batch: 220; loss: 1.54; acc: 0.56
Batch: 240; loss: 1.45; acc: 0.62
Batch: 260; loss: 1.52; acc: 0.58
Batch: 280; loss: 1.69; acc: 0.45
Batch: 300; loss: 1.61; acc: 0.5
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.51; acc: 0.59
Batch: 360; loss: 1.62; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.59
Batch: 400; loss: 1.58; acc: 0.55
Batch: 420; loss: 1.66; acc: 0.56
Batch: 440; loss: 1.45; acc: 0.64
Batch: 460; loss: 1.61; acc: 0.5
Batch: 480; loss: 1.6; acc: 0.59
Batch: 500; loss: 1.69; acc: 0.45
Batch: 520; loss: 1.69; acc: 0.53
Batch: 540; loss: 1.77; acc: 0.42
Batch: 560; loss: 1.59; acc: 0.56
Batch: 580; loss: 1.49; acc: 0.59
Batch: 600; loss: 1.35; acc: 0.7
Batch: 620; loss: 1.5; acc: 0.61
Batch: 640; loss: 1.58; acc: 0.56
Batch: 660; loss: 1.51; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.7
Batch: 700; loss: 1.57; acc: 0.59
Batch: 720; loss: 1.67; acc: 0.47
Batch: 740; loss: 1.53; acc: 0.55
Batch: 760; loss: 1.55; acc: 0.59
Batch: 780; loss: 1.68; acc: 0.45
Train Epoch over. train_loss: 1.55; train_accuracy: 0.57 

4.116107083973475e-05
1.6615029380773194e-05
Batch: 0; loss: 1.69; acc: 0.44
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.46; acc: 0.64
Batch: 80; loss: 1.37; acc: 0.66
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.36; acc: 0.77
Val Epoch over. val_loss: 1.5045937854013625; val_accuracy: 0.5976313694267515 

The current subspace-distance is: 1.6615029380773194e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.37; acc: 0.64
Batch: 20; loss: 1.51; acc: 0.62
Batch: 40; loss: 1.51; acc: 0.59
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.43; acc: 0.64
Batch: 120; loss: 1.62; acc: 0.47
Batch: 140; loss: 1.56; acc: 0.59
Batch: 160; loss: 1.65; acc: 0.5
Batch: 180; loss: 1.51; acc: 0.5
Batch: 200; loss: 1.4; acc: 0.66
Batch: 220; loss: 1.45; acc: 0.66
Batch: 240; loss: 1.45; acc: 0.62
Batch: 260; loss: 1.48; acc: 0.64
Batch: 280; loss: 1.51; acc: 0.67
Batch: 300; loss: 1.49; acc: 0.64
Batch: 320; loss: 1.52; acc: 0.61
Batch: 340; loss: 1.65; acc: 0.52
Batch: 360; loss: 1.51; acc: 0.64
Batch: 380; loss: 1.53; acc: 0.59
Batch: 400; loss: 1.56; acc: 0.56
Batch: 420; loss: 1.45; acc: 0.62
Batch: 440; loss: 1.56; acc: 0.58
Batch: 460; loss: 1.4; acc: 0.7
Batch: 480; loss: 1.39; acc: 0.66
Batch: 500; loss: 1.58; acc: 0.56
Batch: 520; loss: 1.48; acc: 0.64
Batch: 540; loss: 1.55; acc: 0.59
Batch: 560; loss: 1.41; acc: 0.61
Batch: 580; loss: 1.52; acc: 0.52
Batch: 600; loss: 1.47; acc: 0.66
Batch: 620; loss: 1.68; acc: 0.44
Batch: 640; loss: 1.6; acc: 0.52
Batch: 660; loss: 1.58; acc: 0.53
Batch: 680; loss: 1.55; acc: 0.58
Batch: 700; loss: 1.59; acc: 0.59
Batch: 720; loss: 1.6; acc: 0.56
Batch: 740; loss: 1.62; acc: 0.52
Batch: 760; loss: 1.45; acc: 0.66
Batch: 780; loss: 1.69; acc: 0.41
Train Epoch over. train_loss: 1.55; train_accuracy: 0.57 

4.2099451093235984e-05
1.65163837664295e-05
Batch: 0; loss: 1.7; acc: 0.42
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.46; acc: 0.64
Batch: 80; loss: 1.38; acc: 0.67
Batch: 100; loss: 1.39; acc: 0.72
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 1.37; acc: 0.73
Val Epoch over. val_loss: 1.509928748106501; val_accuracy: 0.5901671974522293 

The current subspace-distance is: 1.65163837664295e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_14_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6227353450270319

The number of parameters is: 249772

The number of individual parameters is:

13
234
13
13
20
34840
20
20
39
104520
39
39
64
104832
64
64
4096
64
640
10
64
64

nonzero elements in E: 24977197
elements in E: 24977200
fraction nonzero: 0.9999998798904601
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.12
Batch: 20; loss: 2.26; acc: 0.23
Batch: 40; loss: 2.16; acc: 0.23
Batch: 60; loss: 2.14; acc: 0.22
Batch: 80; loss: 2.05; acc: 0.33
Batch: 100; loss: 2.05; acc: 0.28
Batch: 120; loss: 1.99; acc: 0.31
Batch: 140; loss: 1.91; acc: 0.38
Batch: 160; loss: 1.93; acc: 0.39
Batch: 180; loss: 1.88; acc: 0.52
Batch: 200; loss: 1.9; acc: 0.53
Batch: 220; loss: 1.91; acc: 0.41
Batch: 240; loss: 1.82; acc: 0.53
Batch: 260; loss: 1.92; acc: 0.41
Batch: 280; loss: 1.87; acc: 0.5
Batch: 300; loss: 1.79; acc: 0.5
Batch: 320; loss: 1.74; acc: 0.55
Batch: 340; loss: 1.68; acc: 0.59
Batch: 360; loss: 1.94; acc: 0.47
Batch: 380; loss: 1.73; acc: 0.59
Batch: 400; loss: 1.72; acc: 0.58
Batch: 420; loss: 1.72; acc: 0.56
Batch: 440; loss: 1.73; acc: 0.56
Batch: 460; loss: 1.8; acc: 0.5
Batch: 480; loss: 1.71; acc: 0.53
Batch: 500; loss: 1.78; acc: 0.56
Batch: 520; loss: 1.76; acc: 0.58
Batch: 540; loss: 1.74; acc: 0.61
Batch: 560; loss: 1.66; acc: 0.62
Batch: 580; loss: 1.74; acc: 0.59
Batch: 600; loss: 1.79; acc: 0.48
Batch: 620; loss: 1.71; acc: 0.58
Batch: 640; loss: 1.66; acc: 0.66
Batch: 660; loss: 1.77; acc: 0.55
Batch: 680; loss: 1.69; acc: 0.56
Batch: 700; loss: 1.79; acc: 0.53
Batch: 720; loss: 1.7; acc: 0.56
Batch: 740; loss: 1.75; acc: 0.55
Batch: 760; loss: 1.71; acc: 0.58
Batch: 780; loss: 1.74; acc: 0.56
Train Epoch over. train_loss: 1.83; train_accuracy: 0.49 

4.49558392574545e-05
3.981278132414445e-05
Batch: 0; loss: 1.67; acc: 0.61
Batch: 20; loss: 1.78; acc: 0.47
Batch: 40; loss: 1.45; acc: 0.78
Batch: 60; loss: 1.56; acc: 0.66
Batch: 80; loss: 1.59; acc: 0.69
Batch: 100; loss: 1.59; acc: 0.7
Batch: 120; loss: 1.66; acc: 0.7
Batch: 140; loss: 1.52; acc: 0.73
Val Epoch over. val_loss: 1.6476995656444768; val_accuracy: 0.6194267515923567 

The current subspace-distance is: 3.981278132414445e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.64
Batch: 20; loss: 1.67; acc: 0.59
Batch: 40; loss: 1.65; acc: 0.66
Batch: 60; loss: 1.6; acc: 0.7
Batch: 80; loss: 1.73; acc: 0.56
Batch: 100; loss: 1.64; acc: 0.59
Batch: 120; loss: 1.57; acc: 0.73
Batch: 140; loss: 1.66; acc: 0.58
Batch: 160; loss: 1.62; acc: 0.67
Batch: 180; loss: 1.68; acc: 0.61
Batch: 200; loss: 1.68; acc: 0.59
Batch: 220; loss: 1.67; acc: 0.56
Batch: 240; loss: 1.73; acc: 0.53
Batch: 260; loss: 1.59; acc: 0.64
Batch: 280; loss: 1.72; acc: 0.47
Batch: 300; loss: 1.6; acc: 0.62
Batch: 320; loss: 1.64; acc: 0.59
Batch: 340; loss: 1.69; acc: 0.55
Batch: 360; loss: 1.56; acc: 0.64
Batch: 380; loss: 1.54; acc: 0.69
Batch: 400; loss: 1.62; acc: 0.58
Batch: 420; loss: 1.76; acc: 0.53
Batch: 440; loss: 1.67; acc: 0.58
Batch: 460; loss: 1.67; acc: 0.55
Batch: 480; loss: 1.68; acc: 0.53
Batch: 500; loss: 1.64; acc: 0.58
Batch: 520; loss: 1.5; acc: 0.66
Batch: 540; loss: 1.55; acc: 0.64
Batch: 560; loss: 1.78; acc: 0.52
Batch: 580; loss: 1.59; acc: 0.62
Batch: 600; loss: 1.62; acc: 0.52
Batch: 620; loss: 1.59; acc: 0.61
Batch: 640; loss: 1.47; acc: 0.73
Batch: 660; loss: 1.59; acc: 0.61
Batch: 680; loss: 1.59; acc: 0.62
Batch: 700; loss: 1.38; acc: 0.7
Batch: 720; loss: 1.49; acc: 0.67
Batch: 740; loss: 1.6; acc: 0.64
Batch: 760; loss: 1.52; acc: 0.67
Batch: 780; loss: 1.55; acc: 0.64
Train Epoch over. train_loss: 1.62; train_accuracy: 0.6 

6.177782779559493e-05
5.7299610489280894e-05
Batch: 0; loss: 1.6; acc: 0.61
Batch: 20; loss: 1.6; acc: 0.62
Batch: 40; loss: 1.32; acc: 0.8
Batch: 60; loss: 1.44; acc: 0.7
Batch: 80; loss: 1.44; acc: 0.7
Batch: 100; loss: 1.48; acc: 0.7
Batch: 120; loss: 1.59; acc: 0.66
Batch: 140; loss: 1.43; acc: 0.8
Val Epoch over. val_loss: 1.5160252729039283; val_accuracy: 0.6529657643312102 

The current subspace-distance is: 5.7299610489280894e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.69
Batch: 20; loss: 1.51; acc: 0.72
Batch: 40; loss: 1.6; acc: 0.64
Batch: 60; loss: 1.53; acc: 0.67
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.49; acc: 0.67
Batch: 120; loss: 1.5; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.66
Batch: 160; loss: 1.58; acc: 0.56
Batch: 180; loss: 1.55; acc: 0.52
Batch: 200; loss: 1.47; acc: 0.72
Batch: 220; loss: 1.51; acc: 0.61
Batch: 240; loss: 1.55; acc: 0.64
Batch: 260; loss: 1.55; acc: 0.53
Batch: 280; loss: 1.62; acc: 0.59
Batch: 300; loss: 1.57; acc: 0.56
Batch: 320; loss: 1.44; acc: 0.72
Batch: 340; loss: 1.46; acc: 0.66
Batch: 360; loss: 1.46; acc: 0.67
Batch: 380; loss: 1.47; acc: 0.59
Batch: 400; loss: 1.59; acc: 0.53
Batch: 420; loss: 1.52; acc: 0.62
Batch: 440; loss: 1.63; acc: 0.5
Batch: 460; loss: 1.39; acc: 0.8
Batch: 480; loss: 1.52; acc: 0.56
Batch: 500; loss: 1.59; acc: 0.55
Batch: 520; loss: 1.39; acc: 0.64
Batch: 540; loss: 1.61; acc: 0.5
Batch: 560; loss: 1.52; acc: 0.64
Batch: 580; loss: 1.54; acc: 0.61
Batch: 600; loss: 1.47; acc: 0.62
Batch: 620; loss: 1.47; acc: 0.72
Batch: 640; loss: 1.4; acc: 0.75
Batch: 660; loss: 1.45; acc: 0.64
Batch: 680; loss: 1.49; acc: 0.62
Batch: 700; loss: 1.45; acc: 0.64
Batch: 720; loss: 1.5; acc: 0.56
Batch: 740; loss: 1.47; acc: 0.64
Batch: 760; loss: 1.43; acc: 0.61
Batch: 780; loss: 1.34; acc: 0.75
Train Epoch over. train_loss: 1.5; train_accuracy: 0.64 

7.939349598018453e-05
7.388358062598854e-05
Batch: 0; loss: 1.48; acc: 0.67
Batch: 20; loss: 1.48; acc: 0.58
Batch: 40; loss: 1.22; acc: 0.8
Batch: 60; loss: 1.33; acc: 0.75
Batch: 80; loss: 1.24; acc: 0.77
Batch: 100; loss: 1.39; acc: 0.73
Batch: 120; loss: 1.5; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.75
Val Epoch over. val_loss: 1.3871136850612178; val_accuracy: 0.6807324840764332 

The current subspace-distance is: 7.388358062598854e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.58
Batch: 20; loss: 1.57; acc: 0.64
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 1.5; acc: 0.61
Batch: 80; loss: 1.42; acc: 0.72
Batch: 100; loss: 1.36; acc: 0.69
Batch: 120; loss: 1.38; acc: 0.67
Batch: 140; loss: 1.46; acc: 0.61
Batch: 160; loss: 1.48; acc: 0.62
Batch: 180; loss: 1.5; acc: 0.58
Batch: 200; loss: 1.45; acc: 0.59
Batch: 220; loss: 1.31; acc: 0.69
Batch: 240; loss: 1.54; acc: 0.55
Batch: 260; loss: 1.45; acc: 0.67
Batch: 280; loss: 1.42; acc: 0.7
Batch: 300; loss: 1.36; acc: 0.69
Batch: 320; loss: 1.34; acc: 0.67
Batch: 340; loss: 1.3; acc: 0.67
Batch: 360; loss: 1.45; acc: 0.5
Batch: 380; loss: 1.36; acc: 0.58
Batch: 400; loss: 1.38; acc: 0.66
Batch: 420; loss: 1.33; acc: 0.75
Batch: 440; loss: 1.26; acc: 0.73
Batch: 460; loss: 1.49; acc: 0.52
Batch: 480; loss: 1.31; acc: 0.77
Batch: 500; loss: 1.42; acc: 0.61
Batch: 520; loss: 1.49; acc: 0.64
Batch: 540; loss: 1.3; acc: 0.72
Batch: 560; loss: 1.54; acc: 0.52
Batch: 580; loss: 1.43; acc: 0.66
Batch: 600; loss: 1.38; acc: 0.66
Batch: 620; loss: 1.46; acc: 0.69
Batch: 640; loss: 1.38; acc: 0.7
Batch: 660; loss: 1.37; acc: 0.67
Batch: 680; loss: 1.28; acc: 0.7
Batch: 700; loss: 1.32; acc: 0.62
Batch: 720; loss: 1.37; acc: 0.72
Batch: 740; loss: 1.44; acc: 0.61
Batch: 760; loss: 1.31; acc: 0.73
Batch: 780; loss: 1.34; acc: 0.72
Train Epoch over. train_loss: 1.4; train_accuracy: 0.66 

9.241129009751603e-05
8.747422543819994e-05
Batch: 0; loss: 1.4; acc: 0.72
Batch: 20; loss: 1.41; acc: 0.61
Batch: 40; loss: 1.12; acc: 0.84
Batch: 60; loss: 1.24; acc: 0.73
Batch: 80; loss: 1.13; acc: 0.75
Batch: 100; loss: 1.32; acc: 0.73
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.18; acc: 0.8
Val Epoch over. val_loss: 1.3167070454093301; val_accuracy: 0.6968550955414012 

The current subspace-distance is: 8.747422543819994e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.56
Batch: 20; loss: 1.46; acc: 0.64
Batch: 40; loss: 1.52; acc: 0.66
Batch: 60; loss: 1.41; acc: 0.58
Batch: 80; loss: 1.34; acc: 0.73
Batch: 100; loss: 1.43; acc: 0.61
Batch: 120; loss: 1.24; acc: 0.75
Batch: 140; loss: 1.28; acc: 0.69
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.33; acc: 0.72
Batch: 200; loss: 1.3; acc: 0.69
Batch: 220; loss: 1.19; acc: 0.73
Batch: 240; loss: 1.43; acc: 0.67
Batch: 260; loss: 1.3; acc: 0.69
Batch: 280; loss: 1.22; acc: 0.81
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.43; acc: 0.69
Batch: 340; loss: 1.29; acc: 0.67
Batch: 360; loss: 1.3; acc: 0.64
Batch: 380; loss: 1.35; acc: 0.62
Batch: 400; loss: 1.26; acc: 0.72
Batch: 420; loss: 1.37; acc: 0.62
Batch: 440; loss: 1.43; acc: 0.66
Batch: 460; loss: 1.31; acc: 0.73
Batch: 480; loss: 1.27; acc: 0.62
Batch: 500; loss: 1.32; acc: 0.64
Batch: 520; loss: 1.42; acc: 0.61
Batch: 540; loss: 1.46; acc: 0.66
Batch: 560; loss: 1.17; acc: 0.77
Batch: 580; loss: 1.36; acc: 0.66
Batch: 600; loss: 1.52; acc: 0.55
Batch: 620; loss: 1.31; acc: 0.7
Batch: 640; loss: 1.22; acc: 0.75
Batch: 660; loss: 1.4; acc: 0.61
Batch: 680; loss: 1.32; acc: 0.66
Batch: 700; loss: 1.25; acc: 0.69
Batch: 720; loss: 1.39; acc: 0.59
Batch: 740; loss: 1.41; acc: 0.58
Batch: 760; loss: 1.32; acc: 0.72
Batch: 780; loss: 1.12; acc: 0.73
Train Epoch over. train_loss: 1.34; train_accuracy: 0.67 

0.00010265759920002893
9.668328129919246e-05
Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 1.33; acc: 0.67
Batch: 40; loss: 1.07; acc: 0.81
Batch: 60; loss: 1.2; acc: 0.75
Batch: 80; loss: 1.08; acc: 0.78
Batch: 100; loss: 1.25; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.69
Batch: 140; loss: 1.09; acc: 0.83
Val Epoch over. val_loss: 1.2649927602452078; val_accuracy: 0.7099920382165605 

The current subspace-distance is: 9.668328129919246e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 1.4; acc: 0.69
Batch: 40; loss: 1.29; acc: 0.75
Batch: 60; loss: 1.4; acc: 0.66
Batch: 80; loss: 1.29; acc: 0.67
Batch: 100; loss: 1.3; acc: 0.69
Batch: 120; loss: 1.35; acc: 0.66
Batch: 140; loss: 1.24; acc: 0.67
Batch: 160; loss: 1.38; acc: 0.64
Batch: 180; loss: 1.36; acc: 0.64
Batch: 200; loss: 1.23; acc: 0.73
Batch: 220; loss: 1.37; acc: 0.61
Batch: 240; loss: 1.38; acc: 0.59
Batch: 260; loss: 1.39; acc: 0.59
Batch: 280; loss: 1.2; acc: 0.78
Batch: 300; loss: 1.38; acc: 0.59
Batch: 320; loss: 1.34; acc: 0.67
Batch: 340; loss: 1.32; acc: 0.61
Batch: 360; loss: 1.28; acc: 0.7
Batch: 380; loss: 1.23; acc: 0.73
Batch: 400; loss: 1.25; acc: 0.69
Batch: 420; loss: 1.35; acc: 0.66
Batch: 440; loss: 1.3; acc: 0.78
Batch: 460; loss: 1.37; acc: 0.7
Batch: 480; loss: 1.29; acc: 0.67
Batch: 500; loss: 1.26; acc: 0.59
Batch: 520; loss: 1.3; acc: 0.7
Batch: 540; loss: 1.18; acc: 0.77
Batch: 560; loss: 1.27; acc: 0.69
Batch: 580; loss: 1.3; acc: 0.64
Batch: 600; loss: 1.41; acc: 0.61
Batch: 620; loss: 1.31; acc: 0.61
Batch: 640; loss: 1.45; acc: 0.59
Batch: 660; loss: 1.35; acc: 0.67
Batch: 680; loss: 1.23; acc: 0.72
Batch: 700; loss: 1.22; acc: 0.7
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.33; acc: 0.67
Batch: 760; loss: 1.3; acc: 0.66
Batch: 780; loss: 1.34; acc: 0.64
Train Epoch over. train_loss: 1.3; train_accuracy: 0.68 

0.00011073886707890779
0.00010488143743714318
Batch: 0; loss: 1.31; acc: 0.66
Batch: 20; loss: 1.27; acc: 0.69
Batch: 40; loss: 1.03; acc: 0.86
Batch: 60; loss: 1.19; acc: 0.72
Batch: 80; loss: 1.04; acc: 0.8
Batch: 100; loss: 1.19; acc: 0.77
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 1.06; acc: 0.8
Val Epoch over. val_loss: 1.2246827802081017; val_accuracy: 0.71765525477707 

The current subspace-distance is: 0.00010488143743714318 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.72
Batch: 20; loss: 1.42; acc: 0.61
Batch: 40; loss: 1.35; acc: 0.69
Batch: 60; loss: 1.13; acc: 0.78
Batch: 80; loss: 1.38; acc: 0.64
Batch: 100; loss: 1.34; acc: 0.73
Batch: 120; loss: 1.18; acc: 0.69
Batch: 140; loss: 1.34; acc: 0.61
Batch: 160; loss: 1.27; acc: 0.69
Batch: 180; loss: 1.28; acc: 0.69
Batch: 200; loss: 1.17; acc: 0.72
Batch: 220; loss: 1.35; acc: 0.61
Batch: 240; loss: 1.4; acc: 0.62
Batch: 260; loss: 1.32; acc: 0.67
Batch: 280; loss: 1.29; acc: 0.67
Batch: 300; loss: 1.24; acc: 0.7
Batch: 320; loss: 1.37; acc: 0.58
Batch: 340; loss: 1.47; acc: 0.53
Batch: 360; loss: 1.29; acc: 0.7
Batch: 380; loss: 1.43; acc: 0.61
Batch: 400; loss: 1.25; acc: 0.73
Batch: 420; loss: 1.23; acc: 0.67
Batch: 440; loss: 1.25; acc: 0.75
Batch: 460; loss: 1.18; acc: 0.8
Batch: 480; loss: 1.11; acc: 0.77
Batch: 500; loss: 1.38; acc: 0.64
Batch: 520; loss: 1.09; acc: 0.86
Batch: 540; loss: 1.26; acc: 0.7
Batch: 560; loss: 1.15; acc: 0.72
Batch: 580; loss: 1.25; acc: 0.66
Batch: 600; loss: 1.24; acc: 0.66
Batch: 620; loss: 1.21; acc: 0.7
Batch: 640; loss: 1.16; acc: 0.75
Batch: 660; loss: 1.1; acc: 0.77
Batch: 680; loss: 1.26; acc: 0.69
Batch: 700; loss: 1.25; acc: 0.67
Batch: 720; loss: 1.26; acc: 0.64
Batch: 740; loss: 1.4; acc: 0.62
Batch: 760; loss: 1.35; acc: 0.69
Batch: 780; loss: 1.17; acc: 0.73
Train Epoch over. train_loss: 1.27; train_accuracy: 0.69 

0.00011584602179937065
0.00011254365381319076
Batch: 0; loss: 1.28; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.02; acc: 0.86
Batch: 60; loss: 1.21; acc: 0.72
Batch: 80; loss: 1.03; acc: 0.8
Batch: 100; loss: 1.18; acc: 0.77
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 1.05; acc: 0.78
Val Epoch over. val_loss: 1.203774400957071; val_accuracy: 0.7230294585987261 

The current subspace-distance is: 0.00011254365381319076 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.8
Batch: 40; loss: 1.28; acc: 0.72
Batch: 60; loss: 1.28; acc: 0.67
Batch: 80; loss: 1.24; acc: 0.7
Batch: 100; loss: 1.35; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.59
Batch: 140; loss: 1.21; acc: 0.77
Batch: 160; loss: 1.2; acc: 0.7
Batch: 180; loss: 1.28; acc: 0.7
Batch: 200; loss: 1.37; acc: 0.64
Batch: 220; loss: 1.3; acc: 0.7
Batch: 240; loss: 1.3; acc: 0.64
Batch: 260; loss: 1.41; acc: 0.58
Batch: 280; loss: 1.12; acc: 0.77
Batch: 300; loss: 1.26; acc: 0.7
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.29; acc: 0.67
Batch: 360; loss: 1.24; acc: 0.69
Batch: 380; loss: 1.16; acc: 0.75
Batch: 400; loss: 1.28; acc: 0.66
Batch: 420; loss: 1.3; acc: 0.67
Batch: 440; loss: 1.2; acc: 0.7
Batch: 460; loss: 1.08; acc: 0.83
Batch: 480; loss: 1.3; acc: 0.64
Batch: 500; loss: 1.19; acc: 0.77
Batch: 520; loss: 1.14; acc: 0.73
Batch: 540; loss: 1.26; acc: 0.61
Batch: 560; loss: 1.19; acc: 0.66
Batch: 580; loss: 1.2; acc: 0.7
Batch: 600; loss: 1.21; acc: 0.73
Batch: 620; loss: 1.26; acc: 0.69
Batch: 640; loss: 1.41; acc: 0.62
Batch: 660; loss: 1.24; acc: 0.62
Batch: 680; loss: 1.22; acc: 0.69
Batch: 700; loss: 1.2; acc: 0.7
Batch: 720; loss: 1.23; acc: 0.61
Batch: 740; loss: 1.19; acc: 0.73
Batch: 760; loss: 1.19; acc: 0.73
Batch: 780; loss: 1.25; acc: 0.7
Train Epoch over. train_loss: 1.23; train_accuracy: 0.7 

0.0001256371906492859
0.0001163434426416643
Batch: 0; loss: 1.26; acc: 0.7
Batch: 20; loss: 1.24; acc: 0.75
Batch: 40; loss: 0.96; acc: 0.84
Batch: 60; loss: 1.2; acc: 0.77
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 1.18; acc: 0.69
Batch: 120; loss: 1.34; acc: 0.58
Batch: 140; loss: 1.02; acc: 0.75
Val Epoch over. val_loss: 1.1666625840648723; val_accuracy: 0.7312898089171974 

The current subspace-distance is: 0.0001163434426416643 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.77
Batch: 20; loss: 1.17; acc: 0.67
Batch: 40; loss: 1.28; acc: 0.69
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.15; acc: 0.77
Batch: 100; loss: 1.2; acc: 0.66
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 1.18; acc: 0.72
Batch: 160; loss: 1.18; acc: 0.72
Batch: 180; loss: 1.25; acc: 0.67
Batch: 200; loss: 1.27; acc: 0.62
Batch: 220; loss: 1.31; acc: 0.62
Batch: 240; loss: 1.27; acc: 0.64
Batch: 260; loss: 1.2; acc: 0.66
Batch: 280; loss: 1.09; acc: 0.75
Batch: 300; loss: 1.27; acc: 0.64
Batch: 320; loss: 1.12; acc: 0.78
Batch: 340; loss: 1.2; acc: 0.66
Batch: 360; loss: 1.21; acc: 0.7
Batch: 380; loss: 1.23; acc: 0.7
Batch: 400; loss: 1.29; acc: 0.66
Batch: 420; loss: 1.19; acc: 0.7
Batch: 440; loss: 1.09; acc: 0.8
Batch: 460; loss: 1.06; acc: 0.77
Batch: 480; loss: 1.2; acc: 0.67
Batch: 500; loss: 1.17; acc: 0.7
Batch: 520; loss: 1.2; acc: 0.66
Batch: 540; loss: 1.21; acc: 0.69
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.25; acc: 0.64
Batch: 600; loss: 1.14; acc: 0.77
Batch: 620; loss: 1.19; acc: 0.73
Batch: 640; loss: 1.31; acc: 0.59
Batch: 660; loss: 1.22; acc: 0.7
Batch: 680; loss: 1.09; acc: 0.75
Batch: 700; loss: 1.23; acc: 0.66
Batch: 720; loss: 1.13; acc: 0.72
Batch: 740; loss: 1.13; acc: 0.66
Batch: 760; loss: 1.23; acc: 0.64
Batch: 780; loss: 1.19; acc: 0.69
Train Epoch over. train_loss: 1.2; train_accuracy: 0.7 

0.00013268670591060072
0.00012592504208441824
Batch: 0; loss: 1.19; acc: 0.73
Batch: 20; loss: 1.2; acc: 0.75
Batch: 40; loss: 0.89; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.8
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.33; acc: 0.61
Batch: 140; loss: 0.99; acc: 0.78
Val Epoch over. val_loss: 1.1142545012152119; val_accuracy: 0.7372611464968153 

The current subspace-distance is: 0.00012592504208441824 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.7
Batch: 40; loss: 1.28; acc: 0.69
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 1.14; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.26; acc: 0.69
Batch: 140; loss: 1.2; acc: 0.67
Batch: 160; loss: 1.13; acc: 0.7
Batch: 180; loss: 1.06; acc: 0.78
Batch: 200; loss: 1.26; acc: 0.59
Batch: 220; loss: 1.25; acc: 0.67
Batch: 240; loss: 1.12; acc: 0.8
Batch: 260; loss: 1.11; acc: 0.8
Batch: 280; loss: 1.1; acc: 0.73
Batch: 300; loss: 1.43; acc: 0.58
Batch: 320; loss: 1.28; acc: 0.64
Batch: 340; loss: 1.15; acc: 0.77
Batch: 360; loss: 1.12; acc: 0.72
Batch: 380; loss: 1.24; acc: 0.69
Batch: 400; loss: 1.16; acc: 0.69
Batch: 420; loss: 1.32; acc: 0.61
Batch: 440; loss: 1.26; acc: 0.66
Batch: 460; loss: 1.07; acc: 0.73
Batch: 480; loss: 1.23; acc: 0.72
Batch: 500; loss: 1.0; acc: 0.75
Batch: 520; loss: 1.13; acc: 0.72
Batch: 540; loss: 1.19; acc: 0.77
Batch: 560; loss: 1.28; acc: 0.64
Batch: 580; loss: 1.18; acc: 0.66
Batch: 600; loss: 1.12; acc: 0.72
Batch: 620; loss: 1.09; acc: 0.7
Batch: 640; loss: 1.11; acc: 0.7
Batch: 660; loss: 1.09; acc: 0.77
Batch: 680; loss: 1.14; acc: 0.7
Batch: 700; loss: 1.01; acc: 0.78
Batch: 720; loss: 1.07; acc: 0.73
Batch: 740; loss: 0.93; acc: 0.83
Batch: 760; loss: 1.08; acc: 0.7
Batch: 780; loss: 1.08; acc: 0.77
Train Epoch over. train_loss: 1.15; train_accuracy: 0.7 

0.00014027177530806512
0.0001316989364568144
Batch: 0; loss: 1.15; acc: 0.78
Batch: 20; loss: 1.15; acc: 0.78
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 1.14; acc: 0.75
Batch: 80; loss: 0.94; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 0.97; acc: 0.77
Val Epoch over. val_loss: 1.0796023948936706; val_accuracy: 0.744327229299363 

The current subspace-distance is: 0.0001316989364568144 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.7
Batch: 40; loss: 1.07; acc: 0.77
Batch: 60; loss: 1.24; acc: 0.66
Batch: 80; loss: 1.15; acc: 0.73
Batch: 100; loss: 1.14; acc: 0.67
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 1.11; acc: 0.69
Batch: 180; loss: 1.12; acc: 0.64
Batch: 200; loss: 1.15; acc: 0.67
Batch: 220; loss: 1.33; acc: 0.56
Batch: 240; loss: 0.99; acc: 0.81
Batch: 260; loss: 1.04; acc: 0.78
Batch: 280; loss: 1.13; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.67
Batch: 320; loss: 1.08; acc: 0.75
Batch: 340; loss: 1.11; acc: 0.7
Batch: 360; loss: 1.16; acc: 0.64
Batch: 380; loss: 1.22; acc: 0.67
Batch: 400; loss: 1.06; acc: 0.78
Batch: 420; loss: 1.15; acc: 0.7
Batch: 440; loss: 1.02; acc: 0.8
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.1; acc: 0.72
Batch: 500; loss: 1.16; acc: 0.69
Batch: 520; loss: 1.1; acc: 0.72
Batch: 540; loss: 1.09; acc: 0.72
Batch: 560; loss: 1.08; acc: 0.73
Batch: 580; loss: 1.2; acc: 0.69
Batch: 600; loss: 1.21; acc: 0.66
Batch: 620; loss: 1.11; acc: 0.75
Batch: 640; loss: 1.12; acc: 0.73
Batch: 660; loss: 1.14; acc: 0.73
Batch: 680; loss: 1.08; acc: 0.73
Batch: 700; loss: 1.19; acc: 0.77
Batch: 720; loss: 1.14; acc: 0.66
Batch: 740; loss: 1.08; acc: 0.75
Batch: 760; loss: 1.1; acc: 0.75
Batch: 780; loss: 1.1; acc: 0.69
Train Epoch over. train_loss: 1.13; train_accuracy: 0.71 

0.00014146132161840796
0.00013299933925736696
Batch: 0; loss: 1.13; acc: 0.8
Batch: 20; loss: 1.14; acc: 0.77
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.12; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.77
Val Epoch over. val_loss: 1.065148602625367; val_accuracy: 0.7459195859872612 

The current subspace-distance is: 0.00013299933925736696 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.15; acc: 0.66
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 1.03; acc: 0.77
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.23; acc: 0.59
Batch: 100; loss: 1.18; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 1.14; acc: 0.67
Batch: 160; loss: 1.27; acc: 0.62
Batch: 180; loss: 1.06; acc: 0.78
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 1.06; acc: 0.72
Batch: 240; loss: 1.0; acc: 0.72
Batch: 260; loss: 1.0; acc: 0.77
Batch: 280; loss: 1.13; acc: 0.67
Batch: 300; loss: 1.13; acc: 0.72
Batch: 320; loss: 1.21; acc: 0.61
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.25; acc: 0.62
Batch: 380; loss: 1.11; acc: 0.69
Batch: 400; loss: 1.01; acc: 0.75
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.73
Batch: 480; loss: 1.0; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.81
Batch: 520; loss: 1.14; acc: 0.72
Batch: 540; loss: 1.22; acc: 0.66
Batch: 560; loss: 1.15; acc: 0.66
Batch: 580; loss: 1.1; acc: 0.69
Batch: 600; loss: 1.18; acc: 0.72
Batch: 620; loss: 1.11; acc: 0.72
Batch: 640; loss: 1.14; acc: 0.72
Batch: 660; loss: 1.22; acc: 0.67
Batch: 680; loss: 1.16; acc: 0.62
Batch: 700; loss: 0.99; acc: 0.77
Batch: 720; loss: 1.09; acc: 0.72
Batch: 740; loss: 1.02; acc: 0.75
Batch: 760; loss: 1.05; acc: 0.78
Batch: 780; loss: 1.06; acc: 0.75
Train Epoch over. train_loss: 1.12; train_accuracy: 0.71 

0.00014158966951072216
0.00013420343748293817
Batch: 0; loss: 1.1; acc: 0.81
Batch: 20; loss: 1.11; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 1.12; acc: 0.78
Batch: 80; loss: 0.93; acc: 0.8
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.75
Val Epoch over. val_loss: 1.048550968337211; val_accuracy: 0.7487062101910829 

The current subspace-distance is: 0.00013420343748293817 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.72
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 1.14; acc: 0.64
Batch: 60; loss: 1.06; acc: 0.78
Batch: 80; loss: 1.05; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.17; acc: 0.72
Batch: 140; loss: 1.26; acc: 0.64
Batch: 160; loss: 1.06; acc: 0.75
Batch: 180; loss: 0.99; acc: 0.75
Batch: 200; loss: 1.25; acc: 0.64
Batch: 220; loss: 1.11; acc: 0.66
Batch: 240; loss: 1.18; acc: 0.7
Batch: 260; loss: 1.2; acc: 0.7
Batch: 280; loss: 0.98; acc: 0.8
Batch: 300; loss: 1.2; acc: 0.7
Batch: 320; loss: 1.08; acc: 0.72
Batch: 340; loss: 1.26; acc: 0.64
Batch: 360; loss: 1.19; acc: 0.64
Batch: 380; loss: 0.99; acc: 0.81
Batch: 400; loss: 1.17; acc: 0.59
Batch: 420; loss: 1.09; acc: 0.78
Batch: 440; loss: 0.96; acc: 0.78
Batch: 460; loss: 1.23; acc: 0.66
Batch: 480; loss: 1.19; acc: 0.72
Batch: 500; loss: 1.04; acc: 0.72
Batch: 520; loss: 1.23; acc: 0.7
Batch: 540; loss: 1.01; acc: 0.77
Batch: 560; loss: 1.0; acc: 0.81
Batch: 580; loss: 1.18; acc: 0.69
Batch: 600; loss: 1.04; acc: 0.77
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 0.92; acc: 0.8
Batch: 660; loss: 1.14; acc: 0.73
Batch: 680; loss: 1.14; acc: 0.66
Batch: 700; loss: 0.9; acc: 0.86
Batch: 720; loss: 1.02; acc: 0.78
Batch: 740; loss: 1.21; acc: 0.62
Batch: 760; loss: 1.11; acc: 0.73
Batch: 780; loss: 1.33; acc: 0.55
Train Epoch over. train_loss: 1.11; train_accuracy: 0.71 

0.00014637110871262848
0.0001386240473948419
Batch: 0; loss: 1.12; acc: 0.81
Batch: 20; loss: 1.11; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.77
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 0.94; acc: 0.75
Val Epoch over. val_loss: 1.0541109996996108; val_accuracy: 0.7428343949044586 

The current subspace-distance is: 0.0001386240473948419 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.67
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 1.12; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.67
Batch: 120; loss: 1.1; acc: 0.67
Batch: 140; loss: 1.01; acc: 0.77
Batch: 160; loss: 1.04; acc: 0.72
Batch: 180; loss: 1.09; acc: 0.7
Batch: 200; loss: 1.01; acc: 0.8
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.06; acc: 0.73
Batch: 260; loss: 1.03; acc: 0.72
Batch: 280; loss: 1.1; acc: 0.75
Batch: 300; loss: 1.23; acc: 0.66
Batch: 320; loss: 1.17; acc: 0.59
Batch: 340; loss: 1.23; acc: 0.67
Batch: 360; loss: 1.05; acc: 0.7
Batch: 380; loss: 1.02; acc: 0.72
Batch: 400; loss: 1.28; acc: 0.64
Batch: 420; loss: 1.13; acc: 0.7
Batch: 440; loss: 1.07; acc: 0.75
Batch: 460; loss: 0.98; acc: 0.75
Batch: 480; loss: 1.22; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.7
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 0.99; acc: 0.77
Batch: 560; loss: 0.95; acc: 0.77
Batch: 580; loss: 1.07; acc: 0.72
Batch: 600; loss: 0.97; acc: 0.75
Batch: 620; loss: 1.15; acc: 0.64
Batch: 640; loss: 1.03; acc: 0.69
Batch: 660; loss: 1.0; acc: 0.7
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 1.1; acc: 0.67
Batch: 720; loss: 1.02; acc: 0.75
Batch: 740; loss: 1.09; acc: 0.66
Batch: 760; loss: 1.13; acc: 0.72
Batch: 780; loss: 1.12; acc: 0.75
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00015057438577059656
0.0001432632707292214
Batch: 0; loss: 1.09; acc: 0.83
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.81
Batch: 100; loss: 1.08; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.73
Val Epoch over. val_loss: 1.0366644992190561; val_accuracy: 0.7496019108280255 

The current subspace-distance is: 0.0001432632707292214 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.25; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.52
Batch: 40; loss: 1.16; acc: 0.69
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.88
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 1.17; acc: 0.67
Batch: 160; loss: 1.0; acc: 0.8
Batch: 180; loss: 1.0; acc: 0.8
Batch: 200; loss: 0.91; acc: 0.83
Batch: 220; loss: 1.04; acc: 0.75
Batch: 240; loss: 1.05; acc: 0.72
Batch: 260; loss: 1.13; acc: 0.75
Batch: 280; loss: 1.26; acc: 0.66
Batch: 300; loss: 0.98; acc: 0.81
Batch: 320; loss: 1.04; acc: 0.77
Batch: 340; loss: 0.98; acc: 0.75
Batch: 360; loss: 1.07; acc: 0.7
Batch: 380; loss: 1.04; acc: 0.78
Batch: 400; loss: 1.05; acc: 0.72
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.12; acc: 0.67
Batch: 460; loss: 0.94; acc: 0.81
Batch: 480; loss: 1.06; acc: 0.72
Batch: 500; loss: 0.96; acc: 0.78
Batch: 520; loss: 1.25; acc: 0.73
Batch: 540; loss: 1.02; acc: 0.73
Batch: 560; loss: 0.84; acc: 0.84
Batch: 580; loss: 0.87; acc: 0.83
Batch: 600; loss: 1.21; acc: 0.67
Batch: 620; loss: 1.01; acc: 0.75
Batch: 640; loss: 1.01; acc: 0.7
Batch: 660; loss: 1.34; acc: 0.58
Batch: 680; loss: 1.28; acc: 0.61
Batch: 700; loss: 1.18; acc: 0.62
Batch: 720; loss: 1.14; acc: 0.72
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 0.96; acc: 0.77
Batch: 780; loss: 1.21; acc: 0.62
Train Epoch over. train_loss: 1.09; train_accuracy: 0.72 

0.0001518453354947269
0.00014233346155378968
Batch: 0; loss: 1.06; acc: 0.83
Batch: 20; loss: 1.09; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 1.08; acc: 0.75
Batch: 80; loss: 0.9; acc: 0.81
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.89; acc: 0.77
Val Epoch over. val_loss: 1.0152789369510238; val_accuracy: 0.7531847133757962 

The current subspace-distance is: 0.00014233346155378968 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.7
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 0.87; acc: 0.83
Batch: 60; loss: 1.14; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 0.95; acc: 0.8
Batch: 140; loss: 1.07; acc: 0.73
Batch: 160; loss: 1.12; acc: 0.75
Batch: 180; loss: 1.05; acc: 0.73
Batch: 200; loss: 1.06; acc: 0.78
Batch: 220; loss: 1.13; acc: 0.7
Batch: 240; loss: 0.98; acc: 0.8
Batch: 260; loss: 1.02; acc: 0.78
Batch: 280; loss: 1.04; acc: 0.73
Batch: 300; loss: 1.19; acc: 0.67
Batch: 320; loss: 1.08; acc: 0.69
Batch: 340; loss: 1.14; acc: 0.64
Batch: 360; loss: 1.01; acc: 0.73
Batch: 380; loss: 1.04; acc: 0.7
Batch: 400; loss: 1.08; acc: 0.73
Batch: 420; loss: 1.12; acc: 0.73
Batch: 440; loss: 1.17; acc: 0.62
Batch: 460; loss: 1.29; acc: 0.64
Batch: 480; loss: 0.88; acc: 0.81
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.07; acc: 0.77
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.3; acc: 0.67
Batch: 580; loss: 1.2; acc: 0.66
Batch: 600; loss: 1.07; acc: 0.77
Batch: 620; loss: 1.02; acc: 0.8
Batch: 640; loss: 1.01; acc: 0.8
Batch: 660; loss: 1.09; acc: 0.7
Batch: 680; loss: 1.13; acc: 0.69
Batch: 700; loss: 1.06; acc: 0.78
Batch: 720; loss: 1.27; acc: 0.59
Batch: 740; loss: 1.17; acc: 0.67
Batch: 760; loss: 1.23; acc: 0.67
Batch: 780; loss: 1.18; acc: 0.72
Train Epoch over. train_loss: 1.08; train_accuracy: 0.72 

0.00015432749933097512
0.0001467384718125686
Batch: 0; loss: 1.07; acc: 0.83
Batch: 20; loss: 1.07; acc: 0.77
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 1.08; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.69
Batch: 140; loss: 0.89; acc: 0.75
Val Epoch over. val_loss: 1.016751426420394; val_accuracy: 0.7516918789808917 

The current subspace-distance is: 0.0001467384718125686 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 1.1; acc: 0.7
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 0.92; acc: 0.84
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 0.92; acc: 0.77
Batch: 200; loss: 0.95; acc: 0.8
Batch: 220; loss: 1.12; acc: 0.66
Batch: 240; loss: 1.16; acc: 0.67
Batch: 260; loss: 1.25; acc: 0.58
Batch: 280; loss: 1.01; acc: 0.73
Batch: 300; loss: 1.16; acc: 0.66
Batch: 320; loss: 1.05; acc: 0.7
Batch: 340; loss: 1.15; acc: 0.7
Batch: 360; loss: 0.98; acc: 0.75
Batch: 380; loss: 1.17; acc: 0.72
Batch: 400; loss: 0.91; acc: 0.83
Batch: 420; loss: 1.01; acc: 0.72
Batch: 440; loss: 1.19; acc: 0.64
Batch: 460; loss: 1.24; acc: 0.58
Batch: 480; loss: 1.12; acc: 0.66
Batch: 500; loss: 1.13; acc: 0.72
Batch: 520; loss: 1.08; acc: 0.7
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.16; acc: 0.69
Batch: 580; loss: 1.23; acc: 0.67
Batch: 600; loss: 1.14; acc: 0.62
Batch: 620; loss: 1.18; acc: 0.62
Batch: 640; loss: 1.05; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.78
Batch: 680; loss: 1.12; acc: 0.67
Batch: 700; loss: 0.98; acc: 0.77
Batch: 720; loss: 1.01; acc: 0.83
Batch: 740; loss: 1.03; acc: 0.75
Batch: 760; loss: 1.13; acc: 0.72
Batch: 780; loss: 1.01; acc: 0.77
Train Epoch over. train_loss: 1.08; train_accuracy: 0.72 

0.00015539093874394894
0.00014897626533638686
Batch: 0; loss: 1.08; acc: 0.78
Batch: 20; loss: 1.08; acc: 0.78
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 1.1; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.8
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 0.89; acc: 0.77
Val Epoch over. val_loss: 1.0235903916085602; val_accuracy: 0.7516918789808917 

The current subspace-distance is: 0.00014897626533638686 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 1.05; acc: 0.77
Batch: 60; loss: 1.25; acc: 0.67
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.72
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.96; acc: 0.77
Batch: 160; loss: 1.17; acc: 0.66
Batch: 180; loss: 1.04; acc: 0.73
Batch: 200; loss: 1.09; acc: 0.7
Batch: 220; loss: 1.1; acc: 0.72
Batch: 240; loss: 1.02; acc: 0.8
Batch: 260; loss: 1.09; acc: 0.73
Batch: 280; loss: 0.89; acc: 0.75
Batch: 300; loss: 1.08; acc: 0.72
Batch: 320; loss: 1.06; acc: 0.7
Batch: 340; loss: 1.01; acc: 0.72
Batch: 360; loss: 1.13; acc: 0.59
Batch: 380; loss: 1.05; acc: 0.7
Batch: 400; loss: 0.79; acc: 0.84
Batch: 420; loss: 0.93; acc: 0.75
Batch: 440; loss: 1.0; acc: 0.73
Batch: 460; loss: 1.13; acc: 0.72
Batch: 480; loss: 0.98; acc: 0.84
Batch: 500; loss: 1.03; acc: 0.81
Batch: 520; loss: 1.16; acc: 0.69
Batch: 540; loss: 1.13; acc: 0.7
Batch: 560; loss: 1.02; acc: 0.73
Batch: 580; loss: 0.94; acc: 0.83
Batch: 600; loss: 1.28; acc: 0.64
Batch: 620; loss: 0.82; acc: 0.83
Batch: 640; loss: 1.24; acc: 0.64
Batch: 660; loss: 1.16; acc: 0.69
Batch: 680; loss: 1.03; acc: 0.75
Batch: 700; loss: 1.1; acc: 0.75
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 1.18; acc: 0.67
Batch: 760; loss: 1.04; acc: 0.77
Batch: 780; loss: 1.13; acc: 0.64
Train Epoch over. train_loss: 1.07; train_accuracy: 0.72 

0.00015639257617294788
0.00014945623115636408
Batch: 0; loss: 1.06; acc: 0.77
Batch: 20; loss: 1.07; acc: 0.8
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.73
Batch: 80; loss: 0.92; acc: 0.8
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.9; acc: 0.77
Val Epoch over. val_loss: 1.0171877438095724; val_accuracy: 0.7505971337579618 

The current subspace-distance is: 0.00014945623115636408 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.66
Batch: 40; loss: 1.22; acc: 0.62
Batch: 60; loss: 1.05; acc: 0.78
Batch: 80; loss: 1.13; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.73
Batch: 120; loss: 1.04; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.8
Batch: 160; loss: 1.08; acc: 0.7
Batch: 180; loss: 1.2; acc: 0.61
Batch: 200; loss: 0.99; acc: 0.77
Batch: 220; loss: 0.97; acc: 0.8
Batch: 240; loss: 1.05; acc: 0.73
Batch: 260; loss: 1.06; acc: 0.75
Batch: 280; loss: 1.0; acc: 0.77
Batch: 300; loss: 1.25; acc: 0.61
Batch: 320; loss: 1.18; acc: 0.64
Batch: 340; loss: 1.09; acc: 0.75
Batch: 360; loss: 1.14; acc: 0.7
Batch: 380; loss: 1.06; acc: 0.75
Batch: 400; loss: 1.01; acc: 0.78
Batch: 420; loss: 1.09; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.69
Batch: 460; loss: 0.87; acc: 0.83
Batch: 480; loss: 0.99; acc: 0.78
Batch: 500; loss: 1.01; acc: 0.77
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.07; acc: 0.7
Batch: 560; loss: 1.04; acc: 0.73
Batch: 580; loss: 1.17; acc: 0.69
Batch: 600; loss: 0.99; acc: 0.78
Batch: 620; loss: 0.91; acc: 0.86
Batch: 640; loss: 1.03; acc: 0.8
Batch: 660; loss: 1.05; acc: 0.66
Batch: 680; loss: 1.16; acc: 0.64
Batch: 700; loss: 1.37; acc: 0.59
Batch: 720; loss: 1.1; acc: 0.69
Batch: 740; loss: 1.08; acc: 0.75
Batch: 760; loss: 1.09; acc: 0.73
Batch: 780; loss: 1.09; acc: 0.67
Train Epoch over. train_loss: 1.07; train_accuracy: 0.72 

0.00015903887106105685
0.00014993955846875906
Batch: 0; loss: 1.03; acc: 0.8
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 0.75; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.86; acc: 0.77
Val Epoch over. val_loss: 0.9991395758215789; val_accuracy: 0.7570660828025477 

The current subspace-distance is: 0.00014993955846875906 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.78
Batch: 40; loss: 1.06; acc: 0.73
Batch: 60; loss: 1.17; acc: 0.73
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.16; acc: 0.67
Batch: 120; loss: 1.15; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.78
Batch: 160; loss: 1.05; acc: 0.72
Batch: 180; loss: 0.85; acc: 0.78
Batch: 200; loss: 1.14; acc: 0.7
Batch: 220; loss: 0.87; acc: 0.8
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.14; acc: 0.7
Batch: 280; loss: 1.05; acc: 0.67
Batch: 300; loss: 0.99; acc: 0.77
Batch: 320; loss: 1.07; acc: 0.7
Batch: 340; loss: 1.15; acc: 0.72
Batch: 360; loss: 1.01; acc: 0.7
Batch: 380; loss: 1.02; acc: 0.8
Batch: 400; loss: 0.96; acc: 0.77
Batch: 420; loss: 1.08; acc: 0.69
Batch: 440; loss: 1.05; acc: 0.69
Batch: 460; loss: 1.06; acc: 0.75
Batch: 480; loss: 0.98; acc: 0.73
Batch: 500; loss: 0.94; acc: 0.78
Batch: 520; loss: 1.25; acc: 0.62
Batch: 540; loss: 1.01; acc: 0.7
Batch: 560; loss: 1.16; acc: 0.69
Batch: 580; loss: 1.19; acc: 0.69
Batch: 600; loss: 0.98; acc: 0.83
Batch: 620; loss: 1.05; acc: 0.7
Batch: 640; loss: 1.06; acc: 0.7
Batch: 660; loss: 0.94; acc: 0.83
Batch: 680; loss: 0.98; acc: 0.81
Batch: 700; loss: 0.92; acc: 0.78
Batch: 720; loss: 1.15; acc: 0.64
Batch: 740; loss: 0.96; acc: 0.77
Batch: 760; loss: 0.89; acc: 0.84
Batch: 780; loss: 1.07; acc: 0.69
Train Epoch over. train_loss: 1.06; train_accuracy: 0.72 

0.00015883382002357394
0.00015058797725941986
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.75
Batch: 80; loss: 0.9; acc: 0.81
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 0.85; acc: 0.77
Val Epoch over. val_loss: 0.991553863142706; val_accuracy: 0.7539808917197452 

The current subspace-distance is: 0.00015058797725941986 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.73
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.9; acc: 0.78
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.03; acc: 0.77
Batch: 140; loss: 1.21; acc: 0.67
Batch: 160; loss: 1.19; acc: 0.64
Batch: 180; loss: 1.08; acc: 0.67
Batch: 200; loss: 1.1; acc: 0.67
Batch: 220; loss: 1.16; acc: 0.67
Batch: 240; loss: 1.05; acc: 0.69
Batch: 260; loss: 1.02; acc: 0.7
Batch: 280; loss: 1.07; acc: 0.77
Batch: 300; loss: 1.07; acc: 0.62
Batch: 320; loss: 0.94; acc: 0.73
Batch: 340; loss: 1.15; acc: 0.67
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.19; acc: 0.61
Batch: 400; loss: 1.08; acc: 0.7
Batch: 420; loss: 1.1; acc: 0.69
Batch: 440; loss: 1.02; acc: 0.75
Batch: 460; loss: 0.95; acc: 0.78
Batch: 480; loss: 0.97; acc: 0.73
Batch: 500; loss: 1.12; acc: 0.69
Batch: 520; loss: 1.05; acc: 0.73
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 1.09; acc: 0.67
Batch: 580; loss: 1.02; acc: 0.78
Batch: 600; loss: 1.12; acc: 0.7
Batch: 620; loss: 1.18; acc: 0.66
Batch: 640; loss: 1.18; acc: 0.58
Batch: 660; loss: 1.05; acc: 0.7
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 0.98; acc: 0.69
Batch: 720; loss: 1.06; acc: 0.72
Batch: 740; loss: 0.99; acc: 0.73
Batch: 760; loss: 0.98; acc: 0.8
Batch: 780; loss: 1.12; acc: 0.67
Train Epoch over. train_loss: 1.06; train_accuracy: 0.72 

0.00016204827988985926
0.000155012181494385
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 0.9918948161374231; val_accuracy: 0.7540804140127388 

The current subspace-distance is: 0.000155012181494385 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.77
Batch: 20; loss: 1.09; acc: 0.62
Batch: 40; loss: 1.04; acc: 0.77
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 1.16; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.13; acc: 0.62
Batch: 140; loss: 1.12; acc: 0.67
Batch: 160; loss: 1.1; acc: 0.69
Batch: 180; loss: 1.05; acc: 0.72
Batch: 200; loss: 1.09; acc: 0.69
Batch: 220; loss: 1.27; acc: 0.56
Batch: 240; loss: 1.08; acc: 0.64
Batch: 260; loss: 0.87; acc: 0.83
Batch: 280; loss: 1.02; acc: 0.78
Batch: 300; loss: 1.15; acc: 0.69
Batch: 320; loss: 1.05; acc: 0.8
Batch: 340; loss: 0.97; acc: 0.73
Batch: 360; loss: 1.18; acc: 0.69
Batch: 380; loss: 0.99; acc: 0.72
Batch: 400; loss: 0.93; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.73
Batch: 440; loss: 0.99; acc: 0.78
Batch: 460; loss: 1.23; acc: 0.66
Batch: 480; loss: 1.11; acc: 0.61
Batch: 500; loss: 1.07; acc: 0.72
Batch: 520; loss: 1.05; acc: 0.8
Batch: 540; loss: 0.91; acc: 0.77
Batch: 560; loss: 0.85; acc: 0.88
Batch: 580; loss: 1.03; acc: 0.72
Batch: 600; loss: 1.2; acc: 0.62
Batch: 620; loss: 1.01; acc: 0.75
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.01; acc: 0.75
Batch: 680; loss: 1.16; acc: 0.59
Batch: 700; loss: 0.95; acc: 0.77
Batch: 720; loss: 1.08; acc: 0.75
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 1.22; acc: 0.66
Batch: 780; loss: 1.15; acc: 0.7
Train Epoch over. train_loss: 1.06; train_accuracy: 0.72 

0.00016036290617194027
0.00015307475405279547
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 1.05; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 1.05; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.86; acc: 0.77
Val Epoch over. val_loss: 0.9952897915414943; val_accuracy: 0.7527866242038217 

The current subspace-distance is: 0.00015307475405279547 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 0.95; acc: 0.8
Batch: 80; loss: 1.05; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.07; acc: 0.75
Batch: 140; loss: 1.08; acc: 0.67
Batch: 160; loss: 1.05; acc: 0.75
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 1.1; acc: 0.67
Batch: 220; loss: 0.97; acc: 0.83
Batch: 240; loss: 1.15; acc: 0.7
Batch: 260; loss: 1.06; acc: 0.73
Batch: 280; loss: 0.88; acc: 0.8
Batch: 300; loss: 1.09; acc: 0.7
Batch: 320; loss: 1.09; acc: 0.72
Batch: 340; loss: 1.17; acc: 0.7
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.15; acc: 0.66
Batch: 400; loss: 0.97; acc: 0.73
Batch: 420; loss: 1.05; acc: 0.73
Batch: 440; loss: 1.1; acc: 0.66
Batch: 460; loss: 1.06; acc: 0.62
Batch: 480; loss: 1.02; acc: 0.73
Batch: 500; loss: 1.04; acc: 0.75
Batch: 520; loss: 1.0; acc: 0.8
Batch: 540; loss: 1.08; acc: 0.69
Batch: 560; loss: 1.18; acc: 0.72
Batch: 580; loss: 0.89; acc: 0.78
Batch: 600; loss: 1.16; acc: 0.64
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 0.96; acc: 0.77
Batch: 660; loss: 1.15; acc: 0.62
Batch: 680; loss: 1.06; acc: 0.75
Batch: 700; loss: 1.11; acc: 0.72
Batch: 720; loss: 1.06; acc: 0.72
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 1.12; acc: 0.67
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.0001598364469828084
0.00015496721607632935
Batch: 0; loss: 1.01; acc: 0.8
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 0.9; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.85; acc: 0.77
Val Epoch over. val_loss: 0.9906283897958743; val_accuracy: 0.7507961783439491 

The current subspace-distance is: 0.00015496721607632935 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.95; acc: 0.8
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.8
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 1.04; acc: 0.72
Batch: 140; loss: 1.02; acc: 0.73
Batch: 160; loss: 1.21; acc: 0.67
Batch: 180; loss: 1.11; acc: 0.69
Batch: 200; loss: 0.98; acc: 0.75
Batch: 220; loss: 1.27; acc: 0.64
Batch: 240; loss: 1.09; acc: 0.67
Batch: 260; loss: 0.96; acc: 0.75
Batch: 280; loss: 1.14; acc: 0.7
Batch: 300; loss: 0.98; acc: 0.75
Batch: 320; loss: 1.0; acc: 0.72
Batch: 340; loss: 0.98; acc: 0.72
Batch: 360; loss: 1.08; acc: 0.72
Batch: 380; loss: 1.03; acc: 0.7
Batch: 400; loss: 1.06; acc: 0.78
Batch: 420; loss: 1.06; acc: 0.8
Batch: 440; loss: 1.13; acc: 0.72
Batch: 460; loss: 1.09; acc: 0.72
Batch: 480; loss: 1.11; acc: 0.66
Batch: 500; loss: 1.03; acc: 0.72
Batch: 520; loss: 1.17; acc: 0.61
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.03; acc: 0.73
Batch: 580; loss: 1.1; acc: 0.75
Batch: 600; loss: 1.03; acc: 0.66
Batch: 620; loss: 1.04; acc: 0.69
Batch: 640; loss: 1.0; acc: 0.77
Batch: 660; loss: 0.95; acc: 0.73
Batch: 680; loss: 1.09; acc: 0.72
Batch: 700; loss: 0.93; acc: 0.81
Batch: 720; loss: 1.05; acc: 0.73
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 1.09; acc: 0.66
Batch: 780; loss: 1.1; acc: 0.69
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.0001643604482524097
0.00015356035146396607
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 1.05; acc: 0.77
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 1.07; acc: 0.73
Batch: 80; loss: 0.92; acc: 0.81
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.85; acc: 0.77
Val Epoch over. val_loss: 0.9975729155692326; val_accuracy: 0.7515923566878981 

The current subspace-distance is: 0.00015356035146396607 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.72
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.96; acc: 0.81
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 1.01; acc: 0.77
Batch: 120; loss: 1.19; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.7
Batch: 160; loss: 1.16; acc: 0.66
Batch: 180; loss: 1.07; acc: 0.77
Batch: 200; loss: 0.97; acc: 0.75
Batch: 220; loss: 1.06; acc: 0.73
Batch: 240; loss: 1.08; acc: 0.7
Batch: 260; loss: 1.0; acc: 0.72
Batch: 280; loss: 1.1; acc: 0.67
Batch: 300; loss: 0.97; acc: 0.78
Batch: 320; loss: 0.9; acc: 0.81
Batch: 340; loss: 1.14; acc: 0.66
Batch: 360; loss: 1.09; acc: 0.7
Batch: 380; loss: 0.96; acc: 0.75
Batch: 400; loss: 1.03; acc: 0.73
Batch: 420; loss: 1.06; acc: 0.66
Batch: 440; loss: 1.15; acc: 0.67
Batch: 460; loss: 1.05; acc: 0.77
Batch: 480; loss: 0.97; acc: 0.75
Batch: 500; loss: 0.96; acc: 0.75
Batch: 520; loss: 1.24; acc: 0.62
Batch: 540; loss: 0.91; acc: 0.77
Batch: 560; loss: 0.92; acc: 0.78
Batch: 580; loss: 1.05; acc: 0.72
Batch: 600; loss: 1.21; acc: 0.64
Batch: 620; loss: 1.08; acc: 0.66
Batch: 640; loss: 1.12; acc: 0.66
Batch: 660; loss: 1.1; acc: 0.69
Batch: 680; loss: 1.12; acc: 0.69
Batch: 700; loss: 1.05; acc: 0.73
Batch: 720; loss: 1.16; acc: 0.7
Batch: 740; loss: 1.04; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.64
Batch: 780; loss: 1.05; acc: 0.72
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.00016314393724314868
0.00015451751824002713
Batch: 0; loss: 1.03; acc: 0.78
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.8
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 0.86; acc: 0.77
Val Epoch over. val_loss: 0.9989476355777425; val_accuracy: 0.7535828025477707 

The current subspace-distance is: 0.00015451751824002713 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.78
Batch: 20; loss: 1.09; acc: 0.72
Batch: 40; loss: 0.92; acc: 0.78
Batch: 60; loss: 1.1; acc: 0.77
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.09; acc: 0.75
Batch: 140; loss: 0.94; acc: 0.72
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 0.96; acc: 0.78
Batch: 200; loss: 0.9; acc: 0.81
Batch: 220; loss: 0.91; acc: 0.8
Batch: 240; loss: 1.08; acc: 0.7
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 1.11; acc: 0.66
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 1.02; acc: 0.7
Batch: 340; loss: 1.05; acc: 0.72
Batch: 360; loss: 1.03; acc: 0.73
Batch: 380; loss: 1.22; acc: 0.62
Batch: 400; loss: 0.99; acc: 0.73
Batch: 420; loss: 1.01; acc: 0.77
Batch: 440; loss: 1.23; acc: 0.62
Batch: 460; loss: 1.13; acc: 0.72
Batch: 480; loss: 0.97; acc: 0.78
Batch: 500; loss: 1.11; acc: 0.69
Batch: 520; loss: 1.2; acc: 0.64
Batch: 540; loss: 0.95; acc: 0.78
Batch: 560; loss: 1.08; acc: 0.75
Batch: 580; loss: 1.01; acc: 0.75
Batch: 600; loss: 1.09; acc: 0.73
Batch: 620; loss: 1.1; acc: 0.66
Batch: 640; loss: 0.83; acc: 0.78
Batch: 660; loss: 1.08; acc: 0.69
Batch: 680; loss: 0.96; acc: 0.8
Batch: 700; loss: 1.27; acc: 0.56
Batch: 720; loss: 0.94; acc: 0.7
Batch: 740; loss: 1.04; acc: 0.7
Batch: 760; loss: 1.01; acc: 0.73
Batch: 780; loss: 1.14; acc: 0.73
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.0001630830520298332
0.00015336780052166432
Batch: 0; loss: 1.02; acc: 0.78
Batch: 20; loss: 1.02; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 0.91; acc: 0.8
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 0.85; acc: 0.77
Val Epoch over. val_loss: 0.9918923457716681; val_accuracy: 0.7552746815286624 

The current subspace-distance is: 0.00015336780052166432 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.2; acc: 0.64
Batch: 20; loss: 1.06; acc: 0.64
Batch: 40; loss: 1.04; acc: 0.69
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 0.99; acc: 0.72
Batch: 120; loss: 1.08; acc: 0.62
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 1.1; acc: 0.7
Batch: 180; loss: 1.1; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.7
Batch: 220; loss: 1.08; acc: 0.69
Batch: 240; loss: 1.12; acc: 0.69
Batch: 260; loss: 1.21; acc: 0.69
Batch: 280; loss: 0.79; acc: 0.83
Batch: 300; loss: 0.99; acc: 0.69
Batch: 320; loss: 1.16; acc: 0.67
Batch: 340; loss: 1.02; acc: 0.77
Batch: 360; loss: 1.07; acc: 0.73
Batch: 380; loss: 1.04; acc: 0.72
Batch: 400; loss: 0.94; acc: 0.77
Batch: 420; loss: 0.95; acc: 0.8
Batch: 440; loss: 1.02; acc: 0.7
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 1.19; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 1.19; acc: 0.64
Batch: 540; loss: 0.96; acc: 0.78
Batch: 560; loss: 0.98; acc: 0.78
Batch: 580; loss: 1.01; acc: 0.67
Batch: 600; loss: 0.97; acc: 0.78
Batch: 620; loss: 0.96; acc: 0.7
Batch: 640; loss: 1.09; acc: 0.66
Batch: 660; loss: 1.12; acc: 0.67
Batch: 680; loss: 1.04; acc: 0.7
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.0; acc: 0.84
Batch: 780; loss: 1.05; acc: 0.73
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.00016474440053571016
0.0001571593020344153
Batch: 0; loss: 0.99; acc: 0.81
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 1.03; acc: 0.73
Batch: 80; loss: 0.89; acc: 0.81
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 0.9792693443359084; val_accuracy: 0.754578025477707 

The current subspace-distance is: 0.0001571593020344153 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 0.94; acc: 0.78
Batch: 40; loss: 1.14; acc: 0.67
Batch: 60; loss: 1.03; acc: 0.73
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 0.97; acc: 0.78
Batch: 120; loss: 0.94; acc: 0.8
Batch: 140; loss: 1.21; acc: 0.7
Batch: 160; loss: 1.13; acc: 0.73
Batch: 180; loss: 1.15; acc: 0.72
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 1.06; acc: 0.69
Batch: 240; loss: 0.93; acc: 0.8
Batch: 260; loss: 0.94; acc: 0.78
Batch: 280; loss: 1.0; acc: 0.75
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 1.0; acc: 0.77
Batch: 340; loss: 0.99; acc: 0.8
Batch: 360; loss: 1.12; acc: 0.67
Batch: 380; loss: 1.15; acc: 0.69
Batch: 400; loss: 0.92; acc: 0.81
Batch: 420; loss: 1.1; acc: 0.69
Batch: 440; loss: 0.91; acc: 0.8
Batch: 460; loss: 1.02; acc: 0.73
Batch: 480; loss: 1.07; acc: 0.69
Batch: 500; loss: 0.98; acc: 0.73
Batch: 520; loss: 0.9; acc: 0.77
Batch: 540; loss: 1.11; acc: 0.66
Batch: 560; loss: 1.24; acc: 0.58
Batch: 580; loss: 1.16; acc: 0.69
Batch: 600; loss: 1.01; acc: 0.81
Batch: 620; loss: 1.0; acc: 0.73
Batch: 640; loss: 1.17; acc: 0.62
Batch: 660; loss: 1.12; acc: 0.72
Batch: 680; loss: 1.02; acc: 0.75
Batch: 700; loss: 1.2; acc: 0.62
Batch: 720; loss: 1.19; acc: 0.7
Batch: 740; loss: 1.13; acc: 0.72
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 0.93; acc: 0.75
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

0.00016355383559130132
0.00015645781240891665
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 1.03; acc: 0.73
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 1.03; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 0.9817792594812478; val_accuracy: 0.7570660828025477 

The current subspace-distance is: 0.00015645781240891665 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.18; acc: 0.73
Batch: 40; loss: 1.1; acc: 0.67
Batch: 60; loss: 0.96; acc: 0.77
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 1.14; acc: 0.58
Batch: 120; loss: 1.07; acc: 0.73
Batch: 140; loss: 1.31; acc: 0.67
Batch: 160; loss: 1.06; acc: 0.75
Batch: 180; loss: 0.93; acc: 0.8
Batch: 200; loss: 1.15; acc: 0.66
Batch: 220; loss: 1.1; acc: 0.67
Batch: 240; loss: 1.07; acc: 0.77
Batch: 260; loss: 0.94; acc: 0.75
Batch: 280; loss: 1.09; acc: 0.7
Batch: 300; loss: 1.04; acc: 0.64
Batch: 320; loss: 1.1; acc: 0.72
Batch: 340; loss: 0.78; acc: 0.84
Batch: 360; loss: 1.06; acc: 0.69
Batch: 380; loss: 0.96; acc: 0.73
Batch: 400; loss: 1.14; acc: 0.75
Batch: 420; loss: 0.97; acc: 0.8
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.09; acc: 0.69
Batch: 480; loss: 1.15; acc: 0.62
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.87; acc: 0.83
Batch: 540; loss: 1.14; acc: 0.69
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 1.06; acc: 0.73
Batch: 600; loss: 1.29; acc: 0.59
Batch: 620; loss: 0.9; acc: 0.8
Batch: 640; loss: 1.07; acc: 0.7
Batch: 660; loss: 1.08; acc: 0.67
Batch: 680; loss: 1.12; acc: 0.66
Batch: 700; loss: 1.1; acc: 0.7
Batch: 720; loss: 1.22; acc: 0.59
Batch: 740; loss: 1.09; acc: 0.67
Batch: 760; loss: 1.19; acc: 0.59
Batch: 780; loss: 0.9; acc: 0.81
Train Epoch over. train_loss: 1.04; train_accuracy: 0.72 

0.0001664635055931285
0.00015687324048485607
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 1.03; acc: 0.78
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 0.91; acc: 0.78
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.83; acc: 0.77
Val Epoch over. val_loss: 0.9860730805214802; val_accuracy: 0.7552746815286624 

The current subspace-distance is: 0.00015687324048485607 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 1.38; acc: 0.56
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.25; acc: 0.56
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.89; acc: 0.81
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 0.91; acc: 0.77
Batch: 200; loss: 1.0; acc: 0.73
Batch: 220; loss: 1.0; acc: 0.73
Batch: 240; loss: 1.0; acc: 0.75
Batch: 260; loss: 1.17; acc: 0.66
Batch: 280; loss: 1.11; acc: 0.69
Batch: 300; loss: 1.08; acc: 0.77
Batch: 320; loss: 1.05; acc: 0.73
Batch: 340; loss: 0.89; acc: 0.8
Batch: 360; loss: 1.09; acc: 0.62
Batch: 380; loss: 0.96; acc: 0.77
Batch: 400; loss: 1.15; acc: 0.66
Batch: 420; loss: 1.02; acc: 0.69
Batch: 440; loss: 0.98; acc: 0.73
Batch: 460; loss: 0.84; acc: 0.84
Batch: 480; loss: 1.15; acc: 0.62
Batch: 500; loss: 1.04; acc: 0.75
Batch: 520; loss: 1.01; acc: 0.77
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 1.14; acc: 0.72
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.09; acc: 0.72
Batch: 620; loss: 1.16; acc: 0.64
Batch: 640; loss: 1.02; acc: 0.77
Batch: 660; loss: 1.15; acc: 0.64
Batch: 680; loss: 1.02; acc: 0.75
Batch: 700; loss: 1.07; acc: 0.67
Batch: 720; loss: 0.8; acc: 0.88
Batch: 740; loss: 1.3; acc: 0.64
Batch: 760; loss: 1.09; acc: 0.64
Batch: 780; loss: 1.07; acc: 0.69
Train Epoch over. train_loss: 1.04; train_accuracy: 0.72 

0.00016577223141212016
0.00015791320765856653
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 1.03; acc: 0.78
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.72
Batch: 80; loss: 0.9; acc: 0.81
Batch: 100; loss: 1.03; acc: 0.73
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 0.9808206736661826; val_accuracy: 0.7540804140127388 

The current subspace-distance is: 0.00015791320765856653 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_14_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6227353450270319

The number of parameters is: 249772

The number of individual parameters is:

13
234
13
13
20
34840
20
20
39
104520
39
39
64
104832
64
64
4096
64
640
10
64
64

nonzero elements in E: 49954396
elements in E: 49954400
fraction nonzero: 0.9999999199269733
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.14
Batch: 20; loss: 2.25; acc: 0.11
Batch: 40; loss: 2.1; acc: 0.23
Batch: 60; loss: 1.95; acc: 0.38
Batch: 80; loss: 1.85; acc: 0.47
Batch: 100; loss: 1.79; acc: 0.52
Batch: 120; loss: 1.8; acc: 0.52
Batch: 140; loss: 1.76; acc: 0.58
Batch: 160; loss: 1.77; acc: 0.55
Batch: 180; loss: 1.64; acc: 0.64
Batch: 200; loss: 1.69; acc: 0.61
Batch: 220; loss: 1.67; acc: 0.64
Batch: 240; loss: 1.62; acc: 0.66
Batch: 260; loss: 1.7; acc: 0.62
Batch: 280; loss: 1.57; acc: 0.66
Batch: 300; loss: 1.53; acc: 0.67
Batch: 320; loss: 1.55; acc: 0.72
Batch: 340; loss: 1.53; acc: 0.64
Batch: 360; loss: 1.56; acc: 0.66
Batch: 380; loss: 1.56; acc: 0.62
Batch: 400; loss: 1.51; acc: 0.7
Batch: 420; loss: 1.52; acc: 0.67
Batch: 440; loss: 1.51; acc: 0.7
Batch: 460; loss: 1.47; acc: 0.7
Batch: 480; loss: 1.44; acc: 0.73
Batch: 500; loss: 1.44; acc: 0.72
Batch: 520; loss: 1.49; acc: 0.69
Batch: 540; loss: 1.44; acc: 0.78
Batch: 560; loss: 1.5; acc: 0.67
Batch: 580; loss: 1.48; acc: 0.67
Batch: 600; loss: 1.39; acc: 0.7
Batch: 620; loss: 1.5; acc: 0.69
Batch: 640; loss: 1.33; acc: 0.8
Batch: 660; loss: 1.39; acc: 0.69
Batch: 680; loss: 1.45; acc: 0.72
Batch: 700; loss: 1.33; acc: 0.81
Batch: 720; loss: 1.43; acc: 0.7
Batch: 740; loss: 1.26; acc: 0.81
Batch: 760; loss: 1.22; acc: 0.84
Batch: 780; loss: 1.44; acc: 0.69
Train Epoch over. train_loss: 1.6; train_accuracy: 0.62 

5.715104634873569e-05
5.277758100419305e-05
Batch: 0; loss: 1.35; acc: 0.72
Batch: 20; loss: 1.35; acc: 0.72
Batch: 40; loss: 1.12; acc: 0.86
Batch: 60; loss: 1.33; acc: 0.8
Batch: 80; loss: 1.14; acc: 0.89
Batch: 100; loss: 1.37; acc: 0.77
Batch: 120; loss: 1.48; acc: 0.69
Batch: 140; loss: 1.17; acc: 0.84
Val Epoch over. val_loss: 1.3272102144873066; val_accuracy: 0.7609474522292994 

The current subspace-distance is: 5.277758100419305e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.8
Batch: 20; loss: 1.37; acc: 0.77
Batch: 40; loss: 1.38; acc: 0.73
Batch: 60; loss: 1.31; acc: 0.81
Batch: 80; loss: 1.55; acc: 0.62
Batch: 100; loss: 1.39; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.78
Batch: 140; loss: 1.27; acc: 0.83
Batch: 160; loss: 1.41; acc: 0.7
Batch: 180; loss: 1.32; acc: 0.73
Batch: 200; loss: 1.31; acc: 0.75
Batch: 220; loss: 1.35; acc: 0.69
Batch: 240; loss: 1.27; acc: 0.86
Batch: 260; loss: 1.23; acc: 0.77
Batch: 280; loss: 1.37; acc: 0.67
Batch: 300; loss: 1.24; acc: 0.78
Batch: 320; loss: 1.25; acc: 0.72
Batch: 340; loss: 1.25; acc: 0.77
Batch: 360; loss: 1.34; acc: 0.72
Batch: 380; loss: 1.28; acc: 0.7
Batch: 400; loss: 1.35; acc: 0.69
Batch: 420; loss: 1.2; acc: 0.83
Batch: 440; loss: 1.27; acc: 0.7
Batch: 460; loss: 1.25; acc: 0.75
Batch: 480; loss: 1.18; acc: 0.77
Batch: 500; loss: 1.25; acc: 0.73
Batch: 520; loss: 1.29; acc: 0.7
Batch: 540; loss: 1.23; acc: 0.77
Batch: 560; loss: 1.31; acc: 0.75
Batch: 580; loss: 1.21; acc: 0.8
Batch: 600; loss: 1.17; acc: 0.8
Batch: 620; loss: 1.2; acc: 0.73
Batch: 640; loss: 1.21; acc: 0.78
Batch: 660; loss: 1.28; acc: 0.72
Batch: 680; loss: 1.2; acc: 0.73
Batch: 700; loss: 1.22; acc: 0.75
Batch: 720; loss: 1.16; acc: 0.8
Batch: 740; loss: 1.29; acc: 0.73
Batch: 760; loss: 1.24; acc: 0.75
Batch: 780; loss: 1.07; acc: 0.83
Train Epoch over. train_loss: 1.27; train_accuracy: 0.75 

8.192928362404928e-05
7.686411845497787e-05
Batch: 0; loss: 1.2; acc: 0.73
Batch: 20; loss: 1.17; acc: 0.72
Batch: 40; loss: 0.9; acc: 0.88
Batch: 60; loss: 1.09; acc: 0.86
Batch: 80; loss: 0.96; acc: 0.91
Batch: 100; loss: 1.13; acc: 0.81
Batch: 120; loss: 1.35; acc: 0.69
Batch: 140; loss: 0.93; acc: 0.88
Val Epoch over. val_loss: 1.112830433116597; val_accuracy: 0.8128980891719745 

The current subspace-distance is: 7.686411845497787e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.66
Batch: 20; loss: 1.08; acc: 0.8
Batch: 40; loss: 1.1; acc: 0.78
Batch: 60; loss: 1.16; acc: 0.77
Batch: 80; loss: 1.15; acc: 0.8
Batch: 100; loss: 1.24; acc: 0.75
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 1.27; acc: 0.69
Batch: 160; loss: 1.17; acc: 0.81
Batch: 180; loss: 1.19; acc: 0.73
Batch: 200; loss: 1.15; acc: 0.8
Batch: 220; loss: 1.19; acc: 0.72
Batch: 240; loss: 1.22; acc: 0.75
Batch: 260; loss: 1.12; acc: 0.8
Batch: 280; loss: 1.07; acc: 0.81
Batch: 300; loss: 1.13; acc: 0.81
Batch: 320; loss: 1.12; acc: 0.77
Batch: 340; loss: 1.01; acc: 0.83
Batch: 360; loss: 1.16; acc: 0.8
Batch: 380; loss: 1.09; acc: 0.77
Batch: 400; loss: 1.1; acc: 0.75
Batch: 420; loss: 1.14; acc: 0.78
Batch: 440; loss: 1.18; acc: 0.73
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.01; acc: 0.86
Batch: 500; loss: 1.08; acc: 0.84
Batch: 520; loss: 1.1; acc: 0.8
Batch: 540; loss: 1.14; acc: 0.75
Batch: 560; loss: 0.99; acc: 0.89
Batch: 580; loss: 1.05; acc: 0.83
Batch: 600; loss: 0.96; acc: 0.88
Batch: 620; loss: 1.03; acc: 0.86
Batch: 640; loss: 1.1; acc: 0.78
Batch: 660; loss: 1.06; acc: 0.81
Batch: 680; loss: 1.11; acc: 0.75
Batch: 700; loss: 1.04; acc: 0.81
Batch: 720; loss: 1.03; acc: 0.83
Batch: 740; loss: 0.98; acc: 0.84
Batch: 760; loss: 0.97; acc: 0.88
Batch: 780; loss: 0.91; acc: 0.91
Train Epoch over. train_loss: 1.11; train_accuracy: 0.79 

0.00010273692168993875
9.820938430493698e-05
Batch: 0; loss: 1.07; acc: 0.8
Batch: 20; loss: 1.07; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.91
Batch: 60; loss: 0.92; acc: 0.88
Batch: 80; loss: 0.84; acc: 0.91
Batch: 100; loss: 0.96; acc: 0.86
Batch: 120; loss: 1.26; acc: 0.69
Batch: 140; loss: 0.84; acc: 0.86
Val Epoch over. val_loss: 0.9865504472878328; val_accuracy: 0.8335987261146497 

The current subspace-distance is: 9.820938430493698e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.83
Batch: 20; loss: 1.0; acc: 0.8
Batch: 40; loss: 0.94; acc: 0.81
Batch: 60; loss: 1.02; acc: 0.78
Batch: 80; loss: 1.03; acc: 0.78
Batch: 100; loss: 1.04; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.86
Batch: 140; loss: 1.05; acc: 0.75
Batch: 160; loss: 1.11; acc: 0.75
Batch: 180; loss: 0.91; acc: 0.94
Batch: 200; loss: 1.06; acc: 0.75
Batch: 220; loss: 0.96; acc: 0.83
Batch: 240; loss: 1.0; acc: 0.75
Batch: 260; loss: 1.13; acc: 0.73
Batch: 280; loss: 0.96; acc: 0.84
Batch: 300; loss: 0.98; acc: 0.86
Batch: 320; loss: 0.87; acc: 0.84
Batch: 340; loss: 1.03; acc: 0.77
Batch: 360; loss: 0.96; acc: 0.84
Batch: 380; loss: 0.96; acc: 0.84
Batch: 400; loss: 1.03; acc: 0.86
Batch: 420; loss: 0.98; acc: 0.83
Batch: 440; loss: 0.96; acc: 0.78
Batch: 460; loss: 0.89; acc: 0.86
Batch: 480; loss: 0.91; acc: 0.81
Batch: 500; loss: 1.1; acc: 0.7
Batch: 520; loss: 0.89; acc: 0.84
Batch: 540; loss: 0.82; acc: 0.89
Batch: 560; loss: 0.95; acc: 0.84
Batch: 580; loss: 1.02; acc: 0.78
Batch: 600; loss: 0.77; acc: 0.91
Batch: 620; loss: 0.85; acc: 0.91
Batch: 640; loss: 1.01; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.8
Batch: 680; loss: 0.92; acc: 0.81
Batch: 700; loss: 1.1; acc: 0.81
Batch: 720; loss: 0.87; acc: 0.86
Batch: 740; loss: 1.0; acc: 0.8
Batch: 760; loss: 1.05; acc: 0.78
Batch: 780; loss: 1.0; acc: 0.8
Train Epoch over. train_loss: 0.99; train_accuracy: 0.81 

0.00012116725702071562
0.00011470734170870855
Batch: 0; loss: 0.97; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.75
Batch: 40; loss: 0.66; acc: 0.92
Batch: 60; loss: 0.83; acc: 0.86
Batch: 80; loss: 0.77; acc: 0.91
Batch: 100; loss: 0.85; acc: 0.86
Batch: 120; loss: 1.15; acc: 0.69
Batch: 140; loss: 0.73; acc: 0.89
Val Epoch over. val_loss: 0.8835604433800764; val_accuracy: 0.8468351910828026 

The current subspace-distance is: 0.00011470734170870855 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.8
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 0.9; acc: 0.86
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 0.96; acc: 0.83
Batch: 100; loss: 0.83; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.89; acc: 0.88
Batch: 160; loss: 0.92; acc: 0.81
Batch: 180; loss: 0.85; acc: 0.86
Batch: 200; loss: 0.92; acc: 0.84
Batch: 220; loss: 0.95; acc: 0.8
Batch: 240; loss: 0.93; acc: 0.78
Batch: 260; loss: 0.89; acc: 0.89
Batch: 280; loss: 0.94; acc: 0.8
Batch: 300; loss: 0.9; acc: 0.84
Batch: 320; loss: 0.94; acc: 0.83
Batch: 340; loss: 0.88; acc: 0.84
Batch: 360; loss: 0.91; acc: 0.83
Batch: 380; loss: 0.82; acc: 0.86
Batch: 400; loss: 1.07; acc: 0.67
Batch: 420; loss: 1.05; acc: 0.75
Batch: 440; loss: 0.88; acc: 0.83
Batch: 460; loss: 0.82; acc: 0.8
Batch: 480; loss: 0.86; acc: 0.84
Batch: 500; loss: 0.98; acc: 0.78
Batch: 520; loss: 0.88; acc: 0.81
Batch: 540; loss: 1.01; acc: 0.81
Batch: 560; loss: 0.91; acc: 0.86
Batch: 580; loss: 1.0; acc: 0.83
Batch: 600; loss: 0.94; acc: 0.78
Batch: 620; loss: 0.96; acc: 0.77
Batch: 640; loss: 0.92; acc: 0.8
Batch: 660; loss: 0.89; acc: 0.86
Batch: 680; loss: 0.99; acc: 0.83
Batch: 700; loss: 0.84; acc: 0.83
Batch: 720; loss: 0.93; acc: 0.81
Batch: 740; loss: 0.82; acc: 0.83
Batch: 760; loss: 0.93; acc: 0.83
Batch: 780; loss: 0.78; acc: 0.86
Train Epoch over. train_loss: 0.92; train_accuracy: 0.82 

0.00013518687046598643
0.00012776123185176402
Batch: 0; loss: 0.91; acc: 0.86
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.95
Batch: 60; loss: 0.78; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.89
Batch: 100; loss: 0.83; acc: 0.86
Batch: 120; loss: 1.07; acc: 0.7
Batch: 140; loss: 0.68; acc: 0.92
Val Epoch over. val_loss: 0.8402613986069989; val_accuracy: 0.8469347133757962 

The current subspace-distance is: 0.00012776123185176402 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.73
Batch: 20; loss: 0.85; acc: 0.86
Batch: 40; loss: 0.81; acc: 0.84
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 0.75; acc: 0.88
Batch: 100; loss: 0.85; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.84
Batch: 140; loss: 1.03; acc: 0.72
Batch: 160; loss: 0.83; acc: 0.86
Batch: 180; loss: 0.87; acc: 0.8
Batch: 200; loss: 0.83; acc: 0.86
Batch: 220; loss: 0.9; acc: 0.81
Batch: 240; loss: 1.03; acc: 0.77
Batch: 260; loss: 0.81; acc: 0.83
Batch: 280; loss: 0.95; acc: 0.72
Batch: 300; loss: 0.99; acc: 0.78
Batch: 320; loss: 0.97; acc: 0.75
Batch: 340; loss: 0.78; acc: 0.84
Batch: 360; loss: 1.07; acc: 0.78
Batch: 380; loss: 0.95; acc: 0.78
Batch: 400; loss: 1.01; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.84
Batch: 440; loss: 0.79; acc: 0.91
Batch: 460; loss: 1.02; acc: 0.8
Batch: 480; loss: 0.83; acc: 0.86
Batch: 500; loss: 0.82; acc: 0.83
Batch: 520; loss: 0.9; acc: 0.77
Batch: 540; loss: 1.04; acc: 0.75
Batch: 560; loss: 0.75; acc: 0.91
Batch: 580; loss: 0.85; acc: 0.83
Batch: 600; loss: 0.85; acc: 0.84
Batch: 620; loss: 0.93; acc: 0.8
Batch: 640; loss: 0.88; acc: 0.86
Batch: 660; loss: 0.79; acc: 0.89
Batch: 680; loss: 0.92; acc: 0.77
Batch: 700; loss: 0.8; acc: 0.91
Batch: 720; loss: 0.95; acc: 0.73
Batch: 740; loss: 0.87; acc: 0.78
Batch: 760; loss: 0.85; acc: 0.83
Batch: 780; loss: 0.84; acc: 0.89
Train Epoch over. train_loss: 0.87; train_accuracy: 0.82 

0.0001469315611757338
0.00013945794489700347
Batch: 0; loss: 0.86; acc: 0.89
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.56; acc: 0.97
Batch: 60; loss: 0.73; acc: 0.88
Batch: 80; loss: 0.7; acc: 0.94
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.91
Val Epoch over. val_loss: 0.7968370386749316; val_accuracy: 0.8482285031847133 

The current subspace-distance is: 0.00013945794489700347 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.8
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.9; acc: 0.86
Batch: 60; loss: 1.04; acc: 0.8
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.83
Batch: 140; loss: 1.01; acc: 0.77
Batch: 160; loss: 0.85; acc: 0.8
Batch: 180; loss: 0.78; acc: 0.86
Batch: 200; loss: 0.87; acc: 0.75
Batch: 220; loss: 0.82; acc: 0.86
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.89
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.84; acc: 0.77
Batch: 320; loss: 0.85; acc: 0.84
Batch: 340; loss: 0.92; acc: 0.75
Batch: 360; loss: 0.74; acc: 0.88
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 1.02; acc: 0.75
Batch: 420; loss: 0.98; acc: 0.75
Batch: 440; loss: 0.89; acc: 0.86
Batch: 460; loss: 0.8; acc: 0.81
Batch: 480; loss: 0.76; acc: 0.91
Batch: 500; loss: 0.89; acc: 0.78
Batch: 520; loss: 0.82; acc: 0.84
Batch: 540; loss: 0.9; acc: 0.8
Batch: 560; loss: 0.66; acc: 0.92
Batch: 580; loss: 0.77; acc: 0.92
Batch: 600; loss: 0.81; acc: 0.78
Batch: 620; loss: 0.79; acc: 0.91
Batch: 640; loss: 0.8; acc: 0.83
Batch: 660; loss: 0.84; acc: 0.84
Batch: 680; loss: 0.78; acc: 0.86
Batch: 700; loss: 0.86; acc: 0.86
Batch: 720; loss: 0.92; acc: 0.75
Batch: 740; loss: 0.92; acc: 0.81
Batch: 760; loss: 0.86; acc: 0.78
Batch: 780; loss: 0.76; acc: 0.86
Train Epoch over. train_loss: 0.84; train_accuracy: 0.83 

0.00015752970648463815
0.0001510091678937897
Batch: 0; loss: 0.82; acc: 0.89
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.5; acc: 0.97
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.89
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.56; acc: 0.91
Val Epoch over. val_loss: 0.747541539798117; val_accuracy: 0.8524084394904459 

The current subspace-distance is: 0.0001510091678937897 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.81
Batch: 20; loss: 0.86; acc: 0.81
Batch: 40; loss: 0.79; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.86
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.81; acc: 0.83
Batch: 160; loss: 0.91; acc: 0.78
Batch: 180; loss: 0.87; acc: 0.77
Batch: 200; loss: 0.78; acc: 0.83
Batch: 220; loss: 0.92; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.86
Batch: 260; loss: 0.69; acc: 0.91
Batch: 280; loss: 0.73; acc: 0.84
Batch: 300; loss: 0.77; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.96; acc: 0.73
Batch: 360; loss: 0.83; acc: 0.77
Batch: 380; loss: 0.77; acc: 0.84
Batch: 400; loss: 0.83; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.89
Batch: 440; loss: 0.73; acc: 0.84
Batch: 460; loss: 0.81; acc: 0.8
Batch: 480; loss: 0.74; acc: 0.83
Batch: 500; loss: 0.84; acc: 0.84
Batch: 520; loss: 0.83; acc: 0.8
Batch: 540; loss: 0.83; acc: 0.86
Batch: 560; loss: 0.78; acc: 0.86
Batch: 580; loss: 0.89; acc: 0.78
Batch: 600; loss: 0.94; acc: 0.73
Batch: 620; loss: 0.7; acc: 0.89
Batch: 640; loss: 0.87; acc: 0.77
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.91
Batch: 700; loss: 0.73; acc: 0.84
Batch: 720; loss: 0.95; acc: 0.78
Batch: 740; loss: 0.78; acc: 0.81
Batch: 760; loss: 0.81; acc: 0.84
Batch: 780; loss: 0.75; acc: 0.84
Train Epoch over. train_loss: 0.81; train_accuracy: 0.83 

0.00016404810594394803
0.00015708620776422322
Batch: 0; loss: 0.79; acc: 0.88
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.47; acc: 0.97
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.7254281078174616; val_accuracy: 0.854796974522293 

The current subspace-distance is: 0.00015708620776422322 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.84
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.86
Batch: 100; loss: 0.92; acc: 0.75
Batch: 120; loss: 0.82; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.84
Batch: 180; loss: 0.78; acc: 0.83
Batch: 200; loss: 0.57; acc: 0.92
Batch: 220; loss: 0.76; acc: 0.89
Batch: 240; loss: 0.79; acc: 0.83
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 0.7; acc: 0.94
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.79; acc: 0.84
Batch: 360; loss: 0.83; acc: 0.81
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.94; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.81
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.86; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.75
Batch: 520; loss: 0.88; acc: 0.84
Batch: 540; loss: 0.83; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.88
Batch: 580; loss: 0.75; acc: 0.84
Batch: 600; loss: 0.92; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.8
Batch: 640; loss: 0.82; acc: 0.81
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.87; acc: 0.78
Batch: 700; loss: 0.74; acc: 0.88
Batch: 720; loss: 0.84; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.88; acc: 0.77
Batch: 780; loss: 0.88; acc: 0.81
Train Epoch over. train_loss: 0.78; train_accuracy: 0.83 

0.0001754754630383104
0.0001677230466157198
Batch: 0; loss: 0.77; acc: 0.86
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.6981539485181213; val_accuracy: 0.857484076433121 

The current subspace-distance is: 0.0001677230466157198 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.8
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 0.89; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.78; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.95; acc: 0.8
Batch: 240; loss: 0.92; acc: 0.77
Batch: 260; loss: 0.69; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.72; acc: 0.89
Batch: 320; loss: 0.84; acc: 0.77
Batch: 340; loss: 0.77; acc: 0.8
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.84; acc: 0.77
Batch: 400; loss: 0.89; acc: 0.78
Batch: 420; loss: 0.63; acc: 0.89
Batch: 440; loss: 0.8; acc: 0.83
Batch: 460; loss: 0.87; acc: 0.8
Batch: 480; loss: 0.76; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.87; acc: 0.78
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.85; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.86
Batch: 640; loss: 0.77; acc: 0.84
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.84; acc: 0.78
Batch: 700; loss: 0.67; acc: 0.88
Batch: 720; loss: 0.78; acc: 0.86
Batch: 740; loss: 0.79; acc: 0.81
Batch: 760; loss: 0.87; acc: 0.69
Batch: 780; loss: 0.8; acc: 0.84
Train Epoch over. train_loss: 0.76; train_accuracy: 0.83 

0.00017943991406355053
0.00017236733401659876
Batch: 0; loss: 0.76; acc: 0.88
Batch: 20; loss: 0.76; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.45; acc: 0.94
Val Epoch over. val_loss: 0.6741593252321717; val_accuracy: 0.859375 

The current subspace-distance is: 0.00017236733401659876 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.83
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.82; acc: 0.78
Batch: 220; loss: 0.82; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.84; acc: 0.8
Batch: 300; loss: 0.96; acc: 0.8
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.77
Batch: 380; loss: 0.52; acc: 0.92
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.81; acc: 0.86
Batch: 480; loss: 0.84; acc: 0.81
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.75; acc: 0.83
Batch: 540; loss: 0.78; acc: 0.83
Batch: 560; loss: 0.78; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.86
Batch: 600; loss: 0.58; acc: 0.92
Batch: 620; loss: 0.73; acc: 0.89
Batch: 640; loss: 0.85; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.88
Batch: 680; loss: 0.78; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.86
Batch: 720; loss: 0.81; acc: 0.83
Batch: 740; loss: 0.86; acc: 0.77
Batch: 760; loss: 0.8; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.0001839231263147667
0.00017610909708309919
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 0.44; acc: 0.95
Val Epoch over. val_loss: 0.6648262133643885; val_accuracy: 0.8623606687898089 

The current subspace-distance is: 0.00017610909708309919 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.86; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.88
Batch: 40; loss: 0.75; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.92
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.94
Batch: 240; loss: 0.68; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.89
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.74; acc: 0.84
Batch: 320; loss: 0.93; acc: 0.73
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.7; acc: 0.81
Batch: 400; loss: 0.7; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.89
Batch: 480; loss: 0.7; acc: 0.84
Batch: 500; loss: 0.92; acc: 0.77
Batch: 520; loss: 0.8; acc: 0.81
Batch: 540; loss: 0.91; acc: 0.8
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.81; acc: 0.84
Batch: 640; loss: 0.75; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.95
Batch: 680; loss: 0.67; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.97
Batch: 720; loss: 0.68; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.73; train_accuracy: 0.84 

0.00018536837887950242
0.00017810415010899305
Batch: 0; loss: 0.74; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 0.44; acc: 0.95
Val Epoch over. val_loss: 0.6659911914615874; val_accuracy: 0.8615644904458599 

The current subspace-distance is: 0.00017810415010899305 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.84; acc: 0.83
Batch: 80; loss: 0.82; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.71; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.92
Batch: 220; loss: 0.78; acc: 0.81
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.9; acc: 0.8
Batch: 280; loss: 0.84; acc: 0.73
Batch: 300; loss: 0.79; acc: 0.84
Batch: 320; loss: 0.71; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.74; acc: 0.84
Batch: 380; loss: 0.68; acc: 0.8
Batch: 400; loss: 0.61; acc: 0.92
Batch: 420; loss: 0.79; acc: 0.81
Batch: 440; loss: 0.78; acc: 0.77
Batch: 460; loss: 0.64; acc: 0.94
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.78; acc: 0.8
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.88; acc: 0.7
Batch: 620; loss: 0.79; acc: 0.81
Batch: 640; loss: 0.69; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.91
Batch: 700; loss: 0.8; acc: 0.83
Batch: 720; loss: 0.78; acc: 0.81
Batch: 740; loss: 0.81; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.73; train_accuracy: 0.84 

0.0001877657778095454
0.0001825470826588571
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.69
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.6474533194948913; val_accuracy: 0.8630573248407644 

The current subspace-distance is: 0.0001825470826588571 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.81
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.83; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.88
Batch: 140; loss: 0.85; acc: 0.77
Batch: 160; loss: 0.72; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.89
Batch: 200; loss: 0.81; acc: 0.77
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.66; acc: 0.89
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.82; acc: 0.77
Batch: 320; loss: 0.78; acc: 0.83
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.83
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.76; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.79; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 0.84; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.81
Batch: 560; loss: 0.76; acc: 0.88
Batch: 580; loss: 0.75; acc: 0.81
Batch: 600; loss: 0.74; acc: 0.8
Batch: 620; loss: 0.76; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.91
Batch: 660; loss: 0.77; acc: 0.77
Batch: 680; loss: 0.76; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.76; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.0001903892552945763
0.00018118410662282258
Batch: 0; loss: 0.71; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.72
Batch: 140; loss: 0.43; acc: 0.97
Val Epoch over. val_loss: 0.6547825121955507; val_accuracy: 0.8625597133757962 

The current subspace-distance is: 0.00018118410662282258 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.92
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.95; acc: 0.73
Batch: 200; loss: 0.81; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.91
Batch: 240; loss: 0.73; acc: 0.84
Batch: 260; loss: 0.74; acc: 0.78
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.81
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.79; acc: 0.75
Batch: 480; loss: 0.66; acc: 0.91
Batch: 500; loss: 0.92; acc: 0.75
Batch: 520; loss: 0.65; acc: 0.89
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.88
Batch: 640; loss: 0.82; acc: 0.81
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.83
Batch: 700; loss: 0.79; acc: 0.81
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.88; acc: 0.77
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.72; train_accuracy: 0.84 

0.00019360041187610477
0.00018684560200199485
Batch: 0; loss: 0.72; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.95
Val Epoch over. val_loss: 0.6489875328009296; val_accuracy: 0.8644506369426752 

The current subspace-distance is: 0.00018684560200199485 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.77; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.91
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.72; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.75; acc: 0.84
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.74; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.66; acc: 0.91
Batch: 340; loss: 0.88; acc: 0.7
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.91
Batch: 400; loss: 0.64; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.86
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.79; acc: 0.77
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.78; acc: 0.81
Batch: 540; loss: 0.76; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.82; acc: 0.77
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.81; acc: 0.77
Batch: 640; loss: 0.6; acc: 0.92
Batch: 660; loss: 0.77; acc: 0.78
Batch: 680; loss: 0.7; acc: 0.83
Batch: 700; loss: 0.75; acc: 0.84
Batch: 720; loss: 0.71; acc: 0.88
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.8; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.84 

0.00019311568757984787
0.00018532053218223155
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.7
Batch: 140; loss: 0.41; acc: 0.92
Val Epoch over. val_loss: 0.6341354932375015; val_accuracy: 0.8664410828025477 

The current subspace-distance is: 0.00018532053218223155 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.88
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.86
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.74; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.88
Batch: 320; loss: 0.83; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.81
Batch: 360; loss: 0.69; acc: 0.91
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.83; acc: 0.73
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.64; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.8; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.8; acc: 0.83
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.86; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.71; acc: 0.8
Batch: 780; loss: 0.83; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00019376518321223557
0.00018529962108004838
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.91
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.6303058325485059; val_accuracy: 0.8626592356687898 

The current subspace-distance is: 0.00018529962108004838 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.72; acc: 0.72
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.75
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.84; acc: 0.77
Batch: 200; loss: 0.63; acc: 0.89
Batch: 220; loss: 0.83; acc: 0.75
Batch: 240; loss: 0.65; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.88
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.55; acc: 0.91
Batch: 320; loss: 0.68; acc: 0.88
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 0.6; acc: 0.86
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.91; acc: 0.77
Batch: 460; loss: 0.58; acc: 0.92
Batch: 480; loss: 0.72; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.92
Batch: 520; loss: 0.7; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.82; acc: 0.81
Batch: 580; loss: 0.53; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.79; acc: 0.8
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.78; acc: 0.77
Batch: 760; loss: 0.91; acc: 0.69
Batch: 780; loss: 0.69; acc: 0.83
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00019640543905552477
0.0001887302059913054
Batch: 0; loss: 0.69; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.92
Val Epoch over. val_loss: 0.6278854562978077; val_accuracy: 0.8654458598726115 

The current subspace-distance is: 0.0001887302059913054 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.87; acc: 0.8
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.91
Batch: 200; loss: 0.69; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.67; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.88
Batch: 440; loss: 0.81; acc: 0.77
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.76; acc: 0.78
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 0.61; acc: 0.94
Batch: 560; loss: 0.73; acc: 0.86
Batch: 580; loss: 0.93; acc: 0.69
Batch: 600; loss: 0.52; acc: 0.92
Batch: 620; loss: 0.66; acc: 0.86
Batch: 640; loss: 0.6; acc: 0.89
Batch: 660; loss: 0.67; acc: 0.88
Batch: 680; loss: 0.77; acc: 0.83
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.7; acc: 0.86
Batch: 780; loss: 0.79; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.00019741064170375466
0.00018886230827774853
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.6147659652552028; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 0.00018886230827774853 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.91
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.79; acc: 0.78
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.84
Batch: 360; loss: 0.51; acc: 0.94
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.81; acc: 0.8
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.84
Batch: 460; loss: 0.85; acc: 0.73
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.91
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.91
Batch: 600; loss: 0.79; acc: 0.78
Batch: 620; loss: 0.69; acc: 0.84
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.88
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.64; acc: 0.81
Batch: 720; loss: 0.68; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.75
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.8
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.00020032604516018182
0.00019334345415700227
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.6042335266899911; val_accuracy: 0.8695262738853503 

The current subspace-distance is: 0.00019334345415700227 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.94
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.91
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.75
Batch: 180; loss: 0.76; acc: 0.84
Batch: 200; loss: 0.81; acc: 0.81
Batch: 220; loss: 0.75; acc: 0.83
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.89
Batch: 280; loss: 0.64; acc: 0.88
Batch: 300; loss: 0.74; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.95
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.93; acc: 0.77
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.91
Batch: 500; loss: 0.68; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.91
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.71; acc: 0.81
Batch: 600; loss: 0.66; acc: 0.83
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.77; acc: 0.73
Batch: 660; loss: 0.71; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.92
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00020228097855579108
0.0001944645046023652
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.86; acc: 0.7
Batch: 140; loss: 0.4; acc: 0.95
Val Epoch over. val_loss: 0.6188080096320742; val_accuracy: 0.8643511146496815 

The current subspace-distance is: 0.0001944645046023652 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.6; acc: 0.91
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.59; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.84; acc: 0.77
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.82; acc: 0.83
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.62; acc: 0.91
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.76; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 0.77; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.89
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.92
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.59; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.94
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.89
Batch: 720; loss: 0.71; acc: 0.86
Batch: 740; loss: 0.71; acc: 0.86
Batch: 760; loss: 0.7; acc: 0.89
Batch: 780; loss: 0.68; acc: 0.89
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00020358336041681468
0.0001940060028573498
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.37; acc: 0.94
Val Epoch over. val_loss: 0.6038486528548466; val_accuracy: 0.8684315286624203 

The current subspace-distance is: 0.0001940060028573498 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.9; acc: 0.73
Batch: 160; loss: 0.74; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.88
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.76; acc: 0.78
Batch: 260; loss: 0.8; acc: 0.75
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.91
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.69; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.86
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.72; acc: 0.88
Batch: 640; loss: 0.75; acc: 0.83
Batch: 660; loss: 0.71; acc: 0.81
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.91
Batch: 760; loss: 0.8; acc: 0.77
Batch: 780; loss: 0.66; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00020233026589266956
0.00019381530000828207
Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.598399594521067; val_accuracy: 0.8730095541401274 

The current subspace-distance is: 0.00019381530000828207 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.81; acc: 0.75
Batch: 40; loss: 0.7; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.89; acc: 0.66
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.81; acc: 0.78
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.72; acc: 0.86
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.71; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.71; acc: 0.83
Batch: 340; loss: 0.85; acc: 0.81
Batch: 360; loss: 0.62; acc: 0.88
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.79; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.71; acc: 0.81
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.77
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 0.62; acc: 0.91
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.91
Batch: 740; loss: 0.83; acc: 0.77
Batch: 760; loss: 0.6; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.91
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.0002056148659903556
0.00019819196313619614
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.94
Val Epoch over. val_loss: 0.6093501917495849; val_accuracy: 0.8698248407643312 

The current subspace-distance is: 0.00019819196313619614 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.72
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.78; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.87; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.78; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.65; acc: 0.88
Batch: 380; loss: 0.73; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.84
Batch: 420; loss: 0.72; acc: 0.8
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.74; acc: 0.84
Batch: 540; loss: 0.71; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.76; acc: 0.8
Batch: 640; loss: 0.72; acc: 0.73
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.88
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 0.56; acc: 0.92
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.0002038148231804371
0.0001961435191333294
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.94
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.94
Val Epoch over. val_loss: 0.6040763779050985; val_accuracy: 0.8689291401273885 

The current subspace-distance is: 0.0001961435191333294 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.81; acc: 0.8
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.78
Batch: 160; loss: 0.8; acc: 0.77
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.71; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.88
Batch: 300; loss: 0.78; acc: 0.77
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.72; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.92
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.74; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.63; acc: 0.89
Batch: 580; loss: 0.82; acc: 0.75
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.69; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.69; acc: 0.8
Batch: 680; loss: 0.69; acc: 0.84
Batch: 700; loss: 0.8; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.87; acc: 0.75
Batch: 760; loss: 0.6; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00020503137784544379
0.00019688847532961518
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5998985264331672; val_accuracy: 0.8714171974522293 

The current subspace-distance is: 0.00019688847532961518 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.71; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.55; acc: 0.91
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.76; acc: 0.7
Batch: 380; loss: 0.75; acc: 0.75
Batch: 400; loss: 0.59; acc: 0.89
Batch: 420; loss: 0.71; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.82; acc: 0.77
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 0.65; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.91
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.67; acc: 0.89
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020552492060232908
0.00019894144497811794
Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5971335506742927; val_accuracy: 0.8733081210191083 

The current subspace-distance is: 0.00019894144497811794 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.62; acc: 0.88
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.8; acc: 0.8
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.73; acc: 0.8
Batch: 300; loss: 0.76; acc: 0.78
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 0.72; acc: 0.8
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.88; acc: 0.77
Batch: 460; loss: 0.68; acc: 0.81
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.95
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 0.71; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 0.58; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.89
Batch: 660; loss: 0.71; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.91
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.55; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.88
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.91
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020274794951546937
0.00019617137149907649
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.5989314980187993; val_accuracy: 0.8674363057324841 

The current subspace-distance is: 0.00019617137149907649 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.85; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.72
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.78; acc: 0.77
Batch: 200; loss: 0.81; acc: 0.84
Batch: 220; loss: 0.81; acc: 0.78
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.83; acc: 0.73
Batch: 280; loss: 0.65; acc: 0.89
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.64; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.91
Batch: 380; loss: 0.65; acc: 0.84
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.71; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.61; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.92
Batch: 540; loss: 0.83; acc: 0.78
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.71; acc: 0.83
Batch: 600; loss: 0.83; acc: 0.75
Batch: 620; loss: 0.71; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.94
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.84
Batch: 700; loss: 0.75; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.88
Batch: 740; loss: 0.65; acc: 0.88
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.0002064287255052477
0.00019718112889677286
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.6008719446932435; val_accuracy: 0.8674363057324841 

The current subspace-distance is: 0.00019718112889677286 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.89
Batch: 40; loss: 0.86; acc: 0.72
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.74; acc: 0.81
Batch: 240; loss: 0.88; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.94
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.78; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.86
Batch: 420; loss: 0.78; acc: 0.78
Batch: 440; loss: 0.66; acc: 0.86
Batch: 460; loss: 0.81; acc: 0.77
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.94
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 0.7; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.78
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.89
Batch: 640; loss: 0.65; acc: 0.86
Batch: 660; loss: 0.63; acc: 0.88
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 0.72; acc: 0.78
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.82; acc: 0.75
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020678623695857823
0.00019676492956932634
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.94
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.6002142909605792; val_accuracy: 0.868531050955414 

The current subspace-distance is: 0.00019676492956932634 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_14_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6227353450270319

The number of parameters is: 249772

The number of individual parameters is:

13
234
13
13
20
34840
20
20
39
104520
39
39
64
104832
64
64
4096
64
640
10
64
64

nonzero elements in E: 74931595
elements in E: 74931600
fraction nonzero: 0.9999999332724778
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.25; acc: 0.16
Batch: 20; loss: 2.07; acc: 0.33
Batch: 40; loss: 1.9; acc: 0.41
Batch: 60; loss: 1.83; acc: 0.52
Batch: 80; loss: 1.83; acc: 0.44
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.58; acc: 0.62
Batch: 140; loss: 1.59; acc: 0.66
Batch: 160; loss: 1.55; acc: 0.62
Batch: 180; loss: 1.46; acc: 0.78
Batch: 200; loss: 1.48; acc: 0.72
Batch: 220; loss: 1.42; acc: 0.73
Batch: 240; loss: 1.53; acc: 0.62
Batch: 260; loss: 1.47; acc: 0.67
Batch: 280; loss: 1.39; acc: 0.72
Batch: 300; loss: 1.4; acc: 0.73
Batch: 320; loss: 1.36; acc: 0.75
Batch: 340; loss: 1.38; acc: 0.67
Batch: 360; loss: 1.4; acc: 0.75
Batch: 380; loss: 1.39; acc: 0.67
Batch: 400; loss: 1.28; acc: 0.75
Batch: 420; loss: 1.33; acc: 0.69
Batch: 440; loss: 1.32; acc: 0.7
Batch: 460; loss: 1.35; acc: 0.72
Batch: 480; loss: 1.27; acc: 0.81
Batch: 500; loss: 1.26; acc: 0.8
Batch: 520; loss: 1.32; acc: 0.69
Batch: 540; loss: 1.18; acc: 0.83
Batch: 560; loss: 1.26; acc: 0.75
Batch: 580; loss: 1.36; acc: 0.64
Batch: 600; loss: 1.17; acc: 0.77
Batch: 620; loss: 1.31; acc: 0.67
Batch: 640; loss: 1.29; acc: 0.69
Batch: 660; loss: 1.25; acc: 0.77
Batch: 680; loss: 1.21; acc: 0.8
Batch: 700; loss: 1.41; acc: 0.62
Batch: 720; loss: 1.13; acc: 0.83
Batch: 740; loss: 1.12; acc: 0.77
Batch: 760; loss: 1.24; acc: 0.72
Batch: 780; loss: 1.24; acc: 0.7
Train Epoch over. train_loss: 1.42; train_accuracy: 0.69 

5.9629499446600676e-05
5.4669191740686074e-05
Batch: 0; loss: 1.22; acc: 0.78
Batch: 20; loss: 1.26; acc: 0.72
Batch: 40; loss: 0.86; acc: 0.94
Batch: 60; loss: 1.1; acc: 0.83
Batch: 80; loss: 1.0; acc: 0.84
Batch: 100; loss: 1.1; acc: 0.88
Batch: 120; loss: 1.22; acc: 0.75
Batch: 140; loss: 0.97; acc: 0.89
Val Epoch over. val_loss: 1.1272470924505003; val_accuracy: 0.8090167197452229 

The current subspace-distance is: 5.4669191740686074e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.86
Batch: 20; loss: 1.22; acc: 0.77
Batch: 40; loss: 1.07; acc: 0.89
Batch: 60; loss: 1.07; acc: 0.84
Batch: 80; loss: 1.12; acc: 0.83
Batch: 100; loss: 1.01; acc: 0.89
Batch: 120; loss: 1.14; acc: 0.84
Batch: 140; loss: 1.18; acc: 0.72
Batch: 160; loss: 1.12; acc: 0.81
Batch: 180; loss: 1.15; acc: 0.73
Batch: 200; loss: 1.17; acc: 0.78
Batch: 220; loss: 1.18; acc: 0.8
Batch: 240; loss: 1.09; acc: 0.83
Batch: 260; loss: 1.11; acc: 0.77
Batch: 280; loss: 1.19; acc: 0.73
Batch: 300; loss: 1.21; acc: 0.7
Batch: 320; loss: 1.13; acc: 0.77
Batch: 340; loss: 1.19; acc: 0.77
Batch: 360; loss: 1.08; acc: 0.73
Batch: 380; loss: 1.07; acc: 0.78
Batch: 400; loss: 1.08; acc: 0.77
Batch: 420; loss: 1.11; acc: 0.75
Batch: 440; loss: 1.1; acc: 0.78
Batch: 460; loss: 0.98; acc: 0.86
Batch: 480; loss: 1.04; acc: 0.83
Batch: 500; loss: 1.11; acc: 0.81
Batch: 520; loss: 1.09; acc: 0.81
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 1.04; acc: 0.77
Batch: 580; loss: 0.94; acc: 0.88
Batch: 600; loss: 1.13; acc: 0.73
Batch: 620; loss: 1.07; acc: 0.78
Batch: 640; loss: 1.09; acc: 0.8
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 1.19; acc: 0.67
Batch: 700; loss: 1.07; acc: 0.69
Batch: 720; loss: 0.98; acc: 0.8
Batch: 740; loss: 0.94; acc: 0.84
Batch: 760; loss: 0.99; acc: 0.81
Batch: 780; loss: 0.96; acc: 0.84
Train Epoch over. train_loss: 1.09; train_accuracy: 0.79 

8.07431570137851e-05
7.608810119563714e-05
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.06; acc: 0.73
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.89; acc: 0.91
Batch: 80; loss: 0.79; acc: 0.92
Batch: 100; loss: 0.9; acc: 0.91
Batch: 120; loss: 1.03; acc: 0.83
Batch: 140; loss: 0.81; acc: 0.91
Val Epoch over. val_loss: 0.948321896373846; val_accuracy: 0.838077229299363 

The current subspace-distance is: 7.608810119563714e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.78
Batch: 20; loss: 1.0; acc: 0.84
Batch: 40; loss: 1.06; acc: 0.8
Batch: 60; loss: 1.08; acc: 0.78
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 0.91; acc: 0.89
Batch: 140; loss: 0.94; acc: 0.83
Batch: 160; loss: 0.97; acc: 0.81
Batch: 180; loss: 1.06; acc: 0.77
Batch: 200; loss: 0.9; acc: 0.91
Batch: 220; loss: 0.95; acc: 0.84
Batch: 240; loss: 0.89; acc: 0.83
Batch: 260; loss: 0.9; acc: 0.88
Batch: 280; loss: 1.07; acc: 0.75
Batch: 300; loss: 0.97; acc: 0.78
Batch: 320; loss: 0.99; acc: 0.81
Batch: 340; loss: 1.06; acc: 0.73
Batch: 360; loss: 0.85; acc: 0.88
Batch: 380; loss: 0.97; acc: 0.84
Batch: 400; loss: 0.93; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.84
Batch: 440; loss: 0.92; acc: 0.88
Batch: 460; loss: 0.99; acc: 0.81
Batch: 480; loss: 0.81; acc: 0.88
Batch: 500; loss: 0.98; acc: 0.78
Batch: 520; loss: 0.96; acc: 0.8
Batch: 540; loss: 0.81; acc: 0.86
Batch: 560; loss: 0.79; acc: 0.89
Batch: 580; loss: 0.87; acc: 0.86
Batch: 600; loss: 0.88; acc: 0.86
Batch: 620; loss: 0.91; acc: 0.81
Batch: 640; loss: 0.94; acc: 0.81
Batch: 660; loss: 0.86; acc: 0.86
Batch: 680; loss: 0.9; acc: 0.81
Batch: 700; loss: 0.96; acc: 0.78
Batch: 720; loss: 0.91; acc: 0.86
Batch: 740; loss: 0.81; acc: 0.84
Batch: 760; loss: 0.89; acc: 0.88
Batch: 780; loss: 0.88; acc: 0.88
Train Epoch over. train_loss: 0.94; train_accuracy: 0.82 

9.826543100643903e-05
9.37290460569784e-05
Batch: 0; loss: 0.9; acc: 0.83
Batch: 20; loss: 0.91; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.97
Batch: 60; loss: 0.78; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.94
Batch: 100; loss: 0.78; acc: 0.92
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 0.73; acc: 0.88
Val Epoch over. val_loss: 0.8196928466960882; val_accuracy: 0.8573845541401274 

The current subspace-distance is: 9.37290460569784e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.91
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.85; acc: 0.83
Batch: 100; loss: 0.85; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.93; acc: 0.8
Batch: 160; loss: 0.8; acc: 0.94
Batch: 180; loss: 0.83; acc: 0.88
Batch: 200; loss: 0.9; acc: 0.83
Batch: 220; loss: 1.04; acc: 0.77
Batch: 240; loss: 0.82; acc: 0.84
Batch: 260; loss: 0.92; acc: 0.77
Batch: 280; loss: 0.78; acc: 0.84
Batch: 300; loss: 0.88; acc: 0.81
Batch: 320; loss: 0.73; acc: 0.91
Batch: 340; loss: 0.67; acc: 0.95
Batch: 360; loss: 0.76; acc: 0.86
Batch: 380; loss: 0.85; acc: 0.88
Batch: 400; loss: 0.84; acc: 0.83
Batch: 420; loss: 0.92; acc: 0.78
Batch: 440; loss: 0.93; acc: 0.81
Batch: 460; loss: 0.86; acc: 0.78
Batch: 480; loss: 0.8; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.89
Batch: 520; loss: 0.84; acc: 0.83
Batch: 540; loss: 0.83; acc: 0.83
Batch: 560; loss: 0.74; acc: 0.89
Batch: 580; loss: 0.88; acc: 0.84
Batch: 600; loss: 0.91; acc: 0.86
Batch: 620; loss: 0.84; acc: 0.8
Batch: 640; loss: 0.9; acc: 0.83
Batch: 660; loss: 0.72; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.89
Batch: 700; loss: 0.89; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.95
Batch: 740; loss: 0.77; acc: 0.89
Batch: 760; loss: 0.79; acc: 0.84
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.84; train_accuracy: 0.84 

0.00011239412560826167
0.00010610004392219707
Batch: 0; loss: 0.83; acc: 0.83
Batch: 20; loss: 0.86; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.97
Batch: 60; loss: 0.74; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.94
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.86
Val Epoch over. val_loss: 0.7646284695643528; val_accuracy: 0.8669386942675159 

The current subspace-distance is: 0.00010610004392219707 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.75
Batch: 20; loss: 0.75; acc: 0.89
Batch: 40; loss: 0.8; acc: 0.86
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.82; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.91
Batch: 120; loss: 0.94; acc: 0.78
Batch: 140; loss: 0.69; acc: 0.89
Batch: 160; loss: 0.92; acc: 0.78
Batch: 180; loss: 0.75; acc: 0.84
Batch: 200; loss: 0.81; acc: 0.81
Batch: 220; loss: 0.81; acc: 0.83
Batch: 240; loss: 0.84; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.91
Batch: 280; loss: 0.69; acc: 0.91
Batch: 300; loss: 0.94; acc: 0.83
Batch: 320; loss: 0.9; acc: 0.78
Batch: 340; loss: 0.84; acc: 0.83
Batch: 360; loss: 0.72; acc: 0.92
Batch: 380; loss: 0.79; acc: 0.88
Batch: 400; loss: 0.67; acc: 0.95
Batch: 420; loss: 0.91; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.89
Batch: 480; loss: 0.86; acc: 0.84
Batch: 500; loss: 0.7; acc: 0.88
Batch: 520; loss: 0.73; acc: 0.88
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.79; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.89
Batch: 600; loss: 0.79; acc: 0.83
Batch: 620; loss: 0.87; acc: 0.8
Batch: 640; loss: 0.89; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.92
Batch: 680; loss: 0.84; acc: 0.88
Batch: 700; loss: 0.71; acc: 0.86
Batch: 720; loss: 0.79; acc: 0.84
Batch: 740; loss: 0.78; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.84
Batch: 780; loss: 0.83; acc: 0.84
Train Epoch over. train_loss: 0.78; train_accuracy: 0.84 

0.00012309021258261055
0.0001179899409180507
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.97
Batch: 60; loss: 0.69; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.92
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.65; acc: 0.91
Val Epoch over. val_loss: 0.7063632776403124; val_accuracy: 0.8672372611464968 

The current subspace-distance is: 0.0001179899409180507 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.75; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.75; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.86
Batch: 140; loss: 0.85; acc: 0.77
Batch: 160; loss: 0.72; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.97
Batch: 240; loss: 0.66; acc: 0.86
Batch: 260; loss: 0.79; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.95
Batch: 300; loss: 0.68; acc: 0.89
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.78; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.95
Batch: 400; loss: 0.66; acc: 0.91
Batch: 420; loss: 0.79; acc: 0.8
Batch: 440; loss: 0.73; acc: 0.86
Batch: 460; loss: 0.71; acc: 0.86
Batch: 480; loss: 0.8; acc: 0.83
Batch: 500; loss: 0.84; acc: 0.83
Batch: 520; loss: 0.68; acc: 0.92
Batch: 540; loss: 0.82; acc: 0.77
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.69; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.73; acc: 0.84
Batch: 680; loss: 0.79; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.89
Batch: 720; loss: 0.74; acc: 0.78
Batch: 740; loss: 0.67; acc: 0.91
Batch: 760; loss: 0.74; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.94
Train Epoch over. train_loss: 0.73; train_accuracy: 0.85 

0.0001328561920672655
0.00012833937944378704
Batch: 0; loss: 0.68; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.94
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.57; acc: 0.91
Val Epoch over. val_loss: 0.6507923813762179; val_accuracy: 0.8784832802547771 

The current subspace-distance is: 0.00012833937944378704 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.89; acc: 0.83
Batch: 160; loss: 0.65; acc: 0.94
Batch: 180; loss: 0.58; acc: 0.91
Batch: 200; loss: 0.77; acc: 0.8
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.62; acc: 0.88
Batch: 260; loss: 0.79; acc: 0.81
Batch: 280; loss: 0.76; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.92
Batch: 320; loss: 0.85; acc: 0.78
Batch: 340; loss: 0.62; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.89
Batch: 380; loss: 0.66; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.91
Batch: 480; loss: 0.73; acc: 0.83
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.89
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.76; acc: 0.86
Batch: 640; loss: 0.68; acc: 0.89
Batch: 660; loss: 0.64; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.69; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.81; acc: 0.83
Batch: 780; loss: 0.81; acc: 0.77
Train Epoch over. train_loss: 0.69; train_accuracy: 0.86 

0.0001446801470592618
0.00013940967619419098
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.83
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.611179090229569; val_accuracy: 0.8852507961783439 

The current subspace-distance is: 0.00013940967619419098 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.95
Batch: 40; loss: 0.9; acc: 0.8
Batch: 60; loss: 0.58; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.92
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.74; acc: 0.75
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.5; acc: 0.92
Batch: 480; loss: 0.59; acc: 0.89
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.74; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.73; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.91
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.81; acc: 0.8
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.65; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.65; train_accuracy: 0.86 

0.00015440513379871845
0.00014620317961089313
Batch: 0; loss: 0.59; acc: 0.92
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.5824739889354463; val_accuracy: 0.8898288216560509 

The current subspace-distance is: 0.00014620317961089313 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.94
Batch: 160; loss: 0.6; acc: 0.91
Batch: 180; loss: 0.68; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.95
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.81
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.69; acc: 0.91
Batch: 380; loss: 0.61; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.89
Batch: 580; loss: 0.67; acc: 0.84
Batch: 600; loss: 0.66; acc: 0.86
Batch: 620; loss: 0.76; acc: 0.83
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.97
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.91
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.92
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.86 

0.000163355449330993
0.00015681517834309489
Batch: 0; loss: 0.57; acc: 0.94
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.94
Val Epoch over. val_loss: 0.5591615254332305; val_accuracy: 0.888734076433121 

The current subspace-distance is: 0.00015681517834309489 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 0.61; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.88
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.97
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.43; acc: 0.98
Batch: 540; loss: 0.59; acc: 0.89
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.81; acc: 0.8
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.71; acc: 0.83
Batch: 720; loss: 0.45; acc: 0.97
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.94
Train Epoch over. train_loss: 0.6; train_accuracy: 0.87 

0.00017320031474810094
0.00016507053805980831
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.43; acc: 0.94
Val Epoch over. val_loss: 0.5415844611681191; val_accuracy: 0.8898288216560509 

The current subspace-distance is: 0.00016507053805980831 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.94
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.62; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.97
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.85; acc: 0.8
Batch: 380; loss: 0.45; acc: 0.94
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.89
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.64; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.91
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.59; train_accuracy: 0.87 

0.00017524388385936618
0.00016757506818976253
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.5291557623322602; val_accuracy: 0.8928144904458599 

The current subspace-distance is: 0.00016757506818976253 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.94
Batch: 40; loss: 0.64; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.63; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.95
Batch: 240; loss: 0.58; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.94
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.95
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.94
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.0001764367043506354
0.00016927647811826319
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.5263819011153689; val_accuracy: 0.8905254777070064 

The current subspace-distance is: 0.00016927647811826319 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.46; acc: 0.95
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.94
Batch: 280; loss: 0.58; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.95
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.95
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.94
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.85; acc: 0.73
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00017940986435860395
0.00017272071272600442
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.5160296249921155; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 0.00017272071272600442 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.91
Batch: 80; loss: 0.66; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.94
Batch: 160; loss: 0.55; acc: 0.91
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.61; acc: 0.78
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.95
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.69; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.92
Batch: 600; loss: 0.48; acc: 0.92
Batch: 620; loss: 0.74; acc: 0.77
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00017992053471971303
0.00017380458302795887
Batch: 0; loss: 0.57; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.94
Val Epoch over. val_loss: 0.5115233116848453; val_accuracy: 0.8948049363057324 

The current subspace-distance is: 0.00017380458302795887 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.94
Batch: 280; loss: 0.54; acc: 0.92
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 0.59; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.94
Batch: 400; loss: 0.62; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.94
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.76; acc: 0.75
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.57; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.71; acc: 0.8
Batch: 780; loss: 0.54; acc: 0.89
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00018442448345012963
0.00017790257697924972
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.5072483410880824; val_accuracy: 0.8955015923566879 

The current subspace-distance is: 0.00017790257697924972 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.95
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.53; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.92
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.73; acc: 0.77
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.0001852589484769851
0.00017679540906101465
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.4949976446901917; val_accuracy: 0.8976910828025477 

The current subspace-distance is: 0.00017679540906101465 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.97
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.97
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.72; acc: 0.73
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.6; acc: 0.86
Batch: 420; loss: 0.74; acc: 0.77
Batch: 440; loss: 0.46; acc: 0.94
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.00018834322690963745
0.00018179234757553786
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.48943229171500846; val_accuracy: 0.8967953821656051 

The current subspace-distance is: 0.00018179234757553786 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.97
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.94
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.56; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.91
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.74; acc: 0.75
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.92
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.0001915281027322635
0.0001836182054830715
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.4822867665511028; val_accuracy: 0.9001791401273885 

The current subspace-distance is: 0.0001836182054830715 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.92
Batch: 40; loss: 0.62; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.95
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0001922494120663032
0.0001844611542765051
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.4802221280943816; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 0.0001844611542765051 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.8
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.95
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.57; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.94
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.0001946649863384664
0.0001853113790275529
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.46892320094214884; val_accuracy: 0.9008757961783439 

The current subspace-distance is: 0.0001853113790275529 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.95
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.94
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.56; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00019542047812137753
0.00019133512978442013
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.4686643849512574; val_accuracy: 0.8982882165605095 

The current subspace-distance is: 0.00019133512978442013 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.95
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.5; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019704335136339068
0.00019069877453148365
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.4706090376445442; val_accuracy: 0.8963972929936306 

The current subspace-distance is: 0.00019069877453148365 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.66; acc: 0.78
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.97
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.94
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019638500816654414
0.0001902732183225453
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.4671864841774011; val_accuracy: 0.8968949044585988 

The current subspace-distance is: 0.0001902732183225453 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.95
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.95
Batch: 620; loss: 0.51; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019655974756460637
0.00018971203826367855
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.4713435719726951; val_accuracy: 0.899781050955414 

The current subspace-distance is: 0.00018971203826367855 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.53; acc: 0.91
Batch: 320; loss: 0.6; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.91
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.94
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00019875194993801415
0.00019027017697226256
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.45842546566276793; val_accuracy: 0.900577229299363 

The current subspace-distance is: 0.00019027017697226256 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.92
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.94
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.78; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00020003801910206676
0.00019349173817317933
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.4539475048993044; val_accuracy: 0.9003781847133758 

The current subspace-distance is: 0.00019349173817317933 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.92
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.61; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020118309475947171
0.00019275041995570064
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.45164809703447256; val_accuracy: 0.9004777070063694 

The current subspace-distance is: 0.00019275041995570064 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.78
Batch: 300; loss: 0.43; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.62; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.7; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020118603424634784
0.00019382000027690083
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.46039473554890625; val_accuracy: 0.9000796178343949 

The current subspace-distance is: 0.00019382000027690083 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.94
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.81
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.0002024923887802288
0.00019310596690047532
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.455647727011875; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 0.00019310596690047532 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.94
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00020274933194741607
0.0001950695732375607
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.4549144581434833; val_accuracy: 0.8996815286624203 

The current subspace-distance is: 0.0001950695732375607 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_14_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6227353450270319

The number of parameters is: 249772

The number of individual parameters is:

13
234
13
13
20
34840
20
20
39
104520
39
39
64
104832
64
64
4096
64
640
10
64
64

nonzero elements in E: 99908790
elements in E: 99908800
fraction nonzero: 0.9999998999087167
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.4; acc: 0.05
Batch: 20; loss: 2.02; acc: 0.23
Batch: 40; loss: 1.88; acc: 0.42
Batch: 60; loss: 1.83; acc: 0.42
Batch: 80; loss: 1.63; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.64
Batch: 120; loss: 1.53; acc: 0.75
Batch: 140; loss: 1.55; acc: 0.67
Batch: 160; loss: 1.52; acc: 0.59
Batch: 180; loss: 1.42; acc: 0.75
Batch: 200; loss: 1.39; acc: 0.73
Batch: 220; loss: 1.4; acc: 0.8
Batch: 240; loss: 1.26; acc: 0.81
Batch: 260; loss: 1.41; acc: 0.64
Batch: 280; loss: 1.28; acc: 0.78
Batch: 300; loss: 1.25; acc: 0.81
Batch: 320; loss: 1.21; acc: 0.78
Batch: 340; loss: 1.16; acc: 0.83
Batch: 360; loss: 1.21; acc: 0.8
Batch: 380; loss: 1.17; acc: 0.77
Batch: 400; loss: 1.17; acc: 0.83
Batch: 420; loss: 1.16; acc: 0.81
Batch: 440; loss: 1.26; acc: 0.7
Batch: 460; loss: 1.25; acc: 0.72
Batch: 480; loss: 1.21; acc: 0.73
Batch: 500; loss: 1.11; acc: 0.78
Batch: 520; loss: 1.23; acc: 0.73
Batch: 540; loss: 1.18; acc: 0.78
Batch: 560; loss: 1.03; acc: 0.88
Batch: 580; loss: 1.01; acc: 0.86
Batch: 600; loss: 1.17; acc: 0.77
Batch: 620; loss: 1.18; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.78
Batch: 660; loss: 1.24; acc: 0.66
Batch: 680; loss: 1.01; acc: 0.88
Batch: 700; loss: 1.14; acc: 0.72
Batch: 720; loss: 1.05; acc: 0.81
Batch: 740; loss: 1.0; acc: 0.84
Batch: 760; loss: 1.03; acc: 0.84
Batch: 780; loss: 0.91; acc: 0.91
Train Epoch over. train_loss: 1.29; train_accuracy: 0.74 

2.543405571486801e-05
9.527773727313615e-06
Batch: 0; loss: 1.05; acc: 0.83
Batch: 20; loss: 1.18; acc: 0.72
Batch: 40; loss: 0.73; acc: 0.95
Batch: 60; loss: 0.95; acc: 0.83
Batch: 80; loss: 0.78; acc: 0.94
Batch: 100; loss: 0.94; acc: 0.91
Batch: 120; loss: 1.1; acc: 0.8
Batch: 140; loss: 0.78; acc: 0.92
Val Epoch over. val_loss: 0.9569457636517325; val_accuracy: 0.8546974522292994 

The current subspace-distance is: 9.527773727313615e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.84
Batch: 20; loss: 0.92; acc: 0.88
Batch: 40; loss: 0.97; acc: 0.84
Batch: 60; loss: 1.15; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.89
Batch: 140; loss: 0.95; acc: 0.84
Batch: 160; loss: 0.97; acc: 0.84
Batch: 180; loss: 0.95; acc: 0.83
Batch: 200; loss: 1.08; acc: 0.8
Batch: 220; loss: 0.86; acc: 0.83
Batch: 240; loss: 1.03; acc: 0.8
Batch: 260; loss: 0.93; acc: 0.83
Batch: 280; loss: 0.98; acc: 0.8
Batch: 300; loss: 0.91; acc: 0.86
Batch: 320; loss: 0.79; acc: 0.91
Batch: 340; loss: 0.91; acc: 0.86
Batch: 360; loss: 0.88; acc: 0.81
Batch: 380; loss: 1.04; acc: 0.77
Batch: 400; loss: 0.86; acc: 0.84
Batch: 420; loss: 0.93; acc: 0.8
Batch: 440; loss: 0.92; acc: 0.89
Batch: 460; loss: 0.88; acc: 0.83
Batch: 480; loss: 0.84; acc: 0.92
Batch: 500; loss: 0.93; acc: 0.86
Batch: 520; loss: 0.95; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.84
Batch: 580; loss: 0.83; acc: 0.86
Batch: 600; loss: 0.82; acc: 0.84
Batch: 620; loss: 0.82; acc: 0.83
Batch: 640; loss: 0.93; acc: 0.84
Batch: 660; loss: 0.93; acc: 0.81
Batch: 680; loss: 0.9; acc: 0.81
Batch: 700; loss: 0.93; acc: 0.75
Batch: 720; loss: 0.79; acc: 0.83
Batch: 740; loss: 0.92; acc: 0.78
Batch: 760; loss: 0.75; acc: 0.92
Batch: 780; loss: 0.72; acc: 0.94
Train Epoch over. train_loss: 0.91; train_accuracy: 0.84 

3.047639074793551e-05
1.2505523955042008e-05
Batch: 0; loss: 0.83; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.79; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.95
Batch: 100; loss: 0.73; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.55; acc: 0.97
Val Epoch over. val_loss: 0.7612760887024509; val_accuracy: 0.8801751592356688 

The current subspace-distance is: 1.2505523955042008e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.89
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 0.69; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.92
Batch: 100; loss: 0.72; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.91
Batch: 140; loss: 0.74; acc: 0.92
Batch: 160; loss: 0.85; acc: 0.8
Batch: 180; loss: 0.76; acc: 0.88
Batch: 200; loss: 0.86; acc: 0.77
Batch: 220; loss: 0.87; acc: 0.77
Batch: 240; loss: 0.7; acc: 0.88
Batch: 260; loss: 0.76; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.88
Batch: 300; loss: 0.79; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.89
Batch: 340; loss: 0.8; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.92
Batch: 380; loss: 0.8; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.88
Batch: 420; loss: 0.74; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.88
Batch: 460; loss: 0.88; acc: 0.78
Batch: 480; loss: 0.81; acc: 0.84
Batch: 500; loss: 0.78; acc: 0.83
Batch: 520; loss: 0.85; acc: 0.84
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.91
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.74; acc: 0.91
Batch: 620; loss: 0.63; acc: 0.91
Batch: 640; loss: 0.75; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.88
Batch: 680; loss: 0.79; acc: 0.83
Batch: 700; loss: 0.73; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.88
Batch: 740; loss: 0.7; acc: 0.86
Batch: 760; loss: 0.76; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.89
Train Epoch over. train_loss: 0.76; train_accuracy: 0.86 

3.5485423723002896e-05
1.5203905604721513e-05
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.87; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.97
Batch: 100; loss: 0.6; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.97
Val Epoch over. val_loss: 0.6400110347635427; val_accuracy: 0.89171974522293 

The current subspace-distance is: 1.5203905604721513e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.92
Batch: 180; loss: 0.58; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.92
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.89; acc: 0.7
Batch: 400; loss: 0.72; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.7; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.81
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.92
Batch: 640; loss: 0.61; acc: 0.92
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.64; acc: 0.91
Batch: 720; loss: 0.76; acc: 0.83
Batch: 740; loss: 0.88; acc: 0.72
Batch: 760; loss: 0.56; acc: 0.92
Batch: 780; loss: 0.55; acc: 0.91
Train Epoch over. train_loss: 0.67; train_accuracy: 0.87 

3.859813296003267e-05
1.718488783808425e-05
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.37; acc: 0.97
Batch: 60; loss: 0.59; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.98
Batch: 100; loss: 0.53; acc: 0.95
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.98
Val Epoch over. val_loss: 0.5738663736042703; val_accuracy: 0.899781050955414 

The current subspace-distance is: 1.718488783808425e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.92
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.92
Batch: 260; loss: 0.63; acc: 0.89
Batch: 280; loss: 0.83; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.91
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.95
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.6; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.95
Batch: 580; loss: 0.53; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.89
Batch: 660; loss: 0.6; acc: 0.91
Batch: 680; loss: 0.7; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.94
Batch: 760; loss: 0.5; acc: 0.91
Batch: 780; loss: 0.74; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

4.220490882289596e-05
1.8673705199034885e-05
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.98
Batch: 100; loss: 0.45; acc: 0.95
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.5201577743527236; val_accuracy: 0.90625 

The current subspace-distance is: 1.8673705199034885e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.98
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.61; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.89
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.5; acc: 0.92
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.68; acc: 0.86
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.92
Batch: 740; loss: 0.62; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.89 

4.5395172492135316e-05
2.108716762450058e-05
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.98
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4778376612693641; val_accuracy: 0.9104299363057324 

The current subspace-distance is: 2.108716762450058e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.47; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.94
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.95
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.85; acc: 0.81
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

4.763645483762957e-05
2.155370566470083e-05
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.97
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.25; acc: 1.0
Val Epoch over. val_loss: 0.44413318024699094; val_accuracy: 0.9147093949044586 

The current subspace-distance is: 2.155370566470083e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.97
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.83
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

5.0666771130636334e-05
2.4668706828379072e-05
Batch: 0; loss: 0.45; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.97
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.98
Val Epoch over. val_loss: 0.4225513675979748; val_accuracy: 0.9164012738853503 

The current subspace-distance is: 2.4668706828379072e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.97
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.32; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.46; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.98
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

5.278321623336524e-05
2.3515434804721735e-05
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.97
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.22; acc: 1.0
Val Epoch over. val_loss: 0.40285439210332885; val_accuracy: 0.9167993630573248 

The current subspace-distance is: 2.3515434804721735e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.3; acc: 1.0
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.95
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.405904812505469e-05
2.42012338276254e-05
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.38515278943784675; val_accuracy: 0.9173964968152867 

The current subspace-distance is: 2.42012338276254e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.32; acc: 0.97
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.97
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.95
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.6542990932939574e-05
2.699195647437591e-05
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.37623152174767416; val_accuracy: 0.919187898089172 

The current subspace-distance is: 2.699195647437591e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.26; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.36; acc: 0.97
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.32; acc: 0.98
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.6281089200638235e-05
2.6911404347629286e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.36590088856448033; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 2.6911404347629286e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.58; acc: 0.8
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.8049776271218434e-05
2.8470021788962185e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3651427574408282; val_accuracy: 0.9209792993630573 

The current subspace-distance is: 2.8470021788962185e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.725698792957701e-05
2.572689663793426e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3565465889538929; val_accuracy: 0.9227707006369427 

The current subspace-distance is: 2.572689663793426e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.94
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

5.868763764738105e-05
2.670531830517575e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.3529731805916804; val_accuracy: 0.9215764331210191 

The current subspace-distance is: 2.670531830517575e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.932826024945825e-05
2.8296850359765813e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.34897776792763147; val_accuracy: 0.9216759554140127 

The current subspace-distance is: 2.8296850359765813e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.33; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.98
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.971263090032153e-05
2.883963315980509e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.34539497895225596; val_accuracy: 0.9246616242038217 

The current subspace-distance is: 2.883963315980509e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.98
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.98
Batch: 660; loss: 0.43; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.120523903518915e-05
2.8950096748303622e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3394132622867633; val_accuracy: 0.9263535031847133 

The current subspace-distance is: 2.8950096748303622e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

6.07110996497795e-05
2.8484304493758827e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.33648385211920284; val_accuracy: 0.9255573248407644 

The current subspace-distance is: 2.8484304493758827e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.97
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.095563367125578e-05
2.7806770958704874e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.3300693200746919; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.7806770958704874e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.112658593337983e-05
2.9138425816199742e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.32909751811604593; val_accuracy: 0.9267515923566879 

The current subspace-distance is: 2.9138425816199742e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.152496644062921e-05
2.7980058803223073e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.33098159426716484; val_accuracy: 0.9276472929936306 

The current subspace-distance is: 2.7980058803223073e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.255073822103441e-05
2.9637265470228158e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.327099286731641; val_accuracy: 0.9272492038216561 

The current subspace-distance is: 2.9637265470228158e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.58; acc: 0.78
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.97
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.183421646710485e-05
2.8240619940334e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.3231472669608274; val_accuracy: 0.9272492038216561 

The current subspace-distance is: 2.8240619940334e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.8
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.364035652950406e-05
3.174463563482277e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.32799528074112666; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 3.174463563482277e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.98
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.98
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.8
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.185207894304767e-05
2.965094245155342e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.3266167211684452; val_accuracy: 0.9270501592356688 

The current subspace-distance is: 2.965094245155342e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.25148240942508e-05
2.8643766199820675e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.32691340964690896; val_accuracy: 0.9289410828025477 

The current subspace-distance is: 2.8643766199820675e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.289011798799038e-05
3.0900486308382824e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31956074434291026; val_accuracy: 0.9289410828025477 

The current subspace-distance is: 3.0900486308382824e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.4; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.287692667683586e-05
2.960511301353108e-05
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.3232326005010088; val_accuracy: 0.9281449044585988 

The current subspace-distance is: 2.960511301353108e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.97
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.98
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.297073559835553e-05
2.8399192160577513e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.32503974357038545; val_accuracy: 0.9268511146496815 

The current subspace-distance is: 2.8399192160577513e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_14_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.6227353450270319

The number of parameters is: 249772

The number of individual parameters is:

13
234
13
13
20
34840
20
20
39
104520
39
39
64
104832
64
64
4096
64
640
10
64
64

nonzero elements in E: 124885990
elements in E: 124886000
fraction nonzero: 0.9999999199269733
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.4; acc: 0.08
Batch: 20; loss: 1.88; acc: 0.5
Batch: 40; loss: 1.77; acc: 0.53
Batch: 60; loss: 1.66; acc: 0.64
Batch: 80; loss: 1.57; acc: 0.58
Batch: 100; loss: 1.56; acc: 0.64
Batch: 120; loss: 1.44; acc: 0.7
Batch: 140; loss: 1.37; acc: 0.77
Batch: 160; loss: 1.35; acc: 0.66
Batch: 180; loss: 1.37; acc: 0.66
Batch: 200; loss: 1.36; acc: 0.62
Batch: 220; loss: 1.26; acc: 0.73
Batch: 240; loss: 1.17; acc: 0.78
Batch: 260; loss: 1.22; acc: 0.8
Batch: 280; loss: 1.29; acc: 0.73
Batch: 300; loss: 1.11; acc: 0.86
Batch: 320; loss: 1.13; acc: 0.8
Batch: 340; loss: 1.09; acc: 0.8
Batch: 360; loss: 1.07; acc: 0.8
Batch: 380; loss: 1.07; acc: 0.81
Batch: 400; loss: 0.99; acc: 0.86
Batch: 420; loss: 0.97; acc: 0.84
Batch: 440; loss: 1.08; acc: 0.8
Batch: 460; loss: 1.11; acc: 0.77
Batch: 480; loss: 1.01; acc: 0.8
Batch: 500; loss: 0.97; acc: 0.83
Batch: 520; loss: 0.96; acc: 0.8
Batch: 540; loss: 0.93; acc: 0.89
Batch: 560; loss: 0.9; acc: 0.8
Batch: 580; loss: 1.0; acc: 0.78
Batch: 600; loss: 0.96; acc: 0.83
Batch: 620; loss: 0.93; acc: 0.81
Batch: 640; loss: 0.86; acc: 0.81
Batch: 660; loss: 1.04; acc: 0.8
Batch: 680; loss: 0.77; acc: 0.92
Batch: 700; loss: 0.8; acc: 0.92
Batch: 720; loss: 0.75; acc: 0.91
Batch: 740; loss: 0.81; acc: 0.88
Batch: 760; loss: 0.81; acc: 0.91
Batch: 780; loss: 0.93; acc: 0.81
Train Epoch over. train_loss: 1.15; train_accuracy: 0.76 

2.510398371668998e-05
8.32743080536602e-06
Batch: 0; loss: 0.88; acc: 0.88
Batch: 20; loss: 1.01; acc: 0.78
Batch: 40; loss: 0.53; acc: 0.94
Batch: 60; loss: 0.83; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.95
Batch: 100; loss: 0.77; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.67; acc: 0.94
Val Epoch over. val_loss: 0.7859081971417566; val_accuracy: 0.866640127388535 

The current subspace-distance is: 8.32743080536602e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.8
Batch: 20; loss: 0.94; acc: 0.8
Batch: 40; loss: 0.86; acc: 0.83
Batch: 60; loss: 0.83; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.92
Batch: 100; loss: 0.8; acc: 0.91
Batch: 120; loss: 0.88; acc: 0.83
Batch: 140; loss: 0.79; acc: 0.88
Batch: 160; loss: 0.84; acc: 0.84
Batch: 180; loss: 0.65; acc: 0.95
Batch: 200; loss: 0.72; acc: 0.91
Batch: 220; loss: 0.7; acc: 0.91
Batch: 240; loss: 0.63; acc: 0.91
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.88
Batch: 320; loss: 0.67; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.81; acc: 0.83
Batch: 400; loss: 0.67; acc: 0.89
Batch: 420; loss: 0.68; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.88
Batch: 460; loss: 0.71; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.91
Batch: 500; loss: 0.64; acc: 0.91
Batch: 520; loss: 0.87; acc: 0.8
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.94
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.65; acc: 0.91
Batch: 620; loss: 0.71; acc: 0.89
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.81; acc: 0.81
Batch: 680; loss: 0.57; acc: 0.92
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.72; acc: 0.86
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.68; acc: 0.89
Batch: 780; loss: 0.69; acc: 0.86
Train Epoch over. train_loss: 0.73; train_accuracy: 0.86 

3.168188050040044e-05
1.190037619380746e-05
Batch: 0; loss: 0.64; acc: 0.92
Batch: 20; loss: 0.84; acc: 0.8
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.94
Val Epoch over. val_loss: 0.5866201172588738; val_accuracy: 0.895203025477707 

The current subspace-distance is: 1.190037619380746e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.71; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.65; acc: 0.86
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.88
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.59; acc: 0.89
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.66; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.89
Train Epoch over. train_loss: 0.59; train_accuracy: 0.88 

3.63274484698195e-05
1.5216680367302615e-05
Batch: 0; loss: 0.48; acc: 0.97
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.98
Batch: 100; loss: 0.47; acc: 0.95
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.94
Val Epoch over. val_loss: 0.4939582892663919; val_accuracy: 0.9082404458598726 

The current subspace-distance is: 1.5216680367302615e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.92
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.49; acc: 0.92
Batch: 380; loss: 0.73; acc: 0.77
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.53; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.9 

3.996350278612226e-05
1.73821063071955e-05
Batch: 0; loss: 0.41; acc: 0.97
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.43792196311009157; val_accuracy: 0.916202229299363 

The current subspace-distance is: 1.73821063071955e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.97
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.98
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.97
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

4.338601138442755e-05
1.775314740370959e-05
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.3891015860495294; val_accuracy: 0.9204816878980892 

The current subspace-distance is: 1.775314740370959e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.81
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.97
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.97
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

4.6812794607831165e-05
2.107277759932913e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.351857392176701; val_accuracy: 0.9257563694267515 

The current subspace-distance is: 2.107277759932913e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

4.845714647672139e-05
2.086947461066302e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3254934465809233; val_accuracy: 0.9307324840764332 

The current subspace-distance is: 2.086947461066302e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.98
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.151447840034962e-05
2.2476091544376686e-05
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3127736000308565; val_accuracy: 0.9309315286624203 

The current subspace-distance is: 2.2476091544376686e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.98
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.98
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.33875499968417e-05
2.2786223780713044e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.29621987732922195; val_accuracy: 0.932921974522293 

The current subspace-distance is: 2.2786223780713044e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.601506927632727e-05
2.5498717150185257e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.27776290291244055; val_accuracy: 0.9358081210191083 

The current subspace-distance is: 2.5498717150185257e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.650324237649329e-05
2.3931863324833103e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2765655172573533; val_accuracy: 0.9375995222929936 

The current subspace-distance is: 2.3931863324833103e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.699749453924596e-05
2.491817758709658e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2751795943280694; val_accuracy: 0.9369028662420382 

The current subspace-distance is: 2.491817758709658e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.98
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.742424036725424e-05
2.4524635591660626e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.27081543444448214; val_accuracy: 0.9382961783439491 

The current subspace-distance is: 2.4524635591660626e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.830006557516754e-05
2.532556936785113e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2683396670658877; val_accuracy: 0.9395899681528662 

The current subspace-distance is: 2.532556936785113e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.98
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.98
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.848746150149964e-05
2.51180499617476e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2693265040114427; val_accuracy: 0.9375 

The current subspace-distance is: 2.51180499617476e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.38; acc: 0.84
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.98
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

5.9256093663861975e-05
2.7132131435791962e-05
Batch: 0; loss: 0.27; acc: 0.98
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2647231588033354; val_accuracy: 0.9386942675159236 

The current subspace-distance is: 2.7132131435791962e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

5.977042019367218e-05
2.5877661755657755e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26050017645974066; val_accuracy: 0.9397890127388535 

The current subspace-distance is: 2.5877661755657755e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.98
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.16; acc: 1.0
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.041419328539632e-05
2.6959434762829915e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2571953744835155; val_accuracy: 0.9397890127388535 

The current subspace-distance is: 2.6959434762829915e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.98
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.15; acc: 1.0
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.93 

6.0161695728311315e-05
2.5834639018285088e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2550246675208116; val_accuracy: 0.9398885350318471 

The current subspace-distance is: 2.5834639018285088e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.98
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.081899118726142e-05
2.707737439777702e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2530233207496868; val_accuracy: 0.9409832802547771 

The current subspace-distance is: 2.707737439777702e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.0768968978663906e-05
2.6774023353937082e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.25535983858024996; val_accuracy: 0.93859474522293 

The current subspace-distance is: 2.6774023353937082e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.138406024547294e-05
2.6846946639125235e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.2522615708268372; val_accuracy: 0.9419785031847133 

The current subspace-distance is: 2.6846946639125235e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.16; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.152314745122567e-05
2.8672651751548983e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.24678260377448075; val_accuracy: 0.9423765923566879 

The current subspace-distance is: 2.8672651751548983e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.163887883303687e-05
2.775274151645135e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.25135663991710944; val_accuracy: 0.9395899681528662 

The current subspace-distance is: 2.775274151645135e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.98
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.98
Batch: 600; loss: 0.23; acc: 0.98
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.19715137872845e-05
2.8020802346873097e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24791705304649986; val_accuracy: 0.9396894904458599 

The current subspace-distance is: 2.8020802346873097e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.98
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.171608401928097e-05
2.630917151691392e-05
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2529926195171229; val_accuracy: 0.9381966560509554 

The current subspace-distance is: 2.630917151691392e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.98
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.172583380248398e-05
2.7810840038000606e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2459305138534801; val_accuracy: 0.9421775477707006 

The current subspace-distance is: 2.7810840038000606e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.83
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.231804582057521e-05
2.8428617952158675e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.24508434614747954; val_accuracy: 0.9407842356687898 

The current subspace-distance is: 2.8428617952158675e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.93 

6.260358350118622e-05
2.7680191124090925e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.25098084283482497; val_accuracy: 0.9408837579617835 

The current subspace-distance is: 2.7680191124090925e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.98
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.93 

6.245294207474217e-05
2.8265594664844684e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24668700708325503; val_accuracy: 0.9410828025477707 

The current subspace-distance is: 2.8265594664844684e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_14_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:47/N_14_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
