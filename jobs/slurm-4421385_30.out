model : table13slim
N : 14
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 16:12:31

Channel scaling factor: 1.07

The number of parameters is: 267152

The number of individual parameters is:

9
162
9
9
13
31356
13
13
26
90584
26
26
64
139776
64
64
4096
64
640
10
64
64

nonzero elements in E: 13357598
elements in E: 13357600
fraction nonzero: 0.999999850272504
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.5; acc: 0.08
Batch: 20; loss: 2.43; acc: 0.08
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.24; acc: 0.23
Batch: 80; loss: 2.21; acc: 0.22
Batch: 100; loss: 2.32; acc: 0.22
Batch: 120; loss: 2.09; acc: 0.33
Batch: 140; loss: 2.1; acc: 0.3
Batch: 160; loss: 2.13; acc: 0.23
Batch: 180; loss: 2.07; acc: 0.34
Batch: 200; loss: 2.15; acc: 0.17
Batch: 220; loss: 2.17; acc: 0.3
Batch: 240; loss: 2.05; acc: 0.27
Batch: 260; loss: 2.08; acc: 0.33
Batch: 280; loss: 2.04; acc: 0.33
Batch: 300; loss: 2.1; acc: 0.3
Batch: 320; loss: 2.07; acc: 0.23
Batch: 340; loss: 2.0; acc: 0.34
Batch: 360; loss: 2.0; acc: 0.39
Batch: 380; loss: 1.9; acc: 0.42
Batch: 400; loss: 1.89; acc: 0.34
Batch: 420; loss: 1.87; acc: 0.41
Batch: 440; loss: 1.9; acc: 0.44
Batch: 460; loss: 1.83; acc: 0.45
Batch: 480; loss: 1.8; acc: 0.52
Batch: 500; loss: 1.98; acc: 0.39
Batch: 520; loss: 1.92; acc: 0.44
Batch: 540; loss: 2.0; acc: 0.36
Batch: 560; loss: 1.89; acc: 0.36
Batch: 580; loss: 1.78; acc: 0.48
Batch: 600; loss: 1.98; acc: 0.36
Batch: 620; loss: 1.93; acc: 0.41
Batch: 640; loss: 1.77; acc: 0.5
Batch: 660; loss: 1.97; acc: 0.34
Batch: 680; loss: 1.93; acc: 0.44
Batch: 700; loss: 1.84; acc: 0.48
Batch: 720; loss: 1.93; acc: 0.33
Batch: 740; loss: 1.83; acc: 0.47
Batch: 760; loss: 1.88; acc: 0.44
Batch: 780; loss: 1.79; acc: 0.53
Train Epoch over. train_loss: 2.02; train_accuracy: 0.34 

2.418762778688688e-05
4.593002358888043e-06
Batch: 0; loss: 2.0; acc: 0.33
Batch: 20; loss: 2.0; acc: 0.3
Batch: 40; loss: 1.71; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.52
Batch: 80; loss: 1.82; acc: 0.47
Batch: 100; loss: 1.91; acc: 0.42
Batch: 120; loss: 1.9; acc: 0.31
Batch: 140; loss: 1.8; acc: 0.45
Val Epoch over. val_loss: 1.8643980170511136; val_accuracy: 0.4263535031847134 

The current subspace-distance is: 4.593002358888043e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.88; acc: 0.41
Batch: 20; loss: 1.8; acc: 0.45
Batch: 40; loss: 1.9; acc: 0.39
Batch: 60; loss: 1.96; acc: 0.33
Batch: 80; loss: 1.84; acc: 0.36
Batch: 100; loss: 1.79; acc: 0.44
Batch: 120; loss: 1.8; acc: 0.47
Batch: 140; loss: 1.85; acc: 0.45
Batch: 160; loss: 1.87; acc: 0.42
Batch: 180; loss: 1.88; acc: 0.39
Batch: 200; loss: 1.8; acc: 0.53
Batch: 220; loss: 1.84; acc: 0.39
Batch: 240; loss: 1.86; acc: 0.5
Batch: 260; loss: 1.82; acc: 0.44
Batch: 280; loss: 1.89; acc: 0.38
Batch: 300; loss: 1.8; acc: 0.53
Batch: 320; loss: 1.84; acc: 0.42
Batch: 340; loss: 1.89; acc: 0.5
Batch: 360; loss: 1.86; acc: 0.42
Batch: 380; loss: 1.79; acc: 0.47
Batch: 400; loss: 1.88; acc: 0.42
Batch: 420; loss: 1.84; acc: 0.47
Batch: 440; loss: 1.84; acc: 0.41
Batch: 460; loss: 1.73; acc: 0.47
Batch: 480; loss: 1.96; acc: 0.33
Batch: 500; loss: 1.8; acc: 0.52
Batch: 520; loss: 1.83; acc: 0.48
Batch: 540; loss: 1.79; acc: 0.45
Batch: 560; loss: 1.71; acc: 0.55
Batch: 580; loss: 1.81; acc: 0.45
Batch: 600; loss: 1.74; acc: 0.45
Batch: 620; loss: 1.77; acc: 0.47
Batch: 640; loss: 1.75; acc: 0.45
Batch: 660; loss: 1.74; acc: 0.55
Batch: 680; loss: 1.86; acc: 0.38
Batch: 700; loss: 1.85; acc: 0.44
Batch: 720; loss: 1.83; acc: 0.41
Batch: 740; loss: 1.92; acc: 0.36
Batch: 760; loss: 1.75; acc: 0.5
Batch: 780; loss: 1.92; acc: 0.41
Train Epoch over. train_loss: 1.84; train_accuracy: 0.43 

2.675158430065494e-05
5.670582140737679e-06
Batch: 0; loss: 1.93; acc: 0.36
Batch: 20; loss: 1.95; acc: 0.34
Batch: 40; loss: 1.62; acc: 0.59
Batch: 60; loss: 1.75; acc: 0.53
Batch: 80; loss: 1.76; acc: 0.41
Batch: 100; loss: 1.84; acc: 0.42
Batch: 120; loss: 1.83; acc: 0.45
Batch: 140; loss: 1.76; acc: 0.5
Val Epoch over. val_loss: 1.8034770329287098; val_accuracy: 0.4537221337579618 

The current subspace-distance is: 5.670582140737679e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.47
Batch: 40; loss: 1.75; acc: 0.45
Batch: 60; loss: 1.87; acc: 0.39
Batch: 80; loss: 1.77; acc: 0.41
Batch: 100; loss: 1.87; acc: 0.39
Batch: 120; loss: 1.82; acc: 0.42
Batch: 140; loss: 1.76; acc: 0.45
Batch: 160; loss: 1.78; acc: 0.42
Batch: 180; loss: 1.78; acc: 0.41
Batch: 200; loss: 1.77; acc: 0.47
Batch: 220; loss: 1.85; acc: 0.38
Batch: 240; loss: 1.84; acc: 0.47
Batch: 260; loss: 1.86; acc: 0.47
Batch: 280; loss: 1.73; acc: 0.44
Batch: 300; loss: 1.83; acc: 0.39
Batch: 320; loss: 1.83; acc: 0.47
Batch: 340; loss: 1.84; acc: 0.39
Batch: 360; loss: 1.86; acc: 0.41
Batch: 380; loss: 1.94; acc: 0.33
Batch: 400; loss: 1.85; acc: 0.39
Batch: 420; loss: 1.63; acc: 0.55
Batch: 440; loss: 2.0; acc: 0.33
Batch: 460; loss: 1.88; acc: 0.42
Batch: 480; loss: 1.79; acc: 0.44
Batch: 500; loss: 1.82; acc: 0.36
Batch: 520; loss: 1.85; acc: 0.5
Batch: 540; loss: 1.82; acc: 0.44
Batch: 560; loss: 1.93; acc: 0.33
Batch: 580; loss: 1.86; acc: 0.41
Batch: 600; loss: 1.72; acc: 0.5
Batch: 620; loss: 1.69; acc: 0.53
Batch: 640; loss: 1.75; acc: 0.47
Batch: 660; loss: 1.74; acc: 0.52
Batch: 680; loss: 1.66; acc: 0.52
Batch: 700; loss: 1.91; acc: 0.39
Batch: 720; loss: 1.84; acc: 0.47
Batch: 740; loss: 1.79; acc: 0.41
Batch: 760; loss: 1.82; acc: 0.39
Batch: 780; loss: 1.74; acc: 0.53
Train Epoch over. train_loss: 1.79; train_accuracy: 0.45 

2.862717337848153e-05
8.107189387374092e-06
Batch: 0; loss: 1.87; acc: 0.38
Batch: 20; loss: 1.92; acc: 0.39
Batch: 40; loss: 1.53; acc: 0.62
Batch: 60; loss: 1.72; acc: 0.53
Batch: 80; loss: 1.69; acc: 0.56
Batch: 100; loss: 1.81; acc: 0.42
Batch: 120; loss: 1.79; acc: 0.41
Batch: 140; loss: 1.7; acc: 0.53
Val Epoch over. val_loss: 1.7484302999107701; val_accuracy: 0.48128980891719747 

The current subspace-distance is: 8.107189387374092e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.76; acc: 0.45
Batch: 20; loss: 1.9; acc: 0.34
Batch: 40; loss: 1.81; acc: 0.39
Batch: 60; loss: 1.78; acc: 0.45
Batch: 80; loss: 1.74; acc: 0.47
Batch: 100; loss: 1.72; acc: 0.52
Batch: 120; loss: 1.94; acc: 0.33
Batch: 140; loss: 1.73; acc: 0.44
Batch: 160; loss: 1.72; acc: 0.48
Batch: 180; loss: 1.71; acc: 0.5
Batch: 200; loss: 1.81; acc: 0.47
Batch: 220; loss: 1.76; acc: 0.42
Batch: 240; loss: 1.78; acc: 0.48
Batch: 260; loss: 1.78; acc: 0.53
Batch: 280; loss: 1.77; acc: 0.5
Batch: 300; loss: 1.67; acc: 0.53
Batch: 320; loss: 1.83; acc: 0.42
Batch: 340; loss: 1.72; acc: 0.53
Batch: 360; loss: 1.73; acc: 0.44
Batch: 380; loss: 1.78; acc: 0.47
Batch: 400; loss: 1.69; acc: 0.5
Batch: 420; loss: 1.77; acc: 0.45
Batch: 440; loss: 1.77; acc: 0.48
Batch: 460; loss: 1.81; acc: 0.39
Batch: 480; loss: 1.85; acc: 0.38
Batch: 500; loss: 1.79; acc: 0.47
Batch: 520; loss: 1.8; acc: 0.42
Batch: 540; loss: 1.76; acc: 0.44
Batch: 560; loss: 1.74; acc: 0.48
Batch: 580; loss: 1.78; acc: 0.47
Batch: 600; loss: 1.81; acc: 0.48
Batch: 620; loss: 1.84; acc: 0.45
Batch: 640; loss: 1.75; acc: 0.45
Batch: 660; loss: 1.63; acc: 0.58
Batch: 680; loss: 1.76; acc: 0.44
Batch: 700; loss: 1.71; acc: 0.48
Batch: 720; loss: 1.72; acc: 0.56
Batch: 740; loss: 1.76; acc: 0.5
Batch: 760; loss: 1.78; acc: 0.52
Batch: 780; loss: 1.7; acc: 0.53
Train Epoch over. train_loss: 1.76; train_accuracy: 0.47 

3.029363688256126e-05
8.06024945632089e-06
Batch: 0; loss: 1.82; acc: 0.42
Batch: 20; loss: 1.91; acc: 0.39
Batch: 40; loss: 1.52; acc: 0.67
Batch: 60; loss: 1.7; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.52
Batch: 100; loss: 1.79; acc: 0.5
Batch: 120; loss: 1.8; acc: 0.42
Batch: 140; loss: 1.7; acc: 0.55
Val Epoch over. val_loss: 1.7345081218488656; val_accuracy: 0.4964171974522293 

The current subspace-distance is: 8.06024945632089e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.47
Batch: 20; loss: 1.83; acc: 0.45
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.71; acc: 0.52
Batch: 80; loss: 1.81; acc: 0.48
Batch: 100; loss: 1.78; acc: 0.47
Batch: 120; loss: 1.98; acc: 0.3
Batch: 140; loss: 1.78; acc: 0.44
Batch: 160; loss: 1.83; acc: 0.45
Batch: 180; loss: 1.79; acc: 0.41
Batch: 200; loss: 1.78; acc: 0.47
Batch: 220; loss: 1.72; acc: 0.45
Batch: 240; loss: 1.8; acc: 0.39
Batch: 260; loss: 1.77; acc: 0.48
Batch: 280; loss: 1.7; acc: 0.47
Batch: 300; loss: 1.7; acc: 0.5
Batch: 320; loss: 1.84; acc: 0.45
Batch: 340; loss: 1.79; acc: 0.47
Batch: 360; loss: 1.86; acc: 0.42
Batch: 380; loss: 1.86; acc: 0.33
Batch: 400; loss: 1.83; acc: 0.38
Batch: 420; loss: 1.75; acc: 0.47
Batch: 440; loss: 1.79; acc: 0.45
Batch: 460; loss: 1.74; acc: 0.5
Batch: 480; loss: 1.68; acc: 0.53
Batch: 500; loss: 1.68; acc: 0.48
Batch: 520; loss: 1.73; acc: 0.48
Batch: 540; loss: 1.7; acc: 0.52
Batch: 560; loss: 1.81; acc: 0.47
Batch: 580; loss: 1.73; acc: 0.48
Batch: 600; loss: 1.77; acc: 0.39
Batch: 620; loss: 1.78; acc: 0.41
Batch: 640; loss: 1.74; acc: 0.47
Batch: 660; loss: 1.61; acc: 0.55
Batch: 680; loss: 1.85; acc: 0.38
Batch: 700; loss: 1.72; acc: 0.5
Batch: 720; loss: 1.84; acc: 0.41
Batch: 740; loss: 1.8; acc: 0.45
Batch: 760; loss: 1.65; acc: 0.58
Batch: 780; loss: 1.84; acc: 0.53
Train Epoch over. train_loss: 1.74; train_accuracy: 0.48 

3.102765549556352e-05
7.83502673584735e-06
Batch: 0; loss: 1.79; acc: 0.42
Batch: 20; loss: 1.91; acc: 0.41
Batch: 40; loss: 1.5; acc: 0.69
Batch: 60; loss: 1.68; acc: 0.56
Batch: 80; loss: 1.64; acc: 0.61
Batch: 100; loss: 1.75; acc: 0.47
Batch: 120; loss: 1.8; acc: 0.44
Batch: 140; loss: 1.69; acc: 0.52
Val Epoch over. val_loss: 1.7180529551900876; val_accuracy: 0.5027866242038217 

The current subspace-distance is: 7.83502673584735e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.44
Batch: 20; loss: 1.82; acc: 0.42
Batch: 40; loss: 1.73; acc: 0.53
Batch: 60; loss: 1.85; acc: 0.38
Batch: 80; loss: 1.79; acc: 0.42
Batch: 100; loss: 1.72; acc: 0.48
Batch: 120; loss: 1.84; acc: 0.41
Batch: 140; loss: 1.7; acc: 0.5
Batch: 160; loss: 1.95; acc: 0.36
Batch: 180; loss: 1.75; acc: 0.5
Batch: 200; loss: 1.71; acc: 0.53
Batch: 220; loss: 1.76; acc: 0.53
Batch: 240; loss: 1.69; acc: 0.52
Batch: 260; loss: 1.63; acc: 0.56
Batch: 280; loss: 1.63; acc: 0.56
Batch: 300; loss: 1.66; acc: 0.53
Batch: 320; loss: 1.82; acc: 0.39
Batch: 340; loss: 1.8; acc: 0.42
Batch: 360; loss: 1.69; acc: 0.44
Batch: 380; loss: 1.66; acc: 0.61
Batch: 400; loss: 1.71; acc: 0.5
Batch: 420; loss: 1.68; acc: 0.5
Batch: 440; loss: 1.74; acc: 0.53
Batch: 460; loss: 1.77; acc: 0.47
Batch: 480; loss: 1.79; acc: 0.47
Batch: 500; loss: 1.58; acc: 0.56
Batch: 520; loss: 1.68; acc: 0.52
Batch: 540; loss: 1.7; acc: 0.48
Batch: 560; loss: 1.75; acc: 0.45
Batch: 580; loss: 1.68; acc: 0.53
Batch: 600; loss: 1.76; acc: 0.48
Batch: 620; loss: 1.68; acc: 0.53
Batch: 640; loss: 1.74; acc: 0.45
Batch: 660; loss: 1.63; acc: 0.53
Batch: 680; loss: 1.7; acc: 0.48
Batch: 700; loss: 1.7; acc: 0.5
Batch: 720; loss: 1.66; acc: 0.52
Batch: 740; loss: 1.76; acc: 0.47
Batch: 760; loss: 1.79; acc: 0.52
Batch: 780; loss: 1.67; acc: 0.56
Train Epoch over. train_loss: 1.72; train_accuracy: 0.49 

3.2328087399946526e-05
8.81892628967762e-06
Batch: 0; loss: 1.76; acc: 0.45
Batch: 20; loss: 1.91; acc: 0.42
Batch: 40; loss: 1.45; acc: 0.69
Batch: 60; loss: 1.62; acc: 0.56
Batch: 80; loss: 1.59; acc: 0.61
Batch: 100; loss: 1.71; acc: 0.45
Batch: 120; loss: 1.79; acc: 0.44
Batch: 140; loss: 1.64; acc: 0.58
Val Epoch over. val_loss: 1.6905397366566264; val_accuracy: 0.5140326433121019 

The current subspace-distance is: 8.81892628967762e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.47
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 1.62; acc: 0.56
Batch: 60; loss: 1.55; acc: 0.58
Batch: 80; loss: 1.71; acc: 0.48
Batch: 100; loss: 1.71; acc: 0.53
Batch: 120; loss: 1.71; acc: 0.52
Batch: 140; loss: 1.67; acc: 0.53
Batch: 160; loss: 1.66; acc: 0.56
Batch: 180; loss: 1.75; acc: 0.5
Batch: 200; loss: 1.65; acc: 0.55
Batch: 220; loss: 1.72; acc: 0.48
Batch: 240; loss: 1.74; acc: 0.41
Batch: 260; loss: 1.6; acc: 0.56
Batch: 280; loss: 1.6; acc: 0.55
Batch: 300; loss: 1.66; acc: 0.5
Batch: 320; loss: 1.69; acc: 0.55
Batch: 340; loss: 1.66; acc: 0.58
Batch: 360; loss: 1.68; acc: 0.55
Batch: 380; loss: 1.67; acc: 0.52
Batch: 400; loss: 1.68; acc: 0.47
Batch: 420; loss: 1.67; acc: 0.53
Batch: 440; loss: 1.72; acc: 0.47
Batch: 460; loss: 1.65; acc: 0.55
Batch: 480; loss: 1.69; acc: 0.44
Batch: 500; loss: 1.68; acc: 0.45
Batch: 520; loss: 1.71; acc: 0.48
Batch: 540; loss: 1.63; acc: 0.58
Batch: 560; loss: 1.77; acc: 0.5
Batch: 580; loss: 1.73; acc: 0.5
Batch: 600; loss: 1.73; acc: 0.55
Batch: 620; loss: 1.78; acc: 0.44
Batch: 640; loss: 1.83; acc: 0.39
Batch: 660; loss: 1.78; acc: 0.47
Batch: 680; loss: 1.67; acc: 0.56
Batch: 700; loss: 1.72; acc: 0.48
Batch: 720; loss: 1.62; acc: 0.52
Batch: 740; loss: 1.78; acc: 0.45
Batch: 760; loss: 1.72; acc: 0.44
Batch: 780; loss: 1.83; acc: 0.41
Train Epoch over. train_loss: 1.7; train_accuracy: 0.5 

3.371815546415746e-05
9.501056410954334e-06
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.41; acc: 0.66
Batch: 60; loss: 1.58; acc: 0.58
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.66; acc: 0.48
Batch: 120; loss: 1.78; acc: 0.45
Batch: 140; loss: 1.57; acc: 0.66
Val Epoch over. val_loss: 1.6623273184345027; val_accuracy: 0.5157245222929936 

The current subspace-distance is: 9.501056410954334e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.59; acc: 0.58
Batch: 20; loss: 1.66; acc: 0.52
Batch: 40; loss: 1.56; acc: 0.62
Batch: 60; loss: 1.71; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.55
Batch: 100; loss: 1.78; acc: 0.45
Batch: 120; loss: 1.72; acc: 0.47
Batch: 140; loss: 1.66; acc: 0.53
Batch: 160; loss: 1.64; acc: 0.52
Batch: 180; loss: 1.68; acc: 0.53
Batch: 200; loss: 1.6; acc: 0.53
Batch: 220; loss: 1.68; acc: 0.59
Batch: 240; loss: 1.8; acc: 0.41
Batch: 260; loss: 1.77; acc: 0.47
Batch: 280; loss: 1.75; acc: 0.44
Batch: 300; loss: 1.71; acc: 0.5
Batch: 320; loss: 1.62; acc: 0.56
Batch: 340; loss: 1.72; acc: 0.45
Batch: 360; loss: 1.74; acc: 0.45
Batch: 380; loss: 1.56; acc: 0.66
Batch: 400; loss: 1.75; acc: 0.5
Batch: 420; loss: 1.69; acc: 0.55
Batch: 440; loss: 1.58; acc: 0.58
Batch: 460; loss: 1.72; acc: 0.45
Batch: 480; loss: 1.66; acc: 0.47
Batch: 500; loss: 1.68; acc: 0.5
Batch: 520; loss: 1.72; acc: 0.47
Batch: 540; loss: 1.65; acc: 0.44
Batch: 560; loss: 1.67; acc: 0.48
Batch: 580; loss: 1.77; acc: 0.36
Batch: 600; loss: 1.64; acc: 0.53
Batch: 620; loss: 1.61; acc: 0.59
Batch: 640; loss: 1.71; acc: 0.44
Batch: 660; loss: 1.74; acc: 0.47
Batch: 680; loss: 1.58; acc: 0.55
Batch: 700; loss: 1.72; acc: 0.47
Batch: 720; loss: 1.79; acc: 0.39
Batch: 740; loss: 1.62; acc: 0.5
Batch: 760; loss: 1.68; acc: 0.5
Batch: 780; loss: 1.68; acc: 0.48
Train Epoch over. train_loss: 1.67; train_accuracy: 0.51 

3.5218083212384954e-05
1.0169364031753503e-05
Batch: 0; loss: 1.68; acc: 0.48
Batch: 20; loss: 1.84; acc: 0.33
Batch: 40; loss: 1.38; acc: 0.64
Batch: 60; loss: 1.54; acc: 0.61
Batch: 80; loss: 1.54; acc: 0.52
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.78; acc: 0.47
Batch: 140; loss: 1.47; acc: 0.69
Val Epoch over. val_loss: 1.627877795772188; val_accuracy: 0.5288614649681529 

The current subspace-distance is: 1.0169364031753503e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.59
Batch: 20; loss: 1.61; acc: 0.56
Batch: 40; loss: 1.68; acc: 0.48
Batch: 60; loss: 1.68; acc: 0.42
Batch: 80; loss: 1.75; acc: 0.53
Batch: 100; loss: 1.74; acc: 0.44
Batch: 120; loss: 1.8; acc: 0.39
Batch: 140; loss: 1.74; acc: 0.47
Batch: 160; loss: 1.63; acc: 0.5
Batch: 180; loss: 1.77; acc: 0.45
Batch: 200; loss: 1.73; acc: 0.48
Batch: 220; loss: 1.75; acc: 0.45
Batch: 240; loss: 1.65; acc: 0.45
Batch: 260; loss: 1.66; acc: 0.5
Batch: 280; loss: 1.56; acc: 0.52
Batch: 300; loss: 1.91; acc: 0.33
Batch: 320; loss: 1.59; acc: 0.55
Batch: 340; loss: 1.52; acc: 0.56
Batch: 360; loss: 1.61; acc: 0.55
Batch: 380; loss: 1.65; acc: 0.39
Batch: 400; loss: 1.75; acc: 0.42
Batch: 420; loss: 1.63; acc: 0.53
Batch: 440; loss: 1.7; acc: 0.52
Batch: 460; loss: 1.59; acc: 0.55
Batch: 480; loss: 1.62; acc: 0.55
Batch: 500; loss: 1.71; acc: 0.5
Batch: 520; loss: 1.65; acc: 0.45
Batch: 540; loss: 1.62; acc: 0.52
Batch: 560; loss: 1.66; acc: 0.53
Batch: 580; loss: 1.62; acc: 0.48
Batch: 600; loss: 1.66; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.47
Batch: 640; loss: 1.6; acc: 0.48
Batch: 660; loss: 1.57; acc: 0.61
Batch: 680; loss: 1.65; acc: 0.53
Batch: 700; loss: 1.7; acc: 0.48
Batch: 720; loss: 1.7; acc: 0.42
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.6; acc: 0.56
Batch: 780; loss: 1.64; acc: 0.53
Train Epoch over. train_loss: 1.65; train_accuracy: 0.51 

3.615062450990081e-05
1.1197675121366046e-05
Batch: 0; loss: 1.64; acc: 0.5
Batch: 20; loss: 1.81; acc: 0.34
Batch: 40; loss: 1.33; acc: 0.72
Batch: 60; loss: 1.52; acc: 0.64
Batch: 80; loss: 1.54; acc: 0.53
Batch: 100; loss: 1.6; acc: 0.53
Batch: 120; loss: 1.74; acc: 0.52
Batch: 140; loss: 1.4; acc: 0.77
Val Epoch over. val_loss: 1.5994645448247338; val_accuracy: 0.5319466560509554 

The current subspace-distance is: 1.1197675121366046e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.5
Batch: 20; loss: 1.56; acc: 0.56
Batch: 40; loss: 1.55; acc: 0.58
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.61; acc: 0.53
Batch: 100; loss: 1.64; acc: 0.52
Batch: 120; loss: 1.59; acc: 0.48
Batch: 140; loss: 1.77; acc: 0.44
Batch: 160; loss: 1.58; acc: 0.48
Batch: 180; loss: 1.73; acc: 0.44
Batch: 200; loss: 1.66; acc: 0.55
Batch: 220; loss: 1.61; acc: 0.48
Batch: 240; loss: 1.66; acc: 0.44
Batch: 260; loss: 1.64; acc: 0.39
Batch: 280; loss: 1.73; acc: 0.48
Batch: 300; loss: 1.72; acc: 0.48
Batch: 320; loss: 1.65; acc: 0.47
Batch: 340; loss: 1.75; acc: 0.48
Batch: 360; loss: 1.5; acc: 0.58
Batch: 380; loss: 1.62; acc: 0.56
Batch: 400; loss: 1.69; acc: 0.53
Batch: 420; loss: 1.59; acc: 0.5
Batch: 440; loss: 1.6; acc: 0.53
Batch: 460; loss: 1.59; acc: 0.48
Batch: 480; loss: 1.54; acc: 0.55
Batch: 500; loss: 1.51; acc: 0.66
Batch: 520; loss: 1.64; acc: 0.5
Batch: 540; loss: 1.59; acc: 0.48
Batch: 560; loss: 1.51; acc: 0.58
Batch: 580; loss: 1.49; acc: 0.55
Batch: 600; loss: 1.64; acc: 0.56
Batch: 620; loss: 1.71; acc: 0.52
Batch: 640; loss: 1.61; acc: 0.52
Batch: 660; loss: 1.53; acc: 0.66
Batch: 680; loss: 1.54; acc: 0.53
Batch: 700; loss: 1.73; acc: 0.45
Batch: 720; loss: 1.57; acc: 0.52
Batch: 740; loss: 1.74; acc: 0.45
Batch: 760; loss: 1.58; acc: 0.55
Batch: 780; loss: 1.66; acc: 0.45
Train Epoch over. train_loss: 1.63; train_accuracy: 0.51 

3.724632188095711e-05
1.284179506910732e-05
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.82; acc: 0.34
Batch: 40; loss: 1.33; acc: 0.67
Batch: 60; loss: 1.51; acc: 0.58
Batch: 80; loss: 1.53; acc: 0.52
Batch: 100; loss: 1.6; acc: 0.5
Batch: 120; loss: 1.72; acc: 0.53
Batch: 140; loss: 1.37; acc: 0.7
Val Epoch over. val_loss: 1.5879970113183284; val_accuracy: 0.5222929936305732 

The current subspace-distance is: 1.284179506910732e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.86; acc: 0.33
Batch: 20; loss: 1.51; acc: 0.62
Batch: 40; loss: 1.59; acc: 0.52
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.64; acc: 0.48
Batch: 100; loss: 1.83; acc: 0.41
Batch: 120; loss: 1.54; acc: 0.59
Batch: 140; loss: 1.58; acc: 0.55
Batch: 160; loss: 1.76; acc: 0.41
Batch: 180; loss: 1.56; acc: 0.56
Batch: 200; loss: 1.67; acc: 0.48
Batch: 220; loss: 1.57; acc: 0.5
Batch: 240; loss: 1.59; acc: 0.5
Batch: 260; loss: 1.56; acc: 0.56
Batch: 280; loss: 1.57; acc: 0.5
Batch: 300; loss: 1.6; acc: 0.52
Batch: 320; loss: 1.58; acc: 0.55
Batch: 340; loss: 1.64; acc: 0.47
Batch: 360; loss: 1.56; acc: 0.5
Batch: 380; loss: 1.64; acc: 0.53
Batch: 400; loss: 1.75; acc: 0.39
Batch: 420; loss: 1.7; acc: 0.5
Batch: 440; loss: 1.5; acc: 0.55
Batch: 460; loss: 1.68; acc: 0.45
Batch: 480; loss: 1.67; acc: 0.5
Batch: 500; loss: 1.57; acc: 0.55
Batch: 520; loss: 1.55; acc: 0.59
Batch: 540; loss: 1.63; acc: 0.48
Batch: 560; loss: 1.63; acc: 0.42
Batch: 580; loss: 1.58; acc: 0.44
Batch: 600; loss: 1.56; acc: 0.53
Batch: 620; loss: 1.49; acc: 0.56
Batch: 640; loss: 1.64; acc: 0.5
Batch: 660; loss: 1.58; acc: 0.53
Batch: 680; loss: 1.63; acc: 0.55
Batch: 700; loss: 1.61; acc: 0.55
Batch: 720; loss: 1.77; acc: 0.41
Batch: 740; loss: 1.68; acc: 0.38
Batch: 760; loss: 1.62; acc: 0.58
Batch: 780; loss: 1.53; acc: 0.5
Train Epoch over. train_loss: 1.63; train_accuracy: 0.5 

3.7608017009915784e-05
1.0969828508677892e-05
Batch: 0; loss: 1.62; acc: 0.5
Batch: 20; loss: 1.83; acc: 0.34
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.54; acc: 0.53
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.71; acc: 0.53
Batch: 140; loss: 1.39; acc: 0.72
Val Epoch over. val_loss: 1.5886849939443504; val_accuracy: 0.5249800955414012 

The current subspace-distance is: 1.0969828508677892e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.74; acc: 0.47
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.64; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.55; acc: 0.53
Batch: 100; loss: 1.69; acc: 0.42
Batch: 120; loss: 1.71; acc: 0.47
Batch: 140; loss: 1.48; acc: 0.61
Batch: 160; loss: 1.6; acc: 0.48
Batch: 180; loss: 1.63; acc: 0.48
Batch: 200; loss: 1.56; acc: 0.58
Batch: 220; loss: 1.68; acc: 0.44
Batch: 240; loss: 1.7; acc: 0.39
Batch: 260; loss: 1.63; acc: 0.48
Batch: 280; loss: 1.71; acc: 0.44
Batch: 300; loss: 1.54; acc: 0.47
Batch: 320; loss: 1.68; acc: 0.45
Batch: 340; loss: 1.45; acc: 0.59
Batch: 360; loss: 1.65; acc: 0.44
Batch: 380; loss: 1.67; acc: 0.42
Batch: 400; loss: 1.55; acc: 0.5
Batch: 420; loss: 1.69; acc: 0.48
Batch: 440; loss: 1.63; acc: 0.53
Batch: 460; loss: 1.72; acc: 0.48
Batch: 480; loss: 1.71; acc: 0.41
Batch: 500; loss: 1.75; acc: 0.41
Batch: 520; loss: 1.6; acc: 0.5
Batch: 540; loss: 1.69; acc: 0.53
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.68; acc: 0.42
Batch: 600; loss: 1.6; acc: 0.44
Batch: 620; loss: 1.45; acc: 0.64
Batch: 640; loss: 1.66; acc: 0.5
Batch: 660; loss: 1.72; acc: 0.44
Batch: 680; loss: 1.75; acc: 0.44
Batch: 700; loss: 1.49; acc: 0.64
Batch: 720; loss: 1.62; acc: 0.56
Batch: 740; loss: 1.49; acc: 0.56
Batch: 760; loss: 1.64; acc: 0.39
Batch: 780; loss: 1.63; acc: 0.47
Train Epoch over. train_loss: 1.62; train_accuracy: 0.5 

3.720591848832555e-05
1.0762344572867732e-05
Batch: 0; loss: 1.61; acc: 0.52
Batch: 20; loss: 1.83; acc: 0.34
Batch: 40; loss: 1.34; acc: 0.67
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.54; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.7; acc: 0.53
Batch: 140; loss: 1.38; acc: 0.72
Val Epoch over. val_loss: 1.588605892886022; val_accuracy: 0.5209992038216561 

The current subspace-distance is: 1.0762344572867732e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.61; acc: 0.42
Batch: 20; loss: 1.67; acc: 0.42
Batch: 40; loss: 1.63; acc: 0.53
Batch: 60; loss: 1.64; acc: 0.45
Batch: 80; loss: 1.75; acc: 0.41
Batch: 100; loss: 1.67; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.55
Batch: 140; loss: 1.79; acc: 0.41
Batch: 160; loss: 1.64; acc: 0.45
Batch: 180; loss: 1.79; acc: 0.44
Batch: 200; loss: 1.63; acc: 0.48
Batch: 220; loss: 1.51; acc: 0.5
Batch: 240; loss: 1.64; acc: 0.5
Batch: 260; loss: 1.64; acc: 0.48
Batch: 280; loss: 1.68; acc: 0.47
Batch: 300; loss: 1.45; acc: 0.58
Batch: 320; loss: 1.62; acc: 0.53
Batch: 340; loss: 1.45; acc: 0.64
Batch: 360; loss: 1.73; acc: 0.45
Batch: 380; loss: 1.61; acc: 0.5
Batch: 400; loss: 1.5; acc: 0.61
Batch: 420; loss: 1.5; acc: 0.61
Batch: 440; loss: 1.62; acc: 0.52
Batch: 460; loss: 1.66; acc: 0.47
Batch: 480; loss: 1.73; acc: 0.45
Batch: 500; loss: 1.66; acc: 0.52
Batch: 520; loss: 1.53; acc: 0.55
Batch: 540; loss: 1.57; acc: 0.5
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.56; acc: 0.61
Batch: 600; loss: 1.48; acc: 0.55
Batch: 620; loss: 1.64; acc: 0.47
Batch: 640; loss: 1.6; acc: 0.48
Batch: 660; loss: 1.62; acc: 0.44
Batch: 680; loss: 1.79; acc: 0.36
Batch: 700; loss: 1.49; acc: 0.59
Batch: 720; loss: 1.44; acc: 0.62
Batch: 740; loss: 1.56; acc: 0.53
Batch: 760; loss: 1.69; acc: 0.41
Batch: 780; loss: 1.66; acc: 0.45
Train Epoch over. train_loss: 1.62; train_accuracy: 0.5 

3.7899011658737436e-05
1.0953776836686302e-05
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.83; acc: 0.39
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.52; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.48
Batch: 120; loss: 1.71; acc: 0.53
Batch: 140; loss: 1.38; acc: 0.72
Val Epoch over. val_loss: 1.586828922769826; val_accuracy: 0.5238853503184714 

The current subspace-distance is: 1.0953776836686302e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.56
Batch: 20; loss: 1.44; acc: 0.62
Batch: 40; loss: 1.7; acc: 0.53
Batch: 60; loss: 1.57; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.52
Batch: 100; loss: 1.62; acc: 0.44
Batch: 120; loss: 1.6; acc: 0.47
Batch: 140; loss: 1.54; acc: 0.56
Batch: 160; loss: 1.44; acc: 0.67
Batch: 180; loss: 1.68; acc: 0.45
Batch: 200; loss: 1.54; acc: 0.56
Batch: 220; loss: 1.69; acc: 0.47
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.61; acc: 0.48
Batch: 280; loss: 1.74; acc: 0.44
Batch: 300; loss: 1.61; acc: 0.53
Batch: 320; loss: 1.52; acc: 0.61
Batch: 340; loss: 1.62; acc: 0.41
Batch: 360; loss: 1.54; acc: 0.53
Batch: 380; loss: 1.47; acc: 0.58
Batch: 400; loss: 1.61; acc: 0.56
Batch: 420; loss: 1.69; acc: 0.52
Batch: 440; loss: 1.68; acc: 0.45
Batch: 460; loss: 1.5; acc: 0.5
Batch: 480; loss: 1.74; acc: 0.5
Batch: 500; loss: 1.51; acc: 0.58
Batch: 520; loss: 1.69; acc: 0.48
Batch: 540; loss: 1.5; acc: 0.53
Batch: 560; loss: 1.54; acc: 0.48
Batch: 580; loss: 1.5; acc: 0.64
Batch: 600; loss: 1.72; acc: 0.47
Batch: 620; loss: 1.66; acc: 0.47
Batch: 640; loss: 1.65; acc: 0.47
Batch: 660; loss: 1.49; acc: 0.62
Batch: 680; loss: 1.61; acc: 0.52
Batch: 700; loss: 1.6; acc: 0.53
Batch: 720; loss: 1.51; acc: 0.66
Batch: 740; loss: 1.57; acc: 0.55
Batch: 760; loss: 1.65; acc: 0.48
Batch: 780; loss: 1.6; acc: 0.53
Train Epoch over. train_loss: 1.62; train_accuracy: 0.5 

3.8474401662824675e-05
1.2690297808148898e-05
Batch: 0; loss: 1.6; acc: 0.52
Batch: 20; loss: 1.83; acc: 0.34
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.51; acc: 0.64
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.58; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.53
Batch: 140; loss: 1.38; acc: 0.7
Val Epoch over. val_loss: 1.584423772089041; val_accuracy: 0.5162221337579618 

The current subspace-distance is: 1.2690297808148898e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.56
Batch: 40; loss: 1.63; acc: 0.44
Batch: 60; loss: 1.56; acc: 0.52
Batch: 80; loss: 1.56; acc: 0.53
Batch: 100; loss: 1.49; acc: 0.56
Batch: 120; loss: 1.74; acc: 0.41
Batch: 140; loss: 1.58; acc: 0.48
Batch: 160; loss: 1.82; acc: 0.39
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.55; acc: 0.55
Batch: 220; loss: 1.59; acc: 0.52
Batch: 240; loss: 1.56; acc: 0.52
Batch: 260; loss: 1.48; acc: 0.52
Batch: 280; loss: 1.61; acc: 0.5
Batch: 300; loss: 1.53; acc: 0.58
Batch: 320; loss: 1.63; acc: 0.47
Batch: 340; loss: 1.57; acc: 0.53
Batch: 360; loss: 1.93; acc: 0.34
Batch: 380; loss: 1.6; acc: 0.48
Batch: 400; loss: 1.73; acc: 0.39
Batch: 420; loss: 1.62; acc: 0.55
Batch: 440; loss: 1.54; acc: 0.58
Batch: 460; loss: 1.67; acc: 0.48
Batch: 480; loss: 1.47; acc: 0.58
Batch: 500; loss: 1.66; acc: 0.45
Batch: 520; loss: 1.67; acc: 0.53
Batch: 540; loss: 1.54; acc: 0.5
Batch: 560; loss: 1.64; acc: 0.5
Batch: 580; loss: 1.59; acc: 0.53
Batch: 600; loss: 1.77; acc: 0.39
Batch: 620; loss: 1.61; acc: 0.52
Batch: 640; loss: 1.59; acc: 0.53
Batch: 660; loss: 1.67; acc: 0.48
Batch: 680; loss: 1.66; acc: 0.52
Batch: 700; loss: 1.79; acc: 0.41
Batch: 720; loss: 1.6; acc: 0.53
Batch: 740; loss: 1.62; acc: 0.42
Batch: 760; loss: 1.72; acc: 0.45
Batch: 780; loss: 1.52; acc: 0.59
Train Epoch over. train_loss: 1.62; train_accuracy: 0.5 

3.815762102021836e-05
1.1335915587551426e-05
Batch: 0; loss: 1.59; acc: 0.53
Batch: 20; loss: 1.82; acc: 0.38
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.5; acc: 0.59
Batch: 80; loss: 1.52; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.48
Batch: 120; loss: 1.7; acc: 0.53
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.5752164512682871; val_accuracy: 0.521297770700637 

The current subspace-distance is: 1.1335915587551426e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.44
Batch: 20; loss: 1.52; acc: 0.52
Batch: 40; loss: 1.75; acc: 0.42
Batch: 60; loss: 1.76; acc: 0.45
Batch: 80; loss: 1.64; acc: 0.45
Batch: 100; loss: 1.72; acc: 0.48
Batch: 120; loss: 1.6; acc: 0.56
Batch: 140; loss: 1.56; acc: 0.53
Batch: 160; loss: 1.55; acc: 0.58
Batch: 180; loss: 1.58; acc: 0.56
Batch: 200; loss: 1.55; acc: 0.52
Batch: 220; loss: 1.6; acc: 0.48
Batch: 240; loss: 1.63; acc: 0.55
Batch: 260; loss: 1.67; acc: 0.53
Batch: 280; loss: 1.43; acc: 0.69
Batch: 300; loss: 1.69; acc: 0.47
Batch: 320; loss: 1.64; acc: 0.48
Batch: 340; loss: 1.66; acc: 0.48
Batch: 360; loss: 1.61; acc: 0.53
Batch: 380; loss: 1.58; acc: 0.52
Batch: 400; loss: 1.65; acc: 0.48
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.71; acc: 0.45
Batch: 460; loss: 1.64; acc: 0.48
Batch: 480; loss: 1.75; acc: 0.39
Batch: 500; loss: 1.59; acc: 0.47
Batch: 520; loss: 1.81; acc: 0.39
Batch: 540; loss: 1.54; acc: 0.53
Batch: 560; loss: 1.53; acc: 0.58
Batch: 580; loss: 1.67; acc: 0.53
Batch: 600; loss: 1.59; acc: 0.53
Batch: 620; loss: 1.65; acc: 0.47
Batch: 640; loss: 1.68; acc: 0.45
Batch: 660; loss: 1.6; acc: 0.48
Batch: 680; loss: 1.62; acc: 0.5
Batch: 700; loss: 1.57; acc: 0.5
Batch: 720; loss: 1.71; acc: 0.44
Batch: 740; loss: 1.5; acc: 0.62
Batch: 760; loss: 1.7; acc: 0.44
Batch: 780; loss: 1.67; acc: 0.5
Train Epoch over. train_loss: 1.62; train_accuracy: 0.5 

3.9224843931151554e-05
1.369705751130823e-05
Batch: 0; loss: 1.59; acc: 0.53
Batch: 20; loss: 1.82; acc: 0.34
Batch: 40; loss: 1.33; acc: 0.67
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.53; acc: 0.56
Batch: 100; loss: 1.56; acc: 0.48
Batch: 120; loss: 1.69; acc: 0.53
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.5742544459689194; val_accuracy: 0.5231886942675159 

The current subspace-distance is: 1.369705751130823e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.76; acc: 0.48
Batch: 20; loss: 1.63; acc: 0.45
Batch: 40; loss: 1.74; acc: 0.45
Batch: 60; loss: 1.48; acc: 0.59
Batch: 80; loss: 1.71; acc: 0.48
Batch: 100; loss: 1.58; acc: 0.48
Batch: 120; loss: 1.67; acc: 0.45
Batch: 140; loss: 1.52; acc: 0.5
Batch: 160; loss: 1.69; acc: 0.45
Batch: 180; loss: 1.61; acc: 0.44
Batch: 200; loss: 1.69; acc: 0.47
Batch: 220; loss: 1.63; acc: 0.52
Batch: 240; loss: 1.69; acc: 0.41
Batch: 260; loss: 1.62; acc: 0.5
Batch: 280; loss: 1.66; acc: 0.47
Batch: 300; loss: 1.59; acc: 0.53
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.6; acc: 0.58
Batch: 360; loss: 1.62; acc: 0.58
Batch: 380; loss: 1.59; acc: 0.48
Batch: 400; loss: 1.78; acc: 0.5
Batch: 420; loss: 1.53; acc: 0.45
Batch: 440; loss: 1.58; acc: 0.53
Batch: 460; loss: 1.6; acc: 0.56
Batch: 480; loss: 1.66; acc: 0.44
Batch: 500; loss: 1.65; acc: 0.48
Batch: 520; loss: 1.61; acc: 0.48
Batch: 540; loss: 1.62; acc: 0.5
Batch: 560; loss: 1.67; acc: 0.5
Batch: 580; loss: 1.47; acc: 0.58
Batch: 600; loss: 1.5; acc: 0.56
Batch: 620; loss: 1.65; acc: 0.53
Batch: 640; loss: 1.65; acc: 0.48
Batch: 660; loss: 1.47; acc: 0.55
Batch: 680; loss: 1.71; acc: 0.48
Batch: 700; loss: 1.49; acc: 0.52
Batch: 720; loss: 1.67; acc: 0.5
Batch: 740; loss: 1.58; acc: 0.55
Batch: 760; loss: 1.69; acc: 0.5
Batch: 780; loss: 1.57; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

3.8847607356728986e-05
1.1742155948013533e-05
Batch: 0; loss: 1.59; acc: 0.55
Batch: 20; loss: 1.82; acc: 0.34
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.5; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.55; acc: 0.5
Batch: 120; loss: 1.68; acc: 0.53
Batch: 140; loss: 1.37; acc: 0.67
Val Epoch over. val_loss: 1.570905281479951; val_accuracy: 0.5251791401273885 

The current subspace-distance is: 1.1742155948013533e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.58
Batch: 20; loss: 1.67; acc: 0.44
Batch: 40; loss: 1.64; acc: 0.47
Batch: 60; loss: 1.55; acc: 0.62
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.65; acc: 0.45
Batch: 140; loss: 1.53; acc: 0.64
Batch: 160; loss: 1.65; acc: 0.47
Batch: 180; loss: 1.6; acc: 0.52
Batch: 200; loss: 1.56; acc: 0.56
Batch: 220; loss: 1.7; acc: 0.44
Batch: 240; loss: 1.68; acc: 0.47
Batch: 260; loss: 1.62; acc: 0.47
Batch: 280; loss: 1.56; acc: 0.55
Batch: 300; loss: 1.61; acc: 0.5
Batch: 320; loss: 1.53; acc: 0.5
Batch: 340; loss: 1.7; acc: 0.45
Batch: 360; loss: 1.47; acc: 0.61
Batch: 380; loss: 1.51; acc: 0.58
Batch: 400; loss: 1.55; acc: 0.56
Batch: 420; loss: 1.69; acc: 0.39
Batch: 440; loss: 1.66; acc: 0.48
Batch: 460; loss: 1.75; acc: 0.42
Batch: 480; loss: 1.56; acc: 0.52
Batch: 500; loss: 1.7; acc: 0.45
Batch: 520; loss: 1.78; acc: 0.38
Batch: 540; loss: 1.58; acc: 0.53
Batch: 560; loss: 1.63; acc: 0.53
Batch: 580; loss: 1.67; acc: 0.5
Batch: 600; loss: 1.66; acc: 0.5
Batch: 620; loss: 1.71; acc: 0.47
Batch: 640; loss: 1.56; acc: 0.53
Batch: 660; loss: 1.53; acc: 0.58
Batch: 680; loss: 1.68; acc: 0.48
Batch: 700; loss: 1.56; acc: 0.56
Batch: 720; loss: 1.47; acc: 0.59
Batch: 740; loss: 1.78; acc: 0.47
Batch: 760; loss: 1.5; acc: 0.58
Batch: 780; loss: 1.45; acc: 0.56
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

3.8919333746889606e-05
1.2995849829167128e-05
Batch: 0; loss: 1.58; acc: 0.56
Batch: 20; loss: 1.82; acc: 0.36
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.5; acc: 0.59
Batch: 80; loss: 1.53; acc: 0.53
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.68; acc: 0.53
Batch: 140; loss: 1.37; acc: 0.67
Val Epoch over. val_loss: 1.5697992880632923; val_accuracy: 0.5207006369426752 

The current subspace-distance is: 1.2995849829167128e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.45
Batch: 20; loss: 1.72; acc: 0.58
Batch: 40; loss: 1.65; acc: 0.47
Batch: 60; loss: 1.61; acc: 0.45
Batch: 80; loss: 1.63; acc: 0.45
Batch: 100; loss: 1.63; acc: 0.5
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.55; acc: 0.58
Batch: 160; loss: 1.58; acc: 0.52
Batch: 180; loss: 1.59; acc: 0.56
Batch: 200; loss: 1.76; acc: 0.36
Batch: 220; loss: 1.58; acc: 0.45
Batch: 240; loss: 1.7; acc: 0.44
Batch: 260; loss: 1.51; acc: 0.62
Batch: 280; loss: 1.6; acc: 0.44
Batch: 300; loss: 1.63; acc: 0.47
Batch: 320; loss: 1.68; acc: 0.59
Batch: 340; loss: 1.69; acc: 0.47
Batch: 360; loss: 1.66; acc: 0.47
Batch: 380; loss: 1.7; acc: 0.42
Batch: 400; loss: 1.56; acc: 0.55
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.54; acc: 0.52
Batch: 460; loss: 1.6; acc: 0.45
Batch: 480; loss: 1.77; acc: 0.39
Batch: 500; loss: 1.56; acc: 0.53
Batch: 520; loss: 1.59; acc: 0.59
Batch: 540; loss: 1.66; acc: 0.5
Batch: 560; loss: 1.54; acc: 0.52
Batch: 580; loss: 1.61; acc: 0.5
Batch: 600; loss: 1.62; acc: 0.55
Batch: 620; loss: 1.58; acc: 0.5
Batch: 640; loss: 1.57; acc: 0.53
Batch: 660; loss: 1.72; acc: 0.48
Batch: 680; loss: 1.61; acc: 0.41
Batch: 700; loss: 1.45; acc: 0.56
Batch: 720; loss: 1.63; acc: 0.52
Batch: 740; loss: 1.52; acc: 0.55
Batch: 760; loss: 1.65; acc: 0.45
Batch: 780; loss: 1.75; acc: 0.42
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

3.8785237848060206e-05
1.1650921805994585e-05
Batch: 0; loss: 1.56; acc: 0.58
Batch: 20; loss: 1.81; acc: 0.34
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.5; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.53; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.53
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.5633310960356597; val_accuracy: 0.5250796178343949 

The current subspace-distance is: 1.1650921805994585e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.62; acc: 0.44
Batch: 40; loss: 1.76; acc: 0.47
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.55; acc: 0.53
Batch: 100; loss: 1.64; acc: 0.48
Batch: 120; loss: 1.63; acc: 0.5
Batch: 140; loss: 1.65; acc: 0.47
Batch: 160; loss: 1.52; acc: 0.62
Batch: 180; loss: 1.68; acc: 0.36
Batch: 200; loss: 1.7; acc: 0.5
Batch: 220; loss: 1.72; acc: 0.41
Batch: 240; loss: 1.58; acc: 0.53
Batch: 260; loss: 1.63; acc: 0.5
Batch: 280; loss: 1.64; acc: 0.44
Batch: 300; loss: 1.62; acc: 0.52
Batch: 320; loss: 1.48; acc: 0.59
Batch: 340; loss: 1.7; acc: 0.41
Batch: 360; loss: 1.59; acc: 0.48
Batch: 380; loss: 1.58; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.42
Batch: 420; loss: 1.67; acc: 0.48
Batch: 440; loss: 1.66; acc: 0.47
Batch: 460; loss: 1.61; acc: 0.47
Batch: 480; loss: 1.6; acc: 0.53
Batch: 500; loss: 1.52; acc: 0.53
Batch: 520; loss: 1.55; acc: 0.47
Batch: 540; loss: 1.61; acc: 0.55
Batch: 560; loss: 1.7; acc: 0.45
Batch: 580; loss: 1.65; acc: 0.52
Batch: 600; loss: 1.7; acc: 0.48
Batch: 620; loss: 1.56; acc: 0.53
Batch: 640; loss: 1.57; acc: 0.58
Batch: 660; loss: 1.56; acc: 0.53
Batch: 680; loss: 1.67; acc: 0.47
Batch: 700; loss: 1.66; acc: 0.5
Batch: 720; loss: 1.66; acc: 0.44
Batch: 740; loss: 1.79; acc: 0.39
Batch: 760; loss: 1.67; acc: 0.45
Batch: 780; loss: 1.64; acc: 0.48
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

4.010746124549769e-05
1.2749843335768674e-05
Batch: 0; loss: 1.57; acc: 0.56
Batch: 20; loss: 1.82; acc: 0.36
Batch: 40; loss: 1.35; acc: 0.64
Batch: 60; loss: 1.5; acc: 0.59
Batch: 80; loss: 1.54; acc: 0.55
Batch: 100; loss: 1.54; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.38; acc: 0.67
Val Epoch over. val_loss: 1.571520701335494; val_accuracy: 0.5190087579617835 

The current subspace-distance is: 1.2749843335768674e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.47
Batch: 20; loss: 1.6; acc: 0.58
Batch: 40; loss: 1.49; acc: 0.56
Batch: 60; loss: 1.58; acc: 0.53
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.56
Batch: 120; loss: 1.88; acc: 0.38
Batch: 140; loss: 1.61; acc: 0.52
Batch: 160; loss: 1.67; acc: 0.47
Batch: 180; loss: 1.79; acc: 0.41
Batch: 200; loss: 1.6; acc: 0.53
Batch: 220; loss: 1.6; acc: 0.53
Batch: 240; loss: 1.53; acc: 0.48
Batch: 260; loss: 1.64; acc: 0.52
Batch: 280; loss: 1.45; acc: 0.62
Batch: 300; loss: 1.51; acc: 0.61
Batch: 320; loss: 1.56; acc: 0.55
Batch: 340; loss: 1.79; acc: 0.31
Batch: 360; loss: 1.45; acc: 0.58
Batch: 380; loss: 1.56; acc: 0.56
Batch: 400; loss: 1.56; acc: 0.48
Batch: 420; loss: 1.68; acc: 0.5
Batch: 440; loss: 1.73; acc: 0.41
Batch: 460; loss: 1.69; acc: 0.42
Batch: 480; loss: 1.73; acc: 0.44
Batch: 500; loss: 1.58; acc: 0.53
Batch: 520; loss: 1.63; acc: 0.44
Batch: 540; loss: 1.61; acc: 0.5
Batch: 560; loss: 1.68; acc: 0.5
Batch: 580; loss: 1.57; acc: 0.5
Batch: 600; loss: 1.62; acc: 0.58
Batch: 620; loss: 1.56; acc: 0.56
Batch: 640; loss: 1.7; acc: 0.45
Batch: 660; loss: 1.79; acc: 0.38
Batch: 680; loss: 1.49; acc: 0.56
Batch: 700; loss: 1.69; acc: 0.45
Batch: 720; loss: 1.61; acc: 0.52
Batch: 740; loss: 1.8; acc: 0.39
Batch: 760; loss: 1.62; acc: 0.45
Batch: 780; loss: 1.74; acc: 0.42
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

3.962402115575969e-05
1.1907307452929672e-05
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.82; acc: 0.33
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.52
Batch: 100; loss: 1.52; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.38; acc: 0.69
Val Epoch over. val_loss: 1.5628616787066125; val_accuracy: 0.5249800955414012 

The current subspace-distance is: 1.1907307452929672e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.68; acc: 0.42
Batch: 20; loss: 1.56; acc: 0.5
Batch: 40; loss: 1.45; acc: 0.56
Batch: 60; loss: 1.56; acc: 0.52
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.5
Batch: 120; loss: 1.72; acc: 0.39
Batch: 140; loss: 1.72; acc: 0.47
Batch: 160; loss: 1.59; acc: 0.44
Batch: 180; loss: 1.52; acc: 0.59
Batch: 200; loss: 1.83; acc: 0.39
Batch: 220; loss: 1.65; acc: 0.44
Batch: 240; loss: 1.47; acc: 0.61
Batch: 260; loss: 1.42; acc: 0.58
Batch: 280; loss: 1.68; acc: 0.47
Batch: 300; loss: 1.6; acc: 0.48
Batch: 320; loss: 1.73; acc: 0.44
Batch: 340; loss: 1.52; acc: 0.53
Batch: 360; loss: 1.5; acc: 0.58
Batch: 380; loss: 1.62; acc: 0.53
Batch: 400; loss: 1.61; acc: 0.45
Batch: 420; loss: 1.65; acc: 0.47
Batch: 440; loss: 1.64; acc: 0.47
Batch: 460; loss: 1.61; acc: 0.48
Batch: 480; loss: 1.67; acc: 0.44
Batch: 500; loss: 1.6; acc: 0.53
Batch: 520; loss: 1.58; acc: 0.58
Batch: 540; loss: 1.58; acc: 0.5
Batch: 560; loss: 1.64; acc: 0.47
Batch: 580; loss: 1.68; acc: 0.44
Batch: 600; loss: 1.53; acc: 0.53
Batch: 620; loss: 1.58; acc: 0.47
Batch: 640; loss: 1.63; acc: 0.5
Batch: 660; loss: 1.63; acc: 0.5
Batch: 680; loss: 1.62; acc: 0.44
Batch: 700; loss: 1.6; acc: 0.52
Batch: 720; loss: 1.55; acc: 0.53
Batch: 740; loss: 1.48; acc: 0.59
Batch: 760; loss: 1.7; acc: 0.42
Batch: 780; loss: 1.62; acc: 0.53
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

3.967011798522435e-05
1.2690961739281192e-05
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.36
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.5; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.53
Batch: 100; loss: 1.54; acc: 0.48
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.38; acc: 0.69
Val Epoch over. val_loss: 1.5707199702596968; val_accuracy: 0.5204020700636943 

The current subspace-distance is: 1.2690961739281192e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.6; acc: 0.45
Batch: 40; loss: 1.6; acc: 0.5
Batch: 60; loss: 1.67; acc: 0.42
Batch: 80; loss: 1.56; acc: 0.55
Batch: 100; loss: 1.48; acc: 0.58
Batch: 120; loss: 1.84; acc: 0.39
Batch: 140; loss: 1.63; acc: 0.52
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.55; acc: 0.56
Batch: 200; loss: 1.6; acc: 0.56
Batch: 220; loss: 1.84; acc: 0.36
Batch: 240; loss: 1.6; acc: 0.5
Batch: 260; loss: 1.6; acc: 0.44
Batch: 280; loss: 1.59; acc: 0.5
Batch: 300; loss: 1.49; acc: 0.55
Batch: 320; loss: 1.51; acc: 0.56
Batch: 340; loss: 1.44; acc: 0.59
Batch: 360; loss: 1.61; acc: 0.47
Batch: 380; loss: 1.55; acc: 0.55
Batch: 400; loss: 1.65; acc: 0.48
Batch: 420; loss: 1.68; acc: 0.47
Batch: 440; loss: 1.71; acc: 0.5
Batch: 460; loss: 1.69; acc: 0.47
Batch: 480; loss: 1.66; acc: 0.47
Batch: 500; loss: 1.42; acc: 0.59
Batch: 520; loss: 1.62; acc: 0.52
Batch: 540; loss: 1.43; acc: 0.64
Batch: 560; loss: 1.6; acc: 0.42
Batch: 580; loss: 1.55; acc: 0.53
Batch: 600; loss: 1.62; acc: 0.41
Batch: 620; loss: 1.47; acc: 0.64
Batch: 640; loss: 1.72; acc: 0.44
Batch: 660; loss: 1.51; acc: 0.59
Batch: 680; loss: 1.53; acc: 0.59
Batch: 700; loss: 1.56; acc: 0.5
Batch: 720; loss: 1.58; acc: 0.45
Batch: 740; loss: 1.72; acc: 0.41
Batch: 760; loss: 1.6; acc: 0.5
Batch: 780; loss: 1.59; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

4.013410580228083e-05
1.2601062735484447e-05
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.82; acc: 0.36
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.5; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.54; acc: 0.48
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.38; acc: 0.66
Val Epoch over. val_loss: 1.568855381315681; val_accuracy: 0.5196058917197452 

The current subspace-distance is: 1.2601062735484447e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.57; acc: 0.5
Batch: 40; loss: 1.63; acc: 0.48
Batch: 60; loss: 1.62; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.52
Batch: 100; loss: 1.61; acc: 0.47
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.63; acc: 0.47
Batch: 160; loss: 1.65; acc: 0.47
Batch: 180; loss: 1.76; acc: 0.41
Batch: 200; loss: 1.75; acc: 0.45
Batch: 220; loss: 1.51; acc: 0.58
Batch: 240; loss: 1.39; acc: 0.67
Batch: 260; loss: 1.57; acc: 0.48
Batch: 280; loss: 1.42; acc: 0.58
Batch: 300; loss: 1.61; acc: 0.44
Batch: 320; loss: 1.51; acc: 0.56
Batch: 340; loss: 1.79; acc: 0.45
Batch: 360; loss: 1.46; acc: 0.61
Batch: 380; loss: 1.56; acc: 0.52
Batch: 400; loss: 1.69; acc: 0.52
Batch: 420; loss: 1.47; acc: 0.62
Batch: 440; loss: 1.66; acc: 0.45
Batch: 460; loss: 1.71; acc: 0.55
Batch: 480; loss: 1.69; acc: 0.56
Batch: 500; loss: 1.71; acc: 0.39
Batch: 520; loss: 1.71; acc: 0.47
Batch: 540; loss: 1.75; acc: 0.47
Batch: 560; loss: 1.55; acc: 0.55
Batch: 580; loss: 1.5; acc: 0.59
Batch: 600; loss: 1.77; acc: 0.42
Batch: 620; loss: 1.83; acc: 0.34
Batch: 640; loss: 1.5; acc: 0.55
Batch: 660; loss: 1.77; acc: 0.48
Batch: 680; loss: 1.54; acc: 0.52
Batch: 700; loss: 1.7; acc: 0.48
Batch: 720; loss: 1.63; acc: 0.47
Batch: 740; loss: 1.54; acc: 0.58
Batch: 760; loss: 1.75; acc: 0.42
Batch: 780; loss: 1.54; acc: 0.55
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

4.043804801767692e-05
1.3329485227586702e-05
Batch: 0; loss: 1.57; acc: 0.56
Batch: 20; loss: 1.81; acc: 0.38
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.5
Batch: 100; loss: 1.54; acc: 0.5
Batch: 120; loss: 1.68; acc: 0.52
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.564135010834712; val_accuracy: 0.5227906050955414 

The current subspace-distance is: 1.3329485227586702e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.56; acc: 0.58
Batch: 20; loss: 1.51; acc: 0.55
Batch: 40; loss: 1.53; acc: 0.59
Batch: 60; loss: 1.59; acc: 0.56
Batch: 80; loss: 1.54; acc: 0.56
Batch: 100; loss: 1.69; acc: 0.48
Batch: 120; loss: 1.66; acc: 0.42
Batch: 140; loss: 1.55; acc: 0.55
Batch: 160; loss: 1.71; acc: 0.44
Batch: 180; loss: 1.5; acc: 0.53
Batch: 200; loss: 1.64; acc: 0.42
Batch: 220; loss: 1.62; acc: 0.47
Batch: 240; loss: 1.52; acc: 0.53
Batch: 260; loss: 1.49; acc: 0.55
Batch: 280; loss: 1.68; acc: 0.45
Batch: 300; loss: 1.65; acc: 0.48
Batch: 320; loss: 1.63; acc: 0.47
Batch: 340; loss: 1.57; acc: 0.56
Batch: 360; loss: 1.61; acc: 0.56
Batch: 380; loss: 1.59; acc: 0.47
Batch: 400; loss: 1.56; acc: 0.45
Batch: 420; loss: 1.78; acc: 0.45
Batch: 440; loss: 1.59; acc: 0.53
Batch: 460; loss: 1.72; acc: 0.47
Batch: 480; loss: 1.57; acc: 0.5
Batch: 500; loss: 1.8; acc: 0.39
Batch: 520; loss: 1.72; acc: 0.39
Batch: 540; loss: 1.48; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.5
Batch: 580; loss: 1.55; acc: 0.56
Batch: 600; loss: 1.66; acc: 0.52
Batch: 620; loss: 1.6; acc: 0.55
Batch: 640; loss: 1.68; acc: 0.47
Batch: 660; loss: 1.67; acc: 0.48
Batch: 680; loss: 1.6; acc: 0.53
Batch: 700; loss: 1.48; acc: 0.53
Batch: 720; loss: 1.61; acc: 0.52
Batch: 740; loss: 1.55; acc: 0.52
Batch: 760; loss: 1.48; acc: 0.58
Batch: 780; loss: 1.75; acc: 0.47
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

4.130893648834899e-05
1.3864771062799264e-05
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.82; acc: 0.31
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.55
Batch: 100; loss: 1.54; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.53
Batch: 140; loss: 1.39; acc: 0.67
Val Epoch over. val_loss: 1.5699058854655854; val_accuracy: 0.5177149681528662 

The current subspace-distance is: 1.3864771062799264e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.56
Batch: 20; loss: 1.7; acc: 0.5
Batch: 40; loss: 1.66; acc: 0.52
Batch: 60; loss: 1.53; acc: 0.56
Batch: 80; loss: 1.64; acc: 0.48
Batch: 100; loss: 1.61; acc: 0.48
Batch: 120; loss: 1.55; acc: 0.55
Batch: 140; loss: 1.51; acc: 0.58
Batch: 160; loss: 1.62; acc: 0.5
Batch: 180; loss: 1.62; acc: 0.5
Batch: 200; loss: 1.73; acc: 0.39
Batch: 220; loss: 1.49; acc: 0.58
Batch: 240; loss: 1.74; acc: 0.47
Batch: 260; loss: 1.7; acc: 0.45
Batch: 280; loss: 1.64; acc: 0.39
Batch: 300; loss: 1.6; acc: 0.52
Batch: 320; loss: 1.51; acc: 0.52
Batch: 340; loss: 1.76; acc: 0.41
Batch: 360; loss: 1.47; acc: 0.59
Batch: 380; loss: 1.6; acc: 0.48
Batch: 400; loss: 1.72; acc: 0.45
Batch: 420; loss: 1.53; acc: 0.53
Batch: 440; loss: 1.55; acc: 0.52
Batch: 460; loss: 1.48; acc: 0.55
Batch: 480; loss: 1.5; acc: 0.52
Batch: 500; loss: 1.58; acc: 0.53
Batch: 520; loss: 1.64; acc: 0.48
Batch: 540; loss: 1.48; acc: 0.59
Batch: 560; loss: 1.71; acc: 0.41
Batch: 580; loss: 1.71; acc: 0.39
Batch: 600; loss: 1.53; acc: 0.52
Batch: 620; loss: 1.57; acc: 0.55
Batch: 640; loss: 1.65; acc: 0.5
Batch: 660; loss: 1.59; acc: 0.52
Batch: 680; loss: 1.4; acc: 0.61
Batch: 700; loss: 1.58; acc: 0.5
Batch: 720; loss: 1.61; acc: 0.52
Batch: 740; loss: 1.6; acc: 0.5
Batch: 760; loss: 1.74; acc: 0.36
Batch: 780; loss: 1.46; acc: 0.61
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

4.000368062406778e-05
1.1993756743322592e-05
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.82; acc: 0.33
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.54; acc: 0.52
Batch: 100; loss: 1.53; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.37; acc: 0.66
Val Epoch over. val_loss: 1.563837831946695; val_accuracy: 0.521297770700637 

The current subspace-distance is: 1.1993756743322592e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.64
Batch: 20; loss: 1.63; acc: 0.53
Batch: 40; loss: 1.63; acc: 0.45
Batch: 60; loss: 1.59; acc: 0.55
Batch: 80; loss: 1.58; acc: 0.52
Batch: 100; loss: 1.51; acc: 0.48
Batch: 120; loss: 1.61; acc: 0.56
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.52; acc: 0.52
Batch: 180; loss: 1.57; acc: 0.52
Batch: 200; loss: 1.58; acc: 0.52
Batch: 220; loss: 1.64; acc: 0.48
Batch: 240; loss: 1.51; acc: 0.52
Batch: 260; loss: 1.74; acc: 0.41
Batch: 280; loss: 1.68; acc: 0.5
Batch: 300; loss: 1.65; acc: 0.5
Batch: 320; loss: 1.61; acc: 0.48
Batch: 340; loss: 1.53; acc: 0.61
Batch: 360; loss: 1.55; acc: 0.55
Batch: 380; loss: 1.59; acc: 0.52
Batch: 400; loss: 1.57; acc: 0.52
Batch: 420; loss: 1.66; acc: 0.48
Batch: 440; loss: 1.85; acc: 0.3
Batch: 460; loss: 1.45; acc: 0.62
Batch: 480; loss: 1.7; acc: 0.45
Batch: 500; loss: 1.55; acc: 0.52
Batch: 520; loss: 1.74; acc: 0.45
Batch: 540; loss: 1.55; acc: 0.52
Batch: 560; loss: 1.65; acc: 0.48
Batch: 580; loss: 1.57; acc: 0.55
Batch: 600; loss: 1.62; acc: 0.44
Batch: 620; loss: 1.6; acc: 0.45
Batch: 640; loss: 1.6; acc: 0.52
Batch: 660; loss: 1.54; acc: 0.53
Batch: 680; loss: 1.51; acc: 0.55
Batch: 700; loss: 1.61; acc: 0.44
Batch: 720; loss: 1.49; acc: 0.56
Batch: 740; loss: 1.59; acc: 0.48
Batch: 760; loss: 1.69; acc: 0.42
Batch: 780; loss: 1.58; acc: 0.45
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

3.963442213716917e-05
1.129403699451359e-05
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.81; acc: 0.34
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.53
Batch: 100; loss: 1.54; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.5609288238416053; val_accuracy: 0.5213972929936306 

The current subspace-distance is: 1.129403699451359e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.53
Batch: 20; loss: 1.69; acc: 0.45
Batch: 40; loss: 1.52; acc: 0.47
Batch: 60; loss: 1.51; acc: 0.5
Batch: 80; loss: 1.73; acc: 0.41
Batch: 100; loss: 1.68; acc: 0.47
Batch: 120; loss: 1.63; acc: 0.42
Batch: 140; loss: 1.55; acc: 0.47
Batch: 160; loss: 1.67; acc: 0.5
Batch: 180; loss: 1.57; acc: 0.47
Batch: 200; loss: 1.58; acc: 0.56
Batch: 220; loss: 1.61; acc: 0.53
Batch: 240; loss: 1.7; acc: 0.55
Batch: 260; loss: 1.56; acc: 0.48
Batch: 280; loss: 1.58; acc: 0.44
Batch: 300; loss: 1.56; acc: 0.58
Batch: 320; loss: 1.56; acc: 0.55
Batch: 340; loss: 1.54; acc: 0.5
Batch: 360; loss: 1.61; acc: 0.53
Batch: 380; loss: 1.6; acc: 0.53
Batch: 400; loss: 1.58; acc: 0.59
Batch: 420; loss: 1.69; acc: 0.5
Batch: 440; loss: 1.48; acc: 0.55
Batch: 460; loss: 1.62; acc: 0.52
Batch: 480; loss: 1.76; acc: 0.44
Batch: 500; loss: 1.65; acc: 0.5
Batch: 520; loss: 1.71; acc: 0.34
Batch: 540; loss: 1.67; acc: 0.53
Batch: 560; loss: 1.7; acc: 0.42
Batch: 580; loss: 1.68; acc: 0.47
Batch: 600; loss: 1.59; acc: 0.44
Batch: 620; loss: 1.65; acc: 0.47
Batch: 640; loss: 1.41; acc: 0.67
Batch: 660; loss: 1.57; acc: 0.55
Batch: 680; loss: 1.63; acc: 0.48
Batch: 700; loss: 1.64; acc: 0.42
Batch: 720; loss: 1.79; acc: 0.38
Batch: 740; loss: 1.67; acc: 0.45
Batch: 760; loss: 1.7; acc: 0.41
Batch: 780; loss: 1.61; acc: 0.53
Train Epoch over. train_loss: 1.61; train_accuracy: 0.51 

4.0035312849795446e-05
1.3786629097012337e-05
Batch: 0; loss: 1.58; acc: 0.53
Batch: 20; loss: 1.82; acc: 0.33
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.52
Batch: 100; loss: 1.54; acc: 0.52
Batch: 120; loss: 1.68; acc: 0.53
Batch: 140; loss: 1.38; acc: 0.66
Val Epoch over. val_loss: 1.5646966020013118; val_accuracy: 0.5223925159235668 

The current subspace-distance is: 1.3786629097012337e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.73; acc: 0.44
Batch: 20; loss: 1.65; acc: 0.47
Batch: 40; loss: 1.57; acc: 0.5
Batch: 60; loss: 1.71; acc: 0.45
Batch: 80; loss: 1.61; acc: 0.48
Batch: 100; loss: 1.49; acc: 0.56
Batch: 120; loss: 1.72; acc: 0.45
Batch: 140; loss: 1.71; acc: 0.5
Batch: 160; loss: 1.46; acc: 0.59
Batch: 180; loss: 1.53; acc: 0.55
Batch: 200; loss: 1.39; acc: 0.66
Batch: 220; loss: 1.49; acc: 0.52
Batch: 240; loss: 1.63; acc: 0.5
Batch: 260; loss: 1.69; acc: 0.39
Batch: 280; loss: 1.48; acc: 0.59
Batch: 300; loss: 1.79; acc: 0.42
Batch: 320; loss: 1.79; acc: 0.38
Batch: 340; loss: 1.73; acc: 0.45
Batch: 360; loss: 1.63; acc: 0.41
Batch: 380; loss: 1.67; acc: 0.52
Batch: 400; loss: 1.54; acc: 0.53
Batch: 420; loss: 1.54; acc: 0.52
Batch: 440; loss: 1.78; acc: 0.39
Batch: 460; loss: 1.71; acc: 0.42
Batch: 480; loss: 1.74; acc: 0.36
Batch: 500; loss: 1.7; acc: 0.47
Batch: 520; loss: 1.63; acc: 0.52
Batch: 540; loss: 1.75; acc: 0.45
Batch: 560; loss: 1.54; acc: 0.53
Batch: 580; loss: 1.6; acc: 0.48
Batch: 600; loss: 1.28; acc: 0.72
Batch: 620; loss: 1.73; acc: 0.42
Batch: 640; loss: 1.59; acc: 0.48
Batch: 660; loss: 1.71; acc: 0.45
Batch: 680; loss: 1.66; acc: 0.48
Batch: 700; loss: 1.6; acc: 0.45
Batch: 720; loss: 1.63; acc: 0.52
Batch: 740; loss: 1.47; acc: 0.62
Batch: 760; loss: 1.58; acc: 0.52
Batch: 780; loss: 1.48; acc: 0.58
Train Epoch over. train_loss: 1.6; train_accuracy: 0.5 

4.050723146065138e-05
1.4938090316718444e-05
Batch: 0; loss: 1.57; acc: 0.53
Batch: 20; loss: 1.82; acc: 0.33
Batch: 40; loss: 1.33; acc: 0.67
Batch: 60; loss: 1.49; acc: 0.61
Batch: 80; loss: 1.53; acc: 0.52
Batch: 100; loss: 1.53; acc: 0.52
Batch: 120; loss: 1.67; acc: 0.53
Batch: 140; loss: 1.38; acc: 0.66
Val Epoch over. val_loss: 1.5609529079145688; val_accuracy: 0.5240843949044586 

The current subspace-distance is: 1.4938090316718444e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.56
Batch: 20; loss: 1.59; acc: 0.48
Batch: 40; loss: 1.65; acc: 0.41
Batch: 60; loss: 1.62; acc: 0.45
Batch: 80; loss: 1.48; acc: 0.64
Batch: 100; loss: 1.46; acc: 0.67
Batch: 120; loss: 1.57; acc: 0.56
Batch: 140; loss: 1.66; acc: 0.55
Batch: 160; loss: 1.69; acc: 0.39
Batch: 180; loss: 1.68; acc: 0.55
Batch: 200; loss: 1.62; acc: 0.52
Batch: 220; loss: 1.54; acc: 0.53
Batch: 240; loss: 1.65; acc: 0.45
Batch: 260; loss: 1.66; acc: 0.39
Batch: 280; loss: 1.55; acc: 0.56
Batch: 300; loss: 1.55; acc: 0.55
Batch: 320; loss: 1.67; acc: 0.41
Batch: 340; loss: 1.59; acc: 0.53
Batch: 360; loss: 1.64; acc: 0.55
Batch: 380; loss: 1.52; acc: 0.55
Batch: 400; loss: 1.66; acc: 0.48
Batch: 420; loss: 1.83; acc: 0.36
Batch: 440; loss: 1.54; acc: 0.53
Batch: 460; loss: 1.65; acc: 0.48
Batch: 480; loss: 1.7; acc: 0.39
Batch: 500; loss: 1.56; acc: 0.58
Batch: 520; loss: 1.66; acc: 0.52
Batch: 540; loss: 1.64; acc: 0.39
Batch: 560; loss: 1.69; acc: 0.44
Batch: 580; loss: 1.58; acc: 0.5
Batch: 600; loss: 1.58; acc: 0.59
Batch: 620; loss: 1.57; acc: 0.48
Batch: 640; loss: 1.59; acc: 0.48
Batch: 660; loss: 1.63; acc: 0.48
Batch: 680; loss: 1.51; acc: 0.56
Batch: 700; loss: 1.56; acc: 0.53
Batch: 720; loss: 1.59; acc: 0.59
Batch: 740; loss: 1.47; acc: 0.55
Batch: 760; loss: 1.65; acc: 0.5
Batch: 780; loss: 1.75; acc: 0.42
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

3.966617805417627e-05
9.669707651482895e-06
Batch: 0; loss: 1.57; acc: 0.53
Batch: 20; loss: 1.82; acc: 0.33
Batch: 40; loss: 1.35; acc: 0.66
Batch: 60; loss: 1.49; acc: 0.62
Batch: 80; loss: 1.53; acc: 0.5
Batch: 100; loss: 1.53; acc: 0.5
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.38; acc: 0.66
Val Epoch over. val_loss: 1.5677090837697314; val_accuracy: 0.5192078025477707 

The current subspace-distance is: 9.669707651482895e-06 

plots/subspace_training/table13slim/2020-01-29 16:12:31/N_14_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.07

The number of parameters is: 267152

The number of individual parameters is:

9
162
9
9
13
31356
13
13
26
90584
26
26
64
139776
64
64
4096
64
640
10
64
64

nonzero elements in E: 26715197
elements in E: 26715200
fraction nonzero: 0.999999887704378
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.57; acc: 0.09
Batch: 20; loss: 2.38; acc: 0.12
Batch: 40; loss: 2.31; acc: 0.16
Batch: 60; loss: 2.28; acc: 0.19
Batch: 80; loss: 2.21; acc: 0.27
Batch: 100; loss: 2.2; acc: 0.3
Batch: 120; loss: 2.0; acc: 0.33
Batch: 140; loss: 2.09; acc: 0.27
Batch: 160; loss: 1.95; acc: 0.41
Batch: 180; loss: 1.99; acc: 0.41
Batch: 200; loss: 1.95; acc: 0.36
Batch: 220; loss: 1.92; acc: 0.33
Batch: 240; loss: 1.89; acc: 0.36
Batch: 260; loss: 1.89; acc: 0.38
Batch: 280; loss: 1.81; acc: 0.44
Batch: 300; loss: 1.84; acc: 0.34
Batch: 320; loss: 1.9; acc: 0.39
Batch: 340; loss: 1.81; acc: 0.47
Batch: 360; loss: 1.83; acc: 0.5
Batch: 380; loss: 1.72; acc: 0.55
Batch: 400; loss: 1.88; acc: 0.38
Batch: 420; loss: 1.79; acc: 0.5
Batch: 440; loss: 1.69; acc: 0.55
Batch: 460; loss: 1.75; acc: 0.44
Batch: 480; loss: 1.71; acc: 0.5
Batch: 500; loss: 1.83; acc: 0.47
Batch: 520; loss: 1.62; acc: 0.69
Batch: 540; loss: 1.9; acc: 0.42
Batch: 560; loss: 1.82; acc: 0.42
Batch: 580; loss: 1.76; acc: 0.47
Batch: 600; loss: 1.72; acc: 0.56
Batch: 620; loss: 1.78; acc: 0.42
Batch: 640; loss: 1.81; acc: 0.38
Batch: 660; loss: 1.79; acc: 0.45
Batch: 680; loss: 1.74; acc: 0.5
Batch: 700; loss: 1.63; acc: 0.56
Batch: 720; loss: 1.67; acc: 0.58
Batch: 740; loss: 1.67; acc: 0.56
Batch: 760; loss: 1.76; acc: 0.58
Batch: 780; loss: 1.8; acc: 0.44
Train Epoch over. train_loss: 1.88; train_accuracy: 0.42 

5.728610631194897e-05
5.223018160904758e-05
Batch: 0; loss: 1.76; acc: 0.55
Batch: 20; loss: 1.69; acc: 0.48
Batch: 40; loss: 1.53; acc: 0.62
Batch: 60; loss: 1.66; acc: 0.56
Batch: 80; loss: 1.6; acc: 0.56
Batch: 100; loss: 1.77; acc: 0.47
Batch: 120; loss: 1.74; acc: 0.45
Batch: 140; loss: 1.49; acc: 0.7
Val Epoch over. val_loss: 1.6809743635214058; val_accuracy: 0.5501592356687898 

The current subspace-distance is: 5.223018160904758e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.61
Batch: 40; loss: 1.66; acc: 0.55
Batch: 60; loss: 1.68; acc: 0.52
Batch: 80; loss: 1.69; acc: 0.55
Batch: 100; loss: 1.68; acc: 0.52
Batch: 120; loss: 1.71; acc: 0.58
Batch: 140; loss: 1.73; acc: 0.5
Batch: 160; loss: 1.57; acc: 0.61
Batch: 180; loss: 1.62; acc: 0.55
Batch: 200; loss: 1.73; acc: 0.48
Batch: 220; loss: 1.63; acc: 0.66
Batch: 240; loss: 1.69; acc: 0.59
Batch: 260; loss: 1.72; acc: 0.5
Batch: 280; loss: 1.5; acc: 0.69
Batch: 300; loss: 1.71; acc: 0.52
Batch: 320; loss: 1.51; acc: 0.73
Batch: 340; loss: 1.68; acc: 0.48
Batch: 360; loss: 1.51; acc: 0.62
Batch: 380; loss: 1.65; acc: 0.48
Batch: 400; loss: 1.52; acc: 0.62
Batch: 420; loss: 1.64; acc: 0.59
Batch: 440; loss: 1.51; acc: 0.64
Batch: 460; loss: 1.61; acc: 0.59
Batch: 480; loss: 1.6; acc: 0.61
Batch: 500; loss: 1.62; acc: 0.59
Batch: 520; loss: 1.67; acc: 0.56
Batch: 540; loss: 1.49; acc: 0.64
Batch: 560; loss: 1.43; acc: 0.75
Batch: 580; loss: 1.62; acc: 0.55
Batch: 600; loss: 1.62; acc: 0.64
Batch: 620; loss: 1.71; acc: 0.48
Batch: 640; loss: 1.55; acc: 0.58
Batch: 660; loss: 1.64; acc: 0.61
Batch: 680; loss: 1.61; acc: 0.61
Batch: 700; loss: 1.61; acc: 0.58
Batch: 720; loss: 1.42; acc: 0.72
Batch: 740; loss: 1.54; acc: 0.64
Batch: 760; loss: 1.62; acc: 0.58
Batch: 780; loss: 1.5; acc: 0.69
Train Epoch over. train_loss: 1.63; train_accuracy: 0.58 

7.430074037984014e-05
6.994756404310465e-05
Batch: 0; loss: 1.65; acc: 0.56
Batch: 20; loss: 1.61; acc: 0.55
Batch: 40; loss: 1.44; acc: 0.73
Batch: 60; loss: 1.51; acc: 0.66
Batch: 80; loss: 1.41; acc: 0.7
Batch: 100; loss: 1.66; acc: 0.55
Batch: 120; loss: 1.68; acc: 0.55
Batch: 140; loss: 1.4; acc: 0.72
Val Epoch over. val_loss: 1.5559172896063251; val_accuracy: 0.6181329617834395 

The current subspace-distance is: 6.994756404310465e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.42
Batch: 20; loss: 1.57; acc: 0.62
Batch: 40; loss: 1.58; acc: 0.56
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.64; acc: 0.58
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.53; acc: 0.64
Batch: 140; loss: 1.47; acc: 0.72
Batch: 160; loss: 1.61; acc: 0.59
Batch: 180; loss: 1.48; acc: 0.69
Batch: 200; loss: 1.58; acc: 0.55
Batch: 220; loss: 1.54; acc: 0.61
Batch: 240; loss: 1.56; acc: 0.67
Batch: 260; loss: 1.52; acc: 0.64
Batch: 280; loss: 1.5; acc: 0.67
Batch: 300; loss: 1.51; acc: 0.58
Batch: 320; loss: 1.5; acc: 0.66
Batch: 340; loss: 1.54; acc: 0.64
Batch: 360; loss: 1.61; acc: 0.53
Batch: 380; loss: 1.52; acc: 0.66
Batch: 400; loss: 1.47; acc: 0.67
Batch: 420; loss: 1.48; acc: 0.72
Batch: 440; loss: 1.62; acc: 0.58
Batch: 460; loss: 1.55; acc: 0.64
Batch: 480; loss: 1.54; acc: 0.66
Batch: 500; loss: 1.65; acc: 0.58
Batch: 520; loss: 1.61; acc: 0.61
Batch: 540; loss: 1.54; acc: 0.62
Batch: 560; loss: 1.38; acc: 0.69
Batch: 580; loss: 1.55; acc: 0.59
Batch: 600; loss: 1.52; acc: 0.59
Batch: 620; loss: 1.59; acc: 0.5
Batch: 640; loss: 1.47; acc: 0.69
Batch: 660; loss: 1.5; acc: 0.58
Batch: 680; loss: 1.5; acc: 0.62
Batch: 700; loss: 1.4; acc: 0.72
Batch: 720; loss: 1.59; acc: 0.56
Batch: 740; loss: 1.64; acc: 0.52
Batch: 760; loss: 1.61; acc: 0.5
Batch: 780; loss: 1.44; acc: 0.64
Train Epoch over. train_loss: 1.55; train_accuracy: 0.61 

8.812240412225947e-05
8.221926691476256e-05
Batch: 0; loss: 1.61; acc: 0.55
Batch: 20; loss: 1.56; acc: 0.58
Batch: 40; loss: 1.41; acc: 0.67
Batch: 60; loss: 1.46; acc: 0.64
Batch: 80; loss: 1.34; acc: 0.75
Batch: 100; loss: 1.61; acc: 0.58
Batch: 120; loss: 1.6; acc: 0.66
Batch: 140; loss: 1.34; acc: 0.8
Val Epoch over. val_loss: 1.4907425702757138; val_accuracy: 0.6448049363057324 

The current subspace-distance is: 8.221926691476256e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.57; acc: 0.62
Batch: 20; loss: 1.52; acc: 0.62
Batch: 40; loss: 1.54; acc: 0.59
Batch: 60; loss: 1.48; acc: 0.58
Batch: 80; loss: 1.44; acc: 0.72
Batch: 100; loss: 1.47; acc: 0.69
Batch: 120; loss: 1.52; acc: 0.66
Batch: 140; loss: 1.52; acc: 0.66
Batch: 160; loss: 1.66; acc: 0.56
Batch: 180; loss: 1.5; acc: 0.61
Batch: 200; loss: 1.55; acc: 0.55
Batch: 220; loss: 1.61; acc: 0.58
Batch: 240; loss: 1.47; acc: 0.59
Batch: 260; loss: 1.45; acc: 0.59
Batch: 280; loss: 1.39; acc: 0.67
Batch: 300; loss: 1.53; acc: 0.67
Batch: 320; loss: 1.43; acc: 0.67
Batch: 340; loss: 1.46; acc: 0.61
Batch: 360; loss: 1.45; acc: 0.64
Batch: 380; loss: 1.44; acc: 0.66
Batch: 400; loss: 1.47; acc: 0.62
Batch: 420; loss: 1.45; acc: 0.61
Batch: 440; loss: 1.52; acc: 0.61
Batch: 460; loss: 1.57; acc: 0.59
Batch: 480; loss: 1.49; acc: 0.62
Batch: 500; loss: 1.51; acc: 0.61
Batch: 520; loss: 1.46; acc: 0.7
Batch: 540; loss: 1.45; acc: 0.69
Batch: 560; loss: 1.45; acc: 0.66
Batch: 580; loss: 1.47; acc: 0.62
Batch: 600; loss: 1.53; acc: 0.58
Batch: 620; loss: 1.52; acc: 0.59
Batch: 640; loss: 1.61; acc: 0.5
Batch: 660; loss: 1.55; acc: 0.48
Batch: 680; loss: 1.51; acc: 0.64
Batch: 700; loss: 1.59; acc: 0.53
Batch: 720; loss: 1.6; acc: 0.58
Batch: 740; loss: 1.45; acc: 0.67
Batch: 760; loss: 1.41; acc: 0.72
Batch: 780; loss: 1.29; acc: 0.64
Train Epoch over. train_loss: 1.5; train_accuracy: 0.62 

9.823536674957722e-05
9.389447950525209e-05
Batch: 0; loss: 1.57; acc: 0.56
Batch: 20; loss: 1.53; acc: 0.56
Batch: 40; loss: 1.36; acc: 0.72
Batch: 60; loss: 1.41; acc: 0.69
Batch: 80; loss: 1.31; acc: 0.78
Batch: 100; loss: 1.57; acc: 0.56
Batch: 120; loss: 1.53; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.81
Val Epoch over. val_loss: 1.4427812471511259; val_accuracy: 0.6579418789808917 

The current subspace-distance is: 9.389447950525209e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.62
Batch: 20; loss: 1.43; acc: 0.66
Batch: 40; loss: 1.51; acc: 0.58
Batch: 60; loss: 1.36; acc: 0.64
Batch: 80; loss: 1.48; acc: 0.56
Batch: 100; loss: 1.41; acc: 0.7
Batch: 120; loss: 1.51; acc: 0.5
Batch: 140; loss: 1.5; acc: 0.56
Batch: 160; loss: 1.52; acc: 0.58
Batch: 180; loss: 1.43; acc: 0.72
Batch: 200; loss: 1.49; acc: 0.64
Batch: 220; loss: 1.54; acc: 0.58
Batch: 240; loss: 1.53; acc: 0.64
Batch: 260; loss: 1.41; acc: 0.67
Batch: 280; loss: 1.4; acc: 0.67
Batch: 300; loss: 1.44; acc: 0.62
Batch: 320; loss: 1.45; acc: 0.62
Batch: 340; loss: 1.57; acc: 0.56
Batch: 360; loss: 1.6; acc: 0.5
Batch: 380; loss: 1.44; acc: 0.61
Batch: 400; loss: 1.35; acc: 0.7
Batch: 420; loss: 1.48; acc: 0.69
Batch: 440; loss: 1.48; acc: 0.59
Batch: 460; loss: 1.43; acc: 0.7
Batch: 480; loss: 1.45; acc: 0.66
Batch: 500; loss: 1.44; acc: 0.66
Batch: 520; loss: 1.42; acc: 0.64
Batch: 540; loss: 1.43; acc: 0.59
Batch: 560; loss: 1.34; acc: 0.66
Batch: 580; loss: 1.54; acc: 0.61
Batch: 600; loss: 1.44; acc: 0.61
Batch: 620; loss: 1.47; acc: 0.58
Batch: 640; loss: 1.38; acc: 0.69
Batch: 660; loss: 1.42; acc: 0.67
Batch: 680; loss: 1.56; acc: 0.59
Batch: 700; loss: 1.38; acc: 0.64
Batch: 720; loss: 1.4; acc: 0.64
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.35; acc: 0.7
Batch: 780; loss: 1.37; acc: 0.64
Train Epoch over. train_loss: 1.45; train_accuracy: 0.64 

0.00010938279592664912
0.00010292876686435193
Batch: 0; loss: 1.53; acc: 0.55
Batch: 20; loss: 1.52; acc: 0.59
Batch: 40; loss: 1.29; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.72
Batch: 80; loss: 1.29; acc: 0.7
Batch: 100; loss: 1.5; acc: 0.62
Batch: 120; loss: 1.47; acc: 0.61
Batch: 140; loss: 1.27; acc: 0.78
Val Epoch over. val_loss: 1.3975037138932829; val_accuracy: 0.6714769108280255 

The current subspace-distance is: 0.00010292876686435193 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.69
Batch: 20; loss: 1.48; acc: 0.64
Batch: 40; loss: 1.55; acc: 0.62
Batch: 60; loss: 1.47; acc: 0.66
Batch: 80; loss: 1.25; acc: 0.75
Batch: 100; loss: 1.4; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.67
Batch: 140; loss: 1.45; acc: 0.61
Batch: 160; loss: 1.43; acc: 0.69
Batch: 180; loss: 1.38; acc: 0.72
Batch: 200; loss: 1.43; acc: 0.69
Batch: 220; loss: 1.45; acc: 0.62
Batch: 240; loss: 1.49; acc: 0.56
Batch: 260; loss: 1.31; acc: 0.77
Batch: 280; loss: 1.4; acc: 0.67
Batch: 300; loss: 1.34; acc: 0.64
Batch: 320; loss: 1.34; acc: 0.62
Batch: 340; loss: 1.35; acc: 0.7
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 1.28; acc: 0.75
Batch: 400; loss: 1.52; acc: 0.58
Batch: 420; loss: 1.42; acc: 0.61
Batch: 440; loss: 1.31; acc: 0.75
Batch: 460; loss: 1.43; acc: 0.59
Batch: 480; loss: 1.4; acc: 0.66
Batch: 500; loss: 1.44; acc: 0.62
Batch: 520; loss: 1.31; acc: 0.69
Batch: 540; loss: 1.47; acc: 0.62
Batch: 560; loss: 1.41; acc: 0.64
Batch: 580; loss: 1.49; acc: 0.64
Batch: 600; loss: 1.48; acc: 0.61
Batch: 620; loss: 1.34; acc: 0.67
Batch: 640; loss: 1.26; acc: 0.7
Batch: 660; loss: 1.33; acc: 0.64
Batch: 680; loss: 1.39; acc: 0.62
Batch: 700; loss: 1.39; acc: 0.67
Batch: 720; loss: 1.53; acc: 0.55
Batch: 740; loss: 1.28; acc: 0.67
Batch: 760; loss: 1.42; acc: 0.56
Batch: 780; loss: 1.47; acc: 0.59
Train Epoch over. train_loss: 1.41; train_accuracy: 0.64 

0.00011710527178365737
0.00011132248619105667
Batch: 0; loss: 1.51; acc: 0.53
Batch: 20; loss: 1.54; acc: 0.53
Batch: 40; loss: 1.21; acc: 0.8
Batch: 60; loss: 1.32; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.69
Batch: 100; loss: 1.42; acc: 0.66
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 1.27; acc: 0.77
Val Epoch over. val_loss: 1.3592322556076535; val_accuracy: 0.6715764331210191 

The current subspace-distance is: 0.00011132248619105667 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.48; acc: 0.55
Batch: 20; loss: 1.33; acc: 0.67
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.42; acc: 0.59
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.28; acc: 0.7
Batch: 120; loss: 1.33; acc: 0.66
Batch: 140; loss: 1.37; acc: 0.62
Batch: 160; loss: 1.41; acc: 0.62
Batch: 180; loss: 1.36; acc: 0.59
Batch: 200; loss: 1.34; acc: 0.67
Batch: 220; loss: 1.45; acc: 0.59
Batch: 240; loss: 1.41; acc: 0.58
Batch: 260; loss: 1.3; acc: 0.69
Batch: 280; loss: 1.48; acc: 0.58
Batch: 300; loss: 1.51; acc: 0.64
Batch: 320; loss: 1.37; acc: 0.67
Batch: 340; loss: 1.34; acc: 0.66
Batch: 360; loss: 1.49; acc: 0.64
Batch: 380; loss: 1.3; acc: 0.7
Batch: 400; loss: 1.27; acc: 0.67
Batch: 420; loss: 1.46; acc: 0.56
Batch: 440; loss: 1.45; acc: 0.64
Batch: 460; loss: 1.34; acc: 0.67
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.38; acc: 0.61
Batch: 520; loss: 1.59; acc: 0.52
Batch: 540; loss: 1.35; acc: 0.61
Batch: 560; loss: 1.43; acc: 0.5
Batch: 580; loss: 1.35; acc: 0.58
Batch: 600; loss: 1.41; acc: 0.61
Batch: 620; loss: 1.35; acc: 0.58
Batch: 640; loss: 1.32; acc: 0.7
Batch: 660; loss: 1.41; acc: 0.59
Batch: 680; loss: 1.31; acc: 0.64
Batch: 700; loss: 1.4; acc: 0.53
Batch: 720; loss: 1.26; acc: 0.73
Batch: 740; loss: 1.31; acc: 0.66
Batch: 760; loss: 1.22; acc: 0.72
Batch: 780; loss: 1.5; acc: 0.55
Train Epoch over. train_loss: 1.38; train_accuracy: 0.64 

0.00012829425395466387
0.00012175906158518046
Batch: 0; loss: 1.48; acc: 0.56
Batch: 20; loss: 1.57; acc: 0.48
Batch: 40; loss: 1.17; acc: 0.75
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.25; acc: 0.7
Batch: 100; loss: 1.37; acc: 0.7
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.29; acc: 0.77
Val Epoch over. val_loss: 1.3386245753355086; val_accuracy: 0.6638136942675159 

The current subspace-distance is: 0.00012175906158518046 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.52
Batch: 20; loss: 1.36; acc: 0.62
Batch: 40; loss: 1.25; acc: 0.69
Batch: 60; loss: 1.34; acc: 0.61
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.43; acc: 0.59
Batch: 120; loss: 1.43; acc: 0.61
Batch: 140; loss: 1.6; acc: 0.52
Batch: 160; loss: 1.42; acc: 0.64
Batch: 180; loss: 1.2; acc: 0.73
Batch: 200; loss: 1.35; acc: 0.52
Batch: 220; loss: 1.4; acc: 0.58
Batch: 240; loss: 1.38; acc: 0.62
Batch: 260; loss: 1.29; acc: 0.69
Batch: 280; loss: 1.47; acc: 0.61
Batch: 300; loss: 1.31; acc: 0.69
Batch: 320; loss: 1.4; acc: 0.59
Batch: 340; loss: 1.27; acc: 0.67
Batch: 360; loss: 1.36; acc: 0.61
Batch: 380; loss: 1.3; acc: 0.62
Batch: 400; loss: 1.36; acc: 0.67
Batch: 420; loss: 1.33; acc: 0.61
Batch: 440; loss: 1.44; acc: 0.59
Batch: 460; loss: 1.37; acc: 0.67
Batch: 480; loss: 1.28; acc: 0.66
Batch: 500; loss: 1.35; acc: 0.62
Batch: 520; loss: 1.33; acc: 0.62
Batch: 540; loss: 1.27; acc: 0.67
Batch: 560; loss: 1.39; acc: 0.61
Batch: 580; loss: 1.43; acc: 0.64
Batch: 600; loss: 1.58; acc: 0.55
Batch: 620; loss: 1.31; acc: 0.72
Batch: 640; loss: 1.22; acc: 0.69
Batch: 660; loss: 1.51; acc: 0.52
Batch: 680; loss: 1.45; acc: 0.59
Batch: 700; loss: 1.31; acc: 0.69
Batch: 720; loss: 1.36; acc: 0.73
Batch: 740; loss: 1.39; acc: 0.64
Batch: 760; loss: 1.28; acc: 0.69
Batch: 780; loss: 1.42; acc: 0.59
Train Epoch over. train_loss: 1.36; train_accuracy: 0.64 

0.00013605556159745902
0.0001307092170463875
Batch: 0; loss: 1.48; acc: 0.53
Batch: 20; loss: 1.59; acc: 0.48
Batch: 40; loss: 1.12; acc: 0.78
Batch: 60; loss: 1.28; acc: 0.7
Batch: 80; loss: 1.23; acc: 0.69
Batch: 100; loss: 1.34; acc: 0.7
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.31; acc: 0.72
Val Epoch over. val_loss: 1.3198669009907231; val_accuracy: 0.6571457006369427 

The current subspace-distance is: 0.0001307092170463875 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.7
Batch: 20; loss: 1.42; acc: 0.61
Batch: 40; loss: 1.36; acc: 0.59
Batch: 60; loss: 1.34; acc: 0.62
Batch: 80; loss: 1.35; acc: 0.67
Batch: 100; loss: 1.33; acc: 0.59
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.15; acc: 0.69
Batch: 160; loss: 1.3; acc: 0.62
Batch: 180; loss: 1.4; acc: 0.56
Batch: 200; loss: 1.37; acc: 0.64
Batch: 220; loss: 1.42; acc: 0.58
Batch: 240; loss: 1.34; acc: 0.7
Batch: 260; loss: 1.31; acc: 0.7
Batch: 280; loss: 1.42; acc: 0.59
Batch: 300; loss: 1.29; acc: 0.62
Batch: 320; loss: 1.27; acc: 0.72
Batch: 340; loss: 1.45; acc: 0.62
Batch: 360; loss: 1.38; acc: 0.58
Batch: 380; loss: 1.25; acc: 0.73
Batch: 400; loss: 1.54; acc: 0.59
Batch: 420; loss: 1.31; acc: 0.66
Batch: 440; loss: 1.43; acc: 0.62
Batch: 460; loss: 1.3; acc: 0.67
Batch: 480; loss: 1.25; acc: 0.69
Batch: 500; loss: 1.37; acc: 0.62
Batch: 520; loss: 1.31; acc: 0.67
Batch: 540; loss: 1.33; acc: 0.69
Batch: 560; loss: 1.42; acc: 0.69
Batch: 580; loss: 1.38; acc: 0.66
Batch: 600; loss: 1.22; acc: 0.69
Batch: 620; loss: 1.41; acc: 0.59
Batch: 640; loss: 1.23; acc: 0.67
Batch: 660; loss: 1.37; acc: 0.59
Batch: 680; loss: 1.31; acc: 0.66
Batch: 700; loss: 1.25; acc: 0.64
Batch: 720; loss: 1.35; acc: 0.58
Batch: 740; loss: 1.21; acc: 0.67
Batch: 760; loss: 1.29; acc: 0.62
Batch: 780; loss: 1.38; acc: 0.67
Train Epoch over. train_loss: 1.34; train_accuracy: 0.64 

0.00014120880223345011
0.00013656470400746912
Batch: 0; loss: 1.42; acc: 0.55
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.09; acc: 0.75
Batch: 60; loss: 1.26; acc: 0.72
Batch: 80; loss: 1.21; acc: 0.7
Batch: 100; loss: 1.28; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.58
Batch: 140; loss: 1.29; acc: 0.66
Val Epoch over. val_loss: 1.2869340044677637; val_accuracy: 0.6644108280254777 

The current subspace-distance is: 0.00013656470400746912 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.58
Batch: 20; loss: 1.36; acc: 0.7
Batch: 40; loss: 1.13; acc: 0.8
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.25; acc: 0.69
Batch: 100; loss: 1.28; acc: 0.66
Batch: 120; loss: 1.26; acc: 0.62
Batch: 140; loss: 1.24; acc: 0.73
Batch: 160; loss: 1.4; acc: 0.66
Batch: 180; loss: 1.33; acc: 0.67
Batch: 200; loss: 1.43; acc: 0.53
Batch: 220; loss: 1.35; acc: 0.62
Batch: 240; loss: 1.29; acc: 0.62
Batch: 260; loss: 1.28; acc: 0.66
Batch: 280; loss: 1.33; acc: 0.61
Batch: 300; loss: 1.22; acc: 0.75
Batch: 320; loss: 1.26; acc: 0.67
Batch: 340; loss: 1.36; acc: 0.61
Batch: 360; loss: 1.25; acc: 0.75
Batch: 380; loss: 1.33; acc: 0.56
Batch: 400; loss: 1.32; acc: 0.7
Batch: 420; loss: 1.53; acc: 0.53
Batch: 440; loss: 1.32; acc: 0.66
Batch: 460; loss: 1.41; acc: 0.53
Batch: 480; loss: 1.29; acc: 0.69
Batch: 500; loss: 1.36; acc: 0.59
Batch: 520; loss: 1.35; acc: 0.62
Batch: 540; loss: 1.35; acc: 0.59
Batch: 560; loss: 1.47; acc: 0.56
Batch: 580; loss: 1.13; acc: 0.73
Batch: 600; loss: 1.45; acc: 0.52
Batch: 620; loss: 1.26; acc: 0.66
Batch: 640; loss: 1.26; acc: 0.69
Batch: 660; loss: 1.34; acc: 0.66
Batch: 680; loss: 1.33; acc: 0.66
Batch: 700; loss: 1.38; acc: 0.59
Batch: 720; loss: 1.49; acc: 0.52
Batch: 740; loss: 1.34; acc: 0.58
Batch: 760; loss: 1.42; acc: 0.56
Batch: 780; loss: 1.28; acc: 0.69
Train Epoch over. train_loss: 1.31; train_accuracy: 0.64 

0.00015123414050322026
0.00014469226880464703
Batch: 0; loss: 1.39; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.48
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.25; acc: 0.72
Batch: 80; loss: 1.2; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.69
Batch: 120; loss: 1.4; acc: 0.61
Batch: 140; loss: 1.31; acc: 0.61
Val Epoch over. val_loss: 1.2761778254417857; val_accuracy: 0.663515127388535 

The current subspace-distance is: 0.00014469226880464703 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.14; acc: 0.8
Batch: 20; loss: 1.23; acc: 0.72
Batch: 40; loss: 1.32; acc: 0.62
Batch: 60; loss: 1.2; acc: 0.7
Batch: 80; loss: 1.11; acc: 0.77
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.18; acc: 0.73
Batch: 140; loss: 1.35; acc: 0.59
Batch: 160; loss: 1.48; acc: 0.56
Batch: 180; loss: 1.28; acc: 0.73
Batch: 200; loss: 1.23; acc: 0.7
Batch: 220; loss: 1.18; acc: 0.73
Batch: 240; loss: 1.35; acc: 0.62
Batch: 260; loss: 1.38; acc: 0.59
Batch: 280; loss: 1.26; acc: 0.7
Batch: 300; loss: 1.26; acc: 0.69
Batch: 320; loss: 1.51; acc: 0.53
Batch: 340; loss: 1.3; acc: 0.61
Batch: 360; loss: 1.18; acc: 0.69
Batch: 380; loss: 1.2; acc: 0.7
Batch: 400; loss: 1.27; acc: 0.62
Batch: 420; loss: 1.35; acc: 0.56
Batch: 440; loss: 1.13; acc: 0.7
Batch: 460; loss: 1.35; acc: 0.62
Batch: 480; loss: 1.51; acc: 0.56
Batch: 500; loss: 1.24; acc: 0.64
Batch: 520; loss: 1.24; acc: 0.72
Batch: 540; loss: 1.19; acc: 0.67
Batch: 560; loss: 1.29; acc: 0.59
Batch: 580; loss: 1.16; acc: 0.72
Batch: 600; loss: 1.27; acc: 0.69
Batch: 620; loss: 1.3; acc: 0.67
Batch: 640; loss: 1.42; acc: 0.59
Batch: 660; loss: 1.21; acc: 0.66
Batch: 680; loss: 1.43; acc: 0.5
Batch: 700; loss: 1.33; acc: 0.61
Batch: 720; loss: 1.35; acc: 0.56
Batch: 740; loss: 1.45; acc: 0.5
Batch: 760; loss: 1.34; acc: 0.59
Batch: 780; loss: 1.11; acc: 0.72
Train Epoch over. train_loss: 1.3; train_accuracy: 0.64 

0.00015125414938665926
0.0001448856055503711
Batch: 0; loss: 1.36; acc: 0.58
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.19; acc: 0.72
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.38; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.58
Val Epoch over. val_loss: 1.2614880731910656; val_accuracy: 0.6688893312101911 

The current subspace-distance is: 0.0001448856055503711 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.35; acc: 0.61
Batch: 20; loss: 1.14; acc: 0.67
Batch: 40; loss: 1.32; acc: 0.67
Batch: 60; loss: 1.33; acc: 0.62
Batch: 80; loss: 1.33; acc: 0.64
Batch: 100; loss: 1.26; acc: 0.67
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.26; acc: 0.69
Batch: 160; loss: 1.28; acc: 0.59
Batch: 180; loss: 1.38; acc: 0.56
Batch: 200; loss: 1.29; acc: 0.66
Batch: 220; loss: 1.27; acc: 0.64
Batch: 240; loss: 1.22; acc: 0.72
Batch: 260; loss: 1.18; acc: 0.7
Batch: 280; loss: 1.26; acc: 0.69
Batch: 300; loss: 1.3; acc: 0.69
Batch: 320; loss: 1.32; acc: 0.58
Batch: 340; loss: 1.22; acc: 0.72
Batch: 360; loss: 1.36; acc: 0.61
Batch: 380; loss: 1.31; acc: 0.61
Batch: 400; loss: 1.33; acc: 0.59
Batch: 420; loss: 1.31; acc: 0.62
Batch: 440; loss: 1.35; acc: 0.64
Batch: 460; loss: 1.15; acc: 0.78
Batch: 480; loss: 1.39; acc: 0.55
Batch: 500; loss: 1.25; acc: 0.66
Batch: 520; loss: 1.34; acc: 0.62
Batch: 540; loss: 1.4; acc: 0.58
Batch: 560; loss: 1.2; acc: 0.73
Batch: 580; loss: 1.17; acc: 0.72
Batch: 600; loss: 1.43; acc: 0.48
Batch: 620; loss: 1.47; acc: 0.58
Batch: 640; loss: 1.4; acc: 0.59
Batch: 660; loss: 1.24; acc: 0.69
Batch: 680; loss: 1.36; acc: 0.56
Batch: 700; loss: 1.4; acc: 0.58
Batch: 720; loss: 1.19; acc: 0.67
Batch: 740; loss: 1.18; acc: 0.69
Batch: 760; loss: 1.35; acc: 0.53
Batch: 780; loss: 1.31; acc: 0.69
Train Epoch over. train_loss: 1.29; train_accuracy: 0.64 

0.00015532522229477763
0.0001492217998020351
Batch: 0; loss: 1.36; acc: 0.61
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 1.24; acc: 0.7
Batch: 80; loss: 1.19; acc: 0.67
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.38; acc: 0.61
Batch: 140; loss: 1.3; acc: 0.58
Val Epoch over. val_loss: 1.260118294673361; val_accuracy: 0.6671974522292994 

The current subspace-distance is: 0.0001492217998020351 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.69
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 1.35; acc: 0.56
Batch: 60; loss: 1.5; acc: 0.56
Batch: 80; loss: 1.26; acc: 0.66
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.27; acc: 0.64
Batch: 160; loss: 1.3; acc: 0.72
Batch: 180; loss: 1.3; acc: 0.61
Batch: 200; loss: 1.06; acc: 0.75
Batch: 220; loss: 1.27; acc: 0.64
Batch: 240; loss: 1.28; acc: 0.66
Batch: 260; loss: 1.46; acc: 0.56
Batch: 280; loss: 1.18; acc: 0.72
Batch: 300; loss: 1.15; acc: 0.75
Batch: 320; loss: 1.29; acc: 0.72
Batch: 340; loss: 1.37; acc: 0.61
Batch: 360; loss: 1.35; acc: 0.66
Batch: 380; loss: 1.49; acc: 0.59
Batch: 400; loss: 1.27; acc: 0.72
Batch: 420; loss: 1.23; acc: 0.69
Batch: 440; loss: 1.34; acc: 0.62
Batch: 460; loss: 1.26; acc: 0.62
Batch: 480; loss: 1.19; acc: 0.72
Batch: 500; loss: 1.29; acc: 0.69
Batch: 520; loss: 1.13; acc: 0.77
Batch: 540; loss: 1.23; acc: 0.66
Batch: 560; loss: 1.18; acc: 0.75
Batch: 580; loss: 1.29; acc: 0.61
Batch: 600; loss: 1.52; acc: 0.53
Batch: 620; loss: 1.48; acc: 0.5
Batch: 640; loss: 1.24; acc: 0.64
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.38; acc: 0.66
Batch: 700; loss: 1.11; acc: 0.78
Batch: 720; loss: 1.21; acc: 0.62
Batch: 740; loss: 1.33; acc: 0.58
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.15; acc: 0.73
Train Epoch over. train_loss: 1.29; train_accuracy: 0.64 

0.0001532897149445489
0.00014677544822916389
Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 1.55; acc: 0.48
Batch: 40; loss: 1.02; acc: 0.72
Batch: 60; loss: 1.22; acc: 0.7
Batch: 80; loss: 1.18; acc: 0.72
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 1.28; acc: 0.56
Val Epoch over. val_loss: 1.2280809996993678; val_accuracy: 0.6740644904458599 

The current subspace-distance is: 0.00014677544822916389 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.44; acc: 0.59
Batch: 20; loss: 1.12; acc: 0.7
Batch: 40; loss: 1.17; acc: 0.69
Batch: 60; loss: 1.26; acc: 0.59
Batch: 80; loss: 1.39; acc: 0.53
Batch: 100; loss: 1.17; acc: 0.66
Batch: 120; loss: 1.17; acc: 0.66
Batch: 140; loss: 1.15; acc: 0.66
Batch: 160; loss: 1.4; acc: 0.66
Batch: 180; loss: 1.06; acc: 0.78
Batch: 200; loss: 1.2; acc: 0.7
Batch: 220; loss: 1.22; acc: 0.69
Batch: 240; loss: 1.21; acc: 0.67
Batch: 260; loss: 1.19; acc: 0.64
Batch: 280; loss: 1.38; acc: 0.67
Batch: 300; loss: 1.29; acc: 0.64
Batch: 320; loss: 1.41; acc: 0.64
Batch: 340; loss: 1.23; acc: 0.66
Batch: 360; loss: 1.55; acc: 0.55
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.16; acc: 0.7
Batch: 420; loss: 1.29; acc: 0.61
Batch: 440; loss: 1.24; acc: 0.66
Batch: 460; loss: 1.14; acc: 0.69
Batch: 480; loss: 1.23; acc: 0.69
Batch: 500; loss: 1.33; acc: 0.69
Batch: 520; loss: 1.1; acc: 0.81
Batch: 540; loss: 1.39; acc: 0.58
Batch: 560; loss: 1.25; acc: 0.64
Batch: 580; loss: 1.36; acc: 0.69
Batch: 600; loss: 1.21; acc: 0.66
Batch: 620; loss: 1.36; acc: 0.53
Batch: 640; loss: 1.3; acc: 0.58
Batch: 660; loss: 1.49; acc: 0.5
Batch: 680; loss: 1.28; acc: 0.64
Batch: 700; loss: 1.07; acc: 0.78
Batch: 720; loss: 1.36; acc: 0.59
Batch: 740; loss: 1.23; acc: 0.67
Batch: 760; loss: 1.27; acc: 0.66
Batch: 780; loss: 1.28; acc: 0.62
Train Epoch over. train_loss: 1.28; train_accuracy: 0.65 

0.00015717446513008326
0.0001490925351390615
Batch: 0; loss: 1.33; acc: 0.59
Batch: 20; loss: 1.55; acc: 0.45
Batch: 40; loss: 1.02; acc: 0.72
Batch: 60; loss: 1.23; acc: 0.69
Batch: 80; loss: 1.17; acc: 0.72
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 1.28; acc: 0.56
Val Epoch over. val_loss: 1.2378838688704619; val_accuracy: 0.6709792993630573 

The current subspace-distance is: 0.0001490925351390615 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.3; acc: 0.58
Batch: 20; loss: 1.11; acc: 0.8
Batch: 40; loss: 1.33; acc: 0.66
Batch: 60; loss: 1.27; acc: 0.67
Batch: 80; loss: 1.28; acc: 0.66
Batch: 100; loss: 1.29; acc: 0.64
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 1.23; acc: 0.66
Batch: 160; loss: 1.29; acc: 0.59
Batch: 180; loss: 1.28; acc: 0.7
Batch: 200; loss: 1.35; acc: 0.52
Batch: 220; loss: 1.3; acc: 0.64
Batch: 240; loss: 1.26; acc: 0.59
Batch: 260; loss: 1.26; acc: 0.66
Batch: 280; loss: 1.38; acc: 0.61
Batch: 300; loss: 1.41; acc: 0.58
Batch: 320; loss: 1.24; acc: 0.67
Batch: 340; loss: 1.27; acc: 0.69
Batch: 360; loss: 1.38; acc: 0.62
Batch: 380; loss: 1.22; acc: 0.66
Batch: 400; loss: 1.18; acc: 0.73
Batch: 420; loss: 1.23; acc: 0.67
Batch: 440; loss: 1.32; acc: 0.66
Batch: 460; loss: 1.39; acc: 0.48
Batch: 480; loss: 1.22; acc: 0.64
Batch: 500; loss: 1.38; acc: 0.58
Batch: 520; loss: 1.19; acc: 0.67
Batch: 540; loss: 1.21; acc: 0.66
Batch: 560; loss: 1.35; acc: 0.62
Batch: 580; loss: 1.08; acc: 0.78
Batch: 600; loss: 1.38; acc: 0.62
Batch: 620; loss: 1.31; acc: 0.66
Batch: 640; loss: 1.26; acc: 0.58
Batch: 660; loss: 1.21; acc: 0.7
Batch: 680; loss: 1.25; acc: 0.67
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 1.27; acc: 0.62
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 1.14; acc: 0.69
Train Epoch over. train_loss: 1.27; train_accuracy: 0.65 

0.00015858262486290187
0.00015224765229504555
Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.01; acc: 0.73
Batch: 60; loss: 1.22; acc: 0.66
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.21; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.52
Val Epoch over. val_loss: 1.2296526648436383; val_accuracy: 0.6716759554140127 

The current subspace-distance is: 0.00015224765229504555 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.08; acc: 0.73
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.21; acc: 0.64
Batch: 80; loss: 1.19; acc: 0.66
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.67
Batch: 140; loss: 1.32; acc: 0.61
Batch: 160; loss: 1.18; acc: 0.69
Batch: 180; loss: 1.32; acc: 0.62
Batch: 200; loss: 1.25; acc: 0.64
Batch: 220; loss: 1.24; acc: 0.66
Batch: 240; loss: 1.25; acc: 0.59
Batch: 260; loss: 1.36; acc: 0.56
Batch: 280; loss: 1.29; acc: 0.64
Batch: 300; loss: 1.17; acc: 0.67
Batch: 320; loss: 1.17; acc: 0.77
Batch: 340; loss: 1.22; acc: 0.69
Batch: 360; loss: 1.22; acc: 0.67
Batch: 380; loss: 1.2; acc: 0.72
Batch: 400; loss: 1.23; acc: 0.64
Batch: 420; loss: 1.28; acc: 0.66
Batch: 440; loss: 1.34; acc: 0.64
Batch: 460; loss: 1.26; acc: 0.64
Batch: 480; loss: 1.27; acc: 0.62
Batch: 500; loss: 1.14; acc: 0.7
Batch: 520; loss: 1.39; acc: 0.52
Batch: 540; loss: 1.1; acc: 0.69
Batch: 560; loss: 1.27; acc: 0.61
Batch: 580; loss: 1.11; acc: 0.72
Batch: 600; loss: 1.3; acc: 0.67
Batch: 620; loss: 1.3; acc: 0.59
Batch: 640; loss: 1.43; acc: 0.55
Batch: 660; loss: 1.15; acc: 0.66
Batch: 680; loss: 1.27; acc: 0.61
Batch: 700; loss: 1.14; acc: 0.69
Batch: 720; loss: 1.43; acc: 0.56
Batch: 740; loss: 1.17; acc: 0.69
Batch: 760; loss: 1.36; acc: 0.59
Batch: 780; loss: 1.26; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.65 

0.0001597940135980025
0.00015510786033701152
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.54; acc: 0.48
Batch: 40; loss: 1.0; acc: 0.72
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 1.27; acc: 0.53
Val Epoch over. val_loss: 1.2280155374745654; val_accuracy: 0.6738654458598726 

The current subspace-distance is: 0.00015510786033701152 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.58
Batch: 20; loss: 1.25; acc: 0.66
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.27; acc: 0.58
Batch: 80; loss: 1.15; acc: 0.69
Batch: 100; loss: 1.31; acc: 0.67
Batch: 120; loss: 1.32; acc: 0.62
Batch: 140; loss: 1.19; acc: 0.66
Batch: 160; loss: 1.27; acc: 0.69
Batch: 180; loss: 1.32; acc: 0.64
Batch: 200; loss: 1.26; acc: 0.64
Batch: 220; loss: 1.25; acc: 0.7
Batch: 240; loss: 1.5; acc: 0.55
Batch: 260; loss: 1.12; acc: 0.77
Batch: 280; loss: 1.35; acc: 0.56
Batch: 300; loss: 1.16; acc: 0.69
Batch: 320; loss: 1.29; acc: 0.56
Batch: 340; loss: 1.18; acc: 0.72
Batch: 360; loss: 1.3; acc: 0.58
Batch: 380; loss: 1.27; acc: 0.69
Batch: 400; loss: 1.3; acc: 0.56
Batch: 420; loss: 1.22; acc: 0.66
Batch: 440; loss: 1.4; acc: 0.58
Batch: 460; loss: 1.24; acc: 0.67
Batch: 480; loss: 1.35; acc: 0.62
Batch: 500; loss: 1.3; acc: 0.58
Batch: 520; loss: 1.23; acc: 0.66
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.12; acc: 0.77
Batch: 580; loss: 1.35; acc: 0.67
Batch: 600; loss: 1.1; acc: 0.73
Batch: 620; loss: 1.31; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.61
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.12; acc: 0.7
Batch: 700; loss: 1.34; acc: 0.55
Batch: 720; loss: 1.33; acc: 0.58
Batch: 740; loss: 1.24; acc: 0.66
Batch: 760; loss: 1.34; acc: 0.61
Batch: 780; loss: 1.15; acc: 0.73
Train Epoch over. train_loss: 1.26; train_accuracy: 0.65 

0.00016547131235711277
0.00015899710706435144
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.55; acc: 0.5
Batch: 40; loss: 0.99; acc: 0.73
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.16; acc: 0.69
Batch: 100; loss: 1.2; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 1.27; acc: 0.59
Val Epoch over. val_loss: 1.2197976256631742; val_accuracy: 0.6724721337579618 

The current subspace-distance is: 0.00015899710706435144 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.62
Batch: 20; loss: 1.28; acc: 0.61
Batch: 40; loss: 1.27; acc: 0.61
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.4; acc: 0.55
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 1.23; acc: 0.61
Batch: 160; loss: 1.29; acc: 0.64
Batch: 180; loss: 1.44; acc: 0.58
Batch: 200; loss: 1.43; acc: 0.55
Batch: 220; loss: 1.25; acc: 0.73
Batch: 240; loss: 1.29; acc: 0.58
Batch: 260; loss: 1.21; acc: 0.66
Batch: 280; loss: 1.29; acc: 0.62
Batch: 300; loss: 1.27; acc: 0.69
Batch: 320; loss: 1.18; acc: 0.69
Batch: 340; loss: 1.39; acc: 0.61
Batch: 360; loss: 1.21; acc: 0.7
Batch: 380; loss: 1.38; acc: 0.59
Batch: 400; loss: 1.24; acc: 0.66
Batch: 420; loss: 1.24; acc: 0.59
Batch: 440; loss: 1.25; acc: 0.62
Batch: 460; loss: 1.14; acc: 0.66
Batch: 480; loss: 1.26; acc: 0.69
Batch: 500; loss: 1.19; acc: 0.64
Batch: 520; loss: 1.3; acc: 0.66
Batch: 540; loss: 1.27; acc: 0.58
Batch: 560; loss: 1.18; acc: 0.77
Batch: 580; loss: 1.38; acc: 0.62
Batch: 600; loss: 1.4; acc: 0.66
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.12; acc: 0.73
Batch: 660; loss: 1.12; acc: 0.75
Batch: 680; loss: 1.38; acc: 0.55
Batch: 700; loss: 1.38; acc: 0.69
Batch: 720; loss: 1.23; acc: 0.73
Batch: 740; loss: 1.14; acc: 0.7
Batch: 760; loss: 1.14; acc: 0.73
Batch: 780; loss: 1.32; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.65 

0.0001670897618168965
0.0001603302516741678
Batch: 0; loss: 1.28; acc: 0.61
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 0.98; acc: 0.75
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.19; acc: 0.69
Batch: 120; loss: 1.37; acc: 0.55
Batch: 140; loss: 1.25; acc: 0.59
Val Epoch over. val_loss: 1.2158118095367578; val_accuracy: 0.67078025477707 

The current subspace-distance is: 0.0001603302516741678 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.69
Batch: 20; loss: 1.24; acc: 0.67
Batch: 40; loss: 1.14; acc: 0.62
Batch: 60; loss: 1.26; acc: 0.72
Batch: 80; loss: 1.3; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.73
Batch: 140; loss: 1.13; acc: 0.69
Batch: 160; loss: 1.21; acc: 0.67
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.09; acc: 0.72
Batch: 220; loss: 1.34; acc: 0.62
Batch: 240; loss: 1.26; acc: 0.61
Batch: 260; loss: 1.29; acc: 0.67
Batch: 280; loss: 1.21; acc: 0.67
Batch: 300; loss: 1.38; acc: 0.56
Batch: 320; loss: 1.28; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.64
Batch: 360; loss: 1.38; acc: 0.59
Batch: 380; loss: 1.42; acc: 0.58
Batch: 400; loss: 1.35; acc: 0.62
Batch: 420; loss: 1.31; acc: 0.61
Batch: 440; loss: 1.3; acc: 0.67
Batch: 460; loss: 1.17; acc: 0.69
Batch: 480; loss: 1.3; acc: 0.62
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 1.01; acc: 0.75
Batch: 540; loss: 1.35; acc: 0.66
Batch: 560; loss: 1.34; acc: 0.56
Batch: 580; loss: 1.19; acc: 0.61
Batch: 600; loss: 1.24; acc: 0.66
Batch: 620; loss: 1.35; acc: 0.59
Batch: 640; loss: 1.08; acc: 0.72
Batch: 660; loss: 1.29; acc: 0.58
Batch: 680; loss: 1.32; acc: 0.67
Batch: 700; loss: 1.13; acc: 0.72
Batch: 720; loss: 1.26; acc: 0.64
Batch: 740; loss: 1.33; acc: 0.56
Batch: 760; loss: 1.21; acc: 0.64
Batch: 780; loss: 1.33; acc: 0.62
Train Epoch over. train_loss: 1.24; train_accuracy: 0.65 

0.00016765084001235664
0.00016046308155637234
Batch: 0; loss: 1.28; acc: 0.61
Batch: 20; loss: 1.52; acc: 0.53
Batch: 40; loss: 0.97; acc: 0.72
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.15; acc: 0.69
Batch: 100; loss: 1.18; acc: 0.67
Batch: 120; loss: 1.38; acc: 0.55
Batch: 140; loss: 1.24; acc: 0.56
Val Epoch over. val_loss: 1.2066826307849519; val_accuracy: 0.6712778662420382 

The current subspace-distance is: 0.00016046308155637234 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.69
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.23; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.59
Batch: 80; loss: 1.38; acc: 0.62
Batch: 100; loss: 1.16; acc: 0.72
Batch: 120; loss: 1.15; acc: 0.75
Batch: 140; loss: 1.12; acc: 0.73
Batch: 160; loss: 1.19; acc: 0.67
Batch: 180; loss: 1.2; acc: 0.69
Batch: 200; loss: 1.23; acc: 0.64
Batch: 220; loss: 1.11; acc: 0.81
Batch: 240; loss: 1.26; acc: 0.66
Batch: 260; loss: 1.17; acc: 0.67
Batch: 280; loss: 1.15; acc: 0.73
Batch: 300; loss: 1.25; acc: 0.62
Batch: 320; loss: 1.03; acc: 0.77
Batch: 340; loss: 1.3; acc: 0.58
Batch: 360; loss: 1.2; acc: 0.69
Batch: 380; loss: 1.2; acc: 0.72
Batch: 400; loss: 1.29; acc: 0.62
Batch: 420; loss: 1.29; acc: 0.66
Batch: 440; loss: 1.37; acc: 0.59
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 1.27; acc: 0.67
Batch: 500; loss: 1.25; acc: 0.61
Batch: 520; loss: 1.31; acc: 0.58
Batch: 540; loss: 1.27; acc: 0.69
Batch: 560; loss: 1.42; acc: 0.56
Batch: 580; loss: 1.32; acc: 0.61
Batch: 600; loss: 1.22; acc: 0.69
Batch: 620; loss: 1.19; acc: 0.72
Batch: 640; loss: 1.25; acc: 0.67
Batch: 660; loss: 1.09; acc: 0.67
Batch: 680; loss: 1.22; acc: 0.67
Batch: 700; loss: 1.26; acc: 0.66
Batch: 720; loss: 1.55; acc: 0.5
Batch: 740; loss: 1.19; acc: 0.67
Batch: 760; loss: 1.13; acc: 0.72
Batch: 780; loss: 1.24; acc: 0.67
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00017260875029023737
0.00016401527682319283
Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.51; acc: 0.53
Batch: 40; loss: 0.95; acc: 0.77
Batch: 60; loss: 1.2; acc: 0.67
Batch: 80; loss: 1.14; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 1.23; acc: 0.58
Val Epoch over. val_loss: 1.1986063024041; val_accuracy: 0.673765923566879 

The current subspace-distance is: 0.00016401527682319283 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.18; acc: 0.72
Batch: 40; loss: 1.27; acc: 0.62
Batch: 60; loss: 1.29; acc: 0.61
Batch: 80; loss: 1.14; acc: 0.69
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.39; acc: 0.55
Batch: 140; loss: 1.03; acc: 0.77
Batch: 160; loss: 1.29; acc: 0.62
Batch: 180; loss: 1.18; acc: 0.66
Batch: 200; loss: 1.19; acc: 0.69
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.23; acc: 0.61
Batch: 260; loss: 1.16; acc: 0.73
Batch: 280; loss: 1.31; acc: 0.62
Batch: 300; loss: 1.2; acc: 0.61
Batch: 320; loss: 1.11; acc: 0.72
Batch: 340; loss: 1.25; acc: 0.67
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.17; acc: 0.75
Batch: 400; loss: 1.47; acc: 0.61
Batch: 420; loss: 1.24; acc: 0.61
Batch: 440; loss: 1.23; acc: 0.62
Batch: 460; loss: 1.17; acc: 0.62
Batch: 480; loss: 1.2; acc: 0.67
Batch: 500; loss: 1.27; acc: 0.64
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 1.2; acc: 0.66
Batch: 560; loss: 1.33; acc: 0.66
Batch: 580; loss: 1.16; acc: 0.73
Batch: 600; loss: 1.07; acc: 0.72
Batch: 620; loss: 1.21; acc: 0.73
Batch: 640; loss: 1.33; acc: 0.62
Batch: 660; loss: 1.2; acc: 0.59
Batch: 680; loss: 1.31; acc: 0.69
Batch: 700; loss: 1.12; acc: 0.62
Batch: 720; loss: 1.08; acc: 0.69
Batch: 740; loss: 1.37; acc: 0.59
Batch: 760; loss: 1.21; acc: 0.64
Batch: 780; loss: 1.11; acc: 0.7
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.0001717606355668977
0.00016594293992966413
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.5; acc: 0.53
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 1.2; acc: 0.67
Batch: 80; loss: 1.14; acc: 0.69
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.22; acc: 0.61
Val Epoch over. val_loss: 1.1944271436162814; val_accuracy: 0.6812300955414012 

The current subspace-distance is: 0.00016594293992966413 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.29; acc: 0.66
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 1.27; acc: 0.62
Batch: 60; loss: 1.2; acc: 0.62
Batch: 80; loss: 1.21; acc: 0.59
Batch: 100; loss: 1.2; acc: 0.61
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 1.27; acc: 0.7
Batch: 160; loss: 1.25; acc: 0.7
Batch: 180; loss: 1.32; acc: 0.58
Batch: 200; loss: 1.46; acc: 0.62
Batch: 220; loss: 1.49; acc: 0.58
Batch: 240; loss: 1.31; acc: 0.62
Batch: 260; loss: 1.09; acc: 0.77
Batch: 280; loss: 1.15; acc: 0.64
Batch: 300; loss: 1.13; acc: 0.7
Batch: 320; loss: 1.25; acc: 0.64
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 1.37; acc: 0.64
Batch: 380; loss: 1.21; acc: 0.69
Batch: 400; loss: 1.2; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.66
Batch: 440; loss: 1.06; acc: 0.75
Batch: 460; loss: 1.18; acc: 0.7
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 1.09; acc: 0.77
Batch: 520; loss: 1.15; acc: 0.67
Batch: 540; loss: 1.13; acc: 0.72
Batch: 560; loss: 1.05; acc: 0.8
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.24; acc: 0.7
Batch: 620; loss: 1.26; acc: 0.61
Batch: 640; loss: 1.18; acc: 0.72
Batch: 660; loss: 1.3; acc: 0.56
Batch: 680; loss: 1.15; acc: 0.73
Batch: 700; loss: 1.24; acc: 0.66
Batch: 720; loss: 1.27; acc: 0.53
Batch: 740; loss: 1.31; acc: 0.66
Batch: 760; loss: 1.29; acc: 0.61
Batch: 780; loss: 1.16; acc: 0.73
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00017193006351590157
0.0001661485293880105
Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.51; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.77
Batch: 60; loss: 1.2; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.16; acc: 0.67
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 1.22; acc: 0.58
Val Epoch over. val_loss: 1.1948422273253179; val_accuracy: 0.673765923566879 

The current subspace-distance is: 0.0001661485293880105 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.09; acc: 0.75
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.35; acc: 0.62
Batch: 80; loss: 1.28; acc: 0.58
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.06; acc: 0.77
Batch: 180; loss: 1.3; acc: 0.58
Batch: 200; loss: 1.27; acc: 0.61
Batch: 220; loss: 1.23; acc: 0.66
Batch: 240; loss: 1.2; acc: 0.66
Batch: 260; loss: 1.1; acc: 0.72
Batch: 280; loss: 1.09; acc: 0.75
Batch: 300; loss: 1.27; acc: 0.58
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.2; acc: 0.67
Batch: 360; loss: 1.09; acc: 0.77
Batch: 380; loss: 1.3; acc: 0.64
Batch: 400; loss: 1.28; acc: 0.66
Batch: 420; loss: 1.33; acc: 0.59
Batch: 440; loss: 1.08; acc: 0.7
Batch: 460; loss: 1.41; acc: 0.56
Batch: 480; loss: 1.3; acc: 0.59
Batch: 500; loss: 1.31; acc: 0.67
Batch: 520; loss: 1.34; acc: 0.67
Batch: 540; loss: 1.26; acc: 0.64
Batch: 560; loss: 1.02; acc: 0.75
Batch: 580; loss: 1.09; acc: 0.73
Batch: 600; loss: 1.21; acc: 0.67
Batch: 620; loss: 1.41; acc: 0.58
Batch: 640; loss: 1.15; acc: 0.67
Batch: 660; loss: 1.21; acc: 0.69
Batch: 680; loss: 1.28; acc: 0.61
Batch: 700; loss: 1.29; acc: 0.66
Batch: 720; loss: 1.2; acc: 0.69
Batch: 740; loss: 1.33; acc: 0.59
Batch: 760; loss: 1.16; acc: 0.72
Batch: 780; loss: 1.11; acc: 0.66
Train Epoch over. train_loss: 1.23; train_accuracy: 0.66 

0.00017315488366875798
0.00016867599333636463
Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 0.94; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.7
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 1.2; acc: 0.61
Val Epoch over. val_loss: 1.1812564985007996; val_accuracy: 0.6823248407643312 

The current subspace-distance is: 0.00016867599333636463 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 1.32; acc: 0.56
Batch: 40; loss: 1.26; acc: 0.66
Batch: 60; loss: 1.24; acc: 0.75
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 1.26; acc: 0.64
Batch: 160; loss: 1.23; acc: 0.64
Batch: 180; loss: 1.33; acc: 0.59
Batch: 200; loss: 1.21; acc: 0.61
Batch: 220; loss: 1.23; acc: 0.67
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.29; acc: 0.62
Batch: 280; loss: 1.24; acc: 0.64
Batch: 300; loss: 1.22; acc: 0.62
Batch: 320; loss: 1.27; acc: 0.69
Batch: 340; loss: 1.02; acc: 0.75
Batch: 360; loss: 1.15; acc: 0.67
Batch: 380; loss: 1.36; acc: 0.62
Batch: 400; loss: 1.27; acc: 0.69
Batch: 420; loss: 1.27; acc: 0.64
Batch: 440; loss: 1.24; acc: 0.67
Batch: 460; loss: 1.22; acc: 0.66
Batch: 480; loss: 1.31; acc: 0.59
Batch: 500; loss: 1.17; acc: 0.69
Batch: 520; loss: 1.16; acc: 0.73
Batch: 540; loss: 1.32; acc: 0.59
Batch: 560; loss: 1.05; acc: 0.75
Batch: 580; loss: 1.25; acc: 0.67
Batch: 600; loss: 1.18; acc: 0.69
Batch: 620; loss: 1.29; acc: 0.62
Batch: 640; loss: 1.16; acc: 0.69
Batch: 660; loss: 1.41; acc: 0.59
Batch: 680; loss: 1.25; acc: 0.67
Batch: 700; loss: 1.18; acc: 0.72
Batch: 720; loss: 1.24; acc: 0.64
Batch: 740; loss: 1.28; acc: 0.62
Batch: 760; loss: 1.3; acc: 0.61
Batch: 780; loss: 1.12; acc: 0.7
Train Epoch over. train_loss: 1.22; train_accuracy: 0.66 

0.00017309232498519123
0.00016640183457639068
Batch: 0; loss: 1.25; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.64
Batch: 80; loss: 1.13; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 1.21; acc: 0.64
Val Epoch over. val_loss: 1.1871748847566592; val_accuracy: 0.6796377388535032 

The current subspace-distance is: 0.00016640183457639068 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.3; acc: 0.59
Batch: 40; loss: 1.32; acc: 0.58
Batch: 60; loss: 1.24; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.27; acc: 0.69
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.11; acc: 0.69
Batch: 160; loss: 1.22; acc: 0.62
Batch: 180; loss: 1.16; acc: 0.62
Batch: 200; loss: 1.33; acc: 0.59
Batch: 220; loss: 1.31; acc: 0.59
Batch: 240; loss: 1.3; acc: 0.66
Batch: 260; loss: 1.1; acc: 0.75
Batch: 280; loss: 1.16; acc: 0.66
Batch: 300; loss: 1.21; acc: 0.7
Batch: 320; loss: 1.23; acc: 0.69
Batch: 340; loss: 1.12; acc: 0.72
Batch: 360; loss: 1.36; acc: 0.56
Batch: 380; loss: 1.26; acc: 0.7
Batch: 400; loss: 1.31; acc: 0.58
Batch: 420; loss: 1.34; acc: 0.64
Batch: 440; loss: 1.2; acc: 0.69
Batch: 460; loss: 1.14; acc: 0.66
Batch: 480; loss: 1.27; acc: 0.75
Batch: 500; loss: 1.41; acc: 0.58
Batch: 520; loss: 1.26; acc: 0.66
Batch: 540; loss: 1.32; acc: 0.59
Batch: 560; loss: 1.29; acc: 0.62
Batch: 580; loss: 1.31; acc: 0.62
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.17; acc: 0.72
Batch: 640; loss: 1.17; acc: 0.7
Batch: 660; loss: 1.22; acc: 0.64
Batch: 680; loss: 1.14; acc: 0.62
Batch: 700; loss: 1.21; acc: 0.64
Batch: 720; loss: 1.28; acc: 0.64
Batch: 740; loss: 1.43; acc: 0.55
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.44; acc: 0.56
Train Epoch over. train_loss: 1.22; train_accuracy: 0.66 

0.0001736881968099624
0.0001690913923084736
Batch: 0; loss: 1.23; acc: 0.59
Batch: 20; loss: 1.49; acc: 0.55
Batch: 40; loss: 0.94; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.64
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.13; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.55
Batch: 140; loss: 1.19; acc: 0.64
Val Epoch over. val_loss: 1.1778474761422273; val_accuracy: 0.6821257961783439 

The current subspace-distance is: 0.0001690913923084736 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.75
Batch: 20; loss: 1.27; acc: 0.66
Batch: 40; loss: 1.33; acc: 0.58
Batch: 60; loss: 1.24; acc: 0.62
Batch: 80; loss: 1.28; acc: 0.58
Batch: 100; loss: 1.29; acc: 0.64
Batch: 120; loss: 1.35; acc: 0.62
Batch: 140; loss: 1.21; acc: 0.67
Batch: 160; loss: 1.26; acc: 0.7
Batch: 180; loss: 1.03; acc: 0.77
Batch: 200; loss: 1.1; acc: 0.8
Batch: 220; loss: 1.35; acc: 0.59
Batch: 240; loss: 1.35; acc: 0.56
Batch: 260; loss: 1.11; acc: 0.75
Batch: 280; loss: 1.04; acc: 0.78
Batch: 300; loss: 1.44; acc: 0.58
Batch: 320; loss: 1.28; acc: 0.67
Batch: 340; loss: 1.29; acc: 0.56
Batch: 360; loss: 1.17; acc: 0.69
Batch: 380; loss: 1.38; acc: 0.58
Batch: 400; loss: 1.05; acc: 0.75
Batch: 420; loss: 1.24; acc: 0.69
Batch: 440; loss: 1.28; acc: 0.59
Batch: 460; loss: 1.25; acc: 0.64
Batch: 480; loss: 1.13; acc: 0.69
Batch: 500; loss: 1.16; acc: 0.72
Batch: 520; loss: 1.19; acc: 0.7
Batch: 540; loss: 1.32; acc: 0.58
Batch: 560; loss: 1.33; acc: 0.55
Batch: 580; loss: 1.26; acc: 0.61
Batch: 600; loss: 1.06; acc: 0.78
Batch: 620; loss: 1.26; acc: 0.64
Batch: 640; loss: 1.13; acc: 0.69
Batch: 660; loss: 1.14; acc: 0.69
Batch: 680; loss: 1.13; acc: 0.66
Batch: 700; loss: 1.13; acc: 0.69
Batch: 720; loss: 1.23; acc: 0.67
Batch: 740; loss: 1.21; acc: 0.67
Batch: 760; loss: 1.14; acc: 0.67
Batch: 780; loss: 1.14; acc: 0.67
Train Epoch over. train_loss: 1.22; train_accuracy: 0.66 

0.00017455877969041467
0.00016910942213144153
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.64
Batch: 80; loss: 1.11; acc: 0.7
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.34; acc: 0.58
Batch: 140; loss: 1.17; acc: 0.66
Val Epoch over. val_loss: 1.1686538279436196; val_accuracy: 0.6831210191082803 

The current subspace-distance is: 0.00016910942213144153 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.32; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.73
Batch: 40; loss: 1.14; acc: 0.73
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.24; acc: 0.62
Batch: 100; loss: 1.16; acc: 0.69
Batch: 120; loss: 1.18; acc: 0.69
Batch: 140; loss: 1.3; acc: 0.56
Batch: 160; loss: 1.06; acc: 0.72
Batch: 180; loss: 1.14; acc: 0.69
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 1.34; acc: 0.59
Batch: 260; loss: 1.16; acc: 0.67
Batch: 280; loss: 1.45; acc: 0.52
Batch: 300; loss: 1.12; acc: 0.75
Batch: 320; loss: 1.1; acc: 0.72
Batch: 340; loss: 1.21; acc: 0.67
Batch: 360; loss: 1.28; acc: 0.66
Batch: 380; loss: 1.22; acc: 0.66
Batch: 400; loss: 1.28; acc: 0.66
Batch: 420; loss: 1.36; acc: 0.55
Batch: 440; loss: 1.21; acc: 0.66
Batch: 460; loss: 1.38; acc: 0.62
Batch: 480; loss: 1.08; acc: 0.72
Batch: 500; loss: 1.04; acc: 0.75
Batch: 520; loss: 1.18; acc: 0.72
Batch: 540; loss: 1.13; acc: 0.72
Batch: 560; loss: 1.52; acc: 0.53
Batch: 580; loss: 1.24; acc: 0.58
Batch: 600; loss: 1.34; acc: 0.58
Batch: 620; loss: 1.25; acc: 0.61
Batch: 640; loss: 1.12; acc: 0.75
Batch: 660; loss: 1.23; acc: 0.62
Batch: 680; loss: 1.33; acc: 0.64
Batch: 700; loss: 1.39; acc: 0.59
Batch: 720; loss: 1.2; acc: 0.64
Batch: 740; loss: 1.51; acc: 0.5
Batch: 760; loss: 1.22; acc: 0.67
Batch: 780; loss: 1.23; acc: 0.69
Train Epoch over. train_loss: 1.22; train_accuracy: 0.66 

0.0001776794233592227
0.00017080898396670818
Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.77
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 1.14; acc: 0.72
Batch: 120; loss: 1.35; acc: 0.58
Batch: 140; loss: 1.2; acc: 0.64
Val Epoch over. val_loss: 1.1801157851887356; val_accuracy: 0.6778463375796179 

The current subspace-distance is: 0.00017080898396670818 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.78
Batch: 20; loss: 1.27; acc: 0.62
Batch: 40; loss: 1.37; acc: 0.61
Batch: 60; loss: 1.24; acc: 0.62
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.26; acc: 0.62
Batch: 120; loss: 1.19; acc: 0.75
Batch: 140; loss: 1.34; acc: 0.58
Batch: 160; loss: 1.17; acc: 0.64
Batch: 180; loss: 1.24; acc: 0.64
Batch: 200; loss: 1.18; acc: 0.67
Batch: 220; loss: 1.22; acc: 0.66
Batch: 240; loss: 1.26; acc: 0.58
Batch: 260; loss: 1.11; acc: 0.72
Batch: 280; loss: 1.19; acc: 0.7
Batch: 300; loss: 1.17; acc: 0.7
Batch: 320; loss: 1.15; acc: 0.7
Batch: 340; loss: 1.27; acc: 0.67
Batch: 360; loss: 1.36; acc: 0.66
Batch: 380; loss: 1.0; acc: 0.81
Batch: 400; loss: 1.09; acc: 0.78
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 1.39; acc: 0.55
Batch: 460; loss: 1.21; acc: 0.7
Batch: 480; loss: 1.31; acc: 0.61
Batch: 500; loss: 1.18; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.7
Batch: 540; loss: 1.36; acc: 0.56
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.04; acc: 0.81
Batch: 600; loss: 1.38; acc: 0.58
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 1.16; acc: 0.7
Batch: 660; loss: 1.29; acc: 0.75
Batch: 680; loss: 1.1; acc: 0.8
Batch: 700; loss: 1.14; acc: 0.69
Batch: 720; loss: 1.31; acc: 0.61
Batch: 740; loss: 1.53; acc: 0.53
Batch: 760; loss: 1.08; acc: 0.7
Batch: 780; loss: 1.25; acc: 0.59
Train Epoch over. train_loss: 1.21; train_accuracy: 0.66 

0.00017779209883883595
0.00016931528807617724
Batch: 0; loss: 1.22; acc: 0.61
Batch: 20; loss: 1.48; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.66
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.58
Batch: 140; loss: 1.19; acc: 0.66
Val Epoch over. val_loss: 1.1706047027733675; val_accuracy: 0.6794386942675159 

The current subspace-distance is: 0.00016931528807617724 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.31; acc: 0.55
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 1.11; acc: 0.75
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.29; acc: 0.56
Batch: 100; loss: 1.12; acc: 0.77
Batch: 120; loss: 1.17; acc: 0.67
Batch: 140; loss: 1.24; acc: 0.66
Batch: 160; loss: 1.3; acc: 0.56
Batch: 180; loss: 1.09; acc: 0.7
Batch: 200; loss: 1.42; acc: 0.58
Batch: 220; loss: 1.22; acc: 0.66
Batch: 240; loss: 1.38; acc: 0.56
Batch: 260; loss: 1.2; acc: 0.67
Batch: 280; loss: 1.24; acc: 0.62
Batch: 300; loss: 1.21; acc: 0.64
Batch: 320; loss: 1.3; acc: 0.62
Batch: 340; loss: 1.29; acc: 0.56
Batch: 360; loss: 1.21; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.69
Batch: 400; loss: 1.22; acc: 0.67
Batch: 420; loss: 1.28; acc: 0.59
Batch: 440; loss: 1.13; acc: 0.7
Batch: 460; loss: 1.11; acc: 0.72
Batch: 480; loss: 1.33; acc: 0.64
Batch: 500; loss: 1.39; acc: 0.62
Batch: 520; loss: 1.17; acc: 0.66
Batch: 540; loss: 1.34; acc: 0.61
Batch: 560; loss: 1.26; acc: 0.64
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.06; acc: 0.72
Batch: 640; loss: 1.15; acc: 0.69
Batch: 660; loss: 1.21; acc: 0.66
Batch: 680; loss: 1.26; acc: 0.59
Batch: 700; loss: 1.46; acc: 0.5
Batch: 720; loss: 1.21; acc: 0.64
Batch: 740; loss: 1.21; acc: 0.64
Batch: 760; loss: 1.41; acc: 0.53
Batch: 780; loss: 1.11; acc: 0.72
Train Epoch over. train_loss: 1.21; train_accuracy: 0.66 

0.00017969678447116166
0.00017362990183755755
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.48; acc: 0.56
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.34; acc: 0.56
Batch: 140; loss: 1.18; acc: 0.62
Val Epoch over. val_loss: 1.1732415443013429; val_accuracy: 0.6805334394904459 

The current subspace-distance is: 0.00017362990183755755 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.95; acc: 0.83
Batch: 60; loss: 1.26; acc: 0.66
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 1.11; acc: 0.66
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 1.04; acc: 0.8
Batch: 160; loss: 1.35; acc: 0.56
Batch: 180; loss: 1.24; acc: 0.59
Batch: 200; loss: 1.19; acc: 0.69
Batch: 220; loss: 1.14; acc: 0.7
Batch: 240; loss: 1.31; acc: 0.62
Batch: 260; loss: 1.18; acc: 0.62
Batch: 280; loss: 1.05; acc: 0.77
Batch: 300; loss: 1.18; acc: 0.72
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.22; acc: 0.66
Batch: 360; loss: 1.34; acc: 0.59
Batch: 380; loss: 1.21; acc: 0.7
Batch: 400; loss: 1.12; acc: 0.73
Batch: 420; loss: 1.08; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.62
Batch: 460; loss: 1.2; acc: 0.77
Batch: 480; loss: 1.1; acc: 0.7
Batch: 500; loss: 1.1; acc: 0.69
Batch: 520; loss: 1.08; acc: 0.72
Batch: 540; loss: 1.17; acc: 0.61
Batch: 560; loss: 1.14; acc: 0.69
Batch: 580; loss: 1.34; acc: 0.59
Batch: 600; loss: 1.2; acc: 0.66
Batch: 620; loss: 1.04; acc: 0.72
Batch: 640; loss: 1.34; acc: 0.56
Batch: 660; loss: 1.23; acc: 0.69
Batch: 680; loss: 1.17; acc: 0.72
Batch: 700; loss: 1.31; acc: 0.66
Batch: 720; loss: 1.17; acc: 0.72
Batch: 740; loss: 1.16; acc: 0.78
Batch: 760; loss: 1.21; acc: 0.66
Batch: 780; loss: 1.19; acc: 0.69
Train Epoch over. train_loss: 1.21; train_accuracy: 0.66 

0.00017913531337399036
0.00017100581317208707
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 1.49; acc: 0.56
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.7
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 1.18; acc: 0.66
Val Epoch over. val_loss: 1.1704207753679554; val_accuracy: 0.681031050955414 

The current subspace-distance is: 0.00017100581317208707 

plots/subspace_training/table13slim/2020-01-29 16:12:31/N_14_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.07

The number of parameters is: 267152

The number of individual parameters is:

9
162
9
9
13
31356
13
13
26
90584
26
26
64
139776
64
64
4096
64
640
10
64
64

nonzero elements in E: 53430395
elements in E: 53430400
fraction nonzero: 0.999999906420315
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.16
Batch: 20; loss: 2.29; acc: 0.22
Batch: 40; loss: 2.1; acc: 0.19
Batch: 60; loss: 1.95; acc: 0.34
Batch: 80; loss: 1.89; acc: 0.39
Batch: 100; loss: 1.84; acc: 0.44
Batch: 120; loss: 1.76; acc: 0.52
Batch: 140; loss: 1.9; acc: 0.41
Batch: 160; loss: 1.69; acc: 0.52
Batch: 180; loss: 1.68; acc: 0.5
Batch: 200; loss: 1.63; acc: 0.52
Batch: 220; loss: 1.68; acc: 0.55
Batch: 240; loss: 1.66; acc: 0.61
Batch: 260; loss: 1.49; acc: 0.67
Batch: 280; loss: 1.67; acc: 0.48
Batch: 300; loss: 1.63; acc: 0.62
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.61; acc: 0.59
Batch: 360; loss: 1.43; acc: 0.61
Batch: 380; loss: 1.62; acc: 0.59
Batch: 400; loss: 1.49; acc: 0.61
Batch: 420; loss: 1.37; acc: 0.7
Batch: 440; loss: 1.4; acc: 0.73
Batch: 460; loss: 1.36; acc: 0.75
Batch: 480; loss: 1.44; acc: 0.66
Batch: 500; loss: 1.5; acc: 0.7
Batch: 520; loss: 1.3; acc: 0.75
Batch: 540; loss: 1.51; acc: 0.58
Batch: 560; loss: 1.29; acc: 0.73
Batch: 580; loss: 1.53; acc: 0.62
Batch: 600; loss: 1.26; acc: 0.72
Batch: 620; loss: 1.36; acc: 0.67
Batch: 640; loss: 1.25; acc: 0.81
Batch: 660; loss: 1.23; acc: 0.8
Batch: 680; loss: 1.32; acc: 0.7
Batch: 700; loss: 1.22; acc: 0.83
Batch: 720; loss: 1.26; acc: 0.72
Batch: 740; loss: 1.3; acc: 0.72
Batch: 760; loss: 1.33; acc: 0.72
Batch: 780; loss: 1.33; acc: 0.67
Train Epoch over. train_loss: 1.56; train_accuracy: 0.59 

6.304675480350852e-05
5.836920172441751e-05
Batch: 0; loss: 1.37; acc: 0.69
Batch: 20; loss: 1.4; acc: 0.59
Batch: 40; loss: 1.0; acc: 0.86
Batch: 60; loss: 1.21; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.29; acc: 0.83
Batch: 120; loss: 1.39; acc: 0.73
Batch: 140; loss: 1.1; acc: 0.88
Val Epoch over. val_loss: 1.248768815948705; val_accuracy: 0.7603503184713376 

The current subspace-distance is: 5.836920172441751e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.75
Batch: 20; loss: 1.28; acc: 0.77
Batch: 40; loss: 1.19; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.75
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.36; acc: 0.66
Batch: 120; loss: 1.22; acc: 0.77
Batch: 140; loss: 1.25; acc: 0.77
Batch: 160; loss: 1.26; acc: 0.73
Batch: 180; loss: 1.23; acc: 0.78
Batch: 200; loss: 1.15; acc: 0.83
Batch: 220; loss: 1.17; acc: 0.78
Batch: 240; loss: 1.2; acc: 0.8
Batch: 260; loss: 1.29; acc: 0.7
Batch: 280; loss: 1.29; acc: 0.67
Batch: 300; loss: 1.15; acc: 0.83
Batch: 320; loss: 1.12; acc: 0.77
Batch: 340; loss: 1.24; acc: 0.72
Batch: 360; loss: 1.32; acc: 0.62
Batch: 380; loss: 1.32; acc: 0.67
Batch: 400; loss: 1.15; acc: 0.83
Batch: 420; loss: 1.02; acc: 0.83
Batch: 440; loss: 1.3; acc: 0.67
Batch: 460; loss: 1.36; acc: 0.66
Batch: 480; loss: 1.22; acc: 0.8
Batch: 500; loss: 1.2; acc: 0.75
Batch: 520; loss: 1.16; acc: 0.81
Batch: 540; loss: 1.21; acc: 0.75
Batch: 560; loss: 1.17; acc: 0.75
Batch: 580; loss: 1.23; acc: 0.69
Batch: 600; loss: 1.21; acc: 0.75
Batch: 620; loss: 1.3; acc: 0.67
Batch: 640; loss: 1.17; acc: 0.77
Batch: 660; loss: 1.12; acc: 0.75
Batch: 680; loss: 1.1; acc: 0.75
Batch: 700; loss: 1.08; acc: 0.83
Batch: 720; loss: 1.21; acc: 0.67
Batch: 740; loss: 1.17; acc: 0.73
Batch: 760; loss: 1.2; acc: 0.73
Batch: 780; loss: 1.13; acc: 0.72
Train Epoch over. train_loss: 1.22; train_accuracy: 0.74 

8.27887051855214e-05
7.701662980252877e-05
Batch: 0; loss: 1.25; acc: 0.69
Batch: 20; loss: 1.31; acc: 0.69
Batch: 40; loss: 0.86; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.78
Batch: 80; loss: 0.94; acc: 0.94
Batch: 100; loss: 1.2; acc: 0.75
Batch: 120; loss: 1.28; acc: 0.73
Batch: 140; loss: 0.95; acc: 0.84
Val Epoch over. val_loss: 1.1277560773928454; val_accuracy: 0.7718949044585988 

The current subspace-distance is: 7.701662980252877e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.77
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 1.09; acc: 0.77
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 1.23; acc: 0.72
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.72
Batch: 140; loss: 1.26; acc: 0.72
Batch: 160; loss: 1.18; acc: 0.73
Batch: 180; loss: 1.2; acc: 0.64
Batch: 200; loss: 1.02; acc: 0.77
Batch: 220; loss: 1.11; acc: 0.8
Batch: 240; loss: 1.12; acc: 0.73
Batch: 260; loss: 1.17; acc: 0.78
Batch: 280; loss: 1.04; acc: 0.78
Batch: 300; loss: 1.12; acc: 0.72
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 1.09; acc: 0.81
Batch: 360; loss: 1.13; acc: 0.8
Batch: 380; loss: 1.02; acc: 0.8
Batch: 400; loss: 1.04; acc: 0.81
Batch: 420; loss: 1.16; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.78
Batch: 460; loss: 1.11; acc: 0.81
Batch: 480; loss: 1.04; acc: 0.78
Batch: 500; loss: 1.18; acc: 0.69
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.02; acc: 0.78
Batch: 560; loss: 1.03; acc: 0.77
Batch: 580; loss: 1.12; acc: 0.8
Batch: 600; loss: 1.08; acc: 0.78
Batch: 620; loss: 1.18; acc: 0.73
Batch: 640; loss: 1.13; acc: 0.75
Batch: 660; loss: 0.94; acc: 0.84
Batch: 680; loss: 1.07; acc: 0.77
Batch: 700; loss: 1.07; acc: 0.8
Batch: 720; loss: 0.97; acc: 0.81
Batch: 740; loss: 1.21; acc: 0.72
Batch: 760; loss: 1.09; acc: 0.77
Batch: 780; loss: 1.05; acc: 0.78
Train Epoch over. train_loss: 1.11; train_accuracy: 0.75 

9.856910037342459e-05
9.065785707207397e-05
Batch: 0; loss: 1.15; acc: 0.69
Batch: 20; loss: 1.26; acc: 0.69
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.95; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.91
Batch: 100; loss: 1.09; acc: 0.81
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 0.85; acc: 0.88
Val Epoch over. val_loss: 1.0218995346385202; val_accuracy: 0.791202229299363 

The current subspace-distance is: 9.065785707207397e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.67
Batch: 20; loss: 1.05; acc: 0.73
Batch: 40; loss: 1.02; acc: 0.81
Batch: 60; loss: 1.01; acc: 0.81
Batch: 80; loss: 1.03; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.69
Batch: 120; loss: 0.95; acc: 0.81
Batch: 140; loss: 1.13; acc: 0.73
Batch: 160; loss: 1.09; acc: 0.73
Batch: 180; loss: 1.04; acc: 0.7
Batch: 200; loss: 1.0; acc: 0.8
Batch: 220; loss: 0.99; acc: 0.83
Batch: 240; loss: 1.19; acc: 0.64
Batch: 260; loss: 1.02; acc: 0.77
Batch: 280; loss: 1.05; acc: 0.72
Batch: 300; loss: 1.03; acc: 0.7
Batch: 320; loss: 0.89; acc: 0.81
Batch: 340; loss: 1.13; acc: 0.7
Batch: 360; loss: 1.08; acc: 0.77
Batch: 380; loss: 0.98; acc: 0.81
Batch: 400; loss: 1.0; acc: 0.78
Batch: 420; loss: 0.89; acc: 0.84
Batch: 440; loss: 1.04; acc: 0.75
Batch: 460; loss: 0.94; acc: 0.83
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 1.17; acc: 0.75
Batch: 520; loss: 0.92; acc: 0.77
Batch: 540; loss: 1.02; acc: 0.7
Batch: 560; loss: 1.01; acc: 0.73
Batch: 580; loss: 1.02; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.8
Batch: 620; loss: 1.12; acc: 0.7
Batch: 640; loss: 1.02; acc: 0.75
Batch: 660; loss: 1.16; acc: 0.67
Batch: 680; loss: 0.92; acc: 0.75
Batch: 700; loss: 1.02; acc: 0.7
Batch: 720; loss: 0.95; acc: 0.77
Batch: 740; loss: 1.03; acc: 0.78
Batch: 760; loss: 1.07; acc: 0.8
Batch: 780; loss: 0.98; acc: 0.75
Train Epoch over. train_loss: 1.02; train_accuracy: 0.76 

0.00011507001909194514
0.00010904153896262869
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.21; acc: 0.72
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.84; acc: 0.89
Batch: 80; loss: 0.78; acc: 0.91
Batch: 100; loss: 1.0; acc: 0.8
Batch: 120; loss: 1.13; acc: 0.69
Batch: 140; loss: 0.77; acc: 0.91
Val Epoch over. val_loss: 0.9304976770832281; val_accuracy: 0.8002587579617835 

The current subspace-distance is: 0.00010904153896262869 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 1.09; acc: 0.72
Batch: 60; loss: 1.05; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 0.94; acc: 0.84
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 1.08; acc: 0.7
Batch: 160; loss: 0.99; acc: 0.78
Batch: 180; loss: 1.05; acc: 0.73
Batch: 200; loss: 0.94; acc: 0.78
Batch: 220; loss: 1.05; acc: 0.75
Batch: 240; loss: 0.99; acc: 0.81
Batch: 260; loss: 0.93; acc: 0.8
Batch: 280; loss: 0.85; acc: 0.86
Batch: 300; loss: 0.96; acc: 0.75
Batch: 320; loss: 1.01; acc: 0.72
Batch: 340; loss: 1.17; acc: 0.7
Batch: 360; loss: 0.89; acc: 0.78
Batch: 380; loss: 1.17; acc: 0.72
Batch: 400; loss: 0.97; acc: 0.83
Batch: 420; loss: 1.09; acc: 0.75
Batch: 440; loss: 0.94; acc: 0.72
Batch: 460; loss: 1.06; acc: 0.7
Batch: 480; loss: 1.03; acc: 0.7
Batch: 500; loss: 1.06; acc: 0.73
Batch: 520; loss: 0.9; acc: 0.81
Batch: 540; loss: 1.07; acc: 0.77
Batch: 560; loss: 0.91; acc: 0.8
Batch: 580; loss: 0.98; acc: 0.75
Batch: 600; loss: 0.93; acc: 0.78
Batch: 620; loss: 0.84; acc: 0.83
Batch: 640; loss: 0.79; acc: 0.88
Batch: 660; loss: 0.93; acc: 0.81
Batch: 680; loss: 0.73; acc: 0.89
Batch: 700; loss: 0.91; acc: 0.78
Batch: 720; loss: 0.93; acc: 0.84
Batch: 740; loss: 0.83; acc: 0.8
Batch: 760; loss: 0.87; acc: 0.78
Batch: 780; loss: 0.92; acc: 0.78
Train Epoch over. train_loss: 0.95; train_accuracy: 0.78 

0.00013056944590061903
0.00012637212057597935
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.73
Batch: 40; loss: 0.63; acc: 0.88
Batch: 60; loss: 0.79; acc: 0.88
Batch: 80; loss: 0.72; acc: 0.88
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 0.72; acc: 0.91
Val Epoch over. val_loss: 0.8680500577969156; val_accuracy: 0.8192675159235668 

The current subspace-distance is: 0.00012637212057597935 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.85; acc: 0.86
Batch: 80; loss: 0.75; acc: 0.89
Batch: 100; loss: 0.83; acc: 0.84
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.93; acc: 0.75
Batch: 160; loss: 0.95; acc: 0.81
Batch: 180; loss: 1.08; acc: 0.67
Batch: 200; loss: 0.87; acc: 0.8
Batch: 220; loss: 0.91; acc: 0.75
Batch: 240; loss: 0.87; acc: 0.88
Batch: 260; loss: 0.93; acc: 0.8
Batch: 280; loss: 0.92; acc: 0.81
Batch: 300; loss: 0.89; acc: 0.78
Batch: 320; loss: 0.86; acc: 0.86
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.88; acc: 0.81
Batch: 380; loss: 0.83; acc: 0.88
Batch: 400; loss: 0.77; acc: 0.84
Batch: 420; loss: 0.89; acc: 0.8
Batch: 440; loss: 0.87; acc: 0.8
Batch: 460; loss: 0.92; acc: 0.75
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.95; acc: 0.78
Batch: 520; loss: 0.93; acc: 0.8
Batch: 540; loss: 0.78; acc: 0.88
Batch: 560; loss: 0.89; acc: 0.81
Batch: 580; loss: 0.86; acc: 0.81
Batch: 600; loss: 0.88; acc: 0.83
Batch: 620; loss: 0.99; acc: 0.75
Batch: 640; loss: 0.71; acc: 0.88
Batch: 660; loss: 0.95; acc: 0.75
Batch: 680; loss: 0.91; acc: 0.83
Batch: 700; loss: 0.86; acc: 0.83
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.79; acc: 0.89
Batch: 760; loss: 0.87; acc: 0.86
Batch: 780; loss: 0.94; acc: 0.81
Train Epoch over. train_loss: 0.9; train_accuracy: 0.79 

0.00014476127398665994
0.00013916433090344071
Batch: 0; loss: 0.93; acc: 0.7
Batch: 20; loss: 1.1; acc: 0.73
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.92
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.66; acc: 0.89
Val Epoch over. val_loss: 0.8178740903070778; val_accuracy: 0.8249402866242038 

The current subspace-distance is: 0.00013916433090344071 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.88; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.88
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 0.77; acc: 0.86
Batch: 140; loss: 0.87; acc: 0.83
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 0.69; acc: 0.91
Batch: 200; loss: 0.84; acc: 0.83
Batch: 220; loss: 0.88; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.86
Batch: 260; loss: 0.91; acc: 0.77
Batch: 280; loss: 0.92; acc: 0.77
Batch: 300; loss: 0.79; acc: 0.84
Batch: 320; loss: 0.84; acc: 0.84
Batch: 340; loss: 0.9; acc: 0.77
Batch: 360; loss: 0.81; acc: 0.84
Batch: 380; loss: 0.81; acc: 0.86
Batch: 400; loss: 0.95; acc: 0.72
Batch: 420; loss: 0.92; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.83
Batch: 460; loss: 0.96; acc: 0.73
Batch: 480; loss: 0.9; acc: 0.73
Batch: 500; loss: 0.97; acc: 0.75
Batch: 520; loss: 0.85; acc: 0.78
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 0.83; acc: 0.88
Batch: 600; loss: 0.84; acc: 0.75
Batch: 620; loss: 0.93; acc: 0.75
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.92
Batch: 680; loss: 0.85; acc: 0.83
Batch: 700; loss: 0.96; acc: 0.7
Batch: 720; loss: 0.87; acc: 0.77
Batch: 740; loss: 0.9; acc: 0.75
Batch: 760; loss: 0.77; acc: 0.88
Batch: 780; loss: 0.87; acc: 0.8
Train Epoch over. train_loss: 0.86; train_accuracy: 0.8 

0.00015348981833085418
0.0001489801361458376
Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 1.05; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.94
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.99; acc: 0.77
Batch: 140; loss: 0.63; acc: 0.89
Val Epoch over. val_loss: 0.7828874081183391; val_accuracy: 0.8269307324840764 

The current subspace-distance is: 0.0001489801361458376 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.84
Batch: 80; loss: 0.82; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.94; acc: 0.75
Batch: 160; loss: 0.94; acc: 0.8
Batch: 180; loss: 0.81; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.81
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.78; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.89; acc: 0.78
Batch: 300; loss: 0.97; acc: 0.81
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.7; acc: 0.88
Batch: 360; loss: 0.82; acc: 0.83
Batch: 380; loss: 0.8; acc: 0.86
Batch: 400; loss: 0.82; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.81
Batch: 440; loss: 0.88; acc: 0.78
Batch: 460; loss: 0.81; acc: 0.75
Batch: 480; loss: 1.06; acc: 0.7
Batch: 500; loss: 0.75; acc: 0.84
Batch: 520; loss: 0.79; acc: 0.8
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.69; acc: 0.88
Batch: 580; loss: 0.89; acc: 0.77
Batch: 600; loss: 0.87; acc: 0.78
Batch: 620; loss: 0.69; acc: 0.88
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.91; acc: 0.77
Batch: 700; loss: 0.86; acc: 0.81
Batch: 720; loss: 0.85; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.89
Batch: 780; loss: 0.83; acc: 0.77
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

0.00016500426863785833
0.00015891484508756548
Batch: 0; loss: 0.9; acc: 0.72
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.92
Batch: 100; loss: 0.84; acc: 0.81
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 0.61; acc: 0.86
Val Epoch over. val_loss: 0.7610751931454726; val_accuracy: 0.8310111464968153 

The current subspace-distance is: 0.00015891484508756548 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 0.83; acc: 0.81
Batch: 40; loss: 0.87; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.83; acc: 0.83
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.88; acc: 0.77
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.8; acc: 0.78
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.79; acc: 0.83
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.81; acc: 0.78
Batch: 340; loss: 0.88; acc: 0.8
Batch: 360; loss: 0.86; acc: 0.81
Batch: 380; loss: 0.97; acc: 0.75
Batch: 400; loss: 0.74; acc: 0.84
Batch: 420; loss: 0.84; acc: 0.78
Batch: 440; loss: 0.85; acc: 0.81
Batch: 460; loss: 0.84; acc: 0.78
Batch: 480; loss: 0.83; acc: 0.8
Batch: 500; loss: 0.74; acc: 0.84
Batch: 520; loss: 0.81; acc: 0.83
Batch: 540; loss: 0.9; acc: 0.77
Batch: 560; loss: 0.87; acc: 0.73
Batch: 580; loss: 0.86; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.78
Batch: 620; loss: 0.94; acc: 0.8
Batch: 640; loss: 0.71; acc: 0.88
Batch: 660; loss: 0.78; acc: 0.8
Batch: 680; loss: 0.9; acc: 0.73
Batch: 700; loss: 0.74; acc: 0.81
Batch: 720; loss: 0.74; acc: 0.81
Batch: 740; loss: 0.92; acc: 0.7
Batch: 760; loss: 0.87; acc: 0.8
Batch: 780; loss: 0.87; acc: 0.78
Train Epoch over. train_loss: 0.81; train_accuracy: 0.81 

0.00017417561321053654
0.00016730594506952912
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.94
Batch: 100; loss: 0.83; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.6; acc: 0.89
Val Epoch over. val_loss: 0.7428436412173471; val_accuracy: 0.8397691082802548 

The current subspace-distance is: 0.00016730594506952912 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.72; acc: 0.86
Batch: 80; loss: 0.81; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.84; acc: 0.78
Batch: 160; loss: 0.8; acc: 0.84
Batch: 180; loss: 0.78; acc: 0.8
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 1.16; acc: 0.69
Batch: 260; loss: 0.79; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.84
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.74; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.88
Batch: 360; loss: 0.74; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.82; acc: 0.8
Batch: 420; loss: 1.14; acc: 0.67
Batch: 440; loss: 1.08; acc: 0.66
Batch: 460; loss: 0.59; acc: 0.94
Batch: 480; loss: 0.78; acc: 0.84
Batch: 500; loss: 0.88; acc: 0.8
Batch: 520; loss: 0.67; acc: 0.88
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.88; acc: 0.8
Batch: 580; loss: 0.8; acc: 0.78
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 0.72; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.84
Batch: 680; loss: 0.81; acc: 0.81
Batch: 700; loss: 0.95; acc: 0.73
Batch: 720; loss: 0.78; acc: 0.75
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.88
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.79; train_accuracy: 0.81 

0.00018017187539953738
0.0001738502032821998
Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 0.97; acc: 0.75
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 0.57; acc: 0.86
Val Epoch over. val_loss: 0.7218233679130579; val_accuracy: 0.8403662420382165 

The current subspace-distance is: 0.0001738502032821998 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.92; acc: 0.7
Batch: 20; loss: 0.73; acc: 0.88
Batch: 40; loss: 0.85; acc: 0.77
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.83; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 0.75; acc: 0.88
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.93; acc: 0.72
Batch: 180; loss: 0.64; acc: 0.88
Batch: 200; loss: 0.78; acc: 0.8
Batch: 220; loss: 1.02; acc: 0.75
Batch: 240; loss: 0.64; acc: 0.89
Batch: 260; loss: 0.69; acc: 0.86
Batch: 280; loss: 0.84; acc: 0.77
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 0.77; acc: 0.86
Batch: 340; loss: 0.88; acc: 0.78
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.69; acc: 0.86
Batch: 400; loss: 0.8; acc: 0.81
Batch: 420; loss: 0.97; acc: 0.78
Batch: 440; loss: 0.89; acc: 0.75
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.86; acc: 0.8
Batch: 500; loss: 0.61; acc: 0.91
Batch: 520; loss: 0.75; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.83; acc: 0.75
Batch: 640; loss: 0.83; acc: 0.8
Batch: 660; loss: 0.61; acc: 0.89
Batch: 680; loss: 0.84; acc: 0.78
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.75; acc: 0.84
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.72; acc: 0.88
Batch: 780; loss: 0.76; acc: 0.84
Train Epoch over. train_loss: 0.79; train_accuracy: 0.81 

0.00018576998263597488
0.00017711598775349557
Batch: 0; loss: 0.86; acc: 0.69
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.94
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.88
Val Epoch over. val_loss: 0.7265817335077153; val_accuracy: 0.8425557324840764 

The current subspace-distance is: 0.00017711598775349557 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.74; acc: 0.86
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.88
Batch: 140; loss: 0.96; acc: 0.77
Batch: 160; loss: 0.78; acc: 0.83
Batch: 180; loss: 0.89; acc: 0.77
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.76; acc: 0.84
Batch: 260; loss: 0.92; acc: 0.72
Batch: 280; loss: 0.78; acc: 0.8
Batch: 300; loss: 0.83; acc: 0.73
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.88; acc: 0.75
Batch: 380; loss: 0.76; acc: 0.84
Batch: 400; loss: 0.81; acc: 0.8
Batch: 420; loss: 0.99; acc: 0.73
Batch: 440; loss: 0.81; acc: 0.83
Batch: 460; loss: 0.87; acc: 0.83
Batch: 480; loss: 0.83; acc: 0.75
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.87; acc: 0.81
Batch: 560; loss: 0.81; acc: 0.77
Batch: 580; loss: 0.87; acc: 0.8
Batch: 600; loss: 0.67; acc: 0.91
Batch: 620; loss: 0.76; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.88
Batch: 660; loss: 0.81; acc: 0.77
Batch: 680; loss: 0.94; acc: 0.72
Batch: 700; loss: 0.78; acc: 0.78
Batch: 720; loss: 0.92; acc: 0.73
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.76; acc: 0.8
Train Epoch over. train_loss: 0.78; train_accuracy: 0.81 

0.00018557738803792745
0.00018055290274787694
Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.94
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.88
Val Epoch over. val_loss: 0.7071399504591704; val_accuracy: 0.8454418789808917 

The current subspace-distance is: 0.00018055290274787694 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.92; acc: 0.72
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.77; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.84; acc: 0.77
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.83; acc: 0.8
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.89
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.77; acc: 0.81
Batch: 340; loss: 0.78; acc: 0.77
Batch: 360; loss: 0.85; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.92; acc: 0.77
Batch: 440; loss: 0.85; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.81; acc: 0.8
Batch: 500; loss: 0.76; acc: 0.86
Batch: 520; loss: 0.76; acc: 0.88
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.98; acc: 0.73
Batch: 620; loss: 0.98; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.78; acc: 0.78
Batch: 680; loss: 0.7; acc: 0.92
Batch: 700; loss: 0.69; acc: 0.88
Batch: 720; loss: 0.85; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.72
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.8; acc: 0.78
Train Epoch over. train_loss: 0.78; train_accuracy: 0.81 

0.00018818605167325586
0.00018250974244438112
Batch: 0; loss: 0.85; acc: 0.67
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 0.46; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.91
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.56; acc: 0.88
Val Epoch over. val_loss: 0.7121518147978813; val_accuracy: 0.8434514331210191 

The current subspace-distance is: 0.00018250974244438112 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.82; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.85; acc: 0.81
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.83; acc: 0.75
Batch: 220; loss: 0.74; acc: 0.86
Batch: 240; loss: 0.8; acc: 0.75
Batch: 260; loss: 0.84; acc: 0.77
Batch: 280; loss: 0.78; acc: 0.8
Batch: 300; loss: 0.6; acc: 0.91
Batch: 320; loss: 0.8; acc: 0.86
Batch: 340; loss: 0.7; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.78; acc: 0.81
Batch: 400; loss: 0.85; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 0.91; acc: 0.77
Batch: 460; loss: 0.91; acc: 0.69
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.8; acc: 0.83
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.74; acc: 0.84
Batch: 560; loss: 0.79; acc: 0.8
Batch: 580; loss: 0.9; acc: 0.75
Batch: 600; loss: 0.85; acc: 0.78
Batch: 620; loss: 0.72; acc: 0.88
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.82; acc: 0.77
Batch: 680; loss: 0.92; acc: 0.7
Batch: 700; loss: 1.02; acc: 0.75
Batch: 720; loss: 0.72; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 0.73; acc: 0.77
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.78; train_accuracy: 0.81 

0.00018997315783053637
0.00018364668358117342
Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 0.96; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.91
Val Epoch over. val_loss: 0.7098362778022791; val_accuracy: 0.8406648089171974 

The current subspace-distance is: 0.00018364668358117342 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.98; acc: 0.72
Batch: 160; loss: 0.78; acc: 0.83
Batch: 180; loss: 0.7; acc: 0.86
Batch: 200; loss: 0.79; acc: 0.89
Batch: 220; loss: 0.66; acc: 0.89
Batch: 240; loss: 0.86; acc: 0.75
Batch: 260; loss: 0.76; acc: 0.86
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.88
Batch: 340; loss: 0.77; acc: 0.8
Batch: 360; loss: 0.86; acc: 0.73
Batch: 380; loss: 0.95; acc: 0.75
Batch: 400; loss: 0.67; acc: 0.86
Batch: 420; loss: 0.85; acc: 0.78
Batch: 440; loss: 0.7; acc: 0.89
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.74; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.91
Batch: 520; loss: 0.82; acc: 0.8
Batch: 540; loss: 0.87; acc: 0.78
Batch: 560; loss: 0.65; acc: 0.89
Batch: 580; loss: 0.82; acc: 0.8
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.65; acc: 0.92
Batch: 680; loss: 0.82; acc: 0.77
Batch: 700; loss: 0.81; acc: 0.83
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.8; acc: 0.77
Batch: 780; loss: 0.94; acc: 0.75
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00019313974189572036
0.00018447496404405683
Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 0.95; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.92
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.89
Val Epoch over. val_loss: 0.7087016591600551; val_accuracy: 0.8413614649681529 

The current subspace-distance is: 0.00018447496404405683 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.71; acc: 0.8
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.98; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.86; acc: 0.72
Batch: 160; loss: 0.77; acc: 0.78
Batch: 180; loss: 0.9; acc: 0.75
Batch: 200; loss: 0.76; acc: 0.78
Batch: 220; loss: 0.84; acc: 0.78
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.89; acc: 0.7
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.89
Batch: 320; loss: 0.65; acc: 0.88
Batch: 340; loss: 0.84; acc: 0.75
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.83; acc: 0.78
Batch: 420; loss: 0.77; acc: 0.83
Batch: 440; loss: 0.79; acc: 0.83
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 0.85; acc: 0.78
Batch: 500; loss: 0.98; acc: 0.72
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.8; acc: 0.81
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.77; acc: 0.78
Batch: 680; loss: 0.91; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.86
Batch: 720; loss: 0.75; acc: 0.84
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 1.02; acc: 0.67
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00019544387760106474
0.00018794366042129695
Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.77
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.91
Val Epoch over. val_loss: 0.7038841541785343; val_accuracy: 0.8442476114649682 

The current subspace-distance is: 0.00018794366042129695 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.81; acc: 0.8
Batch: 100; loss: 0.91; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 0.81; acc: 0.81
Batch: 160; loss: 0.94; acc: 0.78
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.78; acc: 0.75
Batch: 220; loss: 0.92; acc: 0.77
Batch: 240; loss: 0.82; acc: 0.73
Batch: 260; loss: 0.8; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.86
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.84; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.76; acc: 0.86
Batch: 420; loss: 0.83; acc: 0.8
Batch: 440; loss: 0.83; acc: 0.81
Batch: 460; loss: 0.88; acc: 0.77
Batch: 480; loss: 0.85; acc: 0.8
Batch: 500; loss: 0.83; acc: 0.81
Batch: 520; loss: 0.9; acc: 0.75
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.95; acc: 0.75
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.77; acc: 0.81
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.66; acc: 0.88
Batch: 720; loss: 0.99; acc: 0.67
Batch: 740; loss: 0.76; acc: 0.84
Batch: 760; loss: 0.76; acc: 0.83
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00019715602684300393
0.00018977189029101282
Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.89
Val Epoch over. val_loss: 0.7065544194856267; val_accuracy: 0.841062898089172 

The current subspace-distance is: 0.00018977189029101282 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 0.95; acc: 0.77
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.82; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.83
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.82; acc: 0.8
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.79; acc: 0.75
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.86; acc: 0.77
Batch: 400; loss: 0.83; acc: 0.73
Batch: 420; loss: 0.76; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.81
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.9; acc: 0.77
Batch: 520; loss: 0.6; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.86; acc: 0.73
Batch: 580; loss: 0.77; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.92; acc: 0.73
Batch: 640; loss: 0.84; acc: 0.8
Batch: 660; loss: 0.8; acc: 0.75
Batch: 680; loss: 0.55; acc: 0.92
Batch: 700; loss: 0.7; acc: 0.84
Batch: 720; loss: 0.81; acc: 0.73
Batch: 740; loss: 0.7; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.78
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00019676220836117864
0.00018997160077560693
Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.89
Val Epoch over. val_loss: 0.6940668705542377; val_accuracy: 0.8422571656050956 

The current subspace-distance is: 0.00018997160077560693 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.88; acc: 0.73
Batch: 60; loss: 0.87; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 0.83; acc: 0.77
Batch: 160; loss: 0.72; acc: 0.86
Batch: 180; loss: 0.88; acc: 0.78
Batch: 200; loss: 0.67; acc: 0.89
Batch: 220; loss: 0.76; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.83
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 1.0; acc: 0.69
Batch: 300; loss: 0.82; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.83; acc: 0.84
Batch: 400; loss: 0.87; acc: 0.77
Batch: 420; loss: 0.8; acc: 0.73
Batch: 440; loss: 0.7; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.74; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.89
Batch: 600; loss: 0.63; acc: 0.88
Batch: 620; loss: 0.65; acc: 0.86
Batch: 640; loss: 0.87; acc: 0.73
Batch: 660; loss: 0.73; acc: 0.86
Batch: 680; loss: 0.74; acc: 0.83
Batch: 700; loss: 0.66; acc: 0.86
Batch: 720; loss: 0.84; acc: 0.8
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.96; acc: 0.73
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00019682632409967482
0.0001919084315886721
Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.91
Val Epoch over. val_loss: 0.6881404957573884; val_accuracy: 0.8446457006369427 

The current subspace-distance is: 0.0001919084315886721 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.8; acc: 0.81
Batch: 180; loss: 0.76; acc: 0.8
Batch: 200; loss: 0.88; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.8; acc: 0.78
Batch: 260; loss: 0.77; acc: 0.8
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.93; acc: 0.69
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.84; acc: 0.8
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.6; acc: 0.86
Batch: 420; loss: 0.88; acc: 0.69
Batch: 440; loss: 0.91; acc: 0.75
Batch: 460; loss: 0.67; acc: 0.91
Batch: 480; loss: 0.71; acc: 0.89
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.77; acc: 0.83
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.79; acc: 0.77
Batch: 580; loss: 0.93; acc: 0.73
Batch: 600; loss: 0.83; acc: 0.81
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.85; acc: 0.72
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.8; acc: 0.8
Batch: 720; loss: 0.81; acc: 0.75
Batch: 740; loss: 0.85; acc: 0.81
Batch: 760; loss: 0.82; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00020180363208055496
0.0001954570907400921
Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.94; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.91
Val Epoch over. val_loss: 0.6895953682577534; val_accuracy: 0.8425557324840764 

The current subspace-distance is: 0.0001954570907400921 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.74; acc: 0.86
Batch: 180; loss: 0.74; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.88
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.78; acc: 0.73
Batch: 280; loss: 0.81; acc: 0.81
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.88
Batch: 340; loss: 0.86; acc: 0.8
Batch: 360; loss: 0.68; acc: 0.86
Batch: 380; loss: 0.8; acc: 0.78
Batch: 400; loss: 0.86; acc: 0.77
Batch: 420; loss: 0.62; acc: 0.86
Batch: 440; loss: 0.66; acc: 0.89
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.86
Batch: 520; loss: 0.8; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.81; acc: 0.83
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.85; acc: 0.81
Batch: 640; loss: 0.8; acc: 0.83
Batch: 660; loss: 0.71; acc: 0.89
Batch: 680; loss: 0.69; acc: 0.86
Batch: 700; loss: 0.73; acc: 0.83
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.87; acc: 0.75
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00019961911311838776
0.00019101027282886207
Batch: 0; loss: 0.83; acc: 0.7
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.6878454626365832; val_accuracy: 0.8469347133757962 

The current subspace-distance is: 0.00019101027282886207 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.96; acc: 0.69
Batch: 60; loss: 0.72; acc: 0.89
Batch: 80; loss: 0.83; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.77
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.77; acc: 0.84
Batch: 160; loss: 0.85; acc: 0.77
Batch: 180; loss: 0.59; acc: 0.92
Batch: 200; loss: 0.94; acc: 0.75
Batch: 220; loss: 0.87; acc: 0.77
Batch: 240; loss: 0.89; acc: 0.75
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.8; acc: 0.8
Batch: 300; loss: 0.82; acc: 0.77
Batch: 320; loss: 0.73; acc: 0.84
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.9; acc: 0.8
Batch: 400; loss: 0.65; acc: 0.89
Batch: 420; loss: 0.82; acc: 0.81
Batch: 440; loss: 0.97; acc: 0.7
Batch: 460; loss: 0.84; acc: 0.7
Batch: 480; loss: 0.92; acc: 0.72
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.81; acc: 0.83
Batch: 580; loss: 0.68; acc: 0.86
Batch: 600; loss: 0.77; acc: 0.81
Batch: 620; loss: 0.72; acc: 0.81
Batch: 640; loss: 0.73; acc: 0.81
Batch: 660; loss: 0.92; acc: 0.72
Batch: 680; loss: 0.97; acc: 0.7
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.71; acc: 0.84
Batch: 740; loss: 0.6; acc: 0.91
Batch: 760; loss: 0.84; acc: 0.8
Batch: 780; loss: 0.81; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00020001652592327446
0.00019118961063213646
Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 0.95; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.89
Val Epoch over. val_loss: 0.6994233828061706; val_accuracy: 0.8405652866242038 

The current subspace-distance is: 0.00019118961063213646 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.91
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 0.78; acc: 0.78
Batch: 80; loss: 0.9; acc: 0.75
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.88; acc: 0.75
Batch: 180; loss: 0.88; acc: 0.77
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.82; acc: 0.8
Batch: 240; loss: 0.85; acc: 0.72
Batch: 260; loss: 0.89; acc: 0.73
Batch: 280; loss: 0.75; acc: 0.86
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.75; acc: 0.8
Batch: 380; loss: 0.88; acc: 0.75
Batch: 400; loss: 0.83; acc: 0.83
Batch: 420; loss: 0.91; acc: 0.75
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.58; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.77; acc: 0.81
Batch: 580; loss: 0.89; acc: 0.72
Batch: 600; loss: 0.88; acc: 0.73
Batch: 620; loss: 0.69; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.84
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.8; acc: 0.77
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.82; acc: 0.75
Batch: 740; loss: 0.75; acc: 0.83
Batch: 760; loss: 0.98; acc: 0.67
Batch: 780; loss: 0.83; acc: 0.84
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.0002008816081797704
0.00019086514657828957
Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6791027533780237; val_accuracy: 0.8457404458598726 

The current subspace-distance is: 0.00019086514657828957 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 0.91; acc: 0.78
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.96; acc: 0.77
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.89; acc: 0.73
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.96; acc: 0.73
Batch: 400; loss: 0.9; acc: 0.78
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.7; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.84
Batch: 480; loss: 0.98; acc: 0.77
Batch: 500; loss: 0.76; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.88; acc: 0.77
Batch: 560; loss: 0.82; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.98; acc: 0.78
Batch: 640; loss: 0.79; acc: 0.81
Batch: 660; loss: 0.8; acc: 0.78
Batch: 680; loss: 0.97; acc: 0.67
Batch: 700; loss: 0.73; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.84; acc: 0.77
Batch: 780; loss: 0.85; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00020127823518123478
0.0001955283514689654
Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.95; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6936880388077656; val_accuracy: 0.841859076433121 

The current subspace-distance is: 0.0001955283514689654 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.72
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.83
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.81; acc: 0.8
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.84; acc: 0.8
Batch: 220; loss: 0.84; acc: 0.83
Batch: 240; loss: 0.92; acc: 0.7
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 0.83; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.81
Batch: 320; loss: 0.85; acc: 0.78
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.94; acc: 0.67
Batch: 380; loss: 0.75; acc: 0.81
Batch: 400; loss: 0.77; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.81; acc: 0.75
Batch: 460; loss: 0.66; acc: 0.89
Batch: 480; loss: 0.87; acc: 0.73
Batch: 500; loss: 0.81; acc: 0.77
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.76; acc: 0.8
Batch: 560; loss: 0.66; acc: 0.88
Batch: 580; loss: 0.87; acc: 0.75
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.78; acc: 0.81
Batch: 660; loss: 0.85; acc: 0.73
Batch: 680; loss: 0.68; acc: 0.89
Batch: 700; loss: 0.67; acc: 0.89
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.0002015985082834959
0.00019373623945284635
Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6892792912805157; val_accuracy: 0.8424562101910829 

The current subspace-distance is: 0.00019373623945284635 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.88; acc: 0.77
Batch: 60; loss: 0.84; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.9; acc: 0.69
Batch: 160; loss: 0.65; acc: 0.94
Batch: 180; loss: 0.63; acc: 0.91
Batch: 200; loss: 0.75; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.75
Batch: 240; loss: 0.75; acc: 0.81
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.78
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.84; acc: 0.84
Batch: 360; loss: 0.78; acc: 0.78
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.83; acc: 0.78
Batch: 420; loss: 0.64; acc: 0.89
Batch: 440; loss: 0.69; acc: 0.84
Batch: 460; loss: 0.94; acc: 0.73
Batch: 480; loss: 0.68; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.81
Batch: 520; loss: 0.91; acc: 0.8
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.73; acc: 0.75
Batch: 620; loss: 0.72; acc: 0.84
Batch: 640; loss: 0.82; acc: 0.84
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.8; acc: 0.81
Batch: 700; loss: 0.74; acc: 0.8
Batch: 720; loss: 0.59; acc: 0.88
Batch: 740; loss: 0.83; acc: 0.84
Batch: 760; loss: 0.63; acc: 0.84
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00020429558935575187
0.00019434797286521643
Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6936975671986866; val_accuracy: 0.8425557324840764 

The current subspace-distance is: 0.00019434797286521643 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.92; acc: 0.73
Batch: 80; loss: 0.79; acc: 0.73
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.84
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.83; acc: 0.78
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.77; acc: 0.78
Batch: 240; loss: 0.73; acc: 0.83
Batch: 260; loss: 0.64; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.7; acc: 0.83
Batch: 340; loss: 0.86; acc: 0.75
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.78; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.84; acc: 0.75
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.83; acc: 0.75
Batch: 500; loss: 0.83; acc: 0.81
Batch: 520; loss: 0.85; acc: 0.75
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.79; acc: 0.8
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.8; acc: 0.81
Batch: 620; loss: 0.87; acc: 0.7
Batch: 640; loss: 0.71; acc: 0.86
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.84; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.89
Batch: 720; loss: 0.93; acc: 0.78
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.000205193180590868
0.00019760556460823864
Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.93; acc: 0.75
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.686991555295932; val_accuracy: 0.8444466560509554 

The current subspace-distance is: 0.00019760556460823864 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.87; acc: 0.73
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.88; acc: 0.72
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 0.9; acc: 0.83
Batch: 140; loss: 0.74; acc: 0.81
Batch: 160; loss: 0.85; acc: 0.8
Batch: 180; loss: 0.66; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.79; acc: 0.81
Batch: 260; loss: 0.72; acc: 0.77
Batch: 280; loss: 0.89; acc: 0.73
Batch: 300; loss: 0.78; acc: 0.73
Batch: 320; loss: 0.78; acc: 0.81
Batch: 340; loss: 0.81; acc: 0.83
Batch: 360; loss: 0.81; acc: 0.8
Batch: 380; loss: 0.8; acc: 0.77
Batch: 400; loss: 0.67; acc: 0.8
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.6; acc: 0.91
Batch: 480; loss: 0.82; acc: 0.81
Batch: 500; loss: 0.68; acc: 0.84
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.71; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.92
Batch: 600; loss: 0.77; acc: 0.84
Batch: 620; loss: 0.77; acc: 0.81
Batch: 640; loss: 0.68; acc: 0.83
Batch: 660; loss: 0.86; acc: 0.73
Batch: 680; loss: 0.7; acc: 0.83
Batch: 700; loss: 0.75; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.77
Batch: 740; loss: 0.73; acc: 0.86
Batch: 760; loss: 0.87; acc: 0.78
Batch: 780; loss: 0.68; acc: 0.88
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00020584547019097954
0.00019624180276878178
Batch: 0; loss: 0.83; acc: 0.72
Batch: 20; loss: 0.92; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.684213734356461; val_accuracy: 0.845640923566879 

The current subspace-distance is: 0.00019624180276878178 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.94; acc: 0.69
Batch: 160; loss: 0.65; acc: 0.84
Batch: 180; loss: 0.9; acc: 0.77
Batch: 200; loss: 0.84; acc: 0.77
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.88; acc: 0.72
Batch: 280; loss: 0.7; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.84; acc: 0.77
Batch: 360; loss: 0.91; acc: 0.75
Batch: 380; loss: 0.62; acc: 0.89
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.8
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.73; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.69; acc: 0.88
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.74; acc: 0.81
Batch: 640; loss: 0.82; acc: 0.78
Batch: 660; loss: 0.83; acc: 0.8
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.78
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 1.03; acc: 0.72
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00020169021445326507
0.0001955935440491885
Batch: 0; loss: 0.83; acc: 0.72
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.6786121934842152; val_accuracy: 0.8403662420382165 

The current subspace-distance is: 0.0001955935440491885 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.77
Batch: 160; loss: 0.77; acc: 0.73
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.86
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.75; acc: 0.86
Batch: 360; loss: 0.94; acc: 0.73
Batch: 380; loss: 0.73; acc: 0.83
Batch: 400; loss: 0.72; acc: 0.86
Batch: 420; loss: 1.0; acc: 0.69
Batch: 440; loss: 0.91; acc: 0.75
Batch: 460; loss: 0.92; acc: 0.75
Batch: 480; loss: 0.83; acc: 0.75
Batch: 500; loss: 0.72; acc: 0.78
Batch: 520; loss: 0.58; acc: 0.91
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.76; acc: 0.84
Batch: 600; loss: 0.75; acc: 0.8
Batch: 620; loss: 0.87; acc: 0.73
Batch: 640; loss: 0.66; acc: 0.89
Batch: 660; loss: 0.85; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.86
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.81; acc: 0.75
Batch: 740; loss: 0.57; acc: 0.94
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00020468528964556754
0.00019694186630658805
Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.91
Val Epoch over. val_loss: 0.6842611714912827; val_accuracy: 0.8414609872611465 

The current subspace-distance is: 0.00019694186630658805 

plots/subspace_training/table13slim/2020-01-29 16:12:31/N_14_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.07

The number of parameters is: 267152

The number of individual parameters is:

9
162
9
9
13
31356
13
13
26
90584
26
26
64
139776
64
64
4096
64
640
10
64
64

nonzero elements in E: 80145593
elements in E: 80145600
fraction nonzero: 0.9999999126589607
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.5; acc: 0.06
Batch: 20; loss: 2.18; acc: 0.28
Batch: 40; loss: 1.99; acc: 0.34
Batch: 60; loss: 1.82; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.58
Batch: 100; loss: 1.64; acc: 0.61
Batch: 120; loss: 1.73; acc: 0.53
Batch: 140; loss: 1.62; acc: 0.55
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.65; acc: 0.52
Batch: 200; loss: 1.57; acc: 0.67
Batch: 220; loss: 1.48; acc: 0.7
Batch: 240; loss: 1.47; acc: 0.62
Batch: 260; loss: 1.62; acc: 0.53
Batch: 280; loss: 1.37; acc: 0.66
Batch: 300; loss: 1.33; acc: 0.78
Batch: 320; loss: 1.36; acc: 0.66
Batch: 340; loss: 1.39; acc: 0.7
Batch: 360; loss: 1.37; acc: 0.7
Batch: 380; loss: 1.26; acc: 0.72
Batch: 400; loss: 1.4; acc: 0.62
Batch: 420; loss: 1.48; acc: 0.59
Batch: 440; loss: 1.26; acc: 0.67
Batch: 460; loss: 1.35; acc: 0.72
Batch: 480; loss: 1.2; acc: 0.8
Batch: 500; loss: 1.27; acc: 0.69
Batch: 520; loss: 1.33; acc: 0.73
Batch: 540; loss: 1.22; acc: 0.81
Batch: 560; loss: 1.25; acc: 0.73
Batch: 580; loss: 1.22; acc: 0.78
Batch: 600; loss: 1.21; acc: 0.77
Batch: 620; loss: 1.25; acc: 0.73
Batch: 640; loss: 1.24; acc: 0.69
Batch: 660; loss: 1.15; acc: 0.78
Batch: 680; loss: 1.12; acc: 0.83
Batch: 700; loss: 1.2; acc: 0.75
Batch: 720; loss: 1.22; acc: 0.75
Batch: 740; loss: 1.15; acc: 0.75
Batch: 760; loss: 1.21; acc: 0.72
Batch: 780; loss: 1.13; acc: 0.78
Train Epoch over. train_loss: 1.42; train_accuracy: 0.65 

6.333997589536011e-05
5.845537089044228e-05
Batch: 0; loss: 1.28; acc: 0.69
Batch: 20; loss: 1.31; acc: 0.67
Batch: 40; loss: 0.87; acc: 0.89
Batch: 60; loss: 1.16; acc: 0.8
Batch: 80; loss: 0.99; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.78
Batch: 120; loss: 1.25; acc: 0.73
Batch: 140; loss: 1.01; acc: 0.88
Val Epoch over. val_loss: 1.1455928034083858; val_accuracy: 0.7710987261146497 

The current subspace-distance is: 5.845537089044228e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.75
Batch: 20; loss: 1.19; acc: 0.73
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.2; acc: 0.81
Batch: 80; loss: 1.24; acc: 0.66
Batch: 100; loss: 1.19; acc: 0.75
Batch: 120; loss: 1.2; acc: 0.7
Batch: 140; loss: 1.17; acc: 0.73
Batch: 160; loss: 1.16; acc: 0.72
Batch: 180; loss: 0.94; acc: 0.84
Batch: 200; loss: 1.1; acc: 0.81
Batch: 220; loss: 1.14; acc: 0.8
Batch: 240; loss: 1.17; acc: 0.73
Batch: 260; loss: 1.01; acc: 0.8
Batch: 280; loss: 1.14; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.77
Batch: 320; loss: 1.27; acc: 0.64
Batch: 340; loss: 1.01; acc: 0.8
Batch: 360; loss: 1.03; acc: 0.77
Batch: 380; loss: 1.11; acc: 0.73
Batch: 400; loss: 1.02; acc: 0.8
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.97; acc: 0.84
Batch: 460; loss: 1.08; acc: 0.78
Batch: 480; loss: 0.98; acc: 0.8
Batch: 500; loss: 0.96; acc: 0.8
Batch: 520; loss: 0.91; acc: 0.8
Batch: 540; loss: 1.21; acc: 0.75
Batch: 560; loss: 1.11; acc: 0.77
Batch: 580; loss: 1.08; acc: 0.69
Batch: 600; loss: 0.92; acc: 0.86
Batch: 620; loss: 1.0; acc: 0.75
Batch: 640; loss: 0.94; acc: 0.84
Batch: 660; loss: 0.98; acc: 0.86
Batch: 680; loss: 0.95; acc: 0.84
Batch: 700; loss: 1.02; acc: 0.78
Batch: 720; loss: 0.92; acc: 0.86
Batch: 740; loss: 1.02; acc: 0.75
Batch: 760; loss: 1.12; acc: 0.75
Batch: 780; loss: 0.91; acc: 0.83
Train Epoch over. train_loss: 1.08; train_accuracy: 0.77 

8.826716657495126e-05
8.379846985917538e-05
Batch: 0; loss: 1.05; acc: 0.8
Batch: 20; loss: 1.21; acc: 0.64
Batch: 40; loss: 0.64; acc: 0.92
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.92
Batch: 100; loss: 0.95; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.77
Batch: 140; loss: 0.8; acc: 0.88
Val Epoch over. val_loss: 0.9442244650452001; val_accuracy: 0.8168789808917197 

The current subspace-distance is: 8.379846985917538e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.75
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.88; acc: 0.77
Batch: 60; loss: 1.0; acc: 0.83
Batch: 80; loss: 1.01; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.84
Batch: 120; loss: 0.91; acc: 0.81
Batch: 140; loss: 0.81; acc: 0.89
Batch: 160; loss: 1.1; acc: 0.73
Batch: 180; loss: 1.01; acc: 0.8
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 1.06; acc: 0.75
Batch: 240; loss: 0.91; acc: 0.84
Batch: 260; loss: 0.92; acc: 0.77
Batch: 280; loss: 0.89; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.75
Batch: 320; loss: 1.03; acc: 0.73
Batch: 340; loss: 0.88; acc: 0.91
Batch: 360; loss: 0.94; acc: 0.77
Batch: 380; loss: 0.9; acc: 0.84
Batch: 400; loss: 1.03; acc: 0.77
Batch: 420; loss: 1.04; acc: 0.72
Batch: 440; loss: 1.13; acc: 0.73
Batch: 460; loss: 0.94; acc: 0.78
Batch: 480; loss: 1.02; acc: 0.75
Batch: 500; loss: 1.01; acc: 0.81
Batch: 520; loss: 0.97; acc: 0.73
Batch: 540; loss: 0.86; acc: 0.88
Batch: 560; loss: 0.87; acc: 0.86
Batch: 580; loss: 0.95; acc: 0.77
Batch: 600; loss: 0.82; acc: 0.84
Batch: 620; loss: 0.93; acc: 0.78
Batch: 640; loss: 0.85; acc: 0.86
Batch: 660; loss: 1.05; acc: 0.84
Batch: 680; loss: 0.95; acc: 0.8
Batch: 700; loss: 0.97; acc: 0.72
Batch: 720; loss: 0.96; acc: 0.81
Batch: 740; loss: 0.86; acc: 0.86
Batch: 760; loss: 0.94; acc: 0.77
Batch: 780; loss: 0.88; acc: 0.86
Train Epoch over. train_loss: 0.95; train_accuracy: 0.8 

0.00010443605424370617
9.899641008814797e-05
Batch: 0; loss: 0.94; acc: 0.83
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 0.54; acc: 0.94
Batch: 60; loss: 0.85; acc: 0.81
Batch: 80; loss: 0.62; acc: 0.94
Batch: 100; loss: 0.82; acc: 0.88
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.7; acc: 0.91
Val Epoch over. val_loss: 0.8416788141438916; val_accuracy: 0.8316082802547771 

The current subspace-distance is: 9.899641008814797e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.88
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.89
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 0.85; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.92; acc: 0.84
Batch: 180; loss: 0.84; acc: 0.81
Batch: 200; loss: 0.9; acc: 0.8
Batch: 220; loss: 0.78; acc: 0.84
Batch: 240; loss: 0.77; acc: 0.84
Batch: 260; loss: 0.95; acc: 0.81
Batch: 280; loss: 0.88; acc: 0.84
Batch: 300; loss: 0.8; acc: 0.89
Batch: 320; loss: 0.76; acc: 0.88
Batch: 340; loss: 0.91; acc: 0.81
Batch: 360; loss: 0.78; acc: 0.89
Batch: 380; loss: 0.86; acc: 0.83
Batch: 400; loss: 0.9; acc: 0.77
Batch: 420; loss: 0.86; acc: 0.8
Batch: 440; loss: 0.94; acc: 0.73
Batch: 460; loss: 0.83; acc: 0.83
Batch: 480; loss: 0.97; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.95
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.84; acc: 0.88
Batch: 560; loss: 0.76; acc: 0.89
Batch: 580; loss: 0.98; acc: 0.78
Batch: 600; loss: 0.97; acc: 0.77
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.79; acc: 0.83
Batch: 680; loss: 0.88; acc: 0.75
Batch: 700; loss: 0.81; acc: 0.84
Batch: 720; loss: 0.79; acc: 0.88
Batch: 740; loss: 0.87; acc: 0.83
Batch: 760; loss: 0.9; acc: 0.8
Batch: 780; loss: 0.78; acc: 0.86
Train Epoch over. train_loss: 0.86; train_accuracy: 0.81 

0.00011938291572732851
0.00011378672934370115
Batch: 0; loss: 0.84; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.7
Batch: 40; loss: 0.48; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.92
Batch: 100; loss: 0.75; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.63; acc: 0.88
Val Epoch over. val_loss: 0.7688698356698274; val_accuracy: 0.8462380573248408 

The current subspace-distance is: 0.00011378672934370115 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.8
Batch: 40; loss: 0.87; acc: 0.8
Batch: 60; loss: 0.74; acc: 0.86
Batch: 80; loss: 0.75; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.86
Batch: 140; loss: 0.77; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.88
Batch: 200; loss: 0.88; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.88
Batch: 240; loss: 0.8; acc: 0.83
Batch: 260; loss: 1.09; acc: 0.69
Batch: 280; loss: 0.86; acc: 0.83
Batch: 300; loss: 0.9; acc: 0.75
Batch: 320; loss: 0.81; acc: 0.86
Batch: 340; loss: 0.68; acc: 0.92
Batch: 360; loss: 0.84; acc: 0.77
Batch: 380; loss: 0.73; acc: 0.88
Batch: 400; loss: 0.93; acc: 0.81
Batch: 420; loss: 0.83; acc: 0.83
Batch: 440; loss: 0.72; acc: 0.86
Batch: 460; loss: 0.83; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.75; acc: 0.86
Batch: 540; loss: 0.67; acc: 0.86
Batch: 560; loss: 0.87; acc: 0.75
Batch: 580; loss: 0.84; acc: 0.84
Batch: 600; loss: 0.76; acc: 0.88
Batch: 620; loss: 0.81; acc: 0.84
Batch: 640; loss: 0.86; acc: 0.8
Batch: 660; loss: 0.91; acc: 0.78
Batch: 680; loss: 0.82; acc: 0.77
Batch: 700; loss: 0.82; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.91
Batch: 740; loss: 0.87; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.89
Batch: 780; loss: 0.86; acc: 0.8
Train Epoch over. train_loss: 0.8; train_accuracy: 0.82 

0.00013469827536027879
0.00012782913108821958
Batch: 0; loss: 0.75; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.95
Batch: 100; loss: 0.68; acc: 0.91
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.56; acc: 0.91
Val Epoch over. val_loss: 0.7018987731948779; val_accuracy: 0.8575835987261147 

The current subspace-distance is: 0.00012782913108821958 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.83
Batch: 20; loss: 0.81; acc: 0.84
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.92
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.92
Batch: 140; loss: 0.67; acc: 0.91
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.98; acc: 0.77
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.65; acc: 0.88
Batch: 340; loss: 0.77; acc: 0.84
Batch: 360; loss: 0.51; acc: 0.95
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.7; acc: 0.8
Batch: 460; loss: 0.74; acc: 0.83
Batch: 480; loss: 0.79; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.89; acc: 0.75
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.83; acc: 0.81
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.82; acc: 0.81
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.85; acc: 0.77
Batch: 700; loss: 0.57; acc: 0.92
Batch: 720; loss: 0.91; acc: 0.78
Batch: 740; loss: 0.72; acc: 0.84
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.00014579150592908263
0.00013897342432755977
Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.97
Batch: 100; loss: 0.61; acc: 0.92
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.89
Val Epoch over. val_loss: 0.6502356310938574; val_accuracy: 0.8656449044585988 

The current subspace-distance is: 0.00013897342432755977 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.83
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.6; acc: 0.91
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.75; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.75
Batch: 280; loss: 0.81; acc: 0.84
Batch: 300; loss: 0.84; acc: 0.8
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.9; acc: 0.77
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.82; acc: 0.73
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.74; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.89
Batch: 560; loss: 0.66; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.91
Batch: 620; loss: 0.69; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.63; acc: 0.89
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.0001553940528538078
0.00014896249922458082
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.81; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.6071395657624409; val_accuracy: 0.8711186305732485 

The current subspace-distance is: 0.00014896249922458082 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.91
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.85; acc: 0.81
Batch: 160; loss: 0.76; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.94
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.71; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.81; acc: 0.75
Batch: 540; loss: 0.53; acc: 0.89
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.89
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.71; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.86; acc: 0.8
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00016858725575730205
0.00016245189181063324
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.5629201328298848; val_accuracy: 0.8782842356687898 

The current subspace-distance is: 0.00016245189181063324 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.63; acc: 0.91
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.83; acc: 0.8
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.94
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.78; acc: 0.8
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.6; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.54; acc: 0.92
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.95
Batch: 740; loss: 0.63; acc: 0.88
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.61; train_accuracy: 0.86 

0.00017689692322164774
0.0001701734436210245
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.7
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.38; acc: 0.94
Val Epoch over. val_loss: 0.5383681812483794; val_accuracy: 0.8844546178343949 

The current subspace-distance is: 0.0001701734436210245 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.82; acc: 0.8
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.94
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.8
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.75; acc: 0.86
Batch: 660; loss: 0.69; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.89
Train Epoch over. train_loss: 0.58; train_accuracy: 0.87 

0.00018814709619618952
0.00018006601021625102
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.5151673348466302; val_accuracy: 0.8859474522292994 

The current subspace-distance is: 0.00018006601021625102 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.68; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00018800441466737539
0.00018163182539865375
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.5033495762165944; val_accuracy: 0.8890326433121019 

The current subspace-distance is: 0.00018163182539865375 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.72; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.64; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.84
Batch: 420; loss: 0.74; acc: 0.81
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.95
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.92
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.53; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.92
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00019159787916578352
0.0001853719149949029
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.94
Val Epoch over. val_loss: 0.5011552297005988; val_accuracy: 0.888734076433121 

The current subspace-distance is: 0.0001853719149949029 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.62; acc: 0.8
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.58; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.5; acc: 0.95
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.64; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.92
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.55; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.95
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00019567289564292878
0.00018647746765054762
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.34; acc: 0.94
Val Epoch over. val_loss: 0.4977250203585169; val_accuracy: 0.8881369426751592 

The current subspace-distance is: 0.00018647746765054762 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.52; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.94
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.45; acc: 0.94
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.57; acc: 0.89
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00019678642274811864
0.00018990803800988942
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.4936080314930837; val_accuracy: 0.8898288216560509 

The current subspace-distance is: 0.00018990803800988942 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.94
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.94
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.77
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.6; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.64; acc: 0.75
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0002015105274040252
0.00019153540779370815
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4887553392701848; val_accuracy: 0.8899283439490446 

The current subspace-distance is: 0.00019153540779370815 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.95
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.94
Batch: 220; loss: 0.58; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.68; acc: 0.89
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.8
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.0002008942683460191
0.00019255114602856338
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.48330672825597654; val_accuracy: 0.8914211783439491 

The current subspace-distance is: 0.00019255114602856338 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.95
Batch: 40; loss: 0.62; acc: 0.8
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.95
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.95
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.52; acc: 0.91
Batch: 620; loss: 0.63; acc: 0.8
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.94
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020322715863585472
0.00019769802747759968
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.4822767674922943; val_accuracy: 0.8921178343949044 

The current subspace-distance is: 0.00019769802747759968 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.95
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.65; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.00020711043907795101
0.00020011788001284003
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.47843796906957203; val_accuracy: 0.8918192675159236 

The current subspace-distance is: 0.00020011788001284003 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.41; acc: 0.97
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.62; acc: 0.77
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

0.0002081437414744869
0.00020101478730794042
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4837636492054933; val_accuracy: 0.8898288216560509 

The current subspace-distance is: 0.00020101478730794042 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.69; acc: 0.75
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.95
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.67; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00021023409499321133
0.00020443009270820767
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.47061983700011184; val_accuracy: 0.8930135350318471 

The current subspace-distance is: 0.00020443009270820767 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.82; acc: 0.7
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.97
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00021200760966166854
0.00020407751435413957
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4657064005257977; val_accuracy: 0.8926154458598726 

The current subspace-distance is: 0.00020407751435413957 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.38; acc: 0.95
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.57; acc: 0.8
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00021486321929842234
0.00020456375204958022
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4674143992411863; val_accuracy: 0.8918192675159236 

The current subspace-distance is: 0.00020456375204958022 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.94
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.95
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0002112436923198402
0.00020516625954769552
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.94
Val Epoch over. val_loss: 0.4616125094093335; val_accuracy: 0.8935111464968153 

The current subspace-distance is: 0.00020516625954769552 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.95
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.64; acc: 0.78
Batch: 300; loss: 0.55; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.58; acc: 0.81
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00021207734243944287
0.00020348976249806583
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.46097816137751196; val_accuracy: 0.8922173566878981 

The current subspace-distance is: 0.00020348976249806583 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.83
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.72; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00021404070139396936
0.0002066546876449138
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.45927757375938877; val_accuracy: 0.8938097133757962 

The current subspace-distance is: 0.0002066546876449138 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.73; acc: 0.75
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00021478260168805718
0.00020387339463923126
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4591754725214782; val_accuracy: 0.892515923566879 

The current subspace-distance is: 0.00020387339463923126 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.51; acc: 0.95
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.95
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00021441625722218305
0.00020606636826414615
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.45375244271983006; val_accuracy: 0.8957006369426752 

The current subspace-distance is: 0.00020606636826414615 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.7; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.52; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.92
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.55; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.98
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.0002140452852472663
0.00020724569912999868
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.46152731576922595; val_accuracy: 0.8941082802547771 

The current subspace-distance is: 0.00020724569912999868 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.73
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.92
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.97
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

0.00021577923325821757
0.00020710717944893986
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.45616637474032723; val_accuracy: 0.894406847133758 

The current subspace-distance is: 0.00020710717944893986 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.95
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

0.0002154508256353438
0.00020765601948369294
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.45951732793811023; val_accuracy: 0.8937101910828026 

The current subspace-distance is: 0.00020765601948369294 

plots/subspace_training/table13slim/2020-01-29 16:12:31/N_14_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.07

The number of parameters is: 267152

The number of individual parameters is:

9
162
9
9
13
31356
13
13
26
90584
26
26
64
139776
64
64
4096
64
640
10
64
64

nonzero elements in E: 106860791
elements in E: 106860800
fraction nonzero: 0.9999999157782835
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.59; acc: 0.05
Batch: 20; loss: 2.1; acc: 0.31
Batch: 40; loss: 1.89; acc: 0.41
Batch: 60; loss: 1.72; acc: 0.45
Batch: 80; loss: 1.46; acc: 0.69
Batch: 100; loss: 1.63; acc: 0.56
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.46; acc: 0.7
Batch: 160; loss: 1.32; acc: 0.8
Batch: 180; loss: 1.33; acc: 0.7
Batch: 200; loss: 1.26; acc: 0.83
Batch: 220; loss: 1.43; acc: 0.66
Batch: 240; loss: 1.28; acc: 0.77
Batch: 260; loss: 1.47; acc: 0.59
Batch: 280; loss: 1.3; acc: 0.7
Batch: 300; loss: 1.26; acc: 0.78
Batch: 320; loss: 1.16; acc: 0.84
Batch: 340; loss: 1.22; acc: 0.77
Batch: 360; loss: 1.2; acc: 0.75
Batch: 380; loss: 1.22; acc: 0.78
Batch: 400; loss: 1.15; acc: 0.81
Batch: 420; loss: 1.11; acc: 0.81
Batch: 440; loss: 1.15; acc: 0.78
Batch: 460; loss: 1.14; acc: 0.78
Batch: 480; loss: 1.15; acc: 0.75
Batch: 500; loss: 1.07; acc: 0.83
Batch: 520; loss: 1.23; acc: 0.75
Batch: 540; loss: 1.12; acc: 0.81
Batch: 560; loss: 1.07; acc: 0.78
Batch: 580; loss: 0.98; acc: 0.86
Batch: 600; loss: 0.94; acc: 0.91
Batch: 620; loss: 1.09; acc: 0.89
Batch: 640; loss: 1.03; acc: 0.83
Batch: 660; loss: 0.99; acc: 0.81
Batch: 680; loss: 1.0; acc: 0.84
Batch: 700; loss: 1.12; acc: 0.73
Batch: 720; loss: 0.98; acc: 0.78
Batch: 740; loss: 0.95; acc: 0.86
Batch: 760; loss: 1.17; acc: 0.73
Batch: 780; loss: 1.27; acc: 0.69
Train Epoch over. train_loss: 1.27; train_accuracy: 0.73 

2.6675083063310012e-05
8.873405022313818e-06
Batch: 0; loss: 0.98; acc: 0.83
Batch: 20; loss: 1.16; acc: 0.77
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.93; acc: 0.78
Batch: 80; loss: 0.83; acc: 0.89
Batch: 100; loss: 0.93; acc: 0.91
Batch: 120; loss: 1.19; acc: 0.77
Batch: 140; loss: 0.79; acc: 0.92
Val Epoch over. val_loss: 0.9544537697628046; val_accuracy: 0.8376791401273885 

The current subspace-distance is: 8.873405022313818e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.84
Batch: 20; loss: 1.02; acc: 0.8
Batch: 40; loss: 0.99; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.84
Batch: 80; loss: 1.03; acc: 0.81
Batch: 100; loss: 0.86; acc: 0.86
Batch: 120; loss: 0.96; acc: 0.88
Batch: 140; loss: 1.0; acc: 0.83
Batch: 160; loss: 1.04; acc: 0.77
Batch: 180; loss: 0.99; acc: 0.73
Batch: 200; loss: 1.01; acc: 0.8
Batch: 220; loss: 0.91; acc: 0.86
Batch: 240; loss: 1.09; acc: 0.78
Batch: 260; loss: 0.99; acc: 0.8
Batch: 280; loss: 0.97; acc: 0.83
Batch: 300; loss: 0.93; acc: 0.83
Batch: 320; loss: 0.99; acc: 0.77
Batch: 340; loss: 0.93; acc: 0.84
Batch: 360; loss: 0.97; acc: 0.8
Batch: 380; loss: 1.01; acc: 0.8
Batch: 400; loss: 0.87; acc: 0.84
Batch: 420; loss: 0.85; acc: 0.86
Batch: 440; loss: 0.85; acc: 0.89
Batch: 460; loss: 0.96; acc: 0.86
Batch: 480; loss: 0.89; acc: 0.86
Batch: 500; loss: 0.9; acc: 0.83
Batch: 520; loss: 0.98; acc: 0.78
Batch: 540; loss: 0.78; acc: 0.88
Batch: 560; loss: 0.89; acc: 0.86
Batch: 580; loss: 1.02; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.91
Batch: 620; loss: 0.98; acc: 0.73
Batch: 640; loss: 1.05; acc: 0.78
Batch: 660; loss: 0.77; acc: 0.92
Batch: 680; loss: 0.82; acc: 0.89
Batch: 700; loss: 0.85; acc: 0.88
Batch: 720; loss: 0.79; acc: 0.86
Batch: 740; loss: 0.89; acc: 0.84
Batch: 760; loss: 0.83; acc: 0.84
Batch: 780; loss: 0.95; acc: 0.78
Train Epoch over. train_loss: 0.93; train_accuracy: 0.83 

3.125713192275725e-05
1.177475496660918e-05
Batch: 0; loss: 0.85; acc: 0.88
Batch: 20; loss: 0.98; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.95
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.88
Batch: 100; loss: 0.74; acc: 0.91
Batch: 120; loss: 1.05; acc: 0.77
Batch: 140; loss: 0.63; acc: 0.92
Val Epoch over. val_loss: 0.7993516299375303; val_accuracy: 0.8629578025477707 

The current subspace-distance is: 1.177475496660918e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.84
Batch: 20; loss: 0.92; acc: 0.84
Batch: 40; loss: 0.91; acc: 0.86
Batch: 60; loss: 0.88; acc: 0.86
Batch: 80; loss: 0.8; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 1.02; acc: 0.73
Batch: 160; loss: 0.86; acc: 0.81
Batch: 180; loss: 0.85; acc: 0.84
Batch: 200; loss: 0.89; acc: 0.8
Batch: 220; loss: 0.78; acc: 0.81
Batch: 240; loss: 0.9; acc: 0.77
Batch: 260; loss: 0.69; acc: 0.91
Batch: 280; loss: 0.83; acc: 0.86
Batch: 300; loss: 0.73; acc: 0.89
Batch: 320; loss: 0.93; acc: 0.8
Batch: 340; loss: 0.65; acc: 0.94
Batch: 360; loss: 0.75; acc: 0.83
Batch: 380; loss: 0.86; acc: 0.78
Batch: 400; loss: 0.8; acc: 0.83
Batch: 420; loss: 0.8; acc: 0.88
Batch: 440; loss: 0.71; acc: 0.88
Batch: 460; loss: 0.87; acc: 0.81
Batch: 480; loss: 0.87; acc: 0.84
Batch: 500; loss: 0.87; acc: 0.86
Batch: 520; loss: 0.82; acc: 0.86
Batch: 540; loss: 0.79; acc: 0.86
Batch: 560; loss: 0.82; acc: 0.83
Batch: 580; loss: 0.83; acc: 0.81
Batch: 600; loss: 0.81; acc: 0.78
Batch: 620; loss: 0.89; acc: 0.78
Batch: 640; loss: 0.72; acc: 0.91
Batch: 660; loss: 0.81; acc: 0.81
Batch: 680; loss: 0.78; acc: 0.86
Batch: 700; loss: 0.78; acc: 0.88
Batch: 720; loss: 0.76; acc: 0.86
Batch: 740; loss: 0.87; acc: 0.81
Batch: 760; loss: 0.89; acc: 0.78
Batch: 780; loss: 1.01; acc: 0.75
Train Epoch over. train_loss: 0.8; train_accuracy: 0.85 

3.478751750662923e-05
1.3360605407797266e-05
Batch: 0; loss: 0.75; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.94
Batch: 120; loss: 0.93; acc: 0.81
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6877998597682662; val_accuracy: 0.8777866242038217 

The current subspace-distance is: 1.3360605407797266e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.88
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 0.86; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.67; acc: 0.89
Batch: 180; loss: 0.78; acc: 0.84
Batch: 200; loss: 0.74; acc: 0.86
Batch: 220; loss: 0.74; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.91
Batch: 260; loss: 0.61; acc: 0.94
Batch: 280; loss: 0.7; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.91
Batch: 320; loss: 0.89; acc: 0.75
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.73; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.73; acc: 0.86
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.88; acc: 0.75
Batch: 500; loss: 0.64; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.94
Batch: 560; loss: 0.88; acc: 0.81
Batch: 580; loss: 0.71; acc: 0.86
Batch: 600; loss: 0.8; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.66; acc: 0.91
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.72; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.95
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.79; acc: 0.8
Train Epoch over. train_loss: 0.71; train_accuracy: 0.86 

3.837353506241925e-05
1.554593472974375e-05
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.94
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.91
Val Epoch over. val_loss: 0.618480597332025; val_accuracy: 0.8866441082802548 

The current subspace-distance is: 1.554593472974375e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.95
Batch: 40; loss: 0.82; acc: 0.78
Batch: 60; loss: 0.57; acc: 0.92
Batch: 80; loss: 0.79; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.95
Batch: 180; loss: 0.81; acc: 0.8
Batch: 200; loss: 0.54; acc: 0.92
Batch: 220; loss: 0.63; acc: 0.89
Batch: 240; loss: 0.7; acc: 0.91
Batch: 260; loss: 0.83; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.94
Batch: 340; loss: 0.6; acc: 0.94
Batch: 360; loss: 0.63; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.76; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.91
Batch: 500; loss: 0.65; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.97
Batch: 560; loss: 0.54; acc: 0.94
Batch: 580; loss: 0.57; acc: 0.91
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.92
Batch: 660; loss: 0.62; acc: 0.92
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.63; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.87 

4.172950139036402e-05
1.7368587577948347e-05
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.91
Val Epoch over. val_loss: 0.5571820660001913; val_accuracy: 0.8915207006369427 

The current subspace-distance is: 1.7368587577948347e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.89
Batch: 140; loss: 0.68; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.8
Batch: 180; loss: 0.6; acc: 0.91
Batch: 200; loss: 0.62; acc: 0.91
Batch: 220; loss: 0.63; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.91
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.94
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.95
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.8; acc: 0.73
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.73; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.74; acc: 0.77
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.91
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.94
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.97
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.59; train_accuracy: 0.88 

4.509446443989873e-05
2.0490593669819646e-05
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.5190142324775647; val_accuracy: 0.8927149681528662 

The current subspace-distance is: 2.0490593669819646e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.95
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.91
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.8
Batch: 480; loss: 0.51; acc: 0.92
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.97
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.79; acc: 0.73
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.58; acc: 0.91
Batch: 660; loss: 0.6; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.92
Batch: 760; loss: 0.52; acc: 0.94
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

4.79545233247336e-05
2.1471589207067154e-05
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.91
Val Epoch over. val_loss: 0.4913601608602864; val_accuracy: 0.893312101910828 

The current subspace-distance is: 2.1471589207067154e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.97
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.94
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.92
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.94
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.95
Batch: 440; loss: 0.6; acc: 0.88
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.37; acc: 0.95
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

5.0627080781850964e-05
2.2722151697962545e-05
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.4552662610817867; val_accuracy: 0.8981886942675159 

The current subspace-distance is: 2.2722151697962545e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.68; acc: 0.77
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.66; acc: 0.8
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

5.249908281257376e-05
2.2765065295971e-05
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.91
Val Epoch over. val_loss: 0.44517915264056745; val_accuracy: 0.896297770700637 

The current subspace-distance is: 2.2765065295971e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.58; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.89 

5.506730667548254e-05
2.4390061298618093e-05
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.42304068433631; val_accuracy: 0.8999800955414012 

The current subspace-distance is: 2.4390061298618093e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.97
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.97
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.95
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.8
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

5.547287219087593e-05
2.3569742552354e-05
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.4218602069434087; val_accuracy: 0.900577229299363 

The current subspace-distance is: 2.3569742552354e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.98
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

5.642712858389132e-05
2.566278271842748e-05
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.21; acc: 1.0
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.4142650191191655; val_accuracy: 0.8990843949044586 

The current subspace-distance is: 2.566278271842748e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.97
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

5.7871307944878936e-05
2.6508025257498957e-05
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.94
Val Epoch over. val_loss: 0.40625183009038307; val_accuracy: 0.9032643312101911 

The current subspace-distance is: 2.6508025257498957e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.78
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.98
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.97
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

5.8172212447971106e-05
2.6684712793212384e-05
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.41317742930096424; val_accuracy: 0.902468152866242 

The current subspace-distance is: 2.6684712793212384e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.53; acc: 0.91
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.89 

5.930006591370329e-05
2.8588932764250785e-05
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.3973238268855271; val_accuracy: 0.9033638535031847 

The current subspace-distance is: 2.8588932764250785e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.56; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.98
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.78; acc: 0.78
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.89 

5.884241545572877e-05
2.5910077965818346e-05
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.3943345147143504; val_accuracy: 0.90515525477707 

The current subspace-distance is: 2.5910077965818346e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.97
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.97
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.57; acc: 0.8
Train Epoch over. train_loss: 0.44; train_accuracy: 0.89 

5.959594273008406e-05
2.6890818844549358e-05
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.39251337432937256; val_accuracy: 0.9054538216560509 

The current subspace-distance is: 2.6890818844549358e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

6.026522532920353e-05
2.7486696126288734e-05
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.3901889527299602; val_accuracy: 0.9075437898089171 

The current subspace-distance is: 2.7486696126288734e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.89 

6.040387961547822e-05
2.779374881356489e-05
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3886264100386079; val_accuracy: 0.9066480891719745 

The current subspace-distance is: 2.779374881356489e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.55; acc: 0.8
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.26; acc: 1.0
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.98
Train Epoch over. train_loss: 0.43; train_accuracy: 0.89 

6.033493627910502e-05
2.6101828552782536e-05
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3779116831957155; val_accuracy: 0.908937101910828 

The current subspace-distance is: 2.6101828552782536e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.53; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.102693441789597e-05
2.6619625714374706e-05
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3841291507528086; val_accuracy: 0.9092356687898089 

The current subspace-distance is: 2.6619625714374706e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.97
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.97
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.150446279207245e-05
2.6940524548990652e-05
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.3831249519138579; val_accuracy: 0.9092356687898089 

The current subspace-distance is: 2.6940524548990652e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.104101339587942e-05
2.708605825318955e-05
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.18; acc: 1.0
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3783033077314401; val_accuracy: 0.9099323248407644 

The current subspace-distance is: 2.708605825318955e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.94
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

6.187027611304075e-05
2.848555232048966e-05
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.3726996384608518; val_accuracy: 0.9110270700636943 

The current subspace-distance is: 2.848555232048966e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.95
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.197666516527534e-05
2.793304156512022e-05
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.37757467786977245; val_accuracy: 0.9088375796178344 

The current subspace-distance is: 2.793304156512022e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.156411836855114e-05
2.8487540475907736e-05
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.37329444763766734; val_accuracy: 0.9094347133757962 

The current subspace-distance is: 2.8487540475907736e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.185524398460984e-05
2.951561691588722e-05
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.37389395979179696; val_accuracy: 0.9102308917197452 

The current subspace-distance is: 2.951561691588722e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.25680186203681e-05
2.952687100332696e-05
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.37320069114493715; val_accuracy: 0.9098328025477707 

The current subspace-distance is: 2.952687100332696e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.8
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.19196507614106e-05
2.8295788069954142e-05
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.16; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.3713987546551759; val_accuracy: 0.910031847133758 

The current subspace-distance is: 2.8295788069954142e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.98
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.52; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.233820749912411e-05
2.7704454623744823e-05
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.17; acc: 1.0
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3711915807739185; val_accuracy: 0.9116242038216561 

The current subspace-distance is: 2.7704454623744823e-05 

plots/subspace_training/table13slim/2020-01-29 16:12:31/N_14_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 1.07

The number of parameters is: 267152

The number of individual parameters is:

9
162
9
9
13
31356
13
13
26
90584
26
26
64
139776
64
64
4096
64
640
10
64
64

nonzero elements in E: 133575991
elements in E: 133576000
fraction nonzero: 0.9999999326226268
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.54; acc: 0.12
Batch: 20; loss: 2.07; acc: 0.27
Batch: 40; loss: 1.86; acc: 0.45
Batch: 60; loss: 1.63; acc: 0.53
Batch: 80; loss: 1.44; acc: 0.67
Batch: 100; loss: 1.48; acc: 0.77
Batch: 120; loss: 1.33; acc: 0.73
Batch: 140; loss: 1.34; acc: 0.66
Batch: 160; loss: 1.32; acc: 0.72
Batch: 180; loss: 1.28; acc: 0.75
Batch: 200; loss: 1.13; acc: 0.83
Batch: 220; loss: 1.25; acc: 0.75
Batch: 240; loss: 1.14; acc: 0.81
Batch: 260; loss: 1.23; acc: 0.73
Batch: 280; loss: 1.1; acc: 0.83
Batch: 300; loss: 1.33; acc: 0.67
Batch: 320; loss: 1.08; acc: 0.83
Batch: 340; loss: 1.14; acc: 0.75
Batch: 360; loss: 1.27; acc: 0.78
Batch: 380; loss: 0.99; acc: 0.84
Batch: 400; loss: 1.11; acc: 0.8
Batch: 420; loss: 0.95; acc: 0.89
Batch: 440; loss: 1.04; acc: 0.8
Batch: 460; loss: 1.03; acc: 0.83
Batch: 480; loss: 1.1; acc: 0.72
Batch: 500; loss: 0.93; acc: 0.88
Batch: 520; loss: 1.02; acc: 0.81
Batch: 540; loss: 0.9; acc: 0.88
Batch: 560; loss: 1.07; acc: 0.83
Batch: 580; loss: 0.95; acc: 0.91
Batch: 600; loss: 0.95; acc: 0.84
Batch: 620; loss: 0.96; acc: 0.84
Batch: 640; loss: 0.85; acc: 0.92
Batch: 660; loss: 0.93; acc: 0.8
Batch: 680; loss: 0.94; acc: 0.89
Batch: 700; loss: 0.91; acc: 0.88
Batch: 720; loss: 0.89; acc: 0.83
Batch: 740; loss: 0.92; acc: 0.83
Batch: 760; loss: 0.82; acc: 0.91
Batch: 780; loss: 0.89; acc: 0.81
Train Epoch over. train_loss: 1.16; train_accuracy: 0.76 

2.6592479116516188e-05
8.687786248628981e-06
Batch: 0; loss: 0.83; acc: 0.89
Batch: 20; loss: 0.94; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.97
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.95
Batch: 100; loss: 0.78; acc: 0.88
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.88
Val Epoch over. val_loss: 0.8261718370352581; val_accuracy: 0.8680334394904459 

The current subspace-distance is: 8.687786248628981e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.91
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 0.84; acc: 0.86
Batch: 60; loss: 0.87; acc: 0.89
Batch: 80; loss: 0.82; acc: 0.83
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.89
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.95; acc: 0.84
Batch: 180; loss: 0.8; acc: 0.86
Batch: 200; loss: 0.81; acc: 0.88
Batch: 220; loss: 0.9; acc: 0.81
Batch: 240; loss: 0.76; acc: 0.88
Batch: 260; loss: 0.91; acc: 0.8
Batch: 280; loss: 0.76; acc: 0.89
Batch: 300; loss: 0.71; acc: 0.94
Batch: 320; loss: 0.8; acc: 0.89
Batch: 340; loss: 0.81; acc: 0.8
Batch: 360; loss: 0.84; acc: 0.83
Batch: 380; loss: 0.71; acc: 0.88
Batch: 400; loss: 0.81; acc: 0.86
Batch: 420; loss: 0.8; acc: 0.86
Batch: 440; loss: 0.83; acc: 0.86
Batch: 460; loss: 0.9; acc: 0.86
Batch: 480; loss: 0.77; acc: 0.89
Batch: 500; loss: 0.72; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.95
Batch: 540; loss: 0.77; acc: 0.86
Batch: 560; loss: 0.75; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.89
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.67; acc: 0.94
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.97
Batch: 680; loss: 0.72; acc: 0.89
Batch: 700; loss: 0.66; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.94
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.72; acc: 0.86
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.78; train_accuracy: 0.86 

3.182419095537625e-05
1.1840315892186482e-05
Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.77; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.97
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.97
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.95
Val Epoch over. val_loss: 0.6482266202853744; val_accuracy: 0.893312101910828 

The current subspace-distance is: 1.1840315892186482e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.88
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.72; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.89
Batch: 140; loss: 0.69; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.61; acc: 0.94
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.92
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.91
Batch: 300; loss: 0.73; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.61; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.57; acc: 0.92
Batch: 420; loss: 0.64; acc: 0.91
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.59; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.95
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.66; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.83
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.42; acc: 0.94
Train Epoch over. train_loss: 0.64; train_accuracy: 0.88 

3.7846039049327374e-05
1.5568770322715864e-05
Batch: 0; loss: 0.51; acc: 0.95
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.31; acc: 1.0
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.97
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5266571439755191; val_accuracy: 0.9026671974522293 

The current subspace-distance is: 1.5568770322715864e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.95
Batch: 280; loss: 0.57; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.94
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.94
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.89 

4.121760503039695e-05
1.8310682207811624e-05
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.98
Val Epoch over. val_loss: 0.46614060251955775; val_accuracy: 0.9092356687898089 

The current subspace-distance is: 1.8310682207811624e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.91
Batch: 260; loss: 0.64; acc: 0.8
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.94
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.48; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

4.4722641177941114e-05
1.9876486476277933e-05
Batch: 0; loss: 0.38; acc: 0.97
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.22; acc: 1.0
Val Epoch over. val_loss: 0.4169900348042227; val_accuracy: 0.9129179936305732 

The current subspace-distance is: 1.9876486476277933e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.48; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.95
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.81
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

4.770383020513691e-05
2.1362675397540443e-05
Batch: 0; loss: 0.35; acc: 0.97
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3877506584498533; val_accuracy: 0.9138136942675159 

The current subspace-distance is: 2.1362675397540443e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.98
Batch: 340; loss: 0.38; acc: 0.95
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.97
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.934557364322245e-05
2.0668596334871836e-05
Batch: 0; loss: 0.32; acc: 0.97
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.35691567666971; val_accuracy: 0.9187898089171974 

The current subspace-distance is: 2.0668596334871836e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.245291322353296e-05
2.400926132395398e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3362270760688053; val_accuracy: 0.9213773885350318 

The current subspace-distance is: 2.400926132395398e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.97
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.4108484619064257e-05
2.38299307966372e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3170677067557718; val_accuracy: 0.9251592356687898 

The current subspace-distance is: 2.38299307966372e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.98
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.744567170040682e-05
2.681335354282055e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.304170296070682; val_accuracy: 0.9269506369426752 

The current subspace-distance is: 2.681335354282055e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.98
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.97
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.7730481785256416e-05
2.61506465903949e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.29701516663382765; val_accuracy: 0.9282444267515924 

The current subspace-distance is: 2.61506465903949e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.98
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.24; acc: 1.0
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.84164954489097e-05
2.6976662411470897e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.29761221478129646; val_accuracy: 0.9285429936305732 

The current subspace-distance is: 2.6976662411470897e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.9487883845577016e-05
2.8422709874575958e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.29270633760910886; val_accuracy: 0.9289410828025477 

The current subspace-distance is: 2.8422709874575958e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.872863403055817e-05
2.555561877670698e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2844787436495921; val_accuracy: 0.931031050955414 

The current subspace-distance is: 2.555561877670698e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.98
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.9642570704454556e-05
2.7226615202380344e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.28751956258609795; val_accuracy: 0.9320262738853503 

The current subspace-distance is: 2.7226615202380344e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.085649874876253e-05
2.8991338695050217e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2790121180331631; val_accuracy: 0.9324243630573248 

The current subspace-distance is: 2.8991338695050217e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.97
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.040980588295497e-05
2.634950214996934e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2782111540911304; val_accuracy: 0.931031050955414 

The current subspace-distance is: 2.634950214996934e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.83
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.98
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.18; acc: 1.0
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.118821329437196e-05
2.6647387130651623e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2726395751830119; val_accuracy: 0.9306329617834395 

The current subspace-distance is: 2.6647387130651623e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.98
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.19; acc: 0.98
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.11366267548874e-05
2.739626324910205e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2706608950712119; val_accuracy: 0.9328224522292994 

The current subspace-distance is: 2.739626324910205e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.17603218415752e-05
2.7447526008472778e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.26494968184240303; val_accuracy: 0.9325238853503185 

The current subspace-distance is: 2.7447526008472778e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.35; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.224936805665493e-05
2.7351688913768157e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.266928880125474; val_accuracy: 0.931827229299363 

The current subspace-distance is: 2.7351688913768157e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.264948024181649e-05
2.8534699595184065e-05
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2664292233574922; val_accuracy: 0.9335191082802548 

The current subspace-distance is: 2.8534699595184065e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.3; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.84
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.200907228048891e-05
2.7497300834511407e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2662183749637786; val_accuracy: 0.9330214968152867 

The current subspace-distance is: 2.7497300834511407e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.2; acc: 1.0
Batch: 200; loss: 0.21; acc: 0.98
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.26; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.98
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.294781633187085e-05
2.8375670808600262e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.26806173924427884; val_accuracy: 0.9327229299363057 

The current subspace-distance is: 2.8375670808600262e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.22; acc: 1.0
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.81
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.98
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.260359077714384e-05
2.7217402021051385e-05
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.26715905670147794; val_accuracy: 0.9317277070063694 

The current subspace-distance is: 2.7217402021051385e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

6.341660628095269e-05
2.9015503969276324e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.26513850769609404; val_accuracy: 0.9334195859872612 

The current subspace-distance is: 2.9015503969276324e-05 

Epoch 27 start
The current lr is: 0.09
