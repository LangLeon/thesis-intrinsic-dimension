model : table13slim
N : 8
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:48

Channel scaling factor: 2.4265201420964964

The number of parameters is: 257601

The number of individual parameters is:

20
360
20
20
30
40800
30
30
59
120360
59
59
64
90624
64
64
4096
64
640
10
64
64

nonzero elements in E: 12880048
elements in E: 12880050
fraction nonzero: 0.9999998447210997
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.31; acc: 0.12
Batch: 20; loss: 2.34; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.26; acc: 0.22
Batch: 80; loss: 2.26; acc: 0.25
Batch: 100; loss: 2.18; acc: 0.19
Batch: 120; loss: 2.11; acc: 0.34
Batch: 140; loss: 2.15; acc: 0.33
Batch: 160; loss: 2.11; acc: 0.3
Batch: 180; loss: 2.07; acc: 0.3
Batch: 200; loss: 2.04; acc: 0.31
Batch: 220; loss: 1.93; acc: 0.36
Batch: 240; loss: 2.06; acc: 0.34
Batch: 260; loss: 2.11; acc: 0.25
Batch: 280; loss: 2.08; acc: 0.27
Batch: 300; loss: 1.96; acc: 0.36
Batch: 320; loss: 1.98; acc: 0.33
Batch: 340; loss: 2.03; acc: 0.31
Batch: 360; loss: 1.97; acc: 0.44
Batch: 380; loss: 1.88; acc: 0.45
Batch: 400; loss: 1.93; acc: 0.41
Batch: 420; loss: 1.87; acc: 0.45
Batch: 440; loss: 2.03; acc: 0.3
Batch: 460; loss: 1.91; acc: 0.42
Batch: 480; loss: 1.93; acc: 0.39
Batch: 500; loss: 1.93; acc: 0.44
Batch: 520; loss: 1.91; acc: 0.42
Batch: 540; loss: 1.86; acc: 0.44
Batch: 560; loss: 2.02; acc: 0.33
Batch: 580; loss: 1.96; acc: 0.34
Batch: 600; loss: 1.99; acc: 0.33
Batch: 620; loss: 1.92; acc: 0.39
Batch: 640; loss: 1.81; acc: 0.48
Batch: 660; loss: 1.88; acc: 0.44
Batch: 680; loss: 1.92; acc: 0.44
Batch: 700; loss: 1.9; acc: 0.44
Batch: 720; loss: 1.86; acc: 0.45
Batch: 740; loss: 1.75; acc: 0.61
Batch: 760; loss: 1.96; acc: 0.38
Batch: 780; loss: 1.87; acc: 0.45
Train Epoch over. train_loss: 2.01; train_accuracy: 0.35 

2.277312523801811e-05
4.2669280446716584e-06
Batch: 0; loss: 2.06; acc: 0.28
Batch: 20; loss: 1.9; acc: 0.48
Batch: 40; loss: 1.68; acc: 0.55
Batch: 60; loss: 1.76; acc: 0.55
Batch: 80; loss: 1.81; acc: 0.48
Batch: 100; loss: 1.85; acc: 0.42
Batch: 120; loss: 1.84; acc: 0.47
Batch: 140; loss: 1.66; acc: 0.61
Val Epoch over. val_loss: 1.8354060589128238; val_accuracy: 0.4772093949044586 

The current subspace-distance is: 4.2669280446716584e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.83; acc: 0.42
Batch: 20; loss: 1.83; acc: 0.47
Batch: 40; loss: 1.89; acc: 0.41
Batch: 60; loss: 1.92; acc: 0.36
Batch: 80; loss: 2.03; acc: 0.3
Batch: 100; loss: 1.9; acc: 0.45
Batch: 120; loss: 1.9; acc: 0.48
Batch: 140; loss: 1.98; acc: 0.36
Batch: 160; loss: 1.99; acc: 0.38
Batch: 180; loss: 1.83; acc: 0.44
Batch: 200; loss: 1.73; acc: 0.55
Batch: 220; loss: 1.83; acc: 0.48
Batch: 240; loss: 1.8; acc: 0.47
Batch: 260; loss: 1.86; acc: 0.44
Batch: 280; loss: 1.84; acc: 0.5
Batch: 300; loss: 1.81; acc: 0.52
Batch: 320; loss: 1.78; acc: 0.5
Batch: 340; loss: 1.79; acc: 0.53
Batch: 360; loss: 1.87; acc: 0.45
Batch: 380; loss: 1.76; acc: 0.52
Batch: 400; loss: 1.81; acc: 0.52
Batch: 420; loss: 1.79; acc: 0.53
Batch: 440; loss: 1.81; acc: 0.53
Batch: 460; loss: 1.87; acc: 0.42
Batch: 480; loss: 1.82; acc: 0.39
Batch: 500; loss: 1.86; acc: 0.47
Batch: 520; loss: 1.79; acc: 0.48
Batch: 540; loss: 1.77; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.72
Batch: 580; loss: 1.86; acc: 0.47
Batch: 600; loss: 1.88; acc: 0.44
Batch: 620; loss: 1.91; acc: 0.38
Batch: 640; loss: 1.77; acc: 0.53
Batch: 660; loss: 1.89; acc: 0.47
Batch: 680; loss: 1.87; acc: 0.36
Batch: 700; loss: 1.76; acc: 0.53
Batch: 720; loss: 1.77; acc: 0.53
Batch: 740; loss: 1.88; acc: 0.5
Batch: 760; loss: 1.89; acc: 0.42
Batch: 780; loss: 1.92; acc: 0.36
Train Epoch over. train_loss: 1.84; train_accuracy: 0.47 

2.4896973627619445e-05
5.410539415606763e-06
Batch: 0; loss: 1.99; acc: 0.34
Batch: 20; loss: 1.81; acc: 0.53
Batch: 40; loss: 1.62; acc: 0.59
Batch: 60; loss: 1.73; acc: 0.52
Batch: 80; loss: 1.76; acc: 0.53
Batch: 100; loss: 1.83; acc: 0.5
Batch: 120; loss: 1.83; acc: 0.42
Batch: 140; loss: 1.65; acc: 0.59
Val Epoch over. val_loss: 1.7842052521978973; val_accuracy: 0.5067675159235668 

The current subspace-distance is: 5.410539415606763e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.81; acc: 0.47
Batch: 20; loss: 1.86; acc: 0.5
Batch: 40; loss: 1.76; acc: 0.5
Batch: 60; loss: 1.74; acc: 0.59
Batch: 80; loss: 1.95; acc: 0.41
Batch: 100; loss: 1.85; acc: 0.44
Batch: 120; loss: 1.8; acc: 0.5
Batch: 140; loss: 1.86; acc: 0.41
Batch: 160; loss: 1.81; acc: 0.45
Batch: 180; loss: 1.83; acc: 0.45
Batch: 200; loss: 1.84; acc: 0.45
Batch: 220; loss: 1.82; acc: 0.5
Batch: 240; loss: 1.75; acc: 0.53
Batch: 260; loss: 1.72; acc: 0.52
Batch: 280; loss: 1.86; acc: 0.48
Batch: 300; loss: 1.8; acc: 0.48
Batch: 320; loss: 1.65; acc: 0.58
Batch: 340; loss: 1.88; acc: 0.48
Batch: 360; loss: 1.8; acc: 0.52
Batch: 380; loss: 1.77; acc: 0.52
Batch: 400; loss: 1.89; acc: 0.31
Batch: 420; loss: 1.68; acc: 0.56
Batch: 440; loss: 1.8; acc: 0.5
Batch: 460; loss: 1.85; acc: 0.48
Batch: 480; loss: 1.76; acc: 0.5
Batch: 500; loss: 1.75; acc: 0.55
Batch: 520; loss: 1.77; acc: 0.47
Batch: 540; loss: 1.81; acc: 0.58
Batch: 560; loss: 1.84; acc: 0.47
Batch: 580; loss: 1.76; acc: 0.58
Batch: 600; loss: 1.71; acc: 0.59
Batch: 620; loss: 1.7; acc: 0.62
Batch: 640; loss: 1.8; acc: 0.47
Batch: 660; loss: 1.82; acc: 0.48
Batch: 680; loss: 1.76; acc: 0.53
Batch: 700; loss: 1.87; acc: 0.52
Batch: 720; loss: 1.88; acc: 0.33
Batch: 740; loss: 1.75; acc: 0.56
Batch: 760; loss: 1.85; acc: 0.52
Batch: 780; loss: 1.77; acc: 0.48
Train Epoch over. train_loss: 1.8; train_accuracy: 0.49 

2.6532597985351458e-05
5.940964911133051e-06
Batch: 0; loss: 1.93; acc: 0.42
Batch: 20; loss: 1.78; acc: 0.48
Batch: 40; loss: 1.58; acc: 0.64
Batch: 60; loss: 1.74; acc: 0.53
Batch: 80; loss: 1.71; acc: 0.59
Batch: 100; loss: 1.83; acc: 0.48
Batch: 120; loss: 1.85; acc: 0.39
Batch: 140; loss: 1.6; acc: 0.61
Val Epoch over. val_loss: 1.758837459952968; val_accuracy: 0.5204020700636943 

The current subspace-distance is: 5.940964911133051e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.88; acc: 0.39
Batch: 20; loss: 1.89; acc: 0.41
Batch: 40; loss: 1.78; acc: 0.5
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.61
Batch: 100; loss: 1.65; acc: 0.56
Batch: 120; loss: 1.82; acc: 0.53
Batch: 140; loss: 1.75; acc: 0.53
Batch: 160; loss: 1.81; acc: 0.47
Batch: 180; loss: 1.72; acc: 0.55
Batch: 200; loss: 1.86; acc: 0.44
Batch: 220; loss: 1.72; acc: 0.56
Batch: 240; loss: 1.71; acc: 0.58
Batch: 260; loss: 1.69; acc: 0.56
Batch: 280; loss: 1.77; acc: 0.5
Batch: 300; loss: 1.65; acc: 0.62
Batch: 320; loss: 1.75; acc: 0.5
Batch: 340; loss: 1.72; acc: 0.61
Batch: 360; loss: 1.8; acc: 0.5
Batch: 380; loss: 1.79; acc: 0.53
Batch: 400; loss: 1.77; acc: 0.5
Batch: 420; loss: 1.8; acc: 0.5
Batch: 440; loss: 1.8; acc: 0.53
Batch: 460; loss: 1.75; acc: 0.48
Batch: 480; loss: 1.84; acc: 0.52
Batch: 500; loss: 1.75; acc: 0.5
Batch: 520; loss: 1.63; acc: 0.56
Batch: 540; loss: 1.91; acc: 0.36
Batch: 560; loss: 1.77; acc: 0.45
Batch: 580; loss: 1.73; acc: 0.5
Batch: 600; loss: 1.74; acc: 0.5
Batch: 620; loss: 1.72; acc: 0.56
Batch: 640; loss: 1.73; acc: 0.47
Batch: 660; loss: 1.86; acc: 0.47
Batch: 680; loss: 1.67; acc: 0.55
Batch: 700; loss: 1.79; acc: 0.48
Batch: 720; loss: 1.78; acc: 0.48
Batch: 740; loss: 1.79; acc: 0.45
Batch: 760; loss: 1.88; acc: 0.44
Batch: 780; loss: 1.72; acc: 0.58
Train Epoch over. train_loss: 1.78; train_accuracy: 0.49 

2.7775919079431333e-05
6.732055226166267e-06
Batch: 0; loss: 1.92; acc: 0.41
Batch: 20; loss: 1.78; acc: 0.53
Batch: 40; loss: 1.59; acc: 0.66
Batch: 60; loss: 1.78; acc: 0.48
Batch: 80; loss: 1.7; acc: 0.55
Batch: 100; loss: 1.83; acc: 0.44
Batch: 120; loss: 1.88; acc: 0.38
Batch: 140; loss: 1.6; acc: 0.61
Val Epoch over. val_loss: 1.7618470260292103; val_accuracy: 0.5146297770700637 

The current subspace-distance is: 6.732055226166267e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.55
Batch: 20; loss: 1.79; acc: 0.52
Batch: 40; loss: 1.73; acc: 0.56
Batch: 60; loss: 1.74; acc: 0.52
Batch: 80; loss: 1.62; acc: 0.62
Batch: 100; loss: 1.76; acc: 0.5
Batch: 120; loss: 1.64; acc: 0.61
Batch: 140; loss: 1.72; acc: 0.52
Batch: 160; loss: 1.79; acc: 0.53
Batch: 180; loss: 1.73; acc: 0.53
Batch: 200; loss: 1.78; acc: 0.5
Batch: 220; loss: 1.76; acc: 0.52
Batch: 240; loss: 1.79; acc: 0.44
Batch: 260; loss: 1.79; acc: 0.52
Batch: 280; loss: 1.8; acc: 0.53
Batch: 300; loss: 1.9; acc: 0.42
Batch: 320; loss: 1.77; acc: 0.44
Batch: 340; loss: 1.7; acc: 0.5
Batch: 360; loss: 1.74; acc: 0.53
Batch: 380; loss: 1.87; acc: 0.47
Batch: 400; loss: 1.84; acc: 0.41
Batch: 420; loss: 1.89; acc: 0.42
Batch: 440; loss: 1.83; acc: 0.47
Batch: 460; loss: 1.76; acc: 0.53
Batch: 480; loss: 1.77; acc: 0.42
Batch: 500; loss: 1.67; acc: 0.52
Batch: 520; loss: 1.78; acc: 0.47
Batch: 540; loss: 1.78; acc: 0.42
Batch: 560; loss: 1.75; acc: 0.55
Batch: 580; loss: 1.82; acc: 0.39
Batch: 600; loss: 1.88; acc: 0.33
Batch: 620; loss: 1.75; acc: 0.48
Batch: 640; loss: 1.92; acc: 0.42
Batch: 660; loss: 1.72; acc: 0.52
Batch: 680; loss: 1.83; acc: 0.41
Batch: 700; loss: 1.8; acc: 0.53
Batch: 720; loss: 1.77; acc: 0.45
Batch: 740; loss: 1.82; acc: 0.45
Batch: 760; loss: 1.73; acc: 0.45
Batch: 780; loss: 1.69; acc: 0.5
Train Epoch over. train_loss: 1.78; train_accuracy: 0.49 

2.8765412935172208e-05
8.441837053396739e-06
Batch: 0; loss: 1.9; acc: 0.41
Batch: 20; loss: 1.75; acc: 0.5
Batch: 40; loss: 1.58; acc: 0.62
Batch: 60; loss: 1.78; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.55
Batch: 100; loss: 1.82; acc: 0.47
Batch: 120; loss: 1.87; acc: 0.34
Batch: 140; loss: 1.58; acc: 0.62
Val Epoch over. val_loss: 1.7480986012015374; val_accuracy: 0.5164211783439491 

The current subspace-distance is: 8.441837053396739e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.59
Batch: 20; loss: 1.75; acc: 0.45
Batch: 40; loss: 1.67; acc: 0.58
Batch: 60; loss: 1.78; acc: 0.5
Batch: 80; loss: 1.82; acc: 0.41
Batch: 100; loss: 1.75; acc: 0.47
Batch: 120; loss: 1.83; acc: 0.38
Batch: 140; loss: 1.7; acc: 0.59
Batch: 160; loss: 1.67; acc: 0.58
Batch: 180; loss: 1.76; acc: 0.53
Batch: 200; loss: 1.68; acc: 0.64
Batch: 220; loss: 1.66; acc: 0.52
Batch: 240; loss: 1.76; acc: 0.48
Batch: 260; loss: 1.79; acc: 0.48
Batch: 280; loss: 1.77; acc: 0.5
Batch: 300; loss: 1.81; acc: 0.44
Batch: 320; loss: 1.77; acc: 0.53
Batch: 340; loss: 1.73; acc: 0.56
Batch: 360; loss: 1.79; acc: 0.44
Batch: 380; loss: 1.82; acc: 0.47
Batch: 400; loss: 1.65; acc: 0.58
Batch: 420; loss: 1.82; acc: 0.48
Batch: 440; loss: 1.92; acc: 0.38
Batch: 460; loss: 1.69; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.56
Batch: 500; loss: 1.78; acc: 0.53
Batch: 520; loss: 1.82; acc: 0.45
Batch: 540; loss: 1.73; acc: 0.48
Batch: 560; loss: 1.83; acc: 0.41
Batch: 580; loss: 1.75; acc: 0.52
Batch: 600; loss: 1.81; acc: 0.47
Batch: 620; loss: 1.78; acc: 0.41
Batch: 640; loss: 1.92; acc: 0.34
Batch: 660; loss: 1.79; acc: 0.42
Batch: 680; loss: 1.74; acc: 0.5
Batch: 700; loss: 1.76; acc: 0.48
Batch: 720; loss: 1.7; acc: 0.59
Batch: 740; loss: 1.84; acc: 0.44
Batch: 760; loss: 1.8; acc: 0.44
Batch: 780; loss: 1.86; acc: 0.38
Train Epoch over. train_loss: 1.77; train_accuracy: 0.49 

2.950184170913417e-05
8.762156539887656e-06
Batch: 0; loss: 1.89; acc: 0.38
Batch: 20; loss: 1.72; acc: 0.47
Batch: 40; loss: 1.58; acc: 0.61
Batch: 60; loss: 1.78; acc: 0.5
Batch: 80; loss: 1.64; acc: 0.53
Batch: 100; loss: 1.8; acc: 0.44
Batch: 120; loss: 1.89; acc: 0.34
Batch: 140; loss: 1.56; acc: 0.64
Val Epoch over. val_loss: 1.7359255119493813; val_accuracy: 0.5113455414012739 

The current subspace-distance is: 8.762156539887656e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.5
Batch: 20; loss: 1.83; acc: 0.5
Batch: 40; loss: 1.75; acc: 0.48
Batch: 60; loss: 1.78; acc: 0.44
Batch: 80; loss: 1.74; acc: 0.45
Batch: 100; loss: 1.72; acc: 0.52
Batch: 120; loss: 1.69; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.48
Batch: 160; loss: 1.77; acc: 0.42
Batch: 180; loss: 1.9; acc: 0.41
Batch: 200; loss: 1.75; acc: 0.52
Batch: 220; loss: 1.64; acc: 0.56
Batch: 240; loss: 1.81; acc: 0.48
Batch: 260; loss: 1.82; acc: 0.41
Batch: 280; loss: 1.73; acc: 0.47
Batch: 300; loss: 1.85; acc: 0.41
Batch: 320; loss: 1.76; acc: 0.5
Batch: 340; loss: 1.84; acc: 0.52
Batch: 360; loss: 1.72; acc: 0.55
Batch: 380; loss: 1.75; acc: 0.52
Batch: 400; loss: 1.78; acc: 0.44
Batch: 420; loss: 1.78; acc: 0.48
Batch: 440; loss: 1.74; acc: 0.52
Batch: 460; loss: 1.74; acc: 0.5
Batch: 480; loss: 1.69; acc: 0.48
Batch: 500; loss: 1.8; acc: 0.45
Batch: 520; loss: 1.73; acc: 0.45
Batch: 540; loss: 1.78; acc: 0.52
Batch: 560; loss: 1.79; acc: 0.42
Batch: 580; loss: 1.67; acc: 0.47
Batch: 600; loss: 1.75; acc: 0.45
Batch: 620; loss: 1.74; acc: 0.44
Batch: 640; loss: 1.67; acc: 0.55
Batch: 660; loss: 1.6; acc: 0.56
Batch: 680; loss: 1.87; acc: 0.45
Batch: 700; loss: 1.71; acc: 0.53
Batch: 720; loss: 1.72; acc: 0.55
Batch: 740; loss: 1.74; acc: 0.53
Batch: 760; loss: 1.75; acc: 0.52
Batch: 780; loss: 1.71; acc: 0.55
Train Epoch over. train_loss: 1.75; train_accuracy: 0.5 

3.0232204153435305e-05
9.174083061225247e-06
Batch: 0; loss: 1.87; acc: 0.34
Batch: 20; loss: 1.7; acc: 0.53
Batch: 40; loss: 1.6; acc: 0.62
Batch: 60; loss: 1.78; acc: 0.48
Batch: 80; loss: 1.61; acc: 0.58
Batch: 100; loss: 1.78; acc: 0.5
Batch: 120; loss: 1.88; acc: 0.36
Batch: 140; loss: 1.56; acc: 0.64
Val Epoch over. val_loss: 1.7238999855746129; val_accuracy: 0.5121417197452229 

The current subspace-distance is: 9.174083061225247e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.48
Batch: 20; loss: 1.72; acc: 0.52
Batch: 40; loss: 1.66; acc: 0.5
Batch: 60; loss: 1.71; acc: 0.47
Batch: 80; loss: 1.82; acc: 0.45
Batch: 100; loss: 1.73; acc: 0.48
Batch: 120; loss: 1.81; acc: 0.5
Batch: 140; loss: 1.72; acc: 0.48
Batch: 160; loss: 1.77; acc: 0.48
Batch: 180; loss: 1.75; acc: 0.53
Batch: 200; loss: 1.62; acc: 0.58
Batch: 220; loss: 1.69; acc: 0.58
Batch: 240; loss: 1.69; acc: 0.55
Batch: 260; loss: 1.74; acc: 0.52
Batch: 280; loss: 1.84; acc: 0.45
Batch: 300; loss: 1.76; acc: 0.41
Batch: 320; loss: 1.6; acc: 0.58
Batch: 340; loss: 1.79; acc: 0.48
Batch: 360; loss: 1.84; acc: 0.39
Batch: 380; loss: 1.72; acc: 0.48
Batch: 400; loss: 1.71; acc: 0.52
Batch: 420; loss: 1.69; acc: 0.48
Batch: 440; loss: 1.85; acc: 0.44
Batch: 460; loss: 1.9; acc: 0.38
Batch: 480; loss: 1.66; acc: 0.53
Batch: 500; loss: 1.77; acc: 0.45
Batch: 520; loss: 1.77; acc: 0.53
Batch: 540; loss: 1.62; acc: 0.61
Batch: 560; loss: 1.8; acc: 0.41
Batch: 580; loss: 1.7; acc: 0.56
Batch: 600; loss: 1.74; acc: 0.44
Batch: 620; loss: 1.72; acc: 0.53
Batch: 640; loss: 1.68; acc: 0.58
Batch: 660; loss: 1.8; acc: 0.48
Batch: 680; loss: 1.74; acc: 0.41
Batch: 700; loss: 1.81; acc: 0.45
Batch: 720; loss: 1.76; acc: 0.56
Batch: 740; loss: 1.69; acc: 0.48
Batch: 760; loss: 1.66; acc: 0.62
Batch: 780; loss: 1.79; acc: 0.5
Train Epoch over. train_loss: 1.72; train_accuracy: 0.51 

3.1756753742229193e-05
9.627212421037257e-06
Batch: 0; loss: 1.81; acc: 0.39
Batch: 20; loss: 1.66; acc: 0.5
Batch: 40; loss: 1.56; acc: 0.61
Batch: 60; loss: 1.73; acc: 0.5
Batch: 80; loss: 1.57; acc: 0.64
Batch: 100; loss: 1.72; acc: 0.55
Batch: 120; loss: 1.86; acc: 0.36
Batch: 140; loss: 1.53; acc: 0.61
Val Epoch over. val_loss: 1.6844811090238534; val_accuracy: 0.5269705414012739 

The current subspace-distance is: 9.627212421037257e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.75; acc: 0.48
Batch: 20; loss: 1.69; acc: 0.52
Batch: 40; loss: 1.64; acc: 0.52
Batch: 60; loss: 1.71; acc: 0.48
Batch: 80; loss: 1.73; acc: 0.45
Batch: 100; loss: 1.62; acc: 0.56
Batch: 120; loss: 1.65; acc: 0.61
Batch: 140; loss: 1.8; acc: 0.42
Batch: 160; loss: 1.65; acc: 0.5
Batch: 180; loss: 1.73; acc: 0.53
Batch: 200; loss: 1.68; acc: 0.5
Batch: 220; loss: 1.65; acc: 0.5
Batch: 240; loss: 1.73; acc: 0.47
Batch: 260; loss: 1.59; acc: 0.56
Batch: 280; loss: 1.63; acc: 0.58
Batch: 300; loss: 1.63; acc: 0.48
Batch: 320; loss: 1.56; acc: 0.58
Batch: 340; loss: 1.71; acc: 0.55
Batch: 360; loss: 1.51; acc: 0.61
Batch: 380; loss: 1.89; acc: 0.42
Batch: 400; loss: 1.7; acc: 0.56
Batch: 420; loss: 1.68; acc: 0.53
Batch: 440; loss: 1.61; acc: 0.59
Batch: 460; loss: 1.78; acc: 0.39
Batch: 480; loss: 1.72; acc: 0.45
Batch: 500; loss: 1.71; acc: 0.41
Batch: 520; loss: 1.78; acc: 0.44
Batch: 540; loss: 1.66; acc: 0.5
Batch: 560; loss: 1.69; acc: 0.5
Batch: 580; loss: 1.57; acc: 0.59
Batch: 600; loss: 1.78; acc: 0.45
Batch: 620; loss: 1.7; acc: 0.47
Batch: 640; loss: 1.6; acc: 0.52
Batch: 660; loss: 1.59; acc: 0.62
Batch: 680; loss: 1.63; acc: 0.55
Batch: 700; loss: 1.61; acc: 0.53
Batch: 720; loss: 1.65; acc: 0.53
Batch: 740; loss: 1.72; acc: 0.55
Batch: 760; loss: 1.69; acc: 0.55
Batch: 780; loss: 1.66; acc: 0.53
Train Epoch over. train_loss: 1.68; train_accuracy: 0.52 

3.2939955417532474e-05
8.926519512897357e-06
Batch: 0; loss: 1.76; acc: 0.42
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.51; acc: 0.67
Batch: 60; loss: 1.67; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.59
Batch: 100; loss: 1.68; acc: 0.56
Batch: 120; loss: 1.79; acc: 0.45
Batch: 140; loss: 1.49; acc: 0.62
Val Epoch over. val_loss: 1.639585557257294; val_accuracy: 0.5428941082802548 

The current subspace-distance is: 8.926519512897357e-06 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.47
Batch: 20; loss: 1.75; acc: 0.48
Batch: 40; loss: 1.55; acc: 0.62
Batch: 60; loss: 1.75; acc: 0.48
Batch: 80; loss: 1.8; acc: 0.42
Batch: 100; loss: 1.66; acc: 0.53
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.71; acc: 0.44
Batch: 160; loss: 1.63; acc: 0.62
Batch: 180; loss: 1.73; acc: 0.47
Batch: 200; loss: 1.74; acc: 0.47
Batch: 220; loss: 1.68; acc: 0.55
Batch: 240; loss: 1.66; acc: 0.59
Batch: 260; loss: 1.62; acc: 0.55
Batch: 280; loss: 1.9; acc: 0.39
Batch: 300; loss: 1.61; acc: 0.52
Batch: 320; loss: 1.67; acc: 0.52
Batch: 340; loss: 1.66; acc: 0.5
Batch: 360; loss: 1.7; acc: 0.56
Batch: 380; loss: 1.57; acc: 0.58
Batch: 400; loss: 1.74; acc: 0.44
Batch: 420; loss: 1.65; acc: 0.55
Batch: 440; loss: 1.58; acc: 0.59
Batch: 460; loss: 1.65; acc: 0.53
Batch: 480; loss: 1.56; acc: 0.66
Batch: 500; loss: 1.73; acc: 0.45
Batch: 520; loss: 1.71; acc: 0.52
Batch: 540; loss: 1.67; acc: 0.56
Batch: 560; loss: 1.58; acc: 0.58
Batch: 580; loss: 1.53; acc: 0.64
Batch: 600; loss: 1.58; acc: 0.61
Batch: 620; loss: 1.65; acc: 0.52
Batch: 640; loss: 1.73; acc: 0.47
Batch: 660; loss: 1.54; acc: 0.64
Batch: 680; loss: 1.6; acc: 0.61
Batch: 700; loss: 1.64; acc: 0.52
Batch: 720; loss: 1.51; acc: 0.67
Batch: 740; loss: 1.68; acc: 0.39
Batch: 760; loss: 1.66; acc: 0.53
Batch: 780; loss: 1.6; acc: 0.55
Train Epoch over. train_loss: 1.65; train_accuracy: 0.53 

3.4761978895403445e-05
1.0186467989115044e-05
Batch: 0; loss: 1.73; acc: 0.45
Batch: 20; loss: 1.62; acc: 0.55
Batch: 40; loss: 1.49; acc: 0.7
Batch: 60; loss: 1.65; acc: 0.58
Batch: 80; loss: 1.53; acc: 0.62
Batch: 100; loss: 1.66; acc: 0.56
Batch: 120; loss: 1.76; acc: 0.42
Batch: 140; loss: 1.47; acc: 0.62
Val Epoch over. val_loss: 1.6113301356127308; val_accuracy: 0.5527468152866242 

The current subspace-distance is: 1.0186467989115044e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.83; acc: 0.44
Batch: 20; loss: 1.76; acc: 0.48
Batch: 40; loss: 1.54; acc: 0.61
Batch: 60; loss: 1.68; acc: 0.45
Batch: 80; loss: 1.61; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.53
Batch: 120; loss: 1.59; acc: 0.48
Batch: 140; loss: 1.52; acc: 0.59
Batch: 160; loss: 1.58; acc: 0.66
Batch: 180; loss: 1.83; acc: 0.39
Batch: 200; loss: 1.62; acc: 0.52
Batch: 220; loss: 1.59; acc: 0.5
Batch: 240; loss: 1.57; acc: 0.62
Batch: 260; loss: 1.66; acc: 0.53
Batch: 280; loss: 1.59; acc: 0.62
Batch: 300; loss: 1.74; acc: 0.42
Batch: 320; loss: 1.54; acc: 0.64
Batch: 340; loss: 1.64; acc: 0.58
Batch: 360; loss: 1.58; acc: 0.5
Batch: 380; loss: 1.65; acc: 0.59
Batch: 400; loss: 1.64; acc: 0.5
Batch: 420; loss: 1.74; acc: 0.47
Batch: 440; loss: 1.52; acc: 0.67
Batch: 460; loss: 1.62; acc: 0.52
Batch: 480; loss: 1.72; acc: 0.48
Batch: 500; loss: 1.7; acc: 0.48
Batch: 520; loss: 1.58; acc: 0.59
Batch: 540; loss: 1.57; acc: 0.56
Batch: 560; loss: 1.63; acc: 0.58
Batch: 580; loss: 1.61; acc: 0.55
Batch: 600; loss: 1.56; acc: 0.58
Batch: 620; loss: 1.61; acc: 0.58
Batch: 640; loss: 1.62; acc: 0.58
Batch: 660; loss: 1.62; acc: 0.52
Batch: 680; loss: 1.56; acc: 0.64
Batch: 700; loss: 1.59; acc: 0.5
Batch: 720; loss: 1.73; acc: 0.58
Batch: 740; loss: 1.61; acc: 0.58
Batch: 760; loss: 1.66; acc: 0.53
Batch: 780; loss: 1.57; acc: 0.56
Train Epoch over. train_loss: 1.64; train_accuracy: 0.54 

3.546731750248e-05
9.670700819697231e-06
Batch: 0; loss: 1.74; acc: 0.42
Batch: 20; loss: 1.65; acc: 0.53
Batch: 40; loss: 1.48; acc: 0.67
Batch: 60; loss: 1.66; acc: 0.55
Batch: 80; loss: 1.54; acc: 0.67
Batch: 100; loss: 1.68; acc: 0.52
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.48; acc: 0.62
Val Epoch over. val_loss: 1.6150477699413421; val_accuracy: 0.5486664012738853 

The current subspace-distance is: 9.670700819697231e-06 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.73; acc: 0.39
Batch: 20; loss: 1.75; acc: 0.48
Batch: 40; loss: 1.52; acc: 0.66
Batch: 60; loss: 1.69; acc: 0.48
Batch: 80; loss: 1.55; acc: 0.58
Batch: 100; loss: 1.63; acc: 0.5
Batch: 120; loss: 1.68; acc: 0.5
Batch: 140; loss: 1.82; acc: 0.44
Batch: 160; loss: 1.57; acc: 0.62
Batch: 180; loss: 1.71; acc: 0.5
Batch: 200; loss: 1.64; acc: 0.52
Batch: 220; loss: 1.64; acc: 0.55
Batch: 240; loss: 1.58; acc: 0.53
Batch: 260; loss: 1.72; acc: 0.48
Batch: 280; loss: 1.59; acc: 0.53
Batch: 300; loss: 1.73; acc: 0.48
Batch: 320; loss: 1.6; acc: 0.56
Batch: 340; loss: 1.55; acc: 0.53
Batch: 360; loss: 1.58; acc: 0.64
Batch: 380; loss: 1.76; acc: 0.48
Batch: 400; loss: 1.48; acc: 0.55
Batch: 420; loss: 1.66; acc: 0.48
Batch: 440; loss: 1.56; acc: 0.55
Batch: 460; loss: 1.47; acc: 0.66
Batch: 480; loss: 1.61; acc: 0.52
Batch: 500; loss: 1.56; acc: 0.62
Batch: 520; loss: 1.67; acc: 0.53
Batch: 540; loss: 1.53; acc: 0.67
Batch: 560; loss: 1.59; acc: 0.5
Batch: 580; loss: 1.72; acc: 0.55
Batch: 600; loss: 1.66; acc: 0.5
Batch: 620; loss: 1.5; acc: 0.69
Batch: 640; loss: 1.65; acc: 0.48
Batch: 660; loss: 1.58; acc: 0.61
Batch: 680; loss: 1.59; acc: 0.53
Batch: 700; loss: 1.6; acc: 0.55
Batch: 720; loss: 1.73; acc: 0.5
Batch: 740; loss: 1.62; acc: 0.61
Batch: 760; loss: 1.79; acc: 0.47
Batch: 780; loss: 1.59; acc: 0.59
Train Epoch over. train_loss: 1.63; train_accuracy: 0.54 

3.645740798674524e-05
1.1083305253123399e-05
Batch: 0; loss: 1.72; acc: 0.47
Batch: 20; loss: 1.62; acc: 0.55
Batch: 40; loss: 1.47; acc: 0.67
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.52; acc: 0.62
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.75; acc: 0.47
Batch: 140; loss: 1.45; acc: 0.61
Val Epoch over. val_loss: 1.598832387833079; val_accuracy: 0.5544386942675159 

The current subspace-distance is: 1.1083305253123399e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.8; acc: 0.38
Batch: 20; loss: 1.47; acc: 0.69
Batch: 40; loss: 1.57; acc: 0.61
Batch: 60; loss: 1.63; acc: 0.56
Batch: 80; loss: 1.61; acc: 0.48
Batch: 100; loss: 1.47; acc: 0.75
Batch: 120; loss: 1.68; acc: 0.55
Batch: 140; loss: 1.59; acc: 0.58
Batch: 160; loss: 1.67; acc: 0.47
Batch: 180; loss: 1.64; acc: 0.52
Batch: 200; loss: 1.63; acc: 0.5
Batch: 220; loss: 1.52; acc: 0.59
Batch: 240; loss: 1.67; acc: 0.48
Batch: 260; loss: 1.66; acc: 0.45
Batch: 280; loss: 1.71; acc: 0.42
Batch: 300; loss: 1.63; acc: 0.55
Batch: 320; loss: 1.63; acc: 0.56
Batch: 340; loss: 1.49; acc: 0.69
Batch: 360; loss: 1.73; acc: 0.48
Batch: 380; loss: 1.73; acc: 0.38
Batch: 400; loss: 1.66; acc: 0.58
Batch: 420; loss: 1.53; acc: 0.62
Batch: 440; loss: 1.62; acc: 0.53
Batch: 460; loss: 1.6; acc: 0.56
Batch: 480; loss: 1.54; acc: 0.56
Batch: 500; loss: 1.61; acc: 0.53
Batch: 520; loss: 1.57; acc: 0.64
Batch: 540; loss: 1.66; acc: 0.52
Batch: 560; loss: 1.62; acc: 0.5
Batch: 580; loss: 1.64; acc: 0.52
Batch: 600; loss: 1.69; acc: 0.5
Batch: 620; loss: 1.71; acc: 0.55
Batch: 640; loss: 1.64; acc: 0.48
Batch: 660; loss: 1.61; acc: 0.53
Batch: 680; loss: 1.56; acc: 0.61
Batch: 700; loss: 1.69; acc: 0.47
Batch: 720; loss: 1.71; acc: 0.47
Batch: 740; loss: 1.67; acc: 0.48
Batch: 760; loss: 1.67; acc: 0.45
Batch: 780; loss: 1.52; acc: 0.66
Train Epoch over. train_loss: 1.62; train_accuracy: 0.54 

3.6530800571199507e-05
1.1157236258441117e-05
Batch: 0; loss: 1.71; acc: 0.5
Batch: 20; loss: 1.65; acc: 0.56
Batch: 40; loss: 1.47; acc: 0.64
Batch: 60; loss: 1.65; acc: 0.52
Batch: 80; loss: 1.51; acc: 0.64
Batch: 100; loss: 1.67; acc: 0.5
Batch: 120; loss: 1.76; acc: 0.48
Batch: 140; loss: 1.45; acc: 0.62
Val Epoch over. val_loss: 1.5986763018711356; val_accuracy: 0.5521496815286624 

The current subspace-distance is: 1.1157236258441117e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.82; acc: 0.3
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.61; acc: 0.48
Batch: 60; loss: 1.62; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.52
Batch: 100; loss: 1.6; acc: 0.47
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.45; acc: 0.61
Batch: 160; loss: 1.73; acc: 0.45
Batch: 180; loss: 1.65; acc: 0.5
Batch: 200; loss: 1.61; acc: 0.56
Batch: 220; loss: 1.62; acc: 0.5
Batch: 240; loss: 1.73; acc: 0.45
Batch: 260; loss: 1.72; acc: 0.44
Batch: 280; loss: 1.73; acc: 0.48
Batch: 300; loss: 1.58; acc: 0.5
Batch: 320; loss: 1.59; acc: 0.56
Batch: 340; loss: 1.65; acc: 0.52
Batch: 360; loss: 1.49; acc: 0.64
Batch: 380; loss: 1.61; acc: 0.52
Batch: 400; loss: 1.58; acc: 0.48
Batch: 420; loss: 1.67; acc: 0.5
Batch: 440; loss: 1.64; acc: 0.56
Batch: 460; loss: 1.59; acc: 0.56
Batch: 480; loss: 1.61; acc: 0.53
Batch: 500; loss: 1.59; acc: 0.53
Batch: 520; loss: 1.55; acc: 0.59
Batch: 540; loss: 1.71; acc: 0.48
Batch: 560; loss: 1.66; acc: 0.47
Batch: 580; loss: 1.81; acc: 0.39
Batch: 600; loss: 1.6; acc: 0.66
Batch: 620; loss: 1.72; acc: 0.5
Batch: 640; loss: 1.58; acc: 0.52
Batch: 660; loss: 1.46; acc: 0.67
Batch: 680; loss: 1.58; acc: 0.59
Batch: 700; loss: 1.6; acc: 0.56
Batch: 720; loss: 1.69; acc: 0.56
Batch: 740; loss: 1.62; acc: 0.48
Batch: 760; loss: 1.71; acc: 0.45
Batch: 780; loss: 1.58; acc: 0.52
Train Epoch over. train_loss: 1.62; train_accuracy: 0.54 

3.7187939597060904e-05
1.1541509593371302e-05
Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.47; acc: 0.66
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.51; acc: 0.61
Batch: 100; loss: 1.67; acc: 0.48
Batch: 120; loss: 1.76; acc: 0.45
Batch: 140; loss: 1.45; acc: 0.62
Val Epoch over. val_loss: 1.5978257777584586; val_accuracy: 0.5500597133757962 

The current subspace-distance is: 1.1541509593371302e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.45; acc: 0.61
Batch: 20; loss: 1.68; acc: 0.52
Batch: 40; loss: 1.53; acc: 0.61
Batch: 60; loss: 1.7; acc: 0.53
Batch: 80; loss: 1.64; acc: 0.56
Batch: 100; loss: 1.61; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.56
Batch: 140; loss: 1.49; acc: 0.61
Batch: 160; loss: 1.65; acc: 0.47
Batch: 180; loss: 1.6; acc: 0.59
Batch: 200; loss: 1.52; acc: 0.66
Batch: 220; loss: 1.52; acc: 0.64
Batch: 240; loss: 1.63; acc: 0.59
Batch: 260; loss: 1.63; acc: 0.53
Batch: 280; loss: 1.61; acc: 0.53
Batch: 300; loss: 1.63; acc: 0.55
Batch: 320; loss: 1.61; acc: 0.55
Batch: 340; loss: 1.66; acc: 0.47
Batch: 360; loss: 1.54; acc: 0.59
Batch: 380; loss: 1.56; acc: 0.55
Batch: 400; loss: 1.53; acc: 0.58
Batch: 420; loss: 1.63; acc: 0.55
Batch: 440; loss: 1.66; acc: 0.56
Batch: 460; loss: 1.63; acc: 0.55
Batch: 480; loss: 1.76; acc: 0.45
Batch: 500; loss: 1.63; acc: 0.45
Batch: 520; loss: 1.67; acc: 0.5
Batch: 540; loss: 1.74; acc: 0.39
Batch: 560; loss: 1.64; acc: 0.52
Batch: 580; loss: 1.55; acc: 0.64
Batch: 600; loss: 1.76; acc: 0.45
Batch: 620; loss: 1.71; acc: 0.44
Batch: 640; loss: 1.68; acc: 0.47
Batch: 660; loss: 1.69; acc: 0.44
Batch: 680; loss: 1.5; acc: 0.61
Batch: 700; loss: 1.61; acc: 0.48
Batch: 720; loss: 1.55; acc: 0.56
Batch: 740; loss: 1.56; acc: 0.58
Batch: 760; loss: 1.6; acc: 0.56
Batch: 780; loss: 1.76; acc: 0.42
Train Epoch over. train_loss: 1.62; train_accuracy: 0.54 

3.722055407706648e-05
1.2305649761401583e-05
Batch: 0; loss: 1.7; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.47; acc: 0.64
Batch: 60; loss: 1.64; acc: 0.5
Batch: 80; loss: 1.49; acc: 0.61
Batch: 100; loss: 1.66; acc: 0.5
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.43; acc: 0.62
Val Epoch over. val_loss: 1.5865116172535405; val_accuracy: 0.5536425159235668 

The current subspace-distance is: 1.2305649761401583e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.58
Batch: 20; loss: 1.63; acc: 0.5
Batch: 40; loss: 1.59; acc: 0.59
Batch: 60; loss: 1.64; acc: 0.55
Batch: 80; loss: 1.63; acc: 0.5
Batch: 100; loss: 1.64; acc: 0.53
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.65; acc: 0.5
Batch: 160; loss: 1.65; acc: 0.58
Batch: 180; loss: 1.64; acc: 0.58
Batch: 200; loss: 1.57; acc: 0.55
Batch: 220; loss: 1.6; acc: 0.53
Batch: 240; loss: 1.72; acc: 0.53
Batch: 260; loss: 1.51; acc: 0.61
Batch: 280; loss: 1.53; acc: 0.66
Batch: 300; loss: 1.74; acc: 0.47
Batch: 320; loss: 1.53; acc: 0.67
Batch: 340; loss: 1.78; acc: 0.39
Batch: 360; loss: 1.64; acc: 0.52
Batch: 380; loss: 1.59; acc: 0.55
Batch: 400; loss: 1.57; acc: 0.52
Batch: 420; loss: 1.57; acc: 0.58
Batch: 440; loss: 1.56; acc: 0.5
Batch: 460; loss: 1.65; acc: 0.55
Batch: 480; loss: 1.56; acc: 0.58
Batch: 500; loss: 1.59; acc: 0.56
Batch: 520; loss: 1.64; acc: 0.56
Batch: 540; loss: 1.7; acc: 0.55
Batch: 560; loss: 1.64; acc: 0.55
Batch: 580; loss: 1.61; acc: 0.56
Batch: 600; loss: 1.68; acc: 0.59
Batch: 620; loss: 1.52; acc: 0.58
Batch: 640; loss: 1.7; acc: 0.38
Batch: 660; loss: 1.61; acc: 0.5
Batch: 680; loss: 1.62; acc: 0.48
Batch: 700; loss: 1.64; acc: 0.55
Batch: 720; loss: 1.63; acc: 0.58
Batch: 740; loss: 1.61; acc: 0.48
Batch: 760; loss: 1.46; acc: 0.56
Batch: 780; loss: 1.65; acc: 0.5
Train Epoch over. train_loss: 1.61; train_accuracy: 0.54 

3.826003012363799e-05
1.2504018741310574e-05
Batch: 0; loss: 1.7; acc: 0.47
Batch: 20; loss: 1.65; acc: 0.52
Batch: 40; loss: 1.45; acc: 0.69
Batch: 60; loss: 1.64; acc: 0.5
Batch: 80; loss: 1.48; acc: 0.62
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.76; acc: 0.45
Batch: 140; loss: 1.43; acc: 0.62
Val Epoch over. val_loss: 1.5840286396111651; val_accuracy: 0.5572253184713376 

The current subspace-distance is: 1.2504018741310574e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.58; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.5
Batch: 40; loss: 1.65; acc: 0.48
Batch: 60; loss: 1.59; acc: 0.5
Batch: 80; loss: 1.58; acc: 0.55
Batch: 100; loss: 1.55; acc: 0.55
Batch: 120; loss: 1.65; acc: 0.44
Batch: 140; loss: 1.56; acc: 0.59
Batch: 160; loss: 1.65; acc: 0.55
Batch: 180; loss: 1.61; acc: 0.53
Batch: 200; loss: 1.66; acc: 0.58
Batch: 220; loss: 1.67; acc: 0.5
Batch: 240; loss: 1.61; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.56
Batch: 280; loss: 1.72; acc: 0.45
Batch: 300; loss: 1.65; acc: 0.52
Batch: 320; loss: 1.59; acc: 0.56
Batch: 340; loss: 1.59; acc: 0.55
Batch: 360; loss: 1.71; acc: 0.47
Batch: 380; loss: 1.62; acc: 0.55
Batch: 400; loss: 1.6; acc: 0.48
Batch: 420; loss: 1.61; acc: 0.56
Batch: 440; loss: 1.6; acc: 0.52
Batch: 460; loss: 1.55; acc: 0.58
Batch: 480; loss: 1.64; acc: 0.48
Batch: 500; loss: 1.57; acc: 0.59
Batch: 520; loss: 1.5; acc: 0.59
Batch: 540; loss: 1.58; acc: 0.48
Batch: 560; loss: 1.52; acc: 0.56
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.47; acc: 0.66
Batch: 620; loss: 1.5; acc: 0.64
Batch: 640; loss: 1.57; acc: 0.53
Batch: 660; loss: 1.81; acc: 0.42
Batch: 680; loss: 1.59; acc: 0.55
Batch: 700; loss: 1.6; acc: 0.59
Batch: 720; loss: 1.6; acc: 0.52
Batch: 740; loss: 1.66; acc: 0.53
Batch: 760; loss: 1.62; acc: 0.53
Batch: 780; loss: 1.65; acc: 0.56
Train Epoch over. train_loss: 1.61; train_accuracy: 0.54 

3.866585757350549e-05
1.3053635484538972e-05
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.47; acc: 0.66
Batch: 60; loss: 1.66; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.62
Batch: 100; loss: 1.67; acc: 0.48
Batch: 120; loss: 1.76; acc: 0.5
Batch: 140; loss: 1.43; acc: 0.62
Val Epoch over. val_loss: 1.5912655470477548; val_accuracy: 0.5499601910828026 

The current subspace-distance is: 1.3053635484538972e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.77; acc: 0.41
Batch: 40; loss: 1.69; acc: 0.55
Batch: 60; loss: 1.7; acc: 0.53
Batch: 80; loss: 1.54; acc: 0.58
Batch: 100; loss: 1.65; acc: 0.48
Batch: 120; loss: 1.72; acc: 0.55
Batch: 140; loss: 1.67; acc: 0.45
Batch: 160; loss: 1.58; acc: 0.53
Batch: 180; loss: 1.65; acc: 0.52
Batch: 200; loss: 1.62; acc: 0.53
Batch: 220; loss: 1.61; acc: 0.58
Batch: 240; loss: 1.67; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.48
Batch: 280; loss: 1.61; acc: 0.55
Batch: 300; loss: 1.69; acc: 0.47
Batch: 320; loss: 1.67; acc: 0.52
Batch: 340; loss: 1.53; acc: 0.61
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 1.67; acc: 0.44
Batch: 400; loss: 1.64; acc: 0.5
Batch: 420; loss: 1.52; acc: 0.62
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.79; acc: 0.36
Batch: 480; loss: 1.54; acc: 0.58
Batch: 500; loss: 1.74; acc: 0.45
Batch: 520; loss: 1.58; acc: 0.56
Batch: 540; loss: 1.51; acc: 0.59
Batch: 560; loss: 1.61; acc: 0.55
Batch: 580; loss: 1.5; acc: 0.56
Batch: 600; loss: 1.58; acc: 0.58
Batch: 620; loss: 1.51; acc: 0.56
Batch: 640; loss: 1.56; acc: 0.56
Batch: 660; loss: 1.52; acc: 0.64
Batch: 680; loss: 1.51; acc: 0.59
Batch: 700; loss: 1.56; acc: 0.5
Batch: 720; loss: 1.66; acc: 0.53
Batch: 740; loss: 1.76; acc: 0.44
Batch: 760; loss: 1.74; acc: 0.47
Batch: 780; loss: 1.53; acc: 0.62
Train Epoch over. train_loss: 1.6; train_accuracy: 0.54 

3.8167541788425297e-05
1.3431949810183141e-05
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.46; acc: 0.67
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.48; acc: 0.64
Batch: 100; loss: 1.66; acc: 0.5
Batch: 120; loss: 1.76; acc: 0.48
Batch: 140; loss: 1.43; acc: 0.66
Val Epoch over. val_loss: 1.5847936701622738; val_accuracy: 0.5540406050955414 

The current subspace-distance is: 1.3431949810183141e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.52
Batch: 20; loss: 1.54; acc: 0.58
Batch: 40; loss: 1.52; acc: 0.52
Batch: 60; loss: 1.65; acc: 0.44
Batch: 80; loss: 1.64; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.53
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.51; acc: 0.56
Batch: 160; loss: 1.64; acc: 0.48
Batch: 180; loss: 1.72; acc: 0.45
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.56; acc: 0.53
Batch: 240; loss: 1.69; acc: 0.47
Batch: 260; loss: 1.52; acc: 0.58
Batch: 280; loss: 1.56; acc: 0.58
Batch: 300; loss: 1.5; acc: 0.59
Batch: 320; loss: 1.51; acc: 0.62
Batch: 340; loss: 1.73; acc: 0.45
Batch: 360; loss: 1.59; acc: 0.55
Batch: 380; loss: 1.6; acc: 0.62
Batch: 400; loss: 1.52; acc: 0.62
Batch: 420; loss: 1.54; acc: 0.56
Batch: 440; loss: 1.73; acc: 0.39
Batch: 460; loss: 1.77; acc: 0.45
Batch: 480; loss: 1.59; acc: 0.52
Batch: 500; loss: 1.73; acc: 0.42
Batch: 520; loss: 1.63; acc: 0.5
Batch: 540; loss: 1.58; acc: 0.48
Batch: 560; loss: 1.58; acc: 0.59
Batch: 580; loss: 1.54; acc: 0.52
Batch: 600; loss: 1.61; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.45
Batch: 640; loss: 1.54; acc: 0.62
Batch: 660; loss: 1.52; acc: 0.56
Batch: 680; loss: 1.6; acc: 0.58
Batch: 700; loss: 1.61; acc: 0.58
Batch: 720; loss: 1.8; acc: 0.45
Batch: 740; loss: 1.54; acc: 0.55
Batch: 760; loss: 1.55; acc: 0.62
Batch: 780; loss: 1.55; acc: 0.61
Train Epoch over. train_loss: 1.6; train_accuracy: 0.54 

3.853159796562977e-05
1.1633888789219782e-05
Batch: 0; loss: 1.69; acc: 0.44
Batch: 20; loss: 1.65; acc: 0.52
Batch: 40; loss: 1.45; acc: 0.66
Batch: 60; loss: 1.66; acc: 0.5
Batch: 80; loss: 1.45; acc: 0.66
Batch: 100; loss: 1.65; acc: 0.52
Batch: 120; loss: 1.75; acc: 0.48
Batch: 140; loss: 1.41; acc: 0.67
Val Epoch over. val_loss: 1.577642934337543; val_accuracy: 0.5529458598726115 

The current subspace-distance is: 1.1633888789219782e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.49; acc: 0.66
Batch: 20; loss: 1.6; acc: 0.56
Batch: 40; loss: 1.59; acc: 0.48
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.52; acc: 0.58
Batch: 100; loss: 1.72; acc: 0.42
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.65; acc: 0.52
Batch: 160; loss: 1.6; acc: 0.58
Batch: 180; loss: 1.52; acc: 0.64
Batch: 200; loss: 1.65; acc: 0.47
Batch: 220; loss: 1.59; acc: 0.64
Batch: 240; loss: 1.69; acc: 0.47
Batch: 260; loss: 1.66; acc: 0.53
Batch: 280; loss: 1.59; acc: 0.44
Batch: 300; loss: 1.58; acc: 0.53
Batch: 320; loss: 1.54; acc: 0.58
Batch: 340; loss: 1.59; acc: 0.59
Batch: 360; loss: 1.66; acc: 0.59
Batch: 380; loss: 1.57; acc: 0.55
Batch: 400; loss: 1.55; acc: 0.56
Batch: 420; loss: 1.55; acc: 0.5
Batch: 440; loss: 1.51; acc: 0.66
Batch: 460; loss: 1.66; acc: 0.45
Batch: 480; loss: 1.5; acc: 0.61
Batch: 500; loss: 1.68; acc: 0.48
Batch: 520; loss: 1.58; acc: 0.56
Batch: 540; loss: 1.54; acc: 0.56
Batch: 560; loss: 1.48; acc: 0.58
Batch: 580; loss: 1.7; acc: 0.42
Batch: 600; loss: 1.63; acc: 0.53
Batch: 620; loss: 1.68; acc: 0.42
Batch: 640; loss: 1.55; acc: 0.59
Batch: 660; loss: 1.68; acc: 0.52
Batch: 680; loss: 1.4; acc: 0.66
Batch: 700; loss: 1.59; acc: 0.53
Batch: 720; loss: 1.55; acc: 0.52
Batch: 740; loss: 1.45; acc: 0.69
Batch: 760; loss: 1.55; acc: 0.58
Batch: 780; loss: 1.59; acc: 0.52
Train Epoch over. train_loss: 1.59; train_accuracy: 0.53 

3.9032453059917316e-05
1.1699567949108314e-05
Batch: 0; loss: 1.68; acc: 0.41
Batch: 20; loss: 1.64; acc: 0.5
Batch: 40; loss: 1.45; acc: 0.66
Batch: 60; loss: 1.66; acc: 0.48
Batch: 80; loss: 1.45; acc: 0.66
Batch: 100; loss: 1.64; acc: 0.52
Batch: 120; loss: 1.74; acc: 0.47
Batch: 140; loss: 1.39; acc: 0.66
Val Epoch over. val_loss: 1.5694088252486698; val_accuracy: 0.5546377388535032 

The current subspace-distance is: 1.1699567949108314e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.72; acc: 0.53
Batch: 20; loss: 1.72; acc: 0.41
Batch: 40; loss: 1.42; acc: 0.64
Batch: 60; loss: 1.74; acc: 0.52
Batch: 80; loss: 1.48; acc: 0.66
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.61; acc: 0.58
Batch: 140; loss: 1.68; acc: 0.47
Batch: 160; loss: 1.67; acc: 0.53
Batch: 180; loss: 1.75; acc: 0.45
Batch: 200; loss: 1.65; acc: 0.55
Batch: 220; loss: 1.46; acc: 0.61
Batch: 240; loss: 1.6; acc: 0.55
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.66; acc: 0.47
Batch: 300; loss: 1.74; acc: 0.36
Batch: 320; loss: 1.62; acc: 0.48
Batch: 340; loss: 1.5; acc: 0.58
Batch: 360; loss: 1.61; acc: 0.61
Batch: 380; loss: 1.67; acc: 0.58
Batch: 400; loss: 1.46; acc: 0.61
Batch: 420; loss: 1.57; acc: 0.48
Batch: 440; loss: 1.66; acc: 0.55
Batch: 460; loss: 1.64; acc: 0.5
Batch: 480; loss: 1.56; acc: 0.56
Batch: 500; loss: 1.73; acc: 0.44
Batch: 520; loss: 1.64; acc: 0.52
Batch: 540; loss: 1.6; acc: 0.59
Batch: 560; loss: 1.45; acc: 0.62
Batch: 580; loss: 1.53; acc: 0.59
Batch: 600; loss: 1.47; acc: 0.59
Batch: 620; loss: 1.57; acc: 0.5
Batch: 640; loss: 1.53; acc: 0.55
Batch: 660; loss: 1.53; acc: 0.61
Batch: 680; loss: 1.53; acc: 0.53
Batch: 700; loss: 1.63; acc: 0.55
Batch: 720; loss: 1.57; acc: 0.47
Batch: 740; loss: 1.55; acc: 0.48
Batch: 760; loss: 1.65; acc: 0.61
Batch: 780; loss: 1.51; acc: 0.56
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

4.0140337659977376e-05
1.466278172301827e-05
Batch: 0; loss: 1.67; acc: 0.42
Batch: 20; loss: 1.63; acc: 0.55
Batch: 40; loss: 1.44; acc: 0.66
Batch: 60; loss: 1.65; acc: 0.5
Batch: 80; loss: 1.43; acc: 0.67
Batch: 100; loss: 1.64; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.47
Batch: 140; loss: 1.38; acc: 0.67
Val Epoch over. val_loss: 1.5604286793690578; val_accuracy: 0.5548367834394905 

The current subspace-distance is: 1.466278172301827e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.67; acc: 0.55
Batch: 20; loss: 1.61; acc: 0.45
Batch: 40; loss: 1.55; acc: 0.58
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.56
Batch: 100; loss: 1.48; acc: 0.62
Batch: 120; loss: 1.59; acc: 0.5
Batch: 140; loss: 1.57; acc: 0.56
Batch: 160; loss: 1.61; acc: 0.44
Batch: 180; loss: 1.65; acc: 0.39
Batch: 200; loss: 1.65; acc: 0.56
Batch: 220; loss: 1.59; acc: 0.55
Batch: 240; loss: 1.65; acc: 0.47
Batch: 260; loss: 1.64; acc: 0.55
Batch: 280; loss: 1.61; acc: 0.53
Batch: 300; loss: 1.6; acc: 0.48
Batch: 320; loss: 1.61; acc: 0.48
Batch: 340; loss: 1.77; acc: 0.48
Batch: 360; loss: 1.57; acc: 0.56
Batch: 380; loss: 1.56; acc: 0.56
Batch: 400; loss: 1.48; acc: 0.58
Batch: 420; loss: 1.7; acc: 0.48
Batch: 440; loss: 1.49; acc: 0.61
Batch: 460; loss: 1.67; acc: 0.53
Batch: 480; loss: 1.54; acc: 0.61
Batch: 500; loss: 1.53; acc: 0.53
Batch: 520; loss: 1.55; acc: 0.59
Batch: 540; loss: 1.69; acc: 0.45
Batch: 560; loss: 1.72; acc: 0.44
Batch: 580; loss: 1.55; acc: 0.52
Batch: 600; loss: 1.56; acc: 0.53
Batch: 620; loss: 1.72; acc: 0.44
Batch: 640; loss: 1.6; acc: 0.53
Batch: 660; loss: 1.58; acc: 0.52
Batch: 680; loss: 1.73; acc: 0.42
Batch: 700; loss: 1.69; acc: 0.47
Batch: 720; loss: 1.66; acc: 0.52
Batch: 740; loss: 1.58; acc: 0.56
Batch: 760; loss: 1.62; acc: 0.47
Batch: 780; loss: 1.71; acc: 0.39
Train Epoch over. train_loss: 1.59; train_accuracy: 0.54 

3.923095573554747e-05
1.2081545719411224e-05
Batch: 0; loss: 1.67; acc: 0.42
Batch: 20; loss: 1.65; acc: 0.53
Batch: 40; loss: 1.44; acc: 0.66
Batch: 60; loss: 1.66; acc: 0.45
Batch: 80; loss: 1.43; acc: 0.69
Batch: 100; loss: 1.65; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.39; acc: 0.69
Val Epoch over. val_loss: 1.5689228089751712; val_accuracy: 0.5544386942675159 

The current subspace-distance is: 1.2081545719411224e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.66; acc: 0.56
Batch: 20; loss: 1.49; acc: 0.53
Batch: 40; loss: 1.74; acc: 0.45
Batch: 60; loss: 1.66; acc: 0.45
Batch: 80; loss: 1.53; acc: 0.58
Batch: 100; loss: 1.52; acc: 0.59
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.49; acc: 0.52
Batch: 160; loss: 1.51; acc: 0.59
Batch: 180; loss: 1.55; acc: 0.59
Batch: 200; loss: 1.61; acc: 0.53
Batch: 220; loss: 1.48; acc: 0.64
Batch: 240; loss: 1.6; acc: 0.52
Batch: 260; loss: 1.47; acc: 0.59
Batch: 280; loss: 1.51; acc: 0.67
Batch: 300; loss: 1.56; acc: 0.5
Batch: 320; loss: 1.77; acc: 0.41
Batch: 340; loss: 1.57; acc: 0.58
Batch: 360; loss: 1.51; acc: 0.66
Batch: 380; loss: 1.52; acc: 0.58
Batch: 400; loss: 1.62; acc: 0.58
Batch: 420; loss: 1.45; acc: 0.66
Batch: 440; loss: 1.58; acc: 0.55
Batch: 460; loss: 1.55; acc: 0.56
Batch: 480; loss: 1.64; acc: 0.48
Batch: 500; loss: 1.67; acc: 0.44
Batch: 520; loss: 1.56; acc: 0.64
Batch: 540; loss: 1.44; acc: 0.61
Batch: 560; loss: 1.66; acc: 0.53
Batch: 580; loss: 1.59; acc: 0.52
Batch: 600; loss: 1.52; acc: 0.61
Batch: 620; loss: 1.57; acc: 0.5
Batch: 640; loss: 1.46; acc: 0.62
Batch: 660; loss: 1.64; acc: 0.52
Batch: 680; loss: 1.52; acc: 0.58
Batch: 700; loss: 1.58; acc: 0.58
Batch: 720; loss: 1.71; acc: 0.47
Batch: 740; loss: 1.47; acc: 0.61
Batch: 760; loss: 1.7; acc: 0.44
Batch: 780; loss: 1.46; acc: 0.62
Train Epoch over. train_loss: 1.59; train_accuracy: 0.53 

3.9522157749161124e-05
1.3342064448806923e-05
Batch: 0; loss: 1.68; acc: 0.42
Batch: 20; loss: 1.64; acc: 0.55
Batch: 40; loss: 1.44; acc: 0.66
Batch: 60; loss: 1.65; acc: 0.48
Batch: 80; loss: 1.43; acc: 0.69
Batch: 100; loss: 1.65; acc: 0.55
Batch: 120; loss: 1.75; acc: 0.47
Batch: 140; loss: 1.39; acc: 0.66
Val Epoch over. val_loss: 1.5668304870083074; val_accuracy: 0.5544386942675159 

The current subspace-distance is: 1.3342064448806923e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.56
Batch: 20; loss: 1.53; acc: 0.58
Batch: 40; loss: 1.43; acc: 0.64
Batch: 60; loss: 1.52; acc: 0.59
Batch: 80; loss: 1.47; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.64; acc: 0.56
Batch: 140; loss: 1.51; acc: 0.58
Batch: 160; loss: 1.69; acc: 0.48
Batch: 180; loss: 1.58; acc: 0.5
Batch: 200; loss: 1.67; acc: 0.5
Batch: 220; loss: 1.52; acc: 0.58
Batch: 240; loss: 1.6; acc: 0.5
Batch: 260; loss: 1.5; acc: 0.61
Batch: 280; loss: 1.5; acc: 0.66
Batch: 300; loss: 1.64; acc: 0.41
Batch: 320; loss: 1.55; acc: 0.58
Batch: 340; loss: 1.66; acc: 0.38
Batch: 360; loss: 1.67; acc: 0.53
Batch: 380; loss: 1.62; acc: 0.56
Batch: 400; loss: 1.72; acc: 0.48
Batch: 420; loss: 1.61; acc: 0.55
Batch: 440; loss: 1.61; acc: 0.55
Batch: 460; loss: 1.49; acc: 0.64
Batch: 480; loss: 1.61; acc: 0.58
Batch: 500; loss: 1.58; acc: 0.55
Batch: 520; loss: 1.59; acc: 0.5
Batch: 540; loss: 1.59; acc: 0.53
Batch: 560; loss: 1.58; acc: 0.55
Batch: 580; loss: 1.57; acc: 0.52
Batch: 600; loss: 1.53; acc: 0.64
Batch: 620; loss: 1.59; acc: 0.52
Batch: 640; loss: 1.65; acc: 0.45
Batch: 660; loss: 1.5; acc: 0.55
Batch: 680; loss: 1.71; acc: 0.45
Batch: 700; loss: 1.67; acc: 0.48
Batch: 720; loss: 1.47; acc: 0.58
Batch: 740; loss: 1.58; acc: 0.55
Batch: 760; loss: 1.74; acc: 0.42
Batch: 780; loss: 1.62; acc: 0.56
Train Epoch over. train_loss: 1.59; train_accuracy: 0.53 

3.970275429310277e-05
1.1692653970385436e-05
Batch: 0; loss: 1.68; acc: 0.41
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.44; acc: 0.61
Batch: 60; loss: 1.65; acc: 0.47
Batch: 80; loss: 1.43; acc: 0.69
Batch: 100; loss: 1.64; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.5
Batch: 140; loss: 1.39; acc: 0.67
Val Epoch over. val_loss: 1.5641512680964864; val_accuracy: 0.5535429936305732 

The current subspace-distance is: 1.1692653970385436e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.61; acc: 0.5
Batch: 20; loss: 1.58; acc: 0.56
Batch: 40; loss: 1.59; acc: 0.56
Batch: 60; loss: 1.53; acc: 0.62
Batch: 80; loss: 1.63; acc: 0.48
Batch: 100; loss: 1.67; acc: 0.47
Batch: 120; loss: 1.63; acc: 0.62
Batch: 140; loss: 1.47; acc: 0.62
Batch: 160; loss: 1.44; acc: 0.66
Batch: 180; loss: 1.64; acc: 0.53
Batch: 200; loss: 1.61; acc: 0.52
Batch: 220; loss: 1.57; acc: 0.59
Batch: 240; loss: 1.64; acc: 0.5
Batch: 260; loss: 1.59; acc: 0.52
Batch: 280; loss: 1.68; acc: 0.5
Batch: 300; loss: 1.48; acc: 0.61
Batch: 320; loss: 1.71; acc: 0.48
Batch: 340; loss: 1.53; acc: 0.52
Batch: 360; loss: 1.73; acc: 0.55
Batch: 380; loss: 1.47; acc: 0.61
Batch: 400; loss: 1.45; acc: 0.53
Batch: 420; loss: 1.64; acc: 0.47
Batch: 440; loss: 1.54; acc: 0.59
Batch: 460; loss: 1.6; acc: 0.53
Batch: 480; loss: 1.63; acc: 0.52
Batch: 500; loss: 1.64; acc: 0.48
Batch: 520; loss: 1.52; acc: 0.59
Batch: 540; loss: 1.57; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.5
Batch: 580; loss: 1.49; acc: 0.52
Batch: 600; loss: 1.7; acc: 0.42
Batch: 620; loss: 1.48; acc: 0.61
Batch: 640; loss: 1.42; acc: 0.72
Batch: 660; loss: 1.69; acc: 0.42
Batch: 680; loss: 1.65; acc: 0.47
Batch: 700; loss: 1.68; acc: 0.44
Batch: 720; loss: 1.4; acc: 0.69
Batch: 740; loss: 1.61; acc: 0.52
Batch: 760; loss: 1.51; acc: 0.61
Batch: 780; loss: 1.52; acc: 0.64
Train Epoch over. train_loss: 1.58; train_accuracy: 0.54 

4.020072447019629e-05
1.3474380466504954e-05
Batch: 0; loss: 1.67; acc: 0.41
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.43; acc: 0.62
Batch: 60; loss: 1.64; acc: 0.5
Batch: 80; loss: 1.41; acc: 0.69
Batch: 100; loss: 1.64; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.36; acc: 0.67
Val Epoch over. val_loss: 1.5556046917180346; val_accuracy: 0.5527468152866242 

The current subspace-distance is: 1.3474380466504954e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.53
Batch: 20; loss: 1.52; acc: 0.55
Batch: 40; loss: 1.59; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.52; acc: 0.62
Batch: 100; loss: 1.6; acc: 0.59
Batch: 120; loss: 1.54; acc: 0.55
Batch: 140; loss: 1.63; acc: 0.52
Batch: 160; loss: 1.6; acc: 0.59
Batch: 180; loss: 1.74; acc: 0.41
Batch: 200; loss: 1.78; acc: 0.38
Batch: 220; loss: 1.75; acc: 0.44
Batch: 240; loss: 1.49; acc: 0.61
Batch: 260; loss: 1.58; acc: 0.52
Batch: 280; loss: 1.73; acc: 0.47
Batch: 300; loss: 1.63; acc: 0.47
Batch: 320; loss: 1.57; acc: 0.53
Batch: 340; loss: 1.62; acc: 0.56
Batch: 360; loss: 1.59; acc: 0.48
Batch: 380; loss: 1.46; acc: 0.61
Batch: 400; loss: 1.53; acc: 0.56
Batch: 420; loss: 1.64; acc: 0.5
Batch: 440; loss: 1.56; acc: 0.58
Batch: 460; loss: 1.7; acc: 0.45
Batch: 480; loss: 1.55; acc: 0.53
Batch: 500; loss: 1.56; acc: 0.61
Batch: 520; loss: 1.53; acc: 0.61
Batch: 540; loss: 1.51; acc: 0.5
Batch: 560; loss: 1.56; acc: 0.53
Batch: 580; loss: 1.55; acc: 0.56
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.78; acc: 0.42
Batch: 640; loss: 1.61; acc: 0.55
Batch: 660; loss: 1.68; acc: 0.45
Batch: 680; loss: 1.59; acc: 0.52
Batch: 700; loss: 1.58; acc: 0.48
Batch: 720; loss: 1.61; acc: 0.47
Batch: 740; loss: 1.66; acc: 0.53
Batch: 760; loss: 1.62; acc: 0.55
Batch: 780; loss: 1.55; acc: 0.52
Train Epoch over. train_loss: 1.58; train_accuracy: 0.53 

4.0098057070281357e-05
1.2866647921327967e-05
Batch: 0; loss: 1.66; acc: 0.42
Batch: 20; loss: 1.64; acc: 0.56
Batch: 40; loss: 1.42; acc: 0.64
Batch: 60; loss: 1.64; acc: 0.48
Batch: 80; loss: 1.4; acc: 0.69
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.73; acc: 0.47
Batch: 140; loss: 1.36; acc: 0.7
Val Epoch over. val_loss: 1.5506526535483682; val_accuracy: 0.5542396496815286 

The current subspace-distance is: 1.2866647921327967e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.56
Batch: 20; loss: 1.62; acc: 0.48
Batch: 40; loss: 1.46; acc: 0.58
Batch: 60; loss: 1.54; acc: 0.53
Batch: 80; loss: 1.57; acc: 0.5
Batch: 100; loss: 1.74; acc: 0.39
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.44; acc: 0.7
Batch: 160; loss: 1.52; acc: 0.59
Batch: 180; loss: 1.47; acc: 0.59
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.57; acc: 0.47
Batch: 240; loss: 1.6; acc: 0.53
Batch: 260; loss: 1.46; acc: 0.66
Batch: 280; loss: 1.66; acc: 0.47
Batch: 300; loss: 1.54; acc: 0.59
Batch: 320; loss: 1.52; acc: 0.48
Batch: 340; loss: 1.41; acc: 0.62
Batch: 360; loss: 1.56; acc: 0.53
Batch: 380; loss: 1.52; acc: 0.58
Batch: 400; loss: 1.59; acc: 0.47
Batch: 420; loss: 1.58; acc: 0.59
Batch: 440; loss: 1.71; acc: 0.48
Batch: 460; loss: 1.69; acc: 0.44
Batch: 480; loss: 1.49; acc: 0.56
Batch: 500; loss: 1.69; acc: 0.36
Batch: 520; loss: 1.67; acc: 0.45
Batch: 540; loss: 1.61; acc: 0.55
Batch: 560; loss: 1.63; acc: 0.48
Batch: 580; loss: 1.54; acc: 0.53
Batch: 600; loss: 1.44; acc: 0.58
Batch: 620; loss: 1.68; acc: 0.48
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.55; acc: 0.56
Batch: 680; loss: 1.58; acc: 0.5
Batch: 700; loss: 1.59; acc: 0.56
Batch: 720; loss: 1.64; acc: 0.52
Batch: 740; loss: 1.49; acc: 0.64
Batch: 760; loss: 1.63; acc: 0.47
Batch: 780; loss: 1.42; acc: 0.66
Train Epoch over. train_loss: 1.58; train_accuracy: 0.53 

4.08640262321569e-05
1.5536263163085096e-05
Batch: 0; loss: 1.66; acc: 0.45
Batch: 20; loss: 1.62; acc: 0.55
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.61; acc: 0.55
Batch: 120; loss: 1.71; acc: 0.48
Batch: 140; loss: 1.35; acc: 0.69
Val Epoch over. val_loss: 1.544341960530372; val_accuracy: 0.5604100318471338 

The current subspace-distance is: 1.5536263163085096e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.64; acc: 0.45
Batch: 20; loss: 1.7; acc: 0.41
Batch: 40; loss: 1.64; acc: 0.55
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.61; acc: 0.5
Batch: 100; loss: 1.49; acc: 0.66
Batch: 120; loss: 1.58; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.53
Batch: 160; loss: 1.61; acc: 0.52
Batch: 180; loss: 1.6; acc: 0.59
Batch: 200; loss: 1.61; acc: 0.5
Batch: 220; loss: 1.57; acc: 0.53
Batch: 240; loss: 1.67; acc: 0.45
Batch: 260; loss: 1.63; acc: 0.45
Batch: 280; loss: 1.56; acc: 0.56
Batch: 300; loss: 1.75; acc: 0.42
Batch: 320; loss: 1.6; acc: 0.56
Batch: 340; loss: 1.7; acc: 0.42
Batch: 360; loss: 1.61; acc: 0.45
Batch: 380; loss: 1.51; acc: 0.53
Batch: 400; loss: 1.57; acc: 0.53
Batch: 420; loss: 1.63; acc: 0.56
Batch: 440; loss: 1.71; acc: 0.41
Batch: 460; loss: 1.54; acc: 0.59
Batch: 480; loss: 1.53; acc: 0.61
Batch: 500; loss: 1.56; acc: 0.52
Batch: 520; loss: 1.39; acc: 0.64
Batch: 540; loss: 1.7; acc: 0.45
Batch: 560; loss: 1.67; acc: 0.45
Batch: 580; loss: 1.67; acc: 0.47
Batch: 600; loss: 1.59; acc: 0.45
Batch: 620; loss: 1.66; acc: 0.5
Batch: 640; loss: 1.65; acc: 0.5
Batch: 660; loss: 1.56; acc: 0.53
Batch: 680; loss: 1.49; acc: 0.58
Batch: 700; loss: 1.54; acc: 0.61
Batch: 720; loss: 1.58; acc: 0.58
Batch: 740; loss: 1.49; acc: 0.58
Batch: 760; loss: 1.66; acc: 0.52
Batch: 780; loss: 1.46; acc: 0.52
Train Epoch over. train_loss: 1.58; train_accuracy: 0.53 

4.075380275025964e-05
1.4124252629699185e-05
Batch: 0; loss: 1.67; acc: 0.45
Batch: 20; loss: 1.63; acc: 0.53
Batch: 40; loss: 1.41; acc: 0.62
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.4; acc: 0.7
Batch: 100; loss: 1.63; acc: 0.55
Batch: 120; loss: 1.72; acc: 0.45
Batch: 140; loss: 1.36; acc: 0.69
Val Epoch over. val_loss: 1.5465320295588985; val_accuracy: 0.5553343949044586 

The current subspace-distance is: 1.4124252629699185e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.47
Batch: 20; loss: 1.51; acc: 0.59
Batch: 40; loss: 1.63; acc: 0.56
Batch: 60; loss: 1.52; acc: 0.56
Batch: 80; loss: 1.47; acc: 0.58
Batch: 100; loss: 1.72; acc: 0.44
Batch: 120; loss: 1.64; acc: 0.61
Batch: 140; loss: 1.46; acc: 0.55
Batch: 160; loss: 1.63; acc: 0.53
Batch: 180; loss: 1.5; acc: 0.58
Batch: 200; loss: 1.68; acc: 0.44
Batch: 220; loss: 1.51; acc: 0.48
Batch: 240; loss: 1.58; acc: 0.53
Batch: 260; loss: 1.47; acc: 0.61
Batch: 280; loss: 1.65; acc: 0.55
Batch: 300; loss: 1.61; acc: 0.53
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.9; acc: 0.34
Batch: 360; loss: 1.62; acc: 0.39
Batch: 380; loss: 1.6; acc: 0.5
Batch: 400; loss: 1.55; acc: 0.56
Batch: 420; loss: 1.68; acc: 0.48
Batch: 440; loss: 1.59; acc: 0.47
Batch: 460; loss: 1.64; acc: 0.5
Batch: 480; loss: 1.71; acc: 0.45
Batch: 500; loss: 1.61; acc: 0.5
Batch: 520; loss: 1.46; acc: 0.56
Batch: 540; loss: 1.6; acc: 0.53
Batch: 560; loss: 1.57; acc: 0.58
Batch: 580; loss: 1.51; acc: 0.53
Batch: 600; loss: 1.56; acc: 0.48
Batch: 620; loss: 1.53; acc: 0.53
Batch: 640; loss: 1.59; acc: 0.5
Batch: 660; loss: 1.63; acc: 0.5
Batch: 680; loss: 1.57; acc: 0.53
Batch: 700; loss: 1.58; acc: 0.5
Batch: 720; loss: 1.57; acc: 0.47
Batch: 740; loss: 1.64; acc: 0.5
Batch: 760; loss: 1.48; acc: 0.59
Batch: 780; loss: 1.52; acc: 0.58
Train Epoch over. train_loss: 1.58; train_accuracy: 0.53 

4.0406743210041896e-05
1.3715347449760884e-05
Batch: 0; loss: 1.67; acc: 0.44
Batch: 20; loss: 1.64; acc: 0.56
Batch: 40; loss: 1.42; acc: 0.62
Batch: 60; loss: 1.64; acc: 0.48
Batch: 80; loss: 1.41; acc: 0.72
Batch: 100; loss: 1.62; acc: 0.55
Batch: 120; loss: 1.71; acc: 0.48
Batch: 140; loss: 1.35; acc: 0.69
Val Epoch over. val_loss: 1.549766155564861; val_accuracy: 0.5615047770700637 

The current subspace-distance is: 1.3715347449760884e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.48; acc: 0.53
Batch: 20; loss: 1.7; acc: 0.41
Batch: 40; loss: 1.59; acc: 0.44
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.66; acc: 0.48
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.61; acc: 0.55
Batch: 140; loss: 1.51; acc: 0.53
Batch: 160; loss: 1.57; acc: 0.56
Batch: 180; loss: 1.52; acc: 0.58
Batch: 200; loss: 1.55; acc: 0.52
Batch: 220; loss: 1.65; acc: 0.41
Batch: 240; loss: 1.67; acc: 0.52
Batch: 260; loss: 1.6; acc: 0.53
Batch: 280; loss: 1.43; acc: 0.66
Batch: 300; loss: 1.46; acc: 0.62
Batch: 320; loss: 1.63; acc: 0.48
Batch: 340; loss: 1.63; acc: 0.56
Batch: 360; loss: 1.63; acc: 0.53
Batch: 380; loss: 1.6; acc: 0.52
Batch: 400; loss: 1.53; acc: 0.56
Batch: 420; loss: 1.74; acc: 0.42
Batch: 440; loss: 1.53; acc: 0.52
Batch: 460; loss: 1.47; acc: 0.64
Batch: 480; loss: 1.59; acc: 0.53
Batch: 500; loss: 1.5; acc: 0.53
Batch: 520; loss: 1.69; acc: 0.48
Batch: 540; loss: 1.78; acc: 0.42
Batch: 560; loss: 1.71; acc: 0.38
Batch: 580; loss: 1.4; acc: 0.66
Batch: 600; loss: 1.57; acc: 0.45
Batch: 620; loss: 1.47; acc: 0.61
Batch: 640; loss: 1.54; acc: 0.59
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.51; acc: 0.61
Batch: 700; loss: 1.68; acc: 0.48
Batch: 720; loss: 1.55; acc: 0.52
Batch: 740; loss: 1.55; acc: 0.59
Batch: 760; loss: 1.57; acc: 0.59
Batch: 780; loss: 1.49; acc: 0.62
Train Epoch over. train_loss: 1.57; train_accuracy: 0.53 

4.05196042265743e-05
1.374101066176081e-05
Batch: 0; loss: 1.66; acc: 0.45
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.4; acc: 0.61
Batch: 60; loss: 1.63; acc: 0.48
Batch: 80; loss: 1.39; acc: 0.72
Batch: 100; loss: 1.62; acc: 0.53
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.35; acc: 0.72
Val Epoch over. val_loss: 1.5416246341292266; val_accuracy: 0.5575238853503185 

The current subspace-distance is: 1.374101066176081e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_8_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.4265201420964964

The number of parameters is: 257601

The number of individual parameters is:

20
360
20
20
30
40800
30
30
59
120360
59
59
64
90624
64
64
4096
64
640
10
64
64

nonzero elements in E: 25760097
elements in E: 25760100
fraction nonzero: 0.9999998835408248
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.47; acc: 0.05
Batch: 20; loss: 2.36; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.12; acc: 0.3
Batch: 80; loss: 2.09; acc: 0.33
Batch: 100; loss: 2.02; acc: 0.38
Batch: 120; loss: 2.08; acc: 0.33
Batch: 140; loss: 2.07; acc: 0.31
Batch: 160; loss: 2.03; acc: 0.31
Batch: 180; loss: 1.87; acc: 0.44
Batch: 200; loss: 1.92; acc: 0.41
Batch: 220; loss: 1.82; acc: 0.45
Batch: 240; loss: 1.91; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.52
Batch: 280; loss: 1.85; acc: 0.41
Batch: 300; loss: 1.78; acc: 0.45
Batch: 320; loss: 1.8; acc: 0.48
Batch: 340; loss: 1.76; acc: 0.47
Batch: 360; loss: 1.76; acc: 0.48
Batch: 380; loss: 1.71; acc: 0.48
Batch: 400; loss: 1.79; acc: 0.5
Batch: 420; loss: 1.73; acc: 0.48
Batch: 440; loss: 1.76; acc: 0.53
Batch: 460; loss: 1.75; acc: 0.47
Batch: 480; loss: 1.69; acc: 0.5
Batch: 500; loss: 1.62; acc: 0.62
Batch: 520; loss: 1.75; acc: 0.42
Batch: 540; loss: 1.76; acc: 0.47
Batch: 560; loss: 1.68; acc: 0.55
Batch: 580; loss: 1.68; acc: 0.53
Batch: 600; loss: 1.6; acc: 0.64
Batch: 620; loss: 1.61; acc: 0.59
Batch: 640; loss: 1.69; acc: 0.55
Batch: 660; loss: 1.59; acc: 0.59
Batch: 680; loss: 1.65; acc: 0.55
Batch: 700; loss: 1.71; acc: 0.53
Batch: 720; loss: 1.6; acc: 0.61
Batch: 740; loss: 1.62; acc: 0.62
Batch: 760; loss: 1.59; acc: 0.59
Batch: 780; loss: 1.62; acc: 0.62
Train Epoch over. train_loss: 1.82; train_accuracy: 0.46 

5.2265320846345276e-05
4.7203775466186926e-05
Batch: 0; loss: 1.64; acc: 0.53
Batch: 20; loss: 1.69; acc: 0.52
Batch: 40; loss: 1.36; acc: 0.75
Batch: 60; loss: 1.58; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.55
Batch: 100; loss: 1.62; acc: 0.67
Batch: 120; loss: 1.66; acc: 0.53
Batch: 140; loss: 1.57; acc: 0.62
Val Epoch over. val_loss: 1.6176977613169676; val_accuracy: 0.5928542993630573 

The current subspace-distance is: 4.7203775466186926e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.56
Batch: 20; loss: 1.65; acc: 0.59
Batch: 40; loss: 1.68; acc: 0.56
Batch: 60; loss: 1.62; acc: 0.56
Batch: 80; loss: 1.6; acc: 0.58
Batch: 100; loss: 1.66; acc: 0.53
Batch: 120; loss: 1.64; acc: 0.59
Batch: 140; loss: 1.55; acc: 0.69
Batch: 160; loss: 1.55; acc: 0.62
Batch: 180; loss: 1.72; acc: 0.5
Batch: 200; loss: 1.59; acc: 0.59
Batch: 220; loss: 1.67; acc: 0.53
Batch: 240; loss: 1.59; acc: 0.62
Batch: 260; loss: 1.62; acc: 0.59
Batch: 280; loss: 1.69; acc: 0.52
Batch: 300; loss: 1.57; acc: 0.55
Batch: 320; loss: 1.68; acc: 0.55
Batch: 340; loss: 1.54; acc: 0.66
Batch: 360; loss: 1.63; acc: 0.58
Batch: 380; loss: 1.55; acc: 0.62
Batch: 400; loss: 1.68; acc: 0.53
Batch: 420; loss: 1.54; acc: 0.66
Batch: 440; loss: 1.59; acc: 0.59
Batch: 460; loss: 1.6; acc: 0.58
Batch: 480; loss: 1.62; acc: 0.59
Batch: 500; loss: 1.53; acc: 0.7
Batch: 520; loss: 1.65; acc: 0.55
Batch: 540; loss: 1.6; acc: 0.53
Batch: 560; loss: 1.64; acc: 0.62
Batch: 580; loss: 1.59; acc: 0.56
Batch: 600; loss: 1.47; acc: 0.67
Batch: 620; loss: 1.57; acc: 0.55
Batch: 640; loss: 1.49; acc: 0.62
Batch: 660; loss: 1.52; acc: 0.66
Batch: 680; loss: 1.54; acc: 0.67
Batch: 700; loss: 1.48; acc: 0.64
Batch: 720; loss: 1.56; acc: 0.62
Batch: 740; loss: 1.57; acc: 0.58
Batch: 760; loss: 1.55; acc: 0.61
Batch: 780; loss: 1.59; acc: 0.59
Train Epoch over. train_loss: 1.61; train_accuracy: 0.58 

6.241656956262887e-05
5.6974990002345294e-05
Batch: 0; loss: 1.52; acc: 0.66
Batch: 20; loss: 1.64; acc: 0.53
Batch: 40; loss: 1.28; acc: 0.8
Batch: 60; loss: 1.5; acc: 0.66
Batch: 80; loss: 1.5; acc: 0.64
Batch: 100; loss: 1.48; acc: 0.67
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.42; acc: 0.61
Val Epoch over. val_loss: 1.5378758292289296; val_accuracy: 0.6057921974522293 

The current subspace-distance is: 5.6974990002345294e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.55
Batch: 20; loss: 1.46; acc: 0.69
Batch: 40; loss: 1.69; acc: 0.5
Batch: 60; loss: 1.48; acc: 0.64
Batch: 80; loss: 1.53; acc: 0.67
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.51; acc: 0.61
Batch: 140; loss: 1.42; acc: 0.67
Batch: 160; loss: 1.59; acc: 0.61
Batch: 180; loss: 1.45; acc: 0.64
Batch: 200; loss: 1.55; acc: 0.66
Batch: 220; loss: 1.51; acc: 0.64
Batch: 240; loss: 1.57; acc: 0.52
Batch: 260; loss: 1.6; acc: 0.52
Batch: 280; loss: 1.8; acc: 0.42
Batch: 300; loss: 1.54; acc: 0.61
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.52; acc: 0.69
Batch: 360; loss: 1.43; acc: 0.7
Batch: 380; loss: 1.43; acc: 0.67
Batch: 400; loss: 1.47; acc: 0.72
Batch: 420; loss: 1.55; acc: 0.66
Batch: 440; loss: 1.5; acc: 0.62
Batch: 460; loss: 1.69; acc: 0.45
Batch: 480; loss: 1.57; acc: 0.64
Batch: 500; loss: 1.63; acc: 0.56
Batch: 520; loss: 1.42; acc: 0.69
Batch: 540; loss: 1.62; acc: 0.53
Batch: 560; loss: 1.65; acc: 0.55
Batch: 580; loss: 1.52; acc: 0.62
Batch: 600; loss: 1.47; acc: 0.62
Batch: 620; loss: 1.51; acc: 0.64
Batch: 640; loss: 1.46; acc: 0.58
Batch: 660; loss: 1.52; acc: 0.62
Batch: 680; loss: 1.58; acc: 0.5
Batch: 700; loss: 1.52; acc: 0.64
Batch: 720; loss: 1.49; acc: 0.62
Batch: 740; loss: 1.54; acc: 0.59
Batch: 760; loss: 1.47; acc: 0.62
Batch: 780; loss: 1.48; acc: 0.62
Train Epoch over. train_loss: 1.54; train_accuracy: 0.6 

6.975232827244326e-05
6.520876195281744e-05
Batch: 0; loss: 1.47; acc: 0.66
Batch: 20; loss: 1.59; acc: 0.52
Batch: 40; loss: 1.22; acc: 0.83
Batch: 60; loss: 1.41; acc: 0.67
Batch: 80; loss: 1.43; acc: 0.75
Batch: 100; loss: 1.36; acc: 0.75
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.69
Val Epoch over. val_loss: 1.4775701495492535; val_accuracy: 0.6356488853503185 

The current subspace-distance is: 6.520876195281744e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.62
Batch: 20; loss: 1.6; acc: 0.55
Batch: 40; loss: 1.46; acc: 0.62
Batch: 60; loss: 1.44; acc: 0.66
Batch: 80; loss: 1.35; acc: 0.72
Batch: 100; loss: 1.54; acc: 0.58
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.78
Batch: 160; loss: 1.53; acc: 0.59
Batch: 180; loss: 1.4; acc: 0.64
Batch: 200; loss: 1.45; acc: 0.66
Batch: 220; loss: 1.47; acc: 0.66
Batch: 240; loss: 1.49; acc: 0.62
Batch: 260; loss: 1.5; acc: 0.64
Batch: 280; loss: 1.45; acc: 0.67
Batch: 300; loss: 1.48; acc: 0.69
Batch: 320; loss: 1.48; acc: 0.64
Batch: 340; loss: 1.63; acc: 0.58
Batch: 360; loss: 1.42; acc: 0.66
Batch: 380; loss: 1.53; acc: 0.62
Batch: 400; loss: 1.48; acc: 0.62
Batch: 420; loss: 1.51; acc: 0.62
Batch: 440; loss: 1.51; acc: 0.66
Batch: 460; loss: 1.6; acc: 0.59
Batch: 480; loss: 1.36; acc: 0.72
Batch: 500; loss: 1.61; acc: 0.45
Batch: 520; loss: 1.48; acc: 0.58
Batch: 540; loss: 1.51; acc: 0.58
Batch: 560; loss: 1.53; acc: 0.59
Batch: 580; loss: 1.45; acc: 0.64
Batch: 600; loss: 1.33; acc: 0.78
Batch: 620; loss: 1.49; acc: 0.7
Batch: 640; loss: 1.6; acc: 0.61
Batch: 660; loss: 1.46; acc: 0.61
Batch: 680; loss: 1.58; acc: 0.61
Batch: 700; loss: 1.47; acc: 0.64
Batch: 720; loss: 1.49; acc: 0.62
Batch: 740; loss: 1.39; acc: 0.67
Batch: 760; loss: 1.47; acc: 0.64
Batch: 780; loss: 1.5; acc: 0.61
Train Epoch over. train_loss: 1.48; train_accuracy: 0.62 

7.655279478058219e-05
7.051404099911451e-05
Batch: 0; loss: 1.42; acc: 0.73
Batch: 20; loss: 1.51; acc: 0.59
Batch: 40; loss: 1.15; acc: 0.88
Batch: 60; loss: 1.31; acc: 0.69
Batch: 80; loss: 1.33; acc: 0.73
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 1.51; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.69
Val Epoch over. val_loss: 1.4119227980352511; val_accuracy: 0.6629179936305732 

The current subspace-distance is: 7.051404099911451e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.41; acc: 0.67
Batch: 40; loss: 1.45; acc: 0.67
Batch: 60; loss: 1.41; acc: 0.67
Batch: 80; loss: 1.42; acc: 0.61
Batch: 100; loss: 1.4; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.69
Batch: 140; loss: 1.36; acc: 0.72
Batch: 160; loss: 1.47; acc: 0.64
Batch: 180; loss: 1.41; acc: 0.66
Batch: 200; loss: 1.38; acc: 0.62
Batch: 220; loss: 1.53; acc: 0.56
Batch: 240; loss: 1.38; acc: 0.7
Batch: 260; loss: 1.44; acc: 0.67
Batch: 280; loss: 1.41; acc: 0.69
Batch: 300; loss: 1.54; acc: 0.59
Batch: 320; loss: 1.47; acc: 0.64
Batch: 340; loss: 1.38; acc: 0.64
Batch: 360; loss: 1.31; acc: 0.78
Batch: 380; loss: 1.27; acc: 0.73
Batch: 400; loss: 1.33; acc: 0.73
Batch: 420; loss: 1.42; acc: 0.69
Batch: 440; loss: 1.44; acc: 0.66
Batch: 460; loss: 1.5; acc: 0.64
Batch: 480; loss: 1.46; acc: 0.62
Batch: 500; loss: 1.48; acc: 0.64
Batch: 520; loss: 1.4; acc: 0.64
Batch: 540; loss: 1.35; acc: 0.62
Batch: 560; loss: 1.48; acc: 0.64
Batch: 580; loss: 1.25; acc: 0.81
Batch: 600; loss: 1.45; acc: 0.55
Batch: 620; loss: 1.39; acc: 0.67
Batch: 640; loss: 1.44; acc: 0.7
Batch: 660; loss: 1.36; acc: 0.73
Batch: 680; loss: 1.41; acc: 0.62
Batch: 700; loss: 1.41; acc: 0.7
Batch: 720; loss: 1.42; acc: 0.64
Batch: 740; loss: 1.33; acc: 0.69
Batch: 760; loss: 1.52; acc: 0.56
Batch: 780; loss: 1.34; acc: 0.67
Train Epoch over. train_loss: 1.43; train_accuracy: 0.65 

8.559541311115026e-05
7.976513734320179e-05
Batch: 0; loss: 1.35; acc: 0.77
Batch: 20; loss: 1.44; acc: 0.61
Batch: 40; loss: 1.11; acc: 0.88
Batch: 60; loss: 1.28; acc: 0.73
Batch: 80; loss: 1.29; acc: 0.69
Batch: 100; loss: 1.24; acc: 0.8
Batch: 120; loss: 1.48; acc: 0.59
Batch: 140; loss: 1.28; acc: 0.69
Val Epoch over. val_loss: 1.3647900759034854; val_accuracy: 0.6860071656050956 

The current subspace-distance is: 7.976513734320179e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.35; acc: 0.69
Batch: 20; loss: 1.47; acc: 0.53
Batch: 40; loss: 1.46; acc: 0.61
Batch: 60; loss: 1.33; acc: 0.69
Batch: 80; loss: 1.49; acc: 0.58
Batch: 100; loss: 1.57; acc: 0.58
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.67
Batch: 160; loss: 1.55; acc: 0.61
Batch: 180; loss: 1.33; acc: 0.72
Batch: 200; loss: 1.45; acc: 0.58
Batch: 220; loss: 1.45; acc: 0.59
Batch: 240; loss: 1.36; acc: 0.77
Batch: 260; loss: 1.34; acc: 0.66
Batch: 280; loss: 1.35; acc: 0.73
Batch: 300; loss: 1.29; acc: 0.75
Batch: 320; loss: 1.41; acc: 0.69
Batch: 340; loss: 1.26; acc: 0.8
Batch: 360; loss: 1.35; acc: 0.69
Batch: 380; loss: 1.49; acc: 0.61
Batch: 400; loss: 1.51; acc: 0.62
Batch: 420; loss: 1.35; acc: 0.75
Batch: 440; loss: 1.4; acc: 0.66
Batch: 460; loss: 1.47; acc: 0.66
Batch: 480; loss: 1.43; acc: 0.58
Batch: 500; loss: 1.41; acc: 0.66
Batch: 520; loss: 1.31; acc: 0.69
Batch: 540; loss: 1.45; acc: 0.64
Batch: 560; loss: 1.5; acc: 0.62
Batch: 580; loss: 1.39; acc: 0.66
Batch: 600; loss: 1.51; acc: 0.56
Batch: 620; loss: 1.24; acc: 0.77
Batch: 640; loss: 1.41; acc: 0.64
Batch: 660; loss: 1.41; acc: 0.66
Batch: 680; loss: 1.46; acc: 0.62
Batch: 700; loss: 1.34; acc: 0.67
Batch: 720; loss: 1.4; acc: 0.72
Batch: 740; loss: 1.35; acc: 0.66
Batch: 760; loss: 1.36; acc: 0.66
Batch: 780; loss: 1.35; acc: 0.62
Train Epoch over. train_loss: 1.38; train_accuracy: 0.66 

9.505141497356817e-05
8.882240945240483e-05
Batch: 0; loss: 1.33; acc: 0.78
Batch: 20; loss: 1.38; acc: 0.67
Batch: 40; loss: 1.03; acc: 0.92
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 1.22; acc: 0.77
Batch: 100; loss: 1.21; acc: 0.8
Batch: 120; loss: 1.41; acc: 0.61
Batch: 140; loss: 1.19; acc: 0.77
Val Epoch over. val_loss: 1.3126554754888935; val_accuracy: 0.6967555732484076 

The current subspace-distance is: 8.882240945240483e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.34; acc: 0.67
Batch: 20; loss: 1.55; acc: 0.56
Batch: 40; loss: 1.4; acc: 0.72
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.36; acc: 0.62
Batch: 100; loss: 1.28; acc: 0.77
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 1.53; acc: 0.56
Batch: 160; loss: 1.27; acc: 0.77
Batch: 180; loss: 1.36; acc: 0.67
Batch: 200; loss: 1.28; acc: 0.66
Batch: 220; loss: 1.37; acc: 0.69
Batch: 240; loss: 1.48; acc: 0.62
Batch: 260; loss: 1.4; acc: 0.66
Batch: 280; loss: 1.38; acc: 0.61
Batch: 300; loss: 1.31; acc: 0.66
Batch: 320; loss: 1.41; acc: 0.61
Batch: 340; loss: 1.32; acc: 0.67
Batch: 360; loss: 1.35; acc: 0.67
Batch: 380; loss: 1.18; acc: 0.75
Batch: 400; loss: 1.33; acc: 0.7
Batch: 420; loss: 1.27; acc: 0.75
Batch: 440; loss: 1.34; acc: 0.66
Batch: 460; loss: 1.36; acc: 0.62
Batch: 480; loss: 1.35; acc: 0.62
Batch: 500; loss: 1.37; acc: 0.7
Batch: 520; loss: 1.2; acc: 0.78
Batch: 540; loss: 1.39; acc: 0.66
Batch: 560; loss: 1.48; acc: 0.64
Batch: 580; loss: 1.28; acc: 0.73
Batch: 600; loss: 1.32; acc: 0.7
Batch: 620; loss: 1.41; acc: 0.53
Batch: 640; loss: 1.33; acc: 0.69
Batch: 660; loss: 1.27; acc: 0.7
Batch: 680; loss: 1.17; acc: 0.78
Batch: 700; loss: 1.34; acc: 0.7
Batch: 720; loss: 1.33; acc: 0.69
Batch: 740; loss: 1.22; acc: 0.67
Batch: 760; loss: 1.33; acc: 0.69
Batch: 780; loss: 1.34; acc: 0.67
Train Epoch over. train_loss: 1.34; train_accuracy: 0.67 

0.00010510633001103997
9.921526361722499e-05
Batch: 0; loss: 1.3; acc: 0.72
Batch: 20; loss: 1.31; acc: 0.69
Batch: 40; loss: 0.96; acc: 0.89
Batch: 60; loss: 1.14; acc: 0.73
Batch: 80; loss: 1.14; acc: 0.73
Batch: 100; loss: 1.16; acc: 0.78
Batch: 120; loss: 1.35; acc: 0.62
Batch: 140; loss: 1.1; acc: 0.8
Val Epoch over. val_loss: 1.2611565973348677; val_accuracy: 0.7078025477707006 

The current subspace-distance is: 9.921526361722499e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.66
Batch: 20; loss: 1.42; acc: 0.55
Batch: 40; loss: 1.45; acc: 0.59
Batch: 60; loss: 1.34; acc: 0.64
Batch: 80; loss: 1.23; acc: 0.72
Batch: 100; loss: 1.33; acc: 0.7
Batch: 120; loss: 1.23; acc: 0.72
Batch: 140; loss: 1.38; acc: 0.66
Batch: 160; loss: 1.32; acc: 0.64
Batch: 180; loss: 1.31; acc: 0.62
Batch: 200; loss: 1.3; acc: 0.69
Batch: 220; loss: 1.2; acc: 0.7
Batch: 240; loss: 1.26; acc: 0.67
Batch: 260; loss: 1.23; acc: 0.72
Batch: 280; loss: 1.28; acc: 0.72
Batch: 300; loss: 1.39; acc: 0.55
Batch: 320; loss: 1.25; acc: 0.7
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.24; acc: 0.72
Batch: 380; loss: 1.27; acc: 0.7
Batch: 400; loss: 1.25; acc: 0.72
Batch: 420; loss: 1.34; acc: 0.67
Batch: 440; loss: 1.39; acc: 0.69
Batch: 460; loss: 1.35; acc: 0.66
Batch: 480; loss: 1.37; acc: 0.64
Batch: 500; loss: 1.29; acc: 0.66
Batch: 520; loss: 1.3; acc: 0.64
Batch: 540; loss: 1.37; acc: 0.72
Batch: 560; loss: 1.26; acc: 0.72
Batch: 580; loss: 1.36; acc: 0.64
Batch: 600; loss: 1.17; acc: 0.73
Batch: 620; loss: 1.36; acc: 0.64
Batch: 640; loss: 1.35; acc: 0.64
Batch: 660; loss: 1.28; acc: 0.72
Batch: 680; loss: 1.41; acc: 0.61
Batch: 700; loss: 1.15; acc: 0.77
Batch: 720; loss: 1.44; acc: 0.59
Batch: 740; loss: 1.31; acc: 0.72
Batch: 760; loss: 1.33; acc: 0.62
Batch: 780; loss: 1.24; acc: 0.67
Train Epoch over. train_loss: 1.3; train_accuracy: 0.68 

0.00011193357204319909
0.00010619415115797892
Batch: 0; loss: 1.26; acc: 0.72
Batch: 20; loss: 1.28; acc: 0.67
Batch: 40; loss: 0.94; acc: 0.89
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 1.11; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.81
Batch: 120; loss: 1.32; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.81
Val Epoch over. val_loss: 1.2284740372827858; val_accuracy: 0.7139729299363057 

The current subspace-distance is: 0.00010619415115797892 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.37; acc: 0.66
Batch: 40; loss: 1.3; acc: 0.67
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.24; acc: 0.72
Batch: 100; loss: 1.26; acc: 0.77
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.61
Batch: 160; loss: 1.15; acc: 0.81
Batch: 180; loss: 1.33; acc: 0.75
Batch: 200; loss: 1.21; acc: 0.66
Batch: 220; loss: 1.25; acc: 0.77
Batch: 240; loss: 1.38; acc: 0.64
Batch: 260; loss: 1.16; acc: 0.75
Batch: 280; loss: 1.15; acc: 0.8
Batch: 300; loss: 1.3; acc: 0.61
Batch: 320; loss: 1.28; acc: 0.67
Batch: 340; loss: 1.34; acc: 0.59
Batch: 360; loss: 1.35; acc: 0.62
Batch: 380; loss: 1.3; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.7
Batch: 420; loss: 1.2; acc: 0.64
Batch: 440; loss: 1.43; acc: 0.52
Batch: 460; loss: 1.18; acc: 0.73
Batch: 480; loss: 1.38; acc: 0.66
Batch: 500; loss: 1.38; acc: 0.55
Batch: 520; loss: 1.3; acc: 0.66
Batch: 540; loss: 1.25; acc: 0.67
Batch: 560; loss: 1.22; acc: 0.77
Batch: 580; loss: 1.26; acc: 0.64
Batch: 600; loss: 1.29; acc: 0.69
Batch: 620; loss: 1.34; acc: 0.62
Batch: 640; loss: 1.45; acc: 0.64
Batch: 660; loss: 1.12; acc: 0.72
Batch: 680; loss: 1.3; acc: 0.66
Batch: 700; loss: 1.17; acc: 0.7
Batch: 720; loss: 1.27; acc: 0.66
Batch: 740; loss: 1.17; acc: 0.67
Batch: 760; loss: 1.27; acc: 0.72
Batch: 780; loss: 1.16; acc: 0.73
Train Epoch over. train_loss: 1.26; train_accuracy: 0.68 

0.00012226865510456264
0.00011646765779005364
Batch: 0; loss: 1.19; acc: 0.75
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 0.89; acc: 0.88
Batch: 60; loss: 1.1; acc: 0.77
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 1.09; acc: 0.83
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 1.04; acc: 0.8
Val Epoch over. val_loss: 1.1817951620004739; val_accuracy: 0.7147691082802548 

The current subspace-distance is: 0.00011646765779005364 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.73
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.12; acc: 0.8
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.33; acc: 0.59
Batch: 120; loss: 1.14; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.66
Batch: 160; loss: 1.18; acc: 0.75
Batch: 180; loss: 1.25; acc: 0.67
Batch: 200; loss: 1.29; acc: 0.59
Batch: 220; loss: 1.24; acc: 0.67
Batch: 240; loss: 1.23; acc: 0.69
Batch: 260; loss: 1.23; acc: 0.69
Batch: 280; loss: 1.43; acc: 0.56
Batch: 300; loss: 1.3; acc: 0.61
Batch: 320; loss: 1.11; acc: 0.72
Batch: 340; loss: 1.21; acc: 0.72
Batch: 360; loss: 1.12; acc: 0.72
Batch: 380; loss: 1.2; acc: 0.73
Batch: 400; loss: 1.22; acc: 0.7
Batch: 420; loss: 1.19; acc: 0.7
Batch: 440; loss: 1.11; acc: 0.8
Batch: 460; loss: 1.12; acc: 0.72
Batch: 480; loss: 1.17; acc: 0.73
Batch: 500; loss: 1.31; acc: 0.67
Batch: 520; loss: 1.14; acc: 0.73
Batch: 540; loss: 1.2; acc: 0.69
Batch: 560; loss: 1.29; acc: 0.64
Batch: 580; loss: 1.19; acc: 0.69
Batch: 600; loss: 1.27; acc: 0.64
Batch: 620; loss: 1.04; acc: 0.78
Batch: 640; loss: 1.23; acc: 0.66
Batch: 660; loss: 1.32; acc: 0.61
Batch: 680; loss: 1.08; acc: 0.73
Batch: 700; loss: 1.03; acc: 0.73
Batch: 720; loss: 1.21; acc: 0.66
Batch: 740; loss: 1.04; acc: 0.83
Batch: 760; loss: 1.02; acc: 0.78
Batch: 780; loss: 1.29; acc: 0.61
Train Epoch over. train_loss: 1.23; train_accuracy: 0.69 

0.0001337354042334482
0.00012666446855291724
Batch: 0; loss: 1.13; acc: 0.75
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 0.84; acc: 0.88
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 1.06; acc: 0.83
Batch: 120; loss: 1.28; acc: 0.7
Batch: 140; loss: 1.01; acc: 0.78
Val Epoch over. val_loss: 1.1426429630844457; val_accuracy: 0.7253184713375797 

The current subspace-distance is: 0.00012666446855291724 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.28; acc: 0.61
Batch: 20; loss: 1.23; acc: 0.7
Batch: 40; loss: 1.16; acc: 0.7
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 1.13; acc: 0.7
Batch: 100; loss: 1.19; acc: 0.72
Batch: 120; loss: 1.16; acc: 0.69
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.04; acc: 0.78
Batch: 180; loss: 1.25; acc: 0.8
Batch: 200; loss: 1.16; acc: 0.7
Batch: 220; loss: 1.17; acc: 0.7
Batch: 240; loss: 1.35; acc: 0.61
Batch: 260; loss: 1.18; acc: 0.77
Batch: 280; loss: 1.28; acc: 0.69
Batch: 300; loss: 1.18; acc: 0.69
Batch: 320; loss: 1.08; acc: 0.8
Batch: 340; loss: 1.22; acc: 0.66
Batch: 360; loss: 1.34; acc: 0.66
Batch: 380; loss: 1.21; acc: 0.69
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 1.19; acc: 0.64
Batch: 440; loss: 1.15; acc: 0.72
Batch: 460; loss: 1.11; acc: 0.73
Batch: 480; loss: 1.09; acc: 0.77
Batch: 500; loss: 1.18; acc: 0.69
Batch: 520; loss: 1.28; acc: 0.66
Batch: 540; loss: 1.11; acc: 0.75
Batch: 560; loss: 1.26; acc: 0.72
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.21; acc: 0.67
Batch: 620; loss: 1.11; acc: 0.73
Batch: 640; loss: 1.28; acc: 0.64
Batch: 660; loss: 1.17; acc: 0.72
Batch: 680; loss: 1.24; acc: 0.61
Batch: 700; loss: 1.18; acc: 0.67
Batch: 720; loss: 1.26; acc: 0.7
Batch: 740; loss: 1.27; acc: 0.62
Batch: 760; loss: 1.3; acc: 0.56
Batch: 780; loss: 1.15; acc: 0.67
Train Epoch over. train_loss: 1.2; train_accuracy: 0.69 

0.00013388968363869935
0.00012700424122158438
Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 0.83; acc: 0.89
Batch: 60; loss: 1.03; acc: 0.77
Batch: 80; loss: 1.05; acc: 0.7
Batch: 100; loss: 1.04; acc: 0.83
Batch: 120; loss: 1.29; acc: 0.67
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1268505735002505; val_accuracy: 0.7278065286624203 

The current subspace-distance is: 0.00012700424122158438 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.23; acc: 0.69
Batch: 20; loss: 1.07; acc: 0.78
Batch: 40; loss: 1.44; acc: 0.52
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.14; acc: 0.64
Batch: 100; loss: 1.22; acc: 0.66
Batch: 120; loss: 1.1; acc: 0.67
Batch: 140; loss: 1.06; acc: 0.73
Batch: 160; loss: 1.19; acc: 0.72
Batch: 180; loss: 1.18; acc: 0.7
Batch: 200; loss: 1.23; acc: 0.69
Batch: 220; loss: 1.18; acc: 0.73
Batch: 240; loss: 1.26; acc: 0.67
Batch: 260; loss: 1.26; acc: 0.7
Batch: 280; loss: 1.22; acc: 0.66
Batch: 300; loss: 1.16; acc: 0.69
Batch: 320; loss: 1.1; acc: 0.78
Batch: 340; loss: 1.21; acc: 0.67
Batch: 360; loss: 1.33; acc: 0.59
Batch: 380; loss: 1.22; acc: 0.67
Batch: 400; loss: 1.26; acc: 0.64
Batch: 420; loss: 1.05; acc: 0.78
Batch: 440; loss: 1.13; acc: 0.75
Batch: 460; loss: 1.23; acc: 0.62
Batch: 480; loss: 1.14; acc: 0.78
Batch: 500; loss: 1.22; acc: 0.66
Batch: 520; loss: 1.12; acc: 0.7
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 1.29; acc: 0.62
Batch: 580; loss: 1.18; acc: 0.7
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 1.1; acc: 0.75
Batch: 640; loss: 1.11; acc: 0.75
Batch: 660; loss: 1.26; acc: 0.62
Batch: 680; loss: 1.27; acc: 0.67
Batch: 700; loss: 1.36; acc: 0.59
Batch: 720; loss: 1.16; acc: 0.78
Batch: 740; loss: 1.29; acc: 0.64
Batch: 760; loss: 1.1; acc: 0.75
Batch: 780; loss: 1.3; acc: 0.67
Train Epoch over. train_loss: 1.19; train_accuracy: 0.69 

0.00013847334776073694
0.0001313341490458697
Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 1.28; acc: 0.64
Batch: 40; loss: 0.83; acc: 0.89
Batch: 60; loss: 1.04; acc: 0.77
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 1.04; acc: 0.81
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 1.01; acc: 0.8
Val Epoch over. val_loss: 1.1260373907484067; val_accuracy: 0.7288017515923567 

The current subspace-distance is: 0.0001313341490458697 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.0; acc: 0.84
Batch: 20; loss: 1.23; acc: 0.67
Batch: 40; loss: 1.28; acc: 0.59
Batch: 60; loss: 1.12; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.66
Batch: 100; loss: 1.14; acc: 0.69
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 1.13; acc: 0.75
Batch: 160; loss: 1.35; acc: 0.66
Batch: 180; loss: 1.25; acc: 0.64
Batch: 200; loss: 1.27; acc: 0.62
Batch: 220; loss: 1.17; acc: 0.72
Batch: 240; loss: 1.17; acc: 0.77
Batch: 260; loss: 1.36; acc: 0.64
Batch: 280; loss: 1.08; acc: 0.69
Batch: 300; loss: 1.2; acc: 0.75
Batch: 320; loss: 1.17; acc: 0.78
Batch: 340; loss: 1.38; acc: 0.62
Batch: 360; loss: 1.2; acc: 0.7
Batch: 380; loss: 1.27; acc: 0.58
Batch: 400; loss: 1.28; acc: 0.67
Batch: 420; loss: 1.16; acc: 0.67
Batch: 440; loss: 1.1; acc: 0.77
Batch: 460; loss: 1.31; acc: 0.62
Batch: 480; loss: 1.24; acc: 0.66
Batch: 500; loss: 1.0; acc: 0.78
Batch: 520; loss: 1.14; acc: 0.7
Batch: 540; loss: 1.16; acc: 0.72
Batch: 560; loss: 1.24; acc: 0.67
Batch: 580; loss: 1.34; acc: 0.58
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 1.16; acc: 0.67
Batch: 640; loss: 1.41; acc: 0.56
Batch: 660; loss: 1.19; acc: 0.67
Batch: 680; loss: 1.28; acc: 0.73
Batch: 700; loss: 1.0; acc: 0.8
Batch: 720; loss: 1.25; acc: 0.7
Batch: 740; loss: 1.18; acc: 0.67
Batch: 760; loss: 1.1; acc: 0.69
Batch: 780; loss: 1.12; acc: 0.7
Train Epoch over. train_loss: 1.19; train_accuracy: 0.69 

0.00014201694284565747
0.00013584842963609844
Batch: 0; loss: 1.1; acc: 0.73
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.81; acc: 0.89
Batch: 60; loss: 1.01; acc: 0.77
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.28; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.1049215892317947; val_accuracy: 0.7286027070063694 

The current subspace-distance is: 0.00013584842963609844 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.77
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 1.29; acc: 0.62
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 1.15; acc: 0.72
Batch: 100; loss: 1.14; acc: 0.69
Batch: 120; loss: 1.06; acc: 0.69
Batch: 140; loss: 1.07; acc: 0.75
Batch: 160; loss: 1.03; acc: 0.75
Batch: 180; loss: 1.13; acc: 0.72
Batch: 200; loss: 0.93; acc: 0.77
Batch: 220; loss: 1.09; acc: 0.78
Batch: 240; loss: 1.2; acc: 0.69
Batch: 260; loss: 1.22; acc: 0.69
Batch: 280; loss: 1.32; acc: 0.55
Batch: 300; loss: 1.44; acc: 0.59
Batch: 320; loss: 1.28; acc: 0.61
Batch: 340; loss: 1.2; acc: 0.61
Batch: 360; loss: 0.97; acc: 0.81
Batch: 380; loss: 1.23; acc: 0.66
Batch: 400; loss: 1.09; acc: 0.73
Batch: 420; loss: 1.26; acc: 0.67
Batch: 440; loss: 1.03; acc: 0.73
Batch: 460; loss: 1.13; acc: 0.77
Batch: 480; loss: 1.08; acc: 0.75
Batch: 500; loss: 1.07; acc: 0.72
Batch: 520; loss: 1.13; acc: 0.77
Batch: 540; loss: 1.45; acc: 0.58
Batch: 560; loss: 0.98; acc: 0.78
Batch: 580; loss: 1.32; acc: 0.59
Batch: 600; loss: 1.24; acc: 0.67
Batch: 620; loss: 1.07; acc: 0.75
Batch: 640; loss: 1.11; acc: 0.69
Batch: 660; loss: 1.02; acc: 0.75
Batch: 680; loss: 1.17; acc: 0.7
Batch: 700; loss: 1.13; acc: 0.72
Batch: 720; loss: 1.2; acc: 0.59
Batch: 740; loss: 1.23; acc: 0.62
Batch: 760; loss: 1.4; acc: 0.56
Batch: 780; loss: 1.19; acc: 0.66
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.00014543601719196886
0.00013746845070272684
Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.81; acc: 0.89
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.1053602255074082; val_accuracy: 0.7257165605095541 

The current subspace-distance is: 0.00013746845070272684 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.02; acc: 0.78
Batch: 20; loss: 1.23; acc: 0.73
Batch: 40; loss: 1.01; acc: 0.81
Batch: 60; loss: 1.02; acc: 0.78
Batch: 80; loss: 1.06; acc: 0.78
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 1.15; acc: 0.69
Batch: 160; loss: 1.08; acc: 0.77
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.11; acc: 0.77
Batch: 220; loss: 1.14; acc: 0.73
Batch: 240; loss: 1.18; acc: 0.7
Batch: 260; loss: 1.24; acc: 0.66
Batch: 280; loss: 0.93; acc: 0.88
Batch: 300; loss: 1.3; acc: 0.64
Batch: 320; loss: 1.35; acc: 0.55
Batch: 340; loss: 1.21; acc: 0.69
Batch: 360; loss: 1.02; acc: 0.77
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 1.17; acc: 0.67
Batch: 420; loss: 1.17; acc: 0.66
Batch: 440; loss: 1.04; acc: 0.72
Batch: 460; loss: 1.13; acc: 0.64
Batch: 480; loss: 1.29; acc: 0.61
Batch: 500; loss: 1.14; acc: 0.8
Batch: 520; loss: 1.3; acc: 0.61
Batch: 540; loss: 1.17; acc: 0.7
Batch: 560; loss: 1.13; acc: 0.72
Batch: 580; loss: 1.18; acc: 0.69
Batch: 600; loss: 1.16; acc: 0.73
Batch: 620; loss: 1.11; acc: 0.72
Batch: 640; loss: 1.09; acc: 0.69
Batch: 660; loss: 1.19; acc: 0.62
Batch: 680; loss: 1.13; acc: 0.7
Batch: 700; loss: 1.01; acc: 0.84
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 1.1; acc: 0.67
Batch: 760; loss: 1.29; acc: 0.64
Batch: 780; loss: 1.19; acc: 0.61
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.00014828331768512726
0.0001397462037857622
Batch: 0; loss: 1.09; acc: 0.8
Batch: 20; loss: 1.26; acc: 0.66
Batch: 40; loss: 0.8; acc: 0.88
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.81
Val Epoch over. val_loss: 1.0933670542042726; val_accuracy: 0.7319864649681529 

The current subspace-distance is: 0.0001397462037857622 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.2; acc: 0.67
Batch: 20; loss: 1.19; acc: 0.7
Batch: 40; loss: 1.09; acc: 0.73
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 1.15; acc: 0.72
Batch: 100; loss: 1.07; acc: 0.69
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 1.03; acc: 0.75
Batch: 160; loss: 1.14; acc: 0.7
Batch: 180; loss: 1.07; acc: 0.78
Batch: 200; loss: 1.21; acc: 0.62
Batch: 220; loss: 1.34; acc: 0.58
Batch: 240; loss: 1.36; acc: 0.59
Batch: 260; loss: 1.1; acc: 0.78
Batch: 280; loss: 1.22; acc: 0.67
Batch: 300; loss: 1.22; acc: 0.69
Batch: 320; loss: 1.15; acc: 0.7
Batch: 340; loss: 1.18; acc: 0.62
Batch: 360; loss: 1.31; acc: 0.55
Batch: 380; loss: 1.18; acc: 0.7
Batch: 400; loss: 1.32; acc: 0.61
Batch: 420; loss: 1.08; acc: 0.8
Batch: 440; loss: 1.24; acc: 0.64
Batch: 460; loss: 1.22; acc: 0.69
Batch: 480; loss: 1.32; acc: 0.61
Batch: 500; loss: 1.15; acc: 0.69
Batch: 520; loss: 1.09; acc: 0.72
Batch: 540; loss: 1.02; acc: 0.77
Batch: 560; loss: 1.17; acc: 0.7
Batch: 580; loss: 1.06; acc: 0.72
Batch: 600; loss: 1.06; acc: 0.78
Batch: 620; loss: 1.06; acc: 0.72
Batch: 640; loss: 1.07; acc: 0.78
Batch: 660; loss: 1.05; acc: 0.75
Batch: 680; loss: 1.04; acc: 0.77
Batch: 700; loss: 1.02; acc: 0.8
Batch: 720; loss: 1.03; acc: 0.77
Batch: 740; loss: 0.99; acc: 0.75
Batch: 760; loss: 1.21; acc: 0.69
Batch: 780; loss: 1.23; acc: 0.61
Train Epoch over. train_loss: 1.16; train_accuracy: 0.69 

0.00014825823018327355
0.00014133844524621964
Batch: 0; loss: 1.09; acc: 0.77
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.8; acc: 0.88
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 1.02; acc: 0.8
Batch: 100; loss: 1.03; acc: 0.75
Batch: 120; loss: 1.29; acc: 0.64
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.0862984437092094; val_accuracy: 0.7280055732484076 

The current subspace-distance is: 0.00014133844524621964 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.1; acc: 0.7
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.13; acc: 0.75
Batch: 60; loss: 1.1; acc: 0.73
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 1.09; acc: 0.75
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 1.16; acc: 0.72
Batch: 160; loss: 1.16; acc: 0.69
Batch: 180; loss: 1.04; acc: 0.75
Batch: 200; loss: 1.14; acc: 0.67
Batch: 220; loss: 1.1; acc: 0.73
Batch: 240; loss: 1.13; acc: 0.69
Batch: 260; loss: 1.15; acc: 0.69
Batch: 280; loss: 1.11; acc: 0.75
Batch: 300; loss: 1.09; acc: 0.8
Batch: 320; loss: 1.23; acc: 0.7
Batch: 340; loss: 1.18; acc: 0.72
Batch: 360; loss: 1.3; acc: 0.59
Batch: 380; loss: 1.1; acc: 0.78
Batch: 400; loss: 1.15; acc: 0.73
Batch: 420; loss: 1.22; acc: 0.64
Batch: 440; loss: 1.13; acc: 0.72
Batch: 460; loss: 1.18; acc: 0.66
Batch: 480; loss: 1.23; acc: 0.64
Batch: 500; loss: 0.96; acc: 0.77
Batch: 520; loss: 1.14; acc: 0.67
Batch: 540; loss: 1.13; acc: 0.77
Batch: 560; loss: 1.19; acc: 0.7
Batch: 580; loss: 1.1; acc: 0.72
Batch: 600; loss: 1.25; acc: 0.66
Batch: 620; loss: 1.13; acc: 0.73
Batch: 640; loss: 1.19; acc: 0.66
Batch: 660; loss: 1.19; acc: 0.7
Batch: 680; loss: 1.09; acc: 0.78
Batch: 700; loss: 1.29; acc: 0.61
Batch: 720; loss: 1.14; acc: 0.72
Batch: 740; loss: 1.06; acc: 0.78
Batch: 760; loss: 1.09; acc: 0.7
Batch: 780; loss: 1.1; acc: 0.75
Train Epoch over. train_loss: 1.15; train_accuracy: 0.7 

0.00015143158088903874
0.00014565461606252939
Batch: 0; loss: 1.08; acc: 0.78
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.79; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.78
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.94; acc: 0.83
Val Epoch over. val_loss: 1.0742702643582775; val_accuracy: 0.7345740445859873 

The current subspace-distance is: 0.00014565461606252939 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.92; acc: 0.89
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 1.16; acc: 0.72
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 1.25; acc: 0.62
Batch: 100; loss: 1.21; acc: 0.64
Batch: 120; loss: 1.1; acc: 0.69
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 1.11; acc: 0.72
Batch: 180; loss: 1.1; acc: 0.7
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 1.06; acc: 0.72
Batch: 240; loss: 1.22; acc: 0.7
Batch: 260; loss: 1.16; acc: 0.77
Batch: 280; loss: 1.1; acc: 0.77
Batch: 300; loss: 1.29; acc: 0.59
Batch: 320; loss: 1.27; acc: 0.56
Batch: 340; loss: 1.09; acc: 0.69
Batch: 360; loss: 1.08; acc: 0.67
Batch: 380; loss: 1.29; acc: 0.58
Batch: 400; loss: 1.15; acc: 0.72
Batch: 420; loss: 1.28; acc: 0.62
Batch: 440; loss: 1.23; acc: 0.61
Batch: 460; loss: 1.21; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.0; acc: 0.78
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 1.36; acc: 0.64
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.04; acc: 0.75
Batch: 640; loss: 1.14; acc: 0.64
Batch: 660; loss: 1.21; acc: 0.59
Batch: 680; loss: 0.95; acc: 0.83
Batch: 700; loss: 1.17; acc: 0.73
Batch: 720; loss: 1.25; acc: 0.66
Batch: 740; loss: 1.05; acc: 0.75
Batch: 760; loss: 1.18; acc: 0.7
Batch: 780; loss: 1.1; acc: 0.7
Train Epoch over. train_loss: 1.14; train_accuracy: 0.7 

0.00015298057405743748
0.0001481269864598289
Batch: 0; loss: 1.07; acc: 0.77
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.78
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.95; acc: 0.81
Val Epoch over. val_loss: 1.0677299480529348; val_accuracy: 0.7384554140127388 

The current subspace-distance is: 0.0001481269864598289 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.22; acc: 0.72
Batch: 40; loss: 1.17; acc: 0.66
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.23; acc: 0.61
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 1.18; acc: 0.73
Batch: 160; loss: 1.21; acc: 0.64
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 1.21; acc: 0.72
Batch: 220; loss: 1.12; acc: 0.7
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 1.19; acc: 0.62
Batch: 280; loss: 1.15; acc: 0.69
Batch: 300; loss: 1.03; acc: 0.7
Batch: 320; loss: 1.03; acc: 0.78
Batch: 340; loss: 1.18; acc: 0.66
Batch: 360; loss: 1.16; acc: 0.7
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 1.13; acc: 0.7
Batch: 420; loss: 1.06; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.73
Batch: 460; loss: 0.94; acc: 0.84
Batch: 480; loss: 1.21; acc: 0.7
Batch: 500; loss: 1.15; acc: 0.75
Batch: 520; loss: 1.19; acc: 0.69
Batch: 540; loss: 1.06; acc: 0.77
Batch: 560; loss: 1.27; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.77
Batch: 600; loss: 1.26; acc: 0.67
Batch: 620; loss: 1.1; acc: 0.69
Batch: 640; loss: 0.93; acc: 0.8
Batch: 660; loss: 1.28; acc: 0.66
Batch: 680; loss: 1.31; acc: 0.64
Batch: 700; loss: 1.21; acc: 0.59
Batch: 720; loss: 1.05; acc: 0.73
Batch: 740; loss: 1.01; acc: 0.75
Batch: 760; loss: 1.1; acc: 0.73
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 1.13; train_accuracy: 0.7 

0.00015838220133446157
0.00015083124162629247
Batch: 0; loss: 1.06; acc: 0.78
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 1.03; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.81
Val Epoch over. val_loss: 1.062227560076744; val_accuracy: 0.7376592356687898 

The current subspace-distance is: 0.00015083124162629247 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 1.06; acc: 0.7
Batch: 40; loss: 1.16; acc: 0.64
Batch: 60; loss: 1.04; acc: 0.77
Batch: 80; loss: 1.03; acc: 0.73
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 1.12; acc: 0.72
Batch: 160; loss: 1.05; acc: 0.73
Batch: 180; loss: 1.25; acc: 0.62
Batch: 200; loss: 1.19; acc: 0.72
Batch: 220; loss: 1.15; acc: 0.7
Batch: 240; loss: 1.08; acc: 0.78
Batch: 260; loss: 1.17; acc: 0.61
Batch: 280; loss: 1.07; acc: 0.7
Batch: 300; loss: 1.24; acc: 0.67
Batch: 320; loss: 1.15; acc: 0.7
Batch: 340; loss: 1.08; acc: 0.69
Batch: 360; loss: 1.21; acc: 0.66
Batch: 380; loss: 1.07; acc: 0.73
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 1.1; acc: 0.73
Batch: 440; loss: 1.01; acc: 0.78
Batch: 460; loss: 1.16; acc: 0.73
Batch: 480; loss: 1.12; acc: 0.78
Batch: 500; loss: 1.19; acc: 0.62
Batch: 520; loss: 0.93; acc: 0.81
Batch: 540; loss: 0.96; acc: 0.83
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 0.9; acc: 0.77
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 1.07; acc: 0.72
Batch: 640; loss: 1.0; acc: 0.8
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.15; acc: 0.66
Batch: 700; loss: 1.26; acc: 0.59
Batch: 720; loss: 1.25; acc: 0.67
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 1.27; acc: 0.59
Batch: 780; loss: 1.0; acc: 0.77
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.00016239823889918625
0.00015282515960279852
Batch: 0; loss: 1.03; acc: 0.78
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.78
Batch: 80; loss: 0.97; acc: 0.8
Batch: 100; loss: 1.01; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.84
Val Epoch over. val_loss: 1.0418035528462404; val_accuracy: 0.7423367834394905 

The current subspace-distance is: 0.00015282515960279852 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.69
Batch: 20; loss: 1.03; acc: 0.73
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.69
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.58
Batch: 140; loss: 1.1; acc: 0.73
Batch: 160; loss: 1.26; acc: 0.55
Batch: 180; loss: 1.12; acc: 0.66
Batch: 200; loss: 1.08; acc: 0.7
Batch: 220; loss: 1.13; acc: 0.72
Batch: 240; loss: 0.97; acc: 0.83
Batch: 260; loss: 0.99; acc: 0.8
Batch: 280; loss: 1.23; acc: 0.64
Batch: 300; loss: 1.19; acc: 0.67
Batch: 320; loss: 0.91; acc: 0.78
Batch: 340; loss: 1.19; acc: 0.67
Batch: 360; loss: 1.16; acc: 0.64
Batch: 380; loss: 1.12; acc: 0.7
Batch: 400; loss: 0.92; acc: 0.75
Batch: 420; loss: 1.21; acc: 0.7
Batch: 440; loss: 1.23; acc: 0.61
Batch: 460; loss: 1.11; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.75
Batch: 500; loss: 1.08; acc: 0.72
Batch: 520; loss: 1.22; acc: 0.66
Batch: 540; loss: 1.06; acc: 0.8
Batch: 560; loss: 1.12; acc: 0.66
Batch: 580; loss: 1.1; acc: 0.67
Batch: 600; loss: 1.22; acc: 0.67
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 1.26; acc: 0.61
Batch: 660; loss: 1.11; acc: 0.75
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 1.05; acc: 0.77
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 1.16; acc: 0.67
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.06; acc: 0.86
Train Epoch over. train_loss: 1.12; train_accuracy: 0.7 

0.0001595913927303627
0.00015409925254061818
Batch: 0; loss: 1.04; acc: 0.77
Batch: 20; loss: 1.26; acc: 0.61
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.99; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.77
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.0480847912988844; val_accuracy: 0.7395501592356688 

The current subspace-distance is: 0.00015409925254061818 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 0.98; acc: 0.75
Batch: 60; loss: 1.14; acc: 0.67
Batch: 80; loss: 1.23; acc: 0.69
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.16; acc: 0.69
Batch: 140; loss: 1.13; acc: 0.73
Batch: 160; loss: 1.1; acc: 0.73
Batch: 180; loss: 1.12; acc: 0.62
Batch: 200; loss: 1.31; acc: 0.59
Batch: 220; loss: 1.01; acc: 0.78
Batch: 240; loss: 1.14; acc: 0.7
Batch: 260; loss: 1.06; acc: 0.75
Batch: 280; loss: 1.18; acc: 0.69
Batch: 300; loss: 0.99; acc: 0.75
Batch: 320; loss: 1.17; acc: 0.69
Batch: 340; loss: 0.96; acc: 0.75
Batch: 360; loss: 1.05; acc: 0.66
Batch: 380; loss: 0.98; acc: 0.8
Batch: 400; loss: 1.14; acc: 0.69
Batch: 420; loss: 1.12; acc: 0.66
Batch: 440; loss: 1.18; acc: 0.66
Batch: 460; loss: 1.28; acc: 0.64
Batch: 480; loss: 1.11; acc: 0.77
Batch: 500; loss: 1.33; acc: 0.56
Batch: 520; loss: 1.11; acc: 0.7
Batch: 540; loss: 1.09; acc: 0.67
Batch: 560; loss: 0.94; acc: 0.78
Batch: 580; loss: 1.08; acc: 0.69
Batch: 600; loss: 1.21; acc: 0.64
Batch: 620; loss: 0.95; acc: 0.73
Batch: 640; loss: 1.2; acc: 0.69
Batch: 660; loss: 1.11; acc: 0.72
Batch: 680; loss: 1.01; acc: 0.73
Batch: 700; loss: 1.04; acc: 0.73
Batch: 720; loss: 1.01; acc: 0.77
Batch: 740; loss: 1.08; acc: 0.78
Batch: 760; loss: 1.05; acc: 0.7
Batch: 780; loss: 1.12; acc: 0.69
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00016214314382523298
0.00015548650117125362
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.24; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.95; acc: 0.8
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 1.02; acc: 0.77
Batch: 120; loss: 1.26; acc: 0.64
Batch: 140; loss: 0.92; acc: 0.83
Val Epoch over. val_loss: 1.0417626257155352; val_accuracy: 0.7423367834394905 

The current subspace-distance is: 0.00015548650117125362 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.26; acc: 0.62
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.0; acc: 0.77
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 1.35; acc: 0.56
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.72
Batch: 140; loss: 1.23; acc: 0.67
Batch: 160; loss: 1.33; acc: 0.56
Batch: 180; loss: 1.13; acc: 0.69
Batch: 200; loss: 1.02; acc: 0.77
Batch: 220; loss: 1.28; acc: 0.62
Batch: 240; loss: 0.97; acc: 0.73
Batch: 260; loss: 1.09; acc: 0.7
Batch: 280; loss: 1.09; acc: 0.75
Batch: 300; loss: 1.01; acc: 0.72
Batch: 320; loss: 1.07; acc: 0.72
Batch: 340; loss: 1.08; acc: 0.8
Batch: 360; loss: 1.2; acc: 0.64
Batch: 380; loss: 1.06; acc: 0.8
Batch: 400; loss: 1.1; acc: 0.7
Batch: 420; loss: 1.17; acc: 0.62
Batch: 440; loss: 1.05; acc: 0.8
Batch: 460; loss: 1.03; acc: 0.73
Batch: 480; loss: 1.33; acc: 0.67
Batch: 500; loss: 1.13; acc: 0.66
Batch: 520; loss: 1.1; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.66
Batch: 560; loss: 1.1; acc: 0.7
Batch: 580; loss: 1.05; acc: 0.72
Batch: 600; loss: 1.09; acc: 0.75
Batch: 620; loss: 1.05; acc: 0.72
Batch: 640; loss: 1.16; acc: 0.69
Batch: 660; loss: 1.06; acc: 0.78
Batch: 680; loss: 1.28; acc: 0.67
Batch: 700; loss: 1.15; acc: 0.7
Batch: 720; loss: 1.13; acc: 0.69
Batch: 740; loss: 1.12; acc: 0.73
Batch: 760; loss: 1.05; acc: 0.72
Batch: 780; loss: 1.03; acc: 0.73
Train Epoch over. train_loss: 1.11; train_accuracy: 0.71 

0.00016685054288245738
0.00016018458700273186
Batch: 0; loss: 1.02; acc: 0.78
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 1.01; acc: 0.77
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.84
Val Epoch over. val_loss: 1.0331811939075495; val_accuracy: 0.7455214968152867 

The current subspace-distance is: 0.00016018458700273186 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.04; acc: 0.75
Batch: 20; loss: 1.09; acc: 0.7
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 1.11; acc: 0.72
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.69
Batch: 160; loss: 1.16; acc: 0.62
Batch: 180; loss: 1.24; acc: 0.66
Batch: 200; loss: 1.04; acc: 0.78
Batch: 220; loss: 1.25; acc: 0.67
Batch: 240; loss: 1.04; acc: 0.7
Batch: 260; loss: 1.25; acc: 0.69
Batch: 280; loss: 1.13; acc: 0.62
Batch: 300; loss: 1.07; acc: 0.69
Batch: 320; loss: 1.06; acc: 0.73
Batch: 340; loss: 1.14; acc: 0.73
Batch: 360; loss: 0.98; acc: 0.8
Batch: 380; loss: 1.21; acc: 0.62
Batch: 400; loss: 1.04; acc: 0.77
Batch: 420; loss: 0.9; acc: 0.83
Batch: 440; loss: 1.22; acc: 0.62
Batch: 460; loss: 0.98; acc: 0.67
Batch: 480; loss: 1.18; acc: 0.64
Batch: 500; loss: 1.22; acc: 0.62
Batch: 520; loss: 1.0; acc: 0.77
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 1.2; acc: 0.69
Batch: 580; loss: 1.15; acc: 0.72
Batch: 600; loss: 1.19; acc: 0.69
Batch: 620; loss: 1.15; acc: 0.64
Batch: 640; loss: 1.06; acc: 0.69
Batch: 660; loss: 1.41; acc: 0.55
Batch: 680; loss: 0.94; acc: 0.78
Batch: 700; loss: 1.21; acc: 0.72
Batch: 720; loss: 1.23; acc: 0.64
Batch: 740; loss: 1.35; acc: 0.61
Batch: 760; loss: 1.02; acc: 0.75
Batch: 780; loss: 1.01; acc: 0.78
Train Epoch over. train_loss: 1.11; train_accuracy: 0.7 

0.00016672455240041018
0.00015962455654516816
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 1.01; acc: 0.77
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.83
Val Epoch over. val_loss: 1.0385220103962407; val_accuracy: 0.7466162420382165 

The current subspace-distance is: 0.00015962455654516816 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 1.11; acc: 0.77
Batch: 40; loss: 1.13; acc: 0.69
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 1.1; acc: 0.8
Batch: 100; loss: 0.94; acc: 0.77
Batch: 120; loss: 1.1; acc: 0.7
Batch: 140; loss: 1.08; acc: 0.73
Batch: 160; loss: 1.26; acc: 0.59
Batch: 180; loss: 1.17; acc: 0.7
Batch: 200; loss: 1.14; acc: 0.75
Batch: 220; loss: 1.02; acc: 0.81
Batch: 240; loss: 1.24; acc: 0.64
Batch: 260; loss: 1.16; acc: 0.69
Batch: 280; loss: 1.14; acc: 0.62
Batch: 300; loss: 1.14; acc: 0.77
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.04; acc: 0.75
Batch: 360; loss: 0.95; acc: 0.75
Batch: 380; loss: 1.01; acc: 0.75
Batch: 400; loss: 1.02; acc: 0.73
Batch: 420; loss: 1.01; acc: 0.73
Batch: 440; loss: 1.13; acc: 0.64
Batch: 460; loss: 1.08; acc: 0.69
Batch: 480; loss: 0.99; acc: 0.72
Batch: 500; loss: 1.06; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.66
Batch: 540; loss: 0.88; acc: 0.88
Batch: 560; loss: 0.97; acc: 0.78
Batch: 580; loss: 1.25; acc: 0.62
Batch: 600; loss: 1.04; acc: 0.73
Batch: 620; loss: 1.03; acc: 0.78
Batch: 640; loss: 1.16; acc: 0.7
Batch: 660; loss: 0.95; acc: 0.77
Batch: 680; loss: 1.06; acc: 0.72
Batch: 700; loss: 1.08; acc: 0.7
Batch: 720; loss: 1.12; acc: 0.72
Batch: 740; loss: 1.12; acc: 0.69
Batch: 760; loss: 1.09; acc: 0.77
Batch: 780; loss: 1.09; acc: 0.75
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00016705751477275044
0.0001594534987816587
Batch: 0; loss: 1.02; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.01; acc: 0.78
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.91; acc: 0.83
Val Epoch over. val_loss: 1.0326866921345899; val_accuracy: 0.7439291401273885 

The current subspace-distance is: 0.0001594534987816587 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.06; acc: 0.72
Batch: 40; loss: 1.11; acc: 0.66
Batch: 60; loss: 1.03; acc: 0.77
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.1; acc: 0.69
Batch: 140; loss: 1.17; acc: 0.59
Batch: 160; loss: 1.08; acc: 0.72
Batch: 180; loss: 1.07; acc: 0.72
Batch: 200; loss: 1.02; acc: 0.72
Batch: 220; loss: 0.96; acc: 0.8
Batch: 240; loss: 1.06; acc: 0.72
Batch: 260; loss: 1.15; acc: 0.7
Batch: 280; loss: 1.06; acc: 0.73
Batch: 300; loss: 1.11; acc: 0.7
Batch: 320; loss: 1.26; acc: 0.67
Batch: 340; loss: 1.09; acc: 0.67
Batch: 360; loss: 1.0; acc: 0.8
Batch: 380; loss: 1.05; acc: 0.73
Batch: 400; loss: 1.03; acc: 0.67
Batch: 420; loss: 1.06; acc: 0.73
Batch: 440; loss: 0.99; acc: 0.72
Batch: 460; loss: 1.11; acc: 0.69
Batch: 480; loss: 1.04; acc: 0.75
Batch: 500; loss: 1.18; acc: 0.7
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 1.1; acc: 0.66
Batch: 560; loss: 1.03; acc: 0.72
Batch: 580; loss: 1.05; acc: 0.72
Batch: 600; loss: 1.14; acc: 0.66
Batch: 620; loss: 1.27; acc: 0.61
Batch: 640; loss: 1.1; acc: 0.81
Batch: 660; loss: 1.1; acc: 0.72
Batch: 680; loss: 1.09; acc: 0.77
Batch: 700; loss: 1.06; acc: 0.66
Batch: 720; loss: 1.25; acc: 0.62
Batch: 740; loss: 0.94; acc: 0.8
Batch: 760; loss: 1.07; acc: 0.73
Batch: 780; loss: 1.23; acc: 0.67
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.0001657611719565466
0.0001607615122338757
Batch: 0; loss: 1.01; acc: 0.77
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.81
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 0.92; acc: 0.81
Val Epoch over. val_loss: 1.0322102577822982; val_accuracy: 0.7467157643312102 

The current subspace-distance is: 0.0001607615122338757 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.14; acc: 0.66
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 1.09; acc: 0.72
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.66
Batch: 100; loss: 1.08; acc: 0.64
Batch: 120; loss: 1.05; acc: 0.69
Batch: 140; loss: 1.06; acc: 0.73
Batch: 160; loss: 1.0; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.75
Batch: 200; loss: 0.97; acc: 0.78
Batch: 220; loss: 1.06; acc: 0.73
Batch: 240; loss: 0.97; acc: 0.77
Batch: 260; loss: 1.2; acc: 0.69
Batch: 280; loss: 1.06; acc: 0.7
Batch: 300; loss: 1.14; acc: 0.73
Batch: 320; loss: 0.94; acc: 0.78
Batch: 340; loss: 1.2; acc: 0.67
Batch: 360; loss: 1.02; acc: 0.73
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 1.16; acc: 0.64
Batch: 420; loss: 0.98; acc: 0.77
Batch: 440; loss: 1.24; acc: 0.62
Batch: 460; loss: 0.96; acc: 0.81
Batch: 480; loss: 1.11; acc: 0.7
Batch: 500; loss: 1.05; acc: 0.72
Batch: 520; loss: 1.11; acc: 0.72
Batch: 540; loss: 0.96; acc: 0.78
Batch: 560; loss: 0.97; acc: 0.77
Batch: 580; loss: 1.12; acc: 0.69
Batch: 600; loss: 0.98; acc: 0.78
Batch: 620; loss: 1.23; acc: 0.64
Batch: 640; loss: 1.24; acc: 0.59
Batch: 660; loss: 1.01; acc: 0.78
Batch: 680; loss: 1.17; acc: 0.67
Batch: 700; loss: 0.93; acc: 0.8
Batch: 720; loss: 1.12; acc: 0.69
Batch: 740; loss: 0.99; acc: 0.83
Batch: 760; loss: 1.06; acc: 0.73
Batch: 780; loss: 1.09; acc: 0.73
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.0001701291766948998
0.00016283884178847075
Batch: 0; loss: 1.01; acc: 0.77
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.93; acc: 0.78
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.92; acc: 0.81
Val Epoch over. val_loss: 1.0274437931692524; val_accuracy: 0.7433320063694268 

The current subspace-distance is: 0.00016283884178847075 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.21; acc: 0.67
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 0.88; acc: 0.89
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 1.13; acc: 0.72
Batch: 160; loss: 1.23; acc: 0.66
Batch: 180; loss: 1.05; acc: 0.67
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 1.16; acc: 0.64
Batch: 240; loss: 1.08; acc: 0.72
Batch: 260; loss: 1.1; acc: 0.78
Batch: 280; loss: 1.16; acc: 0.66
Batch: 300; loss: 1.0; acc: 0.69
Batch: 320; loss: 1.23; acc: 0.62
Batch: 340; loss: 1.0; acc: 0.73
Batch: 360; loss: 1.22; acc: 0.77
Batch: 380; loss: 1.05; acc: 0.75
Batch: 400; loss: 0.96; acc: 0.77
Batch: 420; loss: 1.06; acc: 0.73
Batch: 440; loss: 1.05; acc: 0.69
Batch: 460; loss: 1.05; acc: 0.73
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 1.07; acc: 0.72
Batch: 520; loss: 0.95; acc: 0.77
Batch: 540; loss: 1.09; acc: 0.73
Batch: 560; loss: 0.95; acc: 0.75
Batch: 580; loss: 0.99; acc: 0.73
Batch: 600; loss: 1.03; acc: 0.75
Batch: 620; loss: 1.24; acc: 0.7
Batch: 640; loss: 1.17; acc: 0.67
Batch: 660; loss: 1.01; acc: 0.78
Batch: 680; loss: 1.11; acc: 0.73
Batch: 700; loss: 1.19; acc: 0.66
Batch: 720; loss: 1.29; acc: 0.58
Batch: 740; loss: 1.15; acc: 0.69
Batch: 760; loss: 1.0; acc: 0.78
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 1.1; train_accuracy: 0.71 

0.00016624029376544058
0.00016224161663558334
Batch: 0; loss: 1.01; acc: 0.77
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.81
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.83
Val Epoch over. val_loss: 1.0296695015992328; val_accuracy: 0.7446257961783439 

The current subspace-distance is: 0.00016224161663558334 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.01; acc: 0.77
Batch: 20; loss: 1.0; acc: 0.78
Batch: 40; loss: 0.99; acc: 0.8
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.73
Batch: 100; loss: 0.98; acc: 0.8
Batch: 120; loss: 0.99; acc: 0.78
Batch: 140; loss: 1.09; acc: 0.7
Batch: 160; loss: 1.08; acc: 0.72
Batch: 180; loss: 1.09; acc: 0.66
Batch: 200; loss: 0.98; acc: 0.78
Batch: 220; loss: 1.03; acc: 0.73
Batch: 240; loss: 0.95; acc: 0.81
Batch: 260; loss: 1.15; acc: 0.64
Batch: 280; loss: 1.22; acc: 0.66
Batch: 300; loss: 0.98; acc: 0.72
Batch: 320; loss: 0.99; acc: 0.72
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.13; acc: 0.77
Batch: 380; loss: 1.12; acc: 0.69
Batch: 400; loss: 0.98; acc: 0.78
Batch: 420; loss: 0.97; acc: 0.84
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.2; acc: 0.69
Batch: 480; loss: 1.02; acc: 0.73
Batch: 500; loss: 1.12; acc: 0.72
Batch: 520; loss: 1.06; acc: 0.69
Batch: 540; loss: 1.13; acc: 0.66
Batch: 560; loss: 1.09; acc: 0.72
Batch: 580; loss: 0.98; acc: 0.83
Batch: 600; loss: 1.14; acc: 0.69
Batch: 620; loss: 0.99; acc: 0.72
Batch: 640; loss: 1.2; acc: 0.64
Batch: 660; loss: 0.89; acc: 0.8
Batch: 680; loss: 1.06; acc: 0.72
Batch: 700; loss: 1.03; acc: 0.77
Batch: 720; loss: 1.11; acc: 0.64
Batch: 740; loss: 1.07; acc: 0.69
Batch: 760; loss: 1.32; acc: 0.61
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.00017029639275278896
0.00016533552843611687
Batch: 0; loss: 1.0; acc: 0.77
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.93; acc: 0.78
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.83
Val Epoch over. val_loss: 1.0241059019307421; val_accuracy: 0.745421974522293 

The current subspace-distance is: 0.00016533552843611687 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.72
Batch: 20; loss: 1.06; acc: 0.73
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 1.2; acc: 0.67
Batch: 80; loss: 1.07; acc: 0.75
Batch: 100; loss: 1.18; acc: 0.67
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 1.19; acc: 0.7
Batch: 160; loss: 1.27; acc: 0.66
Batch: 180; loss: 1.05; acc: 0.7
Batch: 200; loss: 1.06; acc: 0.75
Batch: 220; loss: 1.31; acc: 0.58
Batch: 240; loss: 1.02; acc: 0.73
Batch: 260; loss: 1.09; acc: 0.69
Batch: 280; loss: 1.06; acc: 0.75
Batch: 300; loss: 1.0; acc: 0.7
Batch: 320; loss: 1.07; acc: 0.69
Batch: 340; loss: 1.12; acc: 0.69
Batch: 360; loss: 1.0; acc: 0.72
Batch: 380; loss: 0.97; acc: 0.75
Batch: 400; loss: 1.15; acc: 0.61
Batch: 420; loss: 1.01; acc: 0.77
Batch: 440; loss: 1.01; acc: 0.73
Batch: 460; loss: 0.99; acc: 0.77
Batch: 480; loss: 1.12; acc: 0.72
Batch: 500; loss: 1.03; acc: 0.69
Batch: 520; loss: 1.08; acc: 0.67
Batch: 540; loss: 1.22; acc: 0.67
Batch: 560; loss: 1.2; acc: 0.69
Batch: 580; loss: 1.02; acc: 0.7
Batch: 600; loss: 0.98; acc: 0.73
Batch: 620; loss: 1.06; acc: 0.7
Batch: 640; loss: 1.04; acc: 0.77
Batch: 660; loss: 1.02; acc: 0.77
Batch: 680; loss: 1.13; acc: 0.64
Batch: 700; loss: 1.13; acc: 0.7
Batch: 720; loss: 1.16; acc: 0.66
Batch: 740; loss: 1.09; acc: 0.67
Batch: 760; loss: 1.13; acc: 0.69
Batch: 780; loss: 1.14; acc: 0.66
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

0.0001676665706327185
0.00016221639816649258
Batch: 0; loss: 1.0; acc: 0.78
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.0; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 0.92; acc: 0.83
Val Epoch over. val_loss: 1.0316523313522339; val_accuracy: 0.7434315286624203 

The current subspace-distance is: 0.00016221639816649258 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_8_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.4265201420964964

The number of parameters is: 257601

The number of individual parameters is:

20
360
20
20
30
40800
30
30
59
120360
59
59
64
90624
64
64
4096
64
640
10
64
64

nonzero elements in E: 51520196
elements in E: 51520200
fraction nonzero: 0.9999999223605498
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.38; acc: 0.09
Batch: 20; loss: 2.23; acc: 0.16
Batch: 40; loss: 2.09; acc: 0.33
Batch: 60; loss: 1.87; acc: 0.41
Batch: 80; loss: 1.83; acc: 0.44
Batch: 100; loss: 1.87; acc: 0.45
Batch: 120; loss: 1.67; acc: 0.66
Batch: 140; loss: 1.68; acc: 0.61
Batch: 160; loss: 1.8; acc: 0.45
Batch: 180; loss: 1.51; acc: 0.67
Batch: 200; loss: 1.57; acc: 0.72
Batch: 220; loss: 1.57; acc: 0.69
Batch: 240; loss: 1.57; acc: 0.64
Batch: 260; loss: 1.51; acc: 0.66
Batch: 280; loss: 1.47; acc: 0.69
Batch: 300; loss: 1.54; acc: 0.66
Batch: 320; loss: 1.57; acc: 0.53
Batch: 340; loss: 1.53; acc: 0.62
Batch: 360; loss: 1.46; acc: 0.62
Batch: 380; loss: 1.46; acc: 0.61
Batch: 400; loss: 1.43; acc: 0.69
Batch: 420; loss: 1.43; acc: 0.69
Batch: 440; loss: 1.48; acc: 0.62
Batch: 460; loss: 1.44; acc: 0.66
Batch: 480; loss: 1.4; acc: 0.7
Batch: 500; loss: 1.33; acc: 0.69
Batch: 520; loss: 1.42; acc: 0.67
Batch: 540; loss: 1.4; acc: 0.7
Batch: 560; loss: 1.33; acc: 0.69
Batch: 580; loss: 1.47; acc: 0.64
Batch: 600; loss: 1.39; acc: 0.61
Batch: 620; loss: 1.31; acc: 0.72
Batch: 640; loss: 1.25; acc: 0.73
Batch: 660; loss: 1.2; acc: 0.8
Batch: 680; loss: 1.26; acc: 0.8
Batch: 700; loss: 1.27; acc: 0.73
Batch: 720; loss: 1.31; acc: 0.73
Batch: 740; loss: 1.18; acc: 0.84
Batch: 760; loss: 1.21; acc: 0.69
Batch: 780; loss: 1.19; acc: 0.81
Train Epoch over. train_loss: 1.51; train_accuracy: 0.63 

5.7767647376749665e-05
5.3327297791838646e-05
Batch: 0; loss: 1.27; acc: 0.81
Batch: 20; loss: 1.35; acc: 0.7
Batch: 40; loss: 0.95; acc: 0.89
Batch: 60; loss: 1.16; acc: 0.81
Batch: 80; loss: 1.13; acc: 0.8
Batch: 100; loss: 1.16; acc: 0.91
Batch: 120; loss: 1.27; acc: 0.75
Batch: 140; loss: 1.15; acc: 0.83
Val Epoch over. val_loss: 1.2110604795680684; val_accuracy: 0.7701035031847133 

The current subspace-distance is: 5.3327297791838646e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.7
Batch: 20; loss: 1.3; acc: 0.73
Batch: 40; loss: 1.26; acc: 0.72
Batch: 60; loss: 1.21; acc: 0.75
Batch: 80; loss: 1.26; acc: 0.75
Batch: 100; loss: 1.22; acc: 0.77
Batch: 120; loss: 1.3; acc: 0.7
Batch: 140; loss: 1.16; acc: 0.78
Batch: 160; loss: 1.32; acc: 0.7
Batch: 180; loss: 1.23; acc: 0.73
Batch: 200; loss: 1.29; acc: 0.7
Batch: 220; loss: 1.2; acc: 0.72
Batch: 240; loss: 1.29; acc: 0.7
Batch: 260; loss: 1.34; acc: 0.62
Batch: 280; loss: 1.1; acc: 0.8
Batch: 300; loss: 1.17; acc: 0.8
Batch: 320; loss: 1.17; acc: 0.78
Batch: 340; loss: 1.19; acc: 0.8
Batch: 360; loss: 1.14; acc: 0.78
Batch: 380; loss: 1.18; acc: 0.73
Batch: 400; loss: 1.21; acc: 0.7
Batch: 420; loss: 1.22; acc: 0.81
Batch: 440; loss: 1.21; acc: 0.77
Batch: 460; loss: 1.21; acc: 0.73
Batch: 480; loss: 1.22; acc: 0.7
Batch: 500; loss: 1.2; acc: 0.75
Batch: 520; loss: 1.15; acc: 0.84
Batch: 540; loss: 1.01; acc: 0.81
Batch: 560; loss: 1.17; acc: 0.73
Batch: 580; loss: 1.14; acc: 0.84
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.2; acc: 0.78
Batch: 640; loss: 1.23; acc: 0.69
Batch: 660; loss: 1.2; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.75
Batch: 700; loss: 1.17; acc: 0.81
Batch: 720; loss: 1.11; acc: 0.84
Batch: 740; loss: 1.1; acc: 0.77
Batch: 760; loss: 1.18; acc: 0.8
Batch: 780; loss: 1.05; acc: 0.81
Train Epoch over. train_loss: 1.19; train_accuracy: 0.75 

7.576411007903516e-05
6.999056495260447e-05
Batch: 0; loss: 1.16; acc: 0.8
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 0.83; acc: 0.86
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 0.98; acc: 0.84
Batch: 100; loss: 1.05; acc: 0.91
Batch: 120; loss: 1.18; acc: 0.77
Batch: 140; loss: 0.97; acc: 0.86
Val Epoch over. val_loss: 1.0689007693035588; val_accuracy: 0.8038415605095541 

The current subspace-distance is: 6.999056495260447e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.88
Batch: 20; loss: 1.09; acc: 0.8
Batch: 40; loss: 1.25; acc: 0.75
Batch: 60; loss: 1.1; acc: 0.81
Batch: 80; loss: 1.23; acc: 0.75
Batch: 100; loss: 1.09; acc: 0.81
Batch: 120; loss: 1.1; acc: 0.8
Batch: 140; loss: 1.07; acc: 0.73
Batch: 160; loss: 1.09; acc: 0.84
Batch: 180; loss: 1.14; acc: 0.77
Batch: 200; loss: 1.02; acc: 0.83
Batch: 220; loss: 1.11; acc: 0.78
Batch: 240; loss: 1.08; acc: 0.8
Batch: 260; loss: 1.04; acc: 0.83
Batch: 280; loss: 1.03; acc: 0.81
Batch: 300; loss: 1.08; acc: 0.81
Batch: 320; loss: 1.1; acc: 0.69
Batch: 340; loss: 1.18; acc: 0.67
Batch: 360; loss: 1.01; acc: 0.78
Batch: 380; loss: 1.14; acc: 0.72
Batch: 400; loss: 1.18; acc: 0.78
Batch: 420; loss: 1.09; acc: 0.75
Batch: 440; loss: 1.11; acc: 0.75
Batch: 460; loss: 1.03; acc: 0.8
Batch: 480; loss: 1.11; acc: 0.78
Batch: 500; loss: 1.07; acc: 0.75
Batch: 520; loss: 1.06; acc: 0.75
Batch: 540; loss: 1.13; acc: 0.72
Batch: 560; loss: 1.03; acc: 0.84
Batch: 580; loss: 1.02; acc: 0.8
Batch: 600; loss: 0.98; acc: 0.8
Batch: 620; loss: 1.15; acc: 0.78
Batch: 640; loss: 1.02; acc: 0.84
Batch: 660; loss: 1.04; acc: 0.77
Batch: 680; loss: 0.94; acc: 0.81
Batch: 700; loss: 0.94; acc: 0.86
Batch: 720; loss: 1.12; acc: 0.78
Batch: 740; loss: 1.01; acc: 0.75
Batch: 760; loss: 1.11; acc: 0.8
Batch: 780; loss: 1.04; acc: 0.81
Train Epoch over. train_loss: 1.07; train_accuracy: 0.79 

9.093688277062029e-05
8.598875137977302e-05
Batch: 0; loss: 1.05; acc: 0.81
Batch: 20; loss: 1.17; acc: 0.72
Batch: 40; loss: 0.75; acc: 0.89
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.89
Batch: 100; loss: 0.95; acc: 0.91
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 0.84; acc: 0.91
Val Epoch over. val_loss: 0.976126361424756; val_accuracy: 0.8214570063694268 

The current subspace-distance is: 8.598875137977302e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.11; acc: 0.72
Batch: 20; loss: 1.13; acc: 0.73
Batch: 40; loss: 1.06; acc: 0.8
Batch: 60; loss: 0.98; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 0.95; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.88
Batch: 140; loss: 1.0; acc: 0.83
Batch: 160; loss: 1.08; acc: 0.73
Batch: 180; loss: 1.0; acc: 0.8
Batch: 200; loss: 1.02; acc: 0.73
Batch: 220; loss: 0.92; acc: 0.86
Batch: 240; loss: 1.09; acc: 0.72
Batch: 260; loss: 0.94; acc: 0.84
Batch: 280; loss: 1.02; acc: 0.8
Batch: 300; loss: 0.89; acc: 0.84
Batch: 320; loss: 1.01; acc: 0.77
Batch: 340; loss: 1.03; acc: 0.72
Batch: 360; loss: 1.06; acc: 0.77
Batch: 380; loss: 0.93; acc: 0.84
Batch: 400; loss: 1.05; acc: 0.83
Batch: 420; loss: 1.02; acc: 0.77
Batch: 440; loss: 1.18; acc: 0.72
Batch: 460; loss: 0.88; acc: 0.92
Batch: 480; loss: 1.05; acc: 0.75
Batch: 500; loss: 0.95; acc: 0.84
Batch: 520; loss: 0.99; acc: 0.78
Batch: 540; loss: 1.0; acc: 0.78
Batch: 560; loss: 1.07; acc: 0.77
Batch: 580; loss: 1.04; acc: 0.75
Batch: 600; loss: 0.91; acc: 0.86
Batch: 620; loss: 0.87; acc: 0.86
Batch: 640; loss: 1.05; acc: 0.8
Batch: 660; loss: 0.89; acc: 0.86
Batch: 680; loss: 0.93; acc: 0.78
Batch: 700; loss: 1.08; acc: 0.75
Batch: 720; loss: 0.98; acc: 0.81
Batch: 740; loss: 0.96; acc: 0.86
Batch: 760; loss: 0.94; acc: 0.81
Batch: 780; loss: 1.03; acc: 0.72
Train Epoch over. train_loss: 1.0; train_accuracy: 0.8 

0.00010576736531220376
0.00010043502697953954
Batch: 0; loss: 0.96; acc: 0.84
Batch: 20; loss: 1.1; acc: 0.72
Batch: 40; loss: 0.66; acc: 0.91
Batch: 60; loss: 0.9; acc: 0.81
Batch: 80; loss: 0.83; acc: 0.86
Batch: 100; loss: 0.86; acc: 0.88
Batch: 120; loss: 1.05; acc: 0.72
Batch: 140; loss: 0.73; acc: 0.92
Val Epoch over. val_loss: 0.8974589811768502; val_accuracy: 0.8368829617834395 

The current subspace-distance is: 0.00010043502697953954 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.84
Batch: 20; loss: 1.0; acc: 0.81
Batch: 40; loss: 0.91; acc: 0.88
Batch: 60; loss: 0.93; acc: 0.84
Batch: 80; loss: 0.83; acc: 0.92
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.72
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.92; acc: 0.8
Batch: 180; loss: 0.85; acc: 0.83
Batch: 200; loss: 0.95; acc: 0.81
Batch: 220; loss: 0.98; acc: 0.83
Batch: 240; loss: 0.83; acc: 0.83
Batch: 260; loss: 0.85; acc: 0.89
Batch: 280; loss: 0.98; acc: 0.75
Batch: 300; loss: 0.96; acc: 0.83
Batch: 320; loss: 1.11; acc: 0.73
Batch: 340; loss: 0.85; acc: 0.86
Batch: 360; loss: 0.93; acc: 0.86
Batch: 380; loss: 0.87; acc: 0.86
Batch: 400; loss: 0.9; acc: 0.83
Batch: 420; loss: 0.89; acc: 0.81
Batch: 440; loss: 0.95; acc: 0.81
Batch: 460; loss: 0.91; acc: 0.8
Batch: 480; loss: 0.75; acc: 0.92
Batch: 500; loss: 1.0; acc: 0.78
Batch: 520; loss: 1.04; acc: 0.77
Batch: 540; loss: 0.88; acc: 0.83
Batch: 560; loss: 0.85; acc: 0.81
Batch: 580; loss: 0.86; acc: 0.89
Batch: 600; loss: 0.85; acc: 0.84
Batch: 620; loss: 0.92; acc: 0.78
Batch: 640; loss: 0.94; acc: 0.78
Batch: 660; loss: 0.93; acc: 0.77
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 0.87; acc: 0.8
Batch: 720; loss: 1.1; acc: 0.78
Batch: 740; loss: 0.92; acc: 0.81
Batch: 760; loss: 0.96; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.88
Train Epoch over. train_loss: 0.93; train_accuracy: 0.81 

0.00012232112931087613
0.00011664796329569072
Batch: 0; loss: 0.88; acc: 0.89
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 0.57; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.74; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.89
Batch: 120; loss: 0.99; acc: 0.72
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8269633028157957; val_accuracy: 0.8484275477707006 

The current subspace-distance is: 0.00011664796329569072 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.8
Batch: 20; loss: 0.93; acc: 0.78
Batch: 40; loss: 0.89; acc: 0.83
Batch: 60; loss: 1.05; acc: 0.77
Batch: 80; loss: 0.91; acc: 0.73
Batch: 100; loss: 0.94; acc: 0.81
Batch: 120; loss: 0.85; acc: 0.86
Batch: 140; loss: 0.8; acc: 0.84
Batch: 160; loss: 1.04; acc: 0.75
Batch: 180; loss: 0.91; acc: 0.83
Batch: 200; loss: 0.8; acc: 0.89
Batch: 220; loss: 0.91; acc: 0.81
Batch: 240; loss: 0.95; acc: 0.8
Batch: 260; loss: 0.9; acc: 0.83
Batch: 280; loss: 0.86; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.84
Batch: 320; loss: 0.86; acc: 0.84
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.89
Batch: 380; loss: 0.86; acc: 0.83
Batch: 400; loss: 0.89; acc: 0.86
Batch: 420; loss: 0.82; acc: 0.86
Batch: 440; loss: 0.81; acc: 0.81
Batch: 460; loss: 0.71; acc: 0.91
Batch: 480; loss: 0.93; acc: 0.81
Batch: 500; loss: 0.89; acc: 0.84
Batch: 520; loss: 0.9; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.89
Batch: 560; loss: 0.95; acc: 0.73
Batch: 580; loss: 0.8; acc: 0.88
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.83; acc: 0.83
Batch: 640; loss: 0.85; acc: 0.84
Batch: 660; loss: 0.87; acc: 0.83
Batch: 680; loss: 0.92; acc: 0.8
Batch: 700; loss: 1.01; acc: 0.72
Batch: 720; loss: 0.8; acc: 0.84
Batch: 740; loss: 0.78; acc: 0.88
Batch: 760; loss: 0.84; acc: 0.83
Batch: 780; loss: 0.75; acc: 0.89
Train Epoch over. train_loss: 0.86; train_accuracy: 0.82 

0.00013676928938366473
0.00012888223864138126
Batch: 0; loss: 0.82; acc: 0.89
Batch: 20; loss: 1.0; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.81; acc: 0.86
Batch: 80; loss: 0.68; acc: 0.89
Batch: 100; loss: 0.75; acc: 0.91
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 0.7730635651357615; val_accuracy: 0.8540007961783439 

The current subspace-distance is: 0.00012888223864138126 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.84
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.92; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.85; acc: 0.83
Batch: 100; loss: 0.76; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.91
Batch: 140; loss: 0.85; acc: 0.84
Batch: 160; loss: 0.77; acc: 0.84
Batch: 180; loss: 0.86; acc: 0.83
Batch: 200; loss: 0.81; acc: 0.81
Batch: 220; loss: 0.73; acc: 0.89
Batch: 240; loss: 0.76; acc: 0.91
Batch: 260; loss: 0.71; acc: 0.88
Batch: 280; loss: 0.86; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.89
Batch: 320; loss: 0.79; acc: 0.83
Batch: 340; loss: 0.89; acc: 0.81
Batch: 360; loss: 0.9; acc: 0.81
Batch: 380; loss: 1.0; acc: 0.78
Batch: 400; loss: 0.84; acc: 0.78
Batch: 420; loss: 0.85; acc: 0.78
Batch: 440; loss: 0.79; acc: 0.83
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.75; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.89
Batch: 520; loss: 0.8; acc: 0.8
Batch: 540; loss: 0.95; acc: 0.81
Batch: 560; loss: 0.82; acc: 0.78
Batch: 580; loss: 0.83; acc: 0.86
Batch: 600; loss: 0.9; acc: 0.73
Batch: 620; loss: 0.7; acc: 0.86
Batch: 640; loss: 0.76; acc: 0.89
Batch: 660; loss: 0.86; acc: 0.8
Batch: 680; loss: 0.84; acc: 0.77
Batch: 700; loss: 0.91; acc: 0.81
Batch: 720; loss: 0.68; acc: 0.91
Batch: 740; loss: 0.83; acc: 0.78
Batch: 760; loss: 0.81; acc: 0.81
Batch: 780; loss: 0.77; acc: 0.88
Train Epoch over. train_loss: 0.81; train_accuracy: 0.83 

0.00014698528684675694
0.00014206588093657047
Batch: 0; loss: 0.76; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.89
Batch: 100; loss: 0.71; acc: 0.91
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.58; acc: 0.92
Val Epoch over. val_loss: 0.7353575349233712; val_accuracy: 0.853702229299363 

The current subspace-distance is: 0.00014206588093657047 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.82; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.86
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.84
Batch: 200; loss: 0.78; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.78
Batch: 240; loss: 0.77; acc: 0.81
Batch: 260; loss: 0.75; acc: 0.86
Batch: 280; loss: 0.7; acc: 0.89
Batch: 300; loss: 0.66; acc: 0.91
Batch: 320; loss: 0.85; acc: 0.83
Batch: 340; loss: 0.73; acc: 0.84
Batch: 360; loss: 0.81; acc: 0.83
Batch: 380; loss: 0.83; acc: 0.77
Batch: 400; loss: 0.7; acc: 0.88
Batch: 420; loss: 0.74; acc: 0.84
Batch: 440; loss: 0.94; acc: 0.75
Batch: 460; loss: 0.71; acc: 0.89
Batch: 480; loss: 0.69; acc: 0.86
Batch: 500; loss: 0.83; acc: 0.81
Batch: 520; loss: 0.83; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.92
Batch: 560; loss: 0.83; acc: 0.81
Batch: 580; loss: 0.77; acc: 0.86
Batch: 600; loss: 0.85; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.91
Batch: 640; loss: 0.84; acc: 0.77
Batch: 660; loss: 0.76; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.89
Batch: 720; loss: 0.93; acc: 0.72
Batch: 740; loss: 0.74; acc: 0.89
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 0.67; acc: 0.94
Train Epoch over. train_loss: 0.78; train_accuracy: 0.83 

0.00015941156016197056
0.000151584783452563
Batch: 0; loss: 0.73; acc: 0.89
Batch: 20; loss: 0.96; acc: 0.78
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.55; acc: 0.91
Val Epoch over. val_loss: 0.7060711380023106; val_accuracy: 0.8576831210191083 

The current subspace-distance is: 0.000151584783452563 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.88; acc: 0.75
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.91
Batch: 140; loss: 1.0; acc: 0.72
Batch: 160; loss: 0.68; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 0.94; acc: 0.77
Batch: 220; loss: 0.92; acc: 0.78
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.66; acc: 0.91
Batch: 280; loss: 0.88; acc: 0.75
Batch: 300; loss: 0.71; acc: 0.86
Batch: 320; loss: 0.85; acc: 0.77
Batch: 340; loss: 0.83; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.69; acc: 0.89
Batch: 440; loss: 0.67; acc: 0.88
Batch: 460; loss: 0.67; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.88
Batch: 500; loss: 0.83; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.78; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.91
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.77; acc: 0.83
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 0.81; acc: 0.78
Batch: 720; loss: 0.73; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.8
Batch: 760; loss: 0.91; acc: 0.78
Batch: 780; loss: 0.58; acc: 0.88
Train Epoch over. train_loss: 0.75; train_accuracy: 0.83 

0.00017029089212883264
0.0001626230077818036
Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.92
Val Epoch over. val_loss: 0.6837339731538372; val_accuracy: 0.8585788216560509 

The current subspace-distance is: 0.0001626230077818036 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 0.74; acc: 0.88
Batch: 40; loss: 0.64; acc: 0.88
Batch: 60; loss: 0.77; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.8
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.84; acc: 0.8
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.8; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.67; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 0.69; acc: 0.88
Batch: 460; loss: 0.72; acc: 0.81
Batch: 480; loss: 0.73; acc: 0.84
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 0.82; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.75; acc: 0.88
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.64; acc: 0.89
Batch: 680; loss: 0.78; acc: 0.8
Batch: 700; loss: 0.87; acc: 0.67
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.89
Batch: 760; loss: 0.73; acc: 0.81
Batch: 780; loss: 0.69; acc: 0.88
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.00017902374383993447
0.00017101789126172662
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6546924734950825; val_accuracy: 0.8602707006369427 

The current subspace-distance is: 0.00017101789126172662 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.78; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.91; acc: 0.73
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.88
Batch: 200; loss: 0.68; acc: 0.86
Batch: 220; loss: 0.76; acc: 0.84
Batch: 240; loss: 0.81; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 0.69; acc: 0.88
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.66; acc: 0.92
Batch: 420; loss: 0.73; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.89
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.83; acc: 0.8
Batch: 520; loss: 0.63; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.88
Batch: 640; loss: 0.64; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.65; acc: 0.83
Batch: 700; loss: 0.82; acc: 0.8
Batch: 720; loss: 0.62; acc: 0.89
Batch: 740; loss: 0.95; acc: 0.73
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00018259255739394575
0.00017596004181541502
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.92
Val Epoch over. val_loss: 0.6516611490659653; val_accuracy: 0.8588773885350318 

The current subspace-distance is: 0.00017596004181541502 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.92
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.71; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.77
Batch: 240; loss: 0.97; acc: 0.7
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.71; acc: 0.86
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.88; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.88
Batch: 400; loss: 0.7; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.69; acc: 0.88
Batch: 460; loss: 0.77; acc: 0.8
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 0.7; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.92
Batch: 600; loss: 0.78; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.77
Batch: 640; loss: 0.68; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.64; acc: 0.88
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.57; acc: 0.94
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.0001848422543844208
0.0001777620054781437
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.6449410175062289; val_accuracy: 0.8589769108280255 

The current subspace-distance is: 0.0001777620054781437 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.89; acc: 0.75
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.88
Batch: 100; loss: 0.86; acc: 0.75
Batch: 120; loss: 0.83; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.89; acc: 0.73
Batch: 220; loss: 0.79; acc: 0.77
Batch: 240; loss: 0.69; acc: 0.88
Batch: 260; loss: 0.87; acc: 0.77
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.63; acc: 0.89
Batch: 380; loss: 0.83; acc: 0.77
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.83; acc: 0.78
Batch: 440; loss: 0.84; acc: 0.72
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.72; acc: 0.83
Batch: 500; loss: 0.72; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.86
Batch: 540; loss: 0.79; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.87; acc: 0.86
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.57; acc: 0.89
Batch: 640; loss: 0.65; acc: 0.91
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.64; acc: 0.91
Batch: 700; loss: 0.77; acc: 0.86
Batch: 720; loss: 0.8; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.75
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.83 

0.00018871109932661057
0.00017948076128959656
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6405576666828933; val_accuracy: 0.8603702229299363 

The current subspace-distance is: 0.00017948076128959656 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.56; acc: 0.91
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.78; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.91
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 0.65; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.75; acc: 0.8
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.7; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.78; acc: 0.84
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.8; acc: 0.75
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.59; acc: 0.88
Batch: 720; loss: 0.79; acc: 0.78
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.00019048411922995
0.00018214635201729834
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.88; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6292012540777777; val_accuracy: 0.8626592356687898 

The current subspace-distance is: 0.00018214635201729834 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.88
Batch: 220; loss: 0.73; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.8
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.94
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.72; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.98; acc: 0.72
Batch: 440; loss: 0.68; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.77
Batch: 500; loss: 0.75; acc: 0.81
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.92
Batch: 640; loss: 0.68; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.94
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.91
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.88; acc: 0.8
Batch: 780; loss: 0.87; acc: 0.78
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.0001894176530186087
0.00018262118101119995
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.94
Val Epoch over. val_loss: 0.6304823141189138; val_accuracy: 0.8584792993630573 

The current subspace-distance is: 0.00018262118101119995 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.7; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.82; acc: 0.8
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.7; acc: 0.86
Batch: 260; loss: 0.88; acc: 0.75
Batch: 280; loss: 0.72; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.81
Batch: 320; loss: 0.8; acc: 0.78
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.86
Batch: 620; loss: 0.82; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.75; acc: 0.78
Batch: 700; loss: 0.76; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.78
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.83 

0.00019355278345756233
0.00018698596977628767
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.94
Val Epoch over. val_loss: 0.621002216817467; val_accuracy: 0.8622611464968153 

The current subspace-distance is: 0.00018698596977628767 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.91
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.65; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.72; acc: 0.89
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 0.78; acc: 0.81
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.6; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.83; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.67; acc: 0.88
Batch: 480; loss: 0.74; acc: 0.8
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.55; acc: 0.94
Batch: 540; loss: 0.72; acc: 0.81
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.72; acc: 0.78
Batch: 720; loss: 0.71; acc: 0.78
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.0001939915237016976
0.00018694133905228227
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.6210734018474627; val_accuracy: 0.8611664012738853 

The current subspace-distance is: 0.00018694133905228227 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.64; acc: 0.89
Batch: 80; loss: 0.91; acc: 0.78
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.64; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.88
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.9; acc: 0.73
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.81; acc: 0.73
Batch: 420; loss: 0.62; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.71; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.6; acc: 0.89
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.86
Batch: 620; loss: 0.74; acc: 0.84
Batch: 640; loss: 0.73; acc: 0.78
Batch: 660; loss: 0.67; acc: 0.8
Batch: 680; loss: 0.56; acc: 0.91
Batch: 700; loss: 0.7; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.58; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.91
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00019784175674431026
0.00018767551227938384
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.94
Val Epoch over. val_loss: 0.6129638076208199; val_accuracy: 0.8653463375796179 

The current subspace-distance is: 0.00018767551227938384 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.61; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.61; acc: 0.89
Batch: 280; loss: 0.75; acc: 0.8
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.77; acc: 0.78
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.77
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.83; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.91
Batch: 600; loss: 0.91; acc: 0.67
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.77; acc: 0.8
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.71; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.54; acc: 0.92
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020031917665619403
0.00019218069792259485
Batch: 0; loss: 0.63; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.72
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.6101695925566801; val_accuracy: 0.8625597133757962 

The current subspace-distance is: 0.00019218069792259485 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.71; acc: 0.77
Batch: 220; loss: 0.79; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.89
Batch: 300; loss: 0.66; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.77
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.79; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.78; acc: 0.77
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.71; acc: 0.78
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.92
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00020099309040233493
0.00019342132145538926
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.6011367294059438; val_accuracy: 0.8634554140127388 

The current subspace-distance is: 0.00019342132145538926 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.87; acc: 0.8
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.7; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.71; acc: 0.83
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.81; acc: 0.73
Batch: 380; loss: 0.79; acc: 0.75
Batch: 400; loss: 0.5; acc: 0.91
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 0.61; acc: 0.89
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.8
Batch: 560; loss: 0.62; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.78
Batch: 620; loss: 0.66; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.83
Batch: 660; loss: 0.71; acc: 0.86
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.64; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.89
Batch: 780; loss: 0.76; acc: 0.77
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00020284239144530147
0.00019583063840400428
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.5993846271448074; val_accuracy: 0.8635549363057324 

The current subspace-distance is: 0.00019583063840400428 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.76; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.79; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.72
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.81; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.88
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.85; acc: 0.73
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.72; acc: 0.83
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.76; acc: 0.81
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.6; acc: 0.86
Batch: 760; loss: 0.71; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00020504735584836453
0.00019330055511090904
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.92
Val Epoch over. val_loss: 0.596290022704252; val_accuracy: 0.8655453821656051 

The current subspace-distance is: 0.00019330055511090904 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.76; acc: 0.8
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.74; acc: 0.75
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.73; acc: 0.78
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.88
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.66; acc: 0.84
Batch: 640; loss: 0.86; acc: 0.72
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.74; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.78; acc: 0.78
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00020540840341709554
0.00019922906358260661
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.6019022527394021; val_accuracy: 0.8614649681528662 

The current subspace-distance is: 0.00019922906358260661 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.9; acc: 0.73
Batch: 260; loss: 0.48; acc: 0.92
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.78; acc: 0.75
Batch: 340; loss: 0.79; acc: 0.78
Batch: 360; loss: 0.63; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.81
Batch: 400; loss: 0.71; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.89
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.72; acc: 0.8
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.81
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.73; acc: 0.8
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.86; acc: 0.77
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00020606040197890252
0.00019618192163761705
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.6016258181660039; val_accuracy: 0.8600716560509554 

The current subspace-distance is: 0.00019618192163761705 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.71; acc: 0.72
Batch: 180; loss: 0.76; acc: 0.78
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.74; acc: 0.77
Batch: 280; loss: 0.79; acc: 0.78
Batch: 300; loss: 0.89; acc: 0.73
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.56; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.91
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.74; acc: 0.78
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.61; acc: 0.91
Batch: 660; loss: 0.65; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.81; acc: 0.8
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.0002051444025710225
0.00019807285571005195
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.4; acc: 0.92
Val Epoch over. val_loss: 0.5975245003867301; val_accuracy: 0.8615644904458599 

The current subspace-distance is: 0.00019807285571005195 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.78
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.77; acc: 0.75
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.86
Batch: 480; loss: 0.66; acc: 0.8
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.68; acc: 0.78
Batch: 700; loss: 0.75; acc: 0.77
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.64; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00020699332526419312
0.0001983268157346174
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.92
Val Epoch over. val_loss: 0.5937182062370762; val_accuracy: 0.8656449044585988 

The current subspace-distance is: 0.0001983268157346174 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.78
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.7; acc: 0.75
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.78
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.62; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.73
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.78; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.65; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.89
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.81
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.81
Batch: 680; loss: 0.74; acc: 0.8
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.72; acc: 0.84
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00021197961177676916
0.00020153116201981902
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.7
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.5910388715327926; val_accuracy: 0.8634554140127388 

The current subspace-distance is: 0.00020153116201981902 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.59; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.7; acc: 0.78
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.89
Batch: 760; loss: 0.8; acc: 0.73
Batch: 780; loss: 0.68; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00020738726016134024
0.00019830040400847793
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.4; acc: 0.94
Val Epoch over. val_loss: 0.589854862090129; val_accuracy: 0.8661425159235668 

The current subspace-distance is: 0.00019830040400847793 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.79; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.72; acc: 0.75
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.89
Batch: 260; loss: 0.76; acc: 0.8
Batch: 280; loss: 0.48; acc: 0.92
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.61; acc: 0.86
Batch: 380; loss: 0.73; acc: 0.83
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.72; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.55; acc: 0.89
Batch: 680; loss: 0.65; acc: 0.88
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.74; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.6; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.0002102875878335908
0.00020184348977636546
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.91
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.94
Val Epoch over. val_loss: 0.5956269219802444; val_accuracy: 0.8627587579617835 

The current subspace-distance is: 0.00020184348977636546 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.91
Batch: 160; loss: 0.67; acc: 0.83
Batch: 180; loss: 0.84; acc: 0.72
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.76; acc: 0.77
Batch: 240; loss: 0.73; acc: 0.83
Batch: 260; loss: 0.73; acc: 0.86
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.78; acc: 0.8
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.72; acc: 0.77
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.73; acc: 0.78
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.68; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.8; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.79; acc: 0.78
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.75
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00020911138562951237
0.00019888651149813086
Batch: 0; loss: 0.59; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.5823233645812721; val_accuracy: 0.8671377388535032 

The current subspace-distance is: 0.00019888651149813086 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_8_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.4265201420964964

The number of parameters is: 257601

The number of individual parameters is:

20
360
20
20
30
40800
30
30
59
120360
59
59
64
90624
64
64
4096
64
640
10
64
64

nonzero elements in E: 77280292
elements in E: 77280300
fraction nonzero: 0.9999998964807332
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.45; acc: 0.12
Batch: 20; loss: 2.22; acc: 0.2
Batch: 40; loss: 1.95; acc: 0.36
Batch: 60; loss: 1.84; acc: 0.44
Batch: 80; loss: 1.81; acc: 0.42
Batch: 100; loss: 1.59; acc: 0.64
Batch: 120; loss: 1.6; acc: 0.61
Batch: 140; loss: 1.56; acc: 0.61
Batch: 160; loss: 1.6; acc: 0.55
Batch: 180; loss: 1.49; acc: 0.66
Batch: 200; loss: 1.43; acc: 0.78
Batch: 220; loss: 1.53; acc: 0.58
Batch: 240; loss: 1.36; acc: 0.77
Batch: 260; loss: 1.33; acc: 0.72
Batch: 280; loss: 1.4; acc: 0.7
Batch: 300; loss: 1.34; acc: 0.75
Batch: 320; loss: 1.38; acc: 0.72
Batch: 340; loss: 1.3; acc: 0.7
Batch: 360; loss: 1.27; acc: 0.72
Batch: 380; loss: 1.35; acc: 0.69
Batch: 400; loss: 1.16; acc: 0.81
Batch: 420; loss: 1.29; acc: 0.75
Batch: 440; loss: 1.28; acc: 0.78
Batch: 460; loss: 1.25; acc: 0.78
Batch: 480; loss: 1.28; acc: 0.8
Batch: 500; loss: 1.15; acc: 0.81
Batch: 520; loss: 1.12; acc: 0.83
Batch: 540; loss: 1.22; acc: 0.75
Batch: 560; loss: 1.13; acc: 0.86
Batch: 580; loss: 1.14; acc: 0.81
Batch: 600; loss: 1.24; acc: 0.75
Batch: 620; loss: 1.25; acc: 0.75
Batch: 640; loss: 1.17; acc: 0.78
Batch: 660; loss: 1.2; acc: 0.73
Batch: 680; loss: 1.05; acc: 0.86
Batch: 700; loss: 1.22; acc: 0.73
Batch: 720; loss: 1.07; acc: 0.78
Batch: 740; loss: 1.2; acc: 0.81
Batch: 760; loss: 1.14; acc: 0.75
Batch: 780; loss: 1.12; acc: 0.75
Train Epoch over. train_loss: 1.39; train_accuracy: 0.68 

6.455813854699954e-05
5.956838867859915e-05
Batch: 0; loss: 1.08; acc: 0.83
Batch: 20; loss: 1.22; acc: 0.7
Batch: 40; loss: 0.8; acc: 0.91
Batch: 60; loss: 1.0; acc: 0.83
Batch: 80; loss: 1.01; acc: 0.84
Batch: 100; loss: 1.05; acc: 0.89
Batch: 120; loss: 1.11; acc: 0.73
Batch: 140; loss: 1.03; acc: 0.88
Val Epoch over. val_loss: 1.0745358820174151; val_accuracy: 0.8061305732484076 

The current subspace-distance is: 5.956838867859915e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.73
Batch: 20; loss: 1.07; acc: 0.77
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.13; acc: 0.75
Batch: 80; loss: 1.17; acc: 0.75
Batch: 100; loss: 1.09; acc: 0.73
Batch: 120; loss: 1.03; acc: 0.86
Batch: 140; loss: 1.04; acc: 0.8
Batch: 160; loss: 1.11; acc: 0.78
Batch: 180; loss: 1.04; acc: 0.77
Batch: 200; loss: 1.08; acc: 0.83
Batch: 220; loss: 1.05; acc: 0.73
Batch: 240; loss: 1.08; acc: 0.78
Batch: 260; loss: 1.08; acc: 0.78
Batch: 280; loss: 0.87; acc: 0.88
Batch: 300; loss: 0.95; acc: 0.89
Batch: 320; loss: 0.98; acc: 0.88
Batch: 340; loss: 0.97; acc: 0.83
Batch: 360; loss: 1.07; acc: 0.8
Batch: 380; loss: 0.96; acc: 0.83
Batch: 400; loss: 1.12; acc: 0.77
Batch: 420; loss: 1.07; acc: 0.78
Batch: 440; loss: 0.99; acc: 0.84
Batch: 460; loss: 1.04; acc: 0.78
Batch: 480; loss: 0.97; acc: 0.86
Batch: 500; loss: 0.93; acc: 0.84
Batch: 520; loss: 0.97; acc: 0.8
Batch: 540; loss: 1.04; acc: 0.75
Batch: 560; loss: 0.9; acc: 0.86
Batch: 580; loss: 1.07; acc: 0.75
Batch: 600; loss: 1.09; acc: 0.77
Batch: 620; loss: 0.91; acc: 0.86
Batch: 640; loss: 1.01; acc: 0.8
Batch: 660; loss: 0.9; acc: 0.86
Batch: 680; loss: 0.98; acc: 0.88
Batch: 700; loss: 0.89; acc: 0.86
Batch: 720; loss: 0.96; acc: 0.81
Batch: 740; loss: 0.98; acc: 0.83
Batch: 760; loss: 0.88; acc: 0.92
Batch: 780; loss: 1.01; acc: 0.81
Train Epoch over. train_loss: 1.03; train_accuracy: 0.8 

8.518678805558011e-05
8.088527829386294e-05
Batch: 0; loss: 0.94; acc: 0.88
Batch: 20; loss: 1.07; acc: 0.8
Batch: 40; loss: 0.62; acc: 0.92
Batch: 60; loss: 0.9; acc: 0.84
Batch: 80; loss: 0.82; acc: 0.86
Batch: 100; loss: 0.89; acc: 0.86
Batch: 120; loss: 1.07; acc: 0.72
Batch: 140; loss: 0.87; acc: 0.84
Val Epoch over. val_loss: 0.8928456701290836; val_accuracy: 0.8411624203821656 

The current subspace-distance is: 8.088527829386294e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.86
Batch: 20; loss: 0.94; acc: 0.78
Batch: 40; loss: 0.94; acc: 0.8
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 0.89; acc: 0.81
Batch: 100; loss: 0.83; acc: 0.91
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 0.99; acc: 0.77
Batch: 160; loss: 1.02; acc: 0.75
Batch: 180; loss: 0.83; acc: 0.86
Batch: 200; loss: 0.93; acc: 0.81
Batch: 220; loss: 0.94; acc: 0.83
Batch: 240; loss: 0.95; acc: 0.84
Batch: 260; loss: 0.86; acc: 0.84
Batch: 280; loss: 0.92; acc: 0.81
Batch: 300; loss: 0.91; acc: 0.84
Batch: 320; loss: 0.9; acc: 0.8
Batch: 340; loss: 0.97; acc: 0.77
Batch: 360; loss: 0.95; acc: 0.77
Batch: 380; loss: 0.85; acc: 0.84
Batch: 400; loss: 0.92; acc: 0.84
Batch: 420; loss: 0.87; acc: 0.83
Batch: 440; loss: 0.83; acc: 0.83
Batch: 460; loss: 0.96; acc: 0.8
Batch: 480; loss: 0.95; acc: 0.81
Batch: 500; loss: 0.88; acc: 0.81
Batch: 520; loss: 0.89; acc: 0.83
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 0.96; acc: 0.7
Batch: 580; loss: 0.86; acc: 0.81
Batch: 600; loss: 1.01; acc: 0.8
Batch: 620; loss: 0.85; acc: 0.81
Batch: 640; loss: 1.02; acc: 0.73
Batch: 660; loss: 0.77; acc: 0.86
Batch: 680; loss: 0.76; acc: 0.88
Batch: 700; loss: 0.92; acc: 0.77
Batch: 720; loss: 0.75; acc: 0.88
Batch: 740; loss: 0.77; acc: 0.83
Batch: 760; loss: 0.8; acc: 0.84
Batch: 780; loss: 0.81; acc: 0.89
Train Epoch over. train_loss: 0.89; train_accuracy: 0.83 

0.00010308533092029393
9.81648699962534e-05
Batch: 0; loss: 0.83; acc: 0.83
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.81; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.89
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 0.73; acc: 0.88
Val Epoch over. val_loss: 0.7669475477212554; val_accuracy: 0.863953025477707 

The current subspace-distance is: 9.81648699962534e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.81
Batch: 20; loss: 1.07; acc: 0.7
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 0.72; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.91
Batch: 100; loss: 0.91; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.78; acc: 0.89
Batch: 180; loss: 0.81; acc: 0.84
Batch: 200; loss: 0.68; acc: 0.86
Batch: 220; loss: 0.68; acc: 0.89
Batch: 240; loss: 0.8; acc: 0.88
Batch: 260; loss: 0.71; acc: 0.88
Batch: 280; loss: 0.84; acc: 0.8
Batch: 300; loss: 0.69; acc: 0.91
Batch: 320; loss: 0.79; acc: 0.84
Batch: 340; loss: 0.87; acc: 0.81
Batch: 360; loss: 0.82; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.84
Batch: 420; loss: 0.74; acc: 0.89
Batch: 440; loss: 0.74; acc: 0.86
Batch: 460; loss: 0.75; acc: 0.83
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.66; acc: 0.91
Batch: 520; loss: 0.8; acc: 0.8
Batch: 540; loss: 0.81; acc: 0.81
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.91
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.76; acc: 0.84
Batch: 640; loss: 0.75; acc: 0.84
Batch: 660; loss: 0.64; acc: 0.88
Batch: 680; loss: 0.76; acc: 0.84
Batch: 700; loss: 0.7; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.92
Batch: 740; loss: 0.68; acc: 0.89
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.86
Train Epoch over. train_loss: 0.77; train_accuracy: 0.85 

0.00011852400348288938
0.00011350639397278428
Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.73; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.62; acc: 0.91
Val Epoch over. val_loss: 0.6699758549784399; val_accuracy: 0.8752985668789809 

The current subspace-distance is: 0.00011350639397278428 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.86
Batch: 40; loss: 0.98; acc: 0.7
Batch: 60; loss: 0.73; acc: 0.86
Batch: 80; loss: 0.65; acc: 0.92
Batch: 100; loss: 0.83; acc: 0.8
Batch: 120; loss: 0.7; acc: 0.88
Batch: 140; loss: 0.71; acc: 0.86
Batch: 160; loss: 0.83; acc: 0.78
Batch: 180; loss: 0.7; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.92
Batch: 220; loss: 0.73; acc: 0.88
Batch: 240; loss: 0.72; acc: 0.89
Batch: 260; loss: 0.65; acc: 0.92
Batch: 280; loss: 0.73; acc: 0.86
Batch: 300; loss: 0.67; acc: 0.88
Batch: 320; loss: 0.65; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.89
Batch: 380; loss: 0.69; acc: 0.86
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.58; acc: 0.89
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.68; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.93; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.95
Batch: 760; loss: 0.59; acc: 0.89
Batch: 780; loss: 0.81; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.86 

0.00013729317288380116
0.0001309685903834179
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.92
Val Epoch over. val_loss: 0.5989027351710448; val_accuracy: 0.8794785031847133 

The current subspace-distance is: 0.0001309685903834179 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.92
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.95
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.58; acc: 0.91
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.71; acc: 0.81
Batch: 400; loss: 0.59; acc: 0.91
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.91
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.95
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.63; train_accuracy: 0.86 

0.00015112849359866232
0.00014371199358720332
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.95
Val Epoch over. val_loss: 0.5476936709349323; val_accuracy: 0.8878383757961783 

The current subspace-distance is: 0.00014371199358720332 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.72; acc: 0.77
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.84
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.97
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.66; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.81
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.95
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.59; train_accuracy: 0.87 

0.00015966006321832538
0.00015446914767380804
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.4; acc: 0.95
Val Epoch over. val_loss: 0.5225742945245876; val_accuracy: 0.886843152866242 

The current subspace-distance is: 0.00015446914767380804 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.95
Batch: 340; loss: 0.61; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.49; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.75
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.64; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00017097615636885166
0.00016638943634461612
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Val Epoch over. val_loss: 0.4837346135810682; val_accuracy: 0.8965963375796179 

The current subspace-distance is: 0.00016638943634461612 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.67; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00017882716201711446
0.00017440080409869552
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.46168449634958986; val_accuracy: 0.8963972929936306 

The current subspace-distance is: 0.00017440080409869552 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.95
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00019244878785684705
0.00018384285795036703
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.4311243586099831; val_accuracy: 0.9007762738853503 

The current subspace-distance is: 0.00018384285795036703 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.94
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.97
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.5; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00019201771647203714
0.00018384144641458988
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4217546595509645; val_accuracy: 0.9047571656050956 

The current subspace-distance is: 0.00018384144641458988 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.94
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.97
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

0.00019625380809884518
0.00018887658370658755
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.4132112207686066; val_accuracy: 0.9074442675159236 

The current subspace-distance is: 0.00018887658370658755 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.97
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.88
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.89 

0.0001968211290659383
0.00018878976698033512
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.40596632649944087; val_accuracy: 0.908937101910828 

The current subspace-distance is: 0.00018878976698033512 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.57; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.89 

0.00020100812253076583
0.00019324340973980725
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.4064369967598824; val_accuracy: 0.9072452229299363 

The current subspace-distance is: 0.00019324340973980725 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.8
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.59; acc: 0.8
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.89 

0.0002027952577918768
0.0001975016639335081
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.3976324523330494; val_accuracy: 0.9085390127388535 

The current subspace-distance is: 0.0001975016639335081 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.89 

0.0002049196045845747
0.00019856562721543014
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.3919682345192903; val_accuracy: 0.9117237261146497 

The current subspace-distance is: 0.00019856562721543014 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.77
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.98
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.97
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.89 

0.00020622718147933483
0.00019971825531683862
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.38726004198857933; val_accuracy: 0.9115246815286624 

The current subspace-distance is: 0.00019971825531683862 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.58; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.89 

0.00020878564100712538
0.00020249534281902015
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.3807385542969795; val_accuracy: 0.9120222929936306 

The current subspace-distance is: 0.00020249534281902015 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.39; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

0.00020981546549592167
0.0002022403641603887
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3738607471915567; val_accuracy: 0.9131170382165605 

The current subspace-distance is: 0.0002022403641603887 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

0.00021436356473714113
0.00020391991711221635
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.3786711941479118; val_accuracy: 0.9139132165605095 

The current subspace-distance is: 0.00020391991711221635 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.97
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.89 

0.00021350101451389492
0.0002065020817099139
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.36876474937815573; val_accuracy: 0.9131170382165605 

The current subspace-distance is: 0.0002065020817099139 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.98
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.97
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.89 

0.00021413693320937455
0.00020783045329153538
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3708734680323084; val_accuracy: 0.9132165605095541 

The current subspace-distance is: 0.00020783045329153538 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.57; acc: 0.8
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

0.00021512604143936187
0.0002073527139145881
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3754597994362473; val_accuracy: 0.912718949044586 

The current subspace-distance is: 0.0002073527139145881 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.97
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.94
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

0.00021561146422754973
0.00020830360881518573
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.36867324789618233; val_accuracy: 0.9139132165605095 

The current subspace-distance is: 0.00020830360881518573 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.25; acc: 1.0
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.97
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

0.0002167979400837794
0.0002093496295856312
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.370690375281747; val_accuracy: 0.9140127388535032 

The current subspace-distance is: 0.0002093496295856312 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.58; acc: 0.8
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.97
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

0.00021457989350892603
0.00020892125030513853
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.3683959936640065; val_accuracy: 0.9149084394904459 

The current subspace-distance is: 0.00020892125030513853 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.94
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

0.00021757089416496456
0.0002110108034685254
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.36756696727625127; val_accuracy: 0.9143113057324841 

The current subspace-distance is: 0.0002110108034685254 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

0.00021908165945205837
0.0002091495116474107
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.3647912297469036; val_accuracy: 0.9152070063694268 

The current subspace-distance is: 0.0002091495116474107 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.97
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

0.0002179433504352346
0.00021074899996165186
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.36282587506968506; val_accuracy: 0.9149084394904459 

The current subspace-distance is: 0.00021074899996165186 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.2; acc: 1.0
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.97
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

0.0002183598990086466
0.00020973203936591744
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.3690874699954015; val_accuracy: 0.9126194267515924 

The current subspace-distance is: 0.00020973203936591744 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_8_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.4265201420964964

The number of parameters is: 257601

The number of individual parameters is:

20
360
20
20
30
40800
30
30
59
120360
59
59
64
90624
64
64
4096
64
640
10
64
64

nonzero elements in E: 103040389
elements in E: 103040400
fraction nonzero: 0.999999893245756
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.35; acc: 0.09
Batch: 20; loss: 2.09; acc: 0.25
Batch: 40; loss: 1.84; acc: 0.38
Batch: 60; loss: 1.66; acc: 0.53
Batch: 80; loss: 1.62; acc: 0.59
Batch: 100; loss: 1.56; acc: 0.61
Batch: 120; loss: 1.43; acc: 0.72
Batch: 140; loss: 1.45; acc: 0.67
Batch: 160; loss: 1.42; acc: 0.67
Batch: 180; loss: 1.41; acc: 0.67
Batch: 200; loss: 1.37; acc: 0.61
Batch: 220; loss: 1.27; acc: 0.73
Batch: 240; loss: 1.25; acc: 0.67
Batch: 260; loss: 1.19; acc: 0.75
Batch: 280; loss: 1.1; acc: 0.8
Batch: 300; loss: 1.06; acc: 0.84
Batch: 320; loss: 1.17; acc: 0.75
Batch: 340; loss: 1.13; acc: 0.75
Batch: 360; loss: 1.16; acc: 0.69
Batch: 380; loss: 1.2; acc: 0.77
Batch: 400; loss: 1.06; acc: 0.8
Batch: 420; loss: 0.96; acc: 0.89
Batch: 440; loss: 0.98; acc: 0.86
Batch: 460; loss: 1.06; acc: 0.8
Batch: 480; loss: 1.03; acc: 0.8
Batch: 500; loss: 1.12; acc: 0.7
Batch: 520; loss: 1.07; acc: 0.8
Batch: 540; loss: 0.93; acc: 0.84
Batch: 560; loss: 1.14; acc: 0.8
Batch: 580; loss: 1.12; acc: 0.83
Batch: 600; loss: 0.96; acc: 0.83
Batch: 620; loss: 1.1; acc: 0.78
Batch: 640; loss: 0.99; acc: 0.8
Batch: 660; loss: 1.06; acc: 0.78
Batch: 680; loss: 0.88; acc: 0.91
Batch: 700; loss: 0.91; acc: 0.84
Batch: 720; loss: 0.9; acc: 0.88
Batch: 740; loss: 1.03; acc: 0.8
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 0.94; acc: 0.83
Train Epoch over. train_loss: 1.2; train_accuracy: 0.74 

2.4881690478650853e-05
7.999892659427132e-06
Batch: 0; loss: 0.93; acc: 0.88
Batch: 20; loss: 1.05; acc: 0.77
Batch: 40; loss: 0.62; acc: 0.92
Batch: 60; loss: 0.85; acc: 0.88
Batch: 80; loss: 0.73; acc: 0.94
Batch: 100; loss: 0.89; acc: 0.88
Batch: 120; loss: 1.02; acc: 0.77
Batch: 140; loss: 0.71; acc: 0.92
Val Epoch over. val_loss: 0.8731090730162943; val_accuracy: 0.8523089171974523 

The current subspace-distance is: 7.999892659427132e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.88
Batch: 20; loss: 0.98; acc: 0.84
Batch: 40; loss: 0.92; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.91
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 0.87; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.86
Batch: 140; loss: 0.94; acc: 0.84
Batch: 160; loss: 0.82; acc: 0.81
Batch: 180; loss: 0.73; acc: 0.95
Batch: 200; loss: 0.84; acc: 0.88
Batch: 220; loss: 0.76; acc: 0.89
Batch: 240; loss: 0.76; acc: 0.89
Batch: 260; loss: 0.98; acc: 0.83
Batch: 280; loss: 0.74; acc: 0.84
Batch: 300; loss: 0.86; acc: 0.84
Batch: 320; loss: 0.92; acc: 0.83
Batch: 340; loss: 0.77; acc: 0.81
Batch: 360; loss: 0.84; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.92
Batch: 400; loss: 0.7; acc: 0.95
Batch: 420; loss: 0.75; acc: 0.91
Batch: 440; loss: 0.72; acc: 0.86
Batch: 460; loss: 0.69; acc: 0.88
Batch: 480; loss: 0.79; acc: 0.88
Batch: 500; loss: 0.78; acc: 0.88
Batch: 520; loss: 0.83; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.91
Batch: 560; loss: 0.83; acc: 0.91
Batch: 580; loss: 0.85; acc: 0.88
Batch: 600; loss: 0.73; acc: 0.91
Batch: 620; loss: 0.71; acc: 0.91
Batch: 640; loss: 0.87; acc: 0.78
Batch: 660; loss: 0.75; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.91
Batch: 700; loss: 0.76; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.95
Batch: 760; loss: 0.75; acc: 0.84
Batch: 780; loss: 0.69; acc: 0.88
Train Epoch over. train_loss: 0.81; train_accuracy: 0.86 

3.115896106464788e-05
1.2413026524882298e-05
Batch: 0; loss: 0.67; acc: 0.94
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.95
Batch: 100; loss: 0.65; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.94
Val Epoch over. val_loss: 0.6597725777489365; val_accuracy: 0.8916202229299363 

The current subspace-distance is: 1.2413026524882298e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.68; acc: 0.91
Batch: 40; loss: 0.64; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.81; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.73; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.73; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.91
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.92
Batch: 280; loss: 0.56; acc: 0.92
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.69; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.95
Batch: 400; loss: 0.68; acc: 0.89
Batch: 420; loss: 0.78; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.94
Batch: 480; loss: 0.58; acc: 0.94
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.94
Batch: 540; loss: 0.61; acc: 0.92
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.58; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.92
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.95
Batch: 740; loss: 0.62; acc: 0.91
Batch: 760; loss: 0.6; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.91
Train Epoch over. train_loss: 0.66; train_accuracy: 0.88 

3.601511343731545e-05
1.4970810298109427e-05
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.95
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Val Epoch over. val_loss: 0.5547046754390571; val_accuracy: 0.9011743630573248 

The current subspace-distance is: 1.4970810298109427e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.64; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.94
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.94
Batch: 260; loss: 0.55; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.97
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.55; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.92
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.52; acc: 0.92
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.94
Batch: 560; loss: 0.71; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.6; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.91
Batch: 700; loss: 0.72; acc: 0.81
Batch: 720; loss: 0.55; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.89 

3.974042920162901e-05
1.695059472694993e-05
Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.48706874241874476; val_accuracy: 0.9068471337579618 

The current subspace-distance is: 1.695059472694993e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.97
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.36; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.97
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.68; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.97
Batch: 600; loss: 0.51; acc: 0.92
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

4.267220720066689e-05
1.9388136934139766e-05
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.45737632663006994; val_accuracy: 0.9082404458598726 

The current subspace-distance is: 1.9388136934139766e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.29; acc: 1.0
Batch: 500; loss: 0.72; acc: 0.78
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.95
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.9 

4.638750760932453e-05
2.061501618300099e-05
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.4191994648070852; val_accuracy: 0.9145103503184714 

The current subspace-distance is: 2.061501618300099e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.95
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.95
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.95
Batch: 540; loss: 0.71; acc: 0.78
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.9 

4.776359855895862e-05
2.0480778402998112e-05
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.4046706680659276; val_accuracy: 0.9179936305732485 

The current subspace-distance is: 2.0480778402998112e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.94
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.0047663535224274e-05
2.2059086404624395e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.38219162736349044; val_accuracy: 0.9195859872611465 

The current subspace-distance is: 2.2059086404624395e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.95
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

5.259676254354417e-05
2.263809619762469e-05
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.36433386204728657; val_accuracy: 0.9202826433121019 

The current subspace-distance is: 2.263809619762469e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.98
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.98
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.385679833125323e-05
2.409425906080287e-05
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3530063683250148; val_accuracy: 0.9228702229299363 

The current subspace-distance is: 2.409425906080287e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.32; acc: 0.97
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.500012412085198e-05
2.4020959244808182e-05
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.35057680498642524; val_accuracy: 0.9240644904458599 

The current subspace-distance is: 2.4020959244808182e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.98
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.610204971162602e-05
2.6080899260705337e-05
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.35079548208956507; val_accuracy: 0.9225716560509554 

The current subspace-distance is: 2.6080899260705337e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.98
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.22; acc: 1.0
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.516595410881564e-05
2.3354765289695933e-05
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.3474109509758129; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.3354765289695933e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.97
Batch: 440; loss: 0.45; acc: 0.81
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.33; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.97
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.626918937196024e-05
2.5262037524953485e-05
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.34393727561091164; val_accuracy: 0.9232683121019108 

The current subspace-distance is: 2.5262037524953485e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.81
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.723516005673446e-05
2.5544253730913624e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.33533226660672266; val_accuracy: 0.9242635350318471 

The current subspace-distance is: 2.5544253730913624e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.7334960729349405e-05
2.5008150259964168e-05
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3398430422422992; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 2.5008150259964168e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.97
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.7110584748443216e-05
2.55934373853961e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.3284907506624605; val_accuracy: 0.9260549363057324 

The current subspace-distance is: 2.55934373853961e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.97
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.811110895592719e-05
2.546055293350946e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.33355471586725516; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 2.546055293350946e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.97
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.22; acc: 1.0
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.844979386893101e-05
2.6822866857401095e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.77
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.32984987100598157; val_accuracy: 0.925656847133758 

The current subspace-distance is: 2.6822866857401095e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.98
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.887098450330086e-05
2.5096644094446674e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.3260934046309465; val_accuracy: 0.9265525477707006 

The current subspace-distance is: 2.5096644094446674e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.946074816165492e-05
2.6888685169979e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.32179902992240944; val_accuracy: 0.9257563694267515 

The current subspace-distance is: 2.6888685169979e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.97
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.97
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.929317558184266e-05
2.6196372346021235e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.32473585531590093; val_accuracy: 0.926453025477707 

The current subspace-distance is: 2.6196372346021235e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.859222437720746e-05
2.496171509847045e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.75
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.3225651361570237; val_accuracy: 0.925656847133758 

The current subspace-distance is: 2.496171509847045e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.97
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.31; acc: 0.97
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.97
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.942796269664541e-05
2.7582907932810485e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.32125417029212233; val_accuracy: 0.9281449044585988 

The current subspace-distance is: 2.7582907932810485e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.3; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.8
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

5.910619438509457e-05
2.638236946950201e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.321833858159697; val_accuracy: 0.9278463375796179 

The current subspace-distance is: 2.638236946950201e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.98
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.2; acc: 1.0
Batch: 660; loss: 0.4; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

6.01956489845179e-05
2.7616930310614407e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.32064452525346904; val_accuracy: 0.9273487261146497 

The current subspace-distance is: 2.7616930310614407e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.5; acc: 0.81
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.9007426898460835e-05
2.5438157535973005e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.3224763716482053; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 2.5438157535973005e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

5.9939960920019075e-05
2.6654004614101723e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3231985038442976; val_accuracy: 0.9267515923566879 

The current subspace-distance is: 2.6654004614101723e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.907069498789497e-05
2.5000019377330318e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.320102124931706; val_accuracy: 0.9270501592356688 

The current subspace-distance is: 2.5000019377330318e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.98
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.8
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.91 

6.070772360544652e-05
2.7080341169494204e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.31856508861491634; val_accuracy: 0.9267515923566879 

The current subspace-distance is: 2.7080341169494204e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_8_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.4265201420964964

The number of parameters is: 257601

The number of individual parameters is:

20
360
20
20
30
40800
30
30
59
120360
59
59
64
90624
64
64
4096
64
640
10
64
64

nonzero elements in E: 128800489
elements in E: 128800500
fraction nonzero: 0.9999999145966049
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.09; acc: 0.33
Batch: 40; loss: 1.72; acc: 0.53
Batch: 60; loss: 1.58; acc: 0.62
Batch: 80; loss: 1.57; acc: 0.67
Batch: 100; loss: 1.4; acc: 0.72
Batch: 120; loss: 1.5; acc: 0.66
Batch: 140; loss: 1.51; acc: 0.66
Batch: 160; loss: 1.25; acc: 0.81
Batch: 180; loss: 1.39; acc: 0.66
Batch: 200; loss: 1.17; acc: 0.84
Batch: 220; loss: 1.33; acc: 0.69
Batch: 240; loss: 1.24; acc: 0.77
Batch: 260; loss: 1.28; acc: 0.72
Batch: 280; loss: 1.22; acc: 0.72
Batch: 300; loss: 1.15; acc: 0.84
Batch: 320; loss: 1.08; acc: 0.86
Batch: 340; loss: 1.01; acc: 0.91
Batch: 360; loss: 1.06; acc: 0.84
Batch: 380; loss: 0.94; acc: 0.89
Batch: 400; loss: 1.13; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.92
Batch: 440; loss: 0.95; acc: 0.91
Batch: 460; loss: 1.08; acc: 0.78
Batch: 480; loss: 1.05; acc: 0.88
Batch: 500; loss: 0.97; acc: 0.81
Batch: 520; loss: 0.96; acc: 0.84
Batch: 540; loss: 0.86; acc: 0.88
Batch: 560; loss: 1.02; acc: 0.75
Batch: 580; loss: 0.94; acc: 0.77
Batch: 600; loss: 0.9; acc: 0.89
Batch: 620; loss: 0.9; acc: 0.8
Batch: 640; loss: 0.84; acc: 0.88
Batch: 660; loss: 0.87; acc: 0.8
Batch: 680; loss: 0.82; acc: 0.89
Batch: 700; loss: 0.84; acc: 0.91
Batch: 720; loss: 0.89; acc: 0.83
Batch: 740; loss: 0.72; acc: 0.98
Batch: 760; loss: 0.84; acc: 0.89
Batch: 780; loss: 0.92; acc: 0.86
Train Epoch over. train_loss: 1.15; train_accuracy: 0.77 

2.5423156330361962e-05
9.368079190608114e-06
Batch: 0; loss: 0.82; acc: 0.88
Batch: 20; loss: 0.97; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.97
Batch: 60; loss: 0.83; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.77; acc: 0.91
Batch: 120; loss: 0.95; acc: 0.81
Batch: 140; loss: 0.67; acc: 0.92
Val Epoch over. val_loss: 0.7776790880093909; val_accuracy: 0.878781847133758 

The current subspace-distance is: 9.368079190608114e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.86
Batch: 40; loss: 0.75; acc: 0.91
Batch: 60; loss: 0.79; acc: 0.89
Batch: 80; loss: 0.83; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.86
Batch: 140; loss: 0.77; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.83
Batch: 180; loss: 0.7; acc: 0.89
Batch: 200; loss: 0.84; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.95
Batch: 240; loss: 0.76; acc: 0.84
Batch: 260; loss: 0.69; acc: 0.89
Batch: 280; loss: 0.8; acc: 0.84
Batch: 300; loss: 0.74; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.91
Batch: 340; loss: 0.85; acc: 0.83
Batch: 360; loss: 0.69; acc: 0.89
Batch: 380; loss: 0.76; acc: 0.84
Batch: 400; loss: 0.6; acc: 0.94
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.69; acc: 0.91
Batch: 460; loss: 0.61; acc: 0.94
Batch: 480; loss: 0.69; acc: 0.94
Batch: 500; loss: 0.67; acc: 0.91
Batch: 520; loss: 0.67; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.94
Batch: 560; loss: 0.71; acc: 0.88
Batch: 580; loss: 0.6; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.66; acc: 0.92
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.88
Batch: 760; loss: 0.67; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.89
Train Epoch over. train_loss: 0.73; train_accuracy: 0.87 

3.1793839298188686e-05
1.286906081077177e-05
Batch: 0; loss: 0.64; acc: 0.92
Batch: 20; loss: 0.78; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.95
Batch: 100; loss: 0.59; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.97
Val Epoch over. val_loss: 0.5880335351084448; val_accuracy: 0.9044585987261147 

The current subspace-distance is: 1.286906081077177e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.72; acc: 0.89
Batch: 40; loss: 0.66; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.91
Batch: 160; loss: 0.7; acc: 0.88
Batch: 180; loss: 0.64; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.94
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.89
Batch: 260; loss: 0.71; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.91
Batch: 320; loss: 0.78; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.91
Batch: 380; loss: 0.6; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.65; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.97
Batch: 480; loss: 0.48; acc: 0.91
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.95
Batch: 540; loss: 0.54; acc: 0.92
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.91
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.57; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.97
Batch: 720; loss: 0.55; acc: 0.91
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.46; acc: 0.92
Batch: 780; loss: 0.65; acc: 0.88
Train Epoch over. train_loss: 0.59; train_accuracy: 0.89 

3.663814641186036e-05
1.5582540072500706e-05
Batch: 0; loss: 0.53; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.31; acc: 0.98
Val Epoch over. val_loss: 0.48564031644231953; val_accuracy: 0.9130175159235668 

The current subspace-distance is: 1.5582540072500706e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.98
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.92
Batch: 140; loss: 0.61; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.92
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.91
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.9 

4.061294021084905e-05
1.864742262114305e-05
Batch: 0; loss: 0.46; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.4297132429423606; val_accuracy: 0.9173964968152867 

The current subspace-distance is: 1.864742262114305e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.94
Batch: 200; loss: 0.53; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.91
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.45; train_accuracy: 0.91 

4.254941450199112e-05
1.8547278159530833e-05
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.3894847987373923; val_accuracy: 0.9224721337579618 

The current subspace-distance is: 1.8547278159530833e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.95
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.97
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.91 

4.610409450833686e-05
2.113237678713631e-05
Batch: 0; loss: 0.37; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.3598908849392727; val_accuracy: 0.9269506369426752 

The current subspace-distance is: 2.113237678713631e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.97
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.97
Batch: 400; loss: 0.25; acc: 1.0
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.97
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

4.835036088479683e-05
2.1829242541571148e-05
Batch: 0; loss: 0.34; acc: 0.97
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.33579049862114485; val_accuracy: 0.9294386942675159 

The current subspace-distance is: 2.1829242541571148e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.92 

5.125227107782848e-05
2.478650822013151e-05
Batch: 0; loss: 0.32; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3214883802423052; val_accuracy: 0.9301353503184714 

The current subspace-distance is: 2.478650822013151e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.303534271661192e-05
2.53518192039337e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3153172760822211; val_accuracy: 0.9314291401273885 

The current subspace-distance is: 2.53518192039337e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.523177605937235e-05
2.625317756610457e-05
Batch: 0; loss: 0.29; acc: 1.0
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3010633001281957; val_accuracy: 0.9323248407643312 

The current subspace-distance is: 2.625317756610457e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.97
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.5709253501845524e-05
2.457319169479888e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.2964103848786111; val_accuracy: 0.9338176751592356 

The current subspace-distance is: 2.457319169479888e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.84
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.17; acc: 1.0
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.98
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.5836582760093734e-05
2.6177998734056018e-05
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.2973977798109601; val_accuracy: 0.9327229299363057 

The current subspace-distance is: 2.6177998734056018e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.98
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.718001921195537e-05
2.6807316316990182e-05
Batch: 0; loss: 0.29; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.28862687940620313; val_accuracy: 0.9324243630573248 

The current subspace-distance is: 2.6807316316990182e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.686631629941985e-05
2.554049206082709e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2901128304612105; val_accuracy: 0.9341162420382165 

The current subspace-distance is: 2.554049206082709e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.98
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.64047550142277e-05
2.5572078811819665e-05
Batch: 0; loss: 0.28; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2872892745361207; val_accuracy: 0.9340167197452229 

The current subspace-distance is: 2.5572078811819665e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.97
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.832423994434066e-05
2.75629081443185e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.15; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2864984049443986; val_accuracy: 0.9332205414012739 

The current subspace-distance is: 2.75629081443185e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.864039121661335e-05
2.8400963856256567e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.14; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.28275686768209857; val_accuracy: 0.9340167197452229 

The current subspace-distance is: 2.8400963856256567e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.97
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.19; acc: 1.0
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.977303590043448e-05
2.7865462470799685e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2837321508652086; val_accuracy: 0.9340167197452229 

The current subspace-distance is: 2.7865462470799685e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.98
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.892413537367247e-05
2.6866486223298125e-05
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.14; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2779740277369311; val_accuracy: 0.9347133757961783 

The current subspace-distance is: 2.6866486223298125e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.98
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.98
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

5.956704990239814e-05
2.8136393666500226e-05
Batch: 0; loss: 0.26; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.27316796442695485; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.8136393666500226e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.98
Batch: 160; loss: 0.24; acc: 0.98
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.2; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.98
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

5.959424379398115e-05
2.6779571271617897e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.13; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2757037632689355; val_accuracy: 0.9357085987261147 

The current subspace-distance is: 2.6779571271617897e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.132276757853106e-05
2.923136889876332e-05
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.27505794186500987; val_accuracy: 0.9353105095541401 

The current subspace-distance is: 2.923136889876332e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.21; acc: 1.0
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.98
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.98
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.98
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.139564356999472e-05
3.035795634787064e-05
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.27132415425056106; val_accuracy: 0.9365047770700637 

The current subspace-distance is: 3.035795634787064e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.090702299843542e-05
2.9469449145835824e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2721889884134007; val_accuracy: 0.935609076433121 

The current subspace-distance is: 2.9469449145835824e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.21; acc: 1.0
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.058021244825795e-05
2.815535117406398e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.27051152815674523; val_accuracy: 0.9363057324840764 

The current subspace-distance is: 2.815535117406398e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.13; acc: 1.0
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.98
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.127075903350487e-05
3.050707709917333e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.270356240593324; val_accuracy: 0.9365047770700637 

The current subspace-distance is: 3.050707709917333e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.136961746960878e-05
2.7174957722309045e-05
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.27266262689973136; val_accuracy: 0.9358081210191083 

The current subspace-distance is: 2.7174957722309045e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.0563346778508276e-05
2.7448326363810338e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.13; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2687637644114008; val_accuracy: 0.9360071656050956 

The current subspace-distance is: 2.7448326363810338e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.8
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.175128510221839e-05
2.9354743674048223e-05
Batch: 0; loss: 0.25; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.26895254456503376; val_accuracy: 0.9362062101910829 

The current subspace-distance is: 2.9354743674048223e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.98
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.19; acc: 1.0
Batch: 480; loss: 0.19; acc: 1.0
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.18; acc: 1.0
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.149916589492932e-05
2.863673034880776e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.26799584132660725; val_accuracy: 0.9375 

The current subspace-distance is: 2.863673034880776e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_8_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:48/N_8_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
