model : table13slim
N : 9
flips : False
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:48

Channel scaling factor: 2.1990338787749493

The number of parameters is: 264111

The number of individual parameters is:

18
324
18
18
27
42282
27
27
53
124497
53
53
64
91584
64
64
4096
64
640
10
64
64

nonzero elements in E: 13205548
elements in E: 13205550
fraction nonzero: 0.999999848548527
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.36; acc: 0.12
Batch: 20; loss: 2.33; acc: 0.17
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.36; acc: 0.09
Batch: 80; loss: 2.24; acc: 0.17
Batch: 100; loss: 2.19; acc: 0.19
Batch: 120; loss: 2.24; acc: 0.16
Batch: 140; loss: 2.12; acc: 0.25
Batch: 160; loss: 2.21; acc: 0.23
Batch: 180; loss: 2.14; acc: 0.3
Batch: 200; loss: 2.16; acc: 0.19
Batch: 220; loss: 2.17; acc: 0.2
Batch: 240; loss: 2.05; acc: 0.22
Batch: 260; loss: 2.1; acc: 0.25
Batch: 280; loss: 2.11; acc: 0.19
Batch: 300; loss: 2.09; acc: 0.28
Batch: 320; loss: 2.08; acc: 0.25
Batch: 340; loss: 2.03; acc: 0.31
Batch: 360; loss: 2.05; acc: 0.28
Batch: 380; loss: 2.14; acc: 0.27
Batch: 400; loss: 2.07; acc: 0.3
Batch: 420; loss: 2.03; acc: 0.25
Batch: 440; loss: 2.02; acc: 0.33
Batch: 460; loss: 1.99; acc: 0.31
Batch: 480; loss: 1.88; acc: 0.33
Batch: 500; loss: 2.06; acc: 0.23
Batch: 520; loss: 1.92; acc: 0.44
Batch: 540; loss: 2.06; acc: 0.31
Batch: 560; loss: 1.97; acc: 0.34
Batch: 580; loss: 1.94; acc: 0.39
Batch: 600; loss: 1.98; acc: 0.42
Batch: 620; loss: 1.96; acc: 0.38
Batch: 640; loss: 1.9; acc: 0.41
Batch: 660; loss: 1.9; acc: 0.36
Batch: 680; loss: 1.8; acc: 0.52
Batch: 700; loss: 1.86; acc: 0.36
Batch: 720; loss: 1.88; acc: 0.34
Batch: 740; loss: 1.83; acc: 0.41
Batch: 760; loss: 1.83; acc: 0.42
Batch: 780; loss: 1.81; acc: 0.48
Train Epoch over. train_loss: 2.05; train_accuracy: 0.29 

2.2168280338519253e-05
4.365458153188229e-06
Batch: 0; loss: 1.97; acc: 0.34
Batch: 20; loss: 1.8; acc: 0.48
Batch: 40; loss: 1.69; acc: 0.58
Batch: 60; loss: 1.78; acc: 0.5
Batch: 80; loss: 1.82; acc: 0.55
Batch: 100; loss: 1.84; acc: 0.42
Batch: 120; loss: 1.91; acc: 0.38
Batch: 140; loss: 1.68; acc: 0.62
Val Epoch over. val_loss: 1.8416458884621882; val_accuracy: 0.43471337579617836 

The current subspace-distance is: 4.365458153188229e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.92; acc: 0.39
Batch: 20; loss: 1.9; acc: 0.39
Batch: 40; loss: 1.79; acc: 0.44
Batch: 60; loss: 1.88; acc: 0.34
Batch: 80; loss: 1.92; acc: 0.38
Batch: 100; loss: 1.84; acc: 0.39
Batch: 120; loss: 1.81; acc: 0.44
Batch: 140; loss: 1.89; acc: 0.45
Batch: 160; loss: 1.86; acc: 0.44
Batch: 180; loss: 1.87; acc: 0.41
Batch: 200; loss: 1.82; acc: 0.5
Batch: 220; loss: 1.85; acc: 0.45
Batch: 240; loss: 1.76; acc: 0.48
Batch: 260; loss: 1.7; acc: 0.53
Batch: 280; loss: 1.86; acc: 0.47
Batch: 300; loss: 1.81; acc: 0.48
Batch: 320; loss: 1.77; acc: 0.52
Batch: 340; loss: 1.78; acc: 0.52
Batch: 360; loss: 1.82; acc: 0.42
Batch: 380; loss: 1.82; acc: 0.47
Batch: 400; loss: 1.8; acc: 0.47
Batch: 420; loss: 1.85; acc: 0.42
Batch: 440; loss: 1.86; acc: 0.5
Batch: 460; loss: 1.88; acc: 0.41
Batch: 480; loss: 1.83; acc: 0.45
Batch: 500; loss: 1.74; acc: 0.52
Batch: 520; loss: 1.79; acc: 0.48
Batch: 540; loss: 1.71; acc: 0.52
Batch: 560; loss: 1.88; acc: 0.42
Batch: 580; loss: 1.78; acc: 0.5
Batch: 600; loss: 1.74; acc: 0.58
Batch: 620; loss: 1.84; acc: 0.42
Batch: 640; loss: 1.86; acc: 0.41
Batch: 660; loss: 1.79; acc: 0.47
Batch: 680; loss: 1.76; acc: 0.45
Batch: 700; loss: 1.78; acc: 0.52
Batch: 720; loss: 1.82; acc: 0.5
Batch: 740; loss: 1.78; acc: 0.42
Batch: 760; loss: 1.84; acc: 0.45
Batch: 780; loss: 1.8; acc: 0.53
Train Epoch over. train_loss: 1.81; train_accuracy: 0.46 

2.58310537901707e-05
6.58099133943324e-06
Batch: 0; loss: 1.92; acc: 0.42
Batch: 20; loss: 1.74; acc: 0.59
Batch: 40; loss: 1.54; acc: 0.69
Batch: 60; loss: 1.71; acc: 0.58
Batch: 80; loss: 1.65; acc: 0.61
Batch: 100; loss: 1.75; acc: 0.53
Batch: 120; loss: 1.83; acc: 0.45
Batch: 140; loss: 1.59; acc: 0.62
Val Epoch over. val_loss: 1.7387279507460867; val_accuracy: 0.5140326433121019 

The current subspace-distance is: 6.58099133943324e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.8; acc: 0.52
Batch: 40; loss: 1.88; acc: 0.41
Batch: 60; loss: 1.84; acc: 0.41
Batch: 80; loss: 1.82; acc: 0.44
Batch: 100; loss: 1.76; acc: 0.45
Batch: 120; loss: 1.73; acc: 0.55
Batch: 140; loss: 1.78; acc: 0.45
Batch: 160; loss: 1.7; acc: 0.58
Batch: 180; loss: 1.76; acc: 0.45
Batch: 200; loss: 1.77; acc: 0.44
Batch: 220; loss: 1.9; acc: 0.39
Batch: 240; loss: 1.8; acc: 0.53
Batch: 260; loss: 1.78; acc: 0.53
Batch: 280; loss: 1.74; acc: 0.53
Batch: 300; loss: 1.83; acc: 0.42
Batch: 320; loss: 1.68; acc: 0.61
Batch: 340; loss: 1.74; acc: 0.48
Batch: 360; loss: 1.82; acc: 0.44
Batch: 380; loss: 1.74; acc: 0.44
Batch: 400; loss: 1.77; acc: 0.45
Batch: 420; loss: 1.62; acc: 0.59
Batch: 440; loss: 1.8; acc: 0.53
Batch: 460; loss: 1.81; acc: 0.44
Batch: 480; loss: 1.82; acc: 0.47
Batch: 500; loss: 1.8; acc: 0.52
Batch: 520; loss: 1.84; acc: 0.48
Batch: 540; loss: 1.86; acc: 0.33
Batch: 560; loss: 1.79; acc: 0.52
Batch: 580; loss: 1.72; acc: 0.55
Batch: 600; loss: 1.67; acc: 0.66
Batch: 620; loss: 1.91; acc: 0.39
Batch: 640; loss: 1.66; acc: 0.55
Batch: 660; loss: 1.83; acc: 0.44
Batch: 680; loss: 1.75; acc: 0.48
Batch: 700; loss: 1.77; acc: 0.47
Batch: 720; loss: 1.75; acc: 0.55
Batch: 740; loss: 1.81; acc: 0.44
Batch: 760; loss: 1.66; acc: 0.58
Batch: 780; loss: 1.7; acc: 0.52
Train Epoch over. train_loss: 1.76; train_accuracy: 0.49 

2.7892419893760234e-05
8.014870218175929e-06
Batch: 0; loss: 1.89; acc: 0.44
Batch: 20; loss: 1.72; acc: 0.61
Batch: 40; loss: 1.52; acc: 0.64
Batch: 60; loss: 1.68; acc: 0.59
Batch: 80; loss: 1.6; acc: 0.59
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.8; acc: 0.48
Batch: 140; loss: 1.55; acc: 0.64
Val Epoch over. val_loss: 1.7084355445424462; val_accuracy: 0.523984872611465 

The current subspace-distance is: 8.014870218175929e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.5
Batch: 20; loss: 1.73; acc: 0.52
Batch: 40; loss: 1.81; acc: 0.42
Batch: 60; loss: 1.76; acc: 0.39
Batch: 80; loss: 1.89; acc: 0.45
Batch: 100; loss: 1.81; acc: 0.36
Batch: 120; loss: 1.73; acc: 0.56
Batch: 140; loss: 1.84; acc: 0.41
Batch: 160; loss: 1.7; acc: 0.53
Batch: 180; loss: 1.78; acc: 0.48
Batch: 200; loss: 1.74; acc: 0.36
Batch: 220; loss: 1.89; acc: 0.39
Batch: 240; loss: 1.71; acc: 0.55
Batch: 260; loss: 1.72; acc: 0.48
Batch: 280; loss: 1.91; acc: 0.44
Batch: 300; loss: 1.78; acc: 0.47
Batch: 320; loss: 1.78; acc: 0.45
Batch: 340; loss: 1.88; acc: 0.41
Batch: 360; loss: 1.81; acc: 0.47
Batch: 380; loss: 1.82; acc: 0.45
Batch: 400; loss: 1.72; acc: 0.52
Batch: 420; loss: 1.52; acc: 0.62
Batch: 440; loss: 1.79; acc: 0.45
Batch: 460; loss: 1.75; acc: 0.56
Batch: 480; loss: 1.75; acc: 0.42
Batch: 500; loss: 1.73; acc: 0.56
Batch: 520; loss: 1.7; acc: 0.56
Batch: 540; loss: 1.75; acc: 0.48
Batch: 560; loss: 1.84; acc: 0.5
Batch: 580; loss: 1.8; acc: 0.41
Batch: 600; loss: 1.71; acc: 0.5
Batch: 620; loss: 1.88; acc: 0.44
Batch: 640; loss: 1.7; acc: 0.53
Batch: 660; loss: 1.71; acc: 0.52
Batch: 680; loss: 1.75; acc: 0.47
Batch: 700; loss: 1.73; acc: 0.48
Batch: 720; loss: 1.8; acc: 0.42
Batch: 740; loss: 1.65; acc: 0.58
Batch: 760; loss: 1.81; acc: 0.48
Batch: 780; loss: 1.59; acc: 0.5
Train Epoch over. train_loss: 1.74; train_accuracy: 0.49 

2.9280352464411408e-05
7.878510587033816e-06
Batch: 0; loss: 1.86; acc: 0.38
Batch: 20; loss: 1.7; acc: 0.59
Batch: 40; loss: 1.49; acc: 0.69
Batch: 60; loss: 1.63; acc: 0.61
Batch: 80; loss: 1.57; acc: 0.56
Batch: 100; loss: 1.66; acc: 0.58
Batch: 120; loss: 1.77; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.67
Val Epoch over. val_loss: 1.6871968097747512; val_accuracy: 0.5241839171974523 

The current subspace-distance is: 7.878510587033816e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.42
Batch: 20; loss: 1.7; acc: 0.53
Batch: 40; loss: 1.72; acc: 0.45
Batch: 60; loss: 1.72; acc: 0.42
Batch: 80; loss: 1.71; acc: 0.52
Batch: 100; loss: 1.53; acc: 0.62
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.65; acc: 0.56
Batch: 160; loss: 1.79; acc: 0.38
Batch: 180; loss: 1.72; acc: 0.44
Batch: 200; loss: 1.6; acc: 0.56
Batch: 220; loss: 1.69; acc: 0.58
Batch: 240; loss: 1.76; acc: 0.48
Batch: 260; loss: 1.72; acc: 0.53
Batch: 280; loss: 1.71; acc: 0.48
Batch: 300; loss: 1.71; acc: 0.56
Batch: 320; loss: 1.84; acc: 0.45
Batch: 340; loss: 1.69; acc: 0.52
Batch: 360; loss: 1.72; acc: 0.47
Batch: 380; loss: 1.79; acc: 0.5
Batch: 400; loss: 1.66; acc: 0.48
Batch: 420; loss: 1.71; acc: 0.48
Batch: 440; loss: 1.73; acc: 0.47
Batch: 460; loss: 1.7; acc: 0.5
Batch: 480; loss: 1.82; acc: 0.39
Batch: 500; loss: 1.68; acc: 0.5
Batch: 520; loss: 1.62; acc: 0.56
Batch: 540; loss: 1.68; acc: 0.58
Batch: 560; loss: 1.65; acc: 0.52
Batch: 580; loss: 1.68; acc: 0.47
Batch: 600; loss: 1.72; acc: 0.5
Batch: 620; loss: 1.73; acc: 0.45
Batch: 640; loss: 1.65; acc: 0.53
Batch: 660; loss: 1.51; acc: 0.66
Batch: 680; loss: 1.7; acc: 0.53
Batch: 700; loss: 1.74; acc: 0.48
Batch: 720; loss: 1.61; acc: 0.56
Batch: 740; loss: 1.65; acc: 0.53
Batch: 760; loss: 1.64; acc: 0.53
Batch: 780; loss: 1.81; acc: 0.39
Train Epoch over. train_loss: 1.71; train_accuracy: 0.5 

3.172702417941764e-05
1.1095541594841052e-05
Batch: 0; loss: 1.82; acc: 0.39
Batch: 20; loss: 1.68; acc: 0.53
Batch: 40; loss: 1.44; acc: 0.7
Batch: 60; loss: 1.55; acc: 0.64
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.6; acc: 0.64
Batch: 120; loss: 1.75; acc: 0.52
Batch: 140; loss: 1.44; acc: 0.67
Val Epoch over. val_loss: 1.6534327724177367; val_accuracy: 0.5303542993630573 

The current subspace-distance is: 1.1095541594841052e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.77; acc: 0.5
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.76; acc: 0.38
Batch: 60; loss: 1.77; acc: 0.47
Batch: 80; loss: 1.63; acc: 0.55
Batch: 100; loss: 1.79; acc: 0.45
Batch: 120; loss: 1.56; acc: 0.5
Batch: 140; loss: 1.66; acc: 0.53
Batch: 160; loss: 1.72; acc: 0.52
Batch: 180; loss: 1.73; acc: 0.44
Batch: 200; loss: 1.63; acc: 0.58
Batch: 220; loss: 1.57; acc: 0.52
Batch: 240; loss: 1.7; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.66
Batch: 280; loss: 1.63; acc: 0.53
Batch: 300; loss: 1.66; acc: 0.47
Batch: 320; loss: 1.65; acc: 0.52
Batch: 340; loss: 1.53; acc: 0.56
Batch: 360; loss: 1.69; acc: 0.47
Batch: 380; loss: 1.75; acc: 0.47
Batch: 400; loss: 1.68; acc: 0.45
Batch: 420; loss: 1.65; acc: 0.47
Batch: 440; loss: 1.75; acc: 0.42
Batch: 460; loss: 1.81; acc: 0.42
Batch: 480; loss: 1.7; acc: 0.45
Batch: 500; loss: 1.71; acc: 0.45
Batch: 520; loss: 1.63; acc: 0.53
Batch: 540; loss: 1.66; acc: 0.55
Batch: 560; loss: 1.67; acc: 0.47
Batch: 580; loss: 1.69; acc: 0.52
Batch: 600; loss: 1.73; acc: 0.47
Batch: 620; loss: 1.78; acc: 0.44
Batch: 640; loss: 1.74; acc: 0.38
Batch: 660; loss: 1.76; acc: 0.42
Batch: 680; loss: 1.75; acc: 0.53
Batch: 700; loss: 1.61; acc: 0.56
Batch: 720; loss: 1.75; acc: 0.42
Batch: 740; loss: 1.73; acc: 0.58
Batch: 760; loss: 1.68; acc: 0.55
Batch: 780; loss: 1.68; acc: 0.5
Train Epoch over. train_loss: 1.69; train_accuracy: 0.5 

3.247798667871393e-05
1.0672215466911439e-05
Batch: 0; loss: 1.83; acc: 0.36
Batch: 20; loss: 1.65; acc: 0.5
Batch: 40; loss: 1.4; acc: 0.67
Batch: 60; loss: 1.5; acc: 0.62
Batch: 80; loss: 1.51; acc: 0.56
Batch: 100; loss: 1.57; acc: 0.61
Batch: 120; loss: 1.73; acc: 0.5
Batch: 140; loss: 1.42; acc: 0.67
Val Epoch over. val_loss: 1.637456254594645; val_accuracy: 0.5283638535031847 

The current subspace-distance is: 1.0672215466911439e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.42
Batch: 20; loss: 1.62; acc: 0.53
Batch: 40; loss: 1.72; acc: 0.44
Batch: 60; loss: 1.49; acc: 0.67
Batch: 80; loss: 1.57; acc: 0.58
Batch: 100; loss: 1.63; acc: 0.53
Batch: 120; loss: 1.64; acc: 0.56
Batch: 140; loss: 1.68; acc: 0.48
Batch: 160; loss: 1.63; acc: 0.56
Batch: 180; loss: 1.8; acc: 0.38
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.64; acc: 0.53
Batch: 240; loss: 1.82; acc: 0.45
Batch: 260; loss: 1.64; acc: 0.56
Batch: 280; loss: 1.71; acc: 0.53
Batch: 300; loss: 1.71; acc: 0.48
Batch: 320; loss: 1.49; acc: 0.66
Batch: 340; loss: 1.71; acc: 0.48
Batch: 360; loss: 1.57; acc: 0.61
Batch: 380; loss: 1.72; acc: 0.45
Batch: 400; loss: 1.62; acc: 0.5
Batch: 420; loss: 1.64; acc: 0.52
Batch: 440; loss: 1.71; acc: 0.44
Batch: 460; loss: 1.56; acc: 0.52
Batch: 480; loss: 1.61; acc: 0.55
Batch: 500; loss: 1.69; acc: 0.42
Batch: 520; loss: 1.66; acc: 0.48
Batch: 540; loss: 1.7; acc: 0.5
Batch: 560; loss: 1.76; acc: 0.42
Batch: 580; loss: 1.65; acc: 0.53
Batch: 600; loss: 1.63; acc: 0.5
Batch: 620; loss: 1.64; acc: 0.48
Batch: 640; loss: 1.52; acc: 0.58
Batch: 660; loss: 1.76; acc: 0.42
Batch: 680; loss: 1.7; acc: 0.41
Batch: 700; loss: 1.68; acc: 0.5
Batch: 720; loss: 1.79; acc: 0.47
Batch: 740; loss: 1.54; acc: 0.61
Batch: 760; loss: 1.54; acc: 0.58
Batch: 780; loss: 1.48; acc: 0.66
Train Epoch over. train_loss: 1.67; train_accuracy: 0.5 

3.408531847526319e-05
1.0298460438207258e-05
Batch: 0; loss: 1.8; acc: 0.39
Batch: 20; loss: 1.64; acc: 0.45
Batch: 40; loss: 1.38; acc: 0.7
Batch: 60; loss: 1.47; acc: 0.66
Batch: 80; loss: 1.49; acc: 0.58
Batch: 100; loss: 1.53; acc: 0.62
Batch: 120; loss: 1.69; acc: 0.52
Batch: 140; loss: 1.41; acc: 0.66
Val Epoch over. val_loss: 1.6141494892205401; val_accuracy: 0.5394108280254777 

The current subspace-distance is: 1.0298460438207258e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.57; acc: 0.64
Batch: 20; loss: 1.6; acc: 0.56
Batch: 40; loss: 1.62; acc: 0.58
Batch: 60; loss: 1.76; acc: 0.45
Batch: 80; loss: 1.54; acc: 0.61
Batch: 100; loss: 1.58; acc: 0.56
Batch: 120; loss: 1.5; acc: 0.69
Batch: 140; loss: 1.52; acc: 0.58
Batch: 160; loss: 1.65; acc: 0.47
Batch: 180; loss: 1.66; acc: 0.45
Batch: 200; loss: 1.6; acc: 0.55
Batch: 220; loss: 1.54; acc: 0.62
Batch: 240; loss: 1.71; acc: 0.45
Batch: 260; loss: 1.61; acc: 0.56
Batch: 280; loss: 1.74; acc: 0.44
Batch: 300; loss: 1.59; acc: 0.55
Batch: 320; loss: 1.67; acc: 0.47
Batch: 340; loss: 1.73; acc: 0.48
Batch: 360; loss: 1.6; acc: 0.59
Batch: 380; loss: 1.84; acc: 0.38
Batch: 400; loss: 1.68; acc: 0.39
Batch: 420; loss: 1.58; acc: 0.53
Batch: 440; loss: 1.74; acc: 0.52
Batch: 460; loss: 1.8; acc: 0.42
Batch: 480; loss: 1.68; acc: 0.47
Batch: 500; loss: 1.71; acc: 0.48
Batch: 520; loss: 1.8; acc: 0.41
Batch: 540; loss: 1.65; acc: 0.5
Batch: 560; loss: 1.6; acc: 0.52
Batch: 580; loss: 1.62; acc: 0.64
Batch: 600; loss: 1.64; acc: 0.47
Batch: 620; loss: 1.63; acc: 0.55
Batch: 640; loss: 1.57; acc: 0.58
Batch: 660; loss: 1.62; acc: 0.48
Batch: 680; loss: 1.51; acc: 0.56
Batch: 700; loss: 1.53; acc: 0.59
Batch: 720; loss: 1.77; acc: 0.44
Batch: 740; loss: 1.68; acc: 0.47
Batch: 760; loss: 1.66; acc: 0.5
Batch: 780; loss: 1.56; acc: 0.58
Train Epoch over. train_loss: 1.65; train_accuracy: 0.5 

3.5120461689075455e-05
1.2714581316686235e-05
Batch: 0; loss: 1.8; acc: 0.42
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.37; acc: 0.7
Batch: 60; loss: 1.45; acc: 0.66
Batch: 80; loss: 1.49; acc: 0.55
Batch: 100; loss: 1.51; acc: 0.64
Batch: 120; loss: 1.65; acc: 0.55
Batch: 140; loss: 1.4; acc: 0.7
Val Epoch over. val_loss: 1.598822157094433; val_accuracy: 0.5414012738853503 

The current subspace-distance is: 1.2714581316686235e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.44
Batch: 20; loss: 1.67; acc: 0.5
Batch: 40; loss: 1.69; acc: 0.45
Batch: 60; loss: 1.69; acc: 0.55
Batch: 80; loss: 1.68; acc: 0.48
Batch: 100; loss: 1.68; acc: 0.45
Batch: 120; loss: 1.69; acc: 0.53
Batch: 140; loss: 1.62; acc: 0.53
Batch: 160; loss: 1.59; acc: 0.59
Batch: 180; loss: 1.77; acc: 0.44
Batch: 200; loss: 1.63; acc: 0.5
Batch: 220; loss: 1.63; acc: 0.48
Batch: 240; loss: 1.54; acc: 0.59
Batch: 260; loss: 1.53; acc: 0.56
Batch: 280; loss: 1.71; acc: 0.45
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.64; acc: 0.53
Batch: 340; loss: 1.85; acc: 0.42
Batch: 360; loss: 1.77; acc: 0.42
Batch: 380; loss: 1.67; acc: 0.48
Batch: 400; loss: 1.61; acc: 0.53
Batch: 420; loss: 1.55; acc: 0.62
Batch: 440; loss: 1.63; acc: 0.48
Batch: 460; loss: 1.6; acc: 0.42
Batch: 480; loss: 1.44; acc: 0.62
Batch: 500; loss: 1.6; acc: 0.5
Batch: 520; loss: 1.69; acc: 0.44
Batch: 540; loss: 1.66; acc: 0.52
Batch: 560; loss: 1.56; acc: 0.55
Batch: 580; loss: 1.65; acc: 0.55
Batch: 600; loss: 1.62; acc: 0.5
Batch: 620; loss: 1.62; acc: 0.55
Batch: 640; loss: 1.67; acc: 0.52
Batch: 660; loss: 1.63; acc: 0.52
Batch: 680; loss: 1.59; acc: 0.55
Batch: 700; loss: 1.63; acc: 0.5
Batch: 720; loss: 1.63; acc: 0.47
Batch: 740; loss: 1.55; acc: 0.5
Batch: 760; loss: 1.64; acc: 0.42
Batch: 780; loss: 1.71; acc: 0.44
Train Epoch over. train_loss: 1.64; train_accuracy: 0.51 

3.7145407986827195e-05
1.2337239240878262e-05
Batch: 0; loss: 1.79; acc: 0.42
Batch: 20; loss: 1.63; acc: 0.52
Batch: 40; loss: 1.35; acc: 0.69
Batch: 60; loss: 1.43; acc: 0.62
Batch: 80; loss: 1.45; acc: 0.59
Batch: 100; loss: 1.52; acc: 0.66
Batch: 120; loss: 1.64; acc: 0.56
Batch: 140; loss: 1.38; acc: 0.69
Val Epoch over. val_loss: 1.5770892079468746; val_accuracy: 0.5542396496815286 

The current subspace-distance is: 1.2337239240878262e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.65; acc: 0.53
Batch: 20; loss: 1.58; acc: 0.53
Batch: 40; loss: 1.67; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.45
Batch: 80; loss: 1.51; acc: 0.58
Batch: 100; loss: 1.6; acc: 0.5
Batch: 120; loss: 1.48; acc: 0.59
Batch: 140; loss: 1.52; acc: 0.59
Batch: 160; loss: 1.7; acc: 0.48
Batch: 180; loss: 1.55; acc: 0.52
Batch: 200; loss: 1.73; acc: 0.39
Batch: 220; loss: 1.71; acc: 0.42
Batch: 240; loss: 1.53; acc: 0.53
Batch: 260; loss: 1.72; acc: 0.45
Batch: 280; loss: 1.58; acc: 0.58
Batch: 300; loss: 1.53; acc: 0.52
Batch: 320; loss: 1.55; acc: 0.53
Batch: 340; loss: 1.77; acc: 0.42
Batch: 360; loss: 1.62; acc: 0.5
Batch: 380; loss: 1.58; acc: 0.52
Batch: 400; loss: 1.67; acc: 0.45
Batch: 420; loss: 1.72; acc: 0.44
Batch: 440; loss: 1.63; acc: 0.56
Batch: 460; loss: 1.53; acc: 0.59
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.58; acc: 0.48
Batch: 520; loss: 1.56; acc: 0.5
Batch: 540; loss: 1.62; acc: 0.59
Batch: 560; loss: 1.58; acc: 0.56
Batch: 580; loss: 1.6; acc: 0.59
Batch: 600; loss: 1.66; acc: 0.5
Batch: 620; loss: 1.56; acc: 0.5
Batch: 640; loss: 1.5; acc: 0.59
Batch: 660; loss: 1.62; acc: 0.52
Batch: 680; loss: 1.61; acc: 0.5
Batch: 700; loss: 1.64; acc: 0.56
Batch: 720; loss: 1.65; acc: 0.53
Batch: 740; loss: 1.5; acc: 0.62
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.68; acc: 0.42
Train Epoch over. train_loss: 1.63; train_accuracy: 0.51 

3.818458935711533e-05
1.2772588888765313e-05
Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 1.35; acc: 0.69
Batch: 60; loss: 1.42; acc: 0.61
Batch: 80; loss: 1.45; acc: 0.55
Batch: 100; loss: 1.49; acc: 0.66
Batch: 120; loss: 1.61; acc: 0.53
Batch: 140; loss: 1.39; acc: 0.64
Val Epoch over. val_loss: 1.5703395210253965; val_accuracy: 0.5498606687898089 

The current subspace-distance is: 1.2772588888765313e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.67; acc: 0.52
Batch: 20; loss: 1.76; acc: 0.52
Batch: 40; loss: 1.55; acc: 0.58
Batch: 60; loss: 1.64; acc: 0.42
Batch: 80; loss: 1.6; acc: 0.39
Batch: 100; loss: 1.75; acc: 0.44
Batch: 120; loss: 1.63; acc: 0.47
Batch: 140; loss: 1.52; acc: 0.62
Batch: 160; loss: 1.58; acc: 0.53
Batch: 180; loss: 1.67; acc: 0.45
Batch: 200; loss: 1.65; acc: 0.5
Batch: 220; loss: 1.68; acc: 0.52
Batch: 240; loss: 1.59; acc: 0.53
Batch: 260; loss: 1.57; acc: 0.58
Batch: 280; loss: 1.5; acc: 0.69
Batch: 300; loss: 1.6; acc: 0.48
Batch: 320; loss: 1.65; acc: 0.5
Batch: 340; loss: 1.57; acc: 0.52
Batch: 360; loss: 1.74; acc: 0.44
Batch: 380; loss: 1.66; acc: 0.55
Batch: 400; loss: 1.66; acc: 0.42
Batch: 420; loss: 1.58; acc: 0.53
Batch: 440; loss: 1.67; acc: 0.45
Batch: 460; loss: 1.54; acc: 0.59
Batch: 480; loss: 1.57; acc: 0.52
Batch: 500; loss: 1.49; acc: 0.58
Batch: 520; loss: 1.59; acc: 0.55
Batch: 540; loss: 1.54; acc: 0.56
Batch: 560; loss: 1.7; acc: 0.42
Batch: 580; loss: 1.73; acc: 0.47
Batch: 600; loss: 1.58; acc: 0.5
Batch: 620; loss: 1.67; acc: 0.42
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.7; acc: 0.44
Batch: 680; loss: 1.72; acc: 0.41
Batch: 700; loss: 1.56; acc: 0.55
Batch: 720; loss: 1.55; acc: 0.58
Batch: 740; loss: 1.58; acc: 0.52
Batch: 760; loss: 1.71; acc: 0.45
Batch: 780; loss: 1.62; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

3.8963527913438156e-05
1.2570703802339267e-05
Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.59; acc: 0.53
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 1.41; acc: 0.61
Batch: 80; loss: 1.43; acc: 0.55
Batch: 100; loss: 1.48; acc: 0.64
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.37; acc: 0.67
Val Epoch over. val_loss: 1.557128414226945; val_accuracy: 0.5604100318471338 

The current subspace-distance is: 1.2570703802339267e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.48
Batch: 20; loss: 1.5; acc: 0.59
Batch: 40; loss: 1.68; acc: 0.42
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.64; acc: 0.5
Batch: 100; loss: 1.52; acc: 0.62
Batch: 120; loss: 1.5; acc: 0.59
Batch: 140; loss: 1.68; acc: 0.47
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.64; acc: 0.48
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.57; acc: 0.47
Batch: 240; loss: 1.74; acc: 0.41
Batch: 260; loss: 1.56; acc: 0.58
Batch: 280; loss: 1.64; acc: 0.47
Batch: 300; loss: 1.46; acc: 0.59
Batch: 320; loss: 1.62; acc: 0.53
Batch: 340; loss: 1.63; acc: 0.5
Batch: 360; loss: 1.65; acc: 0.48
Batch: 380; loss: 1.64; acc: 0.5
Batch: 400; loss: 1.53; acc: 0.55
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.57; acc: 0.52
Batch: 460; loss: 1.61; acc: 0.47
Batch: 480; loss: 1.68; acc: 0.45
Batch: 500; loss: 1.6; acc: 0.61
Batch: 520; loss: 1.55; acc: 0.55
Batch: 540; loss: 1.47; acc: 0.59
Batch: 560; loss: 1.5; acc: 0.61
Batch: 580; loss: 1.51; acc: 0.53
Batch: 600; loss: 1.61; acc: 0.52
Batch: 620; loss: 1.68; acc: 0.48
Batch: 640; loss: 1.71; acc: 0.42
Batch: 660; loss: 1.47; acc: 0.62
Batch: 680; loss: 1.69; acc: 0.42
Batch: 700; loss: 1.64; acc: 0.47
Batch: 720; loss: 1.67; acc: 0.41
Batch: 740; loss: 1.71; acc: 0.47
Batch: 760; loss: 1.62; acc: 0.42
Batch: 780; loss: 1.56; acc: 0.59
Train Epoch over. train_loss: 1.61; train_accuracy: 0.52 

3.95147581002675e-05
9.725033123686444e-06
Batch: 0; loss: 1.8; acc: 0.41
Batch: 20; loss: 1.6; acc: 0.52
Batch: 40; loss: 1.35; acc: 0.72
Batch: 60; loss: 1.42; acc: 0.61
Batch: 80; loss: 1.43; acc: 0.58
Batch: 100; loss: 1.5; acc: 0.61
Batch: 120; loss: 1.63; acc: 0.55
Batch: 140; loss: 1.37; acc: 0.69
Val Epoch over. val_loss: 1.5627274095632468; val_accuracy: 0.5556329617834395 

The current subspace-distance is: 9.725033123686444e-06 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.59; acc: 0.42
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.65; acc: 0.53
Batch: 60; loss: 1.62; acc: 0.53
Batch: 80; loss: 1.74; acc: 0.47
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.55; acc: 0.55
Batch: 140; loss: 1.71; acc: 0.5
Batch: 160; loss: 1.63; acc: 0.52
Batch: 180; loss: 1.75; acc: 0.41
Batch: 200; loss: 1.43; acc: 0.58
Batch: 220; loss: 1.65; acc: 0.52
Batch: 240; loss: 1.49; acc: 0.61
Batch: 260; loss: 1.63; acc: 0.52
Batch: 280; loss: 1.64; acc: 0.56
Batch: 300; loss: 1.65; acc: 0.44
Batch: 320; loss: 1.58; acc: 0.56
Batch: 340; loss: 1.55; acc: 0.55
Batch: 360; loss: 1.52; acc: 0.58
Batch: 380; loss: 1.57; acc: 0.53
Batch: 400; loss: 1.64; acc: 0.42
Batch: 420; loss: 1.6; acc: 0.45
Batch: 440; loss: 1.58; acc: 0.56
Batch: 460; loss: 1.66; acc: 0.47
Batch: 480; loss: 1.49; acc: 0.61
Batch: 500; loss: 1.67; acc: 0.5
Batch: 520; loss: 1.65; acc: 0.5
Batch: 540; loss: 1.59; acc: 0.48
Batch: 560; loss: 1.58; acc: 0.55
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.59; acc: 0.56
Batch: 620; loss: 1.6; acc: 0.55
Batch: 640; loss: 1.42; acc: 0.7
Batch: 660; loss: 1.66; acc: 0.48
Batch: 680; loss: 1.5; acc: 0.56
Batch: 700; loss: 1.63; acc: 0.48
Batch: 720; loss: 1.57; acc: 0.55
Batch: 740; loss: 1.73; acc: 0.44
Batch: 760; loss: 1.54; acc: 0.56
Batch: 780; loss: 1.6; acc: 0.52
Train Epoch over. train_loss: 1.6; train_accuracy: 0.52 

3.961296897614375e-05
1.213203722727485e-05
Batch: 0; loss: 1.77; acc: 0.42
Batch: 20; loss: 1.58; acc: 0.52
Batch: 40; loss: 1.34; acc: 0.7
Batch: 60; loss: 1.4; acc: 0.62
Batch: 80; loss: 1.42; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.66
Batch: 120; loss: 1.62; acc: 0.53
Batch: 140; loss: 1.35; acc: 0.7
Val Epoch over. val_loss: 1.5457081506206731; val_accuracy: 0.5632961783439491 

The current subspace-distance is: 1.213203722727485e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.5; acc: 0.56
Batch: 20; loss: 1.66; acc: 0.5
Batch: 40; loss: 1.63; acc: 0.52
Batch: 60; loss: 1.64; acc: 0.47
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.72; acc: 0.55
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.49; acc: 0.56
Batch: 160; loss: 1.68; acc: 0.48
Batch: 180; loss: 1.63; acc: 0.42
Batch: 200; loss: 1.6; acc: 0.53
Batch: 220; loss: 1.66; acc: 0.47
Batch: 240; loss: 1.53; acc: 0.59
Batch: 260; loss: 1.64; acc: 0.52
Batch: 280; loss: 1.59; acc: 0.55
Batch: 300; loss: 1.69; acc: 0.48
Batch: 320; loss: 1.6; acc: 0.55
Batch: 340; loss: 1.63; acc: 0.53
Batch: 360; loss: 1.64; acc: 0.47
Batch: 380; loss: 1.65; acc: 0.47
Batch: 400; loss: 1.54; acc: 0.52
Batch: 420; loss: 1.57; acc: 0.55
Batch: 440; loss: 1.5; acc: 0.59
Batch: 460; loss: 1.63; acc: 0.52
Batch: 480; loss: 1.61; acc: 0.53
Batch: 500; loss: 1.69; acc: 0.44
Batch: 520; loss: 1.5; acc: 0.64
Batch: 540; loss: 1.53; acc: 0.55
Batch: 560; loss: 1.53; acc: 0.55
Batch: 580; loss: 1.68; acc: 0.47
Batch: 600; loss: 1.38; acc: 0.61
Batch: 620; loss: 1.54; acc: 0.55
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.62; acc: 0.45
Batch: 680; loss: 1.37; acc: 0.66
Batch: 700; loss: 1.6; acc: 0.52
Batch: 720; loss: 1.61; acc: 0.47
Batch: 740; loss: 1.45; acc: 0.59
Batch: 760; loss: 1.61; acc: 0.48
Batch: 780; loss: 1.73; acc: 0.48
Train Epoch over. train_loss: 1.59; train_accuracy: 0.52 

4.154988710070029e-05
1.4661625755252317e-05
Batch: 0; loss: 1.75; acc: 0.44
Batch: 20; loss: 1.57; acc: 0.48
Batch: 40; loss: 1.33; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.66
Batch: 80; loss: 1.41; acc: 0.59
Batch: 100; loss: 1.47; acc: 0.66
Batch: 120; loss: 1.6; acc: 0.58
Batch: 140; loss: 1.34; acc: 0.67
Val Epoch over. val_loss: 1.5318457989176368; val_accuracy: 0.5704617834394905 

The current subspace-distance is: 1.4661625755252317e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.55
Batch: 20; loss: 1.6; acc: 0.53
Batch: 40; loss: 1.57; acc: 0.55
Batch: 60; loss: 1.74; acc: 0.42
Batch: 80; loss: 1.73; acc: 0.42
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.68; acc: 0.45
Batch: 160; loss: 1.67; acc: 0.44
Batch: 180; loss: 1.64; acc: 0.52
Batch: 200; loss: 1.69; acc: 0.45
Batch: 220; loss: 1.54; acc: 0.61
Batch: 240; loss: 1.66; acc: 0.47
Batch: 260; loss: 1.66; acc: 0.58
Batch: 280; loss: 1.75; acc: 0.47
Batch: 300; loss: 1.67; acc: 0.53
Batch: 320; loss: 1.64; acc: 0.5
Batch: 340; loss: 1.49; acc: 0.55
Batch: 360; loss: 1.79; acc: 0.39
Batch: 380; loss: 1.43; acc: 0.64
Batch: 400; loss: 1.52; acc: 0.61
Batch: 420; loss: 1.74; acc: 0.45
Batch: 440; loss: 1.61; acc: 0.5
Batch: 460; loss: 1.71; acc: 0.47
Batch: 480; loss: 1.57; acc: 0.53
Batch: 500; loss: 1.68; acc: 0.48
Batch: 520; loss: 1.68; acc: 0.47
Batch: 540; loss: 1.72; acc: 0.48
Batch: 560; loss: 1.65; acc: 0.47
Batch: 580; loss: 1.68; acc: 0.52
Batch: 600; loss: 1.56; acc: 0.56
Batch: 620; loss: 1.5; acc: 0.52
Batch: 640; loss: 1.75; acc: 0.39
Batch: 660; loss: 1.53; acc: 0.56
Batch: 680; loss: 1.42; acc: 0.55
Batch: 700; loss: 1.64; acc: 0.48
Batch: 720; loss: 1.44; acc: 0.61
Batch: 740; loss: 1.52; acc: 0.5
Batch: 760; loss: 1.58; acc: 0.53
Batch: 780; loss: 1.49; acc: 0.55
Train Epoch over. train_loss: 1.59; train_accuracy: 0.53 

4.094343967153691e-05
1.3346313608053606e-05
Batch: 0; loss: 1.74; acc: 0.44
Batch: 20; loss: 1.56; acc: 0.5
Batch: 40; loss: 1.33; acc: 0.69
Batch: 60; loss: 1.38; acc: 0.66
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.61; acc: 0.55
Batch: 140; loss: 1.35; acc: 0.67
Val Epoch over. val_loss: 1.530183669108494; val_accuracy: 0.5693670382165605 

The current subspace-distance is: 1.3346313608053606e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.7; acc: 0.41
Batch: 20; loss: 1.56; acc: 0.55
Batch: 40; loss: 1.59; acc: 0.55
Batch: 60; loss: 1.42; acc: 0.62
Batch: 80; loss: 1.67; acc: 0.42
Batch: 100; loss: 1.62; acc: 0.5
Batch: 120; loss: 1.79; acc: 0.41
Batch: 140; loss: 1.6; acc: 0.5
Batch: 160; loss: 1.47; acc: 0.61
Batch: 180; loss: 1.63; acc: 0.5
Batch: 200; loss: 1.74; acc: 0.41
Batch: 220; loss: 1.63; acc: 0.53
Batch: 240; loss: 1.51; acc: 0.64
Batch: 260; loss: 1.61; acc: 0.52
Batch: 280; loss: 1.69; acc: 0.44
Batch: 300; loss: 1.52; acc: 0.48
Batch: 320; loss: 1.54; acc: 0.58
Batch: 340; loss: 1.52; acc: 0.5
Batch: 360; loss: 1.57; acc: 0.52
Batch: 380; loss: 1.48; acc: 0.58
Batch: 400; loss: 1.49; acc: 0.59
Batch: 420; loss: 1.71; acc: 0.41
Batch: 440; loss: 1.72; acc: 0.52
Batch: 460; loss: 1.51; acc: 0.52
Batch: 480; loss: 1.46; acc: 0.64
Batch: 500; loss: 1.55; acc: 0.5
Batch: 520; loss: 1.66; acc: 0.5
Batch: 540; loss: 1.41; acc: 0.61
Batch: 560; loss: 1.65; acc: 0.42
Batch: 580; loss: 1.58; acc: 0.53
Batch: 600; loss: 1.56; acc: 0.58
Batch: 620; loss: 1.64; acc: 0.55
Batch: 640; loss: 1.59; acc: 0.61
Batch: 660; loss: 1.54; acc: 0.58
Batch: 680; loss: 1.55; acc: 0.61
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.56; acc: 0.53
Batch: 740; loss: 1.6; acc: 0.52
Batch: 760; loss: 1.58; acc: 0.55
Batch: 780; loss: 1.51; acc: 0.55
Train Epoch over. train_loss: 1.58; train_accuracy: 0.53 

4.1043702367460355e-05
1.3966078768135048e-05
Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 1.56; acc: 0.5
Batch: 40; loss: 1.33; acc: 0.72
Batch: 60; loss: 1.38; acc: 0.66
Batch: 80; loss: 1.39; acc: 0.56
Batch: 100; loss: 1.47; acc: 0.61
Batch: 120; loss: 1.61; acc: 0.58
Batch: 140; loss: 1.33; acc: 0.69
Val Epoch over. val_loss: 1.5212720427543494; val_accuracy: 0.5711584394904459 

The current subspace-distance is: 1.3966078768135048e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.66; acc: 0.47
Batch: 20; loss: 1.56; acc: 0.53
Batch: 40; loss: 1.46; acc: 0.64
Batch: 60; loss: 1.67; acc: 0.48
Batch: 80; loss: 1.59; acc: 0.55
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.55; acc: 0.45
Batch: 140; loss: 1.63; acc: 0.5
Batch: 160; loss: 1.51; acc: 0.56
Batch: 180; loss: 1.51; acc: 0.53
Batch: 200; loss: 1.59; acc: 0.48
Batch: 220; loss: 1.69; acc: 0.45
Batch: 240; loss: 1.58; acc: 0.5
Batch: 260; loss: 1.55; acc: 0.5
Batch: 280; loss: 1.48; acc: 0.64
Batch: 300; loss: 1.37; acc: 0.67
Batch: 320; loss: 1.62; acc: 0.55
Batch: 340; loss: 1.56; acc: 0.53
Batch: 360; loss: 1.46; acc: 0.62
Batch: 380; loss: 1.65; acc: 0.44
Batch: 400; loss: 1.56; acc: 0.52
Batch: 420; loss: 1.67; acc: 0.5
Batch: 440; loss: 1.66; acc: 0.55
Batch: 460; loss: 1.63; acc: 0.48
Batch: 480; loss: 1.53; acc: 0.62
Batch: 500; loss: 1.62; acc: 0.44
Batch: 520; loss: 1.6; acc: 0.5
Batch: 540; loss: 1.54; acc: 0.48
Batch: 560; loss: 1.53; acc: 0.55
Batch: 580; loss: 1.63; acc: 0.5
Batch: 600; loss: 1.47; acc: 0.62
Batch: 620; loss: 1.54; acc: 0.58
Batch: 640; loss: 1.47; acc: 0.59
Batch: 660; loss: 1.47; acc: 0.56
Batch: 680; loss: 1.58; acc: 0.55
Batch: 700; loss: 1.54; acc: 0.58
Batch: 720; loss: 1.65; acc: 0.5
Batch: 740; loss: 1.65; acc: 0.47
Batch: 760; loss: 1.45; acc: 0.66
Batch: 780; loss: 1.67; acc: 0.55
Train Epoch over. train_loss: 1.58; train_accuracy: 0.53 

4.291473305784166e-05
1.317723399552051e-05
Batch: 0; loss: 1.75; acc: 0.39
Batch: 20; loss: 1.55; acc: 0.5
Batch: 40; loss: 1.34; acc: 0.7
Batch: 60; loss: 1.39; acc: 0.67
Batch: 80; loss: 1.41; acc: 0.56
Batch: 100; loss: 1.46; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.34; acc: 0.67
Val Epoch over. val_loss: 1.524083882380443; val_accuracy: 0.5657842356687898 

The current subspace-distance is: 1.317723399552051e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.63; acc: 0.52
Batch: 20; loss: 1.54; acc: 0.55
Batch: 40; loss: 1.51; acc: 0.58
Batch: 60; loss: 1.65; acc: 0.48
Batch: 80; loss: 1.63; acc: 0.53
Batch: 100; loss: 1.5; acc: 0.53
Batch: 120; loss: 1.6; acc: 0.44
Batch: 140; loss: 1.46; acc: 0.56
Batch: 160; loss: 1.6; acc: 0.55
Batch: 180; loss: 1.64; acc: 0.5
Batch: 200; loss: 1.76; acc: 0.39
Batch: 220; loss: 1.5; acc: 0.58
Batch: 240; loss: 1.55; acc: 0.59
Batch: 260; loss: 1.54; acc: 0.53
Batch: 280; loss: 1.53; acc: 0.45
Batch: 300; loss: 1.52; acc: 0.45
Batch: 320; loss: 1.56; acc: 0.53
Batch: 340; loss: 1.68; acc: 0.45
Batch: 360; loss: 1.63; acc: 0.52
Batch: 380; loss: 1.5; acc: 0.61
Batch: 400; loss: 1.45; acc: 0.55
Batch: 420; loss: 1.67; acc: 0.48
Batch: 440; loss: 1.46; acc: 0.64
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.51; acc: 0.61
Batch: 500; loss: 1.53; acc: 0.5
Batch: 520; loss: 1.64; acc: 0.5
Batch: 540; loss: 1.44; acc: 0.61
Batch: 560; loss: 1.54; acc: 0.48
Batch: 580; loss: 1.56; acc: 0.56
Batch: 600; loss: 1.58; acc: 0.55
Batch: 620; loss: 1.6; acc: 0.55
Batch: 640; loss: 1.53; acc: 0.59
Batch: 660; loss: 1.7; acc: 0.42
Batch: 680; loss: 1.57; acc: 0.48
Batch: 700; loss: 1.41; acc: 0.62
Batch: 720; loss: 1.58; acc: 0.58
Batch: 740; loss: 1.51; acc: 0.53
Batch: 760; loss: 1.48; acc: 0.58
Batch: 780; loss: 1.49; acc: 0.61
Train Epoch over. train_loss: 1.57; train_accuracy: 0.53 

4.297457780921832e-05
1.6484278603456914e-05
Batch: 0; loss: 1.74; acc: 0.38
Batch: 20; loss: 1.55; acc: 0.48
Batch: 40; loss: 1.34; acc: 0.7
Batch: 60; loss: 1.37; acc: 0.67
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 1.45; acc: 0.62
Batch: 120; loss: 1.59; acc: 0.62
Batch: 140; loss: 1.33; acc: 0.7
Val Epoch over. val_loss: 1.513759665428453; val_accuracy: 0.568968949044586 

The current subspace-distance is: 1.6484278603456914e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.61; acc: 0.56
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.55; acc: 0.59
Batch: 60; loss: 1.67; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.42
Batch: 100; loss: 1.6; acc: 0.56
Batch: 120; loss: 1.78; acc: 0.38
Batch: 140; loss: 1.51; acc: 0.58
Batch: 160; loss: 1.58; acc: 0.45
Batch: 180; loss: 1.61; acc: 0.42
Batch: 200; loss: 1.63; acc: 0.55
Batch: 220; loss: 1.52; acc: 0.55
Batch: 240; loss: 1.66; acc: 0.47
Batch: 260; loss: 1.63; acc: 0.52
Batch: 280; loss: 1.59; acc: 0.48
Batch: 300; loss: 1.62; acc: 0.45
Batch: 320; loss: 1.43; acc: 0.58
Batch: 340; loss: 1.45; acc: 0.59
Batch: 360; loss: 1.6; acc: 0.53
Batch: 380; loss: 1.49; acc: 0.59
Batch: 400; loss: 1.62; acc: 0.5
Batch: 420; loss: 1.75; acc: 0.45
Batch: 440; loss: 1.69; acc: 0.47
Batch: 460; loss: 1.81; acc: 0.36
Batch: 480; loss: 1.69; acc: 0.41
Batch: 500; loss: 1.51; acc: 0.61
Batch: 520; loss: 1.56; acc: 0.55
Batch: 540; loss: 1.55; acc: 0.58
Batch: 560; loss: 1.54; acc: 0.58
Batch: 580; loss: 1.51; acc: 0.48
Batch: 600; loss: 1.52; acc: 0.59
Batch: 620; loss: 1.53; acc: 0.56
Batch: 640; loss: 1.51; acc: 0.58
Batch: 660; loss: 1.67; acc: 0.47
Batch: 680; loss: 1.44; acc: 0.61
Batch: 700; loss: 1.46; acc: 0.56
Batch: 720; loss: 1.67; acc: 0.41
Batch: 740; loss: 1.56; acc: 0.5
Batch: 760; loss: 1.61; acc: 0.53
Batch: 780; loss: 1.7; acc: 0.52
Train Epoch over. train_loss: 1.56; train_accuracy: 0.53 

4.245603486197069e-05
1.5374605936813168e-05
Batch: 0; loss: 1.74; acc: 0.41
Batch: 20; loss: 1.53; acc: 0.48
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.39; acc: 0.69
Batch: 80; loss: 1.39; acc: 0.56
Batch: 100; loss: 1.46; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.5076682673897712; val_accuracy: 0.5721536624203821 

The current subspace-distance is: 1.5374605936813168e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.46; acc: 0.58
Batch: 20; loss: 1.58; acc: 0.55
Batch: 40; loss: 1.62; acc: 0.48
Batch: 60; loss: 1.63; acc: 0.47
Batch: 80; loss: 1.55; acc: 0.5
Batch: 100; loss: 1.53; acc: 0.52
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.57; acc: 0.52
Batch: 160; loss: 1.67; acc: 0.5
Batch: 180; loss: 1.49; acc: 0.61
Batch: 200; loss: 1.6; acc: 0.48
Batch: 220; loss: 1.58; acc: 0.5
Batch: 240; loss: 1.46; acc: 0.55
Batch: 260; loss: 1.64; acc: 0.47
Batch: 280; loss: 1.52; acc: 0.44
Batch: 300; loss: 1.8; acc: 0.41
Batch: 320; loss: 1.68; acc: 0.48
Batch: 340; loss: 1.58; acc: 0.53
Batch: 360; loss: 1.53; acc: 0.52
Batch: 380; loss: 1.46; acc: 0.61
Batch: 400; loss: 1.53; acc: 0.59
Batch: 420; loss: 1.56; acc: 0.52
Batch: 440; loss: 1.49; acc: 0.58
Batch: 460; loss: 1.71; acc: 0.41
Batch: 480; loss: 1.51; acc: 0.5
Batch: 500; loss: 1.47; acc: 0.62
Batch: 520; loss: 1.54; acc: 0.55
Batch: 540; loss: 1.43; acc: 0.66
Batch: 560; loss: 1.68; acc: 0.48
Batch: 580; loss: 1.52; acc: 0.61
Batch: 600; loss: 1.5; acc: 0.5
Batch: 620; loss: 1.62; acc: 0.48
Batch: 640; loss: 1.53; acc: 0.56
Batch: 660; loss: 1.61; acc: 0.52
Batch: 680; loss: 1.49; acc: 0.58
Batch: 700; loss: 1.45; acc: 0.61
Batch: 720; loss: 1.63; acc: 0.56
Batch: 740; loss: 1.59; acc: 0.56
Batch: 760; loss: 1.46; acc: 0.61
Batch: 780; loss: 1.56; acc: 0.56
Train Epoch over. train_loss: 1.56; train_accuracy: 0.54 

4.302158049540594e-05
1.6518386473762803e-05
Batch: 0; loss: 1.73; acc: 0.39
Batch: 20; loss: 1.52; acc: 0.5
Batch: 40; loss: 1.33; acc: 0.61
Batch: 60; loss: 1.39; acc: 0.67
Batch: 80; loss: 1.4; acc: 0.58
Batch: 100; loss: 1.44; acc: 0.61
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.32; acc: 0.69
Val Epoch over. val_loss: 1.5023782367159606; val_accuracy: 0.570859872611465 

The current subspace-distance is: 1.6518386473762803e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.47; acc: 0.66
Batch: 20; loss: 1.62; acc: 0.5
Batch: 40; loss: 1.54; acc: 0.52
Batch: 60; loss: 1.41; acc: 0.64
Batch: 80; loss: 1.56; acc: 0.47
Batch: 100; loss: 1.64; acc: 0.47
Batch: 120; loss: 1.48; acc: 0.53
Batch: 140; loss: 1.5; acc: 0.59
Batch: 160; loss: 1.46; acc: 0.58
Batch: 180; loss: 1.48; acc: 0.59
Batch: 200; loss: 1.5; acc: 0.61
Batch: 220; loss: 1.7; acc: 0.45
Batch: 240; loss: 1.52; acc: 0.56
Batch: 260; loss: 1.37; acc: 0.64
Batch: 280; loss: 1.54; acc: 0.55
Batch: 300; loss: 1.57; acc: 0.55
Batch: 320; loss: 1.51; acc: 0.52
Batch: 340; loss: 1.51; acc: 0.56
Batch: 360; loss: 1.51; acc: 0.47
Batch: 380; loss: 1.5; acc: 0.58
Batch: 400; loss: 1.48; acc: 0.53
Batch: 420; loss: 1.4; acc: 0.67
Batch: 440; loss: 1.7; acc: 0.44
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.56; acc: 0.47
Batch: 500; loss: 1.49; acc: 0.61
Batch: 520; loss: 1.52; acc: 0.55
Batch: 540; loss: 1.51; acc: 0.56
Batch: 560; loss: 1.62; acc: 0.52
Batch: 580; loss: 1.57; acc: 0.56
Batch: 600; loss: 1.58; acc: 0.55
Batch: 620; loss: 1.58; acc: 0.47
Batch: 640; loss: 1.52; acc: 0.61
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.65; acc: 0.48
Batch: 700; loss: 1.49; acc: 0.53
Batch: 720; loss: 1.4; acc: 0.62
Batch: 740; loss: 1.69; acc: 0.44
Batch: 760; loss: 1.55; acc: 0.5
Batch: 780; loss: 1.5; acc: 0.55
Train Epoch over. train_loss: 1.56; train_accuracy: 0.54 

4.531349986791611e-05
1.8395367078483105e-05
Batch: 0; loss: 1.72; acc: 0.42
Batch: 20; loss: 1.53; acc: 0.48
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.39; acc: 0.67
Batch: 80; loss: 1.39; acc: 0.59
Batch: 100; loss: 1.44; acc: 0.62
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.5010300358389592; val_accuracy: 0.5732484076433121 

The current subspace-distance is: 1.8395367078483105e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.42
Batch: 20; loss: 1.56; acc: 0.44
Batch: 40; loss: 1.67; acc: 0.56
Batch: 60; loss: 1.57; acc: 0.5
Batch: 80; loss: 1.59; acc: 0.52
Batch: 100; loss: 1.5; acc: 0.59
Batch: 120; loss: 1.52; acc: 0.56
Batch: 140; loss: 1.48; acc: 0.64
Batch: 160; loss: 1.68; acc: 0.44
Batch: 180; loss: 1.69; acc: 0.47
Batch: 200; loss: 1.38; acc: 0.69
Batch: 220; loss: 1.49; acc: 0.55
Batch: 240; loss: 1.59; acc: 0.56
Batch: 260; loss: 1.66; acc: 0.47
Batch: 280; loss: 1.39; acc: 0.67
Batch: 300; loss: 1.61; acc: 0.47
Batch: 320; loss: 1.61; acc: 0.52
Batch: 340; loss: 1.68; acc: 0.45
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.62; acc: 0.41
Batch: 400; loss: 1.72; acc: 0.44
Batch: 420; loss: 1.61; acc: 0.5
Batch: 440; loss: 1.7; acc: 0.41
Batch: 460; loss: 1.6; acc: 0.45
Batch: 480; loss: 1.57; acc: 0.59
Batch: 500; loss: 1.52; acc: 0.58
Batch: 520; loss: 1.45; acc: 0.64
Batch: 540; loss: 1.61; acc: 0.44
Batch: 560; loss: 1.51; acc: 0.48
Batch: 580; loss: 1.57; acc: 0.55
Batch: 600; loss: 1.6; acc: 0.42
Batch: 620; loss: 1.58; acc: 0.48
Batch: 640; loss: 1.61; acc: 0.5
Batch: 660; loss: 1.73; acc: 0.45
Batch: 680; loss: 1.52; acc: 0.55
Batch: 700; loss: 1.57; acc: 0.48
Batch: 720; loss: 1.57; acc: 0.44
Batch: 740; loss: 1.64; acc: 0.47
Batch: 760; loss: 1.59; acc: 0.5
Batch: 780; loss: 1.7; acc: 0.41
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.479540075408295e-05
1.738467653922271e-05
Batch: 0; loss: 1.73; acc: 0.42
Batch: 20; loss: 1.52; acc: 0.48
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.39; acc: 0.53
Batch: 100; loss: 1.45; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.7
Val Epoch over. val_loss: 1.5022784873938104; val_accuracy: 0.5696656050955414 

The current subspace-distance is: 1.738467653922271e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.64
Batch: 20; loss: 1.57; acc: 0.5
Batch: 40; loss: 1.43; acc: 0.56
Batch: 60; loss: 1.63; acc: 0.44
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.49; acc: 0.56
Batch: 120; loss: 1.64; acc: 0.47
Batch: 140; loss: 1.49; acc: 0.55
Batch: 160; loss: 1.49; acc: 0.62
Batch: 180; loss: 1.51; acc: 0.53
Batch: 200; loss: 1.64; acc: 0.53
Batch: 220; loss: 1.52; acc: 0.58
Batch: 240; loss: 1.68; acc: 0.44
Batch: 260; loss: 1.51; acc: 0.58
Batch: 280; loss: 1.59; acc: 0.48
Batch: 300; loss: 1.71; acc: 0.45
Batch: 320; loss: 1.46; acc: 0.62
Batch: 340; loss: 1.5; acc: 0.58
Batch: 360; loss: 1.51; acc: 0.59
Batch: 380; loss: 1.65; acc: 0.39
Batch: 400; loss: 1.58; acc: 0.55
Batch: 420; loss: 1.62; acc: 0.47
Batch: 440; loss: 1.66; acc: 0.53
Batch: 460; loss: 1.59; acc: 0.52
Batch: 480; loss: 1.48; acc: 0.58
Batch: 500; loss: 1.55; acc: 0.56
Batch: 520; loss: 1.58; acc: 0.56
Batch: 540; loss: 1.56; acc: 0.47
Batch: 560; loss: 1.57; acc: 0.56
Batch: 580; loss: 1.59; acc: 0.52
Batch: 600; loss: 1.52; acc: 0.53
Batch: 620; loss: 1.55; acc: 0.52
Batch: 640; loss: 1.45; acc: 0.59
Batch: 660; loss: 1.55; acc: 0.47
Batch: 680; loss: 1.56; acc: 0.53
Batch: 700; loss: 1.61; acc: 0.48
Batch: 720; loss: 1.47; acc: 0.58
Batch: 740; loss: 1.43; acc: 0.58
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.7; acc: 0.45
Train Epoch over. train_loss: 1.55; train_accuracy: 0.53 

4.3521613406483084e-05
1.5355699360952713e-05
Batch: 0; loss: 1.72; acc: 0.44
Batch: 20; loss: 1.52; acc: 0.52
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.4; acc: 0.52
Batch: 100; loss: 1.43; acc: 0.59
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.32; acc: 0.69
Val Epoch over. val_loss: 1.499689230493679; val_accuracy: 0.564390923566879 

The current subspace-distance is: 1.5355699360952713e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.64; acc: 0.52
Batch: 20; loss: 1.4; acc: 0.64
Batch: 40; loss: 1.77; acc: 0.39
Batch: 60; loss: 1.45; acc: 0.64
Batch: 80; loss: 1.51; acc: 0.58
Batch: 100; loss: 1.52; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.69
Batch: 140; loss: 1.64; acc: 0.44
Batch: 160; loss: 1.51; acc: 0.47
Batch: 180; loss: 1.66; acc: 0.47
Batch: 200; loss: 1.56; acc: 0.55
Batch: 220; loss: 1.43; acc: 0.58
Batch: 240; loss: 1.64; acc: 0.53
Batch: 260; loss: 1.51; acc: 0.53
Batch: 280; loss: 1.6; acc: 0.48
Batch: 300; loss: 1.49; acc: 0.58
Batch: 320; loss: 1.48; acc: 0.56
Batch: 340; loss: 1.52; acc: 0.56
Batch: 360; loss: 1.57; acc: 0.53
Batch: 380; loss: 1.36; acc: 0.67
Batch: 400; loss: 1.44; acc: 0.64
Batch: 420; loss: 1.41; acc: 0.62
Batch: 440; loss: 1.45; acc: 0.58
Batch: 460; loss: 1.67; acc: 0.58
Batch: 480; loss: 1.51; acc: 0.59
Batch: 500; loss: 1.46; acc: 0.64
Batch: 520; loss: 1.7; acc: 0.53
Batch: 540; loss: 1.63; acc: 0.45
Batch: 560; loss: 1.54; acc: 0.52
Batch: 580; loss: 1.64; acc: 0.45
Batch: 600; loss: 1.57; acc: 0.56
Batch: 620; loss: 1.43; acc: 0.66
Batch: 640; loss: 1.54; acc: 0.47
Batch: 660; loss: 1.61; acc: 0.45
Batch: 680; loss: 1.55; acc: 0.53
Batch: 700; loss: 1.66; acc: 0.45
Batch: 720; loss: 1.51; acc: 0.56
Batch: 740; loss: 1.43; acc: 0.66
Batch: 760; loss: 1.53; acc: 0.59
Batch: 780; loss: 1.71; acc: 0.48
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.326381895225495e-05
1.1432215615059249e-05
Batch: 0; loss: 1.71; acc: 0.42
Batch: 20; loss: 1.51; acc: 0.5
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.38; acc: 0.67
Batch: 80; loss: 1.38; acc: 0.58
Batch: 100; loss: 1.45; acc: 0.61
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.31; acc: 0.72
Val Epoch over. val_loss: 1.493300265567318; val_accuracy: 0.5778264331210191 

The current subspace-distance is: 1.1432215615059249e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.45; acc: 0.55
Batch: 20; loss: 1.66; acc: 0.47
Batch: 40; loss: 1.7; acc: 0.42
Batch: 60; loss: 1.54; acc: 0.53
Batch: 80; loss: 1.43; acc: 0.59
Batch: 100; loss: 1.54; acc: 0.56
Batch: 120; loss: 1.44; acc: 0.62
Batch: 140; loss: 1.5; acc: 0.61
Batch: 160; loss: 1.47; acc: 0.56
Batch: 180; loss: 1.5; acc: 0.59
Batch: 200; loss: 1.48; acc: 0.56
Batch: 220; loss: 1.58; acc: 0.55
Batch: 240; loss: 1.66; acc: 0.52
Batch: 260; loss: 1.56; acc: 0.45
Batch: 280; loss: 1.57; acc: 0.59
Batch: 300; loss: 1.3; acc: 0.72
Batch: 320; loss: 1.58; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.64
Batch: 360; loss: 1.51; acc: 0.55
Batch: 380; loss: 1.49; acc: 0.59
Batch: 400; loss: 1.65; acc: 0.47
Batch: 420; loss: 1.68; acc: 0.44
Batch: 440; loss: 1.55; acc: 0.56
Batch: 460; loss: 1.44; acc: 0.56
Batch: 480; loss: 1.48; acc: 0.53
Batch: 500; loss: 1.59; acc: 0.44
Batch: 520; loss: 1.62; acc: 0.45
Batch: 540; loss: 1.64; acc: 0.48
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.5; acc: 0.53
Batch: 600; loss: 1.51; acc: 0.52
Batch: 620; loss: 1.64; acc: 0.45
Batch: 640; loss: 1.6; acc: 0.56
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.63; acc: 0.5
Batch: 700; loss: 1.53; acc: 0.59
Batch: 720; loss: 1.58; acc: 0.48
Batch: 740; loss: 1.61; acc: 0.45
Batch: 760; loss: 1.59; acc: 0.56
Batch: 780; loss: 1.62; acc: 0.45
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.443434227141552e-05
1.6306803445331752e-05
Batch: 0; loss: 1.72; acc: 0.41
Batch: 20; loss: 1.52; acc: 0.5
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 1.39; acc: 0.67
Batch: 80; loss: 1.39; acc: 0.56
Batch: 100; loss: 1.45; acc: 0.62
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.31; acc: 0.7
Val Epoch over. val_loss: 1.4970826935616268; val_accuracy: 0.5795183121019108 

The current subspace-distance is: 1.6306803445331752e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.5; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.53
Batch: 40; loss: 1.65; acc: 0.45
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.57; acc: 0.53
Batch: 100; loss: 1.52; acc: 0.61
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.57; acc: 0.47
Batch: 160; loss: 1.56; acc: 0.56
Batch: 180; loss: 1.56; acc: 0.58
Batch: 200; loss: 1.75; acc: 0.34
Batch: 220; loss: 1.46; acc: 0.58
Batch: 240; loss: 1.61; acc: 0.52
Batch: 260; loss: 1.59; acc: 0.5
Batch: 280; loss: 1.57; acc: 0.45
Batch: 300; loss: 1.6; acc: 0.5
Batch: 320; loss: 1.67; acc: 0.5
Batch: 340; loss: 1.68; acc: 0.41
Batch: 360; loss: 1.42; acc: 0.61
Batch: 380; loss: 1.4; acc: 0.62
Batch: 400; loss: 1.4; acc: 0.7
Batch: 420; loss: 1.49; acc: 0.58
Batch: 440; loss: 1.61; acc: 0.42
Batch: 460; loss: 1.58; acc: 0.5
Batch: 480; loss: 1.5; acc: 0.55
Batch: 500; loss: 1.65; acc: 0.48
Batch: 520; loss: 1.67; acc: 0.45
Batch: 540; loss: 1.65; acc: 0.47
Batch: 560; loss: 1.8; acc: 0.38
Batch: 580; loss: 1.63; acc: 0.45
Batch: 600; loss: 1.56; acc: 0.45
Batch: 620; loss: 1.42; acc: 0.61
Batch: 640; loss: 1.52; acc: 0.58
Batch: 660; loss: 1.51; acc: 0.55
Batch: 680; loss: 1.57; acc: 0.5
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.62; acc: 0.41
Batch: 760; loss: 1.53; acc: 0.53
Batch: 780; loss: 1.47; acc: 0.58
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.447844185051508e-05
1.600735959073063e-05
Batch: 0; loss: 1.72; acc: 0.44
Batch: 20; loss: 1.52; acc: 0.52
Batch: 40; loss: 1.34; acc: 0.61
Batch: 60; loss: 1.4; acc: 0.64
Batch: 80; loss: 1.39; acc: 0.52
Batch: 100; loss: 1.44; acc: 0.62
Batch: 120; loss: 1.58; acc: 0.58
Batch: 140; loss: 1.31; acc: 0.69
Val Epoch over. val_loss: 1.495336749751097; val_accuracy: 0.5750398089171974 

The current subspace-distance is: 1.600735959073063e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.48
Batch: 20; loss: 1.66; acc: 0.47
Batch: 40; loss: 1.54; acc: 0.55
Batch: 60; loss: 1.73; acc: 0.36
Batch: 80; loss: 1.66; acc: 0.41
Batch: 100; loss: 1.55; acc: 0.59
Batch: 120; loss: 1.54; acc: 0.56
Batch: 140; loss: 1.52; acc: 0.64
Batch: 160; loss: 1.52; acc: 0.61
Batch: 180; loss: 1.55; acc: 0.62
Batch: 200; loss: 1.48; acc: 0.56
Batch: 220; loss: 1.61; acc: 0.52
Batch: 240; loss: 1.57; acc: 0.52
Batch: 260; loss: 1.5; acc: 0.58
Batch: 280; loss: 1.48; acc: 0.64
Batch: 300; loss: 1.6; acc: 0.45
Batch: 320; loss: 1.55; acc: 0.56
Batch: 340; loss: 1.48; acc: 0.56
Batch: 360; loss: 1.67; acc: 0.47
Batch: 380; loss: 1.56; acc: 0.55
Batch: 400; loss: 1.4; acc: 0.69
Batch: 420; loss: 1.56; acc: 0.55
Batch: 440; loss: 1.58; acc: 0.5
Batch: 460; loss: 1.51; acc: 0.61
Batch: 480; loss: 1.5; acc: 0.56
Batch: 500; loss: 1.56; acc: 0.48
Batch: 520; loss: 1.59; acc: 0.45
Batch: 540; loss: 1.51; acc: 0.64
Batch: 560; loss: 1.6; acc: 0.53
Batch: 580; loss: 1.54; acc: 0.56
Batch: 600; loss: 1.5; acc: 0.61
Batch: 620; loss: 1.6; acc: 0.59
Batch: 640; loss: 1.47; acc: 0.62
Batch: 660; loss: 1.39; acc: 0.61
Batch: 680; loss: 1.63; acc: 0.44
Batch: 700; loss: 1.69; acc: 0.42
Batch: 720; loss: 1.63; acc: 0.48
Batch: 740; loss: 1.46; acc: 0.61
Batch: 760; loss: 1.61; acc: 0.52
Batch: 780; loss: 1.51; acc: 0.56
Train Epoch over. train_loss: 1.55; train_accuracy: 0.54 

4.541264206636697e-05
1.602549309609458e-05
Batch: 0; loss: 1.71; acc: 0.42
Batch: 20; loss: 1.5; acc: 0.5
Batch: 40; loss: 1.33; acc: 0.61
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.4; acc: 0.53
Batch: 100; loss: 1.43; acc: 0.66
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.31; acc: 0.7
Val Epoch over. val_loss: 1.491656143194551; val_accuracy: 0.5722531847133758 

The current subspace-distance is: 1.602549309609458e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.59
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.46; acc: 0.55
Batch: 60; loss: 1.65; acc: 0.48
Batch: 80; loss: 1.53; acc: 0.61
Batch: 100; loss: 1.56; acc: 0.48
Batch: 120; loss: 1.65; acc: 0.5
Batch: 140; loss: 1.62; acc: 0.45
Batch: 160; loss: 1.6; acc: 0.52
Batch: 180; loss: 1.5; acc: 0.56
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 1.66; acc: 0.47
Batch: 240; loss: 1.39; acc: 0.66
Batch: 260; loss: 1.77; acc: 0.42
Batch: 280; loss: 1.64; acc: 0.52
Batch: 300; loss: 1.5; acc: 0.47
Batch: 320; loss: 1.52; acc: 0.52
Batch: 340; loss: 1.68; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.65; acc: 0.41
Batch: 400; loss: 1.44; acc: 0.67
Batch: 420; loss: 1.52; acc: 0.58
Batch: 440; loss: 1.56; acc: 0.52
Batch: 460; loss: 1.43; acc: 0.64
Batch: 480; loss: 1.69; acc: 0.47
Batch: 500; loss: 1.43; acc: 0.58
Batch: 520; loss: 1.5; acc: 0.59
Batch: 540; loss: 1.57; acc: 0.53
Batch: 560; loss: 1.34; acc: 0.66
Batch: 580; loss: 1.53; acc: 0.52
Batch: 600; loss: 1.41; acc: 0.56
Batch: 620; loss: 1.53; acc: 0.61
Batch: 640; loss: 1.51; acc: 0.56
Batch: 660; loss: 1.5; acc: 0.5
Batch: 680; loss: 1.47; acc: 0.56
Batch: 700; loss: 1.56; acc: 0.53
Batch: 720; loss: 1.61; acc: 0.55
Batch: 740; loss: 1.6; acc: 0.5
Batch: 760; loss: 1.66; acc: 0.44
Batch: 780; loss: 1.68; acc: 0.42
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.603872730513103e-05
1.82287967618322e-05
Batch: 0; loss: 1.71; acc: 0.42
Batch: 20; loss: 1.51; acc: 0.52
Batch: 40; loss: 1.34; acc: 0.61
Batch: 60; loss: 1.39; acc: 0.64
Batch: 80; loss: 1.4; acc: 0.53
Batch: 100; loss: 1.43; acc: 0.64
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.31; acc: 0.67
Val Epoch over. val_loss: 1.4965958413045117; val_accuracy: 0.5692675159235668 

The current subspace-distance is: 1.82287967618322e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.54; acc: 0.53
Batch: 20; loss: 1.52; acc: 0.55
Batch: 40; loss: 1.71; acc: 0.45
Batch: 60; loss: 1.45; acc: 0.66
Batch: 80; loss: 1.73; acc: 0.52
Batch: 100; loss: 1.67; acc: 0.5
Batch: 120; loss: 1.49; acc: 0.58
Batch: 140; loss: 1.5; acc: 0.52
Batch: 160; loss: 1.48; acc: 0.53
Batch: 180; loss: 1.54; acc: 0.55
Batch: 200; loss: 1.74; acc: 0.38
Batch: 220; loss: 1.59; acc: 0.55
Batch: 240; loss: 1.73; acc: 0.38
Batch: 260; loss: 1.43; acc: 0.69
Batch: 280; loss: 1.49; acc: 0.59
Batch: 300; loss: 1.62; acc: 0.5
Batch: 320; loss: 1.49; acc: 0.55
Batch: 340; loss: 1.53; acc: 0.58
Batch: 360; loss: 1.56; acc: 0.53
Batch: 380; loss: 1.53; acc: 0.53
Batch: 400; loss: 1.62; acc: 0.59
Batch: 420; loss: 1.52; acc: 0.58
Batch: 440; loss: 1.5; acc: 0.55
Batch: 460; loss: 1.46; acc: 0.58
Batch: 480; loss: 1.45; acc: 0.59
Batch: 500; loss: 1.44; acc: 0.62
Batch: 520; loss: 1.5; acc: 0.59
Batch: 540; loss: 1.62; acc: 0.47
Batch: 560; loss: 1.59; acc: 0.55
Batch: 580; loss: 1.62; acc: 0.47
Batch: 600; loss: 1.59; acc: 0.47
Batch: 620; loss: 1.65; acc: 0.44
Batch: 640; loss: 1.46; acc: 0.66
Batch: 660; loss: 1.53; acc: 0.55
Batch: 680; loss: 1.59; acc: 0.48
Batch: 700; loss: 1.53; acc: 0.55
Batch: 720; loss: 1.64; acc: 0.55
Batch: 740; loss: 1.57; acc: 0.5
Batch: 760; loss: 1.59; acc: 0.53
Batch: 780; loss: 1.45; acc: 0.55
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.417037416715175e-05
1.4128147995506879e-05
Batch: 0; loss: 1.69; acc: 0.42
Batch: 20; loss: 1.51; acc: 0.52
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.38; acc: 0.67
Batch: 80; loss: 1.39; acc: 0.56
Batch: 100; loss: 1.44; acc: 0.64
Batch: 120; loss: 1.57; acc: 0.59
Batch: 140; loss: 1.31; acc: 0.69
Val Epoch over. val_loss: 1.4889814656251554; val_accuracy: 0.5799164012738853 

The current subspace-distance is: 1.4128147995506879e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.52
Batch: 20; loss: 1.55; acc: 0.58
Batch: 40; loss: 1.49; acc: 0.58
Batch: 60; loss: 1.62; acc: 0.42
Batch: 80; loss: 1.69; acc: 0.41
Batch: 100; loss: 1.47; acc: 0.55
Batch: 120; loss: 1.5; acc: 0.53
Batch: 140; loss: 1.57; acc: 0.53
Batch: 160; loss: 1.59; acc: 0.42
Batch: 180; loss: 1.57; acc: 0.45
Batch: 200; loss: 1.47; acc: 0.55
Batch: 220; loss: 1.54; acc: 0.56
Batch: 240; loss: 1.48; acc: 0.59
Batch: 260; loss: 1.57; acc: 0.47
Batch: 280; loss: 1.57; acc: 0.53
Batch: 300; loss: 1.54; acc: 0.64
Batch: 320; loss: 1.57; acc: 0.5
Batch: 340; loss: 1.46; acc: 0.55
Batch: 360; loss: 1.49; acc: 0.61
Batch: 380; loss: 1.52; acc: 0.62
Batch: 400; loss: 1.46; acc: 0.58
Batch: 420; loss: 1.54; acc: 0.58
Batch: 440; loss: 1.53; acc: 0.59
Batch: 460; loss: 1.6; acc: 0.47
Batch: 480; loss: 1.47; acc: 0.62
Batch: 500; loss: 1.63; acc: 0.44
Batch: 520; loss: 1.66; acc: 0.44
Batch: 540; loss: 1.61; acc: 0.58
Batch: 560; loss: 1.55; acc: 0.53
Batch: 580; loss: 1.5; acc: 0.55
Batch: 600; loss: 1.61; acc: 0.44
Batch: 620; loss: 1.48; acc: 0.56
Batch: 640; loss: 1.73; acc: 0.47
Batch: 660; loss: 1.44; acc: 0.64
Batch: 680; loss: 1.48; acc: 0.52
Batch: 700; loss: 1.52; acc: 0.52
Batch: 720; loss: 1.35; acc: 0.67
Batch: 740; loss: 1.65; acc: 0.5
Batch: 760; loss: 1.53; acc: 0.56
Batch: 780; loss: 1.39; acc: 0.67
Train Epoch over. train_loss: 1.54; train_accuracy: 0.54 

4.611839904100634e-05
1.839293327066116e-05
Batch: 0; loss: 1.7; acc: 0.42
Batch: 20; loss: 1.52; acc: 0.5
Batch: 40; loss: 1.33; acc: 0.59
Batch: 60; loss: 1.39; acc: 0.67
Batch: 80; loss: 1.39; acc: 0.55
Batch: 100; loss: 1.45; acc: 0.64
Batch: 120; loss: 1.58; acc: 0.56
Batch: 140; loss: 1.31; acc: 0.69
Val Epoch over. val_loss: 1.4896427020905123; val_accuracy: 0.5787221337579618 

The current subspace-distance is: 1.839293327066116e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_9_flips_False_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.1990338787749493

The number of parameters is: 264111

The number of individual parameters is:

18
324
18
18
27
42282
27
27
53
124497
53
53
64
91584
64
64
4096
64
640
10
64
64

nonzero elements in E: 26411097
elements in E: 26411100
fraction nonzero: 0.9999998864113953
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.39; acc: 0.06
Batch: 20; loss: 2.27; acc: 0.09
Batch: 40; loss: 2.14; acc: 0.28
Batch: 60; loss: 2.09; acc: 0.3
Batch: 80; loss: 1.95; acc: 0.41
Batch: 100; loss: 1.97; acc: 0.39
Batch: 120; loss: 1.95; acc: 0.31
Batch: 140; loss: 1.9; acc: 0.39
Batch: 160; loss: 1.89; acc: 0.42
Batch: 180; loss: 1.86; acc: 0.47
Batch: 200; loss: 1.86; acc: 0.53
Batch: 220; loss: 1.73; acc: 0.61
Batch: 240; loss: 1.76; acc: 0.53
Batch: 260; loss: 1.72; acc: 0.64
Batch: 280; loss: 1.75; acc: 0.62
Batch: 300; loss: 1.81; acc: 0.52
Batch: 320; loss: 1.68; acc: 0.61
Batch: 340; loss: 1.76; acc: 0.59
Batch: 360; loss: 1.75; acc: 0.55
Batch: 380; loss: 1.7; acc: 0.56
Batch: 400; loss: 1.73; acc: 0.56
Batch: 420; loss: 1.64; acc: 0.59
Batch: 440; loss: 1.62; acc: 0.55
Batch: 460; loss: 1.56; acc: 0.66
Batch: 480; loss: 1.65; acc: 0.58
Batch: 500; loss: 1.69; acc: 0.58
Batch: 520; loss: 1.62; acc: 0.58
Batch: 540; loss: 1.68; acc: 0.53
Batch: 560; loss: 1.56; acc: 0.67
Batch: 580; loss: 1.64; acc: 0.56
Batch: 600; loss: 1.56; acc: 0.69
Batch: 620; loss: 1.62; acc: 0.66
Batch: 640; loss: 1.57; acc: 0.66
Batch: 660; loss: 1.6; acc: 0.66
Batch: 680; loss: 1.52; acc: 0.62
Batch: 700; loss: 1.53; acc: 0.64
Batch: 720; loss: 1.45; acc: 0.69
Batch: 740; loss: 1.47; acc: 0.61
Batch: 760; loss: 1.64; acc: 0.62
Batch: 780; loss: 1.48; acc: 0.67
Train Epoch over. train_loss: 1.75; train_accuracy: 0.52 

5.457826773636043e-05
4.9779519031289965e-05
Batch: 0; loss: 1.59; acc: 0.58
Batch: 20; loss: 1.64; acc: 0.64
Batch: 40; loss: 1.36; acc: 0.8
Batch: 60; loss: 1.44; acc: 0.72
Batch: 80; loss: 1.46; acc: 0.72
Batch: 100; loss: 1.5; acc: 0.69
Batch: 120; loss: 1.66; acc: 0.55
Batch: 140; loss: 1.45; acc: 0.72
Val Epoch over. val_loss: 1.4918947136326202; val_accuracy: 0.68640525477707 

The current subspace-distance is: 4.9779519031289965e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.46; acc: 0.7
Batch: 20; loss: 1.46; acc: 0.73
Batch: 40; loss: 1.56; acc: 0.64
Batch: 60; loss: 1.55; acc: 0.64
Batch: 80; loss: 1.51; acc: 0.58
Batch: 100; loss: 1.49; acc: 0.64
Batch: 120; loss: 1.57; acc: 0.56
Batch: 140; loss: 1.44; acc: 0.77
Batch: 160; loss: 1.51; acc: 0.61
Batch: 180; loss: 1.53; acc: 0.69
Batch: 200; loss: 1.44; acc: 0.67
Batch: 220; loss: 1.68; acc: 0.55
Batch: 240; loss: 1.53; acc: 0.64
Batch: 260; loss: 1.51; acc: 0.7
Batch: 280; loss: 1.51; acc: 0.59
Batch: 300; loss: 1.62; acc: 0.59
Batch: 320; loss: 1.56; acc: 0.62
Batch: 340; loss: 1.51; acc: 0.61
Batch: 360; loss: 1.54; acc: 0.55
Batch: 380; loss: 1.51; acc: 0.61
Batch: 400; loss: 1.56; acc: 0.67
Batch: 420; loss: 1.43; acc: 0.67
Batch: 440; loss: 1.42; acc: 0.69
Batch: 460; loss: 1.47; acc: 0.62
Batch: 480; loss: 1.47; acc: 0.69
Batch: 500; loss: 1.62; acc: 0.56
Batch: 520; loss: 1.43; acc: 0.69
Batch: 540; loss: 1.55; acc: 0.58
Batch: 560; loss: 1.47; acc: 0.61
Batch: 580; loss: 1.35; acc: 0.75
Batch: 600; loss: 1.44; acc: 0.7
Batch: 620; loss: 1.46; acc: 0.67
Batch: 640; loss: 1.51; acc: 0.62
Batch: 660; loss: 1.48; acc: 0.7
Batch: 680; loss: 1.5; acc: 0.69
Batch: 700; loss: 1.45; acc: 0.7
Batch: 720; loss: 1.48; acc: 0.69
Batch: 740; loss: 1.44; acc: 0.77
Batch: 760; loss: 1.42; acc: 0.7
Batch: 780; loss: 1.45; acc: 0.61
Train Epoch over. train_loss: 1.5; train_accuracy: 0.65 

6.951568502699956e-05
6.337722879834473e-05
Batch: 0; loss: 1.57; acc: 0.58
Batch: 20; loss: 1.52; acc: 0.67
Batch: 40; loss: 1.2; acc: 0.78
Batch: 60; loss: 1.36; acc: 0.73
Batch: 80; loss: 1.36; acc: 0.75
Batch: 100; loss: 1.48; acc: 0.69
Batch: 120; loss: 1.64; acc: 0.56
Batch: 140; loss: 1.36; acc: 0.78
Val Epoch over. val_loss: 1.4141531803046063; val_accuracy: 0.6972531847133758 

The current subspace-distance is: 6.337722879834473e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.56
Batch: 20; loss: 1.55; acc: 0.58
Batch: 40; loss: 1.53; acc: 0.66
Batch: 60; loss: 1.48; acc: 0.64
Batch: 80; loss: 1.46; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.75
Batch: 120; loss: 1.53; acc: 0.69
Batch: 140; loss: 1.57; acc: 0.62
Batch: 160; loss: 1.43; acc: 0.64
Batch: 180; loss: 1.55; acc: 0.56
Batch: 200; loss: 1.46; acc: 0.69
Batch: 220; loss: 1.46; acc: 0.67
Batch: 240; loss: 1.56; acc: 0.58
Batch: 260; loss: 1.35; acc: 0.73
Batch: 280; loss: 1.41; acc: 0.67
Batch: 300; loss: 1.58; acc: 0.59
Batch: 320; loss: 1.39; acc: 0.73
Batch: 340; loss: 1.37; acc: 0.72
Batch: 360; loss: 1.52; acc: 0.7
Batch: 380; loss: 1.56; acc: 0.61
Batch: 400; loss: 1.57; acc: 0.62
Batch: 420; loss: 1.53; acc: 0.53
Batch: 440; loss: 1.35; acc: 0.72
Batch: 460; loss: 1.43; acc: 0.69
Batch: 480; loss: 1.49; acc: 0.61
Batch: 500; loss: 1.39; acc: 0.69
Batch: 520; loss: 1.36; acc: 0.69
Batch: 540; loss: 1.41; acc: 0.73
Batch: 560; loss: 1.39; acc: 0.7
Batch: 580; loss: 1.53; acc: 0.61
Batch: 600; loss: 1.38; acc: 0.67
Batch: 620; loss: 1.38; acc: 0.69
Batch: 640; loss: 1.59; acc: 0.52
Batch: 660; loss: 1.45; acc: 0.62
Batch: 680; loss: 1.48; acc: 0.59
Batch: 700; loss: 1.37; acc: 0.73
Batch: 720; loss: 1.43; acc: 0.61
Batch: 740; loss: 1.47; acc: 0.61
Batch: 760; loss: 1.38; acc: 0.67
Batch: 780; loss: 1.37; acc: 0.66
Train Epoch over. train_loss: 1.45; train_accuracy: 0.66 

7.686699973419309e-05
7.088486017892137e-05
Batch: 0; loss: 1.54; acc: 0.58
Batch: 20; loss: 1.49; acc: 0.66
Batch: 40; loss: 1.17; acc: 0.75
Batch: 60; loss: 1.37; acc: 0.7
Batch: 80; loss: 1.31; acc: 0.72
Batch: 100; loss: 1.48; acc: 0.67
Batch: 120; loss: 1.62; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.77
Val Epoch over. val_loss: 1.3918165355730967; val_accuracy: 0.6788415605095541 

The current subspace-distance is: 7.088486017892137e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.67
Batch: 20; loss: 1.36; acc: 0.75
Batch: 40; loss: 1.38; acc: 0.66
Batch: 60; loss: 1.53; acc: 0.61
Batch: 80; loss: 1.49; acc: 0.59
Batch: 100; loss: 1.46; acc: 0.66
Batch: 120; loss: 1.4; acc: 0.67
Batch: 140; loss: 1.44; acc: 0.64
Batch: 160; loss: 1.43; acc: 0.58
Batch: 180; loss: 1.48; acc: 0.62
Batch: 200; loss: 1.35; acc: 0.67
Batch: 220; loss: 1.42; acc: 0.69
Batch: 240; loss: 1.4; acc: 0.66
Batch: 260; loss: 1.52; acc: 0.61
Batch: 280; loss: 1.41; acc: 0.66
Batch: 300; loss: 1.47; acc: 0.7
Batch: 320; loss: 1.4; acc: 0.72
Batch: 340; loss: 1.35; acc: 0.73
Batch: 360; loss: 1.37; acc: 0.73
Batch: 380; loss: 1.43; acc: 0.62
Batch: 400; loss: 1.38; acc: 0.7
Batch: 420; loss: 1.55; acc: 0.58
Batch: 440; loss: 1.4; acc: 0.67
Batch: 460; loss: 1.46; acc: 0.61
Batch: 480; loss: 1.33; acc: 0.72
Batch: 500; loss: 1.31; acc: 0.67
Batch: 520; loss: 1.42; acc: 0.62
Batch: 540; loss: 1.33; acc: 0.72
Batch: 560; loss: 1.38; acc: 0.67
Batch: 580; loss: 1.51; acc: 0.58
Batch: 600; loss: 1.37; acc: 0.67
Batch: 620; loss: 1.47; acc: 0.66
Batch: 640; loss: 1.38; acc: 0.69
Batch: 660; loss: 1.4; acc: 0.67
Batch: 680; loss: 1.48; acc: 0.67
Batch: 700; loss: 1.41; acc: 0.62
Batch: 720; loss: 1.39; acc: 0.69
Batch: 740; loss: 1.51; acc: 0.56
Batch: 760; loss: 1.38; acc: 0.69
Batch: 780; loss: 1.41; acc: 0.69
Train Epoch over. train_loss: 1.42; train_accuracy: 0.65 

8.400053047807887e-05
7.845121581340209e-05
Batch: 0; loss: 1.52; acc: 0.55
Batch: 20; loss: 1.48; acc: 0.61
Batch: 40; loss: 1.16; acc: 0.73
Batch: 60; loss: 1.39; acc: 0.72
Batch: 80; loss: 1.25; acc: 0.75
Batch: 100; loss: 1.46; acc: 0.62
Batch: 120; loss: 1.62; acc: 0.58
Batch: 140; loss: 1.34; acc: 0.77
Val Epoch over. val_loss: 1.376011282015758; val_accuracy: 0.6712778662420382 

The current subspace-distance is: 7.845121581340209e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.58
Batch: 20; loss: 1.35; acc: 0.67
Batch: 40; loss: 1.44; acc: 0.61
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.34; acc: 0.75
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.44; acc: 0.66
Batch: 140; loss: 1.44; acc: 0.62
Batch: 160; loss: 1.34; acc: 0.7
Batch: 180; loss: 1.31; acc: 0.67
Batch: 200; loss: 1.36; acc: 0.62
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.4; acc: 0.64
Batch: 260; loss: 1.33; acc: 0.73
Batch: 280; loss: 1.38; acc: 0.62
Batch: 300; loss: 1.41; acc: 0.67
Batch: 320; loss: 1.32; acc: 0.72
Batch: 340; loss: 1.53; acc: 0.58
Batch: 360; loss: 1.28; acc: 0.73
Batch: 380; loss: 1.37; acc: 0.7
Batch: 400; loss: 1.51; acc: 0.59
Batch: 420; loss: 1.32; acc: 0.67
Batch: 440; loss: 1.5; acc: 0.58
Batch: 460; loss: 1.48; acc: 0.69
Batch: 480; loss: 1.33; acc: 0.72
Batch: 500; loss: 1.28; acc: 0.7
Batch: 520; loss: 1.4; acc: 0.66
Batch: 540; loss: 1.46; acc: 0.56
Batch: 560; loss: 1.45; acc: 0.66
Batch: 580; loss: 1.54; acc: 0.55
Batch: 600; loss: 1.39; acc: 0.69
Batch: 620; loss: 1.29; acc: 0.7
Batch: 640; loss: 1.36; acc: 0.75
Batch: 660; loss: 1.25; acc: 0.73
Batch: 680; loss: 1.44; acc: 0.67
Batch: 700; loss: 1.35; acc: 0.67
Batch: 720; loss: 1.26; acc: 0.73
Batch: 740; loss: 1.37; acc: 0.64
Batch: 760; loss: 1.42; acc: 0.64
Batch: 780; loss: 1.37; acc: 0.61
Train Epoch over. train_loss: 1.4; train_accuracy: 0.65 

9.005899482872337e-05
8.610377699369565e-05
Batch: 0; loss: 1.46; acc: 0.59
Batch: 20; loss: 1.44; acc: 0.59
Batch: 40; loss: 1.12; acc: 0.8
Batch: 60; loss: 1.37; acc: 0.66
Batch: 80; loss: 1.17; acc: 0.83
Batch: 100; loss: 1.4; acc: 0.69
Batch: 120; loss: 1.58; acc: 0.59
Batch: 140; loss: 1.3; acc: 0.72
Val Epoch over. val_loss: 1.3363714453521047; val_accuracy: 0.6779458598726115 

The current subspace-distance is: 8.610377699369565e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.67
Batch: 20; loss: 1.38; acc: 0.67
Batch: 40; loss: 1.49; acc: 0.59
Batch: 60; loss: 1.43; acc: 0.61
Batch: 80; loss: 1.32; acc: 0.67
Batch: 100; loss: 1.56; acc: 0.53
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 1.2; acc: 0.72
Batch: 160; loss: 1.33; acc: 0.69
Batch: 180; loss: 1.41; acc: 0.66
Batch: 200; loss: 1.37; acc: 0.7
Batch: 220; loss: 1.37; acc: 0.67
Batch: 240; loss: 1.57; acc: 0.56
Batch: 260; loss: 1.38; acc: 0.73
Batch: 280; loss: 1.31; acc: 0.67
Batch: 300; loss: 1.38; acc: 0.7
Batch: 320; loss: 1.41; acc: 0.64
Batch: 340; loss: 1.27; acc: 0.78
Batch: 360; loss: 1.34; acc: 0.73
Batch: 380; loss: 1.36; acc: 0.66
Batch: 400; loss: 1.42; acc: 0.61
Batch: 420; loss: 1.42; acc: 0.64
Batch: 440; loss: 1.46; acc: 0.61
Batch: 460; loss: 1.3; acc: 0.69
Batch: 480; loss: 1.37; acc: 0.67
Batch: 500; loss: 1.4; acc: 0.62
Batch: 520; loss: 1.37; acc: 0.67
Batch: 540; loss: 1.23; acc: 0.77
Batch: 560; loss: 1.45; acc: 0.61
Batch: 580; loss: 1.31; acc: 0.7
Batch: 600; loss: 1.39; acc: 0.64
Batch: 620; loss: 1.29; acc: 0.69
Batch: 640; loss: 1.38; acc: 0.59
Batch: 660; loss: 1.25; acc: 0.73
Batch: 680; loss: 1.34; acc: 0.73
Batch: 700; loss: 1.42; acc: 0.56
Batch: 720; loss: 1.26; acc: 0.7
Batch: 740; loss: 1.37; acc: 0.66
Batch: 760; loss: 1.29; acc: 0.67
Batch: 780; loss: 1.41; acc: 0.66
Train Epoch over. train_loss: 1.37; train_accuracy: 0.66 

9.728892473503947e-05
9.242725354852155e-05
Batch: 0; loss: 1.42; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.61
Batch: 40; loss: 1.06; acc: 0.84
Batch: 60; loss: 1.36; acc: 0.64
Batch: 80; loss: 1.12; acc: 0.83
Batch: 100; loss: 1.34; acc: 0.75
Batch: 120; loss: 1.57; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.77
Val Epoch over. val_loss: 1.306556492094781; val_accuracy: 0.692078025477707 

The current subspace-distance is: 9.242725354852155e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.7
Batch: 20; loss: 1.18; acc: 0.77
Batch: 40; loss: 1.34; acc: 0.66
Batch: 60; loss: 1.28; acc: 0.7
Batch: 80; loss: 1.38; acc: 0.61
Batch: 100; loss: 1.37; acc: 0.7
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.64
Batch: 160; loss: 1.37; acc: 0.67
Batch: 180; loss: 1.28; acc: 0.64
Batch: 200; loss: 1.28; acc: 0.75
Batch: 220; loss: 1.29; acc: 0.78
Batch: 240; loss: 1.32; acc: 0.67
Batch: 260; loss: 1.36; acc: 0.67
Batch: 280; loss: 1.23; acc: 0.73
Batch: 300; loss: 1.28; acc: 0.69
Batch: 320; loss: 1.39; acc: 0.67
Batch: 340; loss: 1.26; acc: 0.61
Batch: 360; loss: 1.3; acc: 0.72
Batch: 380; loss: 1.27; acc: 0.7
Batch: 400; loss: 1.3; acc: 0.7
Batch: 420; loss: 1.37; acc: 0.67
Batch: 440; loss: 1.3; acc: 0.72
Batch: 460; loss: 1.29; acc: 0.7
Batch: 480; loss: 1.29; acc: 0.69
Batch: 500; loss: 1.19; acc: 0.77
Batch: 520; loss: 1.3; acc: 0.69
Batch: 540; loss: 1.51; acc: 0.64
Batch: 560; loss: 1.11; acc: 0.77
Batch: 580; loss: 1.16; acc: 0.78
Batch: 600; loss: 1.24; acc: 0.72
Batch: 620; loss: 1.35; acc: 0.61
Batch: 640; loss: 1.33; acc: 0.61
Batch: 660; loss: 1.11; acc: 0.8
Batch: 680; loss: 1.34; acc: 0.61
Batch: 700; loss: 1.21; acc: 0.7
Batch: 720; loss: 1.3; acc: 0.67
Batch: 740; loss: 1.3; acc: 0.67
Batch: 760; loss: 1.42; acc: 0.62
Batch: 780; loss: 1.3; acc: 0.67
Train Epoch over. train_loss: 1.32; train_accuracy: 0.67 

0.00010881729394895956
0.00010354154073866084
Batch: 0; loss: 1.37; acc: 0.69
Batch: 20; loss: 1.37; acc: 0.62
Batch: 40; loss: 0.95; acc: 0.88
Batch: 60; loss: 1.3; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.84
Batch: 100; loss: 1.25; acc: 0.77
Batch: 120; loss: 1.52; acc: 0.61
Batch: 140; loss: 1.22; acc: 0.73
Val Epoch over. val_loss: 1.2392741096247533; val_accuracy: 0.7076035031847133 

The current subspace-distance is: 0.00010354154073866084 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.73
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 1.28; acc: 0.67
Batch: 60; loss: 1.23; acc: 0.7
Batch: 80; loss: 1.24; acc: 0.67
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.26; acc: 0.72
Batch: 140; loss: 1.5; acc: 0.56
Batch: 160; loss: 1.27; acc: 0.67
Batch: 180; loss: 1.37; acc: 0.62
Batch: 200; loss: 1.31; acc: 0.66
Batch: 220; loss: 1.22; acc: 0.72
Batch: 240; loss: 1.29; acc: 0.7
Batch: 260; loss: 1.22; acc: 0.69
Batch: 280; loss: 1.3; acc: 0.67
Batch: 300; loss: 1.19; acc: 0.73
Batch: 320; loss: 1.36; acc: 0.59
Batch: 340; loss: 1.36; acc: 0.58
Batch: 360; loss: 1.3; acc: 0.7
Batch: 380; loss: 1.4; acc: 0.61
Batch: 400; loss: 1.24; acc: 0.69
Batch: 420; loss: 1.43; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.7
Batch: 460; loss: 1.46; acc: 0.55
Batch: 480; loss: 1.25; acc: 0.67
Batch: 500; loss: 1.39; acc: 0.61
Batch: 520; loss: 1.41; acc: 0.62
Batch: 540; loss: 1.31; acc: 0.66
Batch: 560; loss: 1.34; acc: 0.66
Batch: 580; loss: 1.13; acc: 0.75
Batch: 600; loss: 1.14; acc: 0.75
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.09; acc: 0.7
Batch: 660; loss: 1.38; acc: 0.69
Batch: 680; loss: 1.22; acc: 0.62
Batch: 700; loss: 1.31; acc: 0.66
Batch: 720; loss: 1.24; acc: 0.62
Batch: 740; loss: 1.45; acc: 0.64
Batch: 760; loss: 1.19; acc: 0.73
Batch: 780; loss: 1.26; acc: 0.66
Train Epoch over. train_loss: 1.28; train_accuracy: 0.68 

0.0001159564417321235
0.00011093159992014989
Batch: 0; loss: 1.32; acc: 0.69
Batch: 20; loss: 1.33; acc: 0.62
Batch: 40; loss: 0.9; acc: 0.88
Batch: 60; loss: 1.26; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.86
Batch: 100; loss: 1.21; acc: 0.8
Batch: 120; loss: 1.45; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.77
Val Epoch over. val_loss: 1.2068024361209504; val_accuracy: 0.716062898089172 

The current subspace-distance is: 0.00011093159992014989 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.27; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.78
Batch: 100; loss: 1.27; acc: 0.75
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.38; acc: 0.66
Batch: 160; loss: 1.39; acc: 0.58
Batch: 180; loss: 1.38; acc: 0.62
Batch: 200; loss: 1.24; acc: 0.67
Batch: 220; loss: 1.33; acc: 0.58
Batch: 240; loss: 1.36; acc: 0.64
Batch: 260; loss: 1.26; acc: 0.7
Batch: 280; loss: 1.23; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.69
Batch: 320; loss: 1.36; acc: 0.59
Batch: 340; loss: 1.28; acc: 0.67
Batch: 360; loss: 1.21; acc: 0.75
Batch: 380; loss: 1.11; acc: 0.73
Batch: 400; loss: 1.27; acc: 0.67
Batch: 420; loss: 1.39; acc: 0.61
Batch: 440; loss: 1.17; acc: 0.7
Batch: 460; loss: 1.28; acc: 0.64
Batch: 480; loss: 1.29; acc: 0.59
Batch: 500; loss: 1.38; acc: 0.62
Batch: 520; loss: 1.16; acc: 0.72
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.21; acc: 0.72
Batch: 580; loss: 1.3; acc: 0.62
Batch: 600; loss: 1.19; acc: 0.73
Batch: 620; loss: 1.2; acc: 0.67
Batch: 640; loss: 1.37; acc: 0.64
Batch: 660; loss: 1.3; acc: 0.67
Batch: 680; loss: 1.27; acc: 0.69
Batch: 700; loss: 1.22; acc: 0.69
Batch: 720; loss: 1.42; acc: 0.55
Batch: 740; loss: 1.27; acc: 0.66
Batch: 760; loss: 1.33; acc: 0.69
Batch: 780; loss: 1.37; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.68 

0.00012591228005476296
0.00011978969996562228
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.31; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.88
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.92
Batch: 100; loss: 1.18; acc: 0.81
Batch: 120; loss: 1.39; acc: 0.61
Batch: 140; loss: 1.08; acc: 0.8
Val Epoch over. val_loss: 1.168851670945526; val_accuracy: 0.7182523885350318 

The current subspace-distance is: 0.00011978969996562228 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.56
Batch: 20; loss: 1.31; acc: 0.66
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.29; acc: 0.7
Batch: 80; loss: 1.16; acc: 0.66
Batch: 100; loss: 1.19; acc: 0.67
Batch: 120; loss: 1.22; acc: 0.67
Batch: 140; loss: 1.2; acc: 0.7
Batch: 160; loss: 1.19; acc: 0.69
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.31; acc: 0.62
Batch: 220; loss: 1.38; acc: 0.58
Batch: 240; loss: 1.22; acc: 0.72
Batch: 260; loss: 1.23; acc: 0.73
Batch: 280; loss: 1.34; acc: 0.61
Batch: 300; loss: 1.29; acc: 0.59
Batch: 320; loss: 1.27; acc: 0.69
Batch: 340; loss: 1.27; acc: 0.64
Batch: 360; loss: 1.24; acc: 0.64
Batch: 380; loss: 1.08; acc: 0.73
Batch: 400; loss: 1.06; acc: 0.77
Batch: 420; loss: 1.29; acc: 0.66
Batch: 440; loss: 1.19; acc: 0.72
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 1.36; acc: 0.64
Batch: 500; loss: 1.24; acc: 0.7
Batch: 520; loss: 1.23; acc: 0.67
Batch: 540; loss: 1.19; acc: 0.66
Batch: 560; loss: 1.31; acc: 0.62
Batch: 580; loss: 1.13; acc: 0.73
Batch: 600; loss: 1.15; acc: 0.73
Batch: 620; loss: 1.24; acc: 0.72
Batch: 640; loss: 1.35; acc: 0.61
Batch: 660; loss: 1.25; acc: 0.64
Batch: 680; loss: 1.17; acc: 0.7
Batch: 700; loss: 1.19; acc: 0.72
Batch: 720; loss: 1.23; acc: 0.62
Batch: 740; loss: 1.16; acc: 0.8
Batch: 760; loss: 1.19; acc: 0.69
Batch: 780; loss: 1.19; acc: 0.7
Train Epoch over. train_loss: 1.23; train_accuracy: 0.68 

0.0001350674865534529
0.00012869610509369522
Batch: 0; loss: 1.31; acc: 0.61
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 0.86; acc: 0.86
Batch: 60; loss: 1.2; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.91
Batch: 100; loss: 1.17; acc: 0.8
Batch: 120; loss: 1.39; acc: 0.64
Batch: 140; loss: 1.04; acc: 0.8
Val Epoch over. val_loss: 1.1609738981647857; val_accuracy: 0.7186504777070064 

The current subspace-distance is: 0.00012869610509369522 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.33; acc: 0.64
Batch: 20; loss: 1.29; acc: 0.66
Batch: 40; loss: 1.21; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 1.18; acc: 0.64
Batch: 100; loss: 1.08; acc: 0.67
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 1.25; acc: 0.67
Batch: 160; loss: 1.27; acc: 0.66
Batch: 180; loss: 1.18; acc: 0.7
Batch: 200; loss: 1.19; acc: 0.75
Batch: 220; loss: 1.17; acc: 0.72
Batch: 240; loss: 1.16; acc: 0.69
Batch: 260; loss: 1.23; acc: 0.67
Batch: 280; loss: 1.26; acc: 0.66
Batch: 300; loss: 1.26; acc: 0.58
Batch: 320; loss: 1.43; acc: 0.58
Batch: 340; loss: 1.34; acc: 0.59
Batch: 360; loss: 1.24; acc: 0.64
Batch: 380; loss: 1.04; acc: 0.83
Batch: 400; loss: 1.21; acc: 0.72
Batch: 420; loss: 1.41; acc: 0.59
Batch: 440; loss: 1.26; acc: 0.72
Batch: 460; loss: 1.1; acc: 0.75
Batch: 480; loss: 1.06; acc: 0.81
Batch: 500; loss: 1.33; acc: 0.61
Batch: 520; loss: 1.17; acc: 0.7
Batch: 540; loss: 1.19; acc: 0.7
Batch: 560; loss: 1.19; acc: 0.7
Batch: 580; loss: 1.17; acc: 0.77
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.21; acc: 0.72
Batch: 640; loss: 1.16; acc: 0.7
Batch: 660; loss: 1.26; acc: 0.67
Batch: 680; loss: 1.21; acc: 0.67
Batch: 700; loss: 1.13; acc: 0.73
Batch: 720; loss: 1.41; acc: 0.58
Batch: 740; loss: 1.1; acc: 0.75
Batch: 760; loss: 1.15; acc: 0.72
Batch: 780; loss: 1.17; acc: 0.64
Train Epoch over. train_loss: 1.22; train_accuracy: 0.69 

0.00013436561857815832
0.00012861340655945241
Batch: 0; loss: 1.29; acc: 0.64
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 0.86; acc: 0.88
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 0.95; acc: 0.91
Batch: 100; loss: 1.17; acc: 0.8
Batch: 120; loss: 1.37; acc: 0.66
Batch: 140; loss: 1.02; acc: 0.8
Val Epoch over. val_loss: 1.1556017410223651; val_accuracy: 0.7235270700636943 

The current subspace-distance is: 0.00012861340655945241 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.26; acc: 0.69
Batch: 80; loss: 1.21; acc: 0.64
Batch: 100; loss: 1.26; acc: 0.62
Batch: 120; loss: 1.03; acc: 0.83
Batch: 140; loss: 1.3; acc: 0.58
Batch: 160; loss: 1.24; acc: 0.69
Batch: 180; loss: 1.32; acc: 0.66
Batch: 200; loss: 1.18; acc: 0.7
Batch: 220; loss: 1.21; acc: 0.67
Batch: 240; loss: 1.19; acc: 0.7
Batch: 260; loss: 1.24; acc: 0.7
Batch: 280; loss: 1.27; acc: 0.64
Batch: 300; loss: 1.03; acc: 0.77
Batch: 320; loss: 1.14; acc: 0.73
Batch: 340; loss: 1.18; acc: 0.75
Batch: 360; loss: 1.22; acc: 0.67
Batch: 380; loss: 1.26; acc: 0.67
Batch: 400; loss: 1.34; acc: 0.64
Batch: 420; loss: 1.24; acc: 0.64
Batch: 440; loss: 1.39; acc: 0.66
Batch: 460; loss: 1.33; acc: 0.66
Batch: 480; loss: 1.3; acc: 0.62
Batch: 500; loss: 1.14; acc: 0.7
Batch: 520; loss: 1.22; acc: 0.72
Batch: 540; loss: 1.29; acc: 0.66
Batch: 560; loss: 1.33; acc: 0.59
Batch: 580; loss: 1.27; acc: 0.55
Batch: 600; loss: 1.15; acc: 0.69
Batch: 620; loss: 1.18; acc: 0.7
Batch: 640; loss: 1.2; acc: 0.73
Batch: 660; loss: 1.16; acc: 0.75
Batch: 680; loss: 1.06; acc: 0.72
Batch: 700; loss: 1.02; acc: 0.83
Batch: 720; loss: 1.05; acc: 0.83
Batch: 740; loss: 1.14; acc: 0.69
Batch: 760; loss: 1.36; acc: 0.67
Batch: 780; loss: 1.4; acc: 0.61
Train Epoch over. train_loss: 1.22; train_accuracy: 0.68 

0.00013599100930150598
0.00013040554767940193
Batch: 0; loss: 1.28; acc: 0.62
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.84; acc: 0.88
Batch: 60; loss: 1.18; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.91
Batch: 100; loss: 1.15; acc: 0.78
Batch: 120; loss: 1.36; acc: 0.66
Batch: 140; loss: 1.0; acc: 0.78
Val Epoch over. val_loss: 1.138190249728549; val_accuracy: 0.7250199044585988 

The current subspace-distance is: 0.00013040554767940193 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 1.19; acc: 0.64
Batch: 40; loss: 1.2; acc: 0.77
Batch: 60; loss: 1.24; acc: 0.62
Batch: 80; loss: 1.18; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.62
Batch: 120; loss: 1.23; acc: 0.66
Batch: 140; loss: 1.38; acc: 0.64
Batch: 160; loss: 1.18; acc: 0.72
Batch: 180; loss: 1.17; acc: 0.69
Batch: 200; loss: 1.25; acc: 0.73
Batch: 220; loss: 1.2; acc: 0.69
Batch: 240; loss: 1.3; acc: 0.67
Batch: 260; loss: 1.2; acc: 0.67
Batch: 280; loss: 1.19; acc: 0.72
Batch: 300; loss: 1.18; acc: 0.73
Batch: 320; loss: 1.3; acc: 0.59
Batch: 340; loss: 1.29; acc: 0.59
Batch: 360; loss: 1.18; acc: 0.69
Batch: 380; loss: 1.22; acc: 0.66
Batch: 400; loss: 1.22; acc: 0.62
Batch: 420; loss: 1.27; acc: 0.61
Batch: 440; loss: 1.23; acc: 0.64
Batch: 460; loss: 1.39; acc: 0.59
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 1.15; acc: 0.72
Batch: 520; loss: 1.13; acc: 0.73
Batch: 540; loss: 1.25; acc: 0.62
Batch: 560; loss: 1.32; acc: 0.58
Batch: 580; loss: 1.4; acc: 0.67
Batch: 600; loss: 1.3; acc: 0.61
Batch: 620; loss: 1.36; acc: 0.59
Batch: 640; loss: 1.26; acc: 0.64
Batch: 660; loss: 1.2; acc: 0.73
Batch: 680; loss: 1.22; acc: 0.72
Batch: 700; loss: 1.21; acc: 0.73
Batch: 720; loss: 1.22; acc: 0.72
Batch: 740; loss: 1.13; acc: 0.77
Batch: 760; loss: 1.17; acc: 0.7
Batch: 780; loss: 1.11; acc: 0.69
Train Epoch over. train_loss: 1.21; train_accuracy: 0.69 

0.00013969359861221164
0.00013437050802167505
Batch: 0; loss: 1.27; acc: 0.64
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 0.84; acc: 0.88
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 0.92; acc: 0.91
Batch: 100; loss: 1.15; acc: 0.8
Batch: 120; loss: 1.35; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.78
Val Epoch over. val_loss: 1.1346341253845555; val_accuracy: 0.7289012738853503 

The current subspace-distance is: 0.00013437050802167505 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.19; acc: 0.7
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.06; acc: 0.77
Batch: 80; loss: 1.26; acc: 0.66
Batch: 100; loss: 1.29; acc: 0.64
Batch: 120; loss: 1.13; acc: 0.78
Batch: 140; loss: 1.11; acc: 0.78
Batch: 160; loss: 1.11; acc: 0.7
Batch: 180; loss: 1.2; acc: 0.75
Batch: 200; loss: 1.2; acc: 0.72
Batch: 220; loss: 1.06; acc: 0.77
Batch: 240; loss: 1.07; acc: 0.69
Batch: 260; loss: 1.28; acc: 0.67
Batch: 280; loss: 1.29; acc: 0.62
Batch: 300; loss: 1.2; acc: 0.75
Batch: 320; loss: 1.35; acc: 0.56
Batch: 340; loss: 1.15; acc: 0.72
Batch: 360; loss: 1.23; acc: 0.69
Batch: 380; loss: 1.15; acc: 0.72
Batch: 400; loss: 1.14; acc: 0.73
Batch: 420; loss: 1.14; acc: 0.75
Batch: 440; loss: 1.09; acc: 0.73
Batch: 460; loss: 1.23; acc: 0.66
Batch: 480; loss: 1.23; acc: 0.72
Batch: 500; loss: 1.28; acc: 0.61
Batch: 520; loss: 1.1; acc: 0.73
Batch: 540; loss: 1.2; acc: 0.66
Batch: 560; loss: 1.29; acc: 0.62
Batch: 580; loss: 1.26; acc: 0.64
Batch: 600; loss: 0.99; acc: 0.86
Batch: 620; loss: 1.14; acc: 0.73
Batch: 640; loss: 1.19; acc: 0.67
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.32; acc: 0.61
Batch: 700; loss: 1.23; acc: 0.69
Batch: 720; loss: 1.24; acc: 0.66
Batch: 740; loss: 1.1; acc: 0.77
Batch: 760; loss: 1.28; acc: 0.69
Batch: 780; loss: 1.21; acc: 0.67
Train Epoch over. train_loss: 1.21; train_accuracy: 0.69 

0.00014008684956934303
0.00013457833847496659
Batch: 0; loss: 1.29; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.91
Batch: 100; loss: 1.16; acc: 0.8
Batch: 120; loss: 1.36; acc: 0.67
Batch: 140; loss: 0.99; acc: 0.78
Val Epoch over. val_loss: 1.1388457151734905; val_accuracy: 0.7224323248407644 

The current subspace-distance is: 0.00013457833847496659 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.3; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.73
Batch: 40; loss: 1.1; acc: 0.72
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.13; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.62
Batch: 120; loss: 1.24; acc: 0.72
Batch: 140; loss: 1.1; acc: 0.73
Batch: 160; loss: 1.23; acc: 0.7
Batch: 180; loss: 1.15; acc: 0.75
Batch: 200; loss: 1.38; acc: 0.56
Batch: 220; loss: 1.27; acc: 0.66
Batch: 240; loss: 1.3; acc: 0.64
Batch: 260; loss: 1.17; acc: 0.64
Batch: 280; loss: 1.1; acc: 0.73
Batch: 300; loss: 1.18; acc: 0.72
Batch: 320; loss: 1.29; acc: 0.66
Batch: 340; loss: 1.28; acc: 0.62
Batch: 360; loss: 1.3; acc: 0.59
Batch: 380; loss: 1.03; acc: 0.81
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.27; acc: 0.62
Batch: 440; loss: 1.33; acc: 0.59
Batch: 460; loss: 1.21; acc: 0.62
Batch: 480; loss: 1.41; acc: 0.56
Batch: 500; loss: 1.26; acc: 0.64
Batch: 520; loss: 1.11; acc: 0.7
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.3; acc: 0.62
Batch: 580; loss: 1.28; acc: 0.69
Batch: 600; loss: 1.11; acc: 0.81
Batch: 620; loss: 1.32; acc: 0.66
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.12; acc: 0.73
Batch: 680; loss: 1.33; acc: 0.64
Batch: 700; loss: 1.29; acc: 0.59
Batch: 720; loss: 1.31; acc: 0.59
Batch: 740; loss: 1.14; acc: 0.72
Batch: 760; loss: 1.17; acc: 0.66
Batch: 780; loss: 1.1; acc: 0.78
Train Epoch over. train_loss: 1.2; train_accuracy: 0.69 

0.00014336721505969763
0.00013780765584670007
Batch: 0; loss: 1.28; acc: 0.62
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 0.85; acc: 0.81
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 0.92; acc: 0.91
Batch: 100; loss: 1.17; acc: 0.78
Batch: 120; loss: 1.35; acc: 0.69
Batch: 140; loss: 0.99; acc: 0.8
Val Epoch over. val_loss: 1.1405601562208432; val_accuracy: 0.7205414012738853 

The current subspace-distance is: 0.00013780765584670007 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.18; acc: 0.77
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.05; acc: 0.75
Batch: 80; loss: 1.26; acc: 0.66
Batch: 100; loss: 1.02; acc: 0.78
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.16; acc: 0.73
Batch: 160; loss: 1.17; acc: 0.67
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.19; acc: 0.72
Batch: 220; loss: 1.26; acc: 0.64
Batch: 240; loss: 1.26; acc: 0.64
Batch: 260; loss: 1.33; acc: 0.62
Batch: 280; loss: 1.02; acc: 0.72
Batch: 300; loss: 1.23; acc: 0.69
Batch: 320; loss: 1.12; acc: 0.75
Batch: 340; loss: 1.39; acc: 0.58
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.07; acc: 0.78
Batch: 400; loss: 1.25; acc: 0.64
Batch: 420; loss: 1.22; acc: 0.69
Batch: 440; loss: 1.24; acc: 0.67
Batch: 460; loss: 1.16; acc: 0.69
Batch: 480; loss: 1.28; acc: 0.62
Batch: 500; loss: 1.06; acc: 0.75
Batch: 520; loss: 1.18; acc: 0.78
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 1.07; acc: 0.75
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.2; acc: 0.64
Batch: 620; loss: 1.08; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.69
Batch: 660; loss: 1.26; acc: 0.67
Batch: 680; loss: 1.25; acc: 0.67
Batch: 700; loss: 1.09; acc: 0.75
Batch: 720; loss: 1.33; acc: 0.64
Batch: 740; loss: 1.18; acc: 0.66
Batch: 760; loss: 1.12; acc: 0.7
Batch: 780; loss: 1.11; acc: 0.72
Train Epoch over. train_loss: 1.2; train_accuracy: 0.69 

0.00014146363537292928
0.00013685131852980703
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 0.84; acc: 0.83
Batch: 60; loss: 1.17; acc: 0.64
Batch: 80; loss: 0.93; acc: 0.91
Batch: 100; loss: 1.16; acc: 0.77
Batch: 120; loss: 1.34; acc: 0.69
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.1357464205687213; val_accuracy: 0.7201433121019108 

The current subspace-distance is: 0.00013685131852980703 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.12; acc: 0.69
Batch: 40; loss: 1.19; acc: 0.69
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 1.09; acc: 0.8
Batch: 100; loss: 1.08; acc: 0.7
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 1.25; acc: 0.66
Batch: 160; loss: 1.08; acc: 0.7
Batch: 180; loss: 1.21; acc: 0.66
Batch: 200; loss: 1.22; acc: 0.72
Batch: 220; loss: 1.15; acc: 0.73
Batch: 240; loss: 1.02; acc: 0.78
Batch: 260; loss: 1.03; acc: 0.78
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 1.12; acc: 0.75
Batch: 320; loss: 1.15; acc: 0.67
Batch: 340; loss: 1.31; acc: 0.66
Batch: 360; loss: 1.4; acc: 0.59
Batch: 380; loss: 1.24; acc: 0.64
Batch: 400; loss: 1.41; acc: 0.59
Batch: 420; loss: 1.19; acc: 0.64
Batch: 440; loss: 1.1; acc: 0.69
Batch: 460; loss: 1.29; acc: 0.69
Batch: 480; loss: 1.16; acc: 0.72
Batch: 500; loss: 1.18; acc: 0.66
Batch: 520; loss: 1.32; acc: 0.61
Batch: 540; loss: 1.39; acc: 0.64
Batch: 560; loss: 1.03; acc: 0.73
Batch: 580; loss: 1.22; acc: 0.72
Batch: 600; loss: 1.26; acc: 0.62
Batch: 620; loss: 1.3; acc: 0.61
Batch: 640; loss: 1.02; acc: 0.81
Batch: 660; loss: 1.04; acc: 0.77
Batch: 680; loss: 1.03; acc: 0.75
Batch: 700; loss: 1.28; acc: 0.64
Batch: 720; loss: 1.27; acc: 0.67
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 1.09; acc: 0.73
Batch: 780; loss: 1.16; acc: 0.7
Train Epoch over. train_loss: 1.2; train_accuracy: 0.68 

0.00014564486627932638
0.00014056626241654158
Batch: 0; loss: 1.25; acc: 0.64
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 1.17; acc: 0.64
Batch: 80; loss: 0.91; acc: 0.89
Batch: 100; loss: 1.15; acc: 0.8
Batch: 120; loss: 1.34; acc: 0.66
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.1242098345118723; val_accuracy: 0.7263136942675159 

The current subspace-distance is: 0.00014056626241654158 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.21; acc: 0.69
Batch: 20; loss: 1.08; acc: 0.72
Batch: 40; loss: 1.12; acc: 0.73
Batch: 60; loss: 1.26; acc: 0.66
Batch: 80; loss: 1.22; acc: 0.69
Batch: 100; loss: 1.33; acc: 0.66
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.66
Batch: 160; loss: 1.31; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.72
Batch: 200; loss: 1.24; acc: 0.66
Batch: 220; loss: 1.24; acc: 0.64
Batch: 240; loss: 1.12; acc: 0.73
Batch: 260; loss: 1.21; acc: 0.72
Batch: 280; loss: 1.23; acc: 0.66
Batch: 300; loss: 1.25; acc: 0.61
Batch: 320; loss: 1.02; acc: 0.72
Batch: 340; loss: 1.05; acc: 0.78
Batch: 360; loss: 1.2; acc: 0.69
Batch: 380; loss: 1.15; acc: 0.75
Batch: 400; loss: 1.24; acc: 0.7
Batch: 420; loss: 1.14; acc: 0.73
Batch: 440; loss: 1.06; acc: 0.7
Batch: 460; loss: 1.14; acc: 0.8
Batch: 480; loss: 1.25; acc: 0.61
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 1.2; acc: 0.73
Batch: 540; loss: 1.09; acc: 0.75
Batch: 560; loss: 1.28; acc: 0.64
Batch: 580; loss: 1.11; acc: 0.75
Batch: 600; loss: 1.16; acc: 0.73
Batch: 620; loss: 1.2; acc: 0.69
Batch: 640; loss: 1.45; acc: 0.64
Batch: 660; loss: 1.1; acc: 0.72
Batch: 680; loss: 1.23; acc: 0.64
Batch: 700; loss: 1.07; acc: 0.67
Batch: 720; loss: 1.17; acc: 0.67
Batch: 740; loss: 1.22; acc: 0.62
Batch: 760; loss: 1.16; acc: 0.72
Batch: 780; loss: 1.18; acc: 0.66
Train Epoch over. train_loss: 1.19; train_accuracy: 0.69 

0.00014690433454234153
0.00014112191274762154
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.81
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 0.92; acc: 0.89
Batch: 100; loss: 1.15; acc: 0.77
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 1.1233682761526411; val_accuracy: 0.7186504777070064 

The current subspace-distance is: 0.00014112191274762154 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 1.08; acc: 0.77
Batch: 60; loss: 1.4; acc: 0.61
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.3; acc: 0.67
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 1.1; acc: 0.69
Batch: 160; loss: 1.39; acc: 0.62
Batch: 180; loss: 1.19; acc: 0.67
Batch: 200; loss: 1.18; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.72
Batch: 240; loss: 1.12; acc: 0.67
Batch: 260; loss: 1.15; acc: 0.66
Batch: 280; loss: 1.21; acc: 0.59
Batch: 300; loss: 1.11; acc: 0.75
Batch: 320; loss: 1.15; acc: 0.67
Batch: 340; loss: 1.26; acc: 0.61
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.21; acc: 0.66
Batch: 400; loss: 1.28; acc: 0.59
Batch: 420; loss: 1.28; acc: 0.62
Batch: 440; loss: 1.1; acc: 0.75
Batch: 460; loss: 1.19; acc: 0.66
Batch: 480; loss: 1.15; acc: 0.67
Batch: 500; loss: 1.16; acc: 0.64
Batch: 520; loss: 1.03; acc: 0.77
Batch: 540; loss: 1.23; acc: 0.61
Batch: 560; loss: 0.89; acc: 0.81
Batch: 580; loss: 1.25; acc: 0.62
Batch: 600; loss: 1.17; acc: 0.67
Batch: 620; loss: 1.12; acc: 0.75
Batch: 640; loss: 1.2; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.72
Batch: 680; loss: 1.23; acc: 0.59
Batch: 700; loss: 1.22; acc: 0.7
Batch: 720; loss: 1.15; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 1.06; acc: 0.75
Batch: 780; loss: 1.21; acc: 0.67
Train Epoch over. train_loss: 1.19; train_accuracy: 0.69 

0.00014855372137390077
0.00014249325613491237
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 1.25; acc: 0.59
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.91; acc: 0.91
Batch: 100; loss: 1.14; acc: 0.78
Batch: 120; loss: 1.34; acc: 0.64
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.1180279809198561; val_accuracy: 0.7197452229299363 

The current subspace-distance is: 0.00014249325613491237 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.7
Batch: 40; loss: 1.24; acc: 0.69
Batch: 60; loss: 1.19; acc: 0.7
Batch: 80; loss: 1.32; acc: 0.62
Batch: 100; loss: 1.06; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.7
Batch: 140; loss: 1.12; acc: 0.73
Batch: 160; loss: 1.19; acc: 0.66
Batch: 180; loss: 1.41; acc: 0.61
Batch: 200; loss: 1.13; acc: 0.7
Batch: 220; loss: 1.23; acc: 0.66
Batch: 240; loss: 1.19; acc: 0.69
Batch: 260; loss: 1.02; acc: 0.81
Batch: 280; loss: 1.22; acc: 0.73
Batch: 300; loss: 1.15; acc: 0.72
Batch: 320; loss: 1.06; acc: 0.77
Batch: 340; loss: 1.13; acc: 0.72
Batch: 360; loss: 1.23; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.7
Batch: 400; loss: 1.12; acc: 0.75
Batch: 420; loss: 1.33; acc: 0.69
Batch: 440; loss: 1.1; acc: 0.75
Batch: 460; loss: 1.35; acc: 0.59
Batch: 480; loss: 1.09; acc: 0.73
Batch: 500; loss: 1.15; acc: 0.69
Batch: 520; loss: 1.32; acc: 0.69
Batch: 540; loss: 1.19; acc: 0.66
Batch: 560; loss: 1.14; acc: 0.7
Batch: 580; loss: 1.07; acc: 0.8
Batch: 600; loss: 1.43; acc: 0.58
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 1.17; acc: 0.67
Batch: 660; loss: 1.07; acc: 0.77
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.24; acc: 0.67
Batch: 720; loss: 1.06; acc: 0.78
Batch: 740; loss: 1.11; acc: 0.72
Batch: 760; loss: 1.23; acc: 0.62
Batch: 780; loss: 1.29; acc: 0.66
Train Epoch over. train_loss: 1.19; train_accuracy: 0.69 

0.00014733412535861135
0.00014113172073848546
Batch: 0; loss: 1.24; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.59
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.91
Batch: 100; loss: 1.14; acc: 0.77
Batch: 120; loss: 1.34; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.78
Val Epoch over. val_loss: 1.1164122873051152; val_accuracy: 0.7220342356687898 

The current subspace-distance is: 0.00014113172073848546 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.18; acc: 0.73
Batch: 40; loss: 1.11; acc: 0.75
Batch: 60; loss: 1.23; acc: 0.73
Batch: 80; loss: 1.25; acc: 0.64
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 1.42; acc: 0.53
Batch: 160; loss: 1.14; acc: 0.7
Batch: 180; loss: 1.22; acc: 0.69
Batch: 200; loss: 1.16; acc: 0.7
Batch: 220; loss: 1.16; acc: 0.67
Batch: 240; loss: 1.17; acc: 0.75
Batch: 260; loss: 1.09; acc: 0.73
Batch: 280; loss: 1.24; acc: 0.64
Batch: 300; loss: 1.25; acc: 0.64
Batch: 320; loss: 1.08; acc: 0.75
Batch: 340; loss: 1.12; acc: 0.73
Batch: 360; loss: 1.3; acc: 0.59
Batch: 380; loss: 1.28; acc: 0.62
Batch: 400; loss: 1.31; acc: 0.59
Batch: 420; loss: 1.33; acc: 0.62
Batch: 440; loss: 1.15; acc: 0.72
Batch: 460; loss: 1.21; acc: 0.69
Batch: 480; loss: 1.27; acc: 0.66
Batch: 500; loss: 1.19; acc: 0.73
Batch: 520; loss: 1.07; acc: 0.73
Batch: 540; loss: 1.16; acc: 0.7
Batch: 560; loss: 1.16; acc: 0.67
Batch: 580; loss: 1.16; acc: 0.72
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 1.23; acc: 0.67
Batch: 640; loss: 1.08; acc: 0.72
Batch: 660; loss: 1.17; acc: 0.67
Batch: 680; loss: 1.13; acc: 0.67
Batch: 700; loss: 0.92; acc: 0.8
Batch: 720; loss: 0.99; acc: 0.78
Batch: 740; loss: 1.25; acc: 0.66
Batch: 760; loss: 1.05; acc: 0.75
Batch: 780; loss: 1.11; acc: 0.78
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00015055389667395502
0.0001460454222979024
Batch: 0; loss: 1.25; acc: 0.61
Batch: 20; loss: 1.25; acc: 0.59
Batch: 40; loss: 0.83; acc: 0.81
Batch: 60; loss: 1.17; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.91
Batch: 100; loss: 1.15; acc: 0.77
Batch: 120; loss: 1.34; acc: 0.66
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.120486812986386; val_accuracy: 0.7226313694267515 

The current subspace-distance is: 0.0001460454222979024 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.16; acc: 0.67
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 1.1; acc: 0.77
Batch: 60; loss: 1.38; acc: 0.59
Batch: 80; loss: 0.98; acc: 0.78
Batch: 100; loss: 1.2; acc: 0.69
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.73
Batch: 160; loss: 1.16; acc: 0.67
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.28; acc: 0.59
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.14; acc: 0.69
Batch: 260; loss: 1.27; acc: 0.66
Batch: 280; loss: 1.24; acc: 0.66
Batch: 300; loss: 1.03; acc: 0.73
Batch: 320; loss: 1.27; acc: 0.64
Batch: 340; loss: 1.28; acc: 0.64
Batch: 360; loss: 1.06; acc: 0.72
Batch: 380; loss: 1.17; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.69
Batch: 420; loss: 1.08; acc: 0.72
Batch: 440; loss: 1.22; acc: 0.67
Batch: 460; loss: 1.23; acc: 0.67
Batch: 480; loss: 1.14; acc: 0.69
Batch: 500; loss: 1.26; acc: 0.66
Batch: 520; loss: 1.1; acc: 0.8
Batch: 540; loss: 1.37; acc: 0.61
Batch: 560; loss: 1.15; acc: 0.75
Batch: 580; loss: 1.12; acc: 0.77
Batch: 600; loss: 1.01; acc: 0.83
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 1.07; acc: 0.73
Batch: 660; loss: 1.2; acc: 0.62
Batch: 680; loss: 1.11; acc: 0.75
Batch: 700; loss: 1.26; acc: 0.69
Batch: 720; loss: 1.21; acc: 0.69
Batch: 740; loss: 1.24; acc: 0.66
Batch: 760; loss: 1.21; acc: 0.69
Batch: 780; loss: 1.16; acc: 0.72
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00015170815458986908
0.0001470221031922847
Batch: 0; loss: 1.24; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.83
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.93; acc: 0.91
Batch: 100; loss: 1.13; acc: 0.78
Batch: 120; loss: 1.33; acc: 0.66
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 1.1198396845987648; val_accuracy: 0.7261146496815286 

The current subspace-distance is: 0.0001470221031922847 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.77
Batch: 20; loss: 1.04; acc: 0.77
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.17; acc: 0.64
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 1.29; acc: 0.62
Batch: 120; loss: 1.23; acc: 0.7
Batch: 140; loss: 1.17; acc: 0.72
Batch: 160; loss: 1.19; acc: 0.61
Batch: 180; loss: 1.17; acc: 0.78
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 1.35; acc: 0.61
Batch: 240; loss: 1.17; acc: 0.73
Batch: 260; loss: 1.18; acc: 0.62
Batch: 280; loss: 1.02; acc: 0.73
Batch: 300; loss: 1.19; acc: 0.66
Batch: 320; loss: 1.16; acc: 0.62
Batch: 340; loss: 1.26; acc: 0.66
Batch: 360; loss: 1.12; acc: 0.72
Batch: 380; loss: 1.29; acc: 0.53
Batch: 400; loss: 1.17; acc: 0.72
Batch: 420; loss: 1.06; acc: 0.72
Batch: 440; loss: 1.07; acc: 0.84
Batch: 460; loss: 1.25; acc: 0.66
Batch: 480; loss: 1.23; acc: 0.72
Batch: 500; loss: 1.27; acc: 0.69
Batch: 520; loss: 1.11; acc: 0.78
Batch: 540; loss: 1.12; acc: 0.77
Batch: 560; loss: 1.1; acc: 0.73
Batch: 580; loss: 1.03; acc: 0.8
Batch: 600; loss: 1.22; acc: 0.64
Batch: 620; loss: 1.08; acc: 0.73
Batch: 640; loss: 1.08; acc: 0.77
Batch: 660; loss: 1.34; acc: 0.62
Batch: 680; loss: 1.22; acc: 0.66
Batch: 700; loss: 1.18; acc: 0.72
Batch: 720; loss: 1.11; acc: 0.73
Batch: 740; loss: 1.31; acc: 0.67
Batch: 760; loss: 1.21; acc: 0.72
Batch: 780; loss: 1.33; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00015321352111641318
0.00014576956164091825
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 0.91; acc: 0.91
Batch: 100; loss: 1.13; acc: 0.78
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.1066503585523861; val_accuracy: 0.7263136942675159 

The current subspace-distance is: 0.00014576956164091825 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.09; acc: 0.77
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 1.14; acc: 0.67
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.64
Batch: 100; loss: 1.18; acc: 0.69
Batch: 120; loss: 1.11; acc: 0.77
Batch: 140; loss: 1.3; acc: 0.61
Batch: 160; loss: 1.15; acc: 0.66
Batch: 180; loss: 1.08; acc: 0.78
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 1.13; acc: 0.67
Batch: 240; loss: 1.14; acc: 0.77
Batch: 260; loss: 1.15; acc: 0.62
Batch: 280; loss: 1.11; acc: 0.7
Batch: 300; loss: 1.28; acc: 0.64
Batch: 320; loss: 1.28; acc: 0.59
Batch: 340; loss: 1.24; acc: 0.67
Batch: 360; loss: 1.19; acc: 0.69
Batch: 380; loss: 1.19; acc: 0.64
Batch: 400; loss: 1.24; acc: 0.64
Batch: 420; loss: 1.15; acc: 0.77
Batch: 440; loss: 1.26; acc: 0.69
Batch: 460; loss: 1.19; acc: 0.64
Batch: 480; loss: 0.99; acc: 0.78
Batch: 500; loss: 0.99; acc: 0.77
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.15; acc: 0.67
Batch: 560; loss: 1.25; acc: 0.66
Batch: 580; loss: 1.12; acc: 0.72
Batch: 600; loss: 1.1; acc: 0.75
Batch: 620; loss: 1.13; acc: 0.69
Batch: 640; loss: 1.25; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.72
Batch: 680; loss: 1.2; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.81
Batch: 720; loss: 1.33; acc: 0.62
Batch: 740; loss: 1.07; acc: 0.67
Batch: 760; loss: 1.09; acc: 0.73
Batch: 780; loss: 1.14; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00015043336316011846
0.0001414227590430528
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.59
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 1.15; acc: 0.64
Batch: 80; loss: 0.91; acc: 0.91
Batch: 100; loss: 1.14; acc: 0.77
Batch: 120; loss: 1.33; acc: 0.64
Batch: 140; loss: 0.95; acc: 0.78
Val Epoch over. val_loss: 1.1124292855050153; val_accuracy: 0.7205414012738853 

The current subspace-distance is: 0.0001414227590430528 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.69
Batch: 20; loss: 1.06; acc: 0.73
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 1.14; acc: 0.67
Batch: 80; loss: 1.25; acc: 0.64
Batch: 100; loss: 1.33; acc: 0.61
Batch: 120; loss: 1.16; acc: 0.77
Batch: 140; loss: 1.3; acc: 0.66
Batch: 160; loss: 1.31; acc: 0.62
Batch: 180; loss: 1.29; acc: 0.59
Batch: 200; loss: 1.14; acc: 0.72
Batch: 220; loss: 1.07; acc: 0.73
Batch: 240; loss: 1.22; acc: 0.69
Batch: 260; loss: 1.09; acc: 0.78
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.73
Batch: 320; loss: 1.4; acc: 0.62
Batch: 340; loss: 0.95; acc: 0.77
Batch: 360; loss: 1.04; acc: 0.73
Batch: 380; loss: 1.24; acc: 0.61
Batch: 400; loss: 1.23; acc: 0.66
Batch: 420; loss: 1.18; acc: 0.67
Batch: 440; loss: 1.3; acc: 0.66
Batch: 460; loss: 1.13; acc: 0.69
Batch: 480; loss: 1.21; acc: 0.64
Batch: 500; loss: 1.28; acc: 0.59
Batch: 520; loss: 1.25; acc: 0.61
Batch: 540; loss: 1.08; acc: 0.72
Batch: 560; loss: 1.26; acc: 0.59
Batch: 580; loss: 1.19; acc: 0.69
Batch: 600; loss: 1.3; acc: 0.61
Batch: 620; loss: 1.16; acc: 0.75
Batch: 640; loss: 1.32; acc: 0.56
Batch: 660; loss: 1.03; acc: 0.78
Batch: 680; loss: 1.12; acc: 0.77
Batch: 700; loss: 1.22; acc: 0.69
Batch: 720; loss: 1.11; acc: 0.73
Batch: 740; loss: 1.18; acc: 0.7
Batch: 760; loss: 1.18; acc: 0.69
Batch: 780; loss: 1.19; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00015180902846623212
0.00014495420327875763
Batch: 0; loss: 1.22; acc: 0.64
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 0.9; acc: 0.89
Batch: 100; loss: 1.13; acc: 0.77
Batch: 120; loss: 1.33; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.8
Val Epoch over. val_loss: 1.105811298652819; val_accuracy: 0.7246218152866242 

The current subspace-distance is: 0.00014495420327875763 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.72
Batch: 40; loss: 1.29; acc: 0.62
Batch: 60; loss: 1.19; acc: 0.66
Batch: 80; loss: 1.11; acc: 0.7
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.16; acc: 0.72
Batch: 140; loss: 1.38; acc: 0.61
Batch: 160; loss: 1.33; acc: 0.58
Batch: 180; loss: 1.14; acc: 0.7
Batch: 200; loss: 1.1; acc: 0.78
Batch: 220; loss: 1.24; acc: 0.69
Batch: 240; loss: 1.03; acc: 0.81
Batch: 260; loss: 1.4; acc: 0.58
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 1.11; acc: 0.67
Batch: 320; loss: 1.24; acc: 0.67
Batch: 340; loss: 1.21; acc: 0.67
Batch: 360; loss: 1.3; acc: 0.58
Batch: 380; loss: 1.18; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.66
Batch: 420; loss: 1.11; acc: 0.8
Batch: 440; loss: 1.23; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.75
Batch: 480; loss: 1.22; acc: 0.7
Batch: 500; loss: 1.08; acc: 0.7
Batch: 520; loss: 1.06; acc: 0.77
Batch: 540; loss: 1.13; acc: 0.78
Batch: 560; loss: 1.2; acc: 0.62
Batch: 580; loss: 1.1; acc: 0.78
Batch: 600; loss: 1.19; acc: 0.66
Batch: 620; loss: 1.08; acc: 0.67
Batch: 640; loss: 1.1; acc: 0.72
Batch: 660; loss: 1.24; acc: 0.69
Batch: 680; loss: 1.18; acc: 0.66
Batch: 700; loss: 0.99; acc: 0.69
Batch: 720; loss: 1.18; acc: 0.66
Batch: 740; loss: 0.98; acc: 0.77
Batch: 760; loss: 1.1; acc: 0.67
Batch: 780; loss: 1.28; acc: 0.64
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.00015584133507218212
0.00014770234702154994
Batch: 0; loss: 1.24; acc: 0.66
Batch: 20; loss: 1.25; acc: 0.62
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 0.9; acc: 0.89
Batch: 100; loss: 1.15; acc: 0.78
Batch: 120; loss: 1.35; acc: 0.62
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.1135552381254306; val_accuracy: 0.7223328025477707 

The current subspace-distance is: 0.00014770234702154994 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.31; acc: 0.61
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.34; acc: 0.59
Batch: 80; loss: 1.25; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 1.41; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.59
Batch: 180; loss: 1.22; acc: 0.64
Batch: 200; loss: 1.06; acc: 0.7
Batch: 220; loss: 1.15; acc: 0.67
Batch: 240; loss: 1.42; acc: 0.55
Batch: 260; loss: 1.23; acc: 0.7
Batch: 280; loss: 1.16; acc: 0.69
Batch: 300; loss: 1.33; acc: 0.53
Batch: 320; loss: 1.12; acc: 0.73
Batch: 340; loss: 1.17; acc: 0.7
Batch: 360; loss: 1.09; acc: 0.69
Batch: 380; loss: 1.25; acc: 0.64
Batch: 400; loss: 1.08; acc: 0.77
Batch: 420; loss: 1.29; acc: 0.58
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.18; acc: 0.69
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 1.24; acc: 0.72
Batch: 540; loss: 1.17; acc: 0.67
Batch: 560; loss: 0.93; acc: 0.86
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.0; acc: 0.75
Batch: 620; loss: 1.25; acc: 0.67
Batch: 640; loss: 1.29; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.72
Batch: 680; loss: 1.06; acc: 0.78
Batch: 700; loss: 1.02; acc: 0.75
Batch: 720; loss: 1.24; acc: 0.67
Batch: 740; loss: 1.11; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.7
Batch: 780; loss: 1.37; acc: 0.58
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.00015506791532970965
0.00014873064355924726
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 0.91; acc: 0.88
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.33; acc: 0.62
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.1011358727315428; val_accuracy: 0.7248208598726115 

The current subspace-distance is: 0.00014873064355924726 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.05; acc: 0.75
Batch: 20; loss: 1.27; acc: 0.62
Batch: 40; loss: 1.29; acc: 0.53
Batch: 60; loss: 1.2; acc: 0.67
Batch: 80; loss: 1.29; acc: 0.62
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 1.1; acc: 0.72
Batch: 160; loss: 1.22; acc: 0.62
Batch: 180; loss: 1.34; acc: 0.64
Batch: 200; loss: 1.12; acc: 0.75
Batch: 220; loss: 1.06; acc: 0.75
Batch: 240; loss: 1.37; acc: 0.69
Batch: 260; loss: 1.24; acc: 0.66
Batch: 280; loss: 1.13; acc: 0.69
Batch: 300; loss: 1.25; acc: 0.7
Batch: 320; loss: 1.22; acc: 0.69
Batch: 340; loss: 1.2; acc: 0.7
Batch: 360; loss: 1.27; acc: 0.62
Batch: 380; loss: 1.36; acc: 0.64
Batch: 400; loss: 1.29; acc: 0.61
Batch: 420; loss: 1.26; acc: 0.7
Batch: 440; loss: 1.14; acc: 0.69
Batch: 460; loss: 1.16; acc: 0.7
Batch: 480; loss: 1.03; acc: 0.81
Batch: 500; loss: 1.21; acc: 0.69
Batch: 520; loss: 1.05; acc: 0.73
Batch: 540; loss: 1.4; acc: 0.56
Batch: 560; loss: 1.14; acc: 0.66
Batch: 580; loss: 1.48; acc: 0.56
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 1.13; acc: 0.72
Batch: 640; loss: 1.24; acc: 0.61
Batch: 660; loss: 1.07; acc: 0.69
Batch: 680; loss: 1.07; acc: 0.73
Batch: 700; loss: 1.18; acc: 0.69
Batch: 720; loss: 1.08; acc: 0.73
Batch: 740; loss: 1.22; acc: 0.72
Batch: 760; loss: 1.21; acc: 0.67
Batch: 780; loss: 1.11; acc: 0.7
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.00015206435637082905
0.00014766452659387141
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.91; acc: 0.91
Batch: 100; loss: 1.13; acc: 0.77
Batch: 120; loss: 1.34; acc: 0.62
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.1132361323210844; val_accuracy: 0.721437101910828 

The current subspace-distance is: 0.00014766452659387141 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.08; acc: 0.73
Batch: 20; loss: 1.08; acc: 0.73
Batch: 40; loss: 1.19; acc: 0.66
Batch: 60; loss: 1.27; acc: 0.58
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.06; acc: 0.75
Batch: 120; loss: 1.2; acc: 0.67
Batch: 140; loss: 1.05; acc: 0.73
Batch: 160; loss: 0.99; acc: 0.72
Batch: 180; loss: 1.18; acc: 0.7
Batch: 200; loss: 1.32; acc: 0.61
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.19; acc: 0.69
Batch: 260; loss: 1.18; acc: 0.62
Batch: 280; loss: 1.28; acc: 0.73
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 1.24; acc: 0.66
Batch: 340; loss: 1.29; acc: 0.64
Batch: 360; loss: 1.01; acc: 0.78
Batch: 380; loss: 1.02; acc: 0.81
Batch: 400; loss: 1.22; acc: 0.61
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.17; acc: 0.72
Batch: 460; loss: 1.16; acc: 0.7
Batch: 480; loss: 1.29; acc: 0.7
Batch: 500; loss: 0.96; acc: 0.81
Batch: 520; loss: 1.23; acc: 0.66
Batch: 540; loss: 1.22; acc: 0.64
Batch: 560; loss: 1.11; acc: 0.69
Batch: 580; loss: 1.12; acc: 0.73
Batch: 600; loss: 1.42; acc: 0.61
Batch: 620; loss: 1.28; acc: 0.58
Batch: 640; loss: 1.09; acc: 0.7
Batch: 660; loss: 1.34; acc: 0.59
Batch: 680; loss: 1.04; acc: 0.75
Batch: 700; loss: 1.18; acc: 0.75
Batch: 720; loss: 1.12; acc: 0.72
Batch: 740; loss: 1.04; acc: 0.75
Batch: 760; loss: 1.26; acc: 0.69
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.0001538392243674025
0.00014642317546531558
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 0.9; acc: 0.89
Batch: 100; loss: 1.12; acc: 0.78
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 0.94; acc: 0.77
Val Epoch over. val_loss: 1.1033701889074532; val_accuracy: 0.720640923566879 

The current subspace-distance is: 0.00014642317546531558 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.07; acc: 0.73
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 1.09; acc: 0.72
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 1.43; acc: 0.61
Batch: 100; loss: 1.13; acc: 0.75
Batch: 120; loss: 1.07; acc: 0.66
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 0.92; acc: 0.84
Batch: 180; loss: 1.12; acc: 0.75
Batch: 200; loss: 1.14; acc: 0.64
Batch: 220; loss: 1.01; acc: 0.75
Batch: 240; loss: 1.26; acc: 0.67
Batch: 260; loss: 1.28; acc: 0.59
Batch: 280; loss: 1.06; acc: 0.72
Batch: 300; loss: 1.16; acc: 0.7
Batch: 320; loss: 1.13; acc: 0.67
Batch: 340; loss: 1.05; acc: 0.77
Batch: 360; loss: 1.17; acc: 0.77
Batch: 380; loss: 1.23; acc: 0.69
Batch: 400; loss: 1.21; acc: 0.67
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 1.24; acc: 0.67
Batch: 480; loss: 1.18; acc: 0.69
Batch: 500; loss: 1.22; acc: 0.67
Batch: 520; loss: 1.24; acc: 0.61
Batch: 540; loss: 1.15; acc: 0.73
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.35; acc: 0.59
Batch: 600; loss: 1.23; acc: 0.7
Batch: 620; loss: 1.12; acc: 0.72
Batch: 640; loss: 1.15; acc: 0.69
Batch: 660; loss: 1.34; acc: 0.58
Batch: 680; loss: 1.32; acc: 0.64
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.29; acc: 0.61
Batch: 760; loss: 1.11; acc: 0.64
Batch: 780; loss: 1.22; acc: 0.7
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.00015639117918908596
0.00014936659135855734
Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.83
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 0.91; acc: 0.89
Batch: 100; loss: 1.14; acc: 0.77
Batch: 120; loss: 1.34; acc: 0.59
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 1.1119688749313354; val_accuracy: 0.7220342356687898 

The current subspace-distance is: 0.00014936659135855734 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_9_flips_False_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.1990338787749493

The number of parameters is: 264111

The number of individual parameters is:

18
324
18
18
27
42282
27
27
53
124497
53
53
64
91584
64
64
4096
64
640
10
64
64

nonzero elements in E: 52822196
elements in E: 52822200
fraction nonzero: 0.9999999242742634
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.4; acc: 0.06
Batch: 20; loss: 2.16; acc: 0.22
Batch: 40; loss: 2.1; acc: 0.23
Batch: 60; loss: 2.04; acc: 0.25
Batch: 80; loss: 2.04; acc: 0.31
Batch: 100; loss: 1.93; acc: 0.42
Batch: 120; loss: 1.9; acc: 0.52
Batch: 140; loss: 1.91; acc: 0.39
Batch: 160; loss: 1.78; acc: 0.53
Batch: 180; loss: 1.8; acc: 0.53
Batch: 200; loss: 1.79; acc: 0.5
Batch: 220; loss: 1.82; acc: 0.47
Batch: 240; loss: 1.6; acc: 0.61
Batch: 260; loss: 1.59; acc: 0.66
Batch: 280; loss: 1.57; acc: 0.66
Batch: 300; loss: 1.59; acc: 0.7
Batch: 320; loss: 1.56; acc: 0.7
Batch: 340; loss: 1.58; acc: 0.72
Batch: 360; loss: 1.64; acc: 0.58
Batch: 380; loss: 1.57; acc: 0.67
Batch: 400; loss: 1.57; acc: 0.69
Batch: 420; loss: 1.5; acc: 0.7
Batch: 440; loss: 1.52; acc: 0.67
Batch: 460; loss: 1.52; acc: 0.69
Batch: 480; loss: 1.59; acc: 0.59
Batch: 500; loss: 1.46; acc: 0.69
Batch: 520; loss: 1.58; acc: 0.64
Batch: 540; loss: 1.46; acc: 0.73
Batch: 560; loss: 1.5; acc: 0.62
Batch: 580; loss: 1.41; acc: 0.73
Batch: 600; loss: 1.38; acc: 0.72
Batch: 620; loss: 1.52; acc: 0.69
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.39; acc: 0.72
Batch: 680; loss: 1.36; acc: 0.78
Batch: 700; loss: 1.46; acc: 0.73
Batch: 720; loss: 1.39; acc: 0.75
Batch: 740; loss: 1.46; acc: 0.72
Batch: 760; loss: 1.43; acc: 0.75
Batch: 780; loss: 1.42; acc: 0.7
Train Epoch over. train_loss: 1.65; train_accuracy: 0.58 

5.495577715919353e-05
5.03442352055572e-05
Batch: 0; loss: 1.38; acc: 0.69
Batch: 20; loss: 1.55; acc: 0.69
Batch: 40; loss: 1.19; acc: 0.83
Batch: 60; loss: 1.39; acc: 0.75
Batch: 80; loss: 1.26; acc: 0.84
Batch: 100; loss: 1.39; acc: 0.78
Batch: 120; loss: 1.49; acc: 0.64
Batch: 140; loss: 1.37; acc: 0.73
Val Epoch over. val_loss: 1.3785016635420975; val_accuracy: 0.7323845541401274 

The current subspace-distance is: 5.03442352055572e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.77
Batch: 20; loss: 1.38; acc: 0.73
Batch: 40; loss: 1.42; acc: 0.67
Batch: 60; loss: 1.33; acc: 0.78
Batch: 80; loss: 1.43; acc: 0.77
Batch: 100; loss: 1.29; acc: 0.77
Batch: 120; loss: 1.38; acc: 0.66
Batch: 140; loss: 1.5; acc: 0.64
Batch: 160; loss: 1.43; acc: 0.67
Batch: 180; loss: 1.46; acc: 0.61
Batch: 200; loss: 1.29; acc: 0.77
Batch: 220; loss: 1.42; acc: 0.72
Batch: 240; loss: 1.3; acc: 0.73
Batch: 260; loss: 1.41; acc: 0.7
Batch: 280; loss: 1.38; acc: 0.66
Batch: 300; loss: 1.26; acc: 0.8
Batch: 320; loss: 1.42; acc: 0.69
Batch: 340; loss: 1.33; acc: 0.73
Batch: 360; loss: 1.31; acc: 0.78
Batch: 380; loss: 1.29; acc: 0.75
Batch: 400; loss: 1.21; acc: 0.86
Batch: 420; loss: 1.33; acc: 0.69
Batch: 440; loss: 1.25; acc: 0.69
Batch: 460; loss: 1.25; acc: 0.75
Batch: 480; loss: 1.07; acc: 0.91
Batch: 500; loss: 1.28; acc: 0.75
Batch: 520; loss: 1.32; acc: 0.7
Batch: 540; loss: 1.27; acc: 0.77
Batch: 560; loss: 1.27; acc: 0.73
Batch: 580; loss: 1.33; acc: 0.73
Batch: 600; loss: 1.29; acc: 0.73
Batch: 620; loss: 1.29; acc: 0.75
Batch: 640; loss: 1.33; acc: 0.66
Batch: 660; loss: 1.22; acc: 0.81
Batch: 680; loss: 1.26; acc: 0.77
Batch: 700; loss: 1.07; acc: 0.88
Batch: 720; loss: 1.18; acc: 0.72
Batch: 740; loss: 1.28; acc: 0.75
Batch: 760; loss: 1.25; acc: 0.73
Batch: 780; loss: 1.31; acc: 0.77
Train Epoch over. train_loss: 1.3; train_accuracy: 0.74 

7.969958824105561e-05
7.485051901312545e-05
Batch: 0; loss: 1.15; acc: 0.8
Batch: 20; loss: 1.35; acc: 0.64
Batch: 40; loss: 0.96; acc: 0.92
Batch: 60; loss: 1.13; acc: 0.78
Batch: 80; loss: 0.98; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.8
Batch: 120; loss: 1.29; acc: 0.7
Batch: 140; loss: 1.15; acc: 0.78
Val Epoch over. val_loss: 1.1668963595560402; val_accuracy: 0.7892117834394905 

The current subspace-distance is: 7.485051901312545e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.84
Batch: 20; loss: 1.17; acc: 0.78
Batch: 40; loss: 1.16; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.64
Batch: 80; loss: 1.12; acc: 0.81
Batch: 100; loss: 1.14; acc: 0.83
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 1.13; acc: 0.83
Batch: 160; loss: 1.14; acc: 0.83
Batch: 180; loss: 1.22; acc: 0.78
Batch: 200; loss: 1.32; acc: 0.69
Batch: 220; loss: 1.23; acc: 0.7
Batch: 240; loss: 1.15; acc: 0.77
Batch: 260; loss: 1.27; acc: 0.72
Batch: 280; loss: 1.16; acc: 0.81
Batch: 300; loss: 1.09; acc: 0.83
Batch: 320; loss: 1.2; acc: 0.75
Batch: 340; loss: 1.21; acc: 0.75
Batch: 360; loss: 1.1; acc: 0.78
Batch: 380; loss: 1.19; acc: 0.73
Batch: 400; loss: 1.08; acc: 0.83
Batch: 420; loss: 1.11; acc: 0.72
Batch: 440; loss: 1.09; acc: 0.83
Batch: 460; loss: 1.05; acc: 0.75
Batch: 480; loss: 1.07; acc: 0.83
Batch: 500; loss: 1.19; acc: 0.78
Batch: 520; loss: 1.02; acc: 0.8
Batch: 540; loss: 1.14; acc: 0.75
Batch: 560; loss: 1.06; acc: 0.77
Batch: 580; loss: 1.08; acc: 0.8
Batch: 600; loss: 1.25; acc: 0.67
Batch: 620; loss: 1.01; acc: 0.8
Batch: 640; loss: 1.05; acc: 0.81
Batch: 660; loss: 1.23; acc: 0.69
Batch: 680; loss: 1.21; acc: 0.72
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.16; acc: 0.73
Batch: 740; loss: 1.07; acc: 0.86
Batch: 760; loss: 1.05; acc: 0.84
Batch: 780; loss: 0.96; acc: 0.86
Train Epoch over. train_loss: 1.14; train_accuracy: 0.78 

9.737777145346627e-05
9.222680819220841e-05
Batch: 0; loss: 1.05; acc: 0.84
Batch: 20; loss: 1.23; acc: 0.7
Batch: 40; loss: 0.82; acc: 0.89
Batch: 60; loss: 1.0; acc: 0.84
Batch: 80; loss: 0.85; acc: 0.92
Batch: 100; loss: 1.04; acc: 0.78
Batch: 120; loss: 1.16; acc: 0.77
Batch: 140; loss: 0.97; acc: 0.83
Val Epoch over. val_loss: 1.0379400515252617; val_accuracy: 0.816281847133758 

The current subspace-distance is: 9.222680819220841e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.81
Batch: 20; loss: 0.97; acc: 0.91
Batch: 40; loss: 1.22; acc: 0.73
Batch: 60; loss: 1.09; acc: 0.8
Batch: 80; loss: 1.17; acc: 0.72
Batch: 100; loss: 1.05; acc: 0.83
Batch: 120; loss: 1.17; acc: 0.73
Batch: 140; loss: 1.16; acc: 0.73
Batch: 160; loss: 1.03; acc: 0.8
Batch: 180; loss: 1.1; acc: 0.83
Batch: 200; loss: 1.05; acc: 0.84
Batch: 220; loss: 0.92; acc: 0.88
Batch: 240; loss: 1.14; acc: 0.73
Batch: 260; loss: 1.14; acc: 0.78
Batch: 280; loss: 1.01; acc: 0.77
Batch: 300; loss: 0.95; acc: 0.84
Batch: 320; loss: 1.17; acc: 0.69
Batch: 340; loss: 0.98; acc: 0.81
Batch: 360; loss: 1.06; acc: 0.78
Batch: 380; loss: 1.03; acc: 0.77
Batch: 400; loss: 0.94; acc: 0.86
Batch: 420; loss: 0.98; acc: 0.86
Batch: 440; loss: 0.99; acc: 0.81
Batch: 460; loss: 1.08; acc: 0.73
Batch: 480; loss: 1.09; acc: 0.75
Batch: 500; loss: 1.07; acc: 0.77
Batch: 520; loss: 1.03; acc: 0.83
Batch: 540; loss: 1.17; acc: 0.66
Batch: 560; loss: 1.05; acc: 0.77
Batch: 580; loss: 1.04; acc: 0.77
Batch: 600; loss: 1.11; acc: 0.77
Batch: 620; loss: 0.94; acc: 0.86
Batch: 640; loss: 1.0; acc: 0.81
Batch: 660; loss: 1.03; acc: 0.8
Batch: 680; loss: 0.91; acc: 0.86
Batch: 700; loss: 1.05; acc: 0.8
Batch: 720; loss: 0.98; acc: 0.8
Batch: 740; loss: 1.0; acc: 0.83
Batch: 760; loss: 1.16; acc: 0.75
Batch: 780; loss: 1.05; acc: 0.78
Train Epoch over. train_loss: 1.05; train_accuracy: 0.8 

0.00011314021685393527
0.00010816059511853382
Batch: 0; loss: 1.01; acc: 0.89
Batch: 20; loss: 1.18; acc: 0.69
Batch: 40; loss: 0.75; acc: 0.91
Batch: 60; loss: 0.95; acc: 0.89
Batch: 80; loss: 0.78; acc: 0.94
Batch: 100; loss: 0.98; acc: 0.8
Batch: 120; loss: 1.09; acc: 0.75
Batch: 140; loss: 0.84; acc: 0.88
Val Epoch over. val_loss: 0.9766311030478994; val_accuracy: 0.8203622611464968 

The current subspace-distance is: 0.00010816059511853382 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.88
Batch: 20; loss: 0.98; acc: 0.78
Batch: 40; loss: 1.0; acc: 0.78
Batch: 60; loss: 1.05; acc: 0.77
Batch: 80; loss: 0.95; acc: 0.75
Batch: 100; loss: 1.06; acc: 0.78
Batch: 120; loss: 1.0; acc: 0.78
Batch: 140; loss: 1.07; acc: 0.8
Batch: 160; loss: 1.05; acc: 0.81
Batch: 180; loss: 0.96; acc: 0.77
Batch: 200; loss: 0.96; acc: 0.78
Batch: 220; loss: 1.03; acc: 0.78
Batch: 240; loss: 1.05; acc: 0.73
Batch: 260; loss: 0.85; acc: 0.92
Batch: 280; loss: 1.09; acc: 0.75
Batch: 300; loss: 1.02; acc: 0.75
Batch: 320; loss: 1.02; acc: 0.78
Batch: 340; loss: 0.93; acc: 0.81
Batch: 360; loss: 0.99; acc: 0.78
Batch: 380; loss: 1.03; acc: 0.78
Batch: 400; loss: 0.95; acc: 0.86
Batch: 420; loss: 0.91; acc: 0.84
Batch: 440; loss: 0.91; acc: 0.78
Batch: 460; loss: 0.95; acc: 0.83
Batch: 480; loss: 1.03; acc: 0.78
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 1.01; acc: 0.78
Batch: 540; loss: 0.92; acc: 0.86
Batch: 560; loss: 1.21; acc: 0.73
Batch: 580; loss: 0.92; acc: 0.84
Batch: 600; loss: 1.01; acc: 0.78
Batch: 620; loss: 0.89; acc: 0.83
Batch: 640; loss: 0.96; acc: 0.81
Batch: 660; loss: 0.87; acc: 0.81
Batch: 680; loss: 0.99; acc: 0.75
Batch: 700; loss: 1.04; acc: 0.78
Batch: 720; loss: 0.92; acc: 0.84
Batch: 740; loss: 0.99; acc: 0.77
Batch: 760; loss: 0.91; acc: 0.83
Batch: 780; loss: 0.82; acc: 0.89
Train Epoch over. train_loss: 0.99; train_accuracy: 0.8 

0.0001274850219488144
0.00012189882545499131
Batch: 0; loss: 0.98; acc: 0.84
Batch: 20; loss: 1.17; acc: 0.69
Batch: 40; loss: 0.68; acc: 0.89
Batch: 60; loss: 0.91; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.92
Batch: 100; loss: 0.96; acc: 0.81
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 0.74; acc: 0.89
Val Epoch over. val_loss: 0.9249723299293761; val_accuracy: 0.818172770700637 

The current subspace-distance is: 0.00012189882545499131 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.75
Batch: 20; loss: 0.97; acc: 0.75
Batch: 40; loss: 0.97; acc: 0.86
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.9; acc: 0.86
Batch: 100; loss: 0.94; acc: 0.83
Batch: 120; loss: 0.99; acc: 0.83
Batch: 140; loss: 0.91; acc: 0.81
Batch: 160; loss: 0.89; acc: 0.8
Batch: 180; loss: 1.0; acc: 0.78
Batch: 200; loss: 0.97; acc: 0.81
Batch: 220; loss: 0.97; acc: 0.77
Batch: 240; loss: 0.88; acc: 0.83
Batch: 260; loss: 1.01; acc: 0.8
Batch: 280; loss: 0.99; acc: 0.72
Batch: 300; loss: 0.91; acc: 0.83
Batch: 320; loss: 0.94; acc: 0.78
Batch: 340; loss: 1.05; acc: 0.8
Batch: 360; loss: 0.94; acc: 0.77
Batch: 380; loss: 0.84; acc: 0.83
Batch: 400; loss: 0.89; acc: 0.8
Batch: 420; loss: 0.96; acc: 0.81
Batch: 440; loss: 0.93; acc: 0.83
Batch: 460; loss: 0.94; acc: 0.78
Batch: 480; loss: 0.94; acc: 0.8
Batch: 500; loss: 0.92; acc: 0.81
Batch: 520; loss: 0.97; acc: 0.8
Batch: 540; loss: 0.85; acc: 0.81
Batch: 560; loss: 0.95; acc: 0.8
Batch: 580; loss: 0.89; acc: 0.78
Batch: 600; loss: 0.89; acc: 0.81
Batch: 620; loss: 1.09; acc: 0.77
Batch: 640; loss: 0.88; acc: 0.8
Batch: 660; loss: 0.94; acc: 0.86
Batch: 680; loss: 0.93; acc: 0.77
Batch: 700; loss: 0.82; acc: 0.84
Batch: 720; loss: 0.98; acc: 0.8
Batch: 740; loss: 0.9; acc: 0.84
Batch: 760; loss: 0.85; acc: 0.86
Batch: 780; loss: 0.98; acc: 0.77
Train Epoch over. train_loss: 0.94; train_accuracy: 0.8 

0.0001409073593094945
0.00013457355089485645
Batch: 0; loss: 0.95; acc: 0.86
Batch: 20; loss: 1.13; acc: 0.67
Batch: 40; loss: 0.63; acc: 0.91
Batch: 60; loss: 0.88; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.91
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 0.65; acc: 0.91
Val Epoch over. val_loss: 0.8828258723210377; val_accuracy: 0.8180732484076433 

The current subspace-distance is: 0.00013457355089485645 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.78
Batch: 20; loss: 1.0; acc: 0.77
Batch: 40; loss: 0.99; acc: 0.75
Batch: 60; loss: 0.95; acc: 0.81
Batch: 80; loss: 0.9; acc: 0.78
Batch: 100; loss: 0.87; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 0.99; acc: 0.78
Batch: 160; loss: 0.86; acc: 0.84
Batch: 180; loss: 1.03; acc: 0.8
Batch: 200; loss: 0.92; acc: 0.84
Batch: 220; loss: 0.91; acc: 0.72
Batch: 240; loss: 1.04; acc: 0.78
Batch: 260; loss: 0.95; acc: 0.77
Batch: 280; loss: 1.04; acc: 0.7
Batch: 300; loss: 0.93; acc: 0.83
Batch: 320; loss: 0.89; acc: 0.81
Batch: 340; loss: 0.93; acc: 0.8
Batch: 360; loss: 0.94; acc: 0.81
Batch: 380; loss: 0.93; acc: 0.78
Batch: 400; loss: 0.78; acc: 0.89
Batch: 420; loss: 0.98; acc: 0.78
Batch: 440; loss: 0.89; acc: 0.84
Batch: 460; loss: 0.85; acc: 0.78
Batch: 480; loss: 1.0; acc: 0.73
Batch: 500; loss: 0.86; acc: 0.83
Batch: 520; loss: 0.96; acc: 0.78
Batch: 540; loss: 0.94; acc: 0.75
Batch: 560; loss: 1.02; acc: 0.73
Batch: 580; loss: 0.78; acc: 0.83
Batch: 600; loss: 0.92; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.81
Batch: 640; loss: 1.01; acc: 0.8
Batch: 660; loss: 1.01; acc: 0.75
Batch: 680; loss: 0.75; acc: 0.86
Batch: 700; loss: 1.05; acc: 0.67
Batch: 720; loss: 0.81; acc: 0.83
Batch: 740; loss: 0.89; acc: 0.84
Batch: 760; loss: 0.89; acc: 0.8
Batch: 780; loss: 0.93; acc: 0.8
Train Epoch over. train_loss: 0.9; train_accuracy: 0.8 

0.0001530592708149925
0.0001470276474719867
Batch: 0; loss: 0.93; acc: 0.84
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.84; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.94
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 0.6; acc: 0.92
Val Epoch over. val_loss: 0.8517509490061718; val_accuracy: 0.8222531847133758 

The current subspace-distance is: 0.0001470276474719867 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.88
Batch: 40; loss: 0.95; acc: 0.75
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 0.89; acc: 0.8
Batch: 100; loss: 0.84; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.83
Batch: 140; loss: 1.02; acc: 0.75
Batch: 160; loss: 0.89; acc: 0.78
Batch: 180; loss: 0.99; acc: 0.72
Batch: 200; loss: 0.88; acc: 0.8
Batch: 220; loss: 0.87; acc: 0.81
Batch: 240; loss: 0.78; acc: 0.86
Batch: 260; loss: 0.8; acc: 0.86
Batch: 280; loss: 1.07; acc: 0.75
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.74; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.89; acc: 0.77
Batch: 380; loss: 0.93; acc: 0.77
Batch: 400; loss: 0.69; acc: 0.91
Batch: 420; loss: 0.95; acc: 0.83
Batch: 440; loss: 0.9; acc: 0.81
Batch: 460; loss: 0.88; acc: 0.83
Batch: 480; loss: 0.85; acc: 0.8
Batch: 500; loss: 0.87; acc: 0.83
Batch: 520; loss: 0.98; acc: 0.77
Batch: 540; loss: 0.95; acc: 0.73
Batch: 560; loss: 0.79; acc: 0.84
Batch: 580; loss: 0.92; acc: 0.75
Batch: 600; loss: 0.83; acc: 0.8
Batch: 620; loss: 0.91; acc: 0.75
Batch: 640; loss: 0.79; acc: 0.84
Batch: 660; loss: 0.84; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.84
Batch: 700; loss: 0.9; acc: 0.81
Batch: 720; loss: 0.83; acc: 0.8
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.84
Batch: 780; loss: 0.79; acc: 0.8
Train Epoch over. train_loss: 0.86; train_accuracy: 0.8 

0.0001612589112482965
0.00015563095803372562
Batch: 0; loss: 0.88; acc: 0.81
Batch: 20; loss: 1.04; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.92
Batch: 60; loss: 0.8; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.86; acc: 0.72
Batch: 120; loss: 1.02; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.89
Val Epoch over. val_loss: 0.811105114639185; val_accuracy: 0.821656050955414 

The current subspace-distance is: 0.00015563095803372562 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.84
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 0.91; acc: 0.73
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.77; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.81
Batch: 140; loss: 0.81; acc: 0.81
Batch: 160; loss: 0.83; acc: 0.78
Batch: 180; loss: 0.95; acc: 0.73
Batch: 200; loss: 0.89; acc: 0.78
Batch: 220; loss: 1.0; acc: 0.77
Batch: 240; loss: 0.8; acc: 0.86
Batch: 260; loss: 0.81; acc: 0.8
Batch: 280; loss: 0.87; acc: 0.77
Batch: 300; loss: 0.84; acc: 0.78
Batch: 320; loss: 0.87; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.93; acc: 0.73
Batch: 380; loss: 0.9; acc: 0.75
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.83; acc: 0.78
Batch: 440; loss: 0.76; acc: 0.88
Batch: 460; loss: 0.73; acc: 0.86
Batch: 480; loss: 0.87; acc: 0.83
Batch: 500; loss: 0.83; acc: 0.78
Batch: 520; loss: 0.76; acc: 0.81
Batch: 540; loss: 0.92; acc: 0.8
Batch: 560; loss: 0.79; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.84
Batch: 600; loss: 0.82; acc: 0.81
Batch: 620; loss: 0.8; acc: 0.83
Batch: 640; loss: 0.9; acc: 0.78
Batch: 660; loss: 0.78; acc: 0.8
Batch: 680; loss: 0.77; acc: 0.84
Batch: 700; loss: 0.9; acc: 0.78
Batch: 720; loss: 0.85; acc: 0.75
Batch: 740; loss: 0.93; acc: 0.78
Batch: 760; loss: 0.84; acc: 0.81
Batch: 780; loss: 0.85; acc: 0.8
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

0.00017395745089743286
0.00016572655295021832
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.75
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.7647770962138085; val_accuracy: 0.825437898089172 

The current subspace-distance is: 0.00016572655295021832 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.84
Batch: 20; loss: 0.88; acc: 0.8
Batch: 40; loss: 0.91; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.89
Batch: 80; loss: 0.84; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.88
Batch: 140; loss: 0.76; acc: 0.81
Batch: 160; loss: 0.79; acc: 0.83
Batch: 180; loss: 0.88; acc: 0.77
Batch: 200; loss: 0.82; acc: 0.78
Batch: 220; loss: 0.78; acc: 0.78
Batch: 240; loss: 0.68; acc: 0.89
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.85; acc: 0.8
Batch: 300; loss: 0.82; acc: 0.83
Batch: 320; loss: 0.71; acc: 0.86
Batch: 340; loss: 0.8; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.86
Batch: 380; loss: 0.78; acc: 0.83
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.88; acc: 0.77
Batch: 440; loss: 0.87; acc: 0.8
Batch: 460; loss: 0.84; acc: 0.77
Batch: 480; loss: 0.86; acc: 0.78
Batch: 500; loss: 0.82; acc: 0.78
Batch: 520; loss: 0.72; acc: 0.78
Batch: 540; loss: 1.06; acc: 0.67
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.72; acc: 0.86
Batch: 600; loss: 0.88; acc: 0.73
Batch: 620; loss: 0.67; acc: 0.86
Batch: 640; loss: 0.88; acc: 0.75
Batch: 660; loss: 0.77; acc: 0.84
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.87; acc: 0.73
Batch: 720; loss: 0.91; acc: 0.77
Batch: 740; loss: 0.66; acc: 0.88
Batch: 760; loss: 0.97; acc: 0.77
Batch: 780; loss: 0.88; acc: 0.8
Train Epoch over. train_loss: 0.8; train_accuracy: 0.81 

0.00018213383737020195
0.00017884628323372453
Batch: 0; loss: 0.81; acc: 0.81
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.57; acc: 0.91
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 0.46; acc: 0.89
Val Epoch over. val_loss: 0.7376297152346107; val_accuracy: 0.8277269108280255 

The current subspace-distance is: 0.00017884628323372453 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.79; acc: 0.84
Batch: 20; loss: 0.89; acc: 0.78
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.83; acc: 0.72
Batch: 180; loss: 0.7; acc: 0.88
Batch: 200; loss: 0.73; acc: 0.89
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.84; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.69; acc: 0.89
Batch: 340; loss: 0.88; acc: 0.78
Batch: 360; loss: 0.76; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.79; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.72; acc: 0.84
Batch: 460; loss: 0.97; acc: 0.72
Batch: 480; loss: 0.8; acc: 0.83
Batch: 500; loss: 0.58; acc: 0.89
Batch: 520; loss: 0.74; acc: 0.86
Batch: 540; loss: 0.84; acc: 0.8
Batch: 560; loss: 0.88; acc: 0.73
Batch: 580; loss: 0.83; acc: 0.83
Batch: 600; loss: 0.81; acc: 0.78
Batch: 620; loss: 0.72; acc: 0.84
Batch: 640; loss: 0.79; acc: 0.83
Batch: 660; loss: 0.76; acc: 0.83
Batch: 680; loss: 0.9; acc: 0.77
Batch: 700; loss: 0.63; acc: 0.91
Batch: 720; loss: 0.69; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.84
Batch: 760; loss: 0.85; acc: 0.8
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.78; train_accuracy: 0.81 

0.0001854095607995987
0.0001795414136722684
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 1.0; acc: 0.73
Batch: 140; loss: 0.46; acc: 0.89
Val Epoch over. val_loss: 0.7297948099625339; val_accuracy: 0.8285230891719745 

The current subspace-distance is: 0.0001795414136722684 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.76; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 0.81; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.78
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.65; acc: 0.89
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.77; acc: 0.83
Batch: 200; loss: 0.81; acc: 0.77
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.83; acc: 0.78
Batch: 260; loss: 0.7; acc: 0.89
Batch: 280; loss: 0.88; acc: 0.73
Batch: 300; loss: 0.81; acc: 0.8
Batch: 320; loss: 0.78; acc: 0.81
Batch: 340; loss: 0.76; acc: 0.8
Batch: 360; loss: 0.82; acc: 0.77
Batch: 380; loss: 0.86; acc: 0.78
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.85; acc: 0.77
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.87; acc: 0.78
Batch: 500; loss: 0.9; acc: 0.77
Batch: 520; loss: 0.8; acc: 0.8
Batch: 540; loss: 0.76; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.82; acc: 0.81
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.81; acc: 0.78
Batch: 700; loss: 0.81; acc: 0.8
Batch: 720; loss: 0.9; acc: 0.73
Batch: 740; loss: 0.82; acc: 0.8
Batch: 760; loss: 1.02; acc: 0.75
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.77; train_accuracy: 0.81 

0.00018700002692639828
0.00018260600336361676
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.89
Val Epoch over. val_loss: 0.7257350070081698; val_accuracy: 0.832703025477707 

The current subspace-distance is: 0.00018260600336361676 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.64; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.89
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 0.68; acc: 0.86
Batch: 80; loss: 0.87; acc: 0.77
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 1.0; acc: 0.72
Batch: 160; loss: 0.69; acc: 0.89
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.83; acc: 0.81
Batch: 220; loss: 0.85; acc: 0.75
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.91
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.75; acc: 0.81
Batch: 320; loss: 0.75; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.83
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.95; acc: 0.7
Batch: 400; loss: 0.73; acc: 0.83
Batch: 420; loss: 0.79; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.77; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.89
Batch: 500; loss: 0.79; acc: 0.83
Batch: 520; loss: 0.84; acc: 0.77
Batch: 540; loss: 0.7; acc: 0.86
Batch: 560; loss: 0.68; acc: 0.89
Batch: 580; loss: 0.73; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.78
Batch: 620; loss: 0.99; acc: 0.7
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.78; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.78; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.81 

0.00019248988246545196
0.00018614224973134696
Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.92
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.7118064550457487; val_accuracy: 0.8336982484076433 

The current subspace-distance is: 0.00018614224973134696 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.91
Batch: 80; loss: 0.76; acc: 0.81
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.7; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.89
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.85; acc: 0.75
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.84; acc: 0.8
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.65; acc: 0.89
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.88; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.85; acc: 0.8
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.88
Batch: 440; loss: 0.87; acc: 0.72
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.86; acc: 0.8
Batch: 500; loss: 0.86; acc: 0.77
Batch: 520; loss: 0.88; acc: 0.72
Batch: 540; loss: 0.59; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.92
Batch: 580; loss: 0.7; acc: 0.78
Batch: 600; loss: 0.79; acc: 0.81
Batch: 620; loss: 0.72; acc: 0.84
Batch: 640; loss: 0.87; acc: 0.8
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.73; acc: 0.77
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.72; acc: 0.88
Batch: 780; loss: 0.75; acc: 0.84
Train Epoch over. train_loss: 0.75; train_accuracy: 0.81 

0.00019689217151608318
0.00018653433653526008
Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.7070933643040384; val_accuracy: 0.8330015923566879 

The current subspace-distance is: 0.00018653433653526008 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.88
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.74; acc: 0.77
Batch: 160; loss: 0.83; acc: 0.78
Batch: 180; loss: 0.82; acc: 0.8
Batch: 200; loss: 0.8; acc: 0.78
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.92; acc: 0.72
Batch: 260; loss: 0.79; acc: 0.73
Batch: 280; loss: 0.86; acc: 0.73
Batch: 300; loss: 0.82; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.76; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.91
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.83; acc: 0.77
Batch: 420; loss: 0.79; acc: 0.78
Batch: 440; loss: 0.93; acc: 0.73
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.92
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.74; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.83
Batch: 580; loss: 0.91; acc: 0.72
Batch: 600; loss: 0.94; acc: 0.73
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.72; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.88
Batch: 680; loss: 0.81; acc: 0.72
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 0.86; acc: 0.75
Batch: 780; loss: 0.69; acc: 0.88
Train Epoch over. train_loss: 0.74; train_accuracy: 0.81 

0.00019894284196197987
0.00019090334535576403
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6848196609384695; val_accuracy: 0.8354896496815286 

The current subspace-distance is: 0.00019090334535576403 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.77; acc: 0.81
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.91; acc: 0.73
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.92
Batch: 140; loss: 0.81; acc: 0.73
Batch: 160; loss: 0.83; acc: 0.7
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.63; acc: 0.89
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.98; acc: 0.73
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.66; acc: 0.91
Batch: 300; loss: 0.89; acc: 0.66
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.77; acc: 0.84
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 0.78; acc: 0.83
Batch: 400; loss: 0.74; acc: 0.77
Batch: 420; loss: 0.79; acc: 0.77
Batch: 440; loss: 0.73; acc: 0.83
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 0.71; acc: 0.77
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.78; acc: 0.78
Batch: 540; loss: 0.78; acc: 0.81
Batch: 560; loss: 0.84; acc: 0.75
Batch: 580; loss: 0.79; acc: 0.78
Batch: 600; loss: 0.74; acc: 0.78
Batch: 620; loss: 0.72; acc: 0.83
Batch: 640; loss: 0.72; acc: 0.84
Batch: 660; loss: 0.65; acc: 0.84
Batch: 680; loss: 0.87; acc: 0.75
Batch: 700; loss: 0.63; acc: 0.8
Batch: 720; loss: 0.92; acc: 0.7
Batch: 740; loss: 0.58; acc: 0.92
Batch: 760; loss: 0.74; acc: 0.73
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.81 

0.0001978132058866322
0.00019130264990963042
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.87; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6832388058589522; val_accuracy: 0.8359872611464968 

The current subspace-distance is: 0.00019130264990963042 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.81; acc: 0.8
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.72; acc: 0.77
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.81
Batch: 160; loss: 0.76; acc: 0.84
Batch: 180; loss: 0.73; acc: 0.83
Batch: 200; loss: 0.92; acc: 0.69
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.83; acc: 0.78
Batch: 260; loss: 0.74; acc: 0.81
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.87; acc: 0.75
Batch: 360; loss: 0.74; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.91
Batch: 440; loss: 0.76; acc: 0.81
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.79; acc: 0.75
Batch: 500; loss: 0.65; acc: 0.86
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.8; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.64; acc: 0.89
Batch: 600; loss: 0.74; acc: 0.78
Batch: 620; loss: 0.73; acc: 0.84
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.76; acc: 0.81
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.85; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.81
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.73; train_accuracy: 0.82 

0.00020318696624599397
0.00019562676607165486
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.72; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.78
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.6731446172781052; val_accuracy: 0.8403662420382165 

The current subspace-distance is: 0.00019562676607165486 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.94; acc: 0.64
Batch: 100; loss: 0.81; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.47; acc: 0.94
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.83; acc: 0.72
Batch: 240; loss: 0.8; acc: 0.73
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.78
Batch: 300; loss: 0.85; acc: 0.73
Batch: 320; loss: 0.73; acc: 0.84
Batch: 340; loss: 0.75; acc: 0.75
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.77
Batch: 420; loss: 0.88; acc: 0.77
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.92
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.82; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.71; acc: 0.84
Batch: 580; loss: 0.79; acc: 0.77
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.87; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.69; acc: 0.86
Batch: 700; loss: 0.83; acc: 0.78
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.9; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.72; train_accuracy: 0.82 

0.00020201248116791248
0.00019715754024218768
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.85; acc: 0.73
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.78
Batch: 140; loss: 0.39; acc: 0.95
Val Epoch over. val_loss: 0.6638028188875527; val_accuracy: 0.841859076433121 

The current subspace-distance is: 0.00019715754024218768 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.84; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.73; acc: 0.83
Batch: 180; loss: 0.88; acc: 0.77
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.91
Batch: 260; loss: 0.76; acc: 0.78
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.74; acc: 0.86
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.91; acc: 0.73
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.79; acc: 0.7
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.79; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.78
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.69; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.86; acc: 0.8
Batch: 620; loss: 0.79; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.75
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.64; acc: 0.86
Batch: 760; loss: 0.77; acc: 0.81
Batch: 780; loss: 0.61; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.82 

0.00020704642520286143
0.00019855546997860074
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.84; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.97
Val Epoch over. val_loss: 0.659014214186152; val_accuracy: 0.8412619426751592 

The current subspace-distance is: 0.00019855546997860074 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.77; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.86; acc: 0.78
Batch: 160; loss: 0.62; acc: 0.89
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.82; acc: 0.77
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.72; acc: 0.8
Batch: 260; loss: 0.67; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.92
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.7; acc: 0.83
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.95; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.86
Batch: 400; loss: 0.56; acc: 0.92
Batch: 420; loss: 0.68; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.82; acc: 0.77
Batch: 520; loss: 0.74; acc: 0.81
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.76; acc: 0.77
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.77
Batch: 620; loss: 0.58; acc: 0.94
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.61; acc: 0.88
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.86; acc: 0.78
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.78; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.0002084313309751451
0.0001990830060094595
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.6439596856855283; val_accuracy: 0.8450437898089171 

The current subspace-distance is: 0.0001990830060094595 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.79; acc: 0.78
Batch: 180; loss: 0.76; acc: 0.84
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.94
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.81; acc: 0.77
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.62; acc: 0.89
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.85; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.8; acc: 0.78
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.74; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.89
Batch: 640; loss: 0.78; acc: 0.73
Batch: 660; loss: 0.78; acc: 0.78
Batch: 680; loss: 0.8; acc: 0.72
Batch: 700; loss: 0.67; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.78; acc: 0.77
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.00020950734324287623
0.00020446095732040703
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.97
Val Epoch over. val_loss: 0.6466173846630534; val_accuracy: 0.8472332802547771 

The current subspace-distance is: 0.00020446095732040703 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 1.0; acc: 0.7
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.69; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.76; acc: 0.8
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.88
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.88
Batch: 340; loss: 0.73; acc: 0.91
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.5; acc: 0.94
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.72; acc: 0.81
Batch: 680; loss: 0.63; acc: 0.91
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.75; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.88
Batch: 760; loss: 0.7; acc: 0.89
Batch: 780; loss: 0.88; acc: 0.75
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.00021030880452599376
0.00020467602007556707
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.6414694674075789; val_accuracy: 0.8468351910828026 

The current subspace-distance is: 0.00020467602007556707 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.63; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.77
Batch: 160; loss: 0.64; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.81; acc: 0.8
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.84; acc: 0.7
Batch: 320; loss: 0.53; acc: 0.94
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.83
Batch: 400; loss: 0.55; acc: 0.8
Batch: 420; loss: 0.66; acc: 0.78
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.73; acc: 0.78
Batch: 500; loss: 0.76; acc: 0.81
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.76; acc: 0.83
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.71; acc: 0.78
Batch: 680; loss: 0.63; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.58; acc: 0.91
Batch: 740; loss: 0.7; acc: 0.8
Batch: 760; loss: 0.58; acc: 0.91
Batch: 780; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.82 

0.00021220228518359363
0.00020524040155578405
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.92
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.6517038969856919; val_accuracy: 0.8441480891719745 

The current subspace-distance is: 0.00020524040155578405 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.92
Batch: 160; loss: 0.77; acc: 0.73
Batch: 180; loss: 0.72; acc: 0.81
Batch: 200; loss: 0.82; acc: 0.69
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.72; acc: 0.77
Batch: 260; loss: 0.63; acc: 0.88
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.96; acc: 0.69
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.82; acc: 0.75
Batch: 360; loss: 0.61; acc: 0.88
Batch: 380; loss: 0.77; acc: 0.73
Batch: 400; loss: 0.74; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.75
Batch: 440; loss: 0.7; acc: 0.81
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.47; acc: 0.94
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.76; acc: 0.78
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.54; acc: 0.92
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.9; acc: 0.67
Batch: 780; loss: 0.77; acc: 0.73
Train Epoch over. train_loss: 0.69; train_accuracy: 0.82 

0.00021150076645426452
0.0002058362733805552
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.6447500973750072; val_accuracy: 0.8455414012738853 

The current subspace-distance is: 0.0002058362733805552 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.8; acc: 0.75
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.65; acc: 0.88
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.86
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.89
Batch: 280; loss: 0.71; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.82; acc: 0.78
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.77
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.74; acc: 0.83
Batch: 460; loss: 0.83; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.56; acc: 0.92
Batch: 520; loss: 0.65; acc: 0.83
Batch: 540; loss: 0.85; acc: 0.81
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.83; acc: 0.77
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.76; acc: 0.77
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.81; acc: 0.77
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.69; train_accuracy: 0.82 

0.00021765928249806166
0.00020819369819946587
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.88
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.6413542838992586; val_accuracy: 0.848328025477707 

The current subspace-distance is: 0.00020819369819946587 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.76; acc: 0.8
Batch: 180; loss: 0.81; acc: 0.72
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.88; acc: 0.7
Batch: 260; loss: 0.84; acc: 0.78
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.91
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.75; acc: 0.78
Batch: 380; loss: 0.57; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.83; acc: 0.73
Batch: 440; loss: 0.67; acc: 0.86
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.65; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.8
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.91
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.72; acc: 0.81
Batch: 660; loss: 0.87; acc: 0.81
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.8; acc: 0.75
Batch: 780; loss: 0.83; acc: 0.78
Train Epoch over. train_loss: 0.69; train_accuracy: 0.83 

0.0002144171012332663
0.0002077482349704951
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6329233716627595; val_accuracy: 0.8489251592356688 

The current subspace-distance is: 0.0002077482349704951 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.94
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.79; acc: 0.81
Batch: 180; loss: 0.84; acc: 0.78
Batch: 200; loss: 0.72; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.73; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 0.8; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 0.75; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.7; acc: 0.88
Batch: 680; loss: 0.84; acc: 0.73
Batch: 700; loss: 0.52; acc: 0.92
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.82 

0.00021655292948707938
0.0002089117915602401
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.6311266091978474; val_accuracy: 0.8490246815286624 

The current subspace-distance is: 0.0002089117915602401 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.72; acc: 0.83
Batch: 220; loss: 0.81; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.63; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.82; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.89; acc: 0.73
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.83; acc: 0.77
Batch: 580; loss: 0.65; acc: 0.78
Batch: 600; loss: 0.48; acc: 0.95
Batch: 620; loss: 0.82; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.88; acc: 0.72
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.87; acc: 0.78
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.69; train_accuracy: 0.82 

0.00021308472787495703
0.00020433205645531416
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.6315088264501778; val_accuracy: 0.8493232484076433 

The current subspace-distance is: 0.00020433205645531416 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.82; acc: 0.73
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.75; acc: 0.72
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.76; acc: 0.72
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.91
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.89
Batch: 400; loss: 0.87; acc: 0.77
Batch: 420; loss: 0.57; acc: 0.89
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 0.77; acc: 0.78
Batch: 540; loss: 0.76; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.68; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.82; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.77; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.85; acc: 0.75
Batch: 760; loss: 0.67; acc: 0.84
Batch: 780; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.82 

0.0002145347825717181
0.00020760549523402005
Batch: 0; loss: 0.71; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.6282035362948278; val_accuracy: 0.849422770700637 

The current subspace-distance is: 0.00020760549523402005 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.67; acc: 0.86
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.95
Batch: 260; loss: 0.74; acc: 0.84
Batch: 280; loss: 0.73; acc: 0.81
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.77
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.74; acc: 0.8
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 0.88; acc: 0.73
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.66; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.63; acc: 0.88
Batch: 620; loss: 0.63; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.8; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.72; acc: 0.88
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.57; acc: 0.91
Train Epoch over. train_loss: 0.68; train_accuracy: 0.82 

0.00021853367798030376
0.00021160235337447375
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.97
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.98
Val Epoch over. val_loss: 0.6385820921818921; val_accuracy: 0.8495222929936306 

The current subspace-distance is: 0.00021160235337447375 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_9_flips_False_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.1990338787749493

The number of parameters is: 264111

The number of individual parameters is:

18
324
18
18
27
42282
27
27
53
124497
53
53
64
91584
64
64
4096
64
640
10
64
64

nonzero elements in E: 79233292
elements in E: 79233300
fraction nonzero: 0.9999998990323513
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.37; acc: 0.09
Batch: 20; loss: 2.05; acc: 0.19
Batch: 40; loss: 1.94; acc: 0.42
Batch: 60; loss: 1.87; acc: 0.39
Batch: 80; loss: 1.8; acc: 0.45
Batch: 100; loss: 1.74; acc: 0.59
Batch: 120; loss: 1.59; acc: 0.7
Batch: 140; loss: 1.67; acc: 0.61
Batch: 160; loss: 1.61; acc: 0.67
Batch: 180; loss: 1.64; acc: 0.61
Batch: 200; loss: 1.46; acc: 0.72
Batch: 220; loss: 1.53; acc: 0.62
Batch: 240; loss: 1.48; acc: 0.7
Batch: 260; loss: 1.43; acc: 0.78
Batch: 280; loss: 1.45; acc: 0.67
Batch: 300; loss: 1.44; acc: 0.72
Batch: 320; loss: 1.37; acc: 0.75
Batch: 340; loss: 1.41; acc: 0.72
Batch: 360; loss: 1.31; acc: 0.84
Batch: 380; loss: 1.3; acc: 0.83
Batch: 400; loss: 1.39; acc: 0.75
Batch: 420; loss: 1.32; acc: 0.77
Batch: 440; loss: 1.31; acc: 0.75
Batch: 460; loss: 1.27; acc: 0.73
Batch: 480; loss: 1.3; acc: 0.77
Batch: 500; loss: 1.26; acc: 0.81
Batch: 520; loss: 1.33; acc: 0.73
Batch: 540; loss: 1.22; acc: 0.83
Batch: 560; loss: 1.21; acc: 0.84
Batch: 580; loss: 1.31; acc: 0.73
Batch: 600; loss: 1.34; acc: 0.7
Batch: 620; loss: 1.23; acc: 0.8
Batch: 640; loss: 1.14; acc: 0.92
Batch: 660; loss: 1.26; acc: 0.67
Batch: 680; loss: 1.21; acc: 0.78
Batch: 700; loss: 1.11; acc: 0.86
Batch: 720; loss: 1.18; acc: 0.78
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.12; acc: 0.83
Batch: 780; loss: 1.28; acc: 0.7
Train Epoch over. train_loss: 1.42; train_accuracy: 0.7 

6.415668030967936e-05
5.973561928840354e-05
Batch: 0; loss: 1.2; acc: 0.77
Batch: 20; loss: 1.27; acc: 0.73
Batch: 40; loss: 0.92; acc: 0.91
Batch: 60; loss: 1.09; acc: 0.81
Batch: 80; loss: 1.05; acc: 0.86
Batch: 100; loss: 1.15; acc: 0.83
Batch: 120; loss: 1.32; acc: 0.67
Batch: 140; loss: 0.94; acc: 0.89
Val Epoch over. val_loss: 1.1199745589001164; val_accuracy: 0.8076234076433121 

The current subspace-distance is: 5.973561928840354e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.77
Batch: 20; loss: 1.2; acc: 0.75
Batch: 40; loss: 1.14; acc: 0.83
Batch: 60; loss: 1.23; acc: 0.73
Batch: 80; loss: 1.19; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.75
Batch: 120; loss: 1.18; acc: 0.7
Batch: 140; loss: 1.09; acc: 0.75
Batch: 160; loss: 1.02; acc: 0.84
Batch: 180; loss: 1.03; acc: 0.84
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 0.96; acc: 0.88
Batch: 240; loss: 1.04; acc: 0.8
Batch: 260; loss: 1.15; acc: 0.78
Batch: 280; loss: 1.08; acc: 0.81
Batch: 300; loss: 1.08; acc: 0.83
Batch: 320; loss: 0.86; acc: 0.92
Batch: 340; loss: 1.09; acc: 0.8
Batch: 360; loss: 1.06; acc: 0.81
Batch: 380; loss: 1.02; acc: 0.81
Batch: 400; loss: 1.0; acc: 0.86
Batch: 420; loss: 0.96; acc: 0.88
Batch: 440; loss: 0.95; acc: 0.89
Batch: 460; loss: 1.22; acc: 0.78
Batch: 480; loss: 1.06; acc: 0.77
Batch: 500; loss: 0.98; acc: 0.8
Batch: 520; loss: 0.99; acc: 0.89
Batch: 540; loss: 1.02; acc: 0.78
Batch: 560; loss: 0.92; acc: 0.86
Batch: 580; loss: 1.03; acc: 0.78
Batch: 600; loss: 0.87; acc: 0.88
Batch: 620; loss: 1.04; acc: 0.8
Batch: 640; loss: 0.94; acc: 0.89
Batch: 660; loss: 0.99; acc: 0.86
Batch: 680; loss: 1.18; acc: 0.72
Batch: 700; loss: 1.0; acc: 0.81
Batch: 720; loss: 0.89; acc: 0.89
Batch: 740; loss: 1.0; acc: 0.81
Batch: 760; loss: 0.98; acc: 0.89
Batch: 780; loss: 0.88; acc: 0.86
Train Epoch over. train_loss: 1.06; train_accuracy: 0.8 

8.920192340156063e-05
8.556569810025394e-05
Batch: 0; loss: 1.01; acc: 0.75
Batch: 20; loss: 1.06; acc: 0.77
Batch: 40; loss: 0.72; acc: 0.92
Batch: 60; loss: 0.92; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.95
Batch: 100; loss: 0.89; acc: 0.88
Batch: 120; loss: 1.16; acc: 0.73
Batch: 140; loss: 0.73; acc: 0.92
Val Epoch over. val_loss: 0.9179724386543225; val_accuracy: 0.8442476114649682 

The current subspace-distance is: 8.556569810025394e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.01; acc: 0.8
Batch: 40; loss: 1.0; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.88
Batch: 100; loss: 1.02; acc: 0.78
Batch: 120; loss: 0.98; acc: 0.81
Batch: 140; loss: 0.89; acc: 0.88
Batch: 160; loss: 1.08; acc: 0.69
Batch: 180; loss: 0.99; acc: 0.86
Batch: 200; loss: 1.02; acc: 0.78
Batch: 220; loss: 0.98; acc: 0.84
Batch: 240; loss: 1.17; acc: 0.69
Batch: 260; loss: 0.88; acc: 0.81
Batch: 280; loss: 0.91; acc: 0.81
Batch: 300; loss: 0.93; acc: 0.81
Batch: 320; loss: 0.99; acc: 0.75
Batch: 340; loss: 0.94; acc: 0.77
Batch: 360; loss: 0.87; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.84
Batch: 400; loss: 0.81; acc: 0.89
Batch: 420; loss: 0.85; acc: 0.88
Batch: 440; loss: 0.78; acc: 0.83
Batch: 460; loss: 0.95; acc: 0.91
Batch: 480; loss: 0.84; acc: 0.84
Batch: 500; loss: 0.75; acc: 0.86
Batch: 520; loss: 0.93; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.88
Batch: 560; loss: 0.88; acc: 0.84
Batch: 580; loss: 0.87; acc: 0.84
Batch: 600; loss: 1.04; acc: 0.7
Batch: 620; loss: 0.86; acc: 0.88
Batch: 640; loss: 0.92; acc: 0.75
Batch: 660; loss: 0.84; acc: 0.86
Batch: 680; loss: 0.8; acc: 0.89
Batch: 700; loss: 0.85; acc: 0.83
Batch: 720; loss: 0.9; acc: 0.88
Batch: 740; loss: 0.93; acc: 0.78
Batch: 760; loss: 0.79; acc: 0.86
Batch: 780; loss: 0.83; acc: 0.89
Train Epoch over. train_loss: 0.92; train_accuracy: 0.83 

0.00010987264249706641
0.00010520849900785834
Batch: 0; loss: 0.89; acc: 0.8
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.94
Batch: 60; loss: 0.88; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.98
Batch: 100; loss: 0.75; acc: 0.92
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 0.6; acc: 0.89
Val Epoch over. val_loss: 0.7998710874539272; val_accuracy: 0.862062101910828 

The current subspace-distance is: 0.00010520849900785834 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.91
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.84
Batch: 80; loss: 0.96; acc: 0.84
Batch: 100; loss: 1.05; acc: 0.72
Batch: 120; loss: 0.94; acc: 0.81
Batch: 140; loss: 0.8; acc: 0.86
Batch: 160; loss: 0.8; acc: 0.84
Batch: 180; loss: 0.79; acc: 0.89
Batch: 200; loss: 0.88; acc: 0.83
Batch: 220; loss: 0.82; acc: 0.84
Batch: 240; loss: 0.79; acc: 0.89
Batch: 260; loss: 0.94; acc: 0.77
Batch: 280; loss: 0.81; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.75; acc: 0.89
Batch: 340; loss: 0.79; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.88
Batch: 380; loss: 0.73; acc: 0.86
Batch: 400; loss: 0.87; acc: 0.83
Batch: 420; loss: 0.73; acc: 0.92
Batch: 440; loss: 0.76; acc: 0.91
Batch: 460; loss: 0.79; acc: 0.83
Batch: 480; loss: 0.97; acc: 0.77
Batch: 500; loss: 0.88; acc: 0.84
Batch: 520; loss: 0.81; acc: 0.88
Batch: 540; loss: 0.71; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.92
Batch: 580; loss: 0.74; acc: 0.88
Batch: 600; loss: 0.74; acc: 0.84
Batch: 620; loss: 0.82; acc: 0.83
Batch: 640; loss: 0.84; acc: 0.88
Batch: 660; loss: 0.7; acc: 0.92
Batch: 680; loss: 0.78; acc: 0.83
Batch: 700; loss: 0.76; acc: 0.92
Batch: 720; loss: 0.67; acc: 0.94
Batch: 740; loss: 0.87; acc: 0.8
Batch: 760; loss: 0.81; acc: 0.83
Batch: 780; loss: 0.68; acc: 0.86
Train Epoch over. train_loss: 0.82; train_accuracy: 0.84 

0.00012794940266758204
0.0001240324490936473
Batch: 0; loss: 0.8; acc: 0.83
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 0.57; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.57; acc: 0.98
Batch: 100; loss: 0.68; acc: 0.92
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.51; acc: 0.91
Val Epoch over. val_loss: 0.7193381755974642; val_accuracy: 0.8706210191082803 

The current subspace-distance is: 0.0001240324490936473 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.92
Batch: 20; loss: 0.88; acc: 0.78
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.92; acc: 0.81
Batch: 80; loss: 0.83; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.86
Batch: 160; loss: 0.71; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.84
Batch: 200; loss: 0.77; acc: 0.83
Batch: 220; loss: 0.81; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.89
Batch: 260; loss: 0.91; acc: 0.83
Batch: 280; loss: 0.74; acc: 0.86
Batch: 300; loss: 0.91; acc: 0.78
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.7; acc: 0.91
Batch: 360; loss: 0.77; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.88
Batch: 400; loss: 0.68; acc: 0.89
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.92
Batch: 460; loss: 0.73; acc: 0.91
Batch: 480; loss: 0.82; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.79; acc: 0.8
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.92
Batch: 600; loss: 0.77; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.89
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.76; acc: 0.8
Batch: 680; loss: 0.79; acc: 0.84
Batch: 700; loss: 0.68; acc: 0.88
Batch: 720; loss: 0.64; acc: 0.89
Batch: 740; loss: 0.71; acc: 0.88
Batch: 760; loss: 0.75; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.75; train_accuracy: 0.85 

0.00014216016279533505
0.00013752319500781596
Batch: 0; loss: 0.75; acc: 0.84
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.94
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.48; acc: 0.89
Val Epoch over. val_loss: 0.6614720022222799; val_accuracy: 0.87609474522293 

The current subspace-distance is: 0.00013752319500781596 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.95
Batch: 20; loss: 0.71; acc: 0.89
Batch: 40; loss: 0.72; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.88
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.91
Batch: 160; loss: 0.73; acc: 0.83
Batch: 180; loss: 0.7; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.72; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.94
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.89
Batch: 380; loss: 0.74; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.94
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.75; acc: 0.84
Batch: 500; loss: 0.83; acc: 0.84
Batch: 520; loss: 0.72; acc: 0.86
Batch: 540; loss: 0.77; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.89
Batch: 600; loss: 0.72; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.56; acc: 0.94
Batch: 660; loss: 0.76; acc: 0.81
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.88
Batch: 720; loss: 0.73; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.81
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.71; acc: 0.89
Train Epoch over. train_loss: 0.69; train_accuracy: 0.86 

0.0001543138932902366
0.0001492352457717061
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.87; acc: 0.75
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.95
Batch: 100; loss: 0.57; acc: 0.92
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.44; acc: 0.91
Val Epoch over. val_loss: 0.6127506382526107; val_accuracy: 0.881468949044586 

The current subspace-distance is: 0.0001492352457717061 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.74; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.6; acc: 0.94
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.77; acc: 0.73
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.83; acc: 0.8
Batch: 380; loss: 0.71; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.88
Batch: 540; loss: 0.94; acc: 0.7
Batch: 560; loss: 0.56; acc: 0.91
Batch: 580; loss: 0.58; acc: 0.95
Batch: 600; loss: 0.58; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.83; acc: 0.75
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.92
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.86 

0.00016792499809525907
0.00016262565623037517
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.84; acc: 0.72
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.95
Batch: 100; loss: 0.54; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.5740318807067385; val_accuracy: 0.8877388535031847 

The current subspace-distance is: 0.00016262565623037517 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.92
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.76; acc: 0.81
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.78
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.91
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.8; acc: 0.83
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.89
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.92
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.46; acc: 0.95
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.91
Train Epoch over. train_loss: 0.62; train_accuracy: 0.87 

0.00017793040024116635
0.00017432300955988467
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.53; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.5456667984746824; val_accuracy: 0.8911226114649682 

The current subspace-distance is: 0.00017432300955988467 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.63; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.74; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.94
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.94
Batch: 340; loss: 0.57; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.97
Batch: 440; loss: 0.71; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.53; acc: 0.91
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.62; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.92
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.91
Batch: 680; loss: 0.57; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.59; train_accuracy: 0.87 

0.00018848924082703888
0.00018226756947115064
Batch: 0; loss: 0.59; acc: 0.88
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.94
Val Epoch over. val_loss: 0.5180791525324439; val_accuracy: 0.8956011146496815 

The current subspace-distance is: 0.00018226756947115064 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.65; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.63; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.59; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.89
Batch: 460; loss: 0.7; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.58; acc: 0.91
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.95
Batch: 680; loss: 0.47; acc: 0.92
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.47; acc: 0.94
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00019774471002165228
0.00019229284953325987
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.48963826951707246; val_accuracy: 0.8986863057324841 

The current subspace-distance is: 0.00019229284953325987 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.56; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.92
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.65; acc: 0.84
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.92
Batch: 660; loss: 0.65; acc: 0.81
Batch: 680; loss: 0.69; acc: 0.77
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.00020215667609591037
0.00019532142323441803
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.94
Val Epoch over. val_loss: 0.48965808787163656; val_accuracy: 0.8968949044585988 

The current subspace-distance is: 0.00019532142323441803 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.92
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.67; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.94
Batch: 660; loss: 0.78; acc: 0.78
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.88 

0.00020582640718203038
0.0001971762685570866
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.48741201268639534; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 0.0001971762685570866 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.94
Batch: 40; loss: 0.54; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.94
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.92
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.95
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.51; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.00020712196419481188
0.00020066479919478297
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4700866862656964; val_accuracy: 0.8999800955414012 

The current subspace-distance is: 0.00020066479919478297 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.94
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.67; acc: 0.78
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.35; acc: 0.97
Batch: 260; loss: 0.51; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.95
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.95
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.94
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.95
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.88 

0.0002103351871483028
0.00020246375061105937
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.4692649933373093; val_accuracy: 0.9025676751592356 

The current subspace-distance is: 0.00020246375061105937 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.49; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.92
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.72; acc: 0.8
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.94
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.88 

0.00020985603623557836
0.00020360415510367602
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4666052345827127; val_accuracy: 0.9022691082802548 

The current subspace-distance is: 0.00020360415510367602 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.64; acc: 0.75
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.62; acc: 0.8
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.95
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.68; acc: 0.78
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.68; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00021305833070073277
0.00020771626441273838
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4592178576880959; val_accuracy: 0.901671974522293 

The current subspace-distance is: 0.00020771626441273838 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.95
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.45; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.42; acc: 0.94
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.45; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00021760333038400859
0.0002093940129270777
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4550804698922832; val_accuracy: 0.9034633757961783 

The current subspace-distance is: 0.0002093940129270777 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.65; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.55; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.95
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.88 

0.00022094807354733348
0.00021270541765261441
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4443433472684994; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 0.00021270541765261441 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.97
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.92
Batch: 540; loss: 0.67; acc: 0.75
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.97
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00022252013150136918
0.00021551283134613186
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.4375545824788938; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 0.00021551283134613186 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.41; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.95
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.88 

0.00022549915593117476
0.00021842685237061232
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.439129930867511; val_accuracy: 0.9038614649681529 

The current subspace-distance is: 0.00021842685237061232 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.98
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.81
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.97
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.95
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.56; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.95
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00022343857563100755
0.00021519414440263063
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.43844856492653017; val_accuracy: 0.9031648089171974 

The current subspace-distance is: 0.00021519414440263063 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.95
Batch: 300; loss: 0.64; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.92
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.95
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.95
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00022575327602680773
0.00021679764904547483
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.23; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.97
Val Epoch over. val_loss: 0.4352126291413216; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 0.00021679764904547483 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00022773008095100522
0.00022021288168616593
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.43852232140340625; val_accuracy: 0.9029657643312102 

The current subspace-distance is: 0.00022021288168616593 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.86
Batch: 180; loss: 0.65; acc: 0.78
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.97
Batch: 640; loss: 0.48; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.94
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00022658782836515456
0.00021795924112666398
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.43427942237656586; val_accuracy: 0.9040605095541401 

The current subspace-distance is: 0.00021795924112666398 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.43; acc: 0.95
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.94
Batch: 600; loss: 0.61; acc: 0.8
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.88 

0.00022735980746801943
0.00021953041141387075
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.24; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.4426758965109564; val_accuracy: 0.9029657643312102 

The current subspace-distance is: 0.00021953041141387075 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.0002290509728481993
0.00022134721803013235
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.43110452127304805; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 0.00022134721803013235 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.92
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.95
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00022680174151901156
0.00022021634504199028
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.4273183211968963; val_accuracy: 0.9044585987261147 

The current subspace-distance is: 0.00022021634504199028 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.94
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.94
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.74; acc: 0.8
Batch: 720; loss: 0.87; acc: 0.7
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.95
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00023008830612525344
0.0002232953265774995
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4242866190185972; val_accuracy: 0.9055533439490446 

The current subspace-distance is: 0.0002232953265774995 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.3; acc: 0.97
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.73
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00022942089708521962
0.00022169733711052686
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.22; acc: 1.0
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4227186418642664; val_accuracy: 0.9054538216560509 

The current subspace-distance is: 0.00022169733711052686 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.8
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.88 

0.00023208091442938894
0.00022504988010041416
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.23; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.43008483775482054; val_accuracy: 0.9050557324840764 

The current subspace-distance is: 0.00022504988010041416 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_9_flips_False_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.1990338787749493

The number of parameters is: 264111

The number of individual parameters is:

18
324
18
18
27
42282
27
27
53
124497
53
53
64
91584
64
64
4096
64
640
10
64
64

nonzero elements in E: 105644389
elements in E: 105644400
fraction nonzero: 0.9999998958771122
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.45; acc: 0.09
Batch: 20; loss: 2.06; acc: 0.33
Batch: 40; loss: 1.81; acc: 0.45
Batch: 60; loss: 1.73; acc: 0.56
Batch: 80; loss: 1.51; acc: 0.69
Batch: 100; loss: 1.5; acc: 0.69
Batch: 120; loss: 1.45; acc: 0.77
Batch: 140; loss: 1.48; acc: 0.69
Batch: 160; loss: 1.44; acc: 0.7
Batch: 180; loss: 1.47; acc: 0.66
Batch: 200; loss: 1.47; acc: 0.66
Batch: 220; loss: 1.31; acc: 0.78
Batch: 240; loss: 1.27; acc: 0.84
Batch: 260; loss: 1.4; acc: 0.67
Batch: 280; loss: 1.27; acc: 0.78
Batch: 300; loss: 1.32; acc: 0.8
Batch: 320; loss: 1.22; acc: 0.8
Batch: 340; loss: 1.2; acc: 0.75
Batch: 360; loss: 1.23; acc: 0.77
Batch: 380; loss: 1.25; acc: 0.72
Batch: 400; loss: 1.12; acc: 0.8
Batch: 420; loss: 1.09; acc: 0.84
Batch: 440; loss: 1.11; acc: 0.83
Batch: 460; loss: 1.08; acc: 0.78
Batch: 480; loss: 1.19; acc: 0.8
Batch: 500; loss: 1.08; acc: 0.88
Batch: 520; loss: 1.05; acc: 0.84
Batch: 540; loss: 1.1; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.7
Batch: 580; loss: 1.13; acc: 0.8
Batch: 600; loss: 1.02; acc: 0.88
Batch: 620; loss: 1.03; acc: 0.81
Batch: 640; loss: 1.05; acc: 0.81
Batch: 660; loss: 1.12; acc: 0.75
Batch: 680; loss: 0.89; acc: 0.91
Batch: 700; loss: 1.03; acc: 0.81
Batch: 720; loss: 1.01; acc: 0.83
Batch: 740; loss: 0.97; acc: 0.84
Batch: 760; loss: 0.9; acc: 0.88
Batch: 780; loss: 1.0; acc: 0.84
Train Epoch over. train_loss: 1.27; train_accuracy: 0.75 

2.4259461497422308e-05
8.452007932646666e-06
Batch: 0; loss: 1.08; acc: 0.81
Batch: 20; loss: 1.2; acc: 0.7
Batch: 40; loss: 0.69; acc: 0.94
Batch: 60; loss: 1.0; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.88
Batch: 100; loss: 0.93; acc: 0.88
Batch: 120; loss: 1.15; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.95
Val Epoch over. val_loss: 0.937875757551497; val_accuracy: 0.8546974522292994 

The current subspace-distance is: 8.452007932646666e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.89
Batch: 40; loss: 0.97; acc: 0.8
Batch: 60; loss: 1.03; acc: 0.83
Batch: 80; loss: 0.9; acc: 0.91
Batch: 100; loss: 0.88; acc: 0.89
Batch: 120; loss: 0.97; acc: 0.81
Batch: 140; loss: 0.91; acc: 0.86
Batch: 160; loss: 0.96; acc: 0.81
Batch: 180; loss: 0.81; acc: 0.91
Batch: 200; loss: 0.87; acc: 0.86
Batch: 220; loss: 1.01; acc: 0.86
Batch: 240; loss: 0.8; acc: 0.94
Batch: 260; loss: 1.05; acc: 0.8
Batch: 280; loss: 0.86; acc: 0.89
Batch: 300; loss: 0.91; acc: 0.86
Batch: 320; loss: 0.94; acc: 0.86
Batch: 340; loss: 1.01; acc: 0.77
Batch: 360; loss: 0.91; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.92
Batch: 400; loss: 0.88; acc: 0.84
Batch: 420; loss: 0.95; acc: 0.8
Batch: 440; loss: 0.9; acc: 0.81
Batch: 460; loss: 0.91; acc: 0.78
Batch: 480; loss: 0.88; acc: 0.78
Batch: 500; loss: 0.86; acc: 0.88
Batch: 520; loss: 0.94; acc: 0.81
Batch: 540; loss: 0.83; acc: 0.84
Batch: 560; loss: 0.83; acc: 0.91
Batch: 580; loss: 0.77; acc: 0.91
Batch: 600; loss: 0.81; acc: 0.86
Batch: 620; loss: 0.87; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.95
Batch: 680; loss: 0.78; acc: 0.88
Batch: 700; loss: 0.89; acc: 0.83
Batch: 720; loss: 0.82; acc: 0.84
Batch: 740; loss: 0.79; acc: 0.91
Batch: 760; loss: 0.91; acc: 0.77
Batch: 780; loss: 0.84; acc: 0.84
Train Epoch over. train_loss: 0.88; train_accuracy: 0.85 

3.021446718776133e-05
1.1447450560808647e-05
Batch: 0; loss: 0.81; acc: 0.91
Batch: 20; loss: 1.02; acc: 0.75
Batch: 40; loss: 0.52; acc: 0.95
Batch: 60; loss: 0.79; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.92
Batch: 100; loss: 0.75; acc: 0.89
Batch: 120; loss: 0.96; acc: 0.81
Batch: 140; loss: 0.52; acc: 0.97
Val Epoch over. val_loss: 0.7369633159440034; val_accuracy: 0.8891321656050956 

The current subspace-distance is: 1.1447450560808647e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.94
Batch: 20; loss: 0.8; acc: 0.88
Batch: 40; loss: 0.89; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.92
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.86
Batch: 140; loss: 0.7; acc: 0.89
Batch: 160; loss: 0.88; acc: 0.78
Batch: 180; loss: 0.74; acc: 0.84
Batch: 200; loss: 0.76; acc: 0.86
Batch: 220; loss: 0.71; acc: 0.89
Batch: 240; loss: 0.71; acc: 0.91
Batch: 260; loss: 0.88; acc: 0.81
Batch: 280; loss: 0.86; acc: 0.81
Batch: 300; loss: 0.73; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.91
Batch: 340; loss: 0.82; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.71; acc: 0.91
Batch: 400; loss: 0.7; acc: 0.89
Batch: 420; loss: 0.66; acc: 0.88
Batch: 440; loss: 0.69; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.92
Batch: 500; loss: 0.72; acc: 0.92
Batch: 520; loss: 0.62; acc: 0.95
Batch: 540; loss: 0.77; acc: 0.83
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 0.7; acc: 0.92
Batch: 600; loss: 0.72; acc: 0.88
Batch: 620; loss: 0.81; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.94
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.8; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.94
Batch: 720; loss: 0.63; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.92
Batch: 780; loss: 0.73; acc: 0.86
Train Epoch over. train_loss: 0.73; train_accuracy: 0.87 

3.4627169952727854e-05
1.3622949154523667e-05
Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.94
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.94
Val Epoch over. val_loss: 0.6242749453729884; val_accuracy: 0.897890127388535 

The current subspace-distance is: 1.3622949154523667e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.95
Batch: 20; loss: 0.67; acc: 0.92
Batch: 40; loss: 0.62; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.92
Batch: 80; loss: 0.76; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.95
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.77; acc: 0.88
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.66; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.89
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 0.65; acc: 0.88
Batch: 380; loss: 0.56; acc: 0.94
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.55; acc: 0.92
Batch: 440; loss: 0.66; acc: 0.89
Batch: 460; loss: 0.67; acc: 0.89
Batch: 480; loss: 0.67; acc: 0.89
Batch: 500; loss: 0.61; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.91
Batch: 540; loss: 0.72; acc: 0.8
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.86
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.66; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.92
Batch: 660; loss: 0.69; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.92
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.95
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.88 

3.9182712498586625e-05
1.816756048356183e-05
Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.83; acc: 0.73
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.5500051627872856; val_accuracy: 0.9029657643312102 

The current subspace-distance is: 1.816756048356183e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.89
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.89
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.94
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.88
Batch: 220; loss: 0.71; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.92
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.91
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.95
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.94
Batch: 560; loss: 0.6; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.95
Batch: 760; loss: 0.68; acc: 0.81
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.88 

4.159494346822612e-05
1.7766209566616453e-05
Batch: 0; loss: 0.5; acc: 0.95
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.34; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.94
Val Epoch over. val_loss: 0.4955505500933167; val_accuracy: 0.9066480891719745 

The current subspace-distance is: 1.7766209566616453e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.73; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.94
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.89
Batch: 280; loss: 0.66; acc: 0.78
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.95
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.89
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.92
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.95
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.89 

4.426208033692092e-05
1.900813731481321e-05
Batch: 0; loss: 0.45; acc: 0.95
Batch: 20; loss: 0.7; acc: 0.75
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.4570107205658202; val_accuracy: 0.9075437898089171 

The current subspace-distance is: 1.900813731481321e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.94
Batch: 180; loss: 0.62; acc: 0.84
Batch: 200; loss: 0.52; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.49; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.89 

4.824727511731908e-05
2.0438807041500695e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42587061444665214; val_accuracy: 0.9153065286624203 

The current subspace-distance is: 2.0438807041500695e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.57; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.3; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.95
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.9 

5.0160604587290436e-05
2.298257822985761e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.4064539226756734; val_accuracy: 0.9161027070063694 

The current subspace-distance is: 2.298257822985761e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.97
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.1864783017663285e-05
2.2088017431087792e-05
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.38577575649425483; val_accuracy: 0.9163017515923567 

The current subspace-distance is: 2.2088017431087792e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.97
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.508501271833666e-05
2.5185099730151705e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3680144827456991; val_accuracy: 0.917296974522293 

The current subspace-distance is: 2.5185099730151705e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.5405784223694354e-05
2.5580533474567346e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.36341225132820715; val_accuracy: 0.9184912420382165 

The current subspace-distance is: 2.5580533474567346e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

5.5774136853870004e-05
2.4159344320651144e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.3578813100696369; val_accuracy: 0.9192874203821656 

The current subspace-distance is: 2.4159344320651144e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.95
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

5.7284836657345295e-05
2.5253397325286642e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.35556189155882334; val_accuracy: 0.9183917197452229 

The current subspace-distance is: 2.5253397325286642e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.97
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

5.779840284958482e-05
2.636946737766266e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.351393866975596; val_accuracy: 0.9188893312101911 

The current subspace-distance is: 2.636946737766266e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.97
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.27; acc: 1.0
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

5.849597800988704e-05
2.638969453983009e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3488225547751044; val_accuracy: 0.919187898089172 

The current subspace-distance is: 2.638969453983009e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.816849443363026e-05
2.7466614483273588e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3441470338470617; val_accuracy: 0.9208797770700637 

The current subspace-distance is: 2.7466614483273588e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.98
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.95
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.92971955484245e-05
2.7054531528847292e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.34131990638887805; val_accuracy: 0.9225716560509554 

The current subspace-distance is: 2.7054531528847292e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.91 

5.961946590105072e-05
2.595447404019069e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.33459639748570263; val_accuracy: 0.9209792993630573 

The current subspace-distance is: 2.595447404019069e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.037846105755307e-05
2.677225165825803e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.33077774026021833; val_accuracy: 0.9232683121019108 

The current subspace-distance is: 2.677225165825803e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.97
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.81
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.92
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.23; acc: 1.0
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.064524131943472e-05
2.7741387384594418e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.32924462736222393; val_accuracy: 0.9222730891719745 

The current subspace-distance is: 2.7741387384594418e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.98
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.035565456841141e-05
2.742777178355027e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3262242607914718; val_accuracy: 0.9204816878980892 

The current subspace-distance is: 2.742777178355027e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.97
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.140070763649419e-05
2.7635944206849672e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3241565652239095; val_accuracy: 0.9221735668789809 

The current subspace-distance is: 2.7635944206849672e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.98
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.98
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

6.131413101684302e-05
2.7223402867093682e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3250982756637464; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 2.7223402867093682e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.159183976706117e-05
2.7963022148469463e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.32560914427421656; val_accuracy: 0.9220740445859873 

The current subspace-distance is: 2.7963022148469463e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.97
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.98
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.198325718287379e-05
2.8684093194897287e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3226601673634189; val_accuracy: 0.9226711783439491 

The current subspace-distance is: 2.8684093194897287e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.97
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.17; acc: 1.0
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.26; acc: 0.98
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.159085023682564e-05
2.80033618764719e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3233268670975023; val_accuracy: 0.9231687898089171 

The current subspace-distance is: 2.80033618764719e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.199780182214454e-05
2.8315645977272652e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3232357797159511; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.8315645977272652e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.22; acc: 1.0
Batch: 20; loss: 0.33; acc: 0.97
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.97
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.22330408077687e-05
2.931202106992714e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3166111568641511; val_accuracy: 0.9247611464968153 

The current subspace-distance is: 2.931202106992714e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.97
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.98
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.98
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.168279651319608e-05
2.7908952688449062e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3228476712847971; val_accuracy: 0.923765923566879 

The current subspace-distance is: 2.7908952688449062e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.97
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.91 

6.258418579818681e-05
2.8410990125848912e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.32067206240953156; val_accuracy: 0.9233678343949044 

The current subspace-distance is: 2.8410990125848912e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_9_flips_False_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.1990338787749493

The number of parameters is: 264111

The number of individual parameters is:

18
324
18
18
27
42282
27
27
53
124497
53
53
64
91584
64
64
4096
64
640
10
64
64

nonzero elements in E: 132055489
elements in E: 132055500
fraction nonzero: 0.9999999167016899
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.08
Batch: 20; loss: 1.96; acc: 0.31
Batch: 40; loss: 1.76; acc: 0.52
Batch: 60; loss: 1.67; acc: 0.53
Batch: 80; loss: 1.52; acc: 0.72
Batch: 100; loss: 1.54; acc: 0.73
Batch: 120; loss: 1.37; acc: 0.77
Batch: 140; loss: 1.43; acc: 0.75
Batch: 160; loss: 1.36; acc: 0.75
Batch: 180; loss: 1.37; acc: 0.69
Batch: 200; loss: 1.34; acc: 0.77
Batch: 220; loss: 1.45; acc: 0.56
Batch: 240; loss: 1.17; acc: 0.88
Batch: 260; loss: 1.06; acc: 0.84
Batch: 280; loss: 1.16; acc: 0.77
Batch: 300; loss: 1.24; acc: 0.78
Batch: 320; loss: 1.14; acc: 0.7
Batch: 340; loss: 1.09; acc: 0.84
Batch: 360; loss: 1.0; acc: 0.84
Batch: 380; loss: 1.14; acc: 0.8
Batch: 400; loss: 0.98; acc: 0.86
Batch: 420; loss: 1.15; acc: 0.83
Batch: 440; loss: 1.17; acc: 0.8
Batch: 460; loss: 1.09; acc: 0.83
Batch: 480; loss: 1.07; acc: 0.83
Batch: 500; loss: 0.98; acc: 0.91
Batch: 520; loss: 1.05; acc: 0.81
Batch: 540; loss: 0.92; acc: 0.88
Batch: 560; loss: 0.99; acc: 0.83
Batch: 580; loss: 0.88; acc: 0.94
Batch: 600; loss: 0.9; acc: 0.88
Batch: 620; loss: 1.01; acc: 0.81
Batch: 640; loss: 0.95; acc: 0.86
Batch: 660; loss: 0.98; acc: 0.83
Batch: 680; loss: 0.96; acc: 0.83
Batch: 700; loss: 0.96; acc: 0.75
Batch: 720; loss: 0.83; acc: 0.88
Batch: 740; loss: 0.92; acc: 0.77
Batch: 760; loss: 0.78; acc: 0.94
Batch: 780; loss: 0.84; acc: 0.88
Train Epoch over. train_loss: 1.16; train_accuracy: 0.78 

2.5062914573936723e-05
8.973610420071054e-06
Batch: 0; loss: 0.86; acc: 0.94
Batch: 20; loss: 1.02; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.97
Batch: 60; loss: 0.85; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.94
Batch: 100; loss: 0.77; acc: 0.89
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.64; acc: 0.95
Val Epoch over. val_loss: 0.8129866783785972; val_accuracy: 0.8708200636942676 

The current subspace-distance is: 8.973610420071054e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 0.84; acc: 0.83
Batch: 40; loss: 0.81; acc: 0.86
Batch: 60; loss: 0.76; acc: 0.92
Batch: 80; loss: 0.79; acc: 0.88
Batch: 100; loss: 0.84; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.88
Batch: 140; loss: 0.83; acc: 0.88
Batch: 160; loss: 0.85; acc: 0.83
Batch: 180; loss: 1.0; acc: 0.8
Batch: 200; loss: 0.84; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.89
Batch: 240; loss: 0.9; acc: 0.84
Batch: 260; loss: 0.86; acc: 0.86
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.72; acc: 0.86
Batch: 320; loss: 0.78; acc: 0.86
Batch: 340; loss: 0.84; acc: 0.78
Batch: 360; loss: 0.77; acc: 0.84
Batch: 380; loss: 0.89; acc: 0.8
Batch: 400; loss: 0.94; acc: 0.78
Batch: 420; loss: 0.73; acc: 0.88
Batch: 440; loss: 0.75; acc: 0.89
Batch: 460; loss: 0.84; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.88
Batch: 520; loss: 0.75; acc: 0.88
Batch: 540; loss: 0.7; acc: 0.89
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.72; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.92
Batch: 620; loss: 0.64; acc: 0.92
Batch: 640; loss: 0.72; acc: 0.86
Batch: 660; loss: 0.76; acc: 0.78
Batch: 680; loss: 0.71; acc: 0.89
Batch: 700; loss: 0.93; acc: 0.8
Batch: 720; loss: 0.69; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.92
Batch: 780; loss: 0.63; acc: 0.89
Train Epoch over. train_loss: 0.76; train_accuracy: 0.87 

3.0586346838390455e-05
1.244148825207958e-05
Batch: 0; loss: 0.7; acc: 0.95
Batch: 20; loss: 0.78; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.98
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.95
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.94
Val Epoch over. val_loss: 0.6161414278540641; val_accuracy: 0.8961982484076433 

The current subspace-distance is: 1.244148825207958e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.94
Batch: 20; loss: 0.76; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.92
Batch: 160; loss: 0.6; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.67; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.92
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.62; acc: 0.89
Batch: 320; loss: 0.73; acc: 0.86
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.61; acc: 0.92
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.5; acc: 0.98
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.63; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.71; acc: 0.84
Batch: 620; loss: 0.48; acc: 0.94
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.95
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.88 

3.5620920243673027e-05
1.4698397535539698e-05
Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.95
Val Epoch over. val_loss: 0.5068441447179028; val_accuracy: 0.9086385350318471 

The current subspace-distance is: 1.4698397535539698e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.95
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.94
Batch: 180; loss: 0.54; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.97
Batch: 260; loss: 0.64; acc: 0.89
Batch: 280; loss: 0.57; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.6; acc: 0.89
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.92
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.95
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.53; train_accuracy: 0.89 

4.000566332251765e-05
1.733288991090376e-05
Batch: 0; loss: 0.51; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.97
Val Epoch over. val_loss: 0.4380635068674756; val_accuracy: 0.9177945859872612 

The current subspace-distance is: 1.733288991090376e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.95
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.97
Batch: 740; loss: 0.52; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

4.2733870941447094e-05
1.8138878658646718e-05
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.3923238009024578; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 1.8138878658646718e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.28; acc: 1.0
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.97
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.91 

4.555061605060473e-05
2.03095878532622e-05
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.36597624810258295; val_accuracy: 0.9277468152866242 

The current subspace-distance is: 2.03095878532622e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.91 

4.879532571067102e-05
2.211080936831422e-05
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.34218138162117856; val_accuracy: 0.9314291401273885 

The current subspace-distance is: 2.211080936831422e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.97
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.98
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.97
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.010491076973267e-05
2.295382000738755e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.32668866928975293; val_accuracy: 0.9335191082802548 

The current subspace-distance is: 2.295382000738755e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.98
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.18; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.98
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.92 

5.2556442824425176e-05
2.3492006221204065e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.30500448898524996; val_accuracy: 0.9352109872611465 

The current subspace-distance is: 2.3492006221204065e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.4232950787991285e-05
2.4287161068059504e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.2927463607518536; val_accuracy: 0.9354100318471338 

The current subspace-distance is: 2.4287161068059504e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.97
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.542723010876216e-05
2.4330123778781854e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.28921343367190877; val_accuracy: 0.9346138535031847 

The current subspace-distance is: 2.4330123778781854e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.98
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.23; acc: 0.98
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.98
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.97
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.641025927616283e-05
2.6203784727840684e-05
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.2860857752762782; val_accuracy: 0.9368033439490446 

The current subspace-distance is: 2.6203784727840684e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.16; acc: 1.0
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.6517470511607826e-05
2.6226452973787673e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.28571825669069956; val_accuracy: 0.9382961783439491 

The current subspace-distance is: 2.6226452973787673e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.98
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.729417171096429e-05
2.482840500306338e-05
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.28475534071208564; val_accuracy: 0.9377985668789809 

The current subspace-distance is: 2.482840500306338e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.701276313629933e-05
2.5318937332485802e-05
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2780138758147598; val_accuracy: 0.9381966560509554 

The current subspace-distance is: 2.5318937332485802e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.52; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.93 

5.7835597544908524e-05
2.6911875465884805e-05
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.278332998608328; val_accuracy: 0.9383957006369427 

The current subspace-distance is: 2.6911875465884805e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.24; acc: 1.0
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.861413956154138e-05
2.6861218429985456e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2743768709575295; val_accuracy: 0.9383957006369427 

The current subspace-distance is: 2.6861218429985456e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.98
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.946061355643906e-05
2.7036086976295337e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.27191092434582437; val_accuracy: 0.9378980891719745 

The current subspace-distance is: 2.7036086976295337e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.98
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.98
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.903776400373317e-05
2.6115843866136856e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.2656802566852539; val_accuracy: 0.9384952229299363 

The current subspace-distance is: 2.6115843866136856e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.98
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.13; acc: 1.0
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.966726166661829e-05
2.6800906198332086e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2663543226707513; val_accuracy: 0.9405851910828026 

The current subspace-distance is: 2.6800906198332086e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.98
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.095444405218586e-05
2.8852015020675026e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.26344210118245165; val_accuracy: 0.9402866242038217 

The current subspace-distance is: 2.8852015020675026e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.0018406657036394e-05
2.751117426669225e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2619676290993478; val_accuracy: 0.940187101910828 

The current subspace-distance is: 2.751117426669225e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.98
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.42; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

5.9837573644472286e-05
2.7075820980826393e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2614322677729236; val_accuracy: 0.9406847133757962 

The current subspace-distance is: 2.7075820980826393e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.81
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.040216612746008e-05
2.7436019081505947e-05
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.26599325054580236; val_accuracy: 0.9379976114649682 

The current subspace-distance is: 2.7436019081505947e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.98
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.128956738393754e-05
2.789752215903718e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.25960319550933353; val_accuracy: 0.9405851910828026 

The current subspace-distance is: 2.789752215903718e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.17; acc: 1.0
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.0708178352797404e-05
2.7274687454337254e-05
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.26150287426770874; val_accuracy: 0.9419785031847133 

The current subspace-distance is: 2.7274687454337254e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.98
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.113322888268158e-05
2.839988883351907e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.26096663002375586; val_accuracy: 0.9414808917197452 

The current subspace-distance is: 2.839988883351907e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.97
Batch: 380; loss: 0.28; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.98
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.142618804005906e-05
2.8421914976206608e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.2581150202424663; val_accuracy: 0.9399880573248408 

The current subspace-distance is: 2.8421914976206608e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.98
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.98
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.085044879000634e-05
2.8008418667013757e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.2570513085385037; val_accuracy: 0.9407842356687898 

The current subspace-distance is: 2.8008418667013757e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.98
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.98
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.93 

6.124351057223976e-05
2.794100873870775e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.25789964668879845; val_accuracy: 0.9415804140127388 

The current subspace-distance is: 2.794100873870775e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:48/N_9_flips_False_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:48/N_9_flips_False_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
