model : table13slim
N : 4
flips : True
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.3
schedule_freq : 10
seed : 1
n_epochs : 30
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : True
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-29 15:59:47

Channel scaling factor: 2.75

The number of parameters is: 269845

The number of individual parameters is:

22
352
22
22
33
40656
33
33
66
121968
66
66
64
101376
64
64
4096
64
640
10
64
64

nonzero elements in E: 13492248
elements in E: 13492250
fraction nonzero: 0.999999851766755
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.47; acc: 0.12
Batch: 20; loss: 2.42; acc: 0.09
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.25; acc: 0.12
Batch: 100; loss: 2.21; acc: 0.17
Batch: 120; loss: 2.1; acc: 0.19
Batch: 140; loss: 2.19; acc: 0.19
Batch: 160; loss: 2.21; acc: 0.16
Batch: 180; loss: 2.04; acc: 0.3
Batch: 200; loss: 2.13; acc: 0.31
Batch: 220; loss: 2.12; acc: 0.19
Batch: 240; loss: 2.02; acc: 0.3
Batch: 260; loss: 2.13; acc: 0.2
Batch: 280; loss: 2.06; acc: 0.33
Batch: 300; loss: 2.06; acc: 0.27
Batch: 320; loss: 2.12; acc: 0.19
Batch: 340; loss: 2.06; acc: 0.34
Batch: 360; loss: 1.97; acc: 0.39
Batch: 380; loss: 2.0; acc: 0.41
Batch: 400; loss: 2.04; acc: 0.38
Batch: 420; loss: 1.88; acc: 0.47
Batch: 440; loss: 2.02; acc: 0.34
Batch: 460; loss: 2.03; acc: 0.27
Batch: 480; loss: 1.98; acc: 0.38
Batch: 500; loss: 1.9; acc: 0.47
Batch: 520; loss: 1.92; acc: 0.38
Batch: 540; loss: 1.91; acc: 0.39
Batch: 560; loss: 1.9; acc: 0.48
Batch: 580; loss: 1.96; acc: 0.39
Batch: 600; loss: 2.07; acc: 0.28
Batch: 620; loss: 1.92; acc: 0.5
Batch: 640; loss: 1.92; acc: 0.44
Batch: 660; loss: 1.76; acc: 0.56
Batch: 680; loss: 1.95; acc: 0.39
Batch: 700; loss: 1.89; acc: 0.45
Batch: 720; loss: 1.85; acc: 0.48
Batch: 740; loss: 1.87; acc: 0.41
Batch: 760; loss: 1.94; acc: 0.31
Batch: 780; loss: 1.81; acc: 0.55
Train Epoch over. train_loss: 2.03; train_accuracy: 0.33 

2.414583468635101e-05
4.2094916352652945e-06
Batch: 0; loss: 1.83; acc: 0.53
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.78; acc: 0.42
Batch: 60; loss: 1.79; acc: 0.55
Batch: 80; loss: 1.79; acc: 0.48
Batch: 100; loss: 1.86; acc: 0.5
Batch: 120; loss: 1.95; acc: 0.39
Batch: 140; loss: 1.86; acc: 0.45
Val Epoch over. val_loss: 1.8718144650671893; val_accuracy: 0.4368033439490446 

The current subspace-distance is: 4.2094916352652945e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.94; acc: 0.41
Batch: 20; loss: 1.84; acc: 0.45
Batch: 40; loss: 1.8; acc: 0.53
Batch: 60; loss: 1.94; acc: 0.41
Batch: 80; loss: 1.85; acc: 0.48
Batch: 100; loss: 1.9; acc: 0.36
Batch: 120; loss: 1.83; acc: 0.48
Batch: 140; loss: 1.9; acc: 0.36
Batch: 160; loss: 1.92; acc: 0.33
Batch: 180; loss: 1.93; acc: 0.38
Batch: 200; loss: 1.9; acc: 0.39
Batch: 220; loss: 1.93; acc: 0.38
Batch: 240; loss: 1.8; acc: 0.56
Batch: 260; loss: 1.84; acc: 0.47
Batch: 280; loss: 1.84; acc: 0.41
Batch: 300; loss: 1.81; acc: 0.45
Batch: 320; loss: 1.82; acc: 0.41
Batch: 340; loss: 1.75; acc: 0.48
Batch: 360; loss: 1.75; acc: 0.56
Batch: 380; loss: 1.88; acc: 0.44
Batch: 400; loss: 1.93; acc: 0.48
Batch: 420; loss: 1.83; acc: 0.44
Batch: 440; loss: 1.89; acc: 0.42
Batch: 460; loss: 1.84; acc: 0.39
Batch: 480; loss: 1.75; acc: 0.5
Batch: 500; loss: 1.96; acc: 0.28
Batch: 520; loss: 1.86; acc: 0.39
Batch: 540; loss: 1.9; acc: 0.44
Batch: 560; loss: 2.07; acc: 0.31
Batch: 580; loss: 1.74; acc: 0.55
Batch: 600; loss: 1.81; acc: 0.47
Batch: 620; loss: 1.81; acc: 0.53
Batch: 640; loss: 1.9; acc: 0.41
Batch: 660; loss: 1.96; acc: 0.3
Batch: 680; loss: 1.93; acc: 0.34
Batch: 700; loss: 1.9; acc: 0.41
Batch: 720; loss: 1.83; acc: 0.41
Batch: 740; loss: 1.88; acc: 0.38
Batch: 760; loss: 1.76; acc: 0.47
Batch: 780; loss: 1.87; acc: 0.42
Train Epoch over. train_loss: 1.86; train_accuracy: 0.42 

2.760419920377899e-05
6.252617822610773e-06
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.96; acc: 0.28
Batch: 40; loss: 1.72; acc: 0.39
Batch: 60; loss: 1.75; acc: 0.52
Batch: 80; loss: 1.79; acc: 0.45
Batch: 100; loss: 1.81; acc: 0.5
Batch: 120; loss: 1.88; acc: 0.39
Batch: 140; loss: 1.77; acc: 0.44
Val Epoch over. val_loss: 1.8130926713821993; val_accuracy: 0.43919187898089174 

The current subspace-distance is: 6.252617822610773e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.9; acc: 0.33
Batch: 20; loss: 1.86; acc: 0.34
Batch: 40; loss: 1.84; acc: 0.42
Batch: 60; loss: 1.9; acc: 0.39
Batch: 80; loss: 1.87; acc: 0.42
Batch: 100; loss: 1.73; acc: 0.45
Batch: 120; loss: 1.94; acc: 0.33
Batch: 140; loss: 1.9; acc: 0.38
Batch: 160; loss: 1.8; acc: 0.52
Batch: 180; loss: 1.78; acc: 0.41
Batch: 200; loss: 1.81; acc: 0.39
Batch: 220; loss: 1.76; acc: 0.45
Batch: 240; loss: 1.81; acc: 0.41
Batch: 260; loss: 1.85; acc: 0.41
Batch: 280; loss: 1.78; acc: 0.45
Batch: 300; loss: 2.01; acc: 0.19
Batch: 320; loss: 1.77; acc: 0.39
Batch: 340; loss: 1.72; acc: 0.52
Batch: 360; loss: 1.75; acc: 0.45
Batch: 380; loss: 1.87; acc: 0.41
Batch: 400; loss: 1.73; acc: 0.44
Batch: 420; loss: 1.81; acc: 0.42
Batch: 440; loss: 1.72; acc: 0.5
Batch: 460; loss: 1.78; acc: 0.41
Batch: 480; loss: 1.88; acc: 0.36
Batch: 500; loss: 1.83; acc: 0.39
Batch: 520; loss: 1.79; acc: 0.39
Batch: 540; loss: 1.83; acc: 0.39
Batch: 560; loss: 1.78; acc: 0.44
Batch: 580; loss: 1.68; acc: 0.48
Batch: 600; loss: 1.72; acc: 0.44
Batch: 620; loss: 1.83; acc: 0.31
Batch: 640; loss: 1.77; acc: 0.47
Batch: 660; loss: 1.79; acc: 0.36
Batch: 680; loss: 1.83; acc: 0.28
Batch: 700; loss: 1.75; acc: 0.45
Batch: 720; loss: 1.71; acc: 0.48
Batch: 740; loss: 1.8; acc: 0.41
Batch: 760; loss: 1.77; acc: 0.38
Batch: 780; loss: 1.68; acc: 0.42
Train Epoch over. train_loss: 1.81; train_accuracy: 0.42 

2.8793985620723106e-05
6.177170234877849e-06
Batch: 0; loss: 1.74; acc: 0.44
Batch: 20; loss: 1.89; acc: 0.28
Batch: 40; loss: 1.63; acc: 0.45
Batch: 60; loss: 1.68; acc: 0.53
Batch: 80; loss: 1.76; acc: 0.34
Batch: 100; loss: 1.74; acc: 0.5
Batch: 120; loss: 1.78; acc: 0.39
Batch: 140; loss: 1.7; acc: 0.45
Val Epoch over. val_loss: 1.7501956444637032; val_accuracy: 0.4325238853503185 

The current subspace-distance is: 6.177170234877849e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.7; acc: 0.47
Batch: 40; loss: 1.78; acc: 0.36
Batch: 60; loss: 1.67; acc: 0.48
Batch: 80; loss: 1.79; acc: 0.38
Batch: 100; loss: 1.68; acc: 0.52
Batch: 120; loss: 1.68; acc: 0.56
Batch: 140; loss: 1.82; acc: 0.39
Batch: 160; loss: 1.76; acc: 0.42
Batch: 180; loss: 1.81; acc: 0.45
Batch: 200; loss: 1.76; acc: 0.42
Batch: 220; loss: 1.73; acc: 0.52
Batch: 240; loss: 1.71; acc: 0.53
Batch: 260; loss: 1.72; acc: 0.48
Batch: 280; loss: 1.74; acc: 0.42
Batch: 300; loss: 1.72; acc: 0.41
Batch: 320; loss: 1.64; acc: 0.56
Batch: 340; loss: 1.72; acc: 0.52
Batch: 360; loss: 1.71; acc: 0.44
Batch: 380; loss: 1.68; acc: 0.39
Batch: 400; loss: 1.74; acc: 0.45
Batch: 420; loss: 1.77; acc: 0.39
Batch: 440; loss: 1.7; acc: 0.47
Batch: 460; loss: 1.77; acc: 0.52
Batch: 480; loss: 1.66; acc: 0.47
Batch: 500; loss: 1.7; acc: 0.41
Batch: 520; loss: 1.93; acc: 0.28
Batch: 540; loss: 1.75; acc: 0.44
Batch: 560; loss: 1.78; acc: 0.36
Batch: 580; loss: 1.76; acc: 0.42
Batch: 600; loss: 1.64; acc: 0.55
Batch: 620; loss: 1.66; acc: 0.45
Batch: 640; loss: 1.7; acc: 0.42
Batch: 660; loss: 1.65; acc: 0.44
Batch: 680; loss: 1.55; acc: 0.55
Batch: 700; loss: 1.82; acc: 0.28
Batch: 720; loss: 1.61; acc: 0.52
Batch: 740; loss: 1.64; acc: 0.48
Batch: 760; loss: 1.66; acc: 0.52
Batch: 780; loss: 1.67; acc: 0.5
Train Epoch over. train_loss: 1.73; train_accuracy: 0.44 

3.085867137997411e-05
7.098086371115642e-06
Batch: 0; loss: 1.7; acc: 0.52
Batch: 20; loss: 1.8; acc: 0.36
Batch: 40; loss: 1.58; acc: 0.48
Batch: 60; loss: 1.63; acc: 0.5
Batch: 80; loss: 1.67; acc: 0.44
Batch: 100; loss: 1.71; acc: 0.41
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.65; acc: 0.42
Val Epoch over. val_loss: 1.685387668336273; val_accuracy: 0.4773089171974522 

The current subspace-distance is: 7.098086371115642e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.68; acc: 0.55
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.68; acc: 0.48
Batch: 60; loss: 1.82; acc: 0.36
Batch: 80; loss: 1.68; acc: 0.47
Batch: 100; loss: 1.65; acc: 0.48
Batch: 120; loss: 1.68; acc: 0.48
Batch: 140; loss: 1.74; acc: 0.47
Batch: 160; loss: 1.79; acc: 0.41
Batch: 180; loss: 1.59; acc: 0.55
Batch: 200; loss: 1.67; acc: 0.52
Batch: 220; loss: 1.68; acc: 0.52
Batch: 240; loss: 1.5; acc: 0.62
Batch: 260; loss: 1.74; acc: 0.45
Batch: 280; loss: 1.55; acc: 0.58
Batch: 300; loss: 1.58; acc: 0.55
Batch: 320; loss: 1.57; acc: 0.52
Batch: 340; loss: 1.62; acc: 0.52
Batch: 360; loss: 1.55; acc: 0.55
Batch: 380; loss: 1.78; acc: 0.39
Batch: 400; loss: 1.71; acc: 0.47
Batch: 420; loss: 1.79; acc: 0.42
Batch: 440; loss: 1.59; acc: 0.59
Batch: 460; loss: 1.71; acc: 0.41
Batch: 480; loss: 1.68; acc: 0.45
Batch: 500; loss: 1.67; acc: 0.52
Batch: 520; loss: 1.68; acc: 0.45
Batch: 540; loss: 1.75; acc: 0.36
Batch: 560; loss: 1.66; acc: 0.47
Batch: 580; loss: 1.69; acc: 0.47
Batch: 600; loss: 1.63; acc: 0.5
Batch: 620; loss: 1.54; acc: 0.64
Batch: 640; loss: 1.57; acc: 0.58
Batch: 660; loss: 1.81; acc: 0.39
Batch: 680; loss: 1.6; acc: 0.53
Batch: 700; loss: 1.78; acc: 0.42
Batch: 720; loss: 1.64; acc: 0.53
Batch: 740; loss: 1.61; acc: 0.47
Batch: 760; loss: 1.6; acc: 0.5
Batch: 780; loss: 1.69; acc: 0.5
Train Epoch over. train_loss: 1.68; train_accuracy: 0.47 

3.199914135620929e-05
8.608044481661636e-06
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.75; acc: 0.41
Batch: 40; loss: 1.54; acc: 0.5
Batch: 60; loss: 1.63; acc: 0.52
Batch: 80; loss: 1.62; acc: 0.48
Batch: 100; loss: 1.67; acc: 0.39
Batch: 120; loss: 1.7; acc: 0.53
Batch: 140; loss: 1.65; acc: 0.38
Val Epoch over. val_loss: 1.6568902024797574; val_accuracy: 0.49233678343949044 

The current subspace-distance is: 8.608044481661636e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.47
Batch: 20; loss: 1.58; acc: 0.56
Batch: 40; loss: 1.56; acc: 0.5
Batch: 60; loss: 1.73; acc: 0.39
Batch: 80; loss: 1.79; acc: 0.41
Batch: 100; loss: 1.55; acc: 0.53
Batch: 120; loss: 1.81; acc: 0.45
Batch: 140; loss: 1.71; acc: 0.45
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.8; acc: 0.36
Batch: 200; loss: 1.64; acc: 0.48
Batch: 220; loss: 1.5; acc: 0.59
Batch: 240; loss: 1.6; acc: 0.53
Batch: 260; loss: 1.76; acc: 0.39
Batch: 280; loss: 1.65; acc: 0.48
Batch: 300; loss: 1.7; acc: 0.47
Batch: 320; loss: 1.81; acc: 0.42
Batch: 340; loss: 1.73; acc: 0.38
Batch: 360; loss: 1.71; acc: 0.55
Batch: 380; loss: 1.71; acc: 0.44
Batch: 400; loss: 1.83; acc: 0.39
Batch: 420; loss: 1.71; acc: 0.45
Batch: 440; loss: 1.82; acc: 0.36
Batch: 460; loss: 1.58; acc: 0.52
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.66; acc: 0.48
Batch: 520; loss: 1.78; acc: 0.52
Batch: 540; loss: 1.82; acc: 0.36
Batch: 560; loss: 1.52; acc: 0.62
Batch: 580; loss: 1.68; acc: 0.5
Batch: 600; loss: 1.6; acc: 0.53
Batch: 620; loss: 1.84; acc: 0.34
Batch: 640; loss: 1.59; acc: 0.55
Batch: 660; loss: 1.67; acc: 0.53
Batch: 680; loss: 1.68; acc: 0.47
Batch: 700; loss: 1.67; acc: 0.42
Batch: 720; loss: 1.78; acc: 0.44
Batch: 740; loss: 1.78; acc: 0.44
Batch: 760; loss: 1.76; acc: 0.39
Batch: 780; loss: 1.79; acc: 0.34
Train Epoch over. train_loss: 1.67; train_accuracy: 0.48 

3.210053910152055e-05
7.356549758696929e-06
Batch: 0; loss: 1.65; acc: 0.5
Batch: 20; loss: 1.74; acc: 0.48
Batch: 40; loss: 1.5; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.52
Batch: 80; loss: 1.58; acc: 0.52
Batch: 100; loss: 1.67; acc: 0.41
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.66; acc: 0.38
Val Epoch over. val_loss: 1.6391973700493006; val_accuracy: 0.5002985668789809 

The current subspace-distance is: 7.356549758696929e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.74; acc: 0.42
Batch: 20; loss: 1.66; acc: 0.45
Batch: 40; loss: 1.72; acc: 0.5
Batch: 60; loss: 1.74; acc: 0.45
Batch: 80; loss: 1.72; acc: 0.41
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.63; acc: 0.52
Batch: 140; loss: 1.54; acc: 0.61
Batch: 160; loss: 1.62; acc: 0.44
Batch: 180; loss: 1.72; acc: 0.45
Batch: 200; loss: 1.64; acc: 0.52
Batch: 220; loss: 1.68; acc: 0.48
Batch: 240; loss: 1.52; acc: 0.62
Batch: 260; loss: 1.7; acc: 0.5
Batch: 280; loss: 1.75; acc: 0.48
Batch: 300; loss: 1.55; acc: 0.53
Batch: 320; loss: 1.63; acc: 0.5
Batch: 340; loss: 1.64; acc: 0.53
Batch: 360; loss: 1.54; acc: 0.56
Batch: 380; loss: 1.64; acc: 0.48
Batch: 400; loss: 1.6; acc: 0.45
Batch: 420; loss: 1.6; acc: 0.45
Batch: 440; loss: 1.66; acc: 0.47
Batch: 460; loss: 1.63; acc: 0.48
Batch: 480; loss: 1.74; acc: 0.47
Batch: 500; loss: 1.76; acc: 0.42
Batch: 520; loss: 1.63; acc: 0.48
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.62; acc: 0.53
Batch: 600; loss: 1.59; acc: 0.53
Batch: 620; loss: 1.81; acc: 0.36
Batch: 640; loss: 1.63; acc: 0.47
Batch: 660; loss: 1.72; acc: 0.44
Batch: 680; loss: 1.61; acc: 0.45
Batch: 700; loss: 1.68; acc: 0.44
Batch: 720; loss: 1.68; acc: 0.52
Batch: 740; loss: 1.6; acc: 0.55
Batch: 760; loss: 1.69; acc: 0.44
Batch: 780; loss: 1.71; acc: 0.5
Train Epoch over. train_loss: 1.65; train_accuracy: 0.49 

3.379088593646884e-05
9.108922313316725e-06
Batch: 0; loss: 1.65; acc: 0.48
Batch: 20; loss: 1.75; acc: 0.42
Batch: 40; loss: 1.48; acc: 0.52
Batch: 60; loss: 1.66; acc: 0.5
Batch: 80; loss: 1.57; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.39
Batch: 120; loss: 1.7; acc: 0.5
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.6314532202520189; val_accuracy: 0.5000995222929936 

The current subspace-distance is: 9.108922313316725e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.66; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.41
Batch: 40; loss: 1.51; acc: 0.55
Batch: 60; loss: 1.67; acc: 0.45
Batch: 80; loss: 1.61; acc: 0.5
Batch: 100; loss: 1.73; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.38
Batch: 140; loss: 1.57; acc: 0.55
Batch: 160; loss: 1.59; acc: 0.55
Batch: 180; loss: 1.69; acc: 0.48
Batch: 200; loss: 1.82; acc: 0.41
Batch: 220; loss: 1.69; acc: 0.5
Batch: 240; loss: 1.67; acc: 0.41
Batch: 260; loss: 1.55; acc: 0.48
Batch: 280; loss: 1.59; acc: 0.56
Batch: 300; loss: 1.68; acc: 0.45
Batch: 320; loss: 1.8; acc: 0.42
Batch: 340; loss: 1.63; acc: 0.53
Batch: 360; loss: 1.58; acc: 0.56
Batch: 380; loss: 1.51; acc: 0.61
Batch: 400; loss: 1.59; acc: 0.58
Batch: 420; loss: 1.81; acc: 0.39
Batch: 440; loss: 1.71; acc: 0.47
Batch: 460; loss: 1.66; acc: 0.45
Batch: 480; loss: 1.66; acc: 0.5
Batch: 500; loss: 1.56; acc: 0.55
Batch: 520; loss: 1.61; acc: 0.56
Batch: 540; loss: 1.58; acc: 0.52
Batch: 560; loss: 1.73; acc: 0.44
Batch: 580; loss: 1.48; acc: 0.62
Batch: 600; loss: 1.6; acc: 0.55
Batch: 620; loss: 1.62; acc: 0.45
Batch: 640; loss: 1.71; acc: 0.41
Batch: 660; loss: 1.43; acc: 0.66
Batch: 680; loss: 1.6; acc: 0.48
Batch: 700; loss: 1.66; acc: 0.5
Batch: 720; loss: 1.59; acc: 0.56
Batch: 740; loss: 1.62; acc: 0.48
Batch: 760; loss: 1.84; acc: 0.3
Batch: 780; loss: 1.58; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.49 

3.470564843155444e-05
9.777114428288769e-06
Batch: 0; loss: 1.64; acc: 0.48
Batch: 20; loss: 1.76; acc: 0.38
Batch: 40; loss: 1.46; acc: 0.53
Batch: 60; loss: 1.67; acc: 0.47
Batch: 80; loss: 1.55; acc: 0.59
Batch: 100; loss: 1.66; acc: 0.41
Batch: 120; loss: 1.69; acc: 0.52
Batch: 140; loss: 1.66; acc: 0.34
Val Epoch over. val_loss: 1.6202484399649748; val_accuracy: 0.5060708598726115 

The current subspace-distance is: 9.777114428288769e-06 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.6; acc: 0.45
Batch: 20; loss: 1.66; acc: 0.42
Batch: 40; loss: 1.72; acc: 0.48
Batch: 60; loss: 1.67; acc: 0.5
Batch: 80; loss: 1.71; acc: 0.48
Batch: 100; loss: 1.49; acc: 0.67
Batch: 120; loss: 1.52; acc: 0.61
Batch: 140; loss: 1.53; acc: 0.56
Batch: 160; loss: 1.63; acc: 0.45
Batch: 180; loss: 1.5; acc: 0.56
Batch: 200; loss: 1.58; acc: 0.59
Batch: 220; loss: 1.67; acc: 0.42
Batch: 240; loss: 1.77; acc: 0.36
Batch: 260; loss: 1.64; acc: 0.53
Batch: 280; loss: 1.61; acc: 0.44
Batch: 300; loss: 1.61; acc: 0.5
Batch: 320; loss: 1.69; acc: 0.48
Batch: 340; loss: 1.57; acc: 0.45
Batch: 360; loss: 1.51; acc: 0.62
Batch: 380; loss: 1.54; acc: 0.55
Batch: 400; loss: 1.57; acc: 0.5
Batch: 420; loss: 1.61; acc: 0.42
Batch: 440; loss: 1.54; acc: 0.56
Batch: 460; loss: 1.6; acc: 0.5
Batch: 480; loss: 1.61; acc: 0.52
Batch: 500; loss: 1.68; acc: 0.42
Batch: 520; loss: 1.72; acc: 0.42
Batch: 540; loss: 1.62; acc: 0.47
Batch: 560; loss: 1.66; acc: 0.55
Batch: 580; loss: 1.47; acc: 0.67
Batch: 600; loss: 1.68; acc: 0.5
Batch: 620; loss: 1.55; acc: 0.59
Batch: 640; loss: 1.54; acc: 0.55
Batch: 660; loss: 1.65; acc: 0.53
Batch: 680; loss: 1.54; acc: 0.58
Batch: 700; loss: 1.6; acc: 0.62
Batch: 720; loss: 1.63; acc: 0.44
Batch: 740; loss: 1.59; acc: 0.45
Batch: 760; loss: 1.64; acc: 0.44
Batch: 780; loss: 1.47; acc: 0.66
Train Epoch over. train_loss: 1.63; train_accuracy: 0.5 

3.529348396114074e-05
7.842580998840276e-06
Batch: 0; loss: 1.61; acc: 0.53
Batch: 20; loss: 1.77; acc: 0.33
Batch: 40; loss: 1.44; acc: 0.53
Batch: 60; loss: 1.67; acc: 0.45
Batch: 80; loss: 1.52; acc: 0.59
Batch: 100; loss: 1.65; acc: 0.45
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.64; acc: 0.42
Val Epoch over. val_loss: 1.6030290407739627; val_accuracy: 0.5153264331210191 

The current subspace-distance is: 7.842580998840276e-06 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.58; acc: 0.61
Batch: 20; loss: 1.72; acc: 0.39
Batch: 40; loss: 1.66; acc: 0.47
Batch: 60; loss: 1.66; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.55
Batch: 100; loss: 1.49; acc: 0.5
Batch: 120; loss: 1.71; acc: 0.41
Batch: 140; loss: 1.61; acc: 0.5
Batch: 160; loss: 1.64; acc: 0.52
Batch: 180; loss: 1.51; acc: 0.53
Batch: 200; loss: 1.62; acc: 0.45
Batch: 220; loss: 1.69; acc: 0.39
Batch: 240; loss: 1.67; acc: 0.52
Batch: 260; loss: 1.61; acc: 0.45
Batch: 280; loss: 1.59; acc: 0.58
Batch: 300; loss: 1.56; acc: 0.42
Batch: 320; loss: 1.62; acc: 0.55
Batch: 340; loss: 1.69; acc: 0.44
Batch: 360; loss: 1.53; acc: 0.61
Batch: 380; loss: 1.51; acc: 0.58
Batch: 400; loss: 1.65; acc: 0.5
Batch: 420; loss: 1.7; acc: 0.47
Batch: 440; loss: 1.9; acc: 0.3
Batch: 460; loss: 1.62; acc: 0.48
Batch: 480; loss: 1.61; acc: 0.44
Batch: 500; loss: 1.61; acc: 0.58
Batch: 520; loss: 1.54; acc: 0.55
Batch: 540; loss: 1.53; acc: 0.58
Batch: 560; loss: 1.63; acc: 0.45
Batch: 580; loss: 1.55; acc: 0.48
Batch: 600; loss: 1.58; acc: 0.55
Batch: 620; loss: 1.66; acc: 0.47
Batch: 640; loss: 1.6; acc: 0.53
Batch: 660; loss: 1.66; acc: 0.5
Batch: 680; loss: 1.53; acc: 0.56
Batch: 700; loss: 1.63; acc: 0.5
Batch: 720; loss: 1.53; acc: 0.56
Batch: 740; loss: 1.6; acc: 0.47
Batch: 760; loss: 1.6; acc: 0.5
Batch: 780; loss: 1.63; acc: 0.52
Train Epoch over. train_loss: 1.61; train_accuracy: 0.5 

3.619716517277993e-05
9.759402018971741e-06
Batch: 0; loss: 1.6; acc: 0.52
Batch: 20; loss: 1.79; acc: 0.31
Batch: 40; loss: 1.41; acc: 0.56
Batch: 60; loss: 1.66; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.58
Batch: 100; loss: 1.64; acc: 0.44
Batch: 120; loss: 1.68; acc: 0.5
Batch: 140; loss: 1.62; acc: 0.41
Val Epoch over. val_loss: 1.5838681444240983; val_accuracy: 0.5197054140127388 

The current subspace-distance is: 9.759402018971741e-06 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.67; acc: 0.48
Batch: 20; loss: 1.58; acc: 0.5
Batch: 40; loss: 1.63; acc: 0.47
Batch: 60; loss: 1.52; acc: 0.53
Batch: 80; loss: 1.51; acc: 0.55
Batch: 100; loss: 1.57; acc: 0.56
Batch: 120; loss: 1.49; acc: 0.59
Batch: 140; loss: 1.65; acc: 0.52
Batch: 160; loss: 1.62; acc: 0.53
Batch: 180; loss: 1.58; acc: 0.52
Batch: 200; loss: 1.6; acc: 0.5
Batch: 220; loss: 1.61; acc: 0.53
Batch: 240; loss: 1.63; acc: 0.47
Batch: 260; loss: 1.6; acc: 0.5
Batch: 280; loss: 1.74; acc: 0.45
Batch: 300; loss: 1.65; acc: 0.53
Batch: 320; loss: 1.47; acc: 0.55
Batch: 340; loss: 1.56; acc: 0.52
Batch: 360; loss: 1.8; acc: 0.45
Batch: 380; loss: 1.67; acc: 0.47
Batch: 400; loss: 1.45; acc: 0.58
Batch: 420; loss: 1.7; acc: 0.39
Batch: 440; loss: 1.7; acc: 0.42
Batch: 460; loss: 1.54; acc: 0.62
Batch: 480; loss: 1.65; acc: 0.44
Batch: 500; loss: 1.69; acc: 0.42
Batch: 520; loss: 1.83; acc: 0.34
Batch: 540; loss: 1.58; acc: 0.47
Batch: 560; loss: 1.74; acc: 0.41
Batch: 580; loss: 1.54; acc: 0.52
Batch: 600; loss: 1.52; acc: 0.55
Batch: 620; loss: 1.69; acc: 0.44
Batch: 640; loss: 1.51; acc: 0.59
Batch: 660; loss: 1.68; acc: 0.41
Batch: 680; loss: 1.53; acc: 0.56
Batch: 700; loss: 1.56; acc: 0.56
Batch: 720; loss: 1.56; acc: 0.55
Batch: 740; loss: 1.69; acc: 0.45
Batch: 760; loss: 1.42; acc: 0.61
Batch: 780; loss: 1.5; acc: 0.59
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

3.6958725104341283e-05
1.0978410500683822e-05
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 1.8; acc: 0.3
Batch: 40; loss: 1.39; acc: 0.62
Batch: 60; loss: 1.63; acc: 0.48
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.63; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.62; acc: 0.39
Val Epoch over. val_loss: 1.570352217953676; val_accuracy: 0.5215963375796179 

The current subspace-distance is: 1.0978410500683822e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.6; acc: 0.45
Batch: 40; loss: 1.56; acc: 0.56
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 1.54; acc: 0.58
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.52; acc: 0.56
Batch: 140; loss: 1.73; acc: 0.38
Batch: 160; loss: 1.53; acc: 0.55
Batch: 180; loss: 1.75; acc: 0.42
Batch: 200; loss: 1.46; acc: 0.58
Batch: 220; loss: 1.44; acc: 0.59
Batch: 240; loss: 1.49; acc: 0.59
Batch: 260; loss: 1.64; acc: 0.42
Batch: 280; loss: 1.65; acc: 0.45
Batch: 300; loss: 1.56; acc: 0.47
Batch: 320; loss: 1.65; acc: 0.53
Batch: 340; loss: 1.63; acc: 0.53
Batch: 360; loss: 1.58; acc: 0.52
Batch: 380; loss: 1.67; acc: 0.45
Batch: 400; loss: 1.68; acc: 0.45
Batch: 420; loss: 1.45; acc: 0.53
Batch: 440; loss: 1.66; acc: 0.48
Batch: 460; loss: 1.52; acc: 0.59
Batch: 480; loss: 1.6; acc: 0.48
Batch: 500; loss: 1.58; acc: 0.45
Batch: 520; loss: 1.63; acc: 0.52
Batch: 540; loss: 1.56; acc: 0.58
Batch: 560; loss: 1.46; acc: 0.55
Batch: 580; loss: 1.75; acc: 0.44
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.7; acc: 0.41
Batch: 640; loss: 1.67; acc: 0.45
Batch: 660; loss: 1.65; acc: 0.48
Batch: 680; loss: 1.49; acc: 0.55
Batch: 700; loss: 1.52; acc: 0.58
Batch: 720; loss: 1.55; acc: 0.56
Batch: 740; loss: 1.68; acc: 0.34
Batch: 760; loss: 1.57; acc: 0.52
Batch: 780; loss: 1.69; acc: 0.42
Train Epoch over. train_loss: 1.6; train_accuracy: 0.51 

3.728149022208527e-05
1.008833078230964e-05
Batch: 0; loss: 1.58; acc: 0.56
Batch: 20; loss: 1.8; acc: 0.31
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 1.63; acc: 0.48
Batch: 80; loss: 1.46; acc: 0.59
Batch: 100; loss: 1.63; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.59; acc: 0.44
Val Epoch over. val_loss: 1.5652225837586031; val_accuracy: 0.5236863057324841 

The current subspace-distance is: 1.008833078230964e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.48
Batch: 20; loss: 1.46; acc: 0.64
Batch: 40; loss: 1.51; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.55; acc: 0.55
Batch: 100; loss: 1.59; acc: 0.47
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.73; acc: 0.39
Batch: 160; loss: 1.57; acc: 0.52
Batch: 180; loss: 1.63; acc: 0.47
Batch: 200; loss: 1.68; acc: 0.48
Batch: 220; loss: 1.61; acc: 0.45
Batch: 240; loss: 1.54; acc: 0.56
Batch: 260; loss: 1.67; acc: 0.41
Batch: 280; loss: 1.75; acc: 0.42
Batch: 300; loss: 1.42; acc: 0.59
Batch: 320; loss: 1.55; acc: 0.53
Batch: 340; loss: 1.62; acc: 0.48
Batch: 360; loss: 1.69; acc: 0.41
Batch: 380; loss: 1.5; acc: 0.59
Batch: 400; loss: 1.49; acc: 0.58
Batch: 420; loss: 1.47; acc: 0.62
Batch: 440; loss: 1.62; acc: 0.52
Batch: 460; loss: 1.62; acc: 0.52
Batch: 480; loss: 1.6; acc: 0.53
Batch: 500; loss: 1.66; acc: 0.53
Batch: 520; loss: 1.67; acc: 0.45
Batch: 540; loss: 1.6; acc: 0.5
Batch: 560; loss: 1.54; acc: 0.61
Batch: 580; loss: 1.57; acc: 0.5
Batch: 600; loss: 1.58; acc: 0.56
Batch: 620; loss: 1.55; acc: 0.52
Batch: 640; loss: 1.62; acc: 0.52
Batch: 660; loss: 1.58; acc: 0.48
Batch: 680; loss: 1.48; acc: 0.61
Batch: 700; loss: 1.67; acc: 0.5
Batch: 720; loss: 1.54; acc: 0.52
Batch: 740; loss: 1.64; acc: 0.44
Batch: 760; loss: 1.52; acc: 0.59
Batch: 780; loss: 1.63; acc: 0.47
Train Epoch over. train_loss: 1.59; train_accuracy: 0.51 

3.7639863876393065e-05
1.0690851922845468e-05
Batch: 0; loss: 1.59; acc: 0.56
Batch: 20; loss: 1.81; acc: 0.31
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 1.64; acc: 0.48
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.65; acc: 0.41
Batch: 120; loss: 1.67; acc: 0.5
Batch: 140; loss: 1.6; acc: 0.44
Val Epoch over. val_loss: 1.570721379510916; val_accuracy: 0.5252786624203821 

The current subspace-distance is: 1.0690851922845468e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.64; acc: 0.48
Batch: 20; loss: 1.64; acc: 0.52
Batch: 40; loss: 1.5; acc: 0.47
Batch: 60; loss: 1.57; acc: 0.52
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.68; acc: 0.45
Batch: 120; loss: 1.7; acc: 0.41
Batch: 140; loss: 1.53; acc: 0.61
Batch: 160; loss: 1.55; acc: 0.52
Batch: 180; loss: 1.71; acc: 0.47
Batch: 200; loss: 1.5; acc: 0.55
Batch: 220; loss: 1.56; acc: 0.5
Batch: 240; loss: 1.5; acc: 0.56
Batch: 260; loss: 1.76; acc: 0.39
Batch: 280; loss: 1.61; acc: 0.55
Batch: 300; loss: 1.59; acc: 0.52
Batch: 320; loss: 1.59; acc: 0.5
Batch: 340; loss: 1.45; acc: 0.62
Batch: 360; loss: 1.78; acc: 0.41
Batch: 380; loss: 1.49; acc: 0.53
Batch: 400; loss: 1.49; acc: 0.56
Batch: 420; loss: 1.63; acc: 0.44
Batch: 440; loss: 1.65; acc: 0.53
Batch: 460; loss: 1.5; acc: 0.55
Batch: 480; loss: 1.61; acc: 0.55
Batch: 500; loss: 1.73; acc: 0.38
Batch: 520; loss: 1.56; acc: 0.5
Batch: 540; loss: 1.53; acc: 0.53
Batch: 560; loss: 1.62; acc: 0.5
Batch: 580; loss: 1.72; acc: 0.39
Batch: 600; loss: 1.66; acc: 0.5
Batch: 620; loss: 1.52; acc: 0.58
Batch: 640; loss: 1.6; acc: 0.5
Batch: 660; loss: 1.48; acc: 0.58
Batch: 680; loss: 1.55; acc: 0.53
Batch: 700; loss: 1.54; acc: 0.56
Batch: 720; loss: 1.55; acc: 0.58
Batch: 740; loss: 1.67; acc: 0.45
Batch: 760; loss: 1.52; acc: 0.56
Batch: 780; loss: 1.58; acc: 0.64
Train Epoch over. train_loss: 1.59; train_accuracy: 0.51 

3.7313329812604934e-05
1.0681088497221936e-05
Batch: 0; loss: 1.59; acc: 0.55
Batch: 20; loss: 1.82; acc: 0.31
Batch: 40; loss: 1.39; acc: 0.59
Batch: 60; loss: 1.64; acc: 0.48
Batch: 80; loss: 1.45; acc: 0.64
Batch: 100; loss: 1.63; acc: 0.42
Batch: 120; loss: 1.67; acc: 0.52
Batch: 140; loss: 1.59; acc: 0.41
Val Epoch over. val_loss: 1.5654314293223581; val_accuracy: 0.5302547770700637 

The current subspace-distance is: 1.0681088497221936e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.55; acc: 0.53
Batch: 20; loss: 1.58; acc: 0.48
Batch: 40; loss: 1.67; acc: 0.47
Batch: 60; loss: 1.7; acc: 0.47
Batch: 80; loss: 1.59; acc: 0.55
Batch: 100; loss: 1.73; acc: 0.41
Batch: 120; loss: 1.51; acc: 0.56
Batch: 140; loss: 1.64; acc: 0.45
Batch: 160; loss: 1.57; acc: 0.5
Batch: 180; loss: 1.59; acc: 0.41
Batch: 200; loss: 1.65; acc: 0.39
Batch: 220; loss: 1.52; acc: 0.52
Batch: 240; loss: 1.58; acc: 0.5
Batch: 260; loss: 1.61; acc: 0.48
Batch: 280; loss: 1.66; acc: 0.44
Batch: 300; loss: 1.73; acc: 0.44
Batch: 320; loss: 1.64; acc: 0.47
Batch: 340; loss: 1.55; acc: 0.5
Batch: 360; loss: 1.67; acc: 0.47
Batch: 380; loss: 1.51; acc: 0.53
Batch: 400; loss: 1.54; acc: 0.52
Batch: 420; loss: 1.41; acc: 0.61
Batch: 440; loss: 1.54; acc: 0.55
Batch: 460; loss: 1.43; acc: 0.61
Batch: 480; loss: 1.58; acc: 0.48
Batch: 500; loss: 1.52; acc: 0.58
Batch: 520; loss: 1.52; acc: 0.58
Batch: 540; loss: 1.48; acc: 0.52
Batch: 560; loss: 1.63; acc: 0.5
Batch: 580; loss: 1.7; acc: 0.48
Batch: 600; loss: 1.63; acc: 0.45
Batch: 620; loss: 1.54; acc: 0.48
Batch: 640; loss: 1.59; acc: 0.47
Batch: 660; loss: 1.68; acc: 0.42
Batch: 680; loss: 1.53; acc: 0.52
Batch: 700; loss: 1.47; acc: 0.55
Batch: 720; loss: 1.49; acc: 0.62
Batch: 740; loss: 1.57; acc: 0.61
Batch: 760; loss: 1.59; acc: 0.55
Batch: 780; loss: 1.67; acc: 0.5
Train Epoch over. train_loss: 1.59; train_accuracy: 0.51 

3.823633960564621e-05
9.87410567176994e-06
Batch: 0; loss: 1.57; acc: 0.58
Batch: 20; loss: 1.83; acc: 0.3
Batch: 40; loss: 1.38; acc: 0.59
Batch: 60; loss: 1.64; acc: 0.5
Batch: 80; loss: 1.44; acc: 0.62
Batch: 100; loss: 1.64; acc: 0.41
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.59; acc: 0.41
Val Epoch over. val_loss: 1.5639771701423986; val_accuracy: 0.5319466560509554 

The current subspace-distance is: 9.87410567176994e-06 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.65; acc: 0.55
Batch: 20; loss: 1.56; acc: 0.61
Batch: 40; loss: 1.51; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.5; acc: 0.56
Batch: 100; loss: 1.63; acc: 0.48
Batch: 120; loss: 1.59; acc: 0.53
Batch: 140; loss: 1.52; acc: 0.56
Batch: 160; loss: 1.67; acc: 0.45
Batch: 180; loss: 1.49; acc: 0.56
Batch: 200; loss: 1.73; acc: 0.36
Batch: 220; loss: 1.67; acc: 0.52
Batch: 240; loss: 1.54; acc: 0.47
Batch: 260; loss: 1.61; acc: 0.47
Batch: 280; loss: 1.58; acc: 0.48
Batch: 300; loss: 1.66; acc: 0.42
Batch: 320; loss: 1.55; acc: 0.58
Batch: 340; loss: 1.47; acc: 0.56
Batch: 360; loss: 1.62; acc: 0.47
Batch: 380; loss: 1.51; acc: 0.52
Batch: 400; loss: 1.7; acc: 0.39
Batch: 420; loss: 1.66; acc: 0.52
Batch: 440; loss: 1.45; acc: 0.55
Batch: 460; loss: 1.56; acc: 0.61
Batch: 480; loss: 1.59; acc: 0.47
Batch: 500; loss: 1.72; acc: 0.34
Batch: 520; loss: 1.64; acc: 0.44
Batch: 540; loss: 1.59; acc: 0.64
Batch: 560; loss: 1.7; acc: 0.48
Batch: 580; loss: 1.54; acc: 0.5
Batch: 600; loss: 1.7; acc: 0.33
Batch: 620; loss: 1.7; acc: 0.45
Batch: 640; loss: 1.55; acc: 0.47
Batch: 660; loss: 1.56; acc: 0.55
Batch: 680; loss: 1.48; acc: 0.58
Batch: 700; loss: 1.67; acc: 0.41
Batch: 720; loss: 1.7; acc: 0.48
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.7; acc: 0.5
Batch: 780; loss: 1.46; acc: 0.62
Train Epoch over. train_loss: 1.59; train_accuracy: 0.51 

3.867604391416535e-05
1.1658368748612702e-05
Batch: 0; loss: 1.57; acc: 0.56
Batch: 20; loss: 1.84; acc: 0.33
Batch: 40; loss: 1.37; acc: 0.59
Batch: 60; loss: 1.62; acc: 0.52
Batch: 80; loss: 1.44; acc: 0.64
Batch: 100; loss: 1.63; acc: 0.42
Batch: 120; loss: 1.65; acc: 0.48
Batch: 140; loss: 1.58; acc: 0.39
Val Epoch over. val_loss: 1.553911828691033; val_accuracy: 0.5324442675159236 

The current subspace-distance is: 1.1658368748612702e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.6; acc: 0.5
Batch: 20; loss: 1.58; acc: 0.52
Batch: 40; loss: 1.65; acc: 0.44
Batch: 60; loss: 1.6; acc: 0.53
Batch: 80; loss: 1.53; acc: 0.52
Batch: 100; loss: 1.65; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.39
Batch: 140; loss: 1.72; acc: 0.45
Batch: 160; loss: 1.57; acc: 0.52
Batch: 180; loss: 1.59; acc: 0.45
Batch: 200; loss: 1.68; acc: 0.42
Batch: 220; loss: 1.67; acc: 0.53
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.57; acc: 0.48
Batch: 280; loss: 1.63; acc: 0.52
Batch: 300; loss: 1.61; acc: 0.39
Batch: 320; loss: 1.55; acc: 0.58
Batch: 340; loss: 1.75; acc: 0.41
Batch: 360; loss: 1.55; acc: 0.5
Batch: 380; loss: 1.47; acc: 0.55
Batch: 400; loss: 1.52; acc: 0.55
Batch: 420; loss: 1.56; acc: 0.55
Batch: 440; loss: 1.41; acc: 0.62
Batch: 460; loss: 1.63; acc: 0.52
Batch: 480; loss: 1.6; acc: 0.48
Batch: 500; loss: 1.55; acc: 0.38
Batch: 520; loss: 1.49; acc: 0.59
Batch: 540; loss: 1.58; acc: 0.59
Batch: 560; loss: 1.56; acc: 0.56
Batch: 580; loss: 1.58; acc: 0.5
Batch: 600; loss: 1.66; acc: 0.5
Batch: 620; loss: 1.54; acc: 0.53
Batch: 640; loss: 1.57; acc: 0.5
Batch: 660; loss: 1.64; acc: 0.42
Batch: 680; loss: 1.6; acc: 0.48
Batch: 700; loss: 1.5; acc: 0.56
Batch: 720; loss: 1.81; acc: 0.42
Batch: 740; loss: 1.47; acc: 0.56
Batch: 760; loss: 1.61; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.45
Train Epoch over. train_loss: 1.58; train_accuracy: 0.51 

3.864748214255087e-05
1.0499294148758054e-05
Batch: 0; loss: 1.57; acc: 0.56
Batch: 20; loss: 1.84; acc: 0.36
Batch: 40; loss: 1.38; acc: 0.58
Batch: 60; loss: 1.64; acc: 0.52
Batch: 80; loss: 1.44; acc: 0.62
Batch: 100; loss: 1.64; acc: 0.44
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.59; acc: 0.41
Val Epoch over. val_loss: 1.5622408443195805; val_accuracy: 0.5343351910828026 

The current subspace-distance is: 1.0499294148758054e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.62; acc: 0.45
Batch: 20; loss: 1.69; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.58
Batch: 60; loss: 1.58; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.48
Batch: 100; loss: 1.51; acc: 0.56
Batch: 120; loss: 1.45; acc: 0.55
Batch: 140; loss: 1.57; acc: 0.53
Batch: 160; loss: 1.57; acc: 0.55
Batch: 180; loss: 1.48; acc: 0.59
Batch: 200; loss: 1.54; acc: 0.45
Batch: 220; loss: 1.51; acc: 0.55
Batch: 240; loss: 1.57; acc: 0.58
Batch: 260; loss: 1.55; acc: 0.44
Batch: 280; loss: 1.51; acc: 0.55
Batch: 300; loss: 1.85; acc: 0.36
Batch: 320; loss: 1.55; acc: 0.58
Batch: 340; loss: 1.5; acc: 0.61
Batch: 360; loss: 1.49; acc: 0.61
Batch: 380; loss: 1.4; acc: 0.7
Batch: 400; loss: 1.5; acc: 0.52
Batch: 420; loss: 1.62; acc: 0.45
Batch: 440; loss: 1.45; acc: 0.56
Batch: 460; loss: 1.44; acc: 0.59
Batch: 480; loss: 1.61; acc: 0.56
Batch: 500; loss: 1.6; acc: 0.5
Batch: 520; loss: 1.58; acc: 0.55
Batch: 540; loss: 1.47; acc: 0.48
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.63; acc: 0.48
Batch: 620; loss: 1.47; acc: 0.56
Batch: 640; loss: 1.61; acc: 0.45
Batch: 660; loss: 1.7; acc: 0.48
Batch: 680; loss: 1.51; acc: 0.55
Batch: 700; loss: 1.55; acc: 0.53
Batch: 720; loss: 1.59; acc: 0.47
Batch: 740; loss: 1.5; acc: 0.59
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.56; acc: 0.52
Train Epoch over. train_loss: 1.58; train_accuracy: 0.51 

3.838104385067709e-05
1.0111988558492158e-05
Batch: 0; loss: 1.56; acc: 0.56
Batch: 20; loss: 1.83; acc: 0.31
Batch: 40; loss: 1.35; acc: 0.56
Batch: 60; loss: 1.62; acc: 0.5
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.62; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.57; acc: 0.41
Val Epoch over. val_loss: 1.5430234366921103; val_accuracy: 0.5278662420382165 

The current subspace-distance is: 1.0111988558492158e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.57; acc: 0.55
Batch: 20; loss: 1.64; acc: 0.48
Batch: 40; loss: 1.61; acc: 0.41
Batch: 60; loss: 1.52; acc: 0.58
Batch: 80; loss: 1.62; acc: 0.55
Batch: 100; loss: 1.83; acc: 0.39
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.64; acc: 0.44
Batch: 160; loss: 1.64; acc: 0.47
Batch: 180; loss: 1.49; acc: 0.55
Batch: 200; loss: 1.58; acc: 0.55
Batch: 220; loss: 1.55; acc: 0.53
Batch: 240; loss: 1.64; acc: 0.47
Batch: 260; loss: 1.58; acc: 0.5
Batch: 280; loss: 1.66; acc: 0.48
Batch: 300; loss: 1.69; acc: 0.42
Batch: 320; loss: 1.5; acc: 0.58
Batch: 340; loss: 1.44; acc: 0.62
Batch: 360; loss: 1.56; acc: 0.47
Batch: 380; loss: 1.6; acc: 0.44
Batch: 400; loss: 1.54; acc: 0.53
Batch: 420; loss: 1.76; acc: 0.38
Batch: 440; loss: 1.75; acc: 0.48
Batch: 460; loss: 1.62; acc: 0.44
Batch: 480; loss: 1.48; acc: 0.48
Batch: 500; loss: 1.53; acc: 0.53
Batch: 520; loss: 1.6; acc: 0.47
Batch: 540; loss: 1.53; acc: 0.53
Batch: 560; loss: 1.7; acc: 0.48
Batch: 580; loss: 1.51; acc: 0.56
Batch: 600; loss: 1.58; acc: 0.56
Batch: 620; loss: 1.4; acc: 0.56
Batch: 640; loss: 1.6; acc: 0.47
Batch: 660; loss: 1.64; acc: 0.52
Batch: 680; loss: 1.79; acc: 0.38
Batch: 700; loss: 1.58; acc: 0.44
Batch: 720; loss: 1.6; acc: 0.5
Batch: 740; loss: 1.43; acc: 0.66
Batch: 760; loss: 1.45; acc: 0.61
Batch: 780; loss: 1.63; acc: 0.53
Train Epoch over. train_loss: 1.58; train_accuracy: 0.51 

3.913720865966752e-05
1.0397690857644193e-05
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.84; acc: 0.33
Batch: 40; loss: 1.36; acc: 0.56
Batch: 60; loss: 1.62; acc: 0.52
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.61; acc: 0.45
Batch: 120; loss: 1.65; acc: 0.47
Batch: 140; loss: 1.57; acc: 0.41
Val Epoch over. val_loss: 1.546612652244082; val_accuracy: 0.537718949044586 

The current subspace-distance is: 1.0397690857644193e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.68; acc: 0.47
Batch: 20; loss: 1.5; acc: 0.55
Batch: 40; loss: 1.63; acc: 0.42
Batch: 60; loss: 1.66; acc: 0.47
Batch: 80; loss: 1.57; acc: 0.48
Batch: 100; loss: 1.6; acc: 0.53
Batch: 120; loss: 1.53; acc: 0.48
Batch: 140; loss: 1.58; acc: 0.53
Batch: 160; loss: 1.54; acc: 0.56
Batch: 180; loss: 1.74; acc: 0.38
Batch: 200; loss: 1.51; acc: 0.61
Batch: 220; loss: 1.65; acc: 0.47
Batch: 240; loss: 1.52; acc: 0.55
Batch: 260; loss: 1.52; acc: 0.53
Batch: 280; loss: 1.49; acc: 0.56
Batch: 300; loss: 1.47; acc: 0.53
Batch: 320; loss: 1.75; acc: 0.36
Batch: 340; loss: 1.91; acc: 0.3
Batch: 360; loss: 1.68; acc: 0.39
Batch: 380; loss: 1.41; acc: 0.62
Batch: 400; loss: 1.69; acc: 0.48
Batch: 420; loss: 1.6; acc: 0.5
Batch: 440; loss: 1.68; acc: 0.47
Batch: 460; loss: 1.68; acc: 0.45
Batch: 480; loss: 1.46; acc: 0.59
Batch: 500; loss: 1.6; acc: 0.52
Batch: 520; loss: 1.69; acc: 0.48
Batch: 540; loss: 1.58; acc: 0.48
Batch: 560; loss: 1.56; acc: 0.52
Batch: 580; loss: 1.6; acc: 0.48
Batch: 600; loss: 1.6; acc: 0.52
Batch: 620; loss: 1.62; acc: 0.52
Batch: 640; loss: 1.5; acc: 0.58
Batch: 660; loss: 1.63; acc: 0.53
Batch: 680; loss: 1.55; acc: 0.53
Batch: 700; loss: 1.58; acc: 0.55
Batch: 720; loss: 1.58; acc: 0.48
Batch: 740; loss: 1.63; acc: 0.58
Batch: 760; loss: 1.58; acc: 0.45
Batch: 780; loss: 1.49; acc: 0.56
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

3.87508662242908e-05
1.000706834020093e-05
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.84; acc: 0.3
Batch: 40; loss: 1.34; acc: 0.55
Batch: 60; loss: 1.62; acc: 0.52
Batch: 80; loss: 1.43; acc: 0.64
Batch: 100; loss: 1.6; acc: 0.45
Batch: 120; loss: 1.65; acc: 0.47
Batch: 140; loss: 1.55; acc: 0.42
Val Epoch over. val_loss: 1.5403997055284537; val_accuracy: 0.5355294585987261 

The current subspace-distance is: 1.000706834020093e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.73; acc: 0.48
Batch: 20; loss: 1.48; acc: 0.58
Batch: 40; loss: 1.64; acc: 0.47
Batch: 60; loss: 1.56; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.47
Batch: 100; loss: 1.64; acc: 0.41
Batch: 120; loss: 1.42; acc: 0.62
Batch: 140; loss: 1.58; acc: 0.45
Batch: 160; loss: 1.54; acc: 0.55
Batch: 180; loss: 1.62; acc: 0.48
Batch: 200; loss: 1.59; acc: 0.45
Batch: 220; loss: 1.66; acc: 0.47
Batch: 240; loss: 1.43; acc: 0.56
Batch: 260; loss: 1.65; acc: 0.47
Batch: 280; loss: 1.55; acc: 0.53
Batch: 300; loss: 1.47; acc: 0.62
Batch: 320; loss: 1.58; acc: 0.5
Batch: 340; loss: 1.43; acc: 0.66
Batch: 360; loss: 1.61; acc: 0.5
Batch: 380; loss: 1.5; acc: 0.52
Batch: 400; loss: 1.68; acc: 0.48
Batch: 420; loss: 1.51; acc: 0.61
Batch: 440; loss: 1.62; acc: 0.48
Batch: 460; loss: 1.52; acc: 0.56
Batch: 480; loss: 1.61; acc: 0.45
Batch: 500; loss: 1.64; acc: 0.45
Batch: 520; loss: 1.73; acc: 0.38
Batch: 540; loss: 1.61; acc: 0.5
Batch: 560; loss: 1.64; acc: 0.48
Batch: 580; loss: 1.55; acc: 0.52
Batch: 600; loss: 1.65; acc: 0.48
Batch: 620; loss: 1.54; acc: 0.48
Batch: 640; loss: 1.79; acc: 0.47
Batch: 660; loss: 1.54; acc: 0.58
Batch: 680; loss: 1.37; acc: 0.62
Batch: 700; loss: 1.49; acc: 0.56
Batch: 720; loss: 1.57; acc: 0.45
Batch: 740; loss: 1.56; acc: 0.52
Batch: 760; loss: 1.43; acc: 0.64
Batch: 780; loss: 1.48; acc: 0.58
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

3.9565911720274016e-05
1.246264946530573e-05
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.85; acc: 0.33
Batch: 40; loss: 1.35; acc: 0.56
Batch: 60; loss: 1.62; acc: 0.53
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.62; acc: 0.44
Batch: 120; loss: 1.65; acc: 0.48
Batch: 140; loss: 1.56; acc: 0.42
Val Epoch over. val_loss: 1.5431134761518734; val_accuracy: 0.5368232484076433 

The current subspace-distance is: 1.246264946530573e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.69; acc: 0.45
Batch: 20; loss: 1.76; acc: 0.44
Batch: 40; loss: 1.5; acc: 0.56
Batch: 60; loss: 1.45; acc: 0.56
Batch: 80; loss: 1.6; acc: 0.5
Batch: 100; loss: 1.54; acc: 0.59
Batch: 120; loss: 1.64; acc: 0.5
Batch: 140; loss: 1.39; acc: 0.59
Batch: 160; loss: 1.58; acc: 0.52
Batch: 180; loss: 1.63; acc: 0.55
Batch: 200; loss: 1.46; acc: 0.58
Batch: 220; loss: 1.63; acc: 0.48
Batch: 240; loss: 1.57; acc: 0.48
Batch: 260; loss: 1.48; acc: 0.64
Batch: 280; loss: 1.61; acc: 0.52
Batch: 300; loss: 1.6; acc: 0.53
Batch: 320; loss: 1.56; acc: 0.48
Batch: 340; loss: 1.56; acc: 0.55
Batch: 360; loss: 1.61; acc: 0.48
Batch: 380; loss: 1.46; acc: 0.56
Batch: 400; loss: 1.69; acc: 0.41
Batch: 420; loss: 1.57; acc: 0.45
Batch: 440; loss: 1.6; acc: 0.55
Batch: 460; loss: 1.61; acc: 0.47
Batch: 480; loss: 1.64; acc: 0.48
Batch: 500; loss: 1.49; acc: 0.52
Batch: 520; loss: 1.54; acc: 0.55
Batch: 540; loss: 1.62; acc: 0.48
Batch: 560; loss: 1.71; acc: 0.44
Batch: 580; loss: 1.5; acc: 0.62
Batch: 600; loss: 1.44; acc: 0.59
Batch: 620; loss: 1.58; acc: 0.52
Batch: 640; loss: 1.59; acc: 0.52
Batch: 660; loss: 1.49; acc: 0.56
Batch: 680; loss: 1.59; acc: 0.55
Batch: 700; loss: 1.58; acc: 0.53
Batch: 720; loss: 1.59; acc: 0.56
Batch: 740; loss: 1.56; acc: 0.47
Batch: 760; loss: 1.56; acc: 0.48
Batch: 780; loss: 1.59; acc: 0.47
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

3.958609522669576e-05
1.2920762856083456e-05
Batch: 0; loss: 1.56; acc: 0.55
Batch: 20; loss: 1.87; acc: 0.3
Batch: 40; loss: 1.35; acc: 0.56
Batch: 60; loss: 1.62; acc: 0.52
Batch: 80; loss: 1.42; acc: 0.61
Batch: 100; loss: 1.63; acc: 0.45
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.57; acc: 0.41
Val Epoch over. val_loss: 1.5481921069940943; val_accuracy: 0.5314490445859873 

The current subspace-distance is: 1.2920762856083456e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.57; acc: 0.5
Batch: 20; loss: 1.53; acc: 0.58
Batch: 40; loss: 1.62; acc: 0.53
Batch: 60; loss: 1.53; acc: 0.44
Batch: 80; loss: 1.45; acc: 0.58
Batch: 100; loss: 1.55; acc: 0.55
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.66; acc: 0.5
Batch: 160; loss: 1.51; acc: 0.58
Batch: 180; loss: 1.59; acc: 0.44
Batch: 200; loss: 1.47; acc: 0.58
Batch: 220; loss: 1.61; acc: 0.48
Batch: 240; loss: 1.35; acc: 0.62
Batch: 260; loss: 1.61; acc: 0.47
Batch: 280; loss: 1.68; acc: 0.48
Batch: 300; loss: 1.69; acc: 0.38
Batch: 320; loss: 1.58; acc: 0.55
Batch: 340; loss: 1.61; acc: 0.5
Batch: 360; loss: 1.51; acc: 0.55
Batch: 380; loss: 1.39; acc: 0.62
Batch: 400; loss: 1.67; acc: 0.45
Batch: 420; loss: 1.6; acc: 0.59
Batch: 440; loss: 1.65; acc: 0.44
Batch: 460; loss: 1.54; acc: 0.58
Batch: 480; loss: 1.48; acc: 0.58
Batch: 500; loss: 1.51; acc: 0.56
Batch: 520; loss: 1.51; acc: 0.56
Batch: 540; loss: 1.49; acc: 0.5
Batch: 560; loss: 1.62; acc: 0.55
Batch: 580; loss: 1.47; acc: 0.53
Batch: 600; loss: 1.64; acc: 0.42
Batch: 620; loss: 1.4; acc: 0.66
Batch: 640; loss: 1.55; acc: 0.48
Batch: 660; loss: 1.55; acc: 0.52
Batch: 680; loss: 1.48; acc: 0.61
Batch: 700; loss: 1.4; acc: 0.59
Batch: 720; loss: 1.68; acc: 0.41
Batch: 740; loss: 1.61; acc: 0.45
Batch: 760; loss: 1.63; acc: 0.48
Batch: 780; loss: 1.51; acc: 0.59
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

4.0127531974576414e-05
1.217526732943952e-05
Batch: 0; loss: 1.54; acc: 0.56
Batch: 20; loss: 1.85; acc: 0.3
Batch: 40; loss: 1.33; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.53
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.61; acc: 0.44
Batch: 120; loss: 1.65; acc: 0.48
Batch: 140; loss: 1.54; acc: 0.42
Val Epoch over. val_loss: 1.5314812622252543; val_accuracy: 0.5384156050955414 

The current subspace-distance is: 1.217526732943952e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.49; acc: 0.58
Batch: 20; loss: 1.63; acc: 0.47
Batch: 40; loss: 1.55; acc: 0.47
Batch: 60; loss: 1.66; acc: 0.48
Batch: 80; loss: 1.63; acc: 0.47
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.58; acc: 0.53
Batch: 140; loss: 1.51; acc: 0.53
Batch: 160; loss: 1.5; acc: 0.55
Batch: 180; loss: 1.57; acc: 0.53
Batch: 200; loss: 1.55; acc: 0.56
Batch: 220; loss: 1.72; acc: 0.41
Batch: 240; loss: 1.69; acc: 0.39
Batch: 260; loss: 1.57; acc: 0.52
Batch: 280; loss: 1.56; acc: 0.53
Batch: 300; loss: 1.47; acc: 0.55
Batch: 320; loss: 1.59; acc: 0.5
Batch: 340; loss: 1.5; acc: 0.56
Batch: 360; loss: 1.52; acc: 0.59
Batch: 380; loss: 1.55; acc: 0.5
Batch: 400; loss: 1.45; acc: 0.59
Batch: 420; loss: 1.68; acc: 0.47
Batch: 440; loss: 1.55; acc: 0.53
Batch: 460; loss: 1.51; acc: 0.55
Batch: 480; loss: 1.58; acc: 0.55
Batch: 500; loss: 1.77; acc: 0.48
Batch: 520; loss: 1.44; acc: 0.58
Batch: 540; loss: 1.48; acc: 0.55
Batch: 560; loss: 1.64; acc: 0.39
Batch: 580; loss: 1.56; acc: 0.55
Batch: 600; loss: 1.65; acc: 0.45
Batch: 620; loss: 1.5; acc: 0.55
Batch: 640; loss: 1.51; acc: 0.59
Batch: 660; loss: 1.62; acc: 0.53
Batch: 680; loss: 1.71; acc: 0.47
Batch: 700; loss: 1.73; acc: 0.42
Batch: 720; loss: 1.59; acc: 0.52
Batch: 740; loss: 1.68; acc: 0.5
Batch: 760; loss: 1.58; acc: 0.53
Batch: 780; loss: 1.58; acc: 0.56
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

4.081633232999593e-05
1.3362022400542628e-05
Batch: 0; loss: 1.55; acc: 0.56
Batch: 20; loss: 1.88; acc: 0.31
Batch: 40; loss: 1.34; acc: 0.56
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.41; acc: 0.61
Batch: 100; loss: 1.62; acc: 0.45
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.56; acc: 0.42
Val Epoch over. val_loss: 1.5402701075669307; val_accuracy: 0.5407046178343949 

The current subspace-distance is: 1.3362022400542628e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.52; acc: 0.55
Batch: 20; loss: 1.55; acc: 0.55
Batch: 40; loss: 1.61; acc: 0.5
Batch: 60; loss: 1.49; acc: 0.53
Batch: 80; loss: 1.7; acc: 0.42
Batch: 100; loss: 1.73; acc: 0.41
Batch: 120; loss: 1.48; acc: 0.55
Batch: 140; loss: 1.66; acc: 0.42
Batch: 160; loss: 1.47; acc: 0.61
Batch: 180; loss: 1.62; acc: 0.55
Batch: 200; loss: 1.65; acc: 0.52
Batch: 220; loss: 1.54; acc: 0.59
Batch: 240; loss: 1.57; acc: 0.52
Batch: 260; loss: 1.4; acc: 0.64
Batch: 280; loss: 1.53; acc: 0.53
Batch: 300; loss: 1.47; acc: 0.56
Batch: 320; loss: 1.63; acc: 0.52
Batch: 340; loss: 1.5; acc: 0.58
Batch: 360; loss: 1.69; acc: 0.53
Batch: 380; loss: 1.43; acc: 0.52
Batch: 400; loss: 1.61; acc: 0.45
Batch: 420; loss: 1.63; acc: 0.44
Batch: 440; loss: 1.55; acc: 0.55
Batch: 460; loss: 1.47; acc: 0.55
Batch: 480; loss: 1.59; acc: 0.5
Batch: 500; loss: 1.43; acc: 0.61
Batch: 520; loss: 1.45; acc: 0.58
Batch: 540; loss: 1.55; acc: 0.48
Batch: 560; loss: 1.52; acc: 0.61
Batch: 580; loss: 1.51; acc: 0.55
Batch: 600; loss: 1.39; acc: 0.64
Batch: 620; loss: 1.42; acc: 0.66
Batch: 640; loss: 1.63; acc: 0.48
Batch: 660; loss: 1.53; acc: 0.55
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.61; acc: 0.41
Batch: 720; loss: 1.62; acc: 0.55
Batch: 740; loss: 1.65; acc: 0.42
Batch: 760; loss: 1.45; acc: 0.56
Batch: 780; loss: 1.52; acc: 0.59
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

3.93919472116977e-05
1.0793704859679565e-05
Batch: 0; loss: 1.56; acc: 0.56
Batch: 20; loss: 1.87; acc: 0.3
Batch: 40; loss: 1.34; acc: 0.58
Batch: 60; loss: 1.62; acc: 0.52
Batch: 80; loss: 1.42; acc: 0.59
Batch: 100; loss: 1.62; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.55; acc: 0.45
Val Epoch over. val_loss: 1.542101853212733; val_accuracy: 0.5362261146496815 

The current subspace-distance is: 1.0793704859679565e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.59; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.59
Batch: 40; loss: 1.65; acc: 0.42
Batch: 60; loss: 1.53; acc: 0.55
Batch: 80; loss: 1.44; acc: 0.56
Batch: 100; loss: 1.69; acc: 0.39
Batch: 120; loss: 1.69; acc: 0.5
Batch: 140; loss: 1.58; acc: 0.58
Batch: 160; loss: 1.5; acc: 0.55
Batch: 180; loss: 1.56; acc: 0.48
Batch: 200; loss: 1.58; acc: 0.52
Batch: 220; loss: 1.55; acc: 0.55
Batch: 240; loss: 1.54; acc: 0.53
Batch: 260; loss: 1.58; acc: 0.56
Batch: 280; loss: 1.45; acc: 0.56
Batch: 300; loss: 1.65; acc: 0.52
Batch: 320; loss: 1.65; acc: 0.48
Batch: 340; loss: 1.53; acc: 0.55
Batch: 360; loss: 1.48; acc: 0.59
Batch: 380; loss: 1.67; acc: 0.41
Batch: 400; loss: 1.66; acc: 0.45
Batch: 420; loss: 1.49; acc: 0.55
Batch: 440; loss: 1.57; acc: 0.52
Batch: 460; loss: 1.65; acc: 0.53
Batch: 480; loss: 1.52; acc: 0.61
Batch: 500; loss: 1.37; acc: 0.55
Batch: 520; loss: 1.54; acc: 0.53
Batch: 540; loss: 1.43; acc: 0.61
Batch: 560; loss: 1.49; acc: 0.53
Batch: 580; loss: 1.5; acc: 0.64
Batch: 600; loss: 1.57; acc: 0.48
Batch: 620; loss: 1.55; acc: 0.58
Batch: 640; loss: 1.58; acc: 0.45
Batch: 660; loss: 1.5; acc: 0.61
Batch: 680; loss: 1.58; acc: 0.55
Batch: 700; loss: 1.61; acc: 0.53
Batch: 720; loss: 1.48; acc: 0.52
Batch: 740; loss: 1.67; acc: 0.41
Batch: 760; loss: 1.48; acc: 0.61
Batch: 780; loss: 1.62; acc: 0.5
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

3.987725722254254e-05
9.89518503047293e-06
Batch: 0; loss: 1.55; acc: 0.55
Batch: 20; loss: 1.86; acc: 0.33
Batch: 40; loss: 1.33; acc: 0.59
Batch: 60; loss: 1.61; acc: 0.55
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.59; acc: 0.44
Batch: 120; loss: 1.66; acc: 0.47
Batch: 140; loss: 1.55; acc: 0.45
Val Epoch over. val_loss: 1.533686398700544; val_accuracy: 0.5326433121019108 

The current subspace-distance is: 9.89518503047293e-06 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.53; acc: 0.58
Batch: 20; loss: 1.52; acc: 0.58
Batch: 40; loss: 1.61; acc: 0.55
Batch: 60; loss: 1.47; acc: 0.53
Batch: 80; loss: 1.58; acc: 0.48
Batch: 100; loss: 1.49; acc: 0.59
Batch: 120; loss: 1.57; acc: 0.47
Batch: 140; loss: 1.85; acc: 0.39
Batch: 160; loss: 1.63; acc: 0.45
Batch: 180; loss: 1.66; acc: 0.53
Batch: 200; loss: 1.47; acc: 0.56
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.47; acc: 0.56
Batch: 260; loss: 1.48; acc: 0.52
Batch: 280; loss: 1.58; acc: 0.5
Batch: 300; loss: 1.5; acc: 0.52
Batch: 320; loss: 1.67; acc: 0.44
Batch: 340; loss: 1.54; acc: 0.52
Batch: 360; loss: 1.53; acc: 0.66
Batch: 380; loss: 1.58; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.52
Batch: 420; loss: 1.46; acc: 0.56
Batch: 440; loss: 1.49; acc: 0.55
Batch: 460; loss: 1.54; acc: 0.58
Batch: 480; loss: 1.51; acc: 0.48
Batch: 500; loss: 1.57; acc: 0.48
Batch: 520; loss: 1.46; acc: 0.56
Batch: 540; loss: 1.63; acc: 0.52
Batch: 560; loss: 1.66; acc: 0.48
Batch: 580; loss: 1.57; acc: 0.5
Batch: 600; loss: 1.55; acc: 0.52
Batch: 620; loss: 1.65; acc: 0.47
Batch: 640; loss: 1.61; acc: 0.5
Batch: 660; loss: 1.6; acc: 0.48
Batch: 680; loss: 1.62; acc: 0.5
Batch: 700; loss: 1.4; acc: 0.58
Batch: 720; loss: 1.59; acc: 0.53
Batch: 740; loss: 1.43; acc: 0.56
Batch: 760; loss: 1.51; acc: 0.58
Batch: 780; loss: 1.62; acc: 0.45
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

4.025342423119582e-05
1.227234497491736e-05
Batch: 0; loss: 1.54; acc: 0.56
Batch: 20; loss: 1.88; acc: 0.3
Batch: 40; loss: 1.33; acc: 0.62
Batch: 60; loss: 1.62; acc: 0.55
Batch: 80; loss: 1.41; acc: 0.59
Batch: 100; loss: 1.62; acc: 0.41
Batch: 120; loss: 1.66; acc: 0.52
Batch: 140; loss: 1.56; acc: 0.42
Val Epoch over. val_loss: 1.5369860655183245; val_accuracy: 0.5424960191082803 

The current subspace-distance is: 1.227234497491736e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.43; acc: 0.64
Batch: 20; loss: 1.44; acc: 0.58
Batch: 40; loss: 1.55; acc: 0.56
Batch: 60; loss: 1.51; acc: 0.59
Batch: 80; loss: 1.66; acc: 0.53
Batch: 100; loss: 1.48; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.53
Batch: 140; loss: 1.52; acc: 0.62
Batch: 160; loss: 1.46; acc: 0.64
Batch: 180; loss: 1.74; acc: 0.41
Batch: 200; loss: 1.5; acc: 0.58
Batch: 220; loss: 1.51; acc: 0.61
Batch: 240; loss: 1.55; acc: 0.48
Batch: 260; loss: 1.51; acc: 0.53
Batch: 280; loss: 1.6; acc: 0.47
Batch: 300; loss: 1.49; acc: 0.55
Batch: 320; loss: 1.62; acc: 0.53
Batch: 340; loss: 1.58; acc: 0.5
Batch: 360; loss: 1.47; acc: 0.61
Batch: 380; loss: 1.48; acc: 0.48
Batch: 400; loss: 1.58; acc: 0.53
Batch: 420; loss: 1.59; acc: 0.5
Batch: 440; loss: 1.51; acc: 0.52
Batch: 460; loss: 1.67; acc: 0.53
Batch: 480; loss: 1.47; acc: 0.56
Batch: 500; loss: 1.61; acc: 0.47
Batch: 520; loss: 1.64; acc: 0.52
Batch: 540; loss: 1.49; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.52
Batch: 580; loss: 1.71; acc: 0.44
Batch: 600; loss: 1.64; acc: 0.48
Batch: 620; loss: 1.67; acc: 0.53
Batch: 640; loss: 1.63; acc: 0.47
Batch: 660; loss: 1.73; acc: 0.36
Batch: 680; loss: 1.69; acc: 0.42
Batch: 700; loss: 1.74; acc: 0.41
Batch: 720; loss: 1.65; acc: 0.45
Batch: 740; loss: 1.58; acc: 0.48
Batch: 760; loss: 1.77; acc: 0.42
Batch: 780; loss: 1.47; acc: 0.59
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

4.00099488615524e-05
1.1114379049104173e-05
Batch: 0; loss: 1.56; acc: 0.56
Batch: 20; loss: 1.87; acc: 0.34
Batch: 40; loss: 1.34; acc: 0.56
Batch: 60; loss: 1.62; acc: 0.55
Batch: 80; loss: 1.43; acc: 0.62
Batch: 100; loss: 1.6; acc: 0.47
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.55; acc: 0.42
Val Epoch over. val_loss: 1.5400893027615394; val_accuracy: 0.5348328025477707 

The current subspace-distance is: 1.1114379049104173e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.44; acc: 0.7
Batch: 20; loss: 1.67; acc: 0.39
Batch: 40; loss: 1.53; acc: 0.56
Batch: 60; loss: 1.59; acc: 0.52
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.52; acc: 0.56
Batch: 120; loss: 1.64; acc: 0.47
Batch: 140; loss: 1.41; acc: 0.64
Batch: 160; loss: 1.52; acc: 0.52
Batch: 180; loss: 1.55; acc: 0.59
Batch: 200; loss: 1.45; acc: 0.69
Batch: 220; loss: 1.42; acc: 0.61
Batch: 240; loss: 1.57; acc: 0.48
Batch: 260; loss: 1.7; acc: 0.42
Batch: 280; loss: 1.57; acc: 0.52
Batch: 300; loss: 1.67; acc: 0.42
Batch: 320; loss: 1.51; acc: 0.61
Batch: 340; loss: 1.69; acc: 0.44
Batch: 360; loss: 1.74; acc: 0.42
Batch: 380; loss: 1.56; acc: 0.55
Batch: 400; loss: 1.53; acc: 0.53
Batch: 420; loss: 1.72; acc: 0.47
Batch: 440; loss: 1.62; acc: 0.48
Batch: 460; loss: 1.49; acc: 0.62
Batch: 480; loss: 1.56; acc: 0.52
Batch: 500; loss: 1.52; acc: 0.44
Batch: 520; loss: 1.64; acc: 0.45
Batch: 540; loss: 1.38; acc: 0.69
Batch: 560; loss: 1.62; acc: 0.45
Batch: 580; loss: 1.51; acc: 0.52
Batch: 600; loss: 1.54; acc: 0.59
Batch: 620; loss: 1.63; acc: 0.47
Batch: 640; loss: 1.48; acc: 0.53
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.66; acc: 0.47
Batch: 700; loss: 1.49; acc: 0.58
Batch: 720; loss: 1.57; acc: 0.53
Batch: 740; loss: 1.35; acc: 0.67
Batch: 760; loss: 1.51; acc: 0.55
Batch: 780; loss: 1.42; acc: 0.64
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

3.991919584223069e-05
1.2845021046814509e-05
Batch: 0; loss: 1.55; acc: 0.56
Batch: 20; loss: 1.85; acc: 0.31
Batch: 40; loss: 1.32; acc: 0.58
Batch: 60; loss: 1.61; acc: 0.53
Batch: 80; loss: 1.42; acc: 0.62
Batch: 100; loss: 1.59; acc: 0.48
Batch: 120; loss: 1.65; acc: 0.52
Batch: 140; loss: 1.53; acc: 0.44
Val Epoch over. val_loss: 1.5335635425178868; val_accuracy: 0.5344347133757962 

The current subspace-distance is: 1.2845021046814509e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 1.6; acc: 0.59
Batch: 20; loss: 1.7; acc: 0.44
Batch: 40; loss: 1.51; acc: 0.55
Batch: 60; loss: 1.47; acc: 0.55
Batch: 80; loss: 1.51; acc: 0.53
Batch: 100; loss: 1.6; acc: 0.5
Batch: 120; loss: 1.47; acc: 0.56
Batch: 140; loss: 1.6; acc: 0.47
Batch: 160; loss: 1.53; acc: 0.53
Batch: 180; loss: 1.65; acc: 0.45
Batch: 200; loss: 1.48; acc: 0.59
Batch: 220; loss: 1.53; acc: 0.59
Batch: 240; loss: 1.65; acc: 0.45
Batch: 260; loss: 1.61; acc: 0.44
Batch: 280; loss: 1.67; acc: 0.47
Batch: 300; loss: 1.45; acc: 0.62
Batch: 320; loss: 1.51; acc: 0.53
Batch: 340; loss: 1.66; acc: 0.42
Batch: 360; loss: 1.55; acc: 0.47
Batch: 380; loss: 1.74; acc: 0.44
Batch: 400; loss: 1.68; acc: 0.41
Batch: 420; loss: 1.46; acc: 0.61
Batch: 440; loss: 1.63; acc: 0.42
Batch: 460; loss: 1.54; acc: 0.52
Batch: 480; loss: 1.54; acc: 0.56
Batch: 500; loss: 1.71; acc: 0.45
Batch: 520; loss: 1.55; acc: 0.52
Batch: 540; loss: 1.75; acc: 0.42
Batch: 560; loss: 1.51; acc: 0.48
Batch: 580; loss: 1.59; acc: 0.5
Batch: 600; loss: 1.55; acc: 0.5
Batch: 620; loss: 1.56; acc: 0.55
Batch: 640; loss: 1.46; acc: 0.64
Batch: 660; loss: 1.47; acc: 0.56
Batch: 680; loss: 1.52; acc: 0.52
Batch: 700; loss: 1.47; acc: 0.56
Batch: 720; loss: 1.6; acc: 0.53
Batch: 740; loss: 1.47; acc: 0.61
Batch: 760; loss: 1.62; acc: 0.52
Batch: 780; loss: 1.64; acc: 0.52
Train Epoch over. train_loss: 1.57; train_accuracy: 0.52 

3.983072019764222e-05
1.2823267752537504e-05
Batch: 0; loss: 1.55; acc: 0.58
Batch: 20; loss: 1.86; acc: 0.34
Batch: 40; loss: 1.33; acc: 0.58
Batch: 60; loss: 1.62; acc: 0.53
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.61; acc: 0.45
Batch: 120; loss: 1.66; acc: 0.5
Batch: 140; loss: 1.54; acc: 0.45
Val Epoch over. val_loss: 1.534034640925705; val_accuracy: 0.5410031847133758 

The current subspace-distance is: 1.2823267752537504e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_4_flips_True_d_dim_50_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.75

The number of parameters is: 269845

The number of individual parameters is:

22
352
22
22
33
40656
33
33
66
121968
66
66
64
101376
64
64
4096
64
640
10
64
64

nonzero elements in E: 26984497
elements in E: 26984500
fraction nonzero: 0.9999998888250663
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.47; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.12
Batch: 40; loss: 2.31; acc: 0.12
Batch: 60; loss: 2.18; acc: 0.17
Batch: 80; loss: 2.14; acc: 0.28
Batch: 100; loss: 2.11; acc: 0.2
Batch: 120; loss: 1.99; acc: 0.38
Batch: 140; loss: 1.98; acc: 0.41
Batch: 160; loss: 1.95; acc: 0.42
Batch: 180; loss: 2.05; acc: 0.34
Batch: 200; loss: 1.93; acc: 0.36
Batch: 220; loss: 1.94; acc: 0.41
Batch: 240; loss: 1.91; acc: 0.36
Batch: 260; loss: 1.89; acc: 0.42
Batch: 280; loss: 1.78; acc: 0.58
Batch: 300; loss: 1.9; acc: 0.48
Batch: 320; loss: 1.86; acc: 0.42
Batch: 340; loss: 1.77; acc: 0.56
Batch: 360; loss: 1.75; acc: 0.56
Batch: 380; loss: 1.84; acc: 0.53
Batch: 400; loss: 1.78; acc: 0.53
Batch: 420; loss: 1.75; acc: 0.55
Batch: 440; loss: 1.71; acc: 0.56
Batch: 460; loss: 1.74; acc: 0.5
Batch: 480; loss: 1.72; acc: 0.58
Batch: 500; loss: 1.64; acc: 0.58
Batch: 520; loss: 1.62; acc: 0.59
Batch: 540; loss: 1.67; acc: 0.61
Batch: 560; loss: 1.77; acc: 0.5
Batch: 580; loss: 1.71; acc: 0.55
Batch: 600; loss: 1.57; acc: 0.58
Batch: 620; loss: 1.68; acc: 0.55
Batch: 640; loss: 1.59; acc: 0.66
Batch: 660; loss: 1.67; acc: 0.53
Batch: 680; loss: 1.65; acc: 0.58
Batch: 700; loss: 1.69; acc: 0.56
Batch: 720; loss: 1.66; acc: 0.59
Batch: 740; loss: 1.63; acc: 0.53
Batch: 760; loss: 1.66; acc: 0.53
Batch: 780; loss: 1.64; acc: 0.55
Train Epoch over. train_loss: 1.84; train_accuracy: 0.45 

5.882661935174838e-05
5.350247738533653e-05
Batch: 0; loss: 1.6; acc: 0.62
Batch: 20; loss: 1.7; acc: 0.52
Batch: 40; loss: 1.44; acc: 0.72
Batch: 60; loss: 1.56; acc: 0.64
Batch: 80; loss: 1.53; acc: 0.62
Batch: 100; loss: 1.6; acc: 0.61
Batch: 120; loss: 1.66; acc: 0.59
Batch: 140; loss: 1.44; acc: 0.62
Val Epoch over. val_loss: 1.584669583921979; val_accuracy: 0.5940485668789809 

The current subspace-distance is: 5.350247738533653e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.6; acc: 0.62
Batch: 20; loss: 1.58; acc: 0.56
Batch: 40; loss: 1.57; acc: 0.59
Batch: 60; loss: 1.58; acc: 0.59
Batch: 80; loss: 1.61; acc: 0.56
Batch: 100; loss: 1.54; acc: 0.59
Batch: 120; loss: 1.46; acc: 0.62
Batch: 140; loss: 1.52; acc: 0.64
Batch: 160; loss: 1.52; acc: 0.55
Batch: 180; loss: 1.55; acc: 0.56
Batch: 200; loss: 1.52; acc: 0.62
Batch: 220; loss: 1.55; acc: 0.61
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.67; acc: 0.52
Batch: 280; loss: 1.66; acc: 0.56
Batch: 300; loss: 1.44; acc: 0.67
Batch: 320; loss: 1.5; acc: 0.61
Batch: 340; loss: 1.58; acc: 0.56
Batch: 360; loss: 1.57; acc: 0.5
Batch: 380; loss: 1.46; acc: 0.64
Batch: 400; loss: 1.6; acc: 0.47
Batch: 420; loss: 1.43; acc: 0.73
Batch: 440; loss: 1.48; acc: 0.64
Batch: 460; loss: 1.4; acc: 0.61
Batch: 480; loss: 1.56; acc: 0.59
Batch: 500; loss: 1.61; acc: 0.58
Batch: 520; loss: 1.43; acc: 0.64
Batch: 540; loss: 1.51; acc: 0.53
Batch: 560; loss: 1.52; acc: 0.61
Batch: 580; loss: 1.54; acc: 0.59
Batch: 600; loss: 1.51; acc: 0.64
Batch: 620; loss: 1.39; acc: 0.7
Batch: 640; loss: 1.39; acc: 0.66
Batch: 660; loss: 1.46; acc: 0.69
Batch: 680; loss: 1.37; acc: 0.7
Batch: 700; loss: 1.42; acc: 0.61
Batch: 720; loss: 1.55; acc: 0.58
Batch: 740; loss: 1.42; acc: 0.7
Batch: 760; loss: 1.57; acc: 0.53
Batch: 780; loss: 1.43; acc: 0.66
Train Epoch over. train_loss: 1.55; train_accuracy: 0.59 

7.837208977434784e-05
7.282825390575454e-05
Batch: 0; loss: 1.43; acc: 0.72
Batch: 20; loss: 1.59; acc: 0.56
Batch: 40; loss: 1.27; acc: 0.73
Batch: 60; loss: 1.47; acc: 0.64
Batch: 80; loss: 1.44; acc: 0.69
Batch: 100; loss: 1.51; acc: 0.62
Batch: 120; loss: 1.6; acc: 0.55
Batch: 140; loss: 1.32; acc: 0.75
Val Epoch over. val_loss: 1.464899515650075; val_accuracy: 0.6358479299363057 

The current subspace-distance is: 7.282825390575454e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.58
Batch: 20; loss: 1.44; acc: 0.66
Batch: 40; loss: 1.6; acc: 0.48
Batch: 60; loss: 1.45; acc: 0.66
Batch: 80; loss: 1.57; acc: 0.61
Batch: 100; loss: 1.41; acc: 0.66
Batch: 120; loss: 1.47; acc: 0.66
Batch: 140; loss: 1.5; acc: 0.58
Batch: 160; loss: 1.59; acc: 0.59
Batch: 180; loss: 1.47; acc: 0.61
Batch: 200; loss: 1.43; acc: 0.64
Batch: 220; loss: 1.49; acc: 0.59
Batch: 240; loss: 1.42; acc: 0.61
Batch: 260; loss: 1.49; acc: 0.61
Batch: 280; loss: 1.48; acc: 0.61
Batch: 300; loss: 1.41; acc: 0.58
Batch: 320; loss: 1.45; acc: 0.58
Batch: 340; loss: 1.45; acc: 0.66
Batch: 360; loss: 1.5; acc: 0.55
Batch: 380; loss: 1.4; acc: 0.69
Batch: 400; loss: 1.38; acc: 0.75
Batch: 420; loss: 1.47; acc: 0.62
Batch: 440; loss: 1.37; acc: 0.66
Batch: 460; loss: 1.34; acc: 0.73
Batch: 480; loss: 1.35; acc: 0.66
Batch: 500; loss: 1.46; acc: 0.61
Batch: 520; loss: 1.42; acc: 0.64
Batch: 540; loss: 1.46; acc: 0.67
Batch: 560; loss: 1.4; acc: 0.64
Batch: 580; loss: 1.47; acc: 0.59
Batch: 600; loss: 1.37; acc: 0.72
Batch: 620; loss: 1.53; acc: 0.61
Batch: 640; loss: 1.36; acc: 0.61
Batch: 660; loss: 1.59; acc: 0.52
Batch: 680; loss: 1.5; acc: 0.56
Batch: 700; loss: 1.35; acc: 0.69
Batch: 720; loss: 1.47; acc: 0.61
Batch: 740; loss: 1.36; acc: 0.7
Batch: 760; loss: 1.36; acc: 0.62
Batch: 780; loss: 1.36; acc: 0.67
Train Epoch over. train_loss: 1.45; train_accuracy: 0.63 

9.272794704884291e-05
8.752720168558881e-05
Batch: 0; loss: 1.35; acc: 0.69
Batch: 20; loss: 1.5; acc: 0.58
Batch: 40; loss: 1.17; acc: 0.8
Batch: 60; loss: 1.39; acc: 0.67
Batch: 80; loss: 1.35; acc: 0.66
Batch: 100; loss: 1.41; acc: 0.66
Batch: 120; loss: 1.55; acc: 0.53
Batch: 140; loss: 1.21; acc: 0.78
Val Epoch over. val_loss: 1.3760150633040507; val_accuracy: 0.6768511146496815 

The current subspace-distance is: 8.752720168558881e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.44; acc: 0.62
Batch: 40; loss: 1.32; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.61
Batch: 80; loss: 1.34; acc: 0.67
Batch: 100; loss: 1.32; acc: 0.7
Batch: 120; loss: 1.43; acc: 0.66
Batch: 140; loss: 1.25; acc: 0.77
Batch: 160; loss: 1.45; acc: 0.55
Batch: 180; loss: 1.37; acc: 0.64
Batch: 200; loss: 1.37; acc: 0.67
Batch: 220; loss: 1.41; acc: 0.69
Batch: 240; loss: 1.38; acc: 0.67
Batch: 260; loss: 1.59; acc: 0.52
Batch: 280; loss: 1.46; acc: 0.59
Batch: 300; loss: 1.43; acc: 0.59
Batch: 320; loss: 1.32; acc: 0.67
Batch: 340; loss: 1.41; acc: 0.64
Batch: 360; loss: 1.49; acc: 0.61
Batch: 380; loss: 1.41; acc: 0.67
Batch: 400; loss: 1.44; acc: 0.59
Batch: 420; loss: 1.37; acc: 0.7
Batch: 440; loss: 1.38; acc: 0.64
Batch: 460; loss: 1.26; acc: 0.78
Batch: 480; loss: 1.45; acc: 0.59
Batch: 500; loss: 1.29; acc: 0.72
Batch: 520; loss: 1.24; acc: 0.77
Batch: 540; loss: 1.35; acc: 0.69
Batch: 560; loss: 1.39; acc: 0.62
Batch: 580; loss: 1.39; acc: 0.72
Batch: 600; loss: 1.36; acc: 0.61
Batch: 620; loss: 1.36; acc: 0.66
Batch: 640; loss: 1.4; acc: 0.66
Batch: 660; loss: 1.39; acc: 0.62
Batch: 680; loss: 1.52; acc: 0.55
Batch: 700; loss: 1.4; acc: 0.64
Batch: 720; loss: 1.28; acc: 0.77
Batch: 740; loss: 1.27; acc: 0.72
Batch: 760; loss: 1.57; acc: 0.53
Batch: 780; loss: 1.38; acc: 0.66
Train Epoch over. train_loss: 1.39; train_accuracy: 0.65 

0.00010892972932197154
0.00010127534187631682
Batch: 0; loss: 1.32; acc: 0.67
Batch: 20; loss: 1.42; acc: 0.61
Batch: 40; loss: 1.1; acc: 0.81
Batch: 60; loss: 1.33; acc: 0.66
Batch: 80; loss: 1.3; acc: 0.7
Batch: 100; loss: 1.35; acc: 0.72
Batch: 120; loss: 1.52; acc: 0.55
Batch: 140; loss: 1.13; acc: 0.83
Val Epoch over. val_loss: 1.3189910498394328; val_accuracy: 0.6973527070063694 

The current subspace-distance is: 0.00010127534187631682 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.4; acc: 0.66
Batch: 20; loss: 1.38; acc: 0.61
Batch: 40; loss: 1.36; acc: 0.69
Batch: 60; loss: 1.46; acc: 0.56
Batch: 80; loss: 1.43; acc: 0.55
Batch: 100; loss: 1.41; acc: 0.59
Batch: 120; loss: 1.37; acc: 0.62
Batch: 140; loss: 1.39; acc: 0.69
Batch: 160; loss: 1.43; acc: 0.58
Batch: 180; loss: 1.28; acc: 0.66
Batch: 200; loss: 1.22; acc: 0.72
Batch: 220; loss: 1.3; acc: 0.67
Batch: 240; loss: 1.26; acc: 0.73
Batch: 260; loss: 1.33; acc: 0.62
Batch: 280; loss: 1.4; acc: 0.66
Batch: 300; loss: 1.32; acc: 0.66
Batch: 320; loss: 1.37; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.77
Batch: 360; loss: 1.36; acc: 0.66
Batch: 380; loss: 1.32; acc: 0.67
Batch: 400; loss: 1.39; acc: 0.59
Batch: 420; loss: 1.25; acc: 0.72
Batch: 440; loss: 1.43; acc: 0.61
Batch: 460; loss: 1.41; acc: 0.62
Batch: 480; loss: 1.35; acc: 0.67
Batch: 500; loss: 1.34; acc: 0.67
Batch: 520; loss: 1.37; acc: 0.62
Batch: 540; loss: 1.29; acc: 0.66
Batch: 560; loss: 1.44; acc: 0.62
Batch: 580; loss: 1.24; acc: 0.72
Batch: 600; loss: 1.25; acc: 0.72
Batch: 620; loss: 1.28; acc: 0.72
Batch: 640; loss: 1.33; acc: 0.61
Batch: 660; loss: 1.2; acc: 0.77
Batch: 680; loss: 1.42; acc: 0.67
Batch: 700; loss: 1.38; acc: 0.67
Batch: 720; loss: 1.32; acc: 0.72
Batch: 740; loss: 1.31; acc: 0.67
Batch: 760; loss: 1.29; acc: 0.73
Batch: 780; loss: 1.42; acc: 0.58
Train Epoch over. train_loss: 1.34; train_accuracy: 0.67 

0.00011918927339138463
0.00011266888759564608
Batch: 0; loss: 1.29; acc: 0.69
Batch: 20; loss: 1.42; acc: 0.59
Batch: 40; loss: 1.03; acc: 0.83
Batch: 60; loss: 1.27; acc: 0.67
Batch: 80; loss: 1.21; acc: 0.72
Batch: 100; loss: 1.31; acc: 0.7
Batch: 120; loss: 1.52; acc: 0.52
Batch: 140; loss: 1.1; acc: 0.83
Val Epoch over. val_loss: 1.2673728401493873; val_accuracy: 0.7081011146496815 

The current subspace-distance is: 0.00011266888759564608 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.8
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.18; acc: 0.8
Batch: 80; loss: 1.3; acc: 0.69
Batch: 100; loss: 1.47; acc: 0.59
Batch: 120; loss: 1.37; acc: 0.62
Batch: 140; loss: 1.23; acc: 0.73
Batch: 160; loss: 1.41; acc: 0.61
Batch: 180; loss: 1.38; acc: 0.59
Batch: 200; loss: 1.33; acc: 0.64
Batch: 220; loss: 1.39; acc: 0.59
Batch: 240; loss: 1.32; acc: 0.69
Batch: 260; loss: 1.25; acc: 0.66
Batch: 280; loss: 1.37; acc: 0.64
Batch: 300; loss: 1.25; acc: 0.69
Batch: 320; loss: 1.15; acc: 0.75
Batch: 340; loss: 1.27; acc: 0.64
Batch: 360; loss: 1.2; acc: 0.78
Batch: 380; loss: 1.19; acc: 0.69
Batch: 400; loss: 1.15; acc: 0.72
Batch: 420; loss: 1.3; acc: 0.64
Batch: 440; loss: 1.4; acc: 0.58
Batch: 460; loss: 1.31; acc: 0.69
Batch: 480; loss: 1.34; acc: 0.66
Batch: 500; loss: 1.44; acc: 0.62
Batch: 520; loss: 1.28; acc: 0.67
Batch: 540; loss: 1.29; acc: 0.69
Batch: 560; loss: 1.26; acc: 0.69
Batch: 580; loss: 1.36; acc: 0.62
Batch: 600; loss: 1.37; acc: 0.58
Batch: 620; loss: 1.29; acc: 0.64
Batch: 640; loss: 1.25; acc: 0.7
Batch: 660; loss: 1.4; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.62
Batch: 700; loss: 1.36; acc: 0.58
Batch: 720; loss: 1.44; acc: 0.62
Batch: 740; loss: 1.31; acc: 0.64
Batch: 760; loss: 1.36; acc: 0.62
Batch: 780; loss: 1.08; acc: 0.75
Train Epoch over. train_loss: 1.3; train_accuracy: 0.68 

0.00012932944810017943
0.00012429358321242034
Batch: 0; loss: 1.2; acc: 0.75
Batch: 20; loss: 1.36; acc: 0.61
Batch: 40; loss: 0.97; acc: 0.83
Batch: 60; loss: 1.21; acc: 0.72
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.73
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 1.08; acc: 0.88
Val Epoch over. val_loss: 1.2118355525527031; val_accuracy: 0.7284036624203821 

The current subspace-distance is: 0.00012429358321242034 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.29; acc: 0.66
Batch: 20; loss: 1.28; acc: 0.69
Batch: 40; loss: 1.13; acc: 0.8
Batch: 60; loss: 1.21; acc: 0.69
Batch: 80; loss: 1.36; acc: 0.67
Batch: 100; loss: 1.2; acc: 0.72
Batch: 120; loss: 1.37; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.64
Batch: 160; loss: 1.14; acc: 0.72
Batch: 180; loss: 1.21; acc: 0.72
Batch: 200; loss: 1.37; acc: 0.69
Batch: 220; loss: 1.18; acc: 0.73
Batch: 240; loss: 1.28; acc: 0.69
Batch: 260; loss: 1.26; acc: 0.67
Batch: 280; loss: 1.25; acc: 0.7
Batch: 300; loss: 1.18; acc: 0.73
Batch: 320; loss: 1.16; acc: 0.77
Batch: 340; loss: 1.18; acc: 0.73
Batch: 360; loss: 1.43; acc: 0.66
Batch: 380; loss: 1.22; acc: 0.72
Batch: 400; loss: 1.27; acc: 0.69
Batch: 420; loss: 1.3; acc: 0.58
Batch: 440; loss: 1.29; acc: 0.69
Batch: 460; loss: 1.22; acc: 0.7
Batch: 480; loss: 1.39; acc: 0.61
Batch: 500; loss: 1.33; acc: 0.67
Batch: 520; loss: 1.26; acc: 0.7
Batch: 540; loss: 1.18; acc: 0.75
Batch: 560; loss: 1.08; acc: 0.83
Batch: 580; loss: 1.47; acc: 0.58
Batch: 600; loss: 1.34; acc: 0.66
Batch: 620; loss: 1.12; acc: 0.8
Batch: 640; loss: 1.16; acc: 0.77
Batch: 660; loss: 1.17; acc: 0.7
Batch: 680; loss: 1.25; acc: 0.69
Batch: 700; loss: 1.07; acc: 0.75
Batch: 720; loss: 1.39; acc: 0.61
Batch: 740; loss: 1.14; acc: 0.77
Batch: 760; loss: 1.17; acc: 0.66
Batch: 780; loss: 1.17; acc: 0.69
Train Epoch over. train_loss: 1.26; train_accuracy: 0.69 

0.00014125971938483417
0.00013526107068173587
Batch: 0; loss: 1.14; acc: 0.77
Batch: 20; loss: 1.35; acc: 0.62
Batch: 40; loss: 0.92; acc: 0.83
Batch: 60; loss: 1.18; acc: 0.7
Batch: 80; loss: 1.12; acc: 0.73
Batch: 100; loss: 1.21; acc: 0.73
Batch: 120; loss: 1.38; acc: 0.61
Batch: 140; loss: 1.07; acc: 0.84
Val Epoch over. val_loss: 1.1829294752163493; val_accuracy: 0.7186504777070064 

The current subspace-distance is: 0.00013526107068173587 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.08; acc: 0.77
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.2; acc: 0.75
Batch: 100; loss: 1.37; acc: 0.61
Batch: 120; loss: 1.11; acc: 0.72
Batch: 140; loss: 1.3; acc: 0.64
Batch: 160; loss: 1.25; acc: 0.72
Batch: 180; loss: 1.29; acc: 0.67
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 1.15; acc: 0.73
Batch: 240; loss: 1.32; acc: 0.58
Batch: 260; loss: 1.2; acc: 0.72
Batch: 280; loss: 1.26; acc: 0.69
Batch: 300; loss: 1.24; acc: 0.7
Batch: 320; loss: 1.18; acc: 0.77
Batch: 340; loss: 1.1; acc: 0.8
Batch: 360; loss: 1.28; acc: 0.66
Batch: 380; loss: 1.28; acc: 0.64
Batch: 400; loss: 1.18; acc: 0.72
Batch: 420; loss: 1.26; acc: 0.66
Batch: 440; loss: 1.16; acc: 0.7
Batch: 460; loss: 1.21; acc: 0.72
Batch: 480; loss: 1.24; acc: 0.69
Batch: 500; loss: 1.26; acc: 0.66
Batch: 520; loss: 1.27; acc: 0.72
Batch: 540; loss: 1.16; acc: 0.77
Batch: 560; loss: 1.19; acc: 0.77
Batch: 580; loss: 1.11; acc: 0.75
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.25; acc: 0.69
Batch: 640; loss: 1.21; acc: 0.73
Batch: 660; loss: 1.16; acc: 0.73
Batch: 680; loss: 1.35; acc: 0.66
Batch: 700; loss: 1.21; acc: 0.7
Batch: 720; loss: 1.12; acc: 0.72
Batch: 740; loss: 1.13; acc: 0.69
Batch: 760; loss: 1.31; acc: 0.64
Batch: 780; loss: 1.14; acc: 0.7
Train Epoch over. train_loss: 1.23; train_accuracy: 0.69 

0.00014992318756412715
0.00014377165643963963
Batch: 0; loss: 1.09; acc: 0.78
Batch: 20; loss: 1.32; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.86
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 1.09; acc: 0.77
Batch: 100; loss: 1.2; acc: 0.73
Batch: 120; loss: 1.35; acc: 0.62
Batch: 140; loss: 1.03; acc: 0.78
Val Epoch over. val_loss: 1.154743979311293; val_accuracy: 0.7246218152866242 

The current subspace-distance is: 0.00014377165643963963 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.67
Batch: 20; loss: 1.2; acc: 0.73
Batch: 40; loss: 1.18; acc: 0.7
Batch: 60; loss: 1.26; acc: 0.62
Batch: 80; loss: 1.17; acc: 0.77
Batch: 100; loss: 1.28; acc: 0.62
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 1.36; acc: 0.66
Batch: 160; loss: 1.26; acc: 0.61
Batch: 180; loss: 1.13; acc: 0.75
Batch: 200; loss: 1.23; acc: 0.69
Batch: 220; loss: 1.28; acc: 0.7
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.23; acc: 0.69
Batch: 280; loss: 1.43; acc: 0.62
Batch: 300; loss: 1.18; acc: 0.75
Batch: 320; loss: 1.18; acc: 0.69
Batch: 340; loss: 1.16; acc: 0.77
Batch: 360; loss: 1.2; acc: 0.7
Batch: 380; loss: 1.25; acc: 0.62
Batch: 400; loss: 1.16; acc: 0.67
Batch: 420; loss: 1.31; acc: 0.61
Batch: 440; loss: 1.14; acc: 0.75
Batch: 460; loss: 1.19; acc: 0.73
Batch: 480; loss: 1.21; acc: 0.73
Batch: 500; loss: 1.26; acc: 0.59
Batch: 520; loss: 1.16; acc: 0.73
Batch: 540; loss: 1.37; acc: 0.55
Batch: 560; loss: 1.33; acc: 0.62
Batch: 580; loss: 1.25; acc: 0.69
Batch: 600; loss: 1.1; acc: 0.81
Batch: 620; loss: 1.22; acc: 0.77
Batch: 640; loss: 1.1; acc: 0.77
Batch: 660; loss: 1.22; acc: 0.7
Batch: 680; loss: 1.23; acc: 0.69
Batch: 700; loss: 1.21; acc: 0.67
Batch: 720; loss: 1.26; acc: 0.64
Batch: 740; loss: 1.19; acc: 0.64
Batch: 760; loss: 1.14; acc: 0.66
Batch: 780; loss: 1.23; acc: 0.67
Train Epoch over. train_loss: 1.21; train_accuracy: 0.69 

0.00015884343883953989
0.0001522297679912299
Batch: 0; loss: 1.04; acc: 0.75
Batch: 20; loss: 1.3; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 1.18; acc: 0.75
Batch: 120; loss: 1.32; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.78
Val Epoch over. val_loss: 1.1285589641066873; val_accuracy: 0.7272093949044586 

The current subspace-distance is: 0.0001522297679912299 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.2; acc: 0.73
Batch: 20; loss: 1.13; acc: 0.75
Batch: 40; loss: 1.29; acc: 0.64
Batch: 60; loss: 1.25; acc: 0.73
Batch: 80; loss: 1.17; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.77
Batch: 120; loss: 1.2; acc: 0.69
Batch: 140; loss: 1.31; acc: 0.61
Batch: 160; loss: 1.11; acc: 0.77
Batch: 180; loss: 1.32; acc: 0.67
Batch: 200; loss: 1.18; acc: 0.75
Batch: 220; loss: 1.13; acc: 0.69
Batch: 240; loss: 1.11; acc: 0.7
Batch: 260; loss: 1.25; acc: 0.69
Batch: 280; loss: 1.2; acc: 0.73
Batch: 300; loss: 1.21; acc: 0.72
Batch: 320; loss: 1.24; acc: 0.67
Batch: 340; loss: 1.2; acc: 0.72
Batch: 360; loss: 1.25; acc: 0.64
Batch: 380; loss: 1.03; acc: 0.8
Batch: 400; loss: 1.09; acc: 0.86
Batch: 420; loss: 1.13; acc: 0.73
Batch: 440; loss: 1.18; acc: 0.69
Batch: 460; loss: 1.08; acc: 0.73
Batch: 480; loss: 1.1; acc: 0.72
Batch: 500; loss: 1.2; acc: 0.64
Batch: 520; loss: 1.11; acc: 0.75
Batch: 540; loss: 1.11; acc: 0.73
Batch: 560; loss: 1.12; acc: 0.72
Batch: 580; loss: 1.34; acc: 0.55
Batch: 600; loss: 1.26; acc: 0.69
Batch: 620; loss: 1.1; acc: 0.7
Batch: 640; loss: 1.39; acc: 0.64
Batch: 660; loss: 1.22; acc: 0.77
Batch: 680; loss: 1.34; acc: 0.62
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 1.18; acc: 0.59
Batch: 740; loss: 1.17; acc: 0.69
Batch: 760; loss: 1.19; acc: 0.77
Batch: 780; loss: 1.21; acc: 0.67
Train Epoch over. train_loss: 1.19; train_accuracy: 0.69 

0.00016607400903012604
0.00015847223403397948
Batch: 0; loss: 1.04; acc: 0.81
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 1.14; acc: 0.73
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 1.19; acc: 0.73
Batch: 120; loss: 1.31; acc: 0.62
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 1.131536533118813; val_accuracy: 0.7250199044585988 

The current subspace-distance is: 0.00015847223403397948 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 1.15; acc: 0.72
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 1.12; acc: 0.69
Batch: 60; loss: 1.32; acc: 0.64
Batch: 80; loss: 1.23; acc: 0.67
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.73
Batch: 140; loss: 1.34; acc: 0.59
Batch: 160; loss: 1.12; acc: 0.8
Batch: 180; loss: 1.17; acc: 0.69
Batch: 200; loss: 1.06; acc: 0.8
Batch: 220; loss: 1.12; acc: 0.67
Batch: 240; loss: 1.39; acc: 0.5
Batch: 260; loss: 1.19; acc: 0.72
Batch: 280; loss: 1.12; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.72
Batch: 320; loss: 1.18; acc: 0.64
Batch: 340; loss: 1.27; acc: 0.66
Batch: 360; loss: 1.1; acc: 0.72
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 1.21; acc: 0.69
Batch: 420; loss: 1.2; acc: 0.77
Batch: 440; loss: 1.23; acc: 0.67
Batch: 460; loss: 1.03; acc: 0.77
Batch: 480; loss: 1.31; acc: 0.61
Batch: 500; loss: 1.25; acc: 0.62
Batch: 520; loss: 1.15; acc: 0.67
Batch: 540; loss: 1.22; acc: 0.62
Batch: 560; loss: 1.05; acc: 0.75
Batch: 580; loss: 1.23; acc: 0.7
Batch: 600; loss: 1.23; acc: 0.69
Batch: 620; loss: 1.24; acc: 0.62
Batch: 640; loss: 1.18; acc: 0.62
Batch: 660; loss: 1.39; acc: 0.56
Batch: 680; loss: 1.46; acc: 0.62
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.2; acc: 0.72
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 1.08; acc: 0.77
Train Epoch over. train_loss: 1.19; train_accuracy: 0.69 

0.00016852794215083122
0.00015868752961978316
Batch: 0; loss: 1.04; acc: 0.8
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 1.03; acc: 0.78
Batch: 100; loss: 1.18; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 1.1267144083976746; val_accuracy: 0.7245222929936306 

The current subspace-distance is: 0.00015868752961978316 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 1.07; acc: 0.8
Batch: 20; loss: 1.32; acc: 0.62
Batch: 40; loss: 1.31; acc: 0.64
Batch: 60; loss: 1.31; acc: 0.62
Batch: 80; loss: 1.28; acc: 0.7
Batch: 100; loss: 1.04; acc: 0.8
Batch: 120; loss: 1.2; acc: 0.7
Batch: 140; loss: 1.18; acc: 0.69
Batch: 160; loss: 1.19; acc: 0.72
Batch: 180; loss: 1.22; acc: 0.7
Batch: 200; loss: 1.27; acc: 0.72
Batch: 220; loss: 1.17; acc: 0.72
Batch: 240; loss: 1.15; acc: 0.72
Batch: 260; loss: 1.34; acc: 0.61
Batch: 280; loss: 1.26; acc: 0.67
Batch: 300; loss: 1.14; acc: 0.69
Batch: 320; loss: 1.05; acc: 0.73
Batch: 340; loss: 1.2; acc: 0.67
Batch: 360; loss: 0.91; acc: 0.81
Batch: 380; loss: 1.16; acc: 0.73
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.03; acc: 0.77
Batch: 440; loss: 1.09; acc: 0.75
Batch: 460; loss: 1.19; acc: 0.72
Batch: 480; loss: 1.16; acc: 0.69
Batch: 500; loss: 1.16; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.27; acc: 0.66
Batch: 560; loss: 1.31; acc: 0.61
Batch: 580; loss: 1.01; acc: 0.83
Batch: 600; loss: 1.2; acc: 0.69
Batch: 620; loss: 1.21; acc: 0.73
Batch: 640; loss: 1.35; acc: 0.56
Batch: 660; loss: 1.15; acc: 0.67
Batch: 680; loss: 1.31; acc: 0.62
Batch: 700; loss: 1.37; acc: 0.58
Batch: 720; loss: 1.19; acc: 0.61
Batch: 740; loss: 1.14; acc: 0.73
Batch: 760; loss: 1.35; acc: 0.69
Batch: 780; loss: 1.29; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.0001677908730925992
0.00016055429296102375
Batch: 0; loss: 1.02; acc: 0.75
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.86
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.17; acc: 0.72
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 0.96; acc: 0.72
Val Epoch over. val_loss: 1.1204408471751366; val_accuracy: 0.7209394904458599 

The current subspace-distance is: 0.00016055429296102375 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 1.17; acc: 0.8
Batch: 20; loss: 1.28; acc: 0.72
Batch: 40; loss: 1.23; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 0.91; acc: 0.81
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.25; acc: 0.64
Batch: 160; loss: 1.14; acc: 0.72
Batch: 180; loss: 1.27; acc: 0.7
Batch: 200; loss: 1.11; acc: 0.75
Batch: 220; loss: 1.07; acc: 0.8
Batch: 240; loss: 1.1; acc: 0.67
Batch: 260; loss: 1.14; acc: 0.73
Batch: 280; loss: 1.13; acc: 0.73
Batch: 300; loss: 1.1; acc: 0.75
Batch: 320; loss: 1.16; acc: 0.66
Batch: 340; loss: 1.08; acc: 0.78
Batch: 360; loss: 1.09; acc: 0.77
Batch: 380; loss: 1.33; acc: 0.66
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.07; acc: 0.7
Batch: 440; loss: 1.28; acc: 0.61
Batch: 460; loss: 1.2; acc: 0.7
Batch: 480; loss: 1.21; acc: 0.72
Batch: 500; loss: 1.3; acc: 0.67
Batch: 520; loss: 1.12; acc: 0.75
Batch: 540; loss: 1.05; acc: 0.77
Batch: 560; loss: 1.32; acc: 0.67
Batch: 580; loss: 1.09; acc: 0.73
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.1; acc: 0.81
Batch: 640; loss: 1.14; acc: 0.69
Batch: 660; loss: 1.16; acc: 0.75
Batch: 680; loss: 1.13; acc: 0.72
Batch: 700; loss: 1.17; acc: 0.75
Batch: 720; loss: 1.25; acc: 0.69
Batch: 740; loss: 1.13; acc: 0.62
Batch: 760; loss: 1.43; acc: 0.59
Batch: 780; loss: 1.33; acc: 0.64
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00016796126146800816
0.00016054043953772634
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 0.83; acc: 0.84
Batch: 60; loss: 1.12; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.78
Batch: 100; loss: 1.19; acc: 0.72
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 0.95; acc: 0.77
Val Epoch over. val_loss: 1.1213126748230806; val_accuracy: 0.7236265923566879 

The current subspace-distance is: 0.00016054043953772634 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 1.14; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.78
Batch: 60; loss: 1.18; acc: 0.7
Batch: 80; loss: 1.22; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.08; acc: 0.67
Batch: 140; loss: 1.18; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.78
Batch: 180; loss: 1.14; acc: 0.73
Batch: 200; loss: 1.04; acc: 0.8
Batch: 220; loss: 1.21; acc: 0.61
Batch: 240; loss: 1.18; acc: 0.66
Batch: 260; loss: 1.21; acc: 0.73
Batch: 280; loss: 1.06; acc: 0.73
Batch: 300; loss: 1.28; acc: 0.66
Batch: 320; loss: 1.26; acc: 0.77
Batch: 340; loss: 1.13; acc: 0.7
Batch: 360; loss: 1.41; acc: 0.58
Batch: 380; loss: 1.19; acc: 0.67
Batch: 400; loss: 1.16; acc: 0.7
Batch: 420; loss: 1.29; acc: 0.59
Batch: 440; loss: 1.24; acc: 0.67
Batch: 460; loss: 1.29; acc: 0.59
Batch: 480; loss: 1.17; acc: 0.73
Batch: 500; loss: 1.2; acc: 0.7
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.31; acc: 0.59
Batch: 560; loss: 1.17; acc: 0.64
Batch: 580; loss: 1.27; acc: 0.59
Batch: 600; loss: 1.19; acc: 0.73
Batch: 620; loss: 1.24; acc: 0.67
Batch: 640; loss: 1.26; acc: 0.66
Batch: 660; loss: 1.06; acc: 0.72
Batch: 680; loss: 1.25; acc: 0.64
Batch: 700; loss: 1.02; acc: 0.77
Batch: 720; loss: 1.24; acc: 0.66
Batch: 740; loss: 1.08; acc: 0.72
Batch: 760; loss: 1.27; acc: 0.64
Batch: 780; loss: 1.09; acc: 0.78
Train Epoch over. train_loss: 1.18; train_accuracy: 0.7 

0.00017354903684463352
0.00016628624871373177
Batch: 0; loss: 1.0; acc: 0.8
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 1.16; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.73
Val Epoch over. val_loss: 1.10658908307932; val_accuracy: 0.7280055732484076 

The current subspace-distance is: 0.00016628624871373177 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 1.18; acc: 0.64
Batch: 60; loss: 1.32; acc: 0.66
Batch: 80; loss: 1.14; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 1.23; acc: 0.69
Batch: 160; loss: 1.19; acc: 0.72
Batch: 180; loss: 1.01; acc: 0.84
Batch: 200; loss: 1.28; acc: 0.62
Batch: 220; loss: 1.06; acc: 0.73
Batch: 240; loss: 1.06; acc: 0.81
Batch: 260; loss: 1.22; acc: 0.72
Batch: 280; loss: 1.21; acc: 0.69
Batch: 300; loss: 1.22; acc: 0.72
Batch: 320; loss: 1.15; acc: 0.66
Batch: 340; loss: 1.11; acc: 0.75
Batch: 360; loss: 1.12; acc: 0.75
Batch: 380; loss: 1.22; acc: 0.69
Batch: 400; loss: 1.15; acc: 0.77
Batch: 420; loss: 1.15; acc: 0.7
Batch: 440; loss: 1.23; acc: 0.67
Batch: 460; loss: 1.12; acc: 0.75
Batch: 480; loss: 1.31; acc: 0.64
Batch: 500; loss: 1.22; acc: 0.61
Batch: 520; loss: 1.14; acc: 0.73
Batch: 540; loss: 1.11; acc: 0.78
Batch: 560; loss: 1.16; acc: 0.75
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.28; acc: 0.64
Batch: 620; loss: 1.1; acc: 0.8
Batch: 640; loss: 1.02; acc: 0.77
Batch: 660; loss: 1.34; acc: 0.62
Batch: 680; loss: 1.21; acc: 0.69
Batch: 700; loss: 1.25; acc: 0.62
Batch: 720; loss: 1.18; acc: 0.73
Batch: 740; loss: 1.18; acc: 0.72
Batch: 760; loss: 1.16; acc: 0.64
Batch: 780; loss: 1.09; acc: 0.77
Train Epoch over. train_loss: 1.18; train_accuracy: 0.69 

0.00017579187988303602
0.00016785769548732787
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.15; acc: 0.72
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.72
Val Epoch over. val_loss: 1.103744362190271; val_accuracy: 0.7222332802547771 

The current subspace-distance is: 0.00016785769548732787 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 1.3; acc: 0.61
Batch: 20; loss: 1.21; acc: 0.69
Batch: 40; loss: 1.16; acc: 0.61
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 0.99; acc: 0.8
Batch: 120; loss: 1.17; acc: 0.77
Batch: 140; loss: 1.09; acc: 0.72
Batch: 160; loss: 1.31; acc: 0.62
Batch: 180; loss: 1.16; acc: 0.73
Batch: 200; loss: 1.13; acc: 0.7
Batch: 220; loss: 1.07; acc: 0.81
Batch: 240; loss: 1.16; acc: 0.77
Batch: 260; loss: 1.21; acc: 0.67
Batch: 280; loss: 1.31; acc: 0.64
Batch: 300; loss: 1.18; acc: 0.67
Batch: 320; loss: 1.34; acc: 0.58
Batch: 340; loss: 1.18; acc: 0.67
Batch: 360; loss: 1.25; acc: 0.64
Batch: 380; loss: 1.16; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.75
Batch: 420; loss: 1.15; acc: 0.7
Batch: 440; loss: 1.22; acc: 0.64
Batch: 460; loss: 1.05; acc: 0.8
Batch: 480; loss: 1.08; acc: 0.78
Batch: 500; loss: 1.25; acc: 0.69
Batch: 520; loss: 0.95; acc: 0.77
Batch: 540; loss: 0.98; acc: 0.84
Batch: 560; loss: 1.23; acc: 0.67
Batch: 580; loss: 1.11; acc: 0.75
Batch: 600; loss: 1.21; acc: 0.7
Batch: 620; loss: 1.07; acc: 0.7
Batch: 640; loss: 1.0; acc: 0.75
Batch: 660; loss: 1.09; acc: 0.66
Batch: 680; loss: 1.06; acc: 0.77
Batch: 700; loss: 1.12; acc: 0.72
Batch: 720; loss: 1.17; acc: 0.73
Batch: 740; loss: 1.08; acc: 0.73
Batch: 760; loss: 1.1; acc: 0.7
Batch: 780; loss: 1.29; acc: 0.61
Train Epoch over. train_loss: 1.17; train_accuracy: 0.69 

0.0001762021129252389
0.00017062407277990133
Batch: 0; loss: 1.0; acc: 0.8
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 0.82; acc: 0.83
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 0.99; acc: 0.78
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.75
Val Epoch over. val_loss: 1.1084590842769404; val_accuracy: 0.721437101910828 

The current subspace-distance is: 0.00017062407277990133 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 1.25; acc: 0.62
Batch: 60; loss: 1.34; acc: 0.64
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.27; acc: 0.62
Batch: 120; loss: 1.07; acc: 0.81
Batch: 140; loss: 1.04; acc: 0.75
Batch: 160; loss: 1.08; acc: 0.78
Batch: 180; loss: 1.21; acc: 0.66
Batch: 200; loss: 1.15; acc: 0.69
Batch: 220; loss: 1.3; acc: 0.69
Batch: 240; loss: 1.06; acc: 0.77
Batch: 260; loss: 1.03; acc: 0.75
Batch: 280; loss: 1.28; acc: 0.66
Batch: 300; loss: 1.3; acc: 0.64
Batch: 320; loss: 1.08; acc: 0.8
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 1.17; acc: 0.69
Batch: 380; loss: 1.25; acc: 0.62
Batch: 400; loss: 1.2; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.72
Batch: 440; loss: 1.08; acc: 0.72
Batch: 460; loss: 1.2; acc: 0.69
Batch: 480; loss: 1.17; acc: 0.7
Batch: 500; loss: 1.3; acc: 0.59
Batch: 520; loss: 1.3; acc: 0.61
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 1.29; acc: 0.66
Batch: 580; loss: 1.2; acc: 0.7
Batch: 600; loss: 1.22; acc: 0.61
Batch: 620; loss: 1.1; acc: 0.75
Batch: 640; loss: 1.16; acc: 0.72
Batch: 660; loss: 1.21; acc: 0.73
Batch: 680; loss: 1.15; acc: 0.73
Batch: 700; loss: 1.12; acc: 0.7
Batch: 720; loss: 1.16; acc: 0.72
Batch: 740; loss: 1.13; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.7
Batch: 780; loss: 1.09; acc: 0.77
Train Epoch over. train_loss: 1.17; train_accuracy: 0.7 

0.00017917605873662978
0.0001716570113785565
Batch: 0; loss: 0.99; acc: 0.81
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.8
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.75
Val Epoch over. val_loss: 1.1006144300387923; val_accuracy: 0.7269108280254777 

The current subspace-distance is: 0.0001716570113785565 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 0.99; acc: 0.8
Batch: 60; loss: 1.04; acc: 0.8
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.25; acc: 0.67
Batch: 120; loss: 1.22; acc: 0.67
Batch: 140; loss: 1.14; acc: 0.7
Batch: 160; loss: 1.12; acc: 0.67
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 1.23; acc: 0.64
Batch: 220; loss: 1.21; acc: 0.66
Batch: 240; loss: 1.25; acc: 0.67
Batch: 260; loss: 1.09; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.66
Batch: 300; loss: 1.1; acc: 0.72
Batch: 320; loss: 1.14; acc: 0.72
Batch: 340; loss: 1.19; acc: 0.7
Batch: 360; loss: 1.07; acc: 0.72
Batch: 380; loss: 1.25; acc: 0.66
Batch: 400; loss: 1.27; acc: 0.61
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 1.16; acc: 0.73
Batch: 460; loss: 1.21; acc: 0.69
Batch: 480; loss: 1.24; acc: 0.61
Batch: 500; loss: 1.04; acc: 0.8
Batch: 520; loss: 1.22; acc: 0.69
Batch: 540; loss: 1.11; acc: 0.7
Batch: 560; loss: 1.13; acc: 0.69
Batch: 580; loss: 1.07; acc: 0.77
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 1.11; acc: 0.77
Batch: 640; loss: 1.18; acc: 0.7
Batch: 660; loss: 1.25; acc: 0.67
Batch: 680; loss: 1.25; acc: 0.62
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 1.17; acc: 0.67
Batch: 740; loss: 1.08; acc: 0.77
Batch: 760; loss: 1.14; acc: 0.66
Batch: 780; loss: 1.08; acc: 0.73
Train Epoch over. train_loss: 1.17; train_accuracy: 0.7 

0.00017705463687889278
0.00016919647168833762
Batch: 0; loss: 0.98; acc: 0.78
Batch: 20; loss: 1.27; acc: 0.59
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.78
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.77
Val Epoch over. val_loss: 1.0960277045608326; val_accuracy: 0.7261146496815286 

The current subspace-distance is: 0.00016919647168833762 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.28; acc: 0.64
Batch: 40; loss: 1.09; acc: 0.67
Batch: 60; loss: 1.23; acc: 0.62
Batch: 80; loss: 1.14; acc: 0.69
Batch: 100; loss: 1.3; acc: 0.59
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 1.19; acc: 0.69
Batch: 160; loss: 1.01; acc: 0.8
Batch: 180; loss: 1.06; acc: 0.72
Batch: 200; loss: 1.33; acc: 0.66
Batch: 220; loss: 1.12; acc: 0.67
Batch: 240; loss: 1.23; acc: 0.62
Batch: 260; loss: 1.38; acc: 0.66
Batch: 280; loss: 1.24; acc: 0.69
Batch: 300; loss: 1.25; acc: 0.67
Batch: 320; loss: 1.06; acc: 0.7
Batch: 340; loss: 1.13; acc: 0.69
Batch: 360; loss: 1.15; acc: 0.72
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 1.23; acc: 0.62
Batch: 420; loss: 1.1; acc: 0.73
Batch: 440; loss: 1.22; acc: 0.7
Batch: 460; loss: 1.18; acc: 0.67
Batch: 480; loss: 1.21; acc: 0.7
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 1.25; acc: 0.66
Batch: 540; loss: 1.2; acc: 0.61
Batch: 560; loss: 1.3; acc: 0.67
Batch: 580; loss: 1.27; acc: 0.59
Batch: 600; loss: 1.19; acc: 0.73
Batch: 620; loss: 1.06; acc: 0.67
Batch: 640; loss: 1.33; acc: 0.58
Batch: 660; loss: 1.15; acc: 0.73
Batch: 680; loss: 1.03; acc: 0.73
Batch: 700; loss: 1.13; acc: 0.73
Batch: 720; loss: 1.15; acc: 0.67
Batch: 740; loss: 0.98; acc: 0.8
Batch: 760; loss: 1.14; acc: 0.69
Batch: 780; loss: 1.17; acc: 0.69
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.0001830767432693392
0.0001748276554280892
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.8
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.67
Batch: 140; loss: 0.93; acc: 0.73
Val Epoch over. val_loss: 1.1034477360688957; val_accuracy: 0.7223328025477707 

The current subspace-distance is: 0.0001748276554280892 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 1.19; acc: 0.69
Batch: 40; loss: 1.21; acc: 0.7
Batch: 60; loss: 1.03; acc: 0.78
Batch: 80; loss: 1.26; acc: 0.61
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 1.19; acc: 0.69
Batch: 160; loss: 1.15; acc: 0.64
Batch: 180; loss: 1.06; acc: 0.73
Batch: 200; loss: 1.16; acc: 0.75
Batch: 220; loss: 1.07; acc: 0.7
Batch: 240; loss: 1.11; acc: 0.75
Batch: 260; loss: 1.21; acc: 0.62
Batch: 280; loss: 1.14; acc: 0.73
Batch: 300; loss: 1.21; acc: 0.67
Batch: 320; loss: 1.16; acc: 0.75
Batch: 340; loss: 1.12; acc: 0.77
Batch: 360; loss: 1.12; acc: 0.72
Batch: 380; loss: 1.08; acc: 0.75
Batch: 400; loss: 1.1; acc: 0.69
Batch: 420; loss: 0.99; acc: 0.8
Batch: 440; loss: 1.15; acc: 0.7
Batch: 460; loss: 1.2; acc: 0.61
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 1.18; acc: 0.69
Batch: 520; loss: 1.24; acc: 0.66
Batch: 540; loss: 0.92; acc: 0.83
Batch: 560; loss: 1.37; acc: 0.52
Batch: 580; loss: 1.41; acc: 0.5
Batch: 600; loss: 1.37; acc: 0.61
Batch: 620; loss: 1.2; acc: 0.7
Batch: 640; loss: 1.18; acc: 0.64
Batch: 660; loss: 1.06; acc: 0.77
Batch: 680; loss: 1.1; acc: 0.75
Batch: 700; loss: 1.33; acc: 0.59
Batch: 720; loss: 1.3; acc: 0.64
Batch: 740; loss: 1.21; acc: 0.72
Batch: 760; loss: 1.22; acc: 0.67
Batch: 780; loss: 1.1; acc: 0.72
Train Epoch over. train_loss: 1.16; train_accuracy: 0.69 

0.0001831160916481167
0.0001766278874129057
Batch: 0; loss: 0.99; acc: 0.78
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.84
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 0.95; acc: 0.8
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.73
Val Epoch over. val_loss: 1.1023159945846364; val_accuracy: 0.7239251592356688 

The current subspace-distance is: 0.0001766278874129057 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.66
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 1.07; acc: 0.72
Batch: 60; loss: 1.17; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 1.37; acc: 0.56
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 0.95; acc: 0.86
Batch: 200; loss: 1.15; acc: 0.64
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 1.26; acc: 0.62
Batch: 260; loss: 1.06; acc: 0.72
Batch: 280; loss: 1.21; acc: 0.66
Batch: 300; loss: 0.91; acc: 0.84
Batch: 320; loss: 1.06; acc: 0.81
Batch: 340; loss: 1.13; acc: 0.77
Batch: 360; loss: 1.18; acc: 0.64
Batch: 380; loss: 1.11; acc: 0.73
Batch: 400; loss: 1.31; acc: 0.7
Batch: 420; loss: 1.09; acc: 0.7
Batch: 440; loss: 1.04; acc: 0.8
Batch: 460; loss: 1.1; acc: 0.75
Batch: 480; loss: 1.17; acc: 0.72
Batch: 500; loss: 0.97; acc: 0.8
Batch: 520; loss: 1.11; acc: 0.75
Batch: 540; loss: 1.08; acc: 0.77
Batch: 560; loss: 1.14; acc: 0.73
Batch: 580; loss: 1.1; acc: 0.75
Batch: 600; loss: 1.1; acc: 0.72
Batch: 620; loss: 1.21; acc: 0.69
Batch: 640; loss: 1.15; acc: 0.73
Batch: 660; loss: 1.3; acc: 0.69
Batch: 680; loss: 1.14; acc: 0.7
Batch: 700; loss: 1.31; acc: 0.64
Batch: 720; loss: 1.15; acc: 0.69
Batch: 740; loss: 1.24; acc: 0.67
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.12; acc: 0.66
Train Epoch over. train_loss: 1.16; train_accuracy: 0.69 

0.00018241707584820688
0.0001747886126395315
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 1.13; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 0.91; acc: 0.77
Val Epoch over. val_loss: 1.0915605702977271; val_accuracy: 0.7269108280254777 

The current subspace-distance is: 0.0001747886126395315 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 1.11; acc: 0.75
Batch: 60; loss: 1.03; acc: 0.78
Batch: 80; loss: 1.03; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.62
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 1.23; acc: 0.69
Batch: 160; loss: 1.01; acc: 0.78
Batch: 180; loss: 1.1; acc: 0.72
Batch: 200; loss: 1.08; acc: 0.7
Batch: 220; loss: 1.17; acc: 0.73
Batch: 240; loss: 0.97; acc: 0.81
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 1.1; acc: 0.78
Batch: 300; loss: 1.06; acc: 0.7
Batch: 320; loss: 1.1; acc: 0.73
Batch: 340; loss: 1.32; acc: 0.61
Batch: 360; loss: 1.08; acc: 0.72
Batch: 380; loss: 1.02; acc: 0.77
Batch: 400; loss: 1.16; acc: 0.77
Batch: 420; loss: 1.02; acc: 0.78
Batch: 440; loss: 1.24; acc: 0.62
Batch: 460; loss: 1.16; acc: 0.73
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 1.09; acc: 0.69
Batch: 520; loss: 1.35; acc: 0.62
Batch: 540; loss: 1.07; acc: 0.77
Batch: 560; loss: 1.06; acc: 0.77
Batch: 580; loss: 1.11; acc: 0.72
Batch: 600; loss: 1.27; acc: 0.61
Batch: 620; loss: 1.04; acc: 0.78
Batch: 640; loss: 1.06; acc: 0.77
Batch: 660; loss: 1.05; acc: 0.77
Batch: 680; loss: 1.27; acc: 0.67
Batch: 700; loss: 1.1; acc: 0.67
Batch: 720; loss: 1.1; acc: 0.75
Batch: 740; loss: 1.22; acc: 0.61
Batch: 760; loss: 1.32; acc: 0.66
Batch: 780; loss: 1.24; acc: 0.64
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.0001819288736442104
0.00017593067605048418
Batch: 0; loss: 0.98; acc: 0.78
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 0.95; acc: 0.8
Batch: 100; loss: 1.13; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.75
Val Epoch over. val_loss: 1.0979945674823348; val_accuracy: 0.7219347133757962 

The current subspace-distance is: 0.00017593067605048418 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 1.23; acc: 0.64
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 1.12; acc: 0.7
Batch: 60; loss: 1.21; acc: 0.73
Batch: 80; loss: 1.06; acc: 0.8
Batch: 100; loss: 1.32; acc: 0.61
Batch: 120; loss: 1.09; acc: 0.77
Batch: 140; loss: 1.07; acc: 0.73
Batch: 160; loss: 0.99; acc: 0.78
Batch: 180; loss: 1.22; acc: 0.67
Batch: 200; loss: 1.27; acc: 0.67
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.02; acc: 0.75
Batch: 260; loss: 1.16; acc: 0.66
Batch: 280; loss: 1.19; acc: 0.62
Batch: 300; loss: 1.14; acc: 0.73
Batch: 320; loss: 1.31; acc: 0.66
Batch: 340; loss: 1.14; acc: 0.73
Batch: 360; loss: 1.19; acc: 0.75
Batch: 380; loss: 1.34; acc: 0.59
Batch: 400; loss: 1.08; acc: 0.75
Batch: 420; loss: 1.18; acc: 0.7
Batch: 440; loss: 1.22; acc: 0.7
Batch: 460; loss: 1.21; acc: 0.67
Batch: 480; loss: 1.23; acc: 0.67
Batch: 500; loss: 1.21; acc: 0.67
Batch: 520; loss: 1.13; acc: 0.7
Batch: 540; loss: 1.33; acc: 0.59
Batch: 560; loss: 1.1; acc: 0.67
Batch: 580; loss: 0.92; acc: 0.81
Batch: 600; loss: 1.02; acc: 0.75
Batch: 620; loss: 1.34; acc: 0.61
Batch: 640; loss: 1.29; acc: 0.58
Batch: 660; loss: 1.23; acc: 0.64
Batch: 680; loss: 1.14; acc: 0.72
Batch: 700; loss: 1.12; acc: 0.67
Batch: 720; loss: 1.21; acc: 0.72
Batch: 740; loss: 1.2; acc: 0.62
Batch: 760; loss: 1.18; acc: 0.69
Batch: 780; loss: 1.18; acc: 0.69
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.00018304430705029517
0.00017362952348776162
Batch: 0; loss: 0.99; acc: 0.78
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.75
Batch: 100; loss: 1.13; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 0.92; acc: 0.77
Val Epoch over. val_loss: 1.096621928321328; val_accuracy: 0.7262141719745223 

The current subspace-distance is: 0.00017362952348776162 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 1.19; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.64
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 1.36; acc: 0.56
Batch: 100; loss: 1.24; acc: 0.61
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 1.19; acc: 0.7
Batch: 160; loss: 1.35; acc: 0.62
Batch: 180; loss: 1.08; acc: 0.7
Batch: 200; loss: 1.11; acc: 0.78
Batch: 220; loss: 1.11; acc: 0.73
Batch: 240; loss: 1.01; acc: 0.8
Batch: 260; loss: 1.05; acc: 0.8
Batch: 280; loss: 1.35; acc: 0.62
Batch: 300; loss: 1.17; acc: 0.7
Batch: 320; loss: 1.31; acc: 0.66
Batch: 340; loss: 1.03; acc: 0.78
Batch: 360; loss: 1.09; acc: 0.72
Batch: 380; loss: 1.12; acc: 0.73
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 0.93; acc: 0.84
Batch: 460; loss: 1.08; acc: 0.75
Batch: 480; loss: 1.17; acc: 0.67
Batch: 500; loss: 1.06; acc: 0.73
Batch: 520; loss: 1.1; acc: 0.77
Batch: 540; loss: 1.16; acc: 0.72
Batch: 560; loss: 1.12; acc: 0.7
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.24; acc: 0.7
Batch: 620; loss: 1.05; acc: 0.8
Batch: 640; loss: 1.15; acc: 0.77
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.08; acc: 0.73
Batch: 700; loss: 1.04; acc: 0.72
Batch: 720; loss: 1.43; acc: 0.56
Batch: 740; loss: 1.35; acc: 0.69
Batch: 760; loss: 1.29; acc: 0.59
Batch: 780; loss: 1.16; acc: 0.72
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.00018528428336139768
0.00017822577501647174
Batch: 0; loss: 0.98; acc: 0.78
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 0.95; acc: 0.8
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.69
Batch: 140; loss: 0.93; acc: 0.75
Val Epoch over. val_loss: 1.0964594111321078; val_accuracy: 0.7274084394904459 

The current subspace-distance is: 0.00017822577501647174 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 1.15; acc: 0.72
Batch: 20; loss: 1.05; acc: 0.78
Batch: 40; loss: 1.01; acc: 0.81
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 1.15; acc: 0.77
Batch: 100; loss: 1.16; acc: 0.67
Batch: 120; loss: 1.16; acc: 0.7
Batch: 140; loss: 1.08; acc: 0.7
Batch: 160; loss: 1.18; acc: 0.67
Batch: 180; loss: 1.2; acc: 0.69
Batch: 200; loss: 1.09; acc: 0.75
Batch: 220; loss: 1.14; acc: 0.69
Batch: 240; loss: 1.12; acc: 0.69
Batch: 260; loss: 1.24; acc: 0.67
Batch: 280; loss: 1.2; acc: 0.64
Batch: 300; loss: 1.14; acc: 0.69
Batch: 320; loss: 1.04; acc: 0.77
Batch: 340; loss: 1.09; acc: 0.78
Batch: 360; loss: 1.14; acc: 0.73
Batch: 380; loss: 1.12; acc: 0.75
Batch: 400; loss: 1.09; acc: 0.73
Batch: 420; loss: 1.07; acc: 0.75
Batch: 440; loss: 1.35; acc: 0.58
Batch: 460; loss: 1.06; acc: 0.73
Batch: 480; loss: 1.22; acc: 0.66
Batch: 500; loss: 1.09; acc: 0.72
Batch: 520; loss: 0.97; acc: 0.8
Batch: 540; loss: 0.99; acc: 0.77
Batch: 560; loss: 1.12; acc: 0.75
Batch: 580; loss: 1.13; acc: 0.72
Batch: 600; loss: 1.16; acc: 0.69
Batch: 620; loss: 1.11; acc: 0.73
Batch: 640; loss: 1.34; acc: 0.59
Batch: 660; loss: 1.23; acc: 0.62
Batch: 680; loss: 1.07; acc: 0.8
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.16; acc: 0.67
Batch: 740; loss: 1.23; acc: 0.67
Batch: 760; loss: 1.1; acc: 0.78
Batch: 780; loss: 1.38; acc: 0.56
Train Epoch over. train_loss: 1.16; train_accuracy: 0.7 

0.0001837612799135968
0.00017685587226878852
Batch: 0; loss: 0.97; acc: 0.78
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.69
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.69
Batch: 140; loss: 0.92; acc: 0.75
Val Epoch over. val_loss: 1.0903284189048086; val_accuracy: 0.723328025477707 

The current subspace-distance is: 0.00017685587226878852 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 1.0; acc: 0.8
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 1.13; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.72
Batch: 80; loss: 1.15; acc: 0.67
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 1.09; acc: 0.73
Batch: 160; loss: 1.02; acc: 0.75
Batch: 180; loss: 1.16; acc: 0.75
Batch: 200; loss: 1.23; acc: 0.64
Batch: 220; loss: 1.17; acc: 0.73
Batch: 240; loss: 1.17; acc: 0.66
Batch: 260; loss: 1.25; acc: 0.64
Batch: 280; loss: 1.11; acc: 0.72
Batch: 300; loss: 1.18; acc: 0.64
Batch: 320; loss: 1.03; acc: 0.75
Batch: 340; loss: 1.23; acc: 0.67
Batch: 360; loss: 1.12; acc: 0.75
Batch: 380; loss: 0.99; acc: 0.75
Batch: 400; loss: 1.25; acc: 0.59
Batch: 420; loss: 1.08; acc: 0.77
Batch: 440; loss: 1.05; acc: 0.77
Batch: 460; loss: 1.19; acc: 0.7
Batch: 480; loss: 1.1; acc: 0.73
Batch: 500; loss: 1.2; acc: 0.69
Batch: 520; loss: 1.2; acc: 0.64
Batch: 540; loss: 1.15; acc: 0.73
Batch: 560; loss: 1.06; acc: 0.81
Batch: 580; loss: 1.16; acc: 0.64
Batch: 600; loss: 1.06; acc: 0.77
Batch: 620; loss: 1.01; acc: 0.75
Batch: 640; loss: 0.99; acc: 0.77
Batch: 660; loss: 1.04; acc: 0.73
Batch: 680; loss: 1.03; acc: 0.83
Batch: 700; loss: 1.14; acc: 0.69
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.06; acc: 0.75
Batch: 760; loss: 1.21; acc: 0.62
Batch: 780; loss: 1.36; acc: 0.58
Train Epoch over. train_loss: 1.15; train_accuracy: 0.7 

0.00018369390454608947
0.00017486140131950378
Batch: 0; loss: 0.97; acc: 0.78
Batch: 20; loss: 1.28; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.69
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.92; acc: 0.75
Val Epoch over. val_loss: 1.096586617694539; val_accuracy: 0.7211385350318471 

The current subspace-distance is: 0.00017486140131950378 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.17; acc: 0.73
Batch: 40; loss: 1.31; acc: 0.66
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.3; acc: 0.66
Batch: 100; loss: 1.23; acc: 0.66
Batch: 120; loss: 1.17; acc: 0.69
Batch: 140; loss: 1.09; acc: 0.78
Batch: 160; loss: 1.21; acc: 0.56
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 1.16; acc: 0.75
Batch: 220; loss: 1.05; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.73
Batch: 260; loss: 1.24; acc: 0.62
Batch: 280; loss: 1.17; acc: 0.69
Batch: 300; loss: 1.17; acc: 0.72
Batch: 320; loss: 1.21; acc: 0.62
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 1.16; acc: 0.69
Batch: 380; loss: 1.21; acc: 0.67
Batch: 400; loss: 1.34; acc: 0.64
Batch: 420; loss: 1.14; acc: 0.66
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 1.1; acc: 0.72
Batch: 480; loss: 1.13; acc: 0.69
Batch: 500; loss: 1.08; acc: 0.7
Batch: 520; loss: 1.04; acc: 0.78
Batch: 540; loss: 1.28; acc: 0.61
Batch: 560; loss: 1.15; acc: 0.72
Batch: 580; loss: 1.22; acc: 0.66
Batch: 600; loss: 1.17; acc: 0.67
Batch: 620; loss: 1.13; acc: 0.67
Batch: 640; loss: 1.04; acc: 0.77
Batch: 660; loss: 1.01; acc: 0.75
Batch: 680; loss: 1.16; acc: 0.66
Batch: 700; loss: 1.28; acc: 0.59
Batch: 720; loss: 1.33; acc: 0.56
Batch: 740; loss: 1.02; acc: 0.83
Batch: 760; loss: 1.29; acc: 0.7
Batch: 780; loss: 1.11; acc: 0.7
Train Epoch over. train_loss: 1.15; train_accuracy: 0.7 

0.00018746618297882378
0.00017910981841851026
Batch: 0; loss: 0.97; acc: 0.78
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.81
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.73
Val Epoch over. val_loss: 1.0949869751930237; val_accuracy: 0.7227308917197452 

The current subspace-distance is: 0.00017910981841851026 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 1.18; acc: 0.64
Batch: 20; loss: 1.25; acc: 0.64
Batch: 40; loss: 1.0; acc: 0.8
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.66
Batch: 100; loss: 0.97; acc: 0.81
Batch: 120; loss: 1.39; acc: 0.52
Batch: 140; loss: 1.23; acc: 0.73
Batch: 160; loss: 1.17; acc: 0.66
Batch: 180; loss: 1.04; acc: 0.73
Batch: 200; loss: 1.28; acc: 0.64
Batch: 220; loss: 1.18; acc: 0.69
Batch: 240; loss: 1.39; acc: 0.53
Batch: 260; loss: 1.03; acc: 0.73
Batch: 280; loss: 1.37; acc: 0.66
Batch: 300; loss: 1.13; acc: 0.72
Batch: 320; loss: 1.31; acc: 0.64
Batch: 340; loss: 1.29; acc: 0.58
Batch: 360; loss: 1.22; acc: 0.62
Batch: 380; loss: 1.2; acc: 0.62
Batch: 400; loss: 1.16; acc: 0.7
Batch: 420; loss: 1.07; acc: 0.72
Batch: 440; loss: 1.16; acc: 0.69
Batch: 460; loss: 1.24; acc: 0.64
Batch: 480; loss: 1.01; acc: 0.78
Batch: 500; loss: 1.19; acc: 0.7
Batch: 520; loss: 1.34; acc: 0.64
Batch: 540; loss: 1.03; acc: 0.73
Batch: 560; loss: 1.18; acc: 0.67
Batch: 580; loss: 1.14; acc: 0.69
Batch: 600; loss: 1.17; acc: 0.72
Batch: 620; loss: 1.24; acc: 0.69
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 1.39; acc: 0.61
Batch: 680; loss: 1.24; acc: 0.7
Batch: 700; loss: 1.25; acc: 0.61
Batch: 720; loss: 1.23; acc: 0.61
Batch: 740; loss: 1.15; acc: 0.66
Batch: 760; loss: 1.05; acc: 0.81
Batch: 780; loss: 1.23; acc: 0.64
Train Epoch over. train_loss: 1.15; train_accuracy: 0.7 

0.0001857579336501658
0.0001822263584472239
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.8
Batch: 100; loss: 1.13; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.73
Val Epoch over. val_loss: 1.095648930710592; val_accuracy: 0.7213375796178344 

The current subspace-distance is: 0.0001822263584472239 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.06; acc: 0.78
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 1.21; acc: 0.67
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.7
Batch: 140; loss: 1.25; acc: 0.67
Batch: 160; loss: 1.22; acc: 0.61
Batch: 180; loss: 0.99; acc: 0.8
Batch: 200; loss: 1.1; acc: 0.75
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.15; acc: 0.67
Batch: 260; loss: 1.11; acc: 0.77
Batch: 280; loss: 1.13; acc: 0.67
Batch: 300; loss: 1.17; acc: 0.64
Batch: 320; loss: 1.09; acc: 0.78
Batch: 340; loss: 1.08; acc: 0.77
Batch: 360; loss: 1.17; acc: 0.72
Batch: 380; loss: 1.23; acc: 0.62
Batch: 400; loss: 1.33; acc: 0.5
Batch: 420; loss: 1.18; acc: 0.73
Batch: 440; loss: 1.04; acc: 0.78
Batch: 460; loss: 1.2; acc: 0.62
Batch: 480; loss: 1.08; acc: 0.73
Batch: 500; loss: 1.19; acc: 0.66
Batch: 520; loss: 1.35; acc: 0.55
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.29; acc: 0.62
Batch: 580; loss: 1.29; acc: 0.67
Batch: 600; loss: 1.34; acc: 0.59
Batch: 620; loss: 1.25; acc: 0.66
Batch: 640; loss: 1.03; acc: 0.73
Batch: 660; loss: 1.38; acc: 0.55
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.02; acc: 0.72
Batch: 720; loss: 0.99; acc: 0.8
Batch: 740; loss: 1.05; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.7
Batch: 780; loss: 1.26; acc: 0.66
Train Epoch over. train_loss: 1.15; train_accuracy: 0.69 

0.0001872457651188597
0.00018104792980011553
Batch: 0; loss: 0.96; acc: 0.78
Batch: 20; loss: 1.29; acc: 0.62
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.8
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.92; acc: 0.75
Val Epoch over. val_loss: 1.0886447387895766; val_accuracy: 0.7264132165605095 

The current subspace-distance is: 0.00018104792980011553 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.98; acc: 0.8
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 1.21; acc: 0.66
Batch: 60; loss: 0.98; acc: 0.8
Batch: 80; loss: 1.06; acc: 0.77
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.29; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.62
Batch: 160; loss: 1.27; acc: 0.56
Batch: 180; loss: 1.09; acc: 0.67
Batch: 200; loss: 1.27; acc: 0.62
Batch: 220; loss: 0.91; acc: 0.86
Batch: 240; loss: 0.93; acc: 0.73
Batch: 260; loss: 1.15; acc: 0.72
Batch: 280; loss: 1.47; acc: 0.62
Batch: 300; loss: 1.22; acc: 0.72
Batch: 320; loss: 1.15; acc: 0.67
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 1.04; acc: 0.72
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.12; acc: 0.77
Batch: 440; loss: 1.19; acc: 0.66
Batch: 460; loss: 1.03; acc: 0.77
Batch: 480; loss: 1.13; acc: 0.72
Batch: 500; loss: 1.15; acc: 0.66
Batch: 520; loss: 1.49; acc: 0.52
Batch: 540; loss: 1.22; acc: 0.59
Batch: 560; loss: 1.23; acc: 0.67
Batch: 580; loss: 1.17; acc: 0.66
Batch: 600; loss: 1.09; acc: 0.66
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.28; acc: 0.55
Batch: 660; loss: 1.22; acc: 0.72
Batch: 680; loss: 1.22; acc: 0.72
Batch: 700; loss: 1.19; acc: 0.66
Batch: 720; loss: 1.19; acc: 0.67
Batch: 740; loss: 1.23; acc: 0.66
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 1.29; acc: 0.66
Train Epoch over. train_loss: 1.15; train_accuracy: 0.7 

0.00018533675756771117
0.00017864089750219136
Batch: 0; loss: 0.97; acc: 0.77
Batch: 20; loss: 1.28; acc: 0.64
Batch: 40; loss: 0.79; acc: 0.86
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 0.92; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.75
Batch: 120; loss: 1.25; acc: 0.67
Batch: 140; loss: 0.9; acc: 0.77
Val Epoch over. val_loss: 1.0873842505133076; val_accuracy: 0.724422770700637 

The current subspace-distance is: 0.00017864089750219136 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_4_flips_True_d_dim_100_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.75

The number of parameters is: 269845

The number of individual parameters is:

22
352
22
22
33
40656
33
33
66
121968
66
66
64
101376
64
64
4096
64
640
10
64
64

nonzero elements in E: 53968996
elements in E: 53969000
fraction nonzero: 0.9999999258833775
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.43; acc: 0.08
Batch: 20; loss: 2.29; acc: 0.16
Batch: 40; loss: 2.05; acc: 0.31
Batch: 60; loss: 2.05; acc: 0.28
Batch: 80; loss: 1.98; acc: 0.42
Batch: 100; loss: 1.87; acc: 0.52
Batch: 120; loss: 1.87; acc: 0.45
Batch: 140; loss: 1.77; acc: 0.52
Batch: 160; loss: 1.79; acc: 0.47
Batch: 180; loss: 1.75; acc: 0.52
Batch: 200; loss: 1.62; acc: 0.7
Batch: 220; loss: 1.74; acc: 0.55
Batch: 240; loss: 1.67; acc: 0.62
Batch: 260; loss: 1.62; acc: 0.64
Batch: 280; loss: 1.54; acc: 0.7
Batch: 300; loss: 1.61; acc: 0.58
Batch: 320; loss: 1.59; acc: 0.59
Batch: 340; loss: 1.61; acc: 0.61
Batch: 360; loss: 1.52; acc: 0.64
Batch: 380; loss: 1.64; acc: 0.67
Batch: 400; loss: 1.6; acc: 0.61
Batch: 420; loss: 1.61; acc: 0.59
Batch: 440; loss: 1.57; acc: 0.61
Batch: 460; loss: 1.56; acc: 0.66
Batch: 480; loss: 1.53; acc: 0.7
Batch: 500; loss: 1.49; acc: 0.66
Batch: 520; loss: 1.45; acc: 0.7
Batch: 540; loss: 1.55; acc: 0.64
Batch: 560; loss: 1.47; acc: 0.62
Batch: 580; loss: 1.54; acc: 0.67
Batch: 600; loss: 1.54; acc: 0.62
Batch: 620; loss: 1.57; acc: 0.61
Batch: 640; loss: 1.45; acc: 0.77
Batch: 660; loss: 1.35; acc: 0.77
Batch: 680; loss: 1.41; acc: 0.77
Batch: 700; loss: 1.44; acc: 0.72
Batch: 720; loss: 1.39; acc: 0.75
Batch: 740; loss: 1.38; acc: 0.72
Batch: 760; loss: 1.35; acc: 0.72
Batch: 780; loss: 1.38; acc: 0.81
Train Epoch over. train_loss: 1.62; train_accuracy: 0.6 

5.9873738791793585e-05
5.413655162556097e-05
Batch: 0; loss: 1.45; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.67
Batch: 40; loss: 1.19; acc: 0.83
Batch: 60; loss: 1.28; acc: 0.77
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.43; acc: 0.67
Batch: 120; loss: 1.49; acc: 0.64
Batch: 140; loss: 1.33; acc: 0.78
Val Epoch over. val_loss: 1.3657332476536939; val_accuracy: 0.7277070063694268 

The current subspace-distance is: 5.413655162556097e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.46; acc: 0.64
Batch: 20; loss: 1.39; acc: 0.72
Batch: 40; loss: 1.44; acc: 0.7
Batch: 60; loss: 1.32; acc: 0.75
Batch: 80; loss: 1.31; acc: 0.7
Batch: 100; loss: 1.28; acc: 0.75
Batch: 120; loss: 1.31; acc: 0.72
Batch: 140; loss: 1.37; acc: 0.66
Batch: 160; loss: 1.27; acc: 0.8
Batch: 180; loss: 1.33; acc: 0.77
Batch: 200; loss: 1.37; acc: 0.64
Batch: 220; loss: 1.39; acc: 0.69
Batch: 240; loss: 1.22; acc: 0.81
Batch: 260; loss: 1.19; acc: 0.81
Batch: 280; loss: 1.28; acc: 0.77
Batch: 300; loss: 1.29; acc: 0.78
Batch: 320; loss: 1.27; acc: 0.73
Batch: 340; loss: 1.16; acc: 0.89
Batch: 360; loss: 1.24; acc: 0.8
Batch: 380; loss: 1.26; acc: 0.73
Batch: 400; loss: 1.23; acc: 0.84
Batch: 420; loss: 1.21; acc: 0.83
Batch: 440; loss: 1.14; acc: 0.8
Batch: 460; loss: 1.19; acc: 0.75
Batch: 480; loss: 1.39; acc: 0.7
Batch: 500; loss: 1.22; acc: 0.75
Batch: 520; loss: 1.29; acc: 0.69
Batch: 540; loss: 1.13; acc: 0.83
Batch: 560; loss: 1.34; acc: 0.66
Batch: 580; loss: 1.35; acc: 0.66
Batch: 600; loss: 1.29; acc: 0.75
Batch: 620; loss: 1.22; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.72
Batch: 660; loss: 1.2; acc: 0.77
Batch: 680; loss: 1.23; acc: 0.75
Batch: 700; loss: 1.28; acc: 0.69
Batch: 720; loss: 1.16; acc: 0.78
Batch: 740; loss: 1.14; acc: 0.8
Batch: 760; loss: 1.19; acc: 0.72
Batch: 780; loss: 1.13; acc: 0.75
Train Epoch over. train_loss: 1.28; train_accuracy: 0.74 

8.507147140335292e-05
7.970723527250811e-05
Batch: 0; loss: 1.22; acc: 0.73
Batch: 20; loss: 1.27; acc: 0.73
Batch: 40; loss: 0.95; acc: 0.88
Batch: 60; loss: 1.1; acc: 0.78
Batch: 80; loss: 1.01; acc: 0.8
Batch: 100; loss: 1.12; acc: 0.84
Batch: 120; loss: 1.34; acc: 0.72
Batch: 140; loss: 1.07; acc: 0.88
Val Epoch over. val_loss: 1.136775489066057; val_accuracy: 0.7842356687898089 

The current subspace-distance is: 7.970723527250811e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.8
Batch: 20; loss: 1.06; acc: 0.81
Batch: 40; loss: 1.14; acc: 0.81
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 1.05; acc: 0.83
Batch: 100; loss: 1.08; acc: 0.86
Batch: 120; loss: 1.09; acc: 0.75
Batch: 140; loss: 1.25; acc: 0.73
Batch: 160; loss: 1.11; acc: 0.83
Batch: 180; loss: 1.15; acc: 0.73
Batch: 200; loss: 1.15; acc: 0.7
Batch: 220; loss: 1.1; acc: 0.78
Batch: 240; loss: 1.05; acc: 0.77
Batch: 260; loss: 1.15; acc: 0.8
Batch: 280; loss: 1.09; acc: 0.77
Batch: 300; loss: 1.13; acc: 0.73
Batch: 320; loss: 1.19; acc: 0.73
Batch: 340; loss: 1.11; acc: 0.8
Batch: 360; loss: 1.08; acc: 0.81
Batch: 380; loss: 1.04; acc: 0.83
Batch: 400; loss: 1.1; acc: 0.77
Batch: 420; loss: 0.95; acc: 0.83
Batch: 440; loss: 1.21; acc: 0.66
Batch: 460; loss: 0.95; acc: 0.89
Batch: 480; loss: 1.0; acc: 0.81
Batch: 500; loss: 1.0; acc: 0.83
Batch: 520; loss: 1.05; acc: 0.86
Batch: 540; loss: 1.01; acc: 0.78
Batch: 560; loss: 1.14; acc: 0.75
Batch: 580; loss: 1.06; acc: 0.78
Batch: 600; loss: 1.08; acc: 0.78
Batch: 620; loss: 1.0; acc: 0.8
Batch: 640; loss: 0.96; acc: 0.89
Batch: 660; loss: 1.01; acc: 0.8
Batch: 680; loss: 1.04; acc: 0.78
Batch: 700; loss: 1.06; acc: 0.8
Batch: 720; loss: 0.89; acc: 0.86
Batch: 740; loss: 0.99; acc: 0.75
Batch: 760; loss: 0.91; acc: 0.86
Batch: 780; loss: 1.04; acc: 0.83
Train Epoch over. train_loss: 1.1; train_accuracy: 0.77 

0.00010435003787279129
9.980711183743551e-05
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.7
Batch: 40; loss: 0.74; acc: 0.94
Batch: 60; loss: 0.95; acc: 0.83
Batch: 80; loss: 0.85; acc: 0.89
Batch: 100; loss: 0.92; acc: 0.84
Batch: 120; loss: 1.19; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.89
Val Epoch over. val_loss: 0.9681869828776949; val_accuracy: 0.8093152866242038 

The current subspace-distance is: 9.980711183743551e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 1.0; acc: 0.75
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 1.14; acc: 0.67
Batch: 80; loss: 1.14; acc: 0.73
Batch: 100; loss: 1.01; acc: 0.83
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 1.01; acc: 0.8
Batch: 160; loss: 0.92; acc: 0.92
Batch: 180; loss: 1.02; acc: 0.77
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.18; acc: 0.73
Batch: 240; loss: 0.98; acc: 0.83
Batch: 260; loss: 1.05; acc: 0.77
Batch: 280; loss: 1.1; acc: 0.67
Batch: 300; loss: 1.02; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 1.08; acc: 0.75
Batch: 360; loss: 1.07; acc: 0.73
Batch: 380; loss: 1.06; acc: 0.77
Batch: 400; loss: 1.08; acc: 0.75
Batch: 420; loss: 0.87; acc: 0.91
Batch: 440; loss: 0.97; acc: 0.78
Batch: 460; loss: 1.03; acc: 0.77
Batch: 480; loss: 0.84; acc: 0.91
Batch: 500; loss: 0.86; acc: 0.83
Batch: 520; loss: 1.06; acc: 0.73
Batch: 540; loss: 0.93; acc: 0.83
Batch: 560; loss: 0.87; acc: 0.86
Batch: 580; loss: 0.98; acc: 0.81
Batch: 600; loss: 1.15; acc: 0.77
Batch: 620; loss: 0.9; acc: 0.84
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 0.74; acc: 0.94
Batch: 680; loss: 0.87; acc: 0.8
Batch: 700; loss: 0.94; acc: 0.77
Batch: 720; loss: 0.96; acc: 0.8
Batch: 740; loss: 1.06; acc: 0.75
Batch: 760; loss: 0.92; acc: 0.8
Batch: 780; loss: 0.94; acc: 0.83
Train Epoch over. train_loss: 1.0; train_accuracy: 0.79 

0.00011888933659065515
0.00011290887778159231
Batch: 0; loss: 1.0; acc: 0.83
Batch: 20; loss: 1.06; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.94
Batch: 60; loss: 0.91; acc: 0.81
Batch: 80; loss: 0.82; acc: 0.84
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 1.11; acc: 0.77
Batch: 140; loss: 0.77; acc: 0.89
Val Epoch over. val_loss: 0.9122578858569929; val_accuracy: 0.8210589171974523 

The current subspace-distance is: 0.00011290887778159231 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.09; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.78
Batch: 40; loss: 1.02; acc: 0.8
Batch: 60; loss: 0.96; acc: 0.78
Batch: 80; loss: 0.85; acc: 0.84
Batch: 100; loss: 0.97; acc: 0.77
Batch: 120; loss: 0.96; acc: 0.83
Batch: 140; loss: 0.93; acc: 0.8
Batch: 160; loss: 0.87; acc: 0.81
Batch: 180; loss: 1.04; acc: 0.8
Batch: 200; loss: 0.98; acc: 0.84
Batch: 220; loss: 0.91; acc: 0.83
Batch: 240; loss: 0.78; acc: 0.88
Batch: 260; loss: 1.09; acc: 0.72
Batch: 280; loss: 0.96; acc: 0.81
Batch: 300; loss: 1.01; acc: 0.73
Batch: 320; loss: 0.91; acc: 0.8
Batch: 340; loss: 0.85; acc: 0.81
Batch: 360; loss: 0.75; acc: 0.91
Batch: 380; loss: 0.82; acc: 0.88
Batch: 400; loss: 0.96; acc: 0.78
Batch: 420; loss: 0.97; acc: 0.8
Batch: 440; loss: 0.84; acc: 0.83
Batch: 460; loss: 0.85; acc: 0.88
Batch: 480; loss: 1.09; acc: 0.7
Batch: 500; loss: 1.0; acc: 0.72
Batch: 520; loss: 1.01; acc: 0.81
Batch: 540; loss: 0.79; acc: 0.89
Batch: 560; loss: 0.9; acc: 0.81
Batch: 580; loss: 0.98; acc: 0.72
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.9; acc: 0.78
Batch: 640; loss: 0.91; acc: 0.83
Batch: 660; loss: 0.98; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.8
Batch: 700; loss: 1.04; acc: 0.72
Batch: 720; loss: 0.71; acc: 0.88
Batch: 740; loss: 0.88; acc: 0.84
Batch: 760; loss: 1.02; acc: 0.72
Batch: 780; loss: 0.85; acc: 0.8
Train Epoch over. train_loss: 0.93; train_accuracy: 0.8 

0.00013090507127344608
0.00012591552513185889
Batch: 0; loss: 0.91; acc: 0.86
Batch: 20; loss: 1.01; acc: 0.72
Batch: 40; loss: 0.6; acc: 0.94
Batch: 60; loss: 0.84; acc: 0.84
Batch: 80; loss: 0.77; acc: 0.86
Batch: 100; loss: 0.81; acc: 0.86
Batch: 120; loss: 1.02; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.92
Val Epoch over. val_loss: 0.8422155425806713; val_accuracy: 0.8348925159235668 

The current subspace-distance is: 0.00012591552513185889 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.83; acc: 0.84
Batch: 40; loss: 0.96; acc: 0.78
Batch: 60; loss: 0.82; acc: 0.86
Batch: 80; loss: 0.77; acc: 0.89
Batch: 100; loss: 0.98; acc: 0.8
Batch: 120; loss: 0.84; acc: 0.91
Batch: 140; loss: 0.82; acc: 0.81
Batch: 160; loss: 0.83; acc: 0.8
Batch: 180; loss: 0.93; acc: 0.78
Batch: 200; loss: 1.03; acc: 0.73
Batch: 220; loss: 0.8; acc: 0.86
Batch: 240; loss: 1.02; acc: 0.66
Batch: 260; loss: 0.98; acc: 0.73
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.88; acc: 0.83
Batch: 320; loss: 0.92; acc: 0.8
Batch: 340; loss: 0.78; acc: 0.84
Batch: 360; loss: 0.9; acc: 0.81
Batch: 380; loss: 1.0; acc: 0.77
Batch: 400; loss: 0.91; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.77
Batch: 440; loss: 0.9; acc: 0.81
Batch: 460; loss: 0.81; acc: 0.84
Batch: 480; loss: 0.93; acc: 0.8
Batch: 500; loss: 0.9; acc: 0.86
Batch: 520; loss: 0.75; acc: 0.91
Batch: 540; loss: 0.88; acc: 0.8
Batch: 560; loss: 0.8; acc: 0.78
Batch: 580; loss: 0.99; acc: 0.69
Batch: 600; loss: 0.69; acc: 0.89
Batch: 620; loss: 0.87; acc: 0.84
Batch: 640; loss: 0.84; acc: 0.83
Batch: 660; loss: 0.78; acc: 0.91
Batch: 680; loss: 0.79; acc: 0.86
Batch: 700; loss: 0.84; acc: 0.86
Batch: 720; loss: 0.8; acc: 0.83
Batch: 740; loss: 0.8; acc: 0.83
Batch: 760; loss: 0.82; acc: 0.86
Batch: 780; loss: 0.82; acc: 0.86
Train Epoch over. train_loss: 0.88; train_accuracy: 0.81 

0.00014281367475632578
0.00013689938350580633
Batch: 0; loss: 0.86; acc: 0.86
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.88
Batch: 100; loss: 0.77; acc: 0.89
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 0.66; acc: 0.94
Val Epoch over. val_loss: 0.8091664838183458; val_accuracy: 0.8370820063694268 

The current subspace-distance is: 0.00013689938350580633 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.86
Batch: 40; loss: 0.81; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.84
Batch: 80; loss: 0.96; acc: 0.77
Batch: 100; loss: 0.84; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.89; acc: 0.81
Batch: 160; loss: 0.87; acc: 0.84
Batch: 180; loss: 1.0; acc: 0.75
Batch: 200; loss: 0.79; acc: 0.83
Batch: 220; loss: 1.1; acc: 0.66
Batch: 240; loss: 0.88; acc: 0.81
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.88; acc: 0.75
Batch: 300; loss: 0.84; acc: 0.75
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.82; acc: 0.81
Batch: 360; loss: 0.88; acc: 0.77
Batch: 380; loss: 0.81; acc: 0.84
Batch: 400; loss: 1.1; acc: 0.64
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.72; acc: 0.86
Batch: 460; loss: 0.92; acc: 0.81
Batch: 480; loss: 0.88; acc: 0.75
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 1.06; acc: 0.72
Batch: 540; loss: 0.84; acc: 0.75
Batch: 560; loss: 0.83; acc: 0.75
Batch: 580; loss: 0.69; acc: 0.91
Batch: 600; loss: 0.72; acc: 0.86
Batch: 620; loss: 0.72; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.79; acc: 0.88
Batch: 680; loss: 0.85; acc: 0.8
Batch: 700; loss: 0.86; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.91
Batch: 740; loss: 0.82; acc: 0.81
Batch: 760; loss: 0.88; acc: 0.78
Batch: 780; loss: 0.77; acc: 0.86
Train Epoch over. train_loss: 0.83; train_accuracy: 0.82 

0.0001533921022200957
0.00014636728155892342
Batch: 0; loss: 0.82; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.77; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.94
Val Epoch over. val_loss: 0.763451803641714; val_accuracy: 0.8478304140127388 

The current subspace-distance is: 0.00014636728155892342 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.8
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.89
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 0.71; acc: 0.91
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 0.77; acc: 0.88
Batch: 180; loss: 0.83; acc: 0.84
Batch: 200; loss: 0.87; acc: 0.8
Batch: 220; loss: 0.87; acc: 0.75
Batch: 240; loss: 0.92; acc: 0.77
Batch: 260; loss: 0.75; acc: 0.89
Batch: 280; loss: 0.76; acc: 0.81
Batch: 300; loss: 0.83; acc: 0.83
Batch: 320; loss: 0.84; acc: 0.8
Batch: 340; loss: 0.7; acc: 0.88
Batch: 360; loss: 0.83; acc: 0.83
Batch: 380; loss: 0.73; acc: 0.88
Batch: 400; loss: 0.81; acc: 0.77
Batch: 420; loss: 0.74; acc: 0.84
Batch: 440; loss: 0.82; acc: 0.81
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.88; acc: 0.73
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.81; acc: 0.81
Batch: 540; loss: 0.9; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.94
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.82; acc: 0.77
Batch: 640; loss: 0.65; acc: 0.81
Batch: 660; loss: 0.75; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.89
Batch: 700; loss: 0.77; acc: 0.75
Batch: 720; loss: 0.8; acc: 0.8
Batch: 740; loss: 0.85; acc: 0.78
Batch: 760; loss: 0.82; acc: 0.81
Batch: 780; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.8; train_accuracy: 0.82 

0.00016392885299865156
0.00015778257511556149
Batch: 0; loss: 0.8; acc: 0.84
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.54; acc: 0.92
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.94
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.78
Batch: 140; loss: 0.6; acc: 0.91
Val Epoch over. val_loss: 0.7282779159819245; val_accuracy: 0.8447452229299363 

The current subspace-distance is: 0.00015778257511556149 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 0.75; acc: 0.86
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 1.11; acc: 0.75
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.81; acc: 0.8
Batch: 160; loss: 0.82; acc: 0.78
Batch: 180; loss: 0.79; acc: 0.77
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 0.8; acc: 0.84
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.89; acc: 0.73
Batch: 280; loss: 0.82; acc: 0.8
Batch: 300; loss: 0.75; acc: 0.83
Batch: 320; loss: 0.81; acc: 0.78
Batch: 340; loss: 0.7; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.92
Batch: 400; loss: 0.7; acc: 0.88
Batch: 420; loss: 0.82; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.83
Batch: 460; loss: 0.8; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.83
Batch: 500; loss: 0.82; acc: 0.84
Batch: 520; loss: 0.79; acc: 0.73
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.83; acc: 0.81
Batch: 580; loss: 0.8; acc: 0.8
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.86
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 0.71; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.94
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.81; acc: 0.8
Batch: 740; loss: 0.88; acc: 0.8
Batch: 760; loss: 0.79; acc: 0.81
Batch: 780; loss: 0.88; acc: 0.81
Train Epoch over. train_loss: 0.76; train_accuracy: 0.83 

0.00016761201550252736
0.00016296615649480373
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.54; acc: 0.92
Val Epoch over. val_loss: 0.6892757345536712; val_accuracy: 0.8467356687898089 

The current subspace-distance is: 0.00016296615649480373 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.82; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.8
Batch: 140; loss: 0.8; acc: 0.78
Batch: 160; loss: 0.66; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.91
Batch: 220; loss: 0.77; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.89
Batch: 260; loss: 0.67; acc: 0.84
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.69; acc: 0.89
Batch: 360; loss: 0.81; acc: 0.75
Batch: 380; loss: 0.84; acc: 0.75
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.78
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.72; acc: 0.88
Batch: 520; loss: 0.78; acc: 0.84
Batch: 540; loss: 0.83; acc: 0.84
Batch: 560; loss: 0.85; acc: 0.78
Batch: 580; loss: 0.76; acc: 0.86
Batch: 600; loss: 0.72; acc: 0.84
Batch: 620; loss: 0.72; acc: 0.8
Batch: 640; loss: 0.83; acc: 0.8
Batch: 660; loss: 0.79; acc: 0.81
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.78; acc: 0.81
Batch: 720; loss: 0.8; acc: 0.77
Batch: 740; loss: 0.8; acc: 0.75
Batch: 760; loss: 0.71; acc: 0.88
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

0.0001799132878659293
0.00017411170119885355
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.91
Val Epoch over. val_loss: 0.6724773736516382; val_accuracy: 0.8507165605095541 

The current subspace-distance is: 0.00017411170119885355 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.88; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.84; acc: 0.81
Batch: 60; loss: 0.7; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.97; acc: 0.7
Batch: 160; loss: 0.86; acc: 0.8
Batch: 180; loss: 0.83; acc: 0.78
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 0.87; acc: 0.8
Batch: 240; loss: 0.83; acc: 0.78
Batch: 260; loss: 0.75; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.83
Batch: 300; loss: 0.78; acc: 0.8
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 0.59; acc: 0.89
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.56; acc: 0.91
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.78; acc: 0.86
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.84
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.81
Batch: 580; loss: 0.79; acc: 0.89
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.73; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.88
Batch: 680; loss: 0.73; acc: 0.81
Batch: 700; loss: 0.8; acc: 0.8
Batch: 720; loss: 0.83; acc: 0.81
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.77; acc: 0.78
Train Epoch over. train_loss: 0.72; train_accuracy: 0.83 

0.0001839708857005462
0.0001763775071594864
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 0.49; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.89
Val Epoch over. val_loss: 0.6608106719840104; val_accuracy: 0.8496218152866242 

The current subspace-distance is: 0.0001763775071594864 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.81
Batch: 20; loss: 0.74; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.71; acc: 0.88
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.84
Batch: 220; loss: 0.82; acc: 0.77
Batch: 240; loss: 0.87; acc: 0.78
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.78; acc: 0.83
Batch: 300; loss: 0.79; acc: 0.77
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.8; acc: 0.81
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.83
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.69; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.73; acc: 0.86
Batch: 540; loss: 0.87; acc: 0.75
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.79; acc: 0.8
Batch: 620; loss: 0.69; acc: 0.81
Batch: 640; loss: 0.74; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.83
Batch: 680; loss: 0.73; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.59; acc: 0.94
Batch: 740; loss: 0.81; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.89
Train Epoch over. train_loss: 0.71; train_accuracy: 0.83 

0.00018636113964021206
0.00017869152361527085
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.655178636122661; val_accuracy: 0.8520103503184714 

The current subspace-distance is: 0.00017869152361527085 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.76; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.77; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.83
Batch: 200; loss: 0.71; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.92
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.86
Batch: 300; loss: 0.74; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.76; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.91
Batch: 420; loss: 0.8; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.81
Batch: 460; loss: 0.69; acc: 0.89
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.82; acc: 0.78
Batch: 520; loss: 0.78; acc: 0.77
Batch: 540; loss: 0.62; acc: 0.91
Batch: 560; loss: 0.7; acc: 0.81
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.94
Batch: 640; loss: 0.8; acc: 0.77
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.97
Batch: 740; loss: 0.8; acc: 0.83
Batch: 760; loss: 0.72; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00018789415480569005
0.00017915047646965832
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.94
Val Epoch over. val_loss: 0.6459475645593776; val_accuracy: 0.8541998407643312 

The current subspace-distance is: 0.00017915047646965832 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.82; acc: 0.77
Batch: 100; loss: 0.82; acc: 0.84
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.85; acc: 0.75
Batch: 160; loss: 0.8; acc: 0.84
Batch: 180; loss: 0.65; acc: 0.88
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.6; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.59; acc: 0.91
Batch: 280; loss: 0.65; acc: 0.89
Batch: 300; loss: 0.64; acc: 0.88
Batch: 320; loss: 0.73; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.75; acc: 0.83
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.51; acc: 0.94
Batch: 440; loss: 0.69; acc: 0.84
Batch: 460; loss: 0.8; acc: 0.8
Batch: 480; loss: 0.73; acc: 0.83
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.88
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.76; acc: 0.83
Batch: 580; loss: 0.72; acc: 0.75
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.84; acc: 0.78
Batch: 760; loss: 0.68; acc: 0.84
Batch: 780; loss: 0.75; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.84 

0.00018799623649101704
0.00018120629829354584
Batch: 0; loss: 0.72; acc: 0.78
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.89
Val Epoch over. val_loss: 0.6468080405596714; val_accuracy: 0.8488256369426752 

The current subspace-distance is: 0.00018120629829354584 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.91; acc: 0.67
Batch: 160; loss: 0.59; acc: 0.83
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.63; acc: 0.88
Batch: 240; loss: 0.82; acc: 0.77
Batch: 260; loss: 0.86; acc: 0.77
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.88
Batch: 360; loss: 0.63; acc: 0.78
Batch: 380; loss: 0.6; acc: 0.88
Batch: 400; loss: 0.84; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.81; acc: 0.8
Batch: 460; loss: 0.71; acc: 0.81
Batch: 480; loss: 0.65; acc: 0.83
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.8
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.83
Batch: 660; loss: 0.68; acc: 0.86
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.89
Batch: 720; loss: 0.71; acc: 0.88
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.88
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.69; train_accuracy: 0.84 

0.0001938878558576107
0.00018481029837857932
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.86; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.47; acc: 0.97
Val Epoch over. val_loss: 0.6329234810012161; val_accuracy: 0.8581807324840764 

The current subspace-distance is: 0.00018481029837857932 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.79; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.78; acc: 0.75
Batch: 240; loss: 0.55; acc: 0.92
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.63; acc: 0.88
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.73; acc: 0.88
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.89; acc: 0.72
Batch: 520; loss: 0.64; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.64; acc: 0.88
Batch: 620; loss: 0.56; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.92
Batch: 660; loss: 0.64; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.84
Batch: 700; loss: 0.8; acc: 0.8
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.00019260388216935098
0.00018513068789616227
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.47; acc: 0.92
Val Epoch over. val_loss: 0.621938042200295; val_accuracy: 0.856687898089172 

The current subspace-distance is: 0.00018513068789616227 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.84
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.72; acc: 0.88
Batch: 100; loss: 0.85; acc: 0.75
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.68; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.59; acc: 0.91
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.92
Batch: 340; loss: 0.77; acc: 0.83
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.73; acc: 0.86
Batch: 440; loss: 0.73; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.67; acc: 0.86
Batch: 620; loss: 0.8; acc: 0.8
Batch: 640; loss: 0.77; acc: 0.77
Batch: 660; loss: 0.67; acc: 0.83
Batch: 680; loss: 0.73; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.84 

0.0001966761046787724
0.00018975157581735402
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.97
Val Epoch over. val_loss: 0.6157936740453076; val_accuracy: 0.8605692675159236 

The current subspace-distance is: 0.00018975157581735402 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.76; acc: 0.81
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.89
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.78; acc: 0.8
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.92
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.66; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.91
Batch: 520; loss: 0.79; acc: 0.81
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.88
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.65; acc: 0.88
Batch: 720; loss: 0.76; acc: 0.78
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.95
Train Epoch over. train_loss: 0.67; train_accuracy: 0.84 

0.00020039849914610386
0.00019151846936438233
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.6029683021223469; val_accuracy: 0.8590764331210191 

The current subspace-distance is: 0.00019151846936438233 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.75
Batch: 60; loss: 0.52; acc: 0.92
Batch: 80; loss: 0.79; acc: 0.77
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.92
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.92
Batch: 200; loss: 0.5; acc: 0.95
Batch: 220; loss: 0.65; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.69; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.89
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.73; acc: 0.81
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.84
Batch: 660; loss: 0.57; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.77
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00019968765263911337
0.0001924204989336431
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.46; acc: 0.89
Val Epoch over. val_loss: 0.6122266809651806; val_accuracy: 0.8560907643312102 

The current subspace-distance is: 0.0001924204989336431 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.77; acc: 0.75
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.46; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.68; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.94
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.71; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.81
Batch: 640; loss: 0.77; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.9; acc: 0.77
Batch: 700; loss: 0.63; acc: 0.83
Batch: 720; loss: 0.61; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00020376509928610176
0.00019441301992628723
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.92
Val Epoch over. val_loss: 0.6091423860401105; val_accuracy: 0.859375 

The current subspace-distance is: 0.00019441301992628723 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.88
Batch: 40; loss: 0.84; acc: 0.73
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.89
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.7; acc: 0.89
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.84
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 0.7; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.77
Batch: 480; loss: 0.66; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.77; acc: 0.8
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.94
Batch: 580; loss: 0.67; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.55; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.89
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.76; acc: 0.8
Train Epoch over. train_loss: 0.66; train_accuracy: 0.84 

0.00020437098282855004
0.00019623702974058688
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.45; acc: 0.92
Val Epoch over. val_loss: 0.6025391570322073; val_accuracy: 0.8608678343949044 

The current subspace-distance is: 0.00019623702974058688 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.84
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.89; acc: 0.81
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.86; acc: 0.75
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.94
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.83; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.77
Batch: 580; loss: 0.83; acc: 0.73
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.73; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.83
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.66; acc: 0.78
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.0002051285991910845
0.00019837231957353652
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.5916769884194538; val_accuracy: 0.8601711783439491 

The current subspace-distance is: 0.00019837231957353652 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.62; acc: 0.89
Batch: 220; loss: 0.77; acc: 0.77
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.75; acc: 0.78
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.69; acc: 0.89
Batch: 400; loss: 0.84; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.72; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.55; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.68; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.74; acc: 0.78
Batch: 740; loss: 0.68; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00020480378589127213
0.0001958394714165479
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.602823523389306; val_accuracy: 0.8605692675159236 

The current subspace-distance is: 0.0001958394714165479 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.69; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.83
Batch: 260; loss: 0.76; acc: 0.75
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.94
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.57; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.76; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.91
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.66; acc: 0.88
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.89
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.89
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00020581038552336395
0.00019610693561844528
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.5901121444003598; val_accuracy: 0.861265923566879 

The current subspace-distance is: 0.00019610693561844528 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.78
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.71; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.91
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.86
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.91
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.73; acc: 0.83
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.91
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.67; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.52; acc: 0.91
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.71; acc: 0.83
Batch: 780; loss: 0.7; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00020578026305884123
0.00019701635756064206
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.94
Val Epoch over. val_loss: 0.6051589679566158; val_accuracy: 0.8597730891719745 

The current subspace-distance is: 0.00019701635756064206 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.78; acc: 0.86
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.54; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.79; acc: 0.73
Batch: 180; loss: 0.66; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.47; acc: 0.94
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.88; acc: 0.75
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.8; acc: 0.78
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.65; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.8
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.77
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.65; train_accuracy: 0.84 

0.00020653734100051224
0.00020139198750257492
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.5969875819364171; val_accuracy: 0.8602707006369427 

The current subspace-distance is: 0.00020139198750257492 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.91
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.77
Batch: 380; loss: 0.7; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.8
Batch: 420; loss: 0.56; acc: 0.91
Batch: 440; loss: 0.77; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.71; acc: 0.78
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.75; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.72; acc: 0.8
Batch: 740; loss: 0.7; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.00020761674386449158
0.00019968629931099713
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.5928285765420099; val_accuracy: 0.859375 

The current subspace-distance is: 0.00019968629931099713 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.56; acc: 0.89
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.94
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.75; acc: 0.77
Batch: 280; loss: 0.7; acc: 0.73
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.83; acc: 0.81
Batch: 460; loss: 0.57; acc: 0.91
Batch: 480; loss: 0.75; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.91
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.77; acc: 0.73
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.86; acc: 0.73
Batch: 720; loss: 0.55; acc: 0.92
Batch: 740; loss: 0.68; acc: 0.75
Batch: 760; loss: 0.73; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00021065525652375072
0.00020105837029404938
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.43; acc: 0.94
Val Epoch over. val_loss: 0.5901815033262703; val_accuracy: 0.8615644904458599 

The current subspace-distance is: 0.00020105837029404938 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.61; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.62; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.71; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.91
Batch: 240; loss: 0.81; acc: 0.75
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.91
Batch: 300; loss: 0.63; acc: 0.86
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.84; acc: 0.73
Batch: 380; loss: 0.75; acc: 0.75
Batch: 400; loss: 0.63; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.92
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.77; acc: 0.75
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.77; acc: 0.78
Batch: 680; loss: 0.64; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.85 

0.00020884073455817997
0.0002009368472499773
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.92
Val Epoch over. val_loss: 0.5938539990953579; val_accuracy: 0.8588773885350318 

The current subspace-distance is: 0.0002009368472499773 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.54; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.78
Batch: 100; loss: 0.73; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.94
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.7; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.59; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 0.7; acc: 0.83
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.92
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.68; acc: 0.81
Batch: 480; loss: 0.56; acc: 0.92
Batch: 500; loss: 0.7; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.88
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.97
Batch: 580; loss: 0.53; acc: 0.91
Batch: 600; loss: 0.7; acc: 0.83
Batch: 620; loss: 0.74; acc: 0.84
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.94
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.82; acc: 0.73
Batch: 740; loss: 0.6; acc: 0.89
Batch: 760; loss: 0.78; acc: 0.78
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.84 

0.0002105144812958315
0.0002024751593125984
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.43; acc: 0.94
Val Epoch over. val_loss: 0.5858313031257338; val_accuracy: 0.8624601910828026 

The current subspace-distance is: 0.0002024751593125984 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_4_flips_True_d_dim_200_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.75

The number of parameters is: 269845

The number of individual parameters is:

22
352
22
22
33
40656
33
33
66
121968
66
66
64
101376
64
64
4096
64
640
10
64
64

nonzero elements in E: 80953492
elements in E: 80953500
fraction nonzero: 0.9999999011778367
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.5; acc: 0.03
Batch: 20; loss: 2.24; acc: 0.17
Batch: 40; loss: 1.99; acc: 0.28
Batch: 60; loss: 1.85; acc: 0.56
Batch: 80; loss: 1.78; acc: 0.5
Batch: 100; loss: 1.72; acc: 0.61
Batch: 120; loss: 1.8; acc: 0.61
Batch: 140; loss: 1.67; acc: 0.62
Batch: 160; loss: 1.62; acc: 0.58
Batch: 180; loss: 1.49; acc: 0.72
Batch: 200; loss: 1.57; acc: 0.61
Batch: 220; loss: 1.54; acc: 0.62
Batch: 240; loss: 1.48; acc: 0.7
Batch: 260; loss: 1.56; acc: 0.66
Batch: 280; loss: 1.48; acc: 0.64
Batch: 300; loss: 1.48; acc: 0.72
Batch: 320; loss: 1.43; acc: 0.7
Batch: 340; loss: 1.35; acc: 0.72
Batch: 360; loss: 1.47; acc: 0.64
Batch: 380; loss: 1.42; acc: 0.7
Batch: 400; loss: 1.35; acc: 0.72
Batch: 420; loss: 1.39; acc: 0.7
Batch: 440; loss: 1.26; acc: 0.8
Batch: 460; loss: 1.39; acc: 0.69
Batch: 480; loss: 1.15; acc: 0.83
Batch: 500; loss: 1.26; acc: 0.78
Batch: 520; loss: 1.16; acc: 0.83
Batch: 540; loss: 1.37; acc: 0.66
Batch: 560; loss: 1.2; acc: 0.81
Batch: 580; loss: 1.17; acc: 0.78
Batch: 600; loss: 1.22; acc: 0.83
Batch: 620; loss: 1.43; acc: 0.66
Batch: 640; loss: 1.24; acc: 0.66
Batch: 660; loss: 1.23; acc: 0.73
Batch: 680; loss: 1.23; acc: 0.77
Batch: 700; loss: 1.23; acc: 0.72
Batch: 720; loss: 1.14; acc: 0.75
Batch: 740; loss: 1.19; acc: 0.75
Batch: 760; loss: 1.09; acc: 0.81
Batch: 780; loss: 1.21; acc: 0.67
Train Epoch over. train_loss: 1.46; train_accuracy: 0.65 

6.577084423042834e-05
6.0590627981582657e-05
Batch: 0; loss: 1.04; acc: 0.91
Batch: 20; loss: 1.29; acc: 0.7
Batch: 40; loss: 0.87; acc: 0.94
Batch: 60; loss: 1.05; acc: 0.75
Batch: 80; loss: 1.05; acc: 0.86
Batch: 100; loss: 1.16; acc: 0.75
Batch: 120; loss: 1.27; acc: 0.7
Batch: 140; loss: 1.15; acc: 0.8
Val Epoch over. val_loss: 1.1366364443378083; val_accuracy: 0.7825437898089171 

The current subspace-distance is: 6.0590627981582657e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.81
Batch: 20; loss: 1.18; acc: 0.8
Batch: 40; loss: 1.27; acc: 0.69
Batch: 60; loss: 1.18; acc: 0.73
Batch: 80; loss: 1.11; acc: 0.81
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 1.05; acc: 0.81
Batch: 140; loss: 1.13; acc: 0.84
Batch: 160; loss: 1.09; acc: 0.8
Batch: 180; loss: 1.16; acc: 0.75
Batch: 200; loss: 1.05; acc: 0.78
Batch: 220; loss: 1.1; acc: 0.81
Batch: 240; loss: 0.99; acc: 0.88
Batch: 260; loss: 1.01; acc: 0.83
Batch: 280; loss: 1.02; acc: 0.78
Batch: 300; loss: 1.23; acc: 0.69
Batch: 320; loss: 1.0; acc: 0.8
Batch: 340; loss: 1.2; acc: 0.7
Batch: 360; loss: 0.97; acc: 0.83
Batch: 380; loss: 1.15; acc: 0.77
Batch: 400; loss: 1.03; acc: 0.83
Batch: 420; loss: 1.04; acc: 0.81
Batch: 440; loss: 1.16; acc: 0.77
Batch: 460; loss: 1.03; acc: 0.81
Batch: 480; loss: 1.11; acc: 0.78
Batch: 500; loss: 1.0; acc: 0.84
Batch: 520; loss: 1.01; acc: 0.89
Batch: 540; loss: 1.23; acc: 0.69
Batch: 560; loss: 1.11; acc: 0.72
Batch: 580; loss: 1.05; acc: 0.78
Batch: 600; loss: 1.01; acc: 0.81
Batch: 620; loss: 1.1; acc: 0.78
Batch: 640; loss: 1.06; acc: 0.84
Batch: 660; loss: 1.12; acc: 0.75
Batch: 680; loss: 0.89; acc: 0.91
Batch: 700; loss: 0.99; acc: 0.84
Batch: 720; loss: 1.05; acc: 0.8
Batch: 740; loss: 1.02; acc: 0.77
Batch: 760; loss: 0.91; acc: 0.84
Batch: 780; loss: 1.02; acc: 0.78
Train Epoch over. train_loss: 1.08; train_accuracy: 0.79 

8.883194823283702e-05
8.227924263337627e-05
Batch: 0; loss: 0.86; acc: 0.94
Batch: 20; loss: 1.06; acc: 0.78
Batch: 40; loss: 0.71; acc: 0.95
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.88; acc: 0.91
Batch: 100; loss: 1.0; acc: 0.83
Batch: 120; loss: 1.08; acc: 0.78
Batch: 140; loss: 0.88; acc: 0.92
Val Epoch over. val_loss: 0.963102376764747; val_accuracy: 0.8341958598726115 

The current subspace-distance is: 8.227924263337627e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.86
Batch: 20; loss: 0.95; acc: 0.86
Batch: 40; loss: 1.09; acc: 0.78
Batch: 60; loss: 0.92; acc: 0.86
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.93; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.81
Batch: 140; loss: 0.86; acc: 0.86
Batch: 160; loss: 0.95; acc: 0.84
Batch: 180; loss: 0.94; acc: 0.78
Batch: 200; loss: 1.04; acc: 0.81
Batch: 220; loss: 0.96; acc: 0.88
Batch: 240; loss: 1.01; acc: 0.83
Batch: 260; loss: 1.08; acc: 0.75
Batch: 280; loss: 0.84; acc: 0.84
Batch: 300; loss: 1.04; acc: 0.81
Batch: 320; loss: 0.94; acc: 0.83
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 0.84; acc: 0.92
Batch: 380; loss: 0.99; acc: 0.83
Batch: 400; loss: 1.04; acc: 0.8
Batch: 420; loss: 0.91; acc: 0.83
Batch: 440; loss: 0.89; acc: 0.88
Batch: 460; loss: 0.92; acc: 0.84
Batch: 480; loss: 0.83; acc: 0.89
Batch: 500; loss: 1.16; acc: 0.75
Batch: 520; loss: 0.88; acc: 0.86
Batch: 540; loss: 0.97; acc: 0.81
Batch: 560; loss: 1.04; acc: 0.78
Batch: 580; loss: 0.89; acc: 0.84
Batch: 600; loss: 0.99; acc: 0.81
Batch: 620; loss: 1.0; acc: 0.78
Batch: 640; loss: 1.0; acc: 0.78
Batch: 660; loss: 0.94; acc: 0.8
Batch: 680; loss: 0.91; acc: 0.8
Batch: 700; loss: 0.84; acc: 0.86
Batch: 720; loss: 0.89; acc: 0.86
Batch: 740; loss: 0.92; acc: 0.83
Batch: 760; loss: 0.97; acc: 0.83
Batch: 780; loss: 0.88; acc: 0.83
Train Epoch over. train_loss: 0.95; train_accuracy: 0.82 

0.0001040427086991258
9.817154932534322e-05
Batch: 0; loss: 0.75; acc: 0.92
Batch: 20; loss: 0.94; acc: 0.84
Batch: 40; loss: 0.62; acc: 0.92
Batch: 60; loss: 0.76; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.91
Batch: 100; loss: 0.9; acc: 0.83
Batch: 120; loss: 1.01; acc: 0.78
Batch: 140; loss: 0.77; acc: 0.95
Val Epoch over. val_loss: 0.856801799148511; val_accuracy: 0.8479299363057324 

The current subspace-distance is: 9.817154932534322e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.84
Batch: 40; loss: 0.9; acc: 0.86
Batch: 60; loss: 0.93; acc: 0.81
Batch: 80; loss: 0.79; acc: 0.89
Batch: 100; loss: 0.93; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.83
Batch: 140; loss: 0.81; acc: 0.88
Batch: 160; loss: 0.85; acc: 0.81
Batch: 180; loss: 0.81; acc: 0.86
Batch: 200; loss: 0.77; acc: 0.86
Batch: 220; loss: 0.89; acc: 0.86
Batch: 240; loss: 0.88; acc: 0.78
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.88; acc: 0.91
Batch: 300; loss: 0.85; acc: 0.81
Batch: 320; loss: 0.94; acc: 0.83
Batch: 340; loss: 0.78; acc: 0.89
Batch: 360; loss: 0.78; acc: 0.84
Batch: 380; loss: 0.94; acc: 0.91
Batch: 400; loss: 0.91; acc: 0.8
Batch: 420; loss: 0.87; acc: 0.83
Batch: 440; loss: 0.78; acc: 0.88
Batch: 460; loss: 0.94; acc: 0.8
Batch: 480; loss: 0.78; acc: 0.88
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.87; acc: 0.8
Batch: 540; loss: 0.92; acc: 0.83
Batch: 560; loss: 0.77; acc: 0.84
Batch: 580; loss: 0.68; acc: 0.91
Batch: 600; loss: 0.81; acc: 0.84
Batch: 620; loss: 0.96; acc: 0.8
Batch: 640; loss: 0.91; acc: 0.8
Batch: 660; loss: 0.86; acc: 0.81
Batch: 680; loss: 0.93; acc: 0.8
Batch: 700; loss: 0.99; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.91
Batch: 740; loss: 0.95; acc: 0.81
Batch: 760; loss: 0.78; acc: 0.88
Batch: 780; loss: 0.78; acc: 0.84
Train Epoch over. train_loss: 0.86; train_accuracy: 0.83 

0.00011946472659474239
0.00011255031131440774
Batch: 0; loss: 0.66; acc: 0.94
Batch: 20; loss: 0.89; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.91
Batch: 100; loss: 0.8; acc: 0.89
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.95
Val Epoch over. val_loss: 0.7778256129307352; val_accuracy: 0.8583797770700637 

The current subspace-distance is: 0.00011255031131440774 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.91
Batch: 20; loss: 0.85; acc: 0.83
Batch: 40; loss: 0.74; acc: 0.89
Batch: 60; loss: 0.77; acc: 0.89
Batch: 80; loss: 0.79; acc: 0.88
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 1.02; acc: 0.72
Batch: 160; loss: 0.78; acc: 0.91
Batch: 180; loss: 0.92; acc: 0.77
Batch: 200; loss: 0.78; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.89
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.9; acc: 0.78
Batch: 280; loss: 0.81; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.83
Batch: 320; loss: 0.71; acc: 0.91
Batch: 340; loss: 0.88; acc: 0.75
Batch: 360; loss: 0.71; acc: 0.86
Batch: 380; loss: 1.0; acc: 0.72
Batch: 400; loss: 0.71; acc: 0.91
Batch: 420; loss: 0.76; acc: 0.88
Batch: 440; loss: 0.78; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.89
Batch: 480; loss: 0.74; acc: 0.83
Batch: 500; loss: 0.99; acc: 0.77
Batch: 520; loss: 0.7; acc: 0.91
Batch: 540; loss: 0.74; acc: 0.86
Batch: 560; loss: 0.73; acc: 0.86
Batch: 580; loss: 0.82; acc: 0.84
Batch: 600; loss: 0.7; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.88
Batch: 640; loss: 0.72; acc: 0.88
Batch: 660; loss: 0.88; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.89
Batch: 700; loss: 0.82; acc: 0.81
Batch: 720; loss: 0.81; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.94
Batch: 760; loss: 0.78; acc: 0.8
Batch: 780; loss: 0.72; acc: 0.88
Train Epoch over. train_loss: 0.79; train_accuracy: 0.84 

0.00013098107592668384
0.0001248611806659028
Batch: 0; loss: 0.61; acc: 0.92
Batch: 20; loss: 0.86; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.91
Batch: 100; loss: 0.71; acc: 0.88
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.65; acc: 0.94
Val Epoch over. val_loss: 0.7126958890325704; val_accuracy: 0.8675358280254777 

The current subspace-distance is: 0.0001248611806659028 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.91
Batch: 40; loss: 0.74; acc: 0.91
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.92
Batch: 200; loss: 0.58; acc: 0.92
Batch: 220; loss: 0.78; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.84
Batch: 280; loss: 0.7; acc: 0.91
Batch: 300; loss: 0.86; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.86
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.64; acc: 0.86
Batch: 380; loss: 0.73; acc: 0.91
Batch: 400; loss: 0.69; acc: 0.88
Batch: 420; loss: 0.79; acc: 0.8
Batch: 440; loss: 0.65; acc: 0.91
Batch: 460; loss: 0.75; acc: 0.88
Batch: 480; loss: 0.95; acc: 0.73
Batch: 500; loss: 0.73; acc: 0.89
Batch: 520; loss: 0.83; acc: 0.81
Batch: 540; loss: 0.82; acc: 0.78
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.86
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.88
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.89
Batch: 700; loss: 0.68; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.91
Batch: 740; loss: 0.74; acc: 0.86
Batch: 760; loss: 0.62; acc: 0.95
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.74; train_accuracy: 0.85 

0.0001424747460987419
0.00013555950135923922
Batch: 0; loss: 0.55; acc: 0.95
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.91
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.95
Val Epoch over. val_loss: 0.6607819235628578; val_accuracy: 0.8762937898089171 

The current subspace-distance is: 0.00013555950135923922 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.79; acc: 0.84
Batch: 40; loss: 0.71; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.89
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.78; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.92
Batch: 200; loss: 0.61; acc: 0.92
Batch: 220; loss: 0.7; acc: 0.88
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.89
Batch: 280; loss: 0.83; acc: 0.77
Batch: 300; loss: 0.59; acc: 0.91
Batch: 320; loss: 0.73; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.88
Batch: 360; loss: 0.72; acc: 0.89
Batch: 380; loss: 0.67; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.88
Batch: 420; loss: 0.64; acc: 0.88
Batch: 440; loss: 0.85; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.83
Batch: 480; loss: 0.71; acc: 0.84
Batch: 500; loss: 0.86; acc: 0.78
Batch: 520; loss: 0.72; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.94
Batch: 580; loss: 0.69; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.84
Batch: 640; loss: 0.7; acc: 0.84
Batch: 660; loss: 0.75; acc: 0.81
Batch: 680; loss: 0.68; acc: 0.88
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.76; acc: 0.83
Batch: 780; loss: 0.81; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.86 

0.00015426153549924493
0.00014650102821178734
Batch: 0; loss: 0.53; acc: 0.95
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.94
Batch: 100; loss: 0.6; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.95
Val Epoch over. val_loss: 0.6219272220590312; val_accuracy: 0.8774880573248408 

The current subspace-distance is: 0.00014650102821178734 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.86
Batch: 140; loss: 0.69; acc: 0.81
Batch: 160; loss: 0.81; acc: 0.81
Batch: 180; loss: 0.54; acc: 0.92
Batch: 200; loss: 0.79; acc: 0.77
Batch: 220; loss: 0.64; acc: 0.88
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.92
Batch: 280; loss: 0.62; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.95
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.89
Batch: 360; loss: 0.68; acc: 0.89
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.72; acc: 0.81
Batch: 420; loss: 0.65; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.91
Batch: 500; loss: 0.67; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.91
Batch: 540; loss: 0.69; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.77; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.89
Batch: 680; loss: 0.71; acc: 0.84
Batch: 700; loss: 0.76; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.88
Batch: 760; loss: 0.72; acc: 0.84
Batch: 780; loss: 0.68; acc: 0.86
Train Epoch over. train_loss: 0.67; train_accuracy: 0.86 

0.00016210957255680114
0.00015593745047226548
Batch: 0; loss: 0.5; acc: 0.94
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.95
Val Epoch over. val_loss: 0.5902339761044569; val_accuracy: 0.8817675159235668 

The current subspace-distance is: 0.00015593745047226548 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.55; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.94
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.95
Batch: 180; loss: 0.55; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.94
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 0.58; acc: 0.89
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.78; acc: 0.86
Batch: 520; loss: 0.68; acc: 0.88
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.94
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.63; acc: 0.91
Batch: 680; loss: 0.75; acc: 0.81
Batch: 700; loss: 0.67; acc: 0.88
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.71; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.86 

0.00017267225484829396
0.0001658877736190334
Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.97
Val Epoch over. val_loss: 0.5651946870764349; val_accuracy: 0.8837579617834395 

The current subspace-distance is: 0.0001658877736190334 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.92
Batch: 160; loss: 0.65; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.92
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.94
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.94
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.94
Batch: 580; loss: 0.59; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.77
Batch: 680; loss: 0.57; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.72; acc: 0.81
Train Epoch over. train_loss: 0.61; train_accuracy: 0.86 

0.0001817262382246554
0.00017410020518582314
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.97
Val Epoch over. val_loss: 0.5364716831286243; val_accuracy: 0.8863455414012739 

The current subspace-distance is: 0.00017410020518582314 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.95
Batch: 500; loss: 0.51; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.94
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.92
Batch: 620; loss: 0.62; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

0.00018340461247134954
0.00017540720000397414
Batch: 0; loss: 0.44; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.37; acc: 0.97
Val Epoch over. val_loss: 0.5329679341832544; val_accuracy: 0.8883359872611465 

The current subspace-distance is: 0.00017540720000397414 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.94
Batch: 260; loss: 0.68; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.48; acc: 0.95
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.59; train_accuracy: 0.86 

0.00018721095693763345
0.00018140375323127955
Batch: 0; loss: 0.42; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.5223774165864203; val_accuracy: 0.8857484076433121 

The current subspace-distance is: 0.00018140375323127955 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.91
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.95
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.77
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.68; acc: 0.8
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.91
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.7; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.88
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.88
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.92
Batch: 720; loss: 0.79; acc: 0.78
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.71; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

0.00018946168711408973
0.00018235837342217565
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.97
Val Epoch over. val_loss: 0.5167185789460589; val_accuracy: 0.8884355095541401 

The current subspace-distance is: 0.00018235837342217565 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.73; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.77; acc: 0.77
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.64; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.48; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

0.00019085148232989013
0.0001839667820604518
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.97
Val Epoch over. val_loss: 0.5156453601114309; val_accuracy: 0.8898288216560509 

The current subspace-distance is: 0.0001839667820604518 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.92
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.95
Batch: 240; loss: 0.46; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.94
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.89
Batch: 640; loss: 0.64; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.97
Batch: 780; loss: 0.67; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00019438200979493558
0.00018734816694632173
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5111703460763215; val_accuracy: 0.8881369426751592 

The current subspace-distance is: 0.00018734816694632173 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.97
Batch: 260; loss: 0.51; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.95
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.7; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.82; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00019479797629173845
0.00018765637651085854
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5084941960444116; val_accuracy: 0.8891321656050956 

The current subspace-distance is: 0.00018765637651085854 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.88
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.39; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.71; acc: 0.83
Batch: 360; loss: 0.55; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.63; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.67; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.52; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.87 

0.00019762635929509997
0.00018977139552589506
Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.78; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5023676098152331; val_accuracy: 0.890625 

The current subspace-distance is: 0.00018977139552589506 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.61; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.78
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.67; acc: 0.86
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.72; acc: 0.81
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00020229454094078392
0.00019375183910597116
Batch: 0; loss: 0.42; acc: 0.97
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.5106649260232403; val_accuracy: 0.8864450636942676 

The current subspace-distance is: 0.00019375183910597116 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.91
Batch: 200; loss: 0.71; acc: 0.73
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.94
Batch: 320; loss: 0.62; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.95
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.51; acc: 0.94
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.56; train_accuracy: 0.87 

0.00020365306409075856
0.00019712904759217054
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.49442568563731615; val_accuracy: 0.8914211783439491 

The current subspace-distance is: 0.00019712904759217054 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.89
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.52; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.92
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.94
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0002066265296889469
0.000196825189050287
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.95
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.48653110804831146; val_accuracy: 0.892515923566879 

The current subspace-distance is: 0.000196825189050287 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.62; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.97
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.75; acc: 0.73
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.75; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.94
Batch: 780; loss: 0.56; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.0002067828900180757
0.00019680631521623582
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.4844250044055805; val_accuracy: 0.8921178343949044 

The current subspace-distance is: 0.00019680631521623582 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.62; acc: 0.8
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.94
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.71; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00020491979375947267
0.00019822458853013813
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.97
Val Epoch over. val_loss: 0.47651423969466217; val_accuracy: 0.8946058917197452 

The current subspace-distance is: 0.00019822458853013813 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.91
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.38; acc: 0.94
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.89
Batch: 420; loss: 0.58; acc: 0.8
Batch: 440; loss: 0.64; acc: 0.78
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.8
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

0.00020775847951881588
0.0001998521329369396
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.48868388184316597; val_accuracy: 0.8878383757961783 

The current subspace-distance is: 0.0001998521329369396 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.97
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.4; acc: 0.95
Batch: 420; loss: 0.59; acc: 0.8
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.66; acc: 0.83
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.97
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020585823222063482
0.0001993706973735243
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4790926793008853; val_accuracy: 0.8936106687898089 

The current subspace-distance is: 0.0001993706973735243 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.49; acc: 0.94
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.84
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.0002091145288432017
0.00020050816237926483
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.95
Val Epoch over. val_loss: 0.4826711837653142; val_accuracy: 0.8904259554140127 

The current subspace-distance is: 0.00020050816237926483 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.95
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.94
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.54; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.95
Batch: 300; loss: 0.61; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.97
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.8
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.64; acc: 0.78
Batch: 780; loss: 0.42; acc: 0.95
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020780311024282128
0.00020193417731206864
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.48682828010267515; val_accuracy: 0.8891321656050956 

The current subspace-distance is: 0.00020193417731206864 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.59; acc: 0.8
Batch: 220; loss: 0.76; acc: 0.73
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.5; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00020924746058881283
0.00020119927648920566
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.4753734881331207; val_accuracy: 0.8949044585987261 

The current subspace-distance is: 0.00020119927648920566 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.79; acc: 0.73
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.88
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.7; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.65; acc: 0.78
Batch: 760; loss: 0.46; acc: 0.94
Batch: 780; loss: 0.59; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.0002081789425574243
0.00019998646166641265
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.95
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.47206472211582645; val_accuracy: 0.8929140127388535 

The current subspace-distance is: 0.00019998646166641265 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.95
Batch: 140; loss: 0.41; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.81; acc: 0.77
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.71; acc: 0.78
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.5; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00021204816584941
0.00020498456433415413
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4754885055456951; val_accuracy: 0.8926154458598726 

The current subspace-distance is: 0.00020498456433415413 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.95
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.5; acc: 0.92
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.77
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.87 

0.00021134216513019055
0.00020262312318664044
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.95
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.95
Val Epoch over. val_loss: 0.4722915964711244; val_accuracy: 0.89171974522293 

The current subspace-distance is: 0.00020262312318664044 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_4_flips_True_d_dim_300_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.75

The number of parameters is: 269845

The number of individual parameters is:

22
352
22
22
33
40656
33
33
66
121968
66
66
64
101376
64
64
4096
64
640
10
64
64

nonzero elements in E: 107937989
elements in E: 107938000
fraction nonzero: 0.9999998980896441
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.49; acc: 0.12
Batch: 20; loss: 2.14; acc: 0.25
Batch: 40; loss: 1.97; acc: 0.38
Batch: 60; loss: 1.75; acc: 0.59
Batch: 80; loss: 1.7; acc: 0.55
Batch: 100; loss: 1.54; acc: 0.72
Batch: 120; loss: 1.63; acc: 0.53
Batch: 140; loss: 1.63; acc: 0.55
Batch: 160; loss: 1.41; acc: 0.78
Batch: 180; loss: 1.35; acc: 0.73
Batch: 200; loss: 1.44; acc: 0.67
Batch: 220; loss: 1.35; acc: 0.77
Batch: 240; loss: 1.32; acc: 0.72
Batch: 260; loss: 1.29; acc: 0.78
Batch: 280; loss: 1.38; acc: 0.66
Batch: 300; loss: 1.3; acc: 0.73
Batch: 320; loss: 1.16; acc: 0.83
Batch: 340; loss: 1.22; acc: 0.81
Batch: 360; loss: 1.3; acc: 0.66
Batch: 380; loss: 1.13; acc: 0.78
Batch: 400; loss: 1.28; acc: 0.73
Batch: 420; loss: 1.3; acc: 0.7
Batch: 440; loss: 1.21; acc: 0.75
Batch: 460; loss: 1.09; acc: 0.81
Batch: 480; loss: 1.2; acc: 0.72
Batch: 500; loss: 1.09; acc: 0.75
Batch: 520; loss: 1.07; acc: 0.88
Batch: 540; loss: 1.13; acc: 0.77
Batch: 560; loss: 1.11; acc: 0.81
Batch: 580; loss: 1.11; acc: 0.77
Batch: 600; loss: 1.07; acc: 0.81
Batch: 620; loss: 1.1; acc: 0.75
Batch: 640; loss: 0.96; acc: 0.84
Batch: 660; loss: 1.0; acc: 0.81
Batch: 680; loss: 1.15; acc: 0.77
Batch: 700; loss: 0.98; acc: 0.83
Batch: 720; loss: 0.9; acc: 0.89
Batch: 740; loss: 1.15; acc: 0.73
Batch: 760; loss: 1.07; acc: 0.81
Batch: 780; loss: 1.08; acc: 0.81
Train Epoch over. train_loss: 1.31; train_accuracy: 0.71 

2.6943860575556755e-05
9.476630111748818e-06
Batch: 0; loss: 0.96; acc: 0.88
Batch: 20; loss: 1.11; acc: 0.72
Batch: 40; loss: 0.71; acc: 0.94
Batch: 60; loss: 0.93; acc: 0.77
Batch: 80; loss: 0.85; acc: 0.84
Batch: 100; loss: 1.01; acc: 0.81
Batch: 120; loss: 1.09; acc: 0.77
Batch: 140; loss: 0.85; acc: 0.94
Val Epoch over. val_loss: 0.9477711646420182; val_accuracy: 0.8354896496815286 

The current subspace-distance is: 9.476630111748818e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.06; acc: 0.75
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 0.91; acc: 0.86
Batch: 80; loss: 0.97; acc: 0.81
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.88
Batch: 140; loss: 0.97; acc: 0.83
Batch: 160; loss: 0.92; acc: 0.88
Batch: 180; loss: 0.94; acc: 0.84
Batch: 200; loss: 1.0; acc: 0.78
Batch: 220; loss: 0.79; acc: 0.89
Batch: 240; loss: 0.93; acc: 0.83
Batch: 260; loss: 0.8; acc: 0.88
Batch: 280; loss: 0.94; acc: 0.77
Batch: 300; loss: 1.04; acc: 0.8
Batch: 320; loss: 0.88; acc: 0.81
Batch: 340; loss: 0.85; acc: 0.84
Batch: 360; loss: 0.77; acc: 0.91
Batch: 380; loss: 0.77; acc: 0.88
Batch: 400; loss: 0.94; acc: 0.84
Batch: 420; loss: 0.95; acc: 0.77
Batch: 440; loss: 0.88; acc: 0.8
Batch: 460; loss: 0.96; acc: 0.83
Batch: 480; loss: 0.97; acc: 0.78
Batch: 500; loss: 0.92; acc: 0.86
Batch: 520; loss: 0.88; acc: 0.8
Batch: 540; loss: 0.88; acc: 0.84
Batch: 560; loss: 0.74; acc: 0.88
Batch: 580; loss: 0.86; acc: 0.83
Batch: 600; loss: 0.91; acc: 0.88
Batch: 620; loss: 0.92; acc: 0.8
Batch: 640; loss: 0.88; acc: 0.86
Batch: 660; loss: 0.79; acc: 0.84
Batch: 680; loss: 0.81; acc: 0.84
Batch: 700; loss: 0.8; acc: 0.88
Batch: 720; loss: 0.86; acc: 0.84
Batch: 740; loss: 0.87; acc: 0.86
Batch: 760; loss: 0.71; acc: 0.92
Batch: 780; loss: 0.72; acc: 0.91
Train Epoch over. train_loss: 0.88; train_accuracy: 0.84 

3.370570266270079e-05
1.2711538147414103e-05
Batch: 0; loss: 0.79; acc: 0.89
Batch: 20; loss: 0.88; acc: 0.84
Batch: 40; loss: 0.48; acc: 0.98
Batch: 60; loss: 0.78; acc: 0.78
Batch: 80; loss: 0.63; acc: 0.89
Batch: 100; loss: 0.79; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.97
Val Epoch over. val_loss: 0.7334420648729725; val_accuracy: 0.8726114649681529 

The current subspace-distance is: 1.2711538147414103e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.78; acc: 0.86
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.89
Batch: 80; loss: 0.75; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.88
Batch: 140; loss: 0.72; acc: 0.86
Batch: 160; loss: 0.81; acc: 0.86
Batch: 180; loss: 0.84; acc: 0.77
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.91
Batch: 240; loss: 0.77; acc: 0.84
Batch: 260; loss: 0.81; acc: 0.8
Batch: 280; loss: 0.88; acc: 0.77
Batch: 300; loss: 0.8; acc: 0.84
Batch: 320; loss: 0.87; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.88
Batch: 360; loss: 0.8; acc: 0.88
Batch: 380; loss: 0.73; acc: 0.89
Batch: 400; loss: 0.79; acc: 0.83
Batch: 420; loss: 0.73; acc: 0.84
Batch: 440; loss: 0.72; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.92
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.89
Batch: 540; loss: 0.7; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.88
Batch: 600; loss: 0.74; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.88
Batch: 640; loss: 0.8; acc: 0.78
Batch: 660; loss: 0.81; acc: 0.83
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.74; acc: 0.83
Batch: 740; loss: 0.77; acc: 0.8
Batch: 760; loss: 0.65; acc: 0.89
Batch: 780; loss: 0.71; acc: 0.89
Train Epoch over. train_loss: 0.74; train_accuracy: 0.86 

3.7434878322528675e-05
1.495875676482683e-05
Batch: 0; loss: 0.69; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.98
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.92
Batch: 100; loss: 0.68; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.97
Val Epoch over. val_loss: 0.6345687018837899; val_accuracy: 0.8818670382165605 

The current subspace-distance is: 1.495875676482683e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.91
Batch: 20; loss: 0.74; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.91
Batch: 80; loss: 0.82; acc: 0.84
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.82; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.89
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.88
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 0.6; acc: 0.92
Batch: 260; loss: 0.58; acc: 0.92
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.94
Batch: 420; loss: 0.61; acc: 0.92
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.76; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.88
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.61; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.6; acc: 0.94
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.68; acc: 0.8
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.91
Batch: 660; loss: 0.61; acc: 0.91
Batch: 680; loss: 0.68; acc: 0.84
Batch: 700; loss: 0.6; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.67; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.94
Train Epoch over. train_loss: 0.66; train_accuracy: 0.87 

4.07766638090834e-05
1.5967594663379714e-05
Batch: 0; loss: 0.6; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.97
Val Epoch over. val_loss: 0.560438445988734; val_accuracy: 0.8928144904458599 

The current subspace-distance is: 1.5967594663379714e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 0.61; acc: 0.89
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.76; acc: 0.8
Batch: 260; loss: 0.54; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.58; acc: 0.92
Batch: 320; loss: 0.57; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.94
Batch: 380; loss: 0.5; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.91
Batch: 440; loss: 0.68; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.91
Batch: 480; loss: 0.62; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.91
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.95
Batch: 640; loss: 0.7; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.94
Batch: 700; loss: 0.6; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.6; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.6; train_accuracy: 0.88 

4.4434793380787596e-05
1.93860505532939e-05
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.97
Batch: 100; loss: 0.55; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.97
Val Epoch over. val_loss: 0.5074035743619226; val_accuracy: 0.902468152866242 

The current subspace-distance is: 1.93860505532939e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.94
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.59; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.95
Batch: 240; loss: 0.57; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.92
Batch: 340; loss: 0.67; acc: 0.91
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.44; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.95
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.94
Batch: 460; loss: 0.55; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.49; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.97
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.52; acc: 0.91
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.55; train_accuracy: 0.89 

4.702727892436087e-05
1.9856110156979412e-05
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.4756149632535922; val_accuracy: 0.9053542993630573 

The current subspace-distance is: 1.9856110156979412e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.97
Batch: 200; loss: 0.51; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.62; acc: 0.91
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.94
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.89 

4.949997673975304e-05
2.065605076495558e-05
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.97
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.45102933134622636; val_accuracy: 0.9090366242038217 

The current subspace-distance is: 2.065605076495558e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.54; acc: 0.91
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.89 

5.21459078299813e-05
2.264550857944414e-05
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.42787342694154973; val_accuracy: 0.9111265923566879 

The current subspace-distance is: 2.264550857944414e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.95
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.98
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.95
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.89 

5.3789255616720766e-05
2.4434322767774574e-05
Batch: 0; loss: 0.42; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.2; acc: 1.0
Val Epoch over. val_loss: 0.40081298474673255; val_accuracy: 0.9156050955414012 

The current subspace-distance is: 2.4434322767774574e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.37; acc: 0.97
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.95
Batch: 180; loss: 0.47; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.97
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.61; acc: 0.77
Batch: 580; loss: 0.35; acc: 0.95
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.92
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.9 

5.618509749183431e-05
2.6120469556190073e-05
Batch: 0; loss: 0.39; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.3852069792663975; val_accuracy: 0.9166998407643312 

The current subspace-distance is: 2.6120469556190073e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.98
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.95
Batch: 460; loss: 0.41; acc: 0.95
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.97
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.8
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.728428004658781e-05
2.565103022789117e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.37903764264978423; val_accuracy: 0.9173964968152867 

The current subspace-distance is: 2.565103022789117e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.97
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.78
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.9 

5.860843157279305e-05
2.6483920009923168e-05
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.18; acc: 1.0
Val Epoch over. val_loss: 0.3811649656409671; val_accuracy: 0.9161027070063694 

The current subspace-distance is: 2.6483920009923168e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.829269503010437e-05
2.4916353140724823e-05
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.379942574888278; val_accuracy: 0.9150079617834395 

The current subspace-distance is: 2.4916353140724823e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.93310542171821e-05
2.7353355108061805e-05
Batch: 0; loss: 0.37; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.369784480067575; val_accuracy: 0.9163017515923567 

The current subspace-distance is: 2.7353355108061805e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.97
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.59; acc: 0.8
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.59; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.98
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.9 

5.962895738775842e-05
2.7266500183031894e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.17; acc: 1.0
Val Epoch over. val_loss: 0.37272214092266787; val_accuracy: 0.9166998407643312 

The current subspace-distance is: 2.7266500183031894e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.31; acc: 1.0
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

6.0040285461582243e-05
2.7584230338106863e-05
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3651181247773444; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 2.7584230338106863e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.97
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.9 

5.959047848591581e-05
2.555567152739968e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3627490236121378; val_accuracy: 0.9169984076433121 

The current subspace-distance is: 2.555567152739968e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.27; acc: 1.0
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.0881349781993777e-05
2.6675032131606713e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.3645419611293039; val_accuracy: 0.9151074840764332 

The current subspace-distance is: 2.6675032131606713e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.110442336648703e-05
2.7312464226270095e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.35898618665850085; val_accuracy: 0.9156050955414012 

The current subspace-distance is: 2.7312464226270095e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.97
Batch: 480; loss: 0.43; acc: 0.83
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.193498120410368e-05
2.7833040803670883e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3518368442347095; val_accuracy: 0.9184912420382165 

The current subspace-distance is: 2.7833040803670883e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.97
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.9 

6.278223736444488e-05
2.8469499739003368e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3550117322403914; val_accuracy: 0.9155055732484076 

The current subspace-distance is: 2.8469499739003368e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.24; acc: 1.0
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.164386286400259e-05
2.7547848731046543e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.35494674087330036; val_accuracy: 0.9173964968152867 

The current subspace-distance is: 2.7547848731046543e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.17162004346028e-05
2.662587576196529e-05
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3447654889837192; val_accuracy: 0.9188893312101911 

The current subspace-distance is: 2.662587576196529e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.229427526704967e-05
2.7999818485113792e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.35263786972708006; val_accuracy: 0.917296974522293 

The current subspace-distance is: 2.7999818485113792e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.81
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.97
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.301249231910333e-05
2.7966880224994384e-05
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3500880653121669; val_accuracy: 0.9194864649681529 

The current subspace-distance is: 2.7966880224994384e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.32; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.289120210567489e-05
2.8657226721406914e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.3486095931689451; val_accuracy: 0.917296974522293 

The current subspace-distance is: 2.8657226721406914e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.302744441200048e-05
2.8598704375326633e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.35618515018444913; val_accuracy: 0.9173964968152867 

The current subspace-distance is: 2.8598704375326633e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.97
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.66; acc: 0.81
Batch: 480; loss: 0.34; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.8
Batch: 680; loss: 0.2; acc: 0.98
Batch: 700; loss: 0.36; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.265786942094564e-05
2.717696042964235e-05
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.34852953739227005; val_accuracy: 0.9177945859872612 

The current subspace-distance is: 2.717696042964235e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.97
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.270754965953529e-05
2.6827472538570873e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.34751162350557413; val_accuracy: 0.9194864649681529 

The current subspace-distance is: 2.6827472538570873e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.97
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.6; acc: 0.78
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.9 

6.364567525452003e-05
2.8916349037899636e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.15; acc: 1.0
Val Epoch over. val_loss: 0.3501243298980081; val_accuracy: 0.917296974522293 

The current subspace-distance is: 2.8916349037899636e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_4_flips_True_d_dim_400_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64

Channel scaling factor: 2.75

The number of parameters is: 269845

The number of individual parameters is:

22
352
22
22
33
40656
33
33
66
121968
66
66
64
101376
64
64
4096
64
640
10
64
64

nonzero elements in E: 134922489
elements in E: 134922500
fraction nonzero: 0.9999999184717152
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.46; acc: 0.03
Batch: 20; loss: 2.13; acc: 0.28
Batch: 40; loss: 1.85; acc: 0.45
Batch: 60; loss: 1.72; acc: 0.58
Batch: 80; loss: 1.53; acc: 0.7
Batch: 100; loss: 1.64; acc: 0.56
Batch: 120; loss: 1.46; acc: 0.75
Batch: 140; loss: 1.39; acc: 0.72
Batch: 160; loss: 1.32; acc: 0.77
Batch: 180; loss: 1.32; acc: 0.66
Batch: 200; loss: 1.28; acc: 0.78
Batch: 220; loss: 1.15; acc: 0.83
Batch: 240; loss: 1.13; acc: 0.84
Batch: 260; loss: 1.3; acc: 0.75
Batch: 280; loss: 1.07; acc: 0.83
Batch: 300; loss: 1.16; acc: 0.8
Batch: 320; loss: 1.1; acc: 0.84
Batch: 340; loss: 1.17; acc: 0.75
Batch: 360; loss: 1.17; acc: 0.72
Batch: 380; loss: 1.1; acc: 0.78
Batch: 400; loss: 1.04; acc: 0.86
Batch: 420; loss: 1.12; acc: 0.77
Batch: 440; loss: 1.19; acc: 0.75
Batch: 460; loss: 0.93; acc: 0.84
Batch: 480; loss: 0.92; acc: 0.91
Batch: 500; loss: 0.98; acc: 0.91
Batch: 520; loss: 1.0; acc: 0.78
Batch: 540; loss: 0.9; acc: 0.84
Batch: 560; loss: 1.06; acc: 0.77
Batch: 580; loss: 0.88; acc: 0.88
Batch: 600; loss: 0.87; acc: 0.88
Batch: 620; loss: 0.97; acc: 0.83
Batch: 640; loss: 0.93; acc: 0.81
Batch: 660; loss: 1.08; acc: 0.83
Batch: 680; loss: 1.01; acc: 0.84
Batch: 700; loss: 0.94; acc: 0.83
Batch: 720; loss: 0.81; acc: 0.91
Batch: 740; loss: 0.83; acc: 0.88
Batch: 760; loss: 0.87; acc: 0.8
Batch: 780; loss: 0.97; acc: 0.81
Train Epoch over. train_loss: 1.19; train_accuracy: 0.75 

2.7614209102466702e-05
8.9232071331935e-06
Batch: 0; loss: 0.82; acc: 0.92
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.74; acc: 0.88
Batch: 100; loss: 0.83; acc: 0.88
Batch: 120; loss: 0.97; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.92
Val Epoch over. val_loss: 0.8435155555700801; val_accuracy: 0.8553941082802548 

The current subspace-distance is: 8.9232071331935e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.91
Batch: 40; loss: 0.91; acc: 0.84
Batch: 60; loss: 0.84; acc: 0.88
Batch: 80; loss: 0.89; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.89
Batch: 140; loss: 0.75; acc: 0.94
Batch: 160; loss: 0.86; acc: 0.86
Batch: 180; loss: 0.91; acc: 0.78
Batch: 200; loss: 0.79; acc: 0.91
Batch: 220; loss: 1.1; acc: 0.73
Batch: 240; loss: 0.86; acc: 0.83
Batch: 260; loss: 0.83; acc: 0.78
Batch: 280; loss: 0.89; acc: 0.8
Batch: 300; loss: 0.99; acc: 0.75
Batch: 320; loss: 0.88; acc: 0.86
Batch: 340; loss: 0.87; acc: 0.83
Batch: 360; loss: 0.85; acc: 0.84
Batch: 380; loss: 0.81; acc: 0.88
Batch: 400; loss: 0.76; acc: 0.84
Batch: 420; loss: 0.82; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.91
Batch: 480; loss: 0.74; acc: 0.84
Batch: 500; loss: 0.72; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.88
Batch: 540; loss: 0.78; acc: 0.84
Batch: 560; loss: 0.7; acc: 0.89
Batch: 580; loss: 0.61; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.68; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.98
Batch: 660; loss: 0.72; acc: 0.83
Batch: 680; loss: 0.71; acc: 0.88
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.74; acc: 0.88
Batch: 760; loss: 0.6; acc: 0.89
Batch: 780; loss: 0.73; acc: 0.84
Train Epoch over. train_loss: 0.79; train_accuracy: 0.85 

3.257767457398586e-05
1.2285812772461213e-05
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.97
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 0.6658547900284931; val_accuracy: 0.8803742038216561 

The current subspace-distance is: 1.2285812772461213e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.94
Batch: 40; loss: 0.67; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.78; acc: 0.84
Batch: 160; loss: 0.77; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.89
Batch: 240; loss: 0.8; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.92
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.72; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.92
Batch: 420; loss: 0.85; acc: 0.75
Batch: 440; loss: 0.62; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.92
Batch: 520; loss: 0.59; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.53; acc: 0.94
Batch: 620; loss: 0.83; acc: 0.8
Batch: 640; loss: 0.48; acc: 0.97
Batch: 660; loss: 0.71; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.94
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.91
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.65; train_accuracy: 0.87 

3.7025372876087204e-05
1.4632903912570328e-05
Batch: 0; loss: 0.47; acc: 0.95
Batch: 20; loss: 0.81; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.97
Val Epoch over. val_loss: 0.5382613306212577; val_accuracy: 0.8976910828025477 

The current subspace-distance is: 1.4632903912570328e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.94
Batch: 40; loss: 0.63; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.94
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.94
Batch: 260; loss: 0.61; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.91
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.48; acc: 0.94
Batch: 340; loss: 0.66; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.92
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 0.46; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.97
Batch: 720; loss: 0.59; acc: 0.88
Batch: 740; loss: 0.7; acc: 0.81
Batch: 760; loss: 0.53; acc: 0.94
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.56; train_accuracy: 0.89 

4.0016024286160246e-05
1.6314223103108816e-05
Batch: 0; loss: 0.43; acc: 0.95
Batch: 20; loss: 0.69; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.98
Val Epoch over. val_loss: 0.4735936668648082; val_accuracy: 0.9061504777070064 

The current subspace-distance is: 1.6314223103108816e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.95
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.92
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.62; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.98
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.95
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.95
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.9 

4.3192787416046485e-05
1.7745682271197438e-05
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.4284627358814713; val_accuracy: 0.9138136942675159 

The current subspace-distance is: 1.7745682271197438e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.64; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.98
Batch: 280; loss: 0.46; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.53; acc: 0.81
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.9 

4.600792453857139e-05
1.9432140106800944e-05
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.98
Val Epoch over. val_loss: 0.39565120352681277; val_accuracy: 0.9156050955414012 

The current subspace-distance is: 1.9432140106800944e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.98
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.95
Batch: 360; loss: 0.3; acc: 1.0
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.95
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.91 

4.844751310884021e-05
2.0358369511086494e-05
Batch: 0; loss: 0.31; acc: 0.97
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.19; acc: 1.0
Val Epoch over. val_loss: 0.36575495949976006; val_accuracy: 0.9215764331210191 

The current subspace-distance is: 2.0358369511086494e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.97
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.53; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.97
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.91 

5.1388316933298483e-05
2.2714561055181548e-05
Batch: 0; loss: 0.28; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.16; acc: 1.0
Val Epoch over. val_loss: 0.33761576387532954; val_accuracy: 0.928343949044586 

The current subspace-distance is: 2.2714561055181548e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.97
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.97
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.91 

5.3136634960537776e-05
2.2534541130880825e-05
Batch: 0; loss: 0.24; acc: 0.98
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.32147144398112204; val_accuracy: 0.931031050955414 

The current subspace-distance is: 2.2534541130880825e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.92 

5.559639248531312e-05
2.4538916477467865e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2945710612330467; val_accuracy: 0.9353105095541401 

The current subspace-distance is: 2.4538916477467865e-05 

Epoch 11 start
The current lr is: 0.3
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.21; acc: 1.0
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.98
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.92 

5.666318247676827e-05
2.472147752996534e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.14; acc: 1.0
Val Epoch over. val_loss: 0.2902410178902043; val_accuracy: 0.93640525477707 

The current subspace-distance is: 2.472147752996534e-05 

Epoch 12 start
The current lr is: 0.3
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.19; acc: 1.0
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.98
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.724937727791257e-05
2.4598253730800934e-05
Batch: 0; loss: 0.22; acc: 0.98
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.13; acc: 1.0
Val Epoch over. val_loss: 0.2876333952614456; val_accuracy: 0.9358081210191083 

The current subspace-distance is: 2.4598253730800934e-05 

Epoch 13 start
The current lr is: 0.3
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.98
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.92 

5.745929229306057e-05
2.442779077682644e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.27628731509302834; val_accuracy: 0.9372014331210191 

The current subspace-distance is: 2.442779077682644e-05 

Epoch 14 start
The current lr is: 0.3
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.8172499848296866e-05
2.5915516744134948e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.27791990102476377; val_accuracy: 0.9371019108280255 

The current subspace-distance is: 2.5915516744134948e-05 

Epoch 15 start
The current lr is: 0.3
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.8661717048380524e-05
2.597835373308044e-05
Batch: 0; loss: 0.2; acc: 1.0
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2753388746908516; val_accuracy: 0.9373009554140127 

The current subspace-distance is: 2.597835373308044e-05 

Epoch 16 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.95
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.92 

5.833558680023998e-05
2.5319806809420697e-05
Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2734159469889228; val_accuracy: 0.9378980891719745 

The current subspace-distance is: 2.5319806809420697e-05 

Epoch 17 start
The current lr is: 0.3
Batch: 0; loss: 0.26; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.2; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.18; acc: 1.0
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.932081330684014e-05
2.6234252800350077e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.2700947777480836; val_accuracy: 0.9391918789808917 

The current subspace-distance is: 2.6234252800350077e-05 

Epoch 18 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.22; acc: 0.98
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.98
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

5.973047518637031e-05
2.590091753518209e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2643175127969426; val_accuracy: 0.9399880573248408 

The current subspace-distance is: 2.590091753518209e-05 

Epoch 19 start
The current lr is: 0.3
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.32; train_accuracy: 0.92 

6.076600038795732e-05
2.690194742172025e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25793852101845344; val_accuracy: 0.9395899681528662 

The current subspace-distance is: 2.690194742172025e-05 

Epoch 20 start
The current lr is: 0.3
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.98
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.060320811229758e-05
2.637089892232325e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2587993998721147; val_accuracy: 0.9395899681528662 

The current subspace-distance is: 2.637089892232325e-05 

Epoch 21 start
The current lr is: 0.09
Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.97
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.98
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.076844147173688e-05
2.6816835088538937e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.26077810803036783; val_accuracy: 0.9400875796178344 

The current subspace-distance is: 2.6816835088538937e-05 

Epoch 22 start
The current lr is: 0.09
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.1013721278868616e-05
2.6875894036493264e-05
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.26177110224013117; val_accuracy: 0.9382961783439491 

The current subspace-distance is: 2.6875894036493264e-05 

Epoch 23 start
The current lr is: 0.09
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.095891149016097e-05
2.5886733055813238e-05
Batch: 0; loss: 0.19; acc: 1.0
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.2660852341799979; val_accuracy: 0.9373009554140127 

The current subspace-distance is: 2.5886733055813238e-05 

Epoch 24 start
The current lr is: 0.09
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.84
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.97
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.163064244901761e-05
2.6911895474768244e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25895797508727214; val_accuracy: 0.9399880573248408 

The current subspace-distance is: 2.6911895474768244e-05 

Epoch 25 start
The current lr is: 0.09
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.97
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.1223421653267e-05
2.6976793378707953e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2585166091941724; val_accuracy: 0.9395899681528662 

The current subspace-distance is: 2.6976793378707953e-05 

Epoch 26 start
The current lr is: 0.09
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.15; acc: 1.0
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

6.0626967751886696e-05
2.606242742331233e-05
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25311250296557786; val_accuracy: 0.9392914012738853 

The current subspace-distance is: 2.606242742331233e-05 

Epoch 27 start
The current lr is: 0.09
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.98
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.2; acc: 1.0
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.097145160310902e-05
2.5466886654612608e-05
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2566210989644573; val_accuracy: 0.9389928343949044 

The current subspace-distance is: 2.5466886654612608e-05 

Epoch 28 start
The current lr is: 0.09
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.98
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.147174281068146e-05
2.667411172296852e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2561250061840768; val_accuracy: 0.9396894904458599 

The current subspace-distance is: 2.667411172296852e-05 

Epoch 29 start
The current lr is: 0.09
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.175632734084502e-05
2.6557439923635684e-05
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25428679148862315; val_accuracy: 0.9400875796178344 

The current subspace-distance is: 2.6557439923635684e-05 

Epoch 30 start
The current lr is: 0.09
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.98
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.98
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.93 

6.109828973421827e-05
2.6165125746047124e-05
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.25393823871187343; val_accuracy: 0.9392914012738853 

The current subspace-distance is: 2.6165125746047124e-05 

plots/subspace_training/table13slim/2020-01-29 15:59:47/N_4_flips_True_d_dim_500_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
plots/subspace_training/table13slim/2020-01-29 15:59:47/N_4_flips_True_d_dim_XXXXX_lr_1.0_gamma_0.3_sched_freq_10_seed_1_epochs_30_batchsize_64
